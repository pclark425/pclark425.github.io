<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-652 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-652</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-652</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-40e8af970329135ec95057d73e239dab805ad128</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/40e8af970329135ec95057d73e239dab805ad128" target="_blank">The Llama 3 Herd of Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks, and performs competitively with the state-of-the-art on image, video, and speech recognition tasks.</p>
                <p><strong>Paper Abstract:</strong> Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e652.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e652.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hardware interruptions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Training job interruptions and hardware-induced variability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical characterization of unexpected and planned interruptions during large-scale pre-training, listing root causes (faulty GPUs, memory faults, network issues) and their frequencies over a 54-day monitoring period.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B (flagship), also 8B and 70B variants mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / large-scale language model training</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Monitoring and diagnosing production-scale pre-training runs (16K GPUs) for Llama 3; root-cause analysis of interruptions and measures to increase effective training time.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Hardware failures (faulty GPUs, GPU HBM3 memory faults, GPU SRAM, GPU system processors, silent data corruption), host issues (SSD, power supply, chassis), network switch/cable failures, software bugs and dependency failures, unplanned host maintenance, NCCL watch-timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Interruption counts and percent contribution per root cause category over a 54-day period (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>466 total interruptions in 54 days; 47 planned, 419 unexpected; among unexpected interruptions, 58.7% attributed to GPU issues; faulty GPUs: 148 interruptions (30.1% of all interruptions), GPU HBM3 memory: 72 (17.2%), software bugs: 54 (12.9%), network switch/cable: 35 (8.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Synchronous training sensitivity—single GPU failure can force job restart; large-scale distributed training increases surface area for rare hardware faults and silent errors which degrade repeatability across long runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Automation for recovery and diagnosis, reduced job startup and checkpoint times, increased checkpoint frequency, use of PyTorch NCCL flight recorder for tracing, NCCLX (fork of NCCL) improvements, prioritization of suspect communications, automated cluster maintenance and monitoring, improved checkpointing strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Achieved >90% effective training time during Llama 3 pre-training; only three events required significant manual intervention during the 54-day window; many interruptions handled by automation (qualitative/operational result).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>54-day monitoring period (466 interruptions recorded; 419 unexpected)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large-scale LLM pre-training is dominated by hardware-related interruptions (≈59% GPU-related for unexpected interruptions); operational automation and tooling (NCCLX, NCCL flight recorder, faster checkpointing) substantially increased effective training time to >90% and reduced need for manual intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e652.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diurnal throughput variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diurnal GPU throughput variation due to environmental temperature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed 1–2% variation in training throughput across the day attributed to higher mid-day temperatures affecting GPU dynamic voltage and frequency scaling (DVFS).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B (flagship)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / systems engineering for LLM training</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Monitoring throughput (job-level training speed) across time-of-day during large-scale pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Environmental factors (data center temperature) leading to GPU DVFS changes; correlated temporal load patterns in data center.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Percent throughput variation relative to baseline (percent of throughput).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Observed diurnal throughput variation of 1–2% based on time-of-day measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Environmental variability (temperature) introduces small, systematic fluctuations in throughput that can affect aggregate training time and timing-sensitive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Not specifically quantified in paper (monitoring and awareness implied); operational mitigations likely involve data center cooling and scheduling but are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Observed over Llama 3 pre-training period (qualitative statement; no explicit run count)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training throughput shows a measurable diurnal cycle (≈1–2%), caused by hardware DVFS sensitivity to ambient temperature — a modest but systematic source of variability for long-duration runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e652.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stragglers & network variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stragglers, NVLink/RoCE stalls and network-induced variability in distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identification of slow/straggling devices and network stalls (NVLink, RoCE) as major sources of performance variability and training instability; engineering mitigations (NCCLX, E-ECMP, multi-flow collectives) reduce their impact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 pre-training</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / distributed systems</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Large-scale distributed training across thousands of GPUs; diagnosing communication stalls and stragglers that affect throughput and job stability.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Slow/straggling GPUs or hosts, NVLink stalls, RoCE network latency, head-of-line blocking in switches, collective communication inefficiencies, imbalance in pipeline parallelism, congestion from checkpointing or synchronized operations.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Operational traces, NCCLX/NCCL collective timings, occurrence of stalls/timeouts; Table 5 interruption breakdown informs contribution of network-related failures (e.g., network switch/cable: 35 interruptions, 8.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Network-related interruptions accounted for 8.4% of total interruptions in the 54-day snapshot; NCCLX and Enhanced-ECMP reduced congestion likelihood enabling a 24K GPU cluster run without traditional congestion control (qualitative operational outcome).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Intermittent and partially observable stalls (NVLink load/store stalls without clear errors) make root-cause attribution hard; stragglers can slow thousands of GPUs leading to non-repeatable performance characteristics and longer tail latencies.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>NCCLX (tuned/forked NCCL) to address latency-sensitive chunking and control messages, Enhanced-ECMP (E-ECMP) and creating multiple network flows (16 flows per GPU pair) for better path balancing, deep-buffer switches in spine, asynchronous point-to-point communication, targeted tracing (NCCL flight recorder), prioritized control messages, and tooling to identify top suspect communications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative improvements: reduced congestion, ability to run large clusters without DCQCN, improved failure localization and reduced manual debugging; specific numeric reductions in congestion events not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Operational monitoring during full pre-training (qualitative); specific microbenchmark counts not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Communication stalls and stragglers are a significant source of variability at scale; network-aware collective and routing changes (NCCLX, E-ECMP, multi-flow collectives) and tracing greatly improve robustness and reduce the operational impact of these variability sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e652.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scaling-law variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Noise and uncertainty in scaling-law predictions for downstream performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Discussion and experimental methodology addressing noise and unreliability in scaling laws (loss vs. compute) when extrapolating to large compute budgets, and a two-stage method to better predict downstream benchmark performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama family (scaling experiments on smaller models extrapolated to Llama 3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Models trained between 40M and 16B for scaling-law experiments; extrapolated to 405B flagship.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / theory and empirical scaling of LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Train many models under varying compute budgets (6e18 to 1e22 FLOPs) and sizes (40M–16B) to fit IsoFLOPs curves, derive compute-optimal model size and token count, and predict downstream benchmark accuracy from negative log-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Statistical noise in fits due to small compute budgets, extrapolation uncertainty over orders of magnitude, dataset/data-mix differences, measurement noise in held-out loss and downstream metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Negative log-likelihood (NLL) on validation/benchmark datasets, fitted polynomial residuals, fitted power-law parameters A and alpha, correlation between NLL and accuracy (sigmoidal relation).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Fitted power-law parameters (A, alpha) = (0.29, 0.53) predicting optimal tokens; extrapolation to 3.8e25 FLOPs suggested ~16.55T tokens and a ~402B model; their two-stage extrapolation slightly underestimated the final flagship performance (qualitative). IsoFLOPs curves flatten at minima as compute increases (qualitative robustness observation).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Scaling-law fits trained on limited compute budgets can be noisy and unreliable when extrapolating across many orders of magnitude; differences in data mix and tokenization between model families complicate direct transferability of scaling predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Two-stage methodology: (1) correlate negative log-likelihood of correct answer with FLOPs using scaling-law models; (2) correlate log-likelihood with task accuracy using both scaling-law models and previously trained larger models (Llama 2 family) to obtain a sigmoidal mapping to accuracy; fit IsoFLOPs parabolas and use power-law fit for tokens vs compute.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Method produced reasonably accurate forecasts for ARC Challenge and slightly underestimated final flagship performance (qualitative); produced actionable design choices (select 405B flagship) thanks to observed flatness near minima increasing robustness to small trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Scaling experiments across compute budgets between 6e18 and 1e22 FLOPs with models sized 40M–16B (multiple runs per budget; exact run counts not enumerated).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Scaling-law extrapolation is noisy but can be made more reliable by a two-stage approach mapping compute to NLL and NLL to accuracy; IsoFLOPs minima flatten with large compute budgets, making flagship performance robust to small changes in size-vs-tokens trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e652.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sampling variability (rejection sampling & synthetic code)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic generation variability from sampling during rejection sampling and synthetic data generation, and the use of execution feedback to mitigate it</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Describes stochasticity in model outputs (sampling K=10–30 responses during rejection sampling), synthetic code generation where ~20% initial solutions were incorrect but often self-corrected using execution feedback, and mitigation by using execution/unit tests and larger models as teachers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 and Llama 3 code expert (domain-specialized continued pre-training)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 70B, 405B variants used; code expert continued pre-trained on ~1T code tokens (size of code expert not single-numbered here).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / data generation for code-model SFT</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Rejection sampling for SFT data collection (sample K outputs, rank by reward model) and synthetic code generation with static/dynamic checks and iterative self-correction to produce high-quality SFT examples.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Sampling randomness (sampling K outputs per prompt), model competence differences (smaller models benefit from larger-model-generated data), non-deterministic generation leading to syntactic/semantic errors in generated code, evaluation noise from auto-generated unit tests and linters.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Proportion of initially incorrect solutions in synthetic code generation (percentage), and sampling hyperparameters (K outputs per prompt during RS).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Rejection sampling used K typically between 10 and 30 outputs per prompt. In synthetic code generation with execution feedback, about 20% of generated solutions were initially incorrect but self-corrected through iterative feedback and only dialogs that passed all checks were kept (i.e., ~20% required iterative correction). Annealing experiments: for Llama 3 38B, annealing improved GSM8k validation by 24.0% and MATH by 6.4%, while improvements for the 405B flagship were negligible.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Sampling-based data collection and model-generated training data introduce stochasticity; smaller models trained on self-generated data can degrade or not improve unless generation is produced by a stronger model and validated; rejection-sampled responses mix NL and code making automated quality filtering harder.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use larger/higher-quality models (code expert) to generate synthetic data; execution feedback pipeline: static analysis (parsers/linters), unit test generation and execution in containers, iterative self-correction with model prompted on failures; rejection sampling combined with a reward model to select best candidates; system prompts during RS to steer outputs; PagedAttention to make RS efficient and reduce swap-induced variability.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Execution feedback and filtering retained only dialogs that passed checks; iterative self-correction led ~20% of initially incorrect solutions to be fixed (qualitative quantitative: ~20% self-corrected). Annealing provided large gains for 38B (GSM8k +24.0%, MATH +6.4%) but negligible gains for 405B, indicating mitigation effectiveness depends on model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Rejection sampling K=10–30 outputs per prompt; synthetic dataset generation produced ~1M execution-validated coding dialogues and ~1.2M backtranslation examples (total >2.7M synthetic examples used during SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sampling introduces substantial variability; using larger/stronger models as data generators plus execution-based validation (linters, unit tests, iterative self-correction) substantially improves the quality of synthetic training data — e.g., ~20% of generated solutions were initially wrong but could be fixed via iterative feedback, and annealing benefits are scale-dependent (strong for 38B, negligible for 405B).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e652.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model averaging & Polyak</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Checkpoint/model averaging (Polyak averaging and ensemble averaging) to stabilize training and final models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Polyak (checkpoint) averaging during annealing and averaging across experimental variants (data/hyperparameters) at RM, SFT, and DPO stages to improve stability and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B and other sizes</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / optimization & model ensembling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Stabilize final pre-trained and post-trained model weights and reduce variance in outputs by averaging checkpoints and models from multiple experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Optimization noise and variability across checkpoints, sensitivity to hyperparameters and data mix across post-training rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Run-to-run variability in weight-space due to stochastic optimization and differing data/hyperparameter choices.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Polyak averaging of checkpoints during annealing; model averaging across experiments with different data/hyperparameters at each RM, SFT, or DPO stage (citing Izmailov et al. style techniques).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported as part of the training pipeline to produce final pre-trained model and to stabilize post-training models; no numeric reduction in variance provided, but used as a standard stabilizing method.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Multiple checkpoints averaged during final 40M token annealing phase; model averaging across experimental variants (exact counts unspecified).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Averaging checkpoints (Polyak) and models across variants is a practical method used in production to reduce instability and produce robust final models, though the paper does not present direct numeric variance reductions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e652.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e652.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Numerical stability methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FP32 gradient accumulation, FP32 reduce-scatter and other numerical-stability interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Address numerical instability in large-scale parallel training via FP32 accumulation of gradients across micro-batches, FP32 reduce-scatter across data-parallel workers, and FP32 accumulation of intermediate tensors' gradients to ensure training convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / numerical stability in distributed training</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Ensure convergence and numerical stability across different 4D parallelism setups (TP, CP, PP, DP) during large-scale pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Floating-point precision effects (mixed precision BF16 vs FP32), parallelism-induced numerical inconsistencies, different parallelism configurations producing small differences in training loss trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Different parallelism setups can introduce numerical discrepancies affecting convergence and reproducibility; mixed-precision operations may cause instability without careful handling.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use FP32 gradient accumulation during backward over multiple micro-batches; reduce-scatter gradients in FP32 across data-parallel workers; accumulate backward gradients for intermediate tensors in FP32; proactive deallocation of tensors to reduce memory pressure; activation of TORCH_NCCL_AVOID_RECORD_STREAMS to reduce memory usage for async PP communication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported qualitatively: fixes to numerical issues ensured training convergence and enabled stable pre-training with few loss spikes; the initial pre-training recipe was described as very stable (few loss spikes and no interventions needed). No explicit numerical variance reductions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Applied iteratively while tuning parallelism configurations and comparing training loss across setups (exact run counts not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Careful handling of numerical precision (using FP32 accumulation for gradients and certain tensors) is necessary to ensure stable convergence in massive multi-parallel training; these interventions removed numerical instabilities observed across different parallelism setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Llama 3 Herd of Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling Laws for Neural Language Models <em>(Rating: 2)</em></li>
                <li>Training Compute-Optimal Large Language Models <em>(Rating: 2)</em></li>
                <li>Averaging Weights Leads to Wider Optima and Better Generalization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-652",
    "paper_id": "paper-40e8af970329135ec95057d73e239dab805ad128",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "Hardware interruptions",
            "name_full": "Training job interruptions and hardware-induced variability",
            "brief_description": "Empirical characterization of unexpected and planned interruptions during large-scale pre-training, listing root causes (faulty GPUs, memory faults, network issues) and their frequencies over a 54-day monitoring period.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 pre-training",
            "model_size": "405B (flagship), also 8B and 70B variants mentioned",
            "scientific_domain": "machine learning / large-scale language model training",
            "experimental_task": "Monitoring and diagnosing production-scale pre-training runs (16K GPUs) for Llama 3; root-cause analysis of interruptions and measures to increase effective training time.",
            "variability_sources": "Hardware failures (faulty GPUs, GPU HBM3 memory faults, GPU SRAM, GPU system processors, silent data corruption), host issues (SSD, power supply, chassis), network switch/cable failures, software bugs and dependency failures, unplanned host maintenance, NCCL watch-timeouts.",
            "variability_measured": true,
            "variability_metrics": "Interruption counts and percent contribution per root cause category over a 54-day period (Table 5).",
            "variability_results": "466 total interruptions in 54 days; 47 planned, 419 unexpected; among unexpected interruptions, 58.7% attributed to GPU issues; faulty GPUs: 148 interruptions (30.1% of all interruptions), GPU HBM3 memory: 72 (17.2%), software bugs: 54 (12.9%), network switch/cable: 35 (8.4%).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Synchronous training sensitivity—single GPU failure can force job restart; large-scale distributed training increases surface area for rare hardware faults and silent errors which degrade repeatability across long runs.",
            "mitigation_methods": "Automation for recovery and diagnosis, reduced job startup and checkpoint times, increased checkpoint frequency, use of PyTorch NCCL flight recorder for tracing, NCCLX (fork of NCCL) improvements, prioritization of suspect communications, automated cluster maintenance and monitoring, improved checkpointing strategies.",
            "mitigation_effectiveness": "Achieved &gt;90% effective training time during Llama 3 pre-training; only three events required significant manual intervention during the 54-day window; many interruptions handled by automation (qualitative/operational result).",
            "comparison_with_without_controls": null,
            "number_of_runs": "54-day monitoring period (466 interruptions recorded; 419 unexpected)",
            "key_findings": "Large-scale LLM pre-training is dominated by hardware-related interruptions (≈59% GPU-related for unexpected interruptions); operational automation and tooling (NCCLX, NCCL flight recorder, faster checkpointing) substantially increased effective training time to &gt;90% and reduced need for manual intervention.",
            "uuid": "e652.0",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Diurnal throughput variability",
            "name_full": "Diurnal GPU throughput variation due to environmental temperature",
            "brief_description": "Observed 1–2% variation in training throughput across the day attributed to higher mid-day temperatures affecting GPU dynamic voltage and frequency scaling (DVFS).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 pre-training",
            "model_size": "405B (flagship)",
            "scientific_domain": "machine learning / systems engineering for LLM training",
            "experimental_task": "Monitoring throughput (job-level training speed) across time-of-day during large-scale pre-training.",
            "variability_sources": "Environmental factors (data center temperature) leading to GPU DVFS changes; correlated temporal load patterns in data center.",
            "variability_measured": true,
            "variability_metrics": "Percent throughput variation relative to baseline (percent of throughput).",
            "variability_results": "Observed diurnal throughput variation of 1–2% based on time-of-day measurements.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Environmental variability (temperature) introduces small, systematic fluctuations in throughput that can affect aggregate training time and timing-sensitive experiments.",
            "mitigation_methods": "Not specifically quantified in paper (monitoring and awareness implied); operational mitigations likely involve data center cooling and scheduling but are not detailed.",
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": null,
            "number_of_runs": "Observed over Llama 3 pre-training period (qualitative statement; no explicit run count)",
            "key_findings": "Training throughput shows a measurable diurnal cycle (≈1–2%), caused by hardware DVFS sensitivity to ambient temperature — a modest but systematic source of variability for long-duration runs.",
            "uuid": "e652.1",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Stragglers & network variability",
            "name_full": "Stragglers, NVLink/RoCE stalls and network-induced variability in distributed training",
            "brief_description": "Identification of slow/straggling devices and network stalls (NVLink, RoCE) as major sources of performance variability and training instability; engineering mitigations (NCCLX, E-ECMP, multi-flow collectives) reduce their impact.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 pre-training",
            "model_size": "405B",
            "scientific_domain": "machine learning / distributed systems",
            "experimental_task": "Large-scale distributed training across thousands of GPUs; diagnosing communication stalls and stragglers that affect throughput and job stability.",
            "variability_sources": "Slow/straggling GPUs or hosts, NVLink stalls, RoCE network latency, head-of-line blocking in switches, collective communication inefficiencies, imbalance in pipeline parallelism, congestion from checkpointing or synchronized operations.",
            "variability_measured": true,
            "variability_metrics": "Operational traces, NCCLX/NCCL collective timings, occurrence of stalls/timeouts; Table 5 interruption breakdown informs contribution of network-related failures (e.g., network switch/cable: 35 interruptions, 8.4%).",
            "variability_results": "Network-related interruptions accounted for 8.4% of total interruptions in the 54-day snapshot; NCCLX and Enhanced-ECMP reduced congestion likelihood enabling a 24K GPU cluster run without traditional congestion control (qualitative operational outcome).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Intermittent and partially observable stalls (NVLink load/store stalls without clear errors) make root-cause attribution hard; stragglers can slow thousands of GPUs leading to non-repeatable performance characteristics and longer tail latencies.",
            "mitigation_methods": "NCCLX (tuned/forked NCCL) to address latency-sensitive chunking and control messages, Enhanced-ECMP (E-ECMP) and creating multiple network flows (16 flows per GPU pair) for better path balancing, deep-buffer switches in spine, asynchronous point-to-point communication, targeted tracing (NCCL flight recorder), prioritized control messages, and tooling to identify top suspect communications.",
            "mitigation_effectiveness": "Qualitative improvements: reduced congestion, ability to run large clusters without DCQCN, improved failure localization and reduced manual debugging; specific numeric reductions in congestion events not provided.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Operational monitoring during full pre-training (qualitative); specific microbenchmark counts not provided.",
            "key_findings": "Communication stalls and stragglers are a significant source of variability at scale; network-aware collective and routing changes (NCCLX, E-ECMP, multi-flow collectives) and tracing greatly improve robustness and reduce the operational impact of these variability sources.",
            "uuid": "e652.2",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Scaling-law variability",
            "name_full": "Noise and uncertainty in scaling-law predictions for downstream performance",
            "brief_description": "Discussion and experimental methodology addressing noise and unreliability in scaling laws (loss vs. compute) when extrapolating to large compute budgets, and a two-stage method to better predict downstream benchmark performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama family (scaling experiments on smaller models extrapolated to Llama 3)",
            "model_size": "Models trained between 40M and 16B for scaling-law experiments; extrapolated to 405B flagship.",
            "scientific_domain": "machine learning / theory and empirical scaling of LLMs",
            "experimental_task": "Train many models under varying compute budgets (6e18 to 1e22 FLOPs) and sizes (40M–16B) to fit IsoFLOPs curves, derive compute-optimal model size and token count, and predict downstream benchmark accuracy from negative log-likelihood.",
            "variability_sources": "Statistical noise in fits due to small compute budgets, extrapolation uncertainty over orders of magnitude, dataset/data-mix differences, measurement noise in held-out loss and downstream metrics.",
            "variability_measured": true,
            "variability_metrics": "Negative log-likelihood (NLL) on validation/benchmark datasets, fitted polynomial residuals, fitted power-law parameters A and alpha, correlation between NLL and accuracy (sigmoidal relation).",
            "variability_results": "Fitted power-law parameters (A, alpha) = (0.29, 0.53) predicting optimal tokens; extrapolation to 3.8e25 FLOPs suggested ~16.55T tokens and a ~402B model; their two-stage extrapolation slightly underestimated the final flagship performance (qualitative). IsoFLOPs curves flatten at minima as compute increases (qualitative robustness observation).",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Scaling-law fits trained on limited compute budgets can be noisy and unreliable when extrapolating across many orders of magnitude; differences in data mix and tokenization between model families complicate direct transferability of scaling predictions.",
            "mitigation_methods": "Two-stage methodology: (1) correlate negative log-likelihood of correct answer with FLOPs using scaling-law models; (2) correlate log-likelihood with task accuracy using both scaling-law models and previously trained larger models (Llama 2 family) to obtain a sigmoidal mapping to accuracy; fit IsoFLOPs parabolas and use power-law fit for tokens vs compute.",
            "mitigation_effectiveness": "Method produced reasonably accurate forecasts for ARC Challenge and slightly underestimated final flagship performance (qualitative); produced actionable design choices (select 405B flagship) thanks to observed flatness near minima increasing robustness to small trade-offs.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Scaling experiments across compute budgets between 6e18 and 1e22 FLOPs with models sized 40M–16B (multiple runs per budget; exact run counts not enumerated).",
            "key_findings": "Scaling-law extrapolation is noisy but can be made more reliable by a two-stage approach mapping compute to NLL and NLL to accuracy; IsoFLOPs minima flatten with large compute budgets, making flagship performance robust to small changes in size-vs-tokens trade-offs.",
            "uuid": "e652.3",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Sampling variability (rejection sampling & synthetic code)",
            "name_full": "Stochastic generation variability from sampling during rejection sampling and synthetic data generation, and the use of execution feedback to mitigate it",
            "brief_description": "Describes stochasticity in model outputs (sampling K=10–30 responses during rejection sampling), synthetic code generation where ~20% initial solutions were incorrect but often self-corrected using execution feedback, and mitigation by using execution/unit tests and larger models as teachers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3 and Llama 3 code expert (domain-specialized continued pre-training)",
            "model_size": "8B, 70B, 405B variants used; code expert continued pre-trained on ~1T code tokens (size of code expert not single-numbered here).",
            "scientific_domain": "machine learning / data generation for code-model SFT",
            "experimental_task": "Rejection sampling for SFT data collection (sample K outputs, rank by reward model) and synthetic code generation with static/dynamic checks and iterative self-correction to produce high-quality SFT examples.",
            "variability_sources": "Sampling randomness (sampling K outputs per prompt), model competence differences (smaller models benefit from larger-model-generated data), non-deterministic generation leading to syntactic/semantic errors in generated code, evaluation noise from auto-generated unit tests and linters.",
            "variability_measured": true,
            "variability_metrics": "Proportion of initially incorrect solutions in synthetic code generation (percentage), and sampling hyperparameters (K outputs per prompt during RS).",
            "variability_results": "Rejection sampling used K typically between 10 and 30 outputs per prompt. In synthetic code generation with execution feedback, about 20% of generated solutions were initially incorrect but self-corrected through iterative feedback and only dialogs that passed all checks were kept (i.e., ~20% required iterative correction). Annealing experiments: for Llama 3 38B, annealing improved GSM8k validation by 24.0% and MATH by 6.4%, while improvements for the 405B flagship were negligible.",
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Sampling-based data collection and model-generated training data introduce stochasticity; smaller models trained on self-generated data can degrade or not improve unless generation is produced by a stronger model and validated; rejection-sampled responses mix NL and code making automated quality filtering harder.",
            "mitigation_methods": "Use larger/higher-quality models (code expert) to generate synthetic data; execution feedback pipeline: static analysis (parsers/linters), unit test generation and execution in containers, iterative self-correction with model prompted on failures; rejection sampling combined with a reward model to select best candidates; system prompts during RS to steer outputs; PagedAttention to make RS efficient and reduce swap-induced variability.",
            "mitigation_effectiveness": "Execution feedback and filtering retained only dialogs that passed checks; iterative self-correction led ~20% of initially incorrect solutions to be fixed (qualitative quantitative: ~20% self-corrected). Annealing provided large gains for 38B (GSM8k +24.0%, MATH +6.4%) but negligible gains for 405B, indicating mitigation effectiveness depends on model scale.",
            "comparison_with_without_controls": true,
            "number_of_runs": "Rejection sampling K=10–30 outputs per prompt; synthetic dataset generation produced ~1M execution-validated coding dialogues and ~1.2M backtranslation examples (total &gt;2.7M synthetic examples used during SFT).",
            "key_findings": "Sampling introduces substantial variability; using larger/stronger models as data generators plus execution-based validation (linters, unit tests, iterative self-correction) substantially improves the quality of synthetic training data — e.g., ~20% of generated solutions were initially wrong but could be fixed via iterative feedback, and annealing benefits are scale-dependent (strong for 38B, negligible for 405B).",
            "uuid": "e652.4",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Model averaging & Polyak",
            "name_full": "Checkpoint/model averaging (Polyak averaging and ensemble averaging) to stabilize training and final models",
            "brief_description": "Use of Polyak (checkpoint) averaging during annealing and averaging across experimental variants (data/hyperparameters) at RM, SFT, and DPO stages to improve stability and generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3",
            "model_size": "405B and other sizes",
            "scientific_domain": "machine learning / optimization & model ensembling",
            "experimental_task": "Stabilize final pre-trained and post-trained model weights and reduce variance in outputs by averaging checkpoints and models from multiple experiments.",
            "variability_sources": "Optimization noise and variability across checkpoints, sensitivity to hyperparameters and data mix across post-training rounds.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Run-to-run variability in weight-space due to stochastic optimization and differing data/hyperparameter choices.",
            "mitigation_methods": "Polyak averaging of checkpoints during annealing; model averaging across experiments with different data/hyperparameters at each RM, SFT, or DPO stage (citing Izmailov et al. style techniques).",
            "mitigation_effectiveness": "Reported as part of the training pipeline to produce final pre-trained model and to stabilize post-training models; no numeric reduction in variance provided, but used as a standard stabilizing method.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Multiple checkpoints averaged during final 40M token annealing phase; model averaging across experimental variants (exact counts unspecified).",
            "key_findings": "Averaging checkpoints (Polyak) and models across variants is a practical method used in production to reduce instability and produce robust final models, though the paper does not present direct numeric variance reductions.",
            "uuid": "e652.5",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Numerical stability methods",
            "name_full": "FP32 gradient accumulation, FP32 reduce-scatter and other numerical-stability interventions",
            "brief_description": "Address numerical instability in large-scale parallel training via FP32 accumulation of gradients across micro-batches, FP32 reduce-scatter across data-parallel workers, and FP32 accumulation of intermediate tensors' gradients to ensure training convergence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3",
            "model_size": "405B",
            "scientific_domain": "machine learning / numerical stability in distributed training",
            "experimental_task": "Ensure convergence and numerical stability across different 4D parallelism setups (TP, CP, PP, DP) during large-scale pre-training.",
            "variability_sources": "Floating-point precision effects (mixed precision BF16 vs FP32), parallelism-induced numerical inconsistencies, different parallelism configurations producing small differences in training loss trajectories.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": null,
            "reproducibility_challenges": "Different parallelism setups can introduce numerical discrepancies affecting convergence and reproducibility; mixed-precision operations may cause instability without careful handling.",
            "mitigation_methods": "Use FP32 gradient accumulation during backward over multiple micro-batches; reduce-scatter gradients in FP32 across data-parallel workers; accumulate backward gradients for intermediate tensors in FP32; proactive deallocation of tensors to reduce memory pressure; activation of TORCH_NCCL_AVOID_RECORD_STREAMS to reduce memory usage for async PP communication.",
            "mitigation_effectiveness": "Reported qualitatively: fixes to numerical issues ensured training convergence and enabled stable pre-training with few loss spikes; the initial pre-training recipe was described as very stable (few loss spikes and no interventions needed). No explicit numerical variance reductions reported.",
            "comparison_with_without_controls": null,
            "number_of_runs": "Applied iteratively while tuning parallelism configurations and comparing training loss across setups (exact run counts not provided).",
            "key_findings": "Careful handling of numerical precision (using FP32 accumulation for gradients and certain tensors) is necessary to ensure stable convergence in massive multi-parallel training; these interventions removed numerical instabilities observed across different parallelism setups.",
            "uuid": "e652.6",
            "source_info": {
                "paper_title": "The Llama 3 Herd of Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling Laws for Neural Language Models",
            "rating": 2
        },
        {
            "paper_title": "Training Compute-Optimal Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Averaging Weights Leads to Wider Optima and Better Generalization",
            "rating": 2
        }
    ],
    "cost": 0.01893075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Llama 3 Herd of Models</h1>
<p>Llama Team, AI @ Meta ${ }^{1}$<br>${ }^{1}$ A detailed contributor list can be found in the appendix of this paper.</p>
<h4>Abstract</h4>
<p>Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128 K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.</p>
<p>Date: July 23, 2024
Website: https://llama.meta.com/</p>
<h2>1 Introduction</h2>
<p>Foundation models are general models of language, vision, speech, and/or other modalities that are designed to support a large variety of AI tasks. They form the basis of many modern AI systems.</p>
<p>The development of modern foundation models consists of two main stages: (1) a pre-training stage in which the model is trained at massive scale using straightforward tasks such as next-word prediction or captioning and (2) a post-training stage in which the model is tuned to follow instructions, align with human preferences, and improve specific capabilities (for example, coding and reasoning).</p>
<p>In this paper, we present a new set of foundation models for language, called Llama 3. The Llama 3 Herd of models natively supports multilinguality, coding, reasoning, and tool usage. Our largest model is dense Transformer with 405B parameters, processing information in a context window of up to 128 K tokens. Each member of the herd is listed in Table 1. All the results presented in this paper are for the Llama 3.1 models, which we will refer to as Llama 3 throughout for brevity.</p>
<p>We believe there are three key levers in the development of high-quality foundation models: data, scale, and managing complexity. We seek to optimize for these three levers in our development process:</p>
<ul>
<li>Data. Compared to prior versions of Llama (Touvron et al., 2023a,b), we improved both the quantity and quality of the data we use for pre-training and post-training. These improvements include the development of more careful pre-processing and curation pipelines for pre-training data and the development of more rigorous quality assurance and filtering approaches for post-training data. We pre-train Llama 3 on a corpus of about 15 T multilingual tokens, compared to 1.8 T tokens for Llama 2.</li>
<li>Scale. We train a model at far larger scale than previous Llama models: our flagship language model was pre-trained using $3.8 \times 10^{25}$ FLOPs, almost $50 \times$ more than the largest version of Llama 2. Specifically, we pre-trained a flagship model with 405B trainable parameters on 15.6 T text tokens. As expected per</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>Finetuned</th>
<th>Multilingual</th>
<th>Long context</th>
<th>Tool use</th>
<th>Release</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.5 Llama 3 8B</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>April 2024</td>
</tr>
<tr>
<td>1.5 Llama 3 8B Instruct</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>April 2024</td>
</tr>
<tr>
<td>1.5 Llama 3 70B</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>April 2024</td>
</tr>
<tr>
<td>1.5 Llama 3 70B Instruct</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>April 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 8B</td>
<td>✗</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
<td>July 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 8B Instruct</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>July 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 70B</td>
<td>✗</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
<td>July 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 70B Instruct</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>July 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 405B</td>
<td>✗</td>
<td>✓</td>
<td>✓</td>
<td>✗</td>
<td>July 2024</td>
</tr>
<tr>
<td>1.5 Llama 3.1 405B Instruct</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>July 2024</td>
</tr>
</tbody>
</table>
<p>Table 1 Overview of the Llama 3 Herd of models. All results in this paper are for the Llama 3.1 models.</p>
<p>scaling laws for foundation models, our flagship model outperforms smaller models trained using the same procedure. While our scaling laws suggest our flagship model is an approximately compute-optimal size for our training budget, we also train our smaller models for much longer than is compute-optimal. The resulting models perform better than compute-optimal models at the same inference budget. We use the flagship model to further improve the quality of those smaller models during post-training.</p>
<ul>
<li>Managing complexity. We make design choices that seek to maximize our ability to scale the model development process. For example, we opt for a standard dense Transformer model architecture Vaswani2017 with minor adaptations, rather than for a mixture-of-experts model Shazeer2017 to maximize training stability. Similarly, we adopt a relatively simple post-training procedure based on supervised finetuning (SFT), rejection sampling (RS), and direct preference optimization (DPO; Rafailov2023) as opposed to more complex reinforcement learning algorithms Ouyang2022, Schulman2017 that tend to be less stable and harder to scale.</li>
</ul>
<p>The result of our work is Llama 3: a herd of three multilingual language models with 8B, 70B, and 405B parameters. We evaluate the performance of Llama 3 on a plethora of benchmark datasets that span a wide range of language understanding tasks. In addition, we perform extensive human evaluations that compare Llama 3 with competing models. An overview of the performance of the flagship Llama 3 model on key benchmarks is presented in Table 2. Our experimental evaluation suggests that our flagship model performs on par with leading language models such as GPT-4 OpenAI2023a across a variety of tasks, and is close to matching the state-of-the-art. Our smaller models are best-in-class, outperforming alternative models with similar numbers of parameters Bai2023, Jiang2023. Llama 3 also delivers a much better balance between helpfulness and harmlessness than its predecessor Touvron2023. We present a detailed analysis of the safety of Llama 3 in Section 5.4.</p>
<p>We are publicly releasing all three Llama 3 models under an updated version of the Llama 3 Community License; see https://llama.meta.com. This includes pre-trained and post-trained versions of our 405B parameter language model and a new version of our Llama Guard model Inan2023 for input and output safety. We hope that the open release of a flagship model will spur a wave of innovation in the research community, and accelerate a responsible path towards the development of artificial general intelligence (AGI).</p>
<p>As part of the Llama 3 development process we also develop multimodal extensions to the models, enabling image recognition, video recognition, and speech understanding capabilities. These models are still under active development and not yet ready for release. In addition to our language modeling results, the paper presents results of our initial experiments with those multimodal models.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Llama 3 BB</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MMLU (5-shot)</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">61.1</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">89.9</td>
</tr>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;">MMLU (0-shot, CoT)</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">$72.3^{\triangle}$</td>
<td style="text-align: center;">60.5</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">$78.7^{\circ}$</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MMLU-Pro (5-shot, CoT)</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">36.9</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">IFEval</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">57.6</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;">Code</td>
<td style="text-align: center;">HumanEval (0-shot)</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBPP EvalPlus (0-shot)</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">88.6</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">90.5</td>
</tr>
<tr>
<td style="text-align: center;">Math</td>
<td style="text-align: center;">GSM8K (8-shot, CoT)</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">$92.3^{\triangle}$</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">$96.4^{\triangle}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MATH (0-shot, CoT)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">43.1</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">71.1</td>
</tr>
<tr>
<td style="text-align: center;">Reasoning</td>
<td style="text-align: center;">ARC Challenge (0-shot)</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">96.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPQA (0-shot, CoT)</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: center;">Tool use</td>
<td style="text-align: center;">BFCL</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">90.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Nexus</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.7</td>
<td style="text-align: center;">56.7</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: center;">Long context</td>
<td style="text-align: center;">ZeroSCROLLS/QuALITY</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">95.2</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">90.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">InfiniteBench/En.MC</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">NIH/Multi-needle</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">98.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">90.8</td>
</tr>
<tr>
<td style="text-align: center;">Multilingual</td>
<td style="text-align: center;">MGSM (0-shot, CoT)</td>
<td style="text-align: center;">68.9</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">29.9</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;">91.6</td>
</tr>
</tbody>
</table>
<p>Table 2 Performance of finetuned Llama 3 models on key benchmark evaluations. The table compares the performance of the 8B, 70B, and 405B versions of Llama 3 with that of competing models. We boldface the best-performing model in each of three model-size equivalence classes. ${ }^{\triangle}$ Results obtained using 5-shot prompting (no CoT). ${ }^{\circ}$ Results obtained without CoT. ${ }^{\triangle}$ Results obtained using zero-shot prompting.</p>
<h1>2 General Overview</h1>
<p>The model architecture of Llama 3 is illustrated in Figure 1. The development of our Llama 3 language models comprises two main stages:</p>
<ul>
<li>Language model pre-training. We start by converting a large, multilingual text corpus to discrete tokens and pre-training a large language model (LLM) on the resulting data to perform next-token prediction. In the language model pre-training stage, the model learns the structure of language and obtains large amounts of knowledge about the world from the text it is "reading". To do this effectively, pre-training is performed at massive scale: we pre-train a model with 405B parameters on 15.6 T tokens using a context window of 8 K tokens. This standard pre-training stage is followed by a continued pre-training stage that increases the supported context window to 128 K tokens. See Section 3 for details.</li>
<li>Language model post-training. The pre-trained language model has a rich understanding of language but it does not yet follow instructions or behave in the way we would expect an assistant to. We align the model with human feedback in several rounds, each of which involves supervised finetuning (SFT) on instruction tuning data and Direct Preference Optimization (DPO; [Rafailov et al., 2024]). At this post-training ${ }^{2}$ stage, we also integrate new capabilities, such as tool-use, and observe strong improvements in other areas, such as coding and reasoning. See Section 4 for details. Finally, safety mitigations are also incorporated into the model at the post-training stage, the details of which are described in Section 5.4.</li>
</ul>
<p>The resulting models have a rich set of capabilities. They can answer questions in at least eight languages, write high-quality code, solve complex reasoning problems, and use tools out-of-the-box or in a zero-shot way.</p>
<p>We also perform experiments in which we add image, video, and speech capabilities to Llama 3 using a compositional approach. The approach we study comprises the three additional stages illustrated in Figure 28:</p>
<ul>
<li>Multi-modal encoder pre-training. We train separate encoders for images and speech. We train our image encoder on large amounts of image-text pairs. This teaches the model the relation between visual content and the description of that content in natural language. Our speech encoder is trained using a</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1 Illustration of the overall architecture and training of Llama 3. Llama 3 is a Transformer language model trained to predict the next token of a textual sequence. See text for details.
self-supervised approach that masks out parts of the speech inputs and tries to reconstruct the masked out parts via a discrete-token representation. As a result, the model learns the structure of speech signals. See Section 7 for details on the image encoder and Section 8 for details on the speech encoder.</p>
<ul>
<li>Vision adapter training. We train an adapter that integrates the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed imageencoder representations into the language model. The adapter is trained on text-image pairs. This aligns the image representations with the language representations. During adapter training, we also update the parameters of the image encoder but we intentionally do not update the language-model parameters. We also train a video adapter on top of the image adapter on paired video-text data. This enables the model to aggregate information across frames. See Section 7 for details.</li>
<li>Speech adapter training. Finally, we integrate the speech encoder into the model via an adapter that converts speech encodings into token representations that can be fed directly into the finetuned language model. The parameters of the adapter and encoder are jointly updated in a supervised finetuning stage to enable high-quality speech understanding. We do not change the language model during speech adapter training. We also integrate a text-to-speech system. See Section 8 for details.</li>
</ul>
<p>Our multimodal experiments lead to models that can recognize the content of images and videos, and support interaction via a speech interface. These models are still under development and not yet ready for release.</p>
<h1>3 Pre-Training</h1>
<p>Language model pre-training involves: (1) the curation and filtering of a large-scale training corpus, (2) the development of a model architecture and corresponding scaling laws for determining model size, (3) the development of techniques for efficient pre-training at large scale, and (4) the development of a pre-training recipe. We present each of these components separately below.</p>
<h3>3.1 Pre-Training Data</h3>
<p>We create our dataset for language model pre-training from a variety of data sources containing knowledge until the end of 2023. We apply several de-duplication methods and data cleaning mechanisms on each data source to obtain high-quality tokens. We remove domains that contain large amounts of personally identifiable information (PII), and domains with known adult content.</p>
<h3>3.1.1 Web Data Curation</h3>
<p>Much of the data we utilize is obtained from the web and we describe our cleaning process below.
PII and safety filtering. Among other mitigations, we implement filters designed to remove data from websites are likely to contain unsafe content or high volumes of PII, domains that have been ranked as harmful according to a variety of Meta safety standards, and domains that are known to contain adult content.</p>
<p>Text extraction and cleaning. We process the raw HTML content for non-truncated web documents to extract high-quality diverse text. To do so, we build a custom parser that extracts the HTML content and optimizes for precision in boilerplate removal and content recall. We evaluate our parser's quality in human evaluations, comparing it with popular third-party HTML parsers that optimize for article-like content, and found it to perform favorably. We carefully process HTML pages with mathematics and code content to preserve the structure of that content. We maintain the image alt attribute text since mathematical content is often represented as pre-rendered images where the math is also provided in the alt attribute. We experimentally evaluate different cleaning configurations. We find markdown is harmful to the performance of a model that is primarily trained on web data compared to plain text, so we remove all markdown markers.</p>
<p>De-duplication. We apply several rounds of de-duplication at the URL, document, and line level:</p>
<ul>
<li>URL-level de-duplication. We perform URL-level de-duplication across the entire dataset. We keep the most recent version for pages corresponding to each URL.</li>
<li>Document-level de-duplication. We perform global MinHash (Broder, 1997) de-duplication across the entire dataset to remove near duplicate documents.</li>
<li>Line-level de-duplication. We perform aggressive line-level de-duplication similar to ccNet (Wenzek et al., 2019). We remove lines that appeared more than 6 times in each bucket of 30 M documents. Although our manual qualitative analysis showed that the line-level de-duplication removes not only leftover boilerplate from various websites such as navigation menus, cookie warnings, but also frequent high-quality text, our empirical evaluations showed strong improvements.</li>
</ul>
<p>Heuristic filtering. We develop heuristics to remove additional low-quality documents, outliers, and documents with excessive repetitions. Some examples of heuristics include:</p>
<ul>
<li>We use duplicated n-gram coverage ratio (Rae et al., 2021) to remove lines that consist of repeated content such as logging or error messages. Those lines could be very long and unique, hence cannot be filtered by line-dedup.</li>
<li>We use "dirty word" counting (Raffel et al., 2020) to filter out adult websites that are not covered by domain block lists.</li>
<li>We use a token-distribution Kullback-Leibler divergence to filter out documents containing excessive numbers of outlier tokens compared to the training corpus distribution.</li>
</ul>
<p>Model-based quality filtering. Further, we experiment with applying various model-based quality classifiers to sub-select high-quality tokens. These include using fast classifiers such as fasttext (Joulin et al., 2017) trained to recognize if a given text would be referenced by Wikipedia (Touvron et al., 2023a), as well as more compute-intensive Roberta-based classifiers (Liu et al., 2019a) trained on Llama 2 predictions. To train a quality classifier based on Llama 2, we create a training set of cleaned web documents, describe the quality requirements, and instruct Llama 2's chat model to determine if the documents meets these requirements. We use DistilRoberta (Sanh et al., 2019) to generate quality scores for each document for efficiency reasons. We experimentally evaluate the efficacy of various quality filtering configurations.</p>
<p>Code and reasoning data. Similar to DeepSeek-AI et al. (2024), we build domain-specific pipelines that extract code and math-relevant web pages. Specifically, both the code and reasoning classifiers are DistilRoberta models trained on web data annotated by Llama 2. Unlike the general quality classifier mentioned above, we conduct prompt tuning to target web pages containing math deduction, reasoning in STEM areas and code interleaved with natural language. Since the token distribution of code and math is substantially different than that of natural language, these pipelines implement domain-specific HTML extraction, customized text features and heuristics for filtering.</p>
<p>Multilingual data. Similar to our processing pipelines for English described above, we implement filters to remove data from websites that are likely to contain PII or unsafe content. Our multilingual text processing pipeline has several unique features:</p>
<ul>
<li>We use a fasttext-based language identification model to categorize documents into 176 languages.</li>
<li>
<p>We perform document-level and line-level de-duplication within data for each language.</p>
</li>
<li>
<p>We apply language-specific heuristics and model-based filters to remove low-quality documents.</p>
</li>
</ul>
<p>In addition, we perform quality ranking of multilingual documents using a multilingual Llama 2-based classifier to ensure that high-quality content is prioritized. We determine the amount of multilingual tokens used in pre-training experimentally, balancing model performance on English and multilingual benchmarks.</p>
<h1>3.1.2 Determining the Data Mix</h1>
<p>To obtain a high-quality language model, it is essential to carefully determine the proportion of different data sources in the pre-training data mix. Our main tools in determining this data mix are knowledge classification and scaling law experiments.</p>
<p>Knowledge classification. We develop a classifier to categorize the types of information contained in our web data to more effectively determine a data mix. We use this classifier to downsample data categories that are over-represented on the web, for example, arts and entertainment.</p>
<p>Scaling laws for data mix. To determine the best data mix, we perform scaling law experiments in which we train several small models on a data mix and use that to predict the performance of a large model on that mix (see Section 3.2.1). We repeat this process multiple times for different data mixes to select a new data mix candidate. Subsequently, we train a larger model on this candidate data mix and evaluate the performance of that model on several key benchmarks.</p>
<p>Data mix summary. Our final data mix contains roughly $50 \%$ of tokens corresponding to general knowledge, $25 \%$ of mathematical and reasoning tokens, $17 \%$ code tokens, and $8 \%$ multilingual tokens.</p>
<h3>3.1.3 Annealing Data</h3>
<p>Empirically, we find that annealing (see Section 3.4.3) on small amounts of high-quality code and mathematical data can boost the performance of pre-trained models on key benchmarks. Akin to Li et al. (2024b), we perform annealing with a data mix that upsamples high-quality data in select domains. We do not include any training sets from commonly used benchmarks in our annealing data. This enables us to assess the true few-shot learning capabilities and out-of-domain generalization of Llama 3.</p>
<p>Following OpenAI (2023a), we evaluate the efficacy of annealing on the GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) training sets in annealing. We find that annealing improved the performance of a pre-trained Llama 38 B model on the GSM8k and MATH validation sets by $24.0 \%$ and $6.4 \%$, respectively. However, the improvements on the 405B model are negligible, suggesting that our flagship model has strong in-context learning and reasoning capabilities and does not require specific in-domain training samples to obtain strong performance.</p>
<p>Using annealing to assess data quality. Similar to Blakeney et al. (2024), we find that annealing enables us to judge the value of small domain-specific datasets. We measure the value of such datasets by annealing the learning rate of a $50 \%$ trained Llama 38 B model linearly to 0 on 40 B tokens. In those experiments, we assign $30 \%$ weight to the new dataset and the remaining $70 \%$ weight to the default data mix. Using annealing to evaluate new data sources is more efficient than performing scaling law experiments for every small dataset.</p>
<h3>3.2 Model Architecture</h3>
<p>Llama 3 uses a standard, dense Transformer architecture (Vaswani et al., 2017). It does not deviate significantly from Llama and Llama 2 (Touvron et al., 2023a,b) in terms of model architecture; our performance gains are primarily driven by improvements in data quality and diversity as well as by increased training scale.</p>
<p>We make a few small modifications compared to Llama 2:</p>
<ul>
<li>We use grouped query attention (GQA; Ainslie et al. (2023)) with 8 key-value heads to improve inference speed and to reduce the size of key-value caches during decoding.</li>
<li>We use an attention mask that prevents self-attention between different documents within the same sequence. We find that this change had limited impact during in standard pre-training, but find it to be important in continued pre-training on very long sequences.</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">8B</th>
<th style="text-align: center;">70B</th>
<th style="text-align: center;">405B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Layers</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">126</td>
</tr>
<tr>
<td style="text-align: left;">Model Dimension</td>
<td style="text-align: center;">4,096</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">16,384</td>
</tr>
<tr>
<td style="text-align: left;">FFN Dimension</td>
<td style="text-align: center;">14,336</td>
<td style="text-align: center;">28,672</td>
<td style="text-align: center;">53,248</td>
</tr>
<tr>
<td style="text-align: left;">Attention Heads</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">Key/Value Heads</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Peak Learning Rate</td>
<td style="text-align: center;">$3 \times 10^{-4}$</td>
<td style="text-align: center;">$1.5 \times 10^{-4}$</td>
<td style="text-align: center;">$8 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: left;">Activation Function</td>
<td style="text-align: center;">SwiGLU</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Vocabulary Size</td>
<td style="text-align: center;">128,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Positional Embeddings</td>
<td style="text-align: center;">RoPE $(\theta=500,000)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3 Overview of the key hyperparameters of Llama 3. We display settings for 8B, 70B, and 405B language models.</p>
<ul>
<li>We use a vocabulary with 128 K tokens. Our token vocabulary combines 100 K tokens from the tiktoken ${ }^{3}$ tokenizer with 28 K additional tokens to better support non-English languages. Compared to the Llama 2 tokenizer, our new tokenizer improves compression rates on a sample of English data from 3.17 to 3.94 characters per token. This enables the model to "read" more text for the same amount of training compute. We also found that adding 28 K tokens from select non-English languages improved both compression ratios and downstream performance, with no impact on English tokenization.</li>
<li>We increase the RoPE base frequency hyperparameter to 500,000 . This enables us to better support longer contexts; Xiong et al. (2023) showed this value to be effective for context lengths up to 32,768 .</li>
</ul>
<p>Llama 3405 B uses an architecture with 126 layers, a token representation dimension of 16,384 , and 128 attention heads; see Table 3 for details. This leads to a model size that is approximately compute-optimal according to scaling laws on our data for our training budget of $3.8 \times 10^{25}$ FLOPs.</p>
<h1>3.2.1 Scaling Laws</h1>
<p>We develop scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020) to determine the optimal model size for our flagship model given our pre-training compute budget. In addition to determining the optimal model size, a major challenge is to forecast the flagship model's performance on downstream benchmark tasks, due to a couple of issues: (1) Existing scaling laws typically predict only next-token prediction loss rather than specific benchmark performance. (2) Scaling laws can be noisy and unreliable because they are developed based on pre-training runs conducted with small compute budgets (Wei et al., 2022b).
To address these challenges, we implement a two-stage methodology to develop scaling laws that accurately predict downstream benchmark performance:</p>
<ol>
<li>We first establish a correlation between the compute-optimal model's negative log-likelihood on downstream tasks and the training FLOPs.</li>
<li>Next, we correlate the negative log-likelihood on downstream tasks with task accuracy, utilizing both the scaling law models and older models trained with higher compute FLOPs. In this step, we specifically leverage the Llama 2 family of models.</li>
</ol>
<p>This approach enables us to predict downstream task performance given a specific number of training FLOPs for compute-optimal models. We use a similar method to select our pre-training data mix (see Section 3.4).</p>
<p>Scaling law experiments. Concretely, we construct our scaling laws by pre-training models using compute budgets between $6 \times 10^{18}$ FLOPs and $10^{22}$ FLOPs. At each compute budget, we pre-train models ranging in size between 40 M and 16 B parameters, using a subset of model sizes at each compute budget. In these training runs, we use a cosine learning rate schedule with a linear warmup for 2,000 training steps. The peak learning rate is set between $2 \times 10^{-4}$ and $4 \times 10^{-4}$ depending on the size of the model. We set the cosine decay to 0.1 of the peak value. The weight decay at each step is set to 0.1 times the learning rate at that step. We use a fixed batch size for each compute scale, ranging between 250 K and 4 M .</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2 Scaling law IsoFLOPs curves between $6 \times 10^{18}$ and $10^{22}$ FLOPs. The loss is the negative loglikelihood on a held-out validation set. We approximate measurements at each compute scale using a second degree polynomial.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3 Number of training tokens in identified computeoptimal models as a function of pre-training compute budget. We include the fitted scaling-law prediction as well. The compute-optimal models correspond to the parabola minimums in Figure 2.</p>
<p>These experiments give rise to the IsoFLOPs curves in Figure 2. The loss in these curves is measured on a separate validation set. We fit the measured loss values using a second-degree polynomial and identify the minimums of each parabola. We refer to minimum of a parabola as the compute-optimal model at the corresponding pre-training compute budget.</p>
<p>We use the compute-optimal models we identified this way to predict the optimal number of training tokens for a specific compute budget. To do so, we assume a power-law relation between compute budget, $C$, and the optimal number of training tokens, $N^{\star}(C)$ :</p>
<p>$$
N^{\star}(C)=A C^{\alpha}
$$</p>
<p>We fit $A$ and $\alpha$ using the data from Figure 2. We find that $(\alpha, A)=(0.53,0.29)$; the corresponding fit is shown in Figure 3. Extrapolation of the resulting scaling law to $3.8 \times 10^{25}$ FLOPs suggests training a 402B parameter model on 16.55 T tokens.</p>
<p>An important observation is that IsoFLOPs curves become flatter around the minimum as the compute budget increases. This implies that performance of the flagship model is relatively robust to small changes in the trade-off between model size and training tokens. Based on this observation, we ultimately decided to train a flagship model with 405B parameters.</p>
<p>Predicting performance on downstream tasks. We use the resulting compute-optimal models to forecast the performance of the flagship Llama 3 model on benchmark data sets. First, we linearly correlate the (normalized) negative log-likelihood of correct answer in the benchmark and the training FLOPs. In this analysis, we use only the scaling law models trained up to $10^{22}$ FLOPs on the data mix described above. Next, we establish a sigmoidal relation between the log-likelihood and accuracy using both the scaling law models and Llama 2 models, which were trained using the Llama 2 data mix and tokenizer. We show the results of this experiment on the ARC Challenge benchmark in Figure 4). We find this two-step scaling law prediction, which extrapolates over four orders of magnitude, to be quite accurate: it only slightly underestimates the final performance of the flagship Llama 3 model.</p>
<h1>3.3 Infrastructure, Scaling, and Efficiency</h1>
<p>We describe our hardware and infrastructure that powered Llama 3405 B pre-training at scale and discuss several optimizations that leads to improvements in training efficiency.</p>
<h3>3.3.1 Training Infrastructure</h3>
<p>The Llama 1 and 2 models were trained on Meta's AI Research SuperCluster (Lee and Sengupta, 2022). As we scaled further, the training for Llama 3 was migrated to Meta's production clusters (Lee et al., 2024).This</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4 Scaling law forecast for ARC Challenge. Left: Normalized negative log-likelihood of the correct answer on the ARC Challenge benchmark as a function of pre-training FLOPs. Right: ARC Challenge benchmark accuracy as a function of the normalized negative log-likelihood of the correct answer. This analysis enables us to predict model performance on the ARC Challenge benchmark before pre-training commences. See text for details.
setup optimizes for production-grade reliability, which is essential as we scale up training.
Compute. Llama 3405 B is trained on up to 16 K H 100 GPUs, each running at 700 W TDP with 80 GB HBM3, using Meta's Grand Teton AI server platform (Matt Bowman, 2022). Each server is equipped with eight GPUs and two CPUs. Within a server, the eight GPUs are connected via NVLink. Training jobs are scheduled using MAST (Choudhury et al., 2024), Meta's global-scale training scheduler.
Storage. Tectonic (Pan et al., 2021), Meta's general-purpose distributed file system, is used to build a storage fabric (Battey and Gupta, 2024) for Llama 3 pre-training. It offers 240 PB of storage out of 7,500 servers equipped with SSDs, and supports a sustainable throughput of $2 \mathrm{~TB} / \mathrm{s}$ and a peak throughput of $7 \mathrm{~TB} / \mathrm{s}$. A major challenge is supporting the highly bursty checkpoint writes that saturate the storage fabric for short durations. Checkpointing saves each GPU's model state, ranging from 1 MB to 4 GB per GPU, for recovery and debugging. We aim to minimize GPU pause time during checkpointing and increase checkpoint frequency to reduce the amount of lost work after a recovery.</p>
<p>Network. Llama 3405 B used RDMA over Converged Ethernet (RoCE) fabric based on the Arista 7800 and Minipack2 Open Compute Project ${ }^{4}$ OCP rack switches. Smaller models in the Llama 3 family were trained using Nvidia Quantum2 Infiniband fabric. Both RoCE and Infiniband clusters leverage 400 Gbps interconnects between GPUs. Despite the underlying network technology differences between these clusters, we tune both of them to provide equivalent performance for these large training workloads. We elaborate further on our RoCE network since we fully own its design.</p>
<ul>
<li>Network topology. Our RoCE-based AI cluster comprises 24 K GPUs $^{5}$ connected by a three-layer Clos network (Lee et al., 2024). At the bottom layer, each rack hosts 16 GPUs split between two servers and connected by a single Minipack2 top-of-the-rack (ToR) switch. In the middle layer, 192 such racks are connected by Cluster Switches to form a pod of 3,072 GPUs with full bisection bandwidth, ensuring no oversubscription. At the top layer, eight such pods within the same datacenter building are connected via Aggregation Switches to form a cluster of 24 K GPUs. However, network connectivity at the aggregation layer does not maintain full bisection bandwidth and instead has an oversubscription ratio of 1:7. Our model parallelism methods (see Section 3.3.2) and training job scheduler (Choudhury et al., 2024) are all optimized to be aware of network topology, aiming to minimize network communication across pods.</li>
<li>Load balancing. LLM training produces fat network flows that are hard to load balance across all available network paths using traditional methods such as Equal-Cost Multi-Path (ECMP) routing. To address this challenge, we employ two techniques. First, our collective library creates 16 network flows between two GPUs, instead of just one, thereby reducing the traffic per flow and providing more flows</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>GPUs</th>
<th>TP</th>
<th>CP</th>
<th>PP</th>
<th>DP</th>
<th>Seq. Len.</th>
<th>Batch size/DP</th>
<th>Tokens/Batch</th>
<th>TFLOPs/GPU</th>
<th>BF16 MFU</th>
</tr>
</thead>
<tbody>
<tr>
<td>8,192</td>
<td>8</td>
<td>1</td>
<td>16</td>
<td>64</td>
<td>8,192</td>
<td>32</td>
<td>16M</td>
<td>430</td>
<td>$43 \%$</td>
</tr>
<tr>
<td>16,384</td>
<td>8</td>
<td>1</td>
<td>16</td>
<td>128</td>
<td>8,192</td>
<td>16</td>
<td>16M</td>
<td>400</td>
<td>$41 \%$</td>
</tr>
<tr>
<td>16,384</td>
<td>8</td>
<td>16</td>
<td>16</td>
<td>8</td>
<td>131,072</td>
<td>16</td>
<td>16M</td>
<td>380</td>
<td>$38 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4 Scaling configurations and MFU for each stage of Llama 3405B pre-training. See text and Figure 5 for descriptions of each type of parallelism.
for load balancing. Second, our Enhanced-ECMP (E-ECMP) protocol effectively balances these 16 flows across different network paths by hashing on additional fields in the RoCE header of packets.</p>
<ul>
<li>Congestion control. We use deep-buffer switches in the spine (Gangidi et al., 2024) to accommodate transient congestion and buffering caused by collective communication patterns. This setup helps limit the impact of persistent congestion and network back pressure caused by slow servers, which is common in training. Finally, better load balancing through E-ECMP significantly reduces the chance of congestion. With these optimizations, we successfully run a 24 K GPU cluster without traditional congestion control methods such as Data Center Quantized Congestion Notification (DCQCN).</li>
</ul>
<h1>3.3.2 Parallelism for Model Scaling</h1>
<p>To scale training for our largest models, we use 4D parallelism - a combination of four different types of parallelism methods - to shard the model. This approach efficiently distributes computation across many GPUs and ensures each GPU's model parameters, optimizer states, gradients, and activations fit in its HBM. Our implementation of 4D parallelism is illustrated in Figure 5. It combines tensor parallelism (TP; Krizhevsky et al. (2012); Shoeybi et al. (2019); Korthikanti et al. (2023)), pipeline parallelism (PP; Huang et al. (2019); Narayanan et al. (2021); Lamy-Poirier (2023)), context parallelism (CP; Liu et al. (2023a)), and data parallelism (DP; Rajbhandari et al. (2020); Ren et al. (2021); Zhao et al. (2023b)).</p>
<p>Tensor parallelism splits individual weight tensors into multiple chunks on different devices. Pipeline parallelism partitions the model vertically into stages by layers, so that different devices can process in parallel different stages of the full model pipeline. Context parallelism divides the input context into segments, reducing memory bottleneck for very long sequence length inputs. We use fully sharded data parallelism (FSDP; Rajbhandari et al., 2020; Ren et al., 2021; Zhao et al., 2023b), which shards the model, optimizer, and gradients while implementing data parallelism which processes data in parallel on multiple GPUs and synchronizes after each training step. Our use of FSDP for Llama 3 shards optimizer states and gradients, but for model shards we do not reshard after forward computation to avoid an extra all-gather communication during backward passes.</p>
<p>GPU utilization. Through careful tuning of the parallelism configuration, hardware, and software, we achieve an overall BF16 Model FLOPs Utilization (MFU; Chowdhery et al. (2023)) of 38-43\% for the configurations shown in Table 4. The slight drop in MFU to $41 \%$ on 16 K GPUs with $\mathrm{DP}=128$ compared to $43 \%$ on 8 K GPUs with $\mathrm{DP}=64$ is due to the lower batch size per DP group needed to keep the global tokens per batch constant during training.</p>
<p>Pipeline parallelism improvements. We encountered several challenges with existing implementations:</p>
<ul>
<li>Batch size constraint. Current implementations have constraints on supported batch size per GPU, requiring it to be divisible by the number of pipeline stages. For the example in Figure 6, the depth-first schedule (DFS) of pipeline parallelism (Narayanan et al., 2021) requires $N=\mathrm{PP}=4$, while the breadth-first schedule (BFS; Lamy-Poirier (2023)) requires $N=M$, where $M$ is the total number of micro-batches and $N$ is the number of contiguous micro-batches for the same stage's forward or backward. However, pre-training often needs flexibility to adjust batch size.</li>
<li>Memory imbalance. Existing pipeline parallelism implementations lead to imbalanced resource consumption. The first stage consumes more memory due to the embedding and the warm-up micro-batches.</li>
<li>Computation imbalance. After the last layer of the model, we need to calculate output and loss, making this stage the execution latency bottleneck.</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5 Illustration of 4D parallelism. GPUs are divided into parallelism groups in the order of [TP, CP, PP, DP], where DP stands for FSDP. In this example, 16 GPUs are configured with a group size of $|\mathrm{TP}|=2,|\mathrm{CP}|=2,|\mathrm{PP}|=2$, and $|\mathrm{DP}|=2$. A GPU's position in 4 D parallelism is represented as a vector, $\left[D_{1}, D_{2}, D_{3}, D_{4}\right]$, where $D_{i}$ is the index on the $i$-th parallelism dimension. In this example, GPU0[TP0, CP0, PP0, DP0] and GPU1[TP1, CP0, PP0, DP0] are in the same TP group, GPU0 and GPU2 are in the same CP group, GPU0 and GPU4 are in the same PP group, and GPU0 and GPU8 are in the same DP group.</p>
<p>To address these issues, we modify our pipeline schedule as shown in Figure 6, which allows setting $N$ flexibly—in this case $N=5$, which can run a arbitrary number of micro-batches in each batch. This allows us to run: (1) fewer micro-batches than the number of stages when we have batch size limit at large scale; or (2) more micro-batches to hide point-to-point communication, finding a sweet spot between DFS and breadth first schedule (BFS) for the best communication and memory efficiency. To balance the pipeline, we reduce one Transformer layer each from the first and the last stages, respectively. This means that the first model chunk on the first stage has only the embedding, and the last model chunk on the last stage has only output projection and loss calculation. To reduce pipeline bubbles, we use an interleaved schedule (Narayanan et al., 2021) with $V$ pipeline stages on one pipeline rank. Overall pipeline bubble ratio is $\frac{\mathrm{PP}-1}{V+M}$. Further, we adopt asynchronous point-to-point communication in PP, which considerably speeds up training, especially in cases when the document mask introduces extra computation imbalance. We enable TORCH_NCCL_AVOID_RECORD_STREAMS to reduce memory usage from asynchronous point-to-point communication. Finally, to reduce memory cost, based on detailed memory allocation profiling, we proactively deallocate tensors that will not be used for future computation, including the input and output tensors of each pipeline stage, that will not be used for future computation. With these optimizations, we could pre-train Llama 3 on sequences of 8 K tokens without activation checkpointing.</p>
<p>Context parallelism for long sequences. We utilize context parallelism (CP) to improve memory efficiency when scaling the context length of Llama 3 and enable training on extremely long sequences up to 128 K in length. In CP, we partition across the sequence dimension, and specifically we partition the input sequence into $2 \times \mathrm{CP}$ chunks so each CP rank receives two chunks for better load balancing. The $i$-th CP rank received both the $i$-th and the $(2 \times \mathrm{CP}-1-i)$-th chunks.</p>
<p>Different from existing CP implementations that overlap communication and computation in a ring-like structure (Liu et al., 2023a), our CP implementation adopts an all-gather based method where we first all-gather the key (K) and value (V) tensors, and then compute attention output for the local query (Q) tensor chunk. Although the all-gather communication latency is exposed in the critical path, we still adopt this approach for two main reasons: (1) it is easier and more flexible to support different types of attention masks in all-gather based CP attention, such as the document mask; and (2) the exposed all-gather latency</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6 Illustration of pipeline parallelism in Llama 3. Pipeline parallelism partitions eight pipeline stages ( 0 to 7 ) across four pipeline ranks ( PP ranks 0 to 3 ), where the GPUs with rank 0 run stages 0 and 4 , the GPUs with P rank 1 run stages 1 and 5, etc. The colored blocks ( 0 to 9 ) represent a sequence of micro-batches, where $M$ is the total number of micro-batches and $N$ is the number of continuous micro-batches for the same stage's forward or backward. Our key insight is to make $N$ tunable.
is small as the communicated K and V tensors are much smaller than Q tensor due to the use of GQA (Ainslie et al., 2023). Hence, the time complexity of attention computation is an order of magnitude larger than all-gather $\left(O\left(S^{2}\right)\right.$ versus $O(S)$, where $S$ represents the sequence length in the full causal mask), making the all-gather overhead negligible.
Network-aware parallelism configuration. The order of parallelism dimensions, [TP, CP, PP, DP], is optimized for network communication. The innermost parallelism requires the highest network bandwidth and lowest latency, and hence is usually constrained to within the same server. The outermost parallelism may spread across a multi-hop network and should tolerate higher network latency. Therefore, based on the requirements for network bandwidth and latency, we place parallelism dimensions in the order of [TP, CP, PP, DP]. DP (i.e., FSDP) is the outermost parallelism because it can tolerate longer network latency by asynchronously prefetching sharded model weights and reducing gradients. Identifying the optimal parallelism configuration with minimal communication overhead while avoiding GPU memory overflow is challenging. We develop a memory consumption estimator and a performance-projection tool which helped us explore various parallelism configurations and project overall training performance and identify memory gaps effectively.</p>
<p>Numerical stability. By comparing training loss between different parallelism setups, we fixed several numerical issues that impact training stability. To ensure training convergence, we use FP32 gradient accumulation during backward computation over multiple micro-batches and also reduce-scatter gradients in FP32 across data parallel workers in FSDP. For intermediate tensors, e.g., vision encoder outputs, that are used multiple times in the forward computation, the backward gradients are also accumulated in FP32.</p>
<h1>3.3.3 Collective Communication</h1>
<p>Our collective communication library for Llama 3 is based on a fork of Nvidia's NCCL library, called NCCLX. NCCLX significantly improves the performance of NCCL, especially for higher latency networks. Recall that the order of parallelism dimensions is [TP, CP, PP, DP], where DP corresponds to FSDP. The outermost parallelism dimensions, PP and DP, may communicate through a multi-hop network, with latency up to tens of microseconds. The original NCCL collectives-all-gather and reduce-scatter in FSDP, and point-to-point in PP-require data chunking and staged data copy. This approach incurs several inefficiencies, including (1) requiring a large number of small control messages to be exchanged over the network to facilitate data transfer, (2) extra memory-copy operations, and (3) using extra GPU cycles for communication. For Llama 3 training, we address a subset of these inefficiencies by tuning chunking and data transfer to fit our network latencies, which can be as high as tens of microseconds for a large cluster. We also allow small control messages to traverse our network at a higher priority, especially avoiding being head-of-line blocked in deep-buffer core switches. Our ongoing work for future Llama versions involves making deeper changes in NCCLX to holistically address all the aforementioned problems.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Component</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Interruption Count</th>
<th style="text-align: center;">\% of Interruptions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Faulty GPU</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">148</td>
<td style="text-align: center;">$30.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPU HBM3 Memory</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">$17.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Software Bug</td>
<td style="text-align: center;">Dependency</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">$12.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Network Switch/Cable</td>
<td style="text-align: center;">Network</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">$8.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Host Maintenance</td>
<td style="text-align: center;">Unplanned</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$7.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Maintenance</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPU SRAM Memory</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">$4.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPU System Processor</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$4.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NIC</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$1.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">NCCL Watchdog Timeouts</td>
<td style="text-align: center;">Unknown</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$1.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Silent Data Corruption</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$1.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPU Thermal Interface + Sensor</td>
<td style="text-align: center;">GPU</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$1.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">SSD</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$0.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Power Supply</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$0.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Server Chassis</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">IO Expansion Board</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Dependency</td>
<td style="text-align: center;">Dependency</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">CPU</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">System Memory</td>
<td style="text-align: center;">Host</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$0.5 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5 Root-cause categorization of unexpected interruptions during a 54-day period of Llama 3405B pre-training. About $78 \%$ of unexpected interruptions were attributed to confirmed or suspected hardware issues.</p>
<h1>3.3.4 Reliability and Operational Challenges</h1>
<p>The complexity and potential failure scenarios of 16 K GPU training surpass those of much larger CPU clusters that we have operated. Moreover, the synchronous nature of training makes it less fault-tolerant - a single GPU failure may require a restart of the entire job. Despite these challenges, for Llama 3, we achieved higher than $90 \%$ effective training time while supporting automated cluster maintenance, such as firmware and Linux kernel upgrades (Vigraham and Leonhardi, 2024), which resulted in at least one training interruption daily. The effective training time measures the time spent on useful training over the elapsed time.</p>
<p>During a 54-day snapshot period of pre-training, we experienced a total of 466 job interruptions. Of these, 47 were planned interruptions due to automated maintenance operations such as firmware upgrades or operatorinitiated operations like configuration or dataset updates. The remaining 419 were unexpected interruptions, which are classified in Table 5. Approximately $78 \%$ of the unexpected interruptions are attributed to confirmed hardware issues, such as GPU or host component failures, or suspected hardware-related issues like silent data corruption and unplanned individual host maintenance events. GPU issues are the largest category, accounting for $58.7 \%$ of all unexpected issues. Despite the large number of failures, significant manual intervention was required only three times during this period, with the rest of issues handled by automation.</p>
<p>To increase the effective training time, we reduced job startup and checkpointing time, and developed tools for fast diagnosis and problem resolution. We extensively use PyTorch's built-in NCCL flight recorder (Ansel et al., 2024), a feature that captures collective metadata and stack traces into a ring buffer, and hence allowing us to diagnose hangs and performance issues quickly at scale, particularly with regard to NCCLX. Using this, we efficiently record every communication event and the duration of each collective operation, and also automatically dump tracing data on NCCLX watchdog or heartbeat timeout. We enable more computationally intensive tracing operations and metadata collection selectively as needed live in production through online configuration changes (Tang et al., 2015) without needing a code release or job restart.</p>
<p>Debugging issues in large-scale training is complicated by the mixed use of NVLink and RoCE in our network. Data transfer over NVLink typically occurs through load/store operations issued by CUDA kernels, and failures in either the remote GPU or NVLink connectivity often manifest as stalled load/store operations within CUDA kernels without returning a clear error code. NCCLX enhances the speed and accuracy of failure</p>
<p>detection and localization through a tight co-design with PyTorch, allowing PyTorch to access NCCLX's internal state and track relevant information. While stalls due to NVLink failures cannot be completely prevented, our system monitors the state of the communication library and automatically times out when such a stall is detected. Additionally, NCCLX traces the kernel and network activities of each NCCLX communication and provides a snapshot of the failing NCCLX collective's internal state, including finished and pending data transfers between all ranks. We analyze this data to debug NCCLX scaling issues.</p>
<p>Sometimes, hardware issues may cause still-functioning but slow stragglers that are hard to detect. Even a single straggler can slow down thousands of other GPUs, often appearing as functioning but slow communications. We developed tools to prioritize potentially problematic communications from selected process groups. By investigating just a few top suspects, we were usually able to effectively identify the stragglers.</p>
<p>One interesting observation is the impact of environmental factors on training performance at scale. For Llama 3405 B , we noted a diurnal $1-2 \%$ throughput variation based on time-of-day. This fluctuation is the result of higher mid-day temperatures impacting GPU dynamic voltage and frequency scaling.</p>
<p>During training, tens of thousands of GPUs may increase or decrease power consumption at the same time, for example, due to all GPUs waiting for checkpointing or collective communications to finish, or the startup or shutdown of the entire training job. When this happens, it can result in instant fluctuations of power consumption across the data center on the order of tens of megawatts, stretching the limits of the power grid. This is an ongoing challenge for us as we scale training for future, even larger Llama models.</p>
<h1>3.4 Training Recipe</h1>
<p>The recipe used to pre-train Llama 3405 B consists of three main stages: (1) initial pre-training, (2) long-context pre-training, and (3) annealing. The three stages are described separately below. We use similar recipes to pre-train the 8 B and 70 B models.</p>
<h3>3.4.1 Initial Pre-Training</h3>
<p>We pre-train Llama 3405 B using AdamW with a peak learning rate of $8 \times 10^{-5}$, a linear warm up of 8,000 steps, and a cosine learning rate schedule decaying to $8 \times 10^{-7}$ over $1,200,000$ steps. We use a lower batch size early in training to improve training stability, and increase it subsequently to improve efficiency. Specifically, we use an initial batch size of 4 M tokens and sequences of length 4,096 , and double these values to a batch size of 8 M sequences of 8,192 tokens after pre-training 252 M tokens. We double the batch size again to 16 M after pre-training on 2.87 T tokens. We found this training recipe to be very stable: we observed few loss spikes and did not require interventions to correct for model training divergence.</p>
<p>Adjusting the data mix. We made a several adjustments to the pre-training data mix during training to improve model performance on particular downstream tasks. In particular, we increased the percentage of non-English data during pre-training to improve the multilingual performance of Llama 3. We also upsample mathematical data to improve the model's mathematical reasoning performance, we added more recent web data in the later stages of pre-training to advance the model's knowledge cut-off, and we downsampled subsets of the pre-training data that were later identified as being lower quality.</p>
<h3>3.4.2 Long Context Pre-Training</h3>
<p>In the final stages of pre-training, we train on long sequences to support context windows of up to 128 K tokens. We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length. We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. We assess successful adaptation by measuring whether (1) model performance on short-context evaluations has recovered completely and (2) the model perfectly solves "needle in a haystack" tasks up to that length. In Llama 3405 B pre-training, we increased context length gradually in six stages, starting from the original 8 K context window and ending in the final 128 K context window. This long-context pre-training stage was performed using approximately 800 B training tokens.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7 Illustration of the overall post-training approach for Llama 3. Our post-training strategy involves rejection sampling, supervised finetuning, and direct preference optimization. See text for details.</p>
<h1>3.4.3 Annealing</h1>
<p>During pre-training on the final 40 M tokens, we linearly annealed the learning rate to 0 , maintaining a context length of 128 K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section 3.1.3. Finally, we compute the average of model checkpoints (Polyak (1991) averaging) during annealing to produce the final pre-trained model.</p>
<h2>4 Post-Training</h2>
<p>We produce the aligned Llama 3 models by applying several rounds of post-training, ${ }^{6}$ or aligning the model with human feedback (Ouyang et al., 2022; Rafailov et al., 2024) on top of a pre-trained checkpoint. Each round of post-training involves supervised finetuning (SFT) followed by Direct Preference Optimization (DPO; Rafailov et al., 2024) on examples collected either via human annotations or generated synthetically. Our post-training modeling and data approaches are described in Sections 4.1 and 4.2 respectively. We further detail custom data curation strategies to improve the reasoning, coding, factuality, multilingual, tool use, long context, and precise instruction following in Section 4.3.</p>
<h3>4.1 Modeling</h3>
<p>The backbone of our post-training strategy is a reward model and a language model. We first train a reward model on top of the pre-trained checkpoint using human-annotated preference data (see Section 4.1.2). We then finetune pre-trained checkpoints with supervised finetuning (SFT; see Section 4.1.3), and further align the checkpoints with Direct Preference Optimization (DPO; see Section 4.1.4). This process is illustrated in Figure 7. Unless otherwise noted, our modeling procedure applies to Llama 3405 B , and we refer to Llama 3405 B as Llama 3 for simplicity.</p>
<h3>4.1.1 Chat Dialog Format</h3>
<p>To tune LLMs for human-AI interaction, we need to define a chat dialog protocol for the model to understand human instructions and perform conversational tasks. Compared to its predecessor, Llama 3 has new capabilities such as tool use (Section 4.3.5) which may require generating multiple messages and sending</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>them to different locations (e.g., user, ipython) within a single dialog turn. To support this, we design a new multi-message chat protocol which uses various special header and termination tokens. The header tokens are used to indicate the source and destination of each message in a conversation. Similarly, the termination tokens indicate when it is the time to alternate between human and AI to speak.</p>
<h1>4.1.2 Reward Modeling</h1>
<p>We train a reward model (RM) covering different capabilities on top of the pre-trained checkpoint. The training objective is the same as Llama 2 except that we remove the margin term in the loss, as we observe diminishing improvements after data scaling. Following Llama 2, we use all of our preference data for reward modeling after filtering out samples with similar responses. In addition to standard preference pair of (chosen, rejected) response, annotations also create a third "edited response" for some prompts, where the chosen response from the pair is further edited for improvement (see Section 4.2.1). Hence, each preference ranking sample has two or three responses with clear ranking (edited $&gt;$ chosen $&gt;$ rejected). We concatenate the prompt and multiple responses into a single row during training with responses randomly shuffled. This is an approximation to the standard scenario of putting the responses in separate rows and computing the scores, but in our ablations, this approach improves training efficiency without a loss in accuracy.</p>
<h3>4.1.3 Supervised Finetuning</h3>
<p>The reward model is then used to perform rejection sampling on our human annotation prompts, the details of which are described in Section 4.2. Together with this rejection-sampled data and other data sources (including synthetic data), we finetune the pre-trained language model using a standard cross entropy loss on the target tokens (while masking loss on prompt tokens). More details about the data mix can be found in Section 4.2. We refer to this stage as supervised finetuning (SFT; Wei et al., 2022a; Sanh et al., 2022; Wang et al., 2022b), even though many of the training targets are model-generated. Our largest models are finetuned with a learning rate of $10^{-5}$ over the course of 8.5 K to 9 K steps. We found these hyperparameter settings to work well across different rounds and data mixes.</p>
<h3>4.1.4 Direct Preference Optimization</h3>
<p>We further train our SFT models with Direct Preference Optimization (DPO; Rafailov et al., 2024) for human preference alignment. For training, we primarily use the most recent batches of preference data collected using the best performing models from the previous alignment rounds. As a result, our training data conforms better to the distribution of the policy model that is being optimized in each round. We also explored on-policy algorithms such as PPO (Schulman et al., 2017), but found that DPO required less compute for large-scale models and performed better, especially on instruction following benchmarks like IFEval (Zhou et al., 2023). For Llama 3, we use a learning rate of $10^{-5}$ and set the $\beta$ hyper-parameter to be 0.1 . In addition, we apply the following algorithmic modifications to DPO:</p>
<ul>
<li>Masking out formatting tokens in DPO loss: We mask out special formatting tokens including header and termination tokens (described in Section 4.1.1) from both chosen and rejected responses in the loss to stabilize DPO training. We observe that having these tokens contribute to the loss may lead to undesired model behaviors such as tail repetition or abruptly generating termination tokens. We hypothesize that this is due to the contrastive nature of the DPO loss - the presence of common tokens in both chosen and rejected responses leads to a conflicting learning objective as the model needs to increase and reduce the likelihood of these tokens simultaneously.</li>
<li>Regularization with NLL loss: We add an additional negative log-likelihood (NLL) loss term with a scaling coefficient of 0.2 on the chosen sequences, similar to Pang et al. (2024). This helps further stabilize DPO training by maintaining desired formatting for generation and preventing the decrease of log probability of chosen responses (Pang et al., 2024; Pal et al., 2024).</li>
</ul>
<h3>4.1.5 Model Averaging</h3>
<p>Finally, we average models obtained from experiments using various versions of data or hyperparameters at each RM, SFT, or DPO stage (Izmailov et al., 2019; Wortsman et al., 2022; Li et al., 2022).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">\% of <br> comparisons</th>
<th style="text-align: center;">Avg. # turns <br> per dialog</th>
<th style="text-align: center;">Avg. # tokens <br> per example</th>
<th style="text-align: center;">Avg. # tokens <br> in prompt</th>
<th style="text-align: center;">Avg. # tokens <br> in response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">General English</td>
<td style="text-align: center;">$81.99 \%$</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">$1,000.4$</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">271.2</td>
</tr>
<tr>
<td style="text-align: left;">Coding</td>
<td style="text-align: center;">$6.93 \%$</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$1,621.0$</td>
<td style="text-align: center;">113.8</td>
<td style="text-align: center;">462.9</td>
</tr>
<tr>
<td style="text-align: left;">Multilingual</td>
<td style="text-align: center;">$5.19 \%$</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">$1,299.4$</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">420.9</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning and tools</td>
<td style="text-align: center;">$5.89 \%$</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">707.7</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">129.9</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">$1,041.6$</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">284.0</td>
</tr>
</tbody>
</table>
<p>Table 6 Statistics of human preference data. We list statistics of the internally collected human preference data used for Llama 3 alignment. We ask annotators to perform multi-turn dialogues with the models and make comparisons among responses at each turn. In post-processing, we split each dialogue to multiple examples at a turn level. Each example consists of a prompt (including previous dialog if available) and a response (e.g., chosen or rejected response).</p>
<h1>4.1.6 Iterative Rounds</h1>
<p>Following Llama 2, we apply the above methods in six rounds. In each cycle, we collect new preference annotations and SFT data, sampling synthetic data from the latest models.</p>
<h3>4.2 Post-training Data</h3>
<p>The post-training data composition plays a critical role in the usefulness and behavior of language models. In this section, we discuss our human annotation procedures and preference data collection (Section 4.2.1), the composition of our SFT data (Section 4.2.2), and methods for data quality control and cleaning (Section 4.2.3).</p>
<h3>4.2.1 Preference Data</h3>
<p>Our preference data annotation process is similar to Llama 2. We deploy multiple models for annotation after each round and sample two responses from two different models for each user prompt. These models can be trained with different data mixes and alignment recipes, allowing for different capability strength (e.g., code expertise) and increased data diversity. We ask annotators to rate the strength of their preference by categorizing it into one of four levels, based on how much more they prefer the chosen response over the rejected one: significantly better, better, slightly better, or marginally better. We also incorporate an editing step after preference ranking to encourage annotators to further improve the preferred response. Annotators edit the chosen response directly or prompt the model with feedback to refine its own response. Consequently, a portion of our preference data has three responses ranked (edited $&gt;$ chosen $&gt;$ rejected).</p>
<p>In Table 6, we report the statistics of preference annotations that we use for Llama 3 training. General English covers multiple subcategories such as knowledge-based question and answering or precise instruction-following, which fall outside the scope of specific capabilities. Compared to Llama 2, we observe an increase in the average length of prompt and response, suggesting that we train Llama 3 on more complex tasks. In addition, we implement a quality analysis and human evaluation process to rigorously assess the data collected, allowing us to refine our prompts and provide systematic, actionable feedback to annotators. For example, as Llama 3 improves after each round, we increase prompt complexity accordingly to target areas where the model lags.</p>
<p>In each round of post-training, we use all the preference data that is available at the time for reward modeling, while only using the latest batches from various capabilities for DPO training. For both reward modeling and DPO, we use samples that are labeled as the chosen response being significantly better or better than the rejected counterpart for training and discard samples with similar responses.</p>
<h3>4.2.2 SFT Data</h3>
<p>Our finetuning data is largely comprised of the following sources:</p>
<ul>
<li>Prompts from our human annotation collection with rejection-sampled responses.</li>
<li>Synthetic data targeting specific capabilities (see Section 4.3 for more details).</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: right;">$\%$ of examples</th>
<th style="text-align: right;">Avg. # turns</th>
<th style="text-align: right;">Avg. # tokens</th>
<th style="text-align: right;">Avg. # tokens <br> in context</th>
<th style="text-align: right;">Avg. # tokens <br> in final response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">General English</td>
<td style="text-align: right;">$52.66 \%$</td>
<td style="text-align: right;">6.3</td>
<td style="text-align: right;">974.0</td>
<td style="text-align: right;">656.7</td>
<td style="text-align: right;">317.1</td>
</tr>
<tr>
<td style="text-align: left;">Code</td>
<td style="text-align: right;">$14.89 \%$</td>
<td style="text-align: right;">2.7</td>
<td style="text-align: right;">753.3</td>
<td style="text-align: right;">378.8</td>
<td style="text-align: right;">374.5</td>
</tr>
<tr>
<td style="text-align: left;">Multilingual</td>
<td style="text-align: right;">$3.01 \%$</td>
<td style="text-align: right;">2.7</td>
<td style="text-align: right;">520.5</td>
<td style="text-align: right;">230.8</td>
<td style="text-align: right;">289.7</td>
</tr>
<tr>
<td style="text-align: left;">Exam-like</td>
<td style="text-align: right;">$8.14 \%$</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">297.8</td>
<td style="text-align: right;">124.4</td>
<td style="text-align: right;">173.4</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning and tools</td>
<td style="text-align: right;">$21.19 \%$</td>
<td style="text-align: right;">3.1</td>
<td style="text-align: right;">661.6</td>
<td style="text-align: right;">359.8</td>
<td style="text-align: right;">301.9</td>
</tr>
<tr>
<td style="text-align: left;">Long context</td>
<td style="text-align: right;">$0.11 \%$</td>
<td style="text-align: right;">6.7</td>
<td style="text-align: right;">$38,135.6$</td>
<td style="text-align: right;">$37,395.2$</td>
<td style="text-align: right;">740.5</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">$100 \%$</td>
<td style="text-align: right;">4.7</td>
<td style="text-align: right;">846.1</td>
<td style="text-align: right;">535.7</td>
<td style="text-align: right;">310.4</td>
</tr>
</tbody>
</table>
<p>Table 7 Statistics of SFT data. We list internally collected SFT data used for Llama 3 alignment. Each SFT example consists of a context (i.e., all conversation turns except the last one) and a final response.</p>
<ul>
<li>Small amounts of human-curated data (see Section 4.3 for more details).</li>
</ul>
<p>As our post-training rounds progress, we develop stronger Llama 3 variants that we use to collect larger datasets that cover a wide range of complex capabilities. In this section, we discuss the details for the rejection-sampling procedure and overall composition of our final SFT datamix.</p>
<p>Rejection sampling. During rejection sampling (RS), for each prompt collected during human annotation (Section 4.2.1) we sample $K$ (typically between 10 and 30) outputs from the latest chat model policy (usually the best performing checkpoint from the previous post-training iteration, or the best performing checkpoint for a particular capability) and use our reward model to select the best candidate, consistent with Bai et al. (2022). In later rounds of post-training, we introduce system prompts to steer RS responses to conform with desirable tone, style, or formatting, which might be different for different capabilities.</p>
<p>To increase the efficiency of rejection sampling, we adopt PagedAttention (Kwon et al., 2023). PagedAttention enhances memory efficiency through dynamic key-value cache allocation. It supports arbitrary output lengths by dynamically scheduling requests based on the current cache capacity. Unfortunately, this carries the risk of swap-out when running out of memory. To eliminate such swap overhead, we define a maximum output length and perform a request only if sufficient memory is available to fit an output with that length. PagedAttention also enables us to share the key-value cache pages for a prompt across all corresponding outputs. Together, this leads to a throughput improvement of over $2 \times$ during rejection sampling.</p>
<p>Overall data composition. Table 7 shows data statistics for each broad category of our "helpfulness" mix. While SFT and preference data contain overlapping domains, they are curated differently, yielding distinct count statistics. In Section 4.2.3 we describe techniques for categorizing topic, complexity, and quality of our data samples. In each round of post-training, we adjust our overall data mix carefully across these axes to tune performance across a wide range of benchmarks. Our final data mix epochs multiple times on some high quality sources and downsamples others.</p>
<h1>4.2.3 Data Processing and Quality Control</h1>
<p>Given that most of our training data is model-generated, it requires careful cleaning and quality control.
Data cleaning. In the early rounds, we observed a number of undesirable patterns common in our data, such as excessive use of emojis or exclamation points. Therefore, we implement a series of rule-based data removal and modification strategies to filter or clean problematic data. For example, to mitigate overly-apologetic tonal issues, we identify overused phrases (such as "I'm sorry" or "I apologize") and carefully balance the proportion of such samples in our dataset.</p>
<p>Data pruning. We also apply a collection of model-based techniques to remove low-quality training samples and improve overall model performance:</p>
<ul>
<li>Topic classification: We first finetune Llama 38 B into a topic classifier, and perform inference over all data to classify it into both coarsely-grained buckets ("mathematical reasoning") and fine-grained</li>
</ul>
<p>buckets ("geometry and trigonometry").</p>
<ul>
<li>Quality scoring: We use both reward model and Llama-based signals to obtain a quality score for each sample. For an RM-based score, we consider data that is in the top quartile of RM scores as high quality. For a Llama-based score, we prompt Llama 3 checkpoint to rate each sample on a three-point scale for general English data (accuracy, instruction following, and tone/presentation) and a two-point scale for coding data (bug identification and user intention), and consider samples that obtain the maximum score as high quality. The RM and Llama-based scores have high disagreement rates, and we find that combining these signals yield the best recall on our internal test set. Ultimately, we select examples that are marked as high quality by the RM or the Llama-based filter.</li>
<li>Difficulty scoring: Because we are also interested in prioritizing examples that are more complex for the model, we score data using two measures of difficulty: Instag (Lu et al., 2023) and Llama-based scoring. For Instag, we prompt Llama 370 B to perform intention tagging of SFT prompts, where more intentions implies more complexity. We also prompt Llama 3 to measure the difficulty (Liu et al., 2024c) of dialogs on a three-point scale.</li>
<li>Semantic deduplication: Finally, we perform semantic deduplication (Abbas et al., 2023; Liu et al., 2024c). We first cluster complete dialogs using RoBERTa (Liu et al., 2019b) and within each cluster sort them by quality score $\times$ difficulty score. We then do greedy selection by iterating through all sorted examples, and only keeping the ones that have maximum cosine similarity less than a threshold to the examples seen so far in the cluster.</li>
</ul>
<h1>4.3 Capabilities</h1>
<p>We highlight special efforts to improve performance for specific capabilities such as code (Section 4.3.1), multilinguality (Section 4.3.2), math and reasoning (Section 4.3.3), long context (Section 4.3.4), tool use (Section 4.3.5), factuality (Section 4.3.6), and steerability (Section 4.3.7).</p>
<h3>4.3.1 Code</h3>
<p>LLMs for code have received significant attention since the release of Copilot and Codex (Chen et al., 2021). Developers are now widely using these models to generate code snippets, debug, automate tasks, and improve code quality. For Llama 3, we target improving and evaluating code generation, documentation, debugging, and review capabilities for the following high priority programming languages: Python, Java, Javascript, C/C++, Typescript, Rust, PHP, HTML/CSS, SQL, bash/shell. Here, we present our work on improving these coding capabilities via training a code expert, generating synthetic data for SFT, improving formatting with system prompt steering, and creating quality filters to remove bad samples from our training data.</p>
<p>Expert training. We train a code expert which we use to collect high quality human annotations for code throughout subsequent rounds of post-training. This is accomplished by branching the main pre-training run and continuing pre-training on a 1 T token mix of mostly ( $&gt;85 \%$ ) code data. Continued pre-training on domainspecific data has been shown to be effective for improving performance in a specific domain (Gururangan et al., 2020). We follow a recipe similar to that of CodeLlama (Rozière et al., 2023). For the last several thousand steps of training we perform long-context finetuning (LCFT) to extend the expert's context length to 16 K tokens on a high quality mix of repo-level code data. Finally, we follow the similar post-training modeling recipes described in Section 4.1 to align this model, except with SFT and DPO data mixes primarily targeting code. This model is also used for rejection sampling (Section 4.2.2) for coding prompts.</p>
<p>Synthetic data generation. During development, we identified key issues in code generation, including difficulty in following instructions, code syntax errors, incorrect code generation, and difficulty in fixing bugs. While intensive human annotation could theoretically resolve these issues, synthetic data generation offers a complementary approach at a lower cost and higher scale, unconstrained by the expertise level of annotators. As such, we use Llama 3 and the code expert to generate a large quantity of synthetic SFT dialogs.</p>
<p>We describe three high-level approaches for generating synthetic code data. In total, we generate over 2.7 M synthetic examples which were used during SFT.</p>
<ol>
<li>
<p>Synthetic data generation: execution feedback. The 8B and 70B models show significant performance improvements when trained on data generated by a larger, more competent model. However, our initial experiments revealed that training Llama 3405 B on its own generated data is not helpful (and can even degrade performance). To address this limitation, we introduced execution feedback as a source of truth, enabling the model to learn from its mistakes and stay on track. In particular, we generate large dataset of approximately one million synthetic coding dialogues using the following process:</p>
</li>
<li>
<p>Problem description generation: First, we generate a large collection of programming problem descriptions that span a diverse range of topics, including those in the long tail distribution. To achieve this diversity, we sample random code snippets from various sources and prompt the model to generate programming problems inspired by these examples. This allowed us to tap into a wide range of topics and create a comprehensive set of problem descriptions (Wei et al., 2024).</p>
</li>
<li>Solution generation: Then, we prompt Llama 3 to solve each problem in a given programming language. We observe that adding general rules of good programming to the prompt improves the generated solution quality. Also, we find it is helpful to require the model to explain its thought process in comments.</li>
<li>Correctness analysis: After generating a solution, it is crucial to recognize that its correctness is not guaranteed, and including incorrect solutions in the finetuning dataset could harm the model's quality. While we do not ensure complete correctness, we develop methods to approximate it. To achieve this, we extract the source code from the generated solution and applied a combination of static and dynamic analysis techniques to test its correctness, including:</li>
<li>Static analysis: We run all generated code through a parser and a linter to ensure syntactic correctness, catching errors such as syntax errors, use of uninitialized variables or non-imported functions, code style issues, typing errors, and others.</li>
<li>Unit test generation and execution: For each problem and solution, we prompt the model to generate unit tests, executed in a containerized environment together with the solution, catching run-time execution errors and some semantic errors.</li>
<li>Error feedback and iterative self-correction: When a solution fails at any step, we prompt the model to revise it. The prompt included the original problem description, the faulty solution, and feedback from the parser/linter/tester (stdout, stderr/ and return code). After a unit test execution failure, the model could either fix the code to pass the existing tests or modify its unit tests to accommodate the generated code. Only dialogs that pass all checks are included in the final dataset, used for supervised finetuning (SFT). Notably, we observed that about $20 \%$ of solutions were initially incorrect but self-corrected, indicating that the model learned from the execution feedback and improved its performance.</li>
<li>
<p>Fine-tuning and iterative improvement: The finetuning process is conducted over multiple rounds, with each round building on the previous one. After each round, the model is improved, generating higher-quality synthetic data for the next round. This iterative process allows for progressive refinement and enhancement of the model's performance.</p>
</li>
<li>
<p>Synthetic data generation: programming language translation. We observe a performance gap between major programming languages (e.g., Python/C++) and less common ones (e.g., Typescript/PHP). This is not surprising as we have less training data for less common programming languages. To mitigate this, we supplement our existing data by translating data from common programming languages to less common languages (similar to Chen et al. (2023) in the context of reasoning). This is achieved by prompting Llama 3 and ensuring quality via syntax parsing, compilation, and execution. Figure 8 demonstrates an example of synthetic PHP code translated from Python. This improves performance significantly for less common languages as measured by the MultiPL-E (Cassano et al., 2023) benchmark.</p>
</li>
<li>Synthetic data generation: backtranslation. To improve certain coding capabilities (e.g., documentation, explanations) where execution feedback is less informative for determining quality, we employ an alternative multi-step approach. Using this procedure, we generated approximately 1.2 M synthetic</li>
</ol>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8 Code translation example. We display an example of using Llama 3 to translate Python code (left) to PHP code (right) to augment our SFT dataset with a wider range of programming languages.</p>
<div class="codehilite"><pre><span></span><code><span class="k">public</span><span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="nc">int</span><span class="w"> </span><span class="n">ClimbStairs</span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>
<span class="err">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span>
<span class="w">    </span><span class="err">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span>
<span class="w">    </span><span class="err">{</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">    </span><span class="nc">int</span><span class="err">[]</span><span class="w"> </span><span class="n">dp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="nc">int</span><span class="o">[</span><span class="n">n + 1</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="n">dp</span><span class="o">[</span><span class="n">1</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="n">dp</span><span class="o">[</span><span class="n">2</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nc">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="w">    </span><span class="err">{</span>
<span class="w">        </span><span class="n">dp</span><span class="o">[</span><span class="n">i</span><span class="o">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dp</span><span class="o">[</span><span class="n">i - 1</span><span class="o">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">dp</span><span class="o">[</span><span class="n">i - 2</span><span class="o">]</span><span class="p">;</span>
<span class="w">    </span><span class="err">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">dp</span><span class="o">[</span><span class="n">n</span><span class="o">]</span><span class="p">;</span>
<span class="err">}</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="n">public</span><span class="w"> </span><span class="k">static</span><span class="w"> </span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">ClimbStairs</span><span class="p">(</span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">Base</span><span class="w"> </span><span class="n">cases</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="o">//</span><span class="w"> </span><span class="n">Initialise</span><span class="w"> </span><span class="n">variables</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">store</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span>
<span class="w">    </span><span class="n">of</span><span class="w"> </span><span class="n">ways</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">climb</span>
<span class="w">        </span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">prev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">        </span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">curr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span>
<span class="w">        </span><span class="o">//</span><span class="w"> </span><span class="n">Calculate</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">ways</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">climb</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">n</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">2</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;=</span><span class="w"> </span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nb nb-Type">int</span><span class="w"> </span><span class="n">temp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">curr</span><span class="p">;</span>
<span class="w">        </span><span class="n">curr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prev</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">curr</span><span class="p">;</span>
<span class="w">        </span><span class="n">prev</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">temp</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">curr</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>Figure 9 Improving generated code quality with system prompts. Left: without system prompt Right: with system prompt.
dialogs related to code explanation, generation, documentation, and debugging. Beginning with code snippets from a variety of languages in our pre-training data:</p>
<ul>
<li>Generate: We prompt Llama 3 to generate data that represents our target capability (e.g., we add comments and docstrings for the code snippet, or we ask the model to explain a piece of code).</li>
<li>Backtranslate: We then prompt the model to "backtranslate" the synthetically generated data to the original code (e.g., we prompt the model to generate code only from its documentation, or we ask the model to generate code only from its explanation).</li>
<li>Filter: Using the original code as a reference, we prompt the Llama 3 to determine the quality of the output (e.g., we ask the model how faithful the backtranslated code is to the original). We then use the generated examples that have the highest self-verification scores in SFT.</li>
</ul>
<p>System prompt steering during rejection sampling. During the rejection sampling process, we used code specific system prompts to improve code readability, documentation, thoroughness, and specificity. Recall, from Section 7 this data is used to finetune the language model. Figure 9 shows an example of how the system prompt helps improve the generated code quality - it adds necessary comments, uses more informative variable names, saves memory, etc.</p>
<p>Filtering training data with execution and model-as-judge signals. As described in Section 4.2.3, we occasionally encounter quality issues in our rejection-sampled data, such as code blocks containing bugs. Detecting these issues in our rejection-sampled data is not as straightforward as it is for our synthetic code data, as the rejection-sampled responses typically contain a mix of natural language and code for which the code may not</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We use the term "post-training" to refer to any model training that happens outside of pre-training.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
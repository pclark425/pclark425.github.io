<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9272 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9272</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9272</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-270199843</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.20701v1.pdf" target="_blank">Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks. Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions. In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans. By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different. Following this property, we propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE). COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence. Experiments show that even widely-used human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9272.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9272.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>neighborhood_lexical_variation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neighborhood Lexical Variations in Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small, semantically-preserving single-word substitutions in the task description portion of prompts can produce large and inconsistent changes in LLM task performance; authors generate 'neighborhood' prompts with a masked LM and evaluate their performance on validation pools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (STEM subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice subject-matter questions (MMLU STEM subset); models are expected to output answer choice labels (A/B/C/D).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot style task description prompt: the task description string 'The following are multiple choice questions (with answers) about {task}.' and single-word lexical variants produced by masking each token and using a pre-trained MLM to fill-in.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Alternative single-word variants of the same sentence produced by MLM fill-ins (examples: 'The following lists multiple choice questions (with answers) about {task}.'; 'The following are some choice questions (with answers) about {task}.')</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: original prompt 28.04%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>accuracy: variant 'lists' 23.92%; variant 'some choice questions' 30.86%</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.82% (30.86 vs 28.04) for one variant; -4.12% (23.92 vs 28.04) for another (example differences reported on MMLU-STEM with Llama-2-7B-chat).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that LLMs are 'lexically sensitive': surface lexical choices alter internal model attention/processing despite negligible semantic differences to humans; nearby prompts in embedding space can still map to different model behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Neighborhood prompts created by masking each token in the task description and taking the MLM top-10 fill-ins; sentence representations obtained from the target model and visualized with t-SNE; evaluation on MMLU-STEM validation examples; reported accuracies per prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9272.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>latent_proximity_no_perf_correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latent Representation Proximity vs Task Performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts that are nearby in the model's sentence-embedding/latent space can nonetheless produce widely different downstream task performance; embedding proximity is not predictive of prompt effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoLA (Corpus of Linguistic Acceptability)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary acceptability judgment for single sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Set of neighborhood prompts differing by one word around the original task description 'Does this sentence make sense?'; sentence embeddings projected to 2D using t-SNE to examine representation clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompts that are close in embedding space vs distant prompts; performance compared across neighborhood prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: best neighborhood prompt 51.2%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>accuracy: a nearby (embedding-proximal) prompt nearly worst at 32.4%; difference ~18.8 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>≈18.8% absolute difference between nearby prompts (CoLA with Llama-2-7B-chat example).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors observe that sentence-level latent proximity does not reliably predict downstream performance; they note that models encode lexical differences that affect output behavior even when semantic embeddings cluster together.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Neighborhood prompts generated via MLM fill-ins; sentence representations extracted from the target LLM and projected with t-SNE; performance measured on CoLA validation set.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9272.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPLE_recover_QQP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>COPLE Lexical Optimization Recovering Performance on QQP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>COPLE, a black-box combinatorial optimization over semantically similar word substitutions guided by proxy-task feedback, can substantially recover or improve LLM accuracy on downstream tasks (example: QQP paraphrase detection).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QQP (Quora Question Pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary paraphrase detection: decide whether two questions have the same meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Human-crafted prompt 'Please identify whether the sentences have the same meaning' (and 3-shot variants); COPLE performs iterative single-word substitutions (semantically similar candidates) guided by performance on proxy tasks to produce an optimized prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original human-crafted prompt and degraded prompt variants vs COPLE-optimized prompt (example optimized prompt: 'Please identify since the sentences repeat the same theme').</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: initial example reported ~35% with the original prompt (Figure 1 example).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>accuracy after COPLE optimization increased to 57% (Figure 1 example); another example: 3-shot accuracy originally 27.58% decreased to 23.03% with a variant, then COPLE increased the degraded 23.03% to 57.61% (Llama-2-7B-chat on QQP).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+22 percentage points (35% -> 57%) in Figure 1 example; +34.58 percentage points (23.03% -> 57.61%) for the 3-shot degraded -> COPLE recovery example.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>COPLE finds lexically different prompts that better align with the model's idiosyncratic lexical sensitivities; optimizing lexical choices can recover both instruct-following and problem-solving ability without gradient access or human re-engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>COPLE uses a pre-trained MLM to propose top-k candidate substitutions per token, ranks tokens by word-influence (expected loss change when removed), uses a small batch of proxy reference tasks (sampled from training set) to evaluate candidates, and iteratively substitutes the most influential tokens; reported example uses the QQP validation/test setups.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9272.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPLE_word_count_ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Number of Words Changed in COPLE Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Changing only a small number of the most influential words in the task description is sufficient to materially improve performance; increasing the number of changed words tends to further increase accuracy but with diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoLA and MMLU-Other (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CoLA: sentence acceptability; MMLU-Other: MMLU subset 'Other' domain multiple-choice questions.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Original human-crafted prompt with varying counts of top-influence words substituted by COPLE (e.g., change 1 word, 2 words, ...).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different numbers of substituted words (e.g., 1-word change vs more words).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: CoLA original 33.27% (reported baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>accuracy after changing only the single most influential word: CoLA improved to 46.50% ±1.23%; MMLU-Other improved from 38.59% to 44.04% ±1.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CoLA +13.23 percentage points for single-word change (33.27 -> 46.50); MMLU-Other +5.45 points (38.59 -> 44.04).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>A small number of high-influence lexical tokens strongly mediate prompt utility; targeting these tokens is an efficient way to recover performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors compute token influence I(d_i) as expected change in proxy-task loss when token removed; then substitute tokens in descending influence order; reported mean and std across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9272.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPLE_proxy_batch_size</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Proxy Reference Task Batch Size on COPLE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The size of the proxy reference task batch used to evaluate candidate prompt substitutions affects COPLE's effectiveness and stability; even small proxy batches (20 examples) can produce substantial improvements, while larger batches improve accuracy and lower variance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoLA and MMLU-Other (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Downstream validation tasks (CoLA; MMLU subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>COPLE optimization using proxy batches of varying sizes (|Z_ref| varied, including 20 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different proxy batch sizes (e.g., 20 examples vs larger sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: CoLA baseline 33.27%; MMLU-Other baseline 38.59%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>With proxy batch of 20 examples, CoLA improved to 62.96% ±1.78%; MMLU-Other improved to 41.13% ±1.69%. Larger proxy batches yielded higher accuracy and lower std dev.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CoLA +29.69 points (33.27 -> 62.96) with only 20 proxy examples (reported); MMLU-Other +2.54 points (38.59 -> 41.13).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Proxy tasks act as a reasonable approximation of the target task distribution; even small proxy samples provide useful optimization feedback, but larger samples improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Proxy tasks sampled i.i.d. from the same distribution (e.g., training set); reported results show mean ± std across runs using batch sizes including 20 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9272.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPLE_k_candidates</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Number of Candidate Words (k) in COPLE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The number of MLM top-k candidate words considered per target token affects COPLE's success; small k is often sufficient for large gains, but larger k can further improve the chance of finding a better prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoLA and MMLU-Other (examples reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CoLA and MMLU subsets evaluated on validation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>COPLE search with per-token candidate set size k (varied), candidates are top-k MLM predictions plus an empty token (delete).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Different k values (e.g., k=5 vs larger k).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: CoLA baseline 33.27%; MMLU-Other baseline 38.59%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>With k=5, CoLA improved to 57.65% ±0.62%; MMLU-Other improved to 44.79% ±0.75%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>CoLA +24.38 points (33.27 -> 57.65) with k=5; MMLU-Other +6.20 points (38.59 -> 44.79).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>A modest candidate set (k around 5) balances efficiency and effectiveness; expanding the search space increases the likelihood of reaching higher-performance prompts at the cost of more evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per-iteration, authors mask the target token and take MLM top-k predictions + delete option; evaluate each candidate on proxy tasks and choose the best that reduces proxy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9272.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>COPLE_search_strategy_vs_random</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Word-Influence Guided Search vs Random Search Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using a word-influence ordering to decide which tokens to search first yields better and more stable COPLE optimization than searching tokens in a random order.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (reported aggregate on MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (aggregate reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>MMLU multiple-choice benchmark (aggregate across subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>COPLE optimization with two search strategies: (a) order tokens by computed influence (abs change in proxy loss when removed), (b) random token order.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Word-influence guided ordering vs random ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: COPLE with influence-guided search average on MMLU 35.66% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>With random ordering, average performance decreased to 34.17% and had larger standard deviation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>-1.49 percentage points average when replacing influence-guided order with random on MMLU (35.66 -> 34.17); variance increased.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Targeting high-influence words first is more efficient at finding beneficial lexical substitutions; random ordering is less stable and less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Influence I(d_i) computed once on the initial prompt using proxy tasks; ablation compares average MMLU accuracy and std across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9272.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>optimized_prompt_semantics_vs_ppl</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Optimized Prompts Preserve Semantics but Increase Perplexity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompts found by COPLE maintain high semantic similarity to the originals (measured by USE and BERTScore) while showing substantially higher perplexity under GPT-2; authors suggest more 'challenging' token choices may improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat (semantics/perplexity comparisons measured across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GLUE tasks (SST2, CoLA, MNLI, QNLI, RTE, MRPC, QQP) and MMLU subsets (STEM, Humanities, Social Sciences, Other)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various NLU classification and MMLU multiple-choice tasks used to evaluate prompt changes.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Original human-crafted prompts vs COPLE-optimized prompts; semantic similarity measured by Universal Sentence Encoder and BERTScore; perplexity measured with GPT-2.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompt vs COPLE-optimized prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>semantic similarity: USE and BERTScore remain high (authors report consistently high similarity across tasks); perplexity: optimized prompts show substantial increase (authors report 'perplexity of the optimized prompts increases significantly compared to the original').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Although lexical surface changes preserve semantics, they alter model-facing token distributions; higher perplexity (less common wording) may better elicit desired model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>USE and BERTScore used to quantify semantic preservation; GPT-2 used to compute perplexity (PPL) of original vs optimized prompts; numeric PPLs reported in Table 3 of the paper (authors note consistent increase).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9272.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9272.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>prompt_scenarios_compared</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Presentation Scenarios (Original, 1-shot, 3-shot, EP02, EP03, Zero-shot-CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper evaluates COPLE and prompt lexical sensitivity across several prompt presentation formats: human-crafted 'Original' prompts, 1-shot/3-shot in-context examples, two 'Emotion Prompt' variants (EP02/EP03), and Zero-shot Chain-of-Thought triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-chat, Mistral-7B-Instruct-v0.1, ChatGPT (gpt-3.5-turbo-0125) in parts</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B for open models; null for ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GLUE and MMLU benchmarks (full set of evaluated tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard GLUE classification tasks and MMLU multiple-choice subsets; prompts vary by inclusion of demonstrations (1/3-shot), added emotional monitoring text (EP02/EP03), or CoT trigger.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Original manual prompts (from HELM/lm-evaluationharness), 1-shot and 3-shot by concatenating examples, Emotion Prompts (EP02/EP03) appending emotional self-monitoring style text, and Zero-shot-CoT trigger appended to prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Across these initial prompt scenarios, the paper reports COPLE applied to each scenario and compares pre/post-COPLE performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>varied per task; example: for Llama-2-7B-chat on QQP 3-shot Original: 27.58% (declines reported for some variants), COPLE improves many scenarios substantially (see COPLE examples above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>COPLE is effective across initial formats; concrete per-scenario numbers are reported in Table 1 and Table 2 (paper presents per-task accuracies with and without COPLE).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Lexical sensitivity affects all tested prompt scenarios; therefore lexical optimization (COPLE) is recommended before more complex prompt engineering like increasing shots or adding CoT triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Original and shot-based prompts taken from HELM and lm-evaluationharness; Emotion prompts follow Li et al. (2023) design; COPLE applied to the task description portion only; evaluations report Accuracy per GLUE/MMLU task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Large language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Grips: Gradient-free, edit-based instruction search for prompting large language models. <em>(Rating: 2)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. <em>(Rating: 2)</em></li>
                <li>Large language models are human-level prompt engineers. <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
                <li>Black-box generation of adversarial text sequences to evade deep learning classifiers. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9272",
    "paper_id": "paper-270199843",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "neighborhood_lexical_variation",
            "name_full": "Neighborhood Lexical Variations in Prompts",
            "brief_description": "Small, semantically-preserving single-word substitutions in the task description portion of prompts can produce large and inconsistent changes in LLM task performance; authors generate 'neighborhood' prompts with a masked LM and evaluate their performance on validation pools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "MMLU (STEM subset)",
            "task_description": "Multiple-choice subject-matter questions (MMLU STEM subset); models are expected to output answer choice labels (A/B/C/D).",
            "presentation_format": "Zero-shot style task description prompt: the task description string 'The following are multiple choice questions (with answers) about {task}.' and single-word lexical variants produced by masking each token and using a pre-trained MLM to fill-in.",
            "comparison_format": "Alternative single-word variants of the same sentence produced by MLM fill-ins (examples: 'The following lists multiple choice questions (with answers) about {task}.'; 'The following are some choice questions (with answers) about {task}.')",
            "performance": "accuracy: original prompt 28.04%",
            "performance_comparison": "accuracy: variant 'lists' 23.92%; variant 'some choice questions' 30.86%",
            "format_effect_size": "+2.82% (30.86 vs 28.04) for one variant; -4.12% (23.92 vs 28.04) for another (example differences reported on MMLU-STEM with Llama-2-7B-chat).",
            "explanation_or_hypothesis": "Authors hypothesize that LLMs are 'lexically sensitive': surface lexical choices alter internal model attention/processing despite negligible semantic differences to humans; nearby prompts in embedding space can still map to different model behaviors.",
            "null_or_negative_result": false,
            "experimental_details": "Neighborhood prompts created by masking each token in the task description and taking the MLM top-10 fill-ins; sentence representations obtained from the target model and visualized with t-SNE; evaluation on MMLU-STEM validation examples; reported accuracies per prompt variant.",
            "uuid": "e9272.0",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "latent_proximity_no_perf_correlation",
            "name_full": "Latent Representation Proximity vs Task Performance",
            "brief_description": "Prompts that are nearby in the model's sentence-embedding/latent space can nonetheless produce widely different downstream task performance; embedding proximity is not predictive of prompt effectiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "CoLA (Corpus of Linguistic Acceptability)",
            "task_description": "Binary acceptability judgment for single sentences.",
            "presentation_format": "Set of neighborhood prompts differing by one word around the original task description 'Does this sentence make sense?'; sentence embeddings projected to 2D using t-SNE to examine representation clustering.",
            "comparison_format": "Prompts that are close in embedding space vs distant prompts; performance compared across neighborhood prompts.",
            "performance": "accuracy: best neighborhood prompt 51.2%",
            "performance_comparison": "accuracy: a nearby (embedding-proximal) prompt nearly worst at 32.4%; difference ~18.8 percentage points.",
            "format_effect_size": "≈18.8% absolute difference between nearby prompts (CoLA with Llama-2-7B-chat example).",
            "explanation_or_hypothesis": "Authors observe that sentence-level latent proximity does not reliably predict downstream performance; they note that models encode lexical differences that affect output behavior even when semantic embeddings cluster together.",
            "null_or_negative_result": true,
            "experimental_details": "Neighborhood prompts generated via MLM fill-ins; sentence representations extracted from the target LLM and projected with t-SNE; performance measured on CoLA validation set.",
            "uuid": "e9272.1",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "COPLE_recover_QQP",
            "name_full": "COPLE Lexical Optimization Recovering Performance on QQP",
            "brief_description": "COPLE, a black-box combinatorial optimization over semantically similar word substitutions guided by proxy-task feedback, can substantially recover or improve LLM accuracy on downstream tasks (example: QQP paraphrase detection).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "QQP (Quora Question Pairs)",
            "task_description": "Binary paraphrase detection: decide whether two questions have the same meaning.",
            "presentation_format": "Human-crafted prompt 'Please identify whether the sentences have the same meaning' (and 3-shot variants); COPLE performs iterative single-word substitutions (semantically similar candidates) guided by performance on proxy tasks to produce an optimized prompt.",
            "comparison_format": "Original human-crafted prompt and degraded prompt variants vs COPLE-optimized prompt (example optimized prompt: 'Please identify since the sentences repeat the same theme').",
            "performance": "accuracy: initial example reported ~35% with the original prompt (Figure 1 example).",
            "performance_comparison": "accuracy after COPLE optimization increased to 57% (Figure 1 example); another example: 3-shot accuracy originally 27.58% decreased to 23.03% with a variant, then COPLE increased the degraded 23.03% to 57.61% (Llama-2-7B-chat on QQP).",
            "format_effect_size": "+22 percentage points (35% -&gt; 57%) in Figure 1 example; +34.58 percentage points (23.03% -&gt; 57.61%) for the 3-shot degraded -&gt; COPLE recovery example.",
            "explanation_or_hypothesis": "COPLE finds lexically different prompts that better align with the model's idiosyncratic lexical sensitivities; optimizing lexical choices can recover both instruct-following and problem-solving ability without gradient access or human re-engineering.",
            "null_or_negative_result": false,
            "experimental_details": "COPLE uses a pre-trained MLM to propose top-k candidate substitutions per token, ranks tokens by word-influence (expected loss change when removed), uses a small batch of proxy reference tasks (sampled from training set) to evaluate candidates, and iteratively substitutes the most influential tokens; reported example uses the QQP validation/test setups.",
            "uuid": "e9272.2",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "COPLE_word_count_ablation",
            "name_full": "Effect of Number of Words Changed in COPLE Optimization",
            "brief_description": "Changing only a small number of the most influential words in the task description is sufficient to materially improve performance; increasing the number of changed words tends to further increase accuracy but with diminishing returns.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "CoLA and MMLU-Other (examples reported)",
            "task_description": "CoLA: sentence acceptability; MMLU-Other: MMLU subset 'Other' domain multiple-choice questions.",
            "presentation_format": "Original human-crafted prompt with varying counts of top-influence words substituted by COPLE (e.g., change 1 word, 2 words, ...).",
            "comparison_format": "Different numbers of substituted words (e.g., 1-word change vs more words).",
            "performance": "accuracy: CoLA original 33.27% (reported baseline).",
            "performance_comparison": "accuracy after changing only the single most influential word: CoLA improved to 46.50% ±1.23%; MMLU-Other improved from 38.59% to 44.04% ±1.07%.",
            "format_effect_size": "CoLA +13.23 percentage points for single-word change (33.27 -&gt; 46.50); MMLU-Other +5.45 points (38.59 -&gt; 44.04).",
            "explanation_or_hypothesis": "A small number of high-influence lexical tokens strongly mediate prompt utility; targeting these tokens is an efficient way to recover performance.",
            "null_or_negative_result": false,
            "experimental_details": "Authors compute token influence I(d_i) as expected change in proxy-task loss when token removed; then substitute tokens in descending influence order; reported mean and std across runs.",
            "uuid": "e9272.3",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "COPLE_proxy_batch_size",
            "name_full": "Effect of Proxy Reference Task Batch Size on COPLE",
            "brief_description": "The size of the proxy reference task batch used to evaluate candidate prompt substitutions affects COPLE's effectiveness and stability; even small proxy batches (20 examples) can produce substantial improvements, while larger batches improve accuracy and lower variance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "CoLA and MMLU-Other (examples reported)",
            "task_description": "Downstream validation tasks (CoLA; MMLU subsets).",
            "presentation_format": "COPLE optimization using proxy batches of varying sizes (|Z_ref| varied, including 20 examples).",
            "comparison_format": "Different proxy batch sizes (e.g., 20 examples vs larger sizes).",
            "performance": "accuracy: CoLA baseline 33.27%; MMLU-Other baseline 38.59%.",
            "performance_comparison": "With proxy batch of 20 examples, CoLA improved to 62.96% ±1.78%; MMLU-Other improved to 41.13% ±1.69%. Larger proxy batches yielded higher accuracy and lower std dev.",
            "format_effect_size": "CoLA +29.69 points (33.27 -&gt; 62.96) with only 20 proxy examples (reported); MMLU-Other +2.54 points (38.59 -&gt; 41.13).",
            "explanation_or_hypothesis": "Proxy tasks act as a reasonable approximation of the target task distribution; even small proxy samples provide useful optimization feedback, but larger samples improve robustness.",
            "null_or_negative_result": false,
            "experimental_details": "Proxy tasks sampled i.i.d. from the same distribution (e.g., training set); reported results show mean ± std across runs using batch sizes including 20 examples.",
            "uuid": "e9272.4",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "COPLE_k_candidates",
            "name_full": "Effect of Number of Candidate Words (k) in COPLE",
            "brief_description": "The number of MLM top-k candidate words considered per target token affects COPLE's success; small k is often sufficient for large gains, but larger k can further improve the chance of finding a better prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat",
            "model_size": "7B",
            "task_name": "CoLA and MMLU-Other (examples reported)",
            "task_description": "CoLA and MMLU subsets evaluated on validation sets.",
            "presentation_format": "COPLE search with per-token candidate set size k (varied), candidates are top-k MLM predictions plus an empty token (delete).",
            "comparison_format": "Different k values (e.g., k=5 vs larger k).",
            "performance": "accuracy: CoLA baseline 33.27%; MMLU-Other baseline 38.59%.",
            "performance_comparison": "With k=5, CoLA improved to 57.65% ±0.62%; MMLU-Other improved to 44.79% ±0.75%.",
            "format_effect_size": "CoLA +24.38 points (33.27 -&gt; 57.65) with k=5; MMLU-Other +6.20 points (38.59 -&gt; 44.79).",
            "explanation_or_hypothesis": "A modest candidate set (k around 5) balances efficiency and effectiveness; expanding the search space increases the likelihood of reaching higher-performance prompts at the cost of more evaluations.",
            "null_or_negative_result": false,
            "experimental_details": "Per-iteration, authors mask the target token and take MLM top-k predictions + delete option; evaluate each candidate on proxy tasks and choose the best that reduces proxy loss.",
            "uuid": "e9272.5",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "COPLE_search_strategy_vs_random",
            "name_full": "Word-Influence Guided Search vs Random Search Ablation",
            "brief_description": "Using a word-influence ordering to decide which tokens to search first yields better and more stable COPLE optimization than searching tokens in a random order.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (reported aggregate on MMLU)",
            "model_size": null,
            "task_name": "MMLU (aggregate reported)",
            "task_description": "MMLU multiple-choice benchmark (aggregate across subsets).",
            "presentation_format": "COPLE optimization with two search strategies: (a) order tokens by computed influence (abs change in proxy loss when removed), (b) random token order.",
            "comparison_format": "Word-influence guided ordering vs random ordering.",
            "performance": "accuracy: COPLE with influence-guided search average on MMLU 35.66% (reported).",
            "performance_comparison": "With random ordering, average performance decreased to 34.17% and had larger standard deviation.",
            "format_effect_size": "-1.49 percentage points average when replacing influence-guided order with random on MMLU (35.66 -&gt; 34.17); variance increased.",
            "explanation_or_hypothesis": "Targeting high-influence words first is more efficient at finding beneficial lexical substitutions; random ordering is less stable and less effective.",
            "null_or_negative_result": false,
            "experimental_details": "Influence I(d_i) computed once on the initial prompt using proxy tasks; ablation compares average MMLU accuracy and std across runs.",
            "uuid": "e9272.6",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "optimized_prompt_semantics_vs_ppl",
            "name_full": "Optimized Prompts Preserve Semantics but Increase Perplexity",
            "brief_description": "Prompts found by COPLE maintain high semantic similarity to the originals (measured by USE and BERTScore) while showing substantially higher perplexity under GPT-2; authors suggest more 'challenging' token choices may improve performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat (semantics/perplexity comparisons measured across tasks)",
            "model_size": "7B",
            "task_name": "GLUE tasks (SST2, CoLA, MNLI, QNLI, RTE, MRPC, QQP) and MMLU subsets (STEM, Humanities, Social Sciences, Other)",
            "task_description": "Various NLU classification and MMLU multiple-choice tasks used to evaluate prompt changes.",
            "presentation_format": "Original human-crafted prompts vs COPLE-optimized prompts; semantic similarity measured by Universal Sentence Encoder and BERTScore; perplexity measured with GPT-2.",
            "comparison_format": "Original prompt vs COPLE-optimized prompt.",
            "performance": "semantic similarity: USE and BERTScore remain high (authors report consistently high similarity across tasks); perplexity: optimized prompts show substantial increase (authors report 'perplexity of the optimized prompts increases significantly compared to the original').",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Although lexical surface changes preserve semantics, they alter model-facing token distributions; higher perplexity (less common wording) may better elicit desired model behavior.",
            "null_or_negative_result": false,
            "experimental_details": "USE and BERTScore used to quantify semantic preservation; GPT-2 used to compute perplexity (PPL) of original vs optimized prompts; numeric PPLs reported in Table 3 of the paper (authors note consistent increase).",
            "uuid": "e9272.7",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "prompt_scenarios_compared",
            "name_full": "Prompt Presentation Scenarios (Original, 1-shot, 3-shot, EP02, EP03, Zero-shot-CoT)",
            "brief_description": "Paper evaluates COPLE and prompt lexical sensitivity across several prompt presentation formats: human-crafted 'Original' prompts, 1-shot/3-shot in-context examples, two 'Emotion Prompt' variants (EP02/EP03), and Zero-shot Chain-of-Thought triggers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-chat, Mistral-7B-Instruct-v0.1, ChatGPT (gpt-3.5-turbo-0125) in parts",
            "model_size": "7B for open models; null for ChatGPT",
            "task_name": "GLUE and MMLU benchmarks (full set of evaluated tasks)",
            "task_description": "Standard GLUE classification tasks and MMLU multiple-choice subsets; prompts vary by inclusion of demonstrations (1/3-shot), added emotional monitoring text (EP02/EP03), or CoT trigger.",
            "presentation_format": "Original manual prompts (from HELM/lm-evaluationharness), 1-shot and 3-shot by concatenating examples, Emotion Prompts (EP02/EP03) appending emotional self-monitoring style text, and Zero-shot-CoT trigger appended to prompt.",
            "comparison_format": "Across these initial prompt scenarios, the paper reports COPLE applied to each scenario and compares pre/post-COPLE performance.",
            "performance": "varied per task; example: for Llama-2-7B-chat on QQP 3-shot Original: 27.58% (declines reported for some variants), COPLE improves many scenarios substantially (see COPLE examples above).",
            "performance_comparison": "COPLE is effective across initial formats; concrete per-scenario numbers are reported in Table 1 and Table 2 (paper presents per-task accuracies with and without COPLE).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Lexical sensitivity affects all tested prompt scenarios; therefore lexical optimization (COPLE) is recommended before more complex prompt engineering like increasing shots or adding CoT triggers.",
            "null_or_negative_result": null,
            "experimental_details": "Original and shot-based prompts taken from HELM and lm-evaluationharness; Emotion prompts follow Li et al. (2023) design; COPLE applied to the task description portion only; evaluations report Accuracy per GLUE/MMLU task.",
            "uuid": "e9272.8",
            "source_info": {
                "paper_title": "Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Grips: Gradient-free, edit-based instruction search for prompting large language models.",
            "rating": 2,
            "sanitized_title": "grips_gradientfree_editbased_instruction_search_for_prompting_large_language_models"
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "rating": 2,
            "sanitized_title": "autoprompt_eliciting_knowledge_from_language_models_with_automatically_generated_prompts"
        },
        {
            "paper_title": "Large language models are human-level prompt engineers.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_humanlevel_prompt_engineers"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        },
        {
            "paper_title": "Black-box generation of adversarial text sequences to evade deep learning classifiers.",
            "rating": 1,
            "sanitized_title": "blackbox_generation_of_adversarial_text_sequences_to_evade_deep_learning_classifiers"
        }
    ],
    "cost": 0.0184525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement
31 May 2024</p>
<p>Pengwei Zhan zhanpengwei@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Cyber Security
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Zhen Xu xuzhen@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>Qian Tan tanqian@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>Jie Song songjie@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Cyber Security
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Ru Xie xieru@iie.ac.cn 
Institute of Information Engineering
Chinese Academy of Sciences
BeijingChina</p>
<p>School of Cyber Security
University of Chinese Academy of Sciences
BeijingChina</p>
<p>Leo Gao 
Jonathan Tow 
Baber Abbasi 
Stella Biderman 
Sid Black 
Anthony Dipofi 
Charles Foster 
Laurence Golding 
Jeffrey Hsu 
Alain Le Noac'h 
Haonan Li 
Jordan Hoffmann 
Sebastian Borgeaud 
Arthur Mensch 
Elena Buchatskaya 
Trevor Cai 
Eliza Rutherford 
Diego De 
Las Casas 
Lisa Anne Hendricks 
Johannes Welbl 
Aidan Clark 
Tom Hennigan 
Eric Noland 
Katie Millican 
George Van Den Driessche 
Bogdan Damoc 
Aurelia Guy 
Simon Osindero 
Karen Si- Monyan 
Erich Elsen 
Jack W Rae 
Jared Kaplan 
Sam Mccandlish 
Tom Henighan 
Tom B Brown 
Benjamin Chess 
Rewon Child 
Scott Gray 
Alec Radford 
Jeffrey Wu 
Dario 2020 Amodei 
Tony Lee 
Michihiro Yasunaga 
Chenlin Meng 
Yifan Mai 
Joon Sung Park 
Agrim Gupta 
Yunzhi Zhang 
Deepak Narayanan 
Hannah Teufel 
Marco Bella- Gente 
Minguk Kang 
Taesung Park 
Jure Leskovec 
Jun-Yan Zhu 
Fei-Fei Li 
Jiajun Wu 
Stefano Ermon 
Percy Liang 
Holistic 
Cheng Li 
Jindong Wang 
Kaijie Zhu 
Yixuan Zhang 
Yao Lu 
Max Bartolo 
Alastair Moore 
Sebastian Riedel 
Long Ouyang 
Jeffrey Wu 
Xu Jiang 
Diogo Almeida 
Carroll L Wainwright 
Pamela Mishkin 
Chong Zhang 
Sandhini Agarwal 
Katarina Slama 
Alex Ray 
John Schulman 
Jacob Hilton 
Fraser Kelton 
Luke Miller 
Maddie Simens 
Amanda Askell 
Peter Welin- Der 
Paul F Christiano 
Jan Leike </p>
<p>Kyle McDonell
Niklas Muennighoff
Chris Ociepa, Jason Phang</p>
<p>Laria Reynolds
Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, An-ish Thite, Kevin Wang, and Andy ZouBen Wang</p>
<p>Unveiling the Lexical Sensitivity of LLMs: Combinatorial Optimization for Prompt Enhancement
31 May 202434CA016D2FFB6091E0F8214A0A9C4318arXiv:2405.20701v1[cs.CL]
Large language models (LLMs) demonstrate exceptional instruct-following ability to complete various downstream tasks.Although this impressive ability makes LLMs flexible task solvers, their performance in solving tasks also heavily relies on instructions.In this paper, we reveal that LLMs are over-sensitive to lexical variations in task instructions, even when the variations are imperceptible to humans.By providing models with neighborhood instructions, which are closely situated in the latent representation space and differ by only one semantically similar word, the performance on downstream tasks can be vastly different.Following this property, we propose a blackbox Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).COPLE performs iterative lexical optimization according to the feedback from a batch of proxy tasks, using a search strategy related to word influence.Experiments show that even widelyused human-crafted prompts for current benchmarks suffer from the lexical sensitivity of models, and COPLE recovers the declined model ability in both instruct-following and solving downstream tasks.</p>
<p>Introduction</p>
<p>Language models have achieved remarkable performance in recent years, particularly those referred to as large language models (LLMs), which contain scaled-up parameters and size (Kaplan et al., 2020;Brown et al., 2020;Hoffmann et al., 2022;OpenAI, 2022;Touvron et al., 2023;Jiang et al., 2023).These models demonstrate an exceptional ability to follow human instructions and complete downstream tasks after instruction tuning (Ouyang et al., 2022).In contrast to masked language models (MLMs) like BERT (Devlin et al., 2019)</p>
<p>New Prompt</p>
<p>Please identify whether the sentences have the same meaning.</p>
<p>Influence</p>
<p>Figure 1: Prompt lexical enhancement from a combinatorial optimization perspective.Initially, we provide the prompt "Please identify whether the sentences have the same meaning" for Llama-2-7B-chat to complete the tasks from Quora Question Pairs2 (QQP), and combine the validation set of QQP with the prompt as a predefined task pool, with each example being an individual task.By iteratively substituting the most influential words in the prompt with semantically similar words picked from the potential search space, we find the optimal prompt "Please identify since the sentences repeat the same theme" that increases the accuracy from 35% to 57%.The details of operations can be found in §3.3.</p>
<p>different downstream tasks.Instead, they complete a wide range of tasks in the same way of generating text, by following different task instructions.</p>
<p>Although the instruction-following ability of LLMs makes them flexible task solvers, their performance on solving tasks also significantly depends on the instructions (i.e., prompts), which are mainly designed by human intuitively and empirically (Wei et al., 2022;Lu et al., 2022;Kojima et al., 2022;Zhou et al., 2023a).These manually designed prompts that incorporated with human knowledge effectively improve the model's performance on specific tasks.However, following Gao et al. (2018), Garg and Ramakrishnan (2020) and Feng et al. (2018), even a minor lexical modification in the input that is imperceptible to humans can lead to vastly different model attention and outputs.Therefore, it is natural to wonder: whether the prompts carefully constructed by humans maximize LLMs' performance on downstream tasks?For example, in the context of a sentiment classification task, while humans may confidently assert that the prompt "Please classify the sentiment of the given text" outperforms "Check the given text", it is hard to say whether it would outperform a prompt like "Please analyze the sentiment of the given text".</p>
<p>The unexpected sensitivity of language models to these imperceptible lexical perturbations suggests the possible existence of an alternate prompt, which is differs from the original prompt by only a few substituted words, yet yields superior performance on downstream tasks.This insight allows us to frame the process of discovering such an optimal prompt as a combinatorial optimization problem (Blair, 1990), which consists of two key components: the search space and the search method.The search space can be defined as the set of all potential substitutions for each word in the original prompt, while the search method specifies the strategy for exploring this space and identifying the optimal substitutions.Figure 1 provides a more intuitive example of the process of finding the optimal prompt from a lexical combinatorial optimization perspective.We argue that even without the complex prompt engineering, minor lexical modifications to prompts yield substantial improvements to a model's performance.</p>
<p>In this paper, we reveal the notable sensitivity of LLMs to lexical variations in prompts, which potentially undermine the effectiveness of human-crafted prompts, from the view of combinatorial optimization.Based on our findings, we also propose a black-box Combinatorial Optimization framework for Prompt Lexical Enhancement (COPLE).We summarize our main contributions as follows:</p>
<ol>
<li>
<p>We intuitively reveal the notable sensitivity of LLMs to the lexical choices in prompts, which suggests the existence of prompts that, while highly similar to the original, can lead to improved performance on downstream tasks.</p>
</li>
<li>
<p>We propose COPLE, a black-box combina-torial optimization framework that enhances prompts on downstream tasks, which performs iteratively lexical optimization under the guidance of word influence.</p>
</li>
<li>
<p>We evaluate COPLE on popular datasets, models, and initial prompts.The results show that COPLE effectively maximizes the performance of prompts in downstream tasks with only lexical modifications, without accessing model parameters or involving complex prompt engineering with human participation.</p>
</li>
</ol>
<p>Related Work</p>
<p>Sensitivity to Imperceptible Changes.The outstanding performance of language models seems to be built upon their excellent understanding of text (Devlin et al., 2019;Dong et al., 2019;Radford et al., 2018a,b;Brown et al., 2020).However, previous works reveal that even imperceptible input perturbations, which do not affect human comprehension, can lead to significant changes in the model's output (Goodfellow et al., 2015;Papernot et al., 2016;Zhan et al., 2022aZhan et al., , 2023a;;Carlini and Wagner, 2017).This property has been widely exploited to create adversarial examples, where small modifications to the embedding or input text can cause the model to generate incorrect answers (Gao et al., 2018;Zhan et al., 2022bZhan et al., , 2024;;Li et al., 2019Li et al., , 2020;;Zhan et al., 2023b;Zang et al., 2020).Therefore, we believe even humans experienced in designing prompts may overlook the performance discrepancies caused by such imperceptible changes.</p>
<p>Prompt Tuning and Optimizing.Similarly, recent efforts to optimize prompts for LLMs find that not only the content (Kojima et al., 2022;Yang et al., 2023) but also the format (Zhou et al., 2023a;Lu et al., 2022;Wei et al., 2023;Madaan and Yazdanbakhsh, 2022;Prasad et al., 2023) of the prompt, such as the order of examples and phrases, significantly influence the model performance.Consequently, this sensitivity of language models to minor changes makes the optimal prompt found by the community increasingly complex.For example, Xu et al. (2023) transforms a prompt of length 6 into one exceeding 900 tokens.However, we argue that also due to this sensitivity, complex variations of the prompt should not be the first operation in prompt optimization, as the performance could be inadvertently constrained by the specific words employed in the prompt.This proposition distinguishes our work from previous studies on prompt optimization: given a prompt that has proven initially effective, we focus on the lexical influence of the prompt on model performance, attempting to recover the potential performance drop caused by lexical choices, rather than creating a prompt that yields optimal results from scratch (Shin et al., 2020;Zhou et al., 2023b;Zhang et al., 2022;Yang et al., 2023;Prasad et al., 2023).</p>
<p>Methodology</p>
<p>Prompt Enhancement</p>
<p>Suppose we are given a data distribution D over a sequence of downstream tasks Z = {X , Y}, and each task in the entire task set can be seen as a pair of question and answer {X, Y } that both consist of multiple tokens.To recognize a pre-trained autoregressive language model f θ as the task solver on the task set Z, we hope it can map the input questions to the output answers f θ : X → Y.However, as model f θ is not fine-tuned for a specific task, this mapping can only be held with the help of a task-specific prompt
P Z (X) = (D, E ′ , X, V ),
where D is the task description, E ′ are optional demo examples for few-shot learning, and V is the verbalizer that limits the responses of the model to a set of label words.We can then formulate the performance of the model on the task set as:
E (X,Y )∼D [L(f θ (P Z (X)), Y )] (1)
where L is a task-specific loss function that measures the discrepancy between the model's output and the ground truth answer.Following this, we can find that directly optimizing the prompt P Z (X) in the discrete token space is challenging due to the non-differentiable nature of text and the large search space.Therefore, it is more suitable to frame the process of optimizing the prompt as a combinatorial optimization problem, where we aim to find the optimal combination of tokens from a predefined search space that consists of candidate tokens.In this paper, to be more specific, we focus on investigating the influence of minor lexical changes on the task description part of the prompt.Let D = (d 1 , d 2 , . . ., d n ) be the sequence of tokens in the task description, and C i ∈ C denote the search space of token d i .The optimal alternative task description that recovers the potential performance drop caused by wording can thus be formulated as:
D * = (d * 1 , d * 2 , . . . , d * n ), s.t. d * i ∈ C i and ∀i ∈ {1, . . . , n}, ∆d i &lt; δ (2)
where ∆d i denotes the difference between d i and d * i , and δ denotes a small maximum allowed difference between them, which limits the possible candidates in the search space to tokens that are semantically similar to the original one.This optimal task description D * is expected to minimize the expected loss on downstream tasks:
D * = arg min D E (X,Y )∼D [L(f θ (P Z (X)), Y )] (3)
Therefore, the optimal prompt for task Z can be formulated as P * Z (X) = (D * , E ′ , X, V ).</p>
<p>Impact of Minor Lexical Changes</p>
<p>The analysis presented in the previous section relies on a crucial premise: imperceptible lexical changes in prompts can significantly affect the model performance on downstream tasks.Before further explaining our approach, we first try to show the validity of this assumption.</p>
<p>Given an initially proven effective prompt, a model, and a predefined task pool, we attempt to demonstrate how prompts within the neighborhood of the original prompt influence the model's performance on the task pool.In this context, we broadly define a prompt's neighborhood as prompts that differ from the original by only one word while maintaining a similar meaning.For instance, we consider "Does the sentence make sense?" to be within the neighborhood of "Does this sentence make sense?".To obtain these qualifying prompts, we employ a MLM.First, we iteratively replace each word in the task description with a [MASK] token.We then expect the MLM, by understanding the context, to provide the most probable fill-in words at each position based on the entire task description.After replacing the original words with the fill-in words at each position, we obtain a series of prompts within the neighborhood of the original prompt.We then evaluate the performance of each resulting prompt on the task pool.</p>
<p>Following these definitions and operations, we employ the validation sets of CoLA (Warstadt et al., 2019) and MMLU-STEM (Hendrycks et al., 2021) subtasks, respectively, as predefined task pools.We use Llama-2-7B-chat (Touvron et al., 2023) and Mistral-7B- Instruct-v0.1 (Jiang et al., 2023) as target models and use RoBERTa (Liu et al., 2019) for obtaining neighborhood prompts.The initial prompts are picked from lm-evaluationharness (Gao et al., 2023), and we generate ten most probable fill-in words for each position in The following lists multiple choice questions (with answers) about {task}.</p>
<p>The following are multiple choice questions (with answers) about {task}.</p>
<p>The following are some choice questions (with answers) about {task}.</p>
<p>ith answ ith answ ith answers) about {task}.</p>
<p>Prompt</p>
<p>The following are multiple choice questions (with answers) about {task}.</p>
<p>The following are multiple choice questions (with hints) about {task}.</p>
<p>The following are multiple user questions (with answers) about {task}.</p>
<p>The following are multiple choice questions (with answers) about {task}.task description.We then obtain their sentence representations in the target model and project them into a two-dimensional space using t-SNE (van der Maaten and Hinton, 2008).Figure 2 shows the performance of neighborhood prompts on downstream tasks with the distribution of their sentence representations.We can then reach several conclusions on the impact of minor lexical changes in prompts on downstream performance.1.Semantically similar prompts have vastly different performances on downstream tasks, even if they differ by only one word.For example, when using neighborhood prompts on MMLU-STEM and Llama-2-7B-chat (Figure 2(c)), their performance differences can reach 7%.Specifically, changing "The following are multiple choice questions (with answers) about {task}."(■) to "The following lists multiple choice questions (with answers) about {task}."(•) reduces the accuracy from the original 28.04% to 23.92%, while changing it to "The following are some choice questions (with answers) about {task}."(•) increases the accuracy to 30.86%.Intuitively, such minor lexical variations should have a minimal impact on semantics, and human performance would likely remain consistent when completing downstream tasks guided by these three prompts (Adam Drewnowski, 1978;McCusker et al., 1981;van Orden, 1987;Rayner et al., 2006).However, models exhibit a high degree of sensitivity to these changes.</p>
<p>Prompt Representation with Accuracy Representation with Accuracy Representation with Accuracy Representation with Accuracy</p>
<p>2.In the latent representation space, prompts that are in close proximity may have vastly different performance on downstream tasks.In most cases, the performance of neighborhood prompts on downstream tasks does not demonstrate a clear correlation with the distribution of their sentence representations.Even when the representations of prompts are clustered together, they can still have substantial performance discrepancies.For example, in the representation space of Llama-2-7B-chat, the best-performing prompt (51.2%, •) on CoLA (Figure 2(a)) is situated in very close proximity to the prompt with nearly the worst performance (32.4%, •).From the perspective of sentence representations, this observation indicates that even for semantically highly similar prompts, their performance may be vastly different, and it is difficult to infer their performance from one another directly.</p>
<p>COPLE</p>
<p>According to our findings, even for semantically similar prompts with only one word difference, their performance on downstream tasks may be very different, and we cannot infer the performance of one prompt from another seemingly similar prompt.Therefore, we propose COPLE, trying to recover the degraded ability of models caused by lexical sensitivity.The key idea behind COPLE is to guide the lexical optimization of the initial prompt by the model performance on a batch of reference tasks i.i.d. to the downstream tasks, and iteratively improve the prompt based on the feedback from these references to converge towards an optimal prompt that maximizes performance across the task distribution.Specifically, COPLE consists of the following four parts: Proxy Reference Tasks.To find the optimal D * defined in (3), while avoiding data leakage, we first randomly sample a batch of reference tasks from the same distribution D, denoted as Z ref .For example, when targeting the validation set of a dataset as downstream tasks, we construct the reference tasks by sampling from the training set.These reference tasks serve as a proxy for evaluating the prompt on the task distribution, which also accelerates COPLE, as evaluating on a small batch of examples is not as expensive as evaluating on the full validation set.Therefore, the optimal task description that COPLE tries to find can be transformed from (3) to:
D * = arg min D L ref (D) = arg min D E (X,Y )∼Z ref <a href="4">L(f θ (P Z ref (X)), Y )</a>
where P Z ref denotes the entire prompt for the reference tasks Z ref .</p>
<p>Search by Word Influence.With the proxy reference tasks, COPLE then performs an iterative optimization process to find the optimal task description D * .As COPLE serves as a black-box method without accessing the gradient information of the model, we first define the influence of each word in the task description as the expected performance difference on proxy tasks when the word is deleted from the task description.Formally:
I(d i ) = |L ref (D) − L ref (D \i )|(5)
where D \i denotes the task description with token d i removed.For efficiency purposes, COPLE obtains the influence of each word only on the initial task description.Then, COPLE tries to iteratively find the optimal substitution for the most influential words in the descending order of their influence.</p>
<p>Lexical Search Space.To construct the search space C i for token d i in the task description, similar to that in §3.2, we reuse a pre-trained MLM to find semantically similar words.Formally, at each iteration t, we mask out the target d i in current task description and feed the masked description into a pre-trained MLM f MLM .The MLM then predicts a probability distribution over its vocabulary V for the masked position:
p(w|D (t) \i ) = f MLM (d 1 , . . . , [MASK], . . . , d n ) (6)
where w ∈ V, D</p>
<p>\i denotes the task description at iteration t with token d i masked out.We then select the top-k words with the highest probabilities and a empty token (delete) as the candidates.</p>
<p>Iterative Optimization.At each iteration t, COPLE selects the most influential token that has not been searched and constructs its corresponding search space according to (6).For each candidate c ∈ C i , COPLE substitutes d i with c and evaluates the performance of the updated task description D (t) on the small proxy reference tasks:
L ref (D (t) ) = E (X,Y )∼Z ref [L(f θ (P (t) Z ref (X)), Y )] (7)
where D (t) = (d 1 , . . ., c, . . ., d n ), and
P (t)
Z ref is the prompt with the task description D (t) at iteration t.COPLE then selects the candidate c * that minimizes the expected loss on the proxy reference tasks:
c * = arg min c∈C i L ref (D (t) ) (8)
and updates the task description to D (t+1) by replacing d i with c * if its performance is better than
D (t) , i.e., if L ref (D (t+1) ) &lt; L ref (D (t) ): D (t+1) = (d 1 , . . . , c * , . . . , d n )(9)
otherwise, D (t+1) is kept the same as D (t) .This process is repeated until all the most influential words are traversed (detailed in §4.1).Ideally, we take the found best D after optimization as the D * defined in (4).The final optimized task description D * is then used to construct the optimal prompt P * Z (X) for the downstream tasks.</p>
<p>Experiment</p>
<p>Experiment Setup</p>
<p>Dataset and Model.We use GLUE (Wang et al., 2019) and MMLU (Hendrycks et al., 2021) for evaluation.For GLUE, we report the results on SST2 (Socher et al., 2013), CoLA (Warstadt et al., 2019), MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Giampiccolo et al., 2007), MRPC (Dolan and Brockett, 2005), and QQP (Cer et al., 2017).For MMLU, we separately report the results on the subset of STEM, Humanities, Social Sciences, and Other.We use the Llama-2-7B-chat (Touvron et al., 2023)</p>
<p>Analysis and Ablation Study</p>
<p>In this section, we conduct further analysis and ablation studies on COPLE.When not specified, the results are obtained on Llama-2-7B-chat.</p>
<p>Difference Between Prompts.To measure the difference between original prompts and optimized prompts, we utilize Universal Sentence Encoder (USE) (Cer et al., 2018) and BERTScore (Zhang et al., 2020) to obtain semantic similarity.We also obtain their perplexity (PPL) (Jelinek et al., 1977) with GPT-2 (Radford et al., 2018b).Table 3 illustrates the differences between prompts.The USE similarity and BERTScore between the original and optimized prompts are consistently high across all tasks, indicating that the semantics of the prompts are well-preserved after optimization, which also confirms our conclusions in §3.2.However, the perplexity of the optimized prompts increases significantly compared to the original, indicating that using challenging words for the language model in prompts, rather than common words picked by humans, may help to improve the model performance on solving downstream tasks.</p>
<h1>.Word Change in Prompt.Figure 3 shows the impact of the number of words changed in prompt.Even if only changing the one word with the highest influence, the model performance signifi-    Word Influence.Table 4 shows the ablation results of the search strategy related to word influence in COPLE.When replacing the search strategy with the random method, the average performance optimized by COPLE on MMLU decreases from 35.66% to 34.17%, and COPLE is less stable as the standard deviation gets larger.</h1>
<p>How far is COPLE from the optimal results? Figure 6 shows the performance gap between the potential best prompts found by COPLE directly on the validation set (3) and on proxy tasks (4).The results show that the proxy tasks provide a reasonable approximation of the target task distribution, and the performance of COPLE is close to optimal.</p>
<p>Conclusion</p>
<p>In this paper, we demonstrate the notable lexical sensitivity of LLMs to prompts, which potentially degrades their performance on downstream tasks.We show that even semantically similar prompts located in the neighborhood of the latent representation space may yield very different results.</p>
<p>To recover the performance drop caused by the sensitivity, we propose COPLE, a black-box combinatorial optimization framework that iteratively improves lexical choices in prompts.Experiments illustrate the effectiveness of COPLE in recovering both the model's ability of instruct-following and solving downstream tasks.We believe that carefully checking the word usage is essential before performing complex prompt engineering.</p>
<p>Limitations</p>
<p>Despite the effectiveness of COPLE, we want to discuss some limitations of this work.Firstly, our experimental scope is primarily restricted to models around the 7-billion-parameter scale, as our computational resources are limited.Secondly, while we focus on optimizing the lexical choices within the task description component of the prompts, it is possible that lexical sensitivity affects the entirety of a prompt.However, expanding our optimization to include the full prompt significantly increases the size of search space, making the experiment computationally infeasible with our current resources.Thirdly, although we believe that lexical optimization should be a fundamental step prior to more complex prompt engineering methods, our research does not explore the combination of our proposed COPLE framework with other prompt engineering strategies that potentially yield further improvements in model performance.Despite these limitations, our study provides insights into the influence of lexical variation on language model prompts, from both the perspective of downstream performance and latent sentence representation.The findings highlight that even subtle lexical changes, when systematically optimized, can significantly enhance the performance of language models on downstream tasks.</p>
<p>IEEE Security and Privacy Workshops, SP Work-</p>
<p>A Additional Experimental Details</p>
<p>A.1 Details on Dataset</p>
<p>The General Language Understanding Evaluation (GLUE) (Wang et al., 2019) benchmark is a collection of datasets for training, evaluating, and analyzing natural language understanding systems.The subset used in our experiment include: (1) The Stanford Sentiment Treebank (SST2) (Socher et al., 2013) consists of movie review sentences annotated for sentiment.</p>
<p>(2) The Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2019) contains English acceptability judgments drawn from linguistic theory publications.</p>
<p>(3) The Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2018) corpus includes sentence pairs annotated with textual entailment information.(4) The Question-answering NLI (QNLI) (Rajpurkar et al., 2016) is derived from SQuAD, converted to a binary sentence pair classification task.( 5) The Recognizing Textual Entailment (RTE) datasets come from a series of textual entailment challenges (Dagan et al., 2005;Bar-Haim et al., 2006;Giampiccolo et al., 2007;Bentivogli et al., 2009).( 6) The Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) contains sentence pairs annotated for semantic equivalence.( 7) The Quora Question Pairs2 (QQP) (Cer et al., 2017) includes question pairs from Quora annotated for semantic equivalence.</p>
<p>A.2 Details on Evaluation</p>
<p>We follow the same evaluation style as HELM (Lee et al., 2023), which expects the model to output the correct label word (for GLUE) or the letter of the correct option (for MMLU), rather than directly using the probability of the output token for judgement.For example, on SST2, for a sentence with positive sentiment, we expect the model output to be "positive".Similarly, on MMLU, the model is expected to output one of the letters "A", "B", "C", or "D" that matches the correct answer.Otherwise, we consider the model makes incorrect decisions.Note that this evaluation approach may result in model performance worse than that of random guessing.However, we believe that it provides a more accurate indication of the model's ability on instruction following and on solving downstream tasks.For all datasets, we report the performance with Accuracy.</p>
<p>A.3 Details on Baseline Prompts</p>
<p>In the main text, we perform COPLE on various scenarios, including Original, 1-shot, 3-shot, EP02, EP03, and Zero-shot-CoT; here we report the detailed prompts of these scenarios.The initial prompts for GLUE are in Table 6-12, and the initial prompts for MMLU are in Table 13-14.It should also be noted that for scenarios of Emotion Prompts, following Li et al. ( 2023), we insert the additional text at the end of the task description and verbalizer, but before the demo examples, and we do not consider the additional text to be a part of the task descriptions to perform optimization.</p>
<p>Dataset Scenario Prompt</p>
<p>SST2</p>
<p>Original</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Do not respond with anything other than the labels 'positive' or 'negative'.</p>
<p>Question: {content} Answer:</p>
<p>1-shot</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Do not respond with anything other than the labels 'positive' or 'negative'.</p>
<p>Question: {demo_content} Answer: {demo_answer} Question: {content} Answer:</p>
<p>3-shot</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Do not respond with anything other than the labels 'positive' or 'negative'.</p>
<p>Question: {demo_content_1}</p>
<p>Answer: {demo_answer_1}</p>
<p>Question: {demo_content_2} Answer: {demo_answer_2}</p>
<p>Question: {demo_content_3} Answer: {demo_answer_3}</p>
<p>Question: {content} Answer:</p>
<p>EP02</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Do not respond with anything other than the labels 'positive' or 'negative'.This is very important to my career.</p>
<p>Question: {content} Answer:</p>
<p>EP03</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Do not respond with anything other than the labels 'positive' or 'negative'.You'd better be sure.</p>
<p>Question: {content} Answer:</p>
<p>Zero-shot CoT</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.Let's think step by step.Then, end the response with "Therefore, the answer is: <label>'positive' / 'negative'</label>."</p>
<p>Question: {content} Answer:</p>
<p>Table 6: Detailed baseline prompts for SST2.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.Before the given sentence, label the sentiment of the user as positively or negative.</p>
<p>1-shot</p>
<p>For the closing sentence, label this rest of next sentence as positive or positive.</p>
<p>3-shot</p>
<p>For another given sentence, label the comment preceding no sentence as angry or negative.</p>
<p>EP02</p>
<p>Regarding ANY OTHER NEWS, label the sentiment of the sentence positive or negative.</p>
<p>EP03</p>
<p>If the given sentence, label the quality of the statement as positively or negative.</p>
<p>Zero-shot CoT For the introductory discussion, label the sentiment of the reviewer for positive or opposite.</p>
<p>CoLA</p>
<p>Original Does this sentence make sense?</p>
<p>Did THIS sentence make?</p>
<p>1-shot Does every word make Sense?</p>
<p>3-shot Does my sentence any sense?</p>
<p>EP02</p>
<p>Has this now make sense?</p>
<p>EP03</p>
<p>Made THIS sentence make it?</p>
<p>Zero-shot CoT Should THAT sentence made sense?</p>
<p>MNLI</p>
<p>Original</p>
<p>Please identify whether the premise entails the hypothesis.</p>
<p>Please indicate whether the content entails valid conclusion.</p>
<p>1-shot</p>
<p>Please identify all the elements entails the hypothesis.</p>
<p>3-shot</p>
<p>Please identify whether the premise is the facts.</p>
<p>EP02 must identify whether the premise opposes the hypothesis.</p>
<p>EP03</p>
<p>Results evaluate Whether your data entails the hypothesis.</p>
<p>Zero-shot CoT</p>
<p>Note whether the answer satisfies the evidence.</p>
<p>QNLI</p>
<p>Original</p>
<p>Please identify whether the sentence answers the question.</p>
<p>Please specify whether the above answers the question.</p>
<p>1-shot</p>
<p>Please comment whether sentence answers the question.</p>
<p>3-shot</p>
<p>We consider whether the sentence answers the question.</p>
<p>EP02</p>
<p>Please assess whether the above answered the error.</p>
<p>EP03</p>
<p>Please correct whether this answered the question.</p>
<p>Zero-shot CoT Please assess whether the sentence answers the queries.</p>
<p>RTE</p>
<p>Original</p>
<p>Please identify whether the premise entails the hypothesis.</p>
<p>First estimate whether premise entails each hypothesis.</p>
<p>1-shot Please confirm whether current knowledge advances the hypothesis.</p>
<p>3-shot Now identify whether the correction entails the hypothesis.</p>
<p>EP02</p>
<p>First identify whether the premise confirms each hypothesis.</p>
<p>EP03</p>
<p>Please state whether the conclusion justifies the hypothesis.</p>
<p>Zero-shot CoT Please identify whether certain premise entails correct hypothesis.Please identify since the posts have the same text.</p>
<p>MRPC</p>
<p>1-shot will identify Where unrelated sentences have nearly same significance.</p>
<p>3-shot</p>
<p>Please identify whether the sentences have the same ending.</p>
<p>EP02</p>
<p>Please determine are the have the same meaning.</p>
<p>EP03</p>
<p>Please identify both your works have the same subject.</p>
<p>Zero-shot CoT Please note whether adjacent sentences display the identical meaning.</p>
<p>SST2</p>
<p>Original</p>
<p>For the given sentence, label the sentiment of the sentence as positive or negative.</p>
<p>For the given sentence, label the sentiment of the sentence as positive or constructive.</p>
<p>1-shot</p>
<p>For the given sentence, keep positive sentiment of the sentence as positive not negative.</p>
<p>3-shot</p>
<p>Before the given sentence, express components of the sentence as positive OR negatively.</p>
<p>EP02</p>
<p>After the desired context, label every result of the process as happy or negative.</p>
<p>EP03</p>
<p>For my given question, label the sentiment of the sentence as positive or optimistic.</p>
<p>Zero-shot CoT</p>
<p>For the given expression, label the outcome of the sentence as positive or.</p>
<p>CoLA</p>
<p>Original Does this sentence make sense?Does this sentence form follows?</p>
<p>1-shot Does each word contain sense?</p>
<p>3-shot Does each sentence make points?EP02 Do I sentence make sense?EP03 Does each sentence make sense?</p>
<p>Zero-shot CoT Has this sentence make sense?</p>
<p>MNLI</p>
<p>Original</p>
<p>Please identify whether the premise entails the hypothesis.</p>
<p>Please assess how the result entails proposed hypothesis.</p>
<p>1-shot</p>
<p>Please assess whether sufficient premise entails the claim.</p>
<p>3-shot</p>
<p>Please identify between and premise the hypothesis.</p>
<p>EP02</p>
<p>Please show whether the answer entails the hypothesis.</p>
<p>EP03</p>
<p>Please identify whether the result entails the hypothesis.</p>
<p>Zero-shot CoT Please evaluate whether the hypothesis entails my observations.</p>
<p>QNLI</p>
<p>Original</p>
<p>Please identify whether the sentence answers the question.</p>
<p>Please identify whether any sentence asked either question.</p>
<p>1-shot</p>
<p>Please repeat in the affirmative regarding the question.</p>
<p>3-shot please identify unless our article answers further question.</p>
<p>EP02 must identify whether the article answers the question.</p>
<p>EP03</p>
<p>Please identify whether the sentence addressed previous question.</p>
<p>Zero-shot CoT Please identify whether her sentence supports the question.</p>
<p>RTE</p>
<p>Original</p>
<p>Please identify whether the premise entails the hypothesis.</p>
<p>Please find sure the premise matches any hypothesis.</p>
<p>1-shot</p>
<p>Please assess whether the premise supports the premises.</p>
<p>3-shot</p>
<p>Please identify HOW either premise fits any other.</p>
<p>EP02</p>
<p>Please verify either possible facts entails the claims.</p>
<p>EP03</p>
<p>Please verify when the claim matches the fact.</p>
<p>Zero-shot CoT</p>
<p>Please identify that The premise entails the hypothesis.Please identify whether following articles have the same keywords.</p>
<p>MRPC</p>
<p>1-shot Please compare whether these sentences match the exact meaning.</p>
<p>3-shot</p>
<p>Please identify whenever individual sentences have the equivalent content.</p>
<p>EP02</p>
<p>Please tell whether two quotes have the identical message.</p>
<p>EP03</p>
<p>Please identify whether the arguments have the same context.</p>
<p>Zero-shot CoT</p>
<p>Please identify between the sentences have the same value.</p>
<p>, LLMs do not require the addition and training of extra layers on top of the pre-trained base model to adapt to Please identify whether the sentences have the same meaning.Question: Sentence 1: How do you post a question on Quora?Sentence 2: How many topics can you add to a</p>
<p>Figure 2 :
2
Figure 2: The visualization of model performance on CoLA and MMLU-STEM validation set with neighborhood prompts.The task description of the original prompt picked for CoLA is "Does this sentence make sense?", and for MMLU-STEM is "The following are multiple choice questions (with answers) about {task}", where {task} is a placeholder to replace with detailed subset type, e.g., "abstract algebra".The point • in lighter color indicates better performance, and the square ■ indicates the original prompt, with the ▶ in the color bar indicating the original performance.The words in the upper prompts indicate the changed words, and words indicate the substitutions.</p>
<p>Figure 3 :
3
Figure 3: Impact of the number of words changed in prompt on downstream performance.</p>
<p>Figure 4 :
4
Figure 4: Impact of the number of sampled examples in proxy reference tasks on downstream performance.</p>
<p>Figure 5 :
5
Figure 5: Impact of the number of candidate words in search space on downstream performance.</p>
<p>Table 1 :
1
COPLE 92.43 0.70 65.72 1.29 52.42 1.33 69.38 2.10 68.59 4.08 68.17 0.53 57.11 1.80 31.46 0.54 27.90 1.77 37.09 0.79 46.20 0.40 COPLE 91.21 1.48 78.24 1.58 65.37 0.89 79.34 0.35 71.60 0.55 71.08 0.98 75.93 0.15 35.83 0.54 37.45 0.51 53.02 0.17 52.96 0.28 Performance comparison (Accuracy) of models on GLUE and MMLU benchmarks using the human-crafted prompts (Original) with and without applying COPLE.The bold values indicate the better results, while the standard deviations are provided in smaller font.For MNLI, we report the average results on the matched and mismatched subsets.Some results for gpt-3.5-turbo-0125are denoted as "\", indicating that, due to the huge validation set and cost and efficiency considerations, corresponding experiments are not conducted.COPLE 88.13 3.16 69.13 0.29 38.46 5.36 52.61 0.62 55.23 0.51 68.23 0.12 45.93 17.25 28.97 0.31 24.15 0.27 30.12 0.63 23.38 0.23 COPLE 70.07 1.62 68.36 0.38 31.170.19 55.78 0.22 57.76 0.26 68.59 0.28 57.61 1.22 30.22 0.88 26.38 0.11 29.82 0.21 25.45 0.33 COPLE 92.26 0.57 67.98 0.54 50.15 3.93 64.59 1.85 57.16 4.59 68.42 0.34 COPLE 92.78 0.65 68.41 1.02 52.23 0.03 67.40 3.60 62.09 3.06 68.63 0.35 42.34 6.23 33.02 1.32 27.12 0.14 36.65 2.31 43.80 0.20 COPLE 84.29 3.92 67.45 1.02 50.51 1.03 69.64 0.92 59.81 0.83 66.42 1.04 23.49 0.88 27.73 0.54 32.72 0.68 34.42 1.26 31.271.59 COPLE 91.80 0.41 72.42 0.77 60.72 1.46 71.94 0.18 65.16 2.81 65.07 0.52 64.96 0.16 31.150.44 31.98 0.73 44.31 0.95 46.90 0.28 COPLE 93.43 0.07 72.77 0.27 56.35 0.68 75.62 0.58 68.77 5.87 67.65 0.35 75.42 0.70 35.31 2.12 39.67 0.14 49.11 1.05 51.97 0.60 COPLE 89.11 1.78 75.36 0.41 64.36 0.82 72.571.54 72.56 2.04 70.10 0.69 72.43 0.98 37.07 0.44 37.64 0.82 52.08 0.30 51.97 1.COPLE 91.44 0.33 76.32 0.14 67.24 0.36 76.37 0.91 72.38 1.28 74.39 0.17 75.10 0.90 36.76 0.88 37.45 0.27 52.67 0.21 52.02 0.16 COPLE 90.90 0.26 76.27 1.02 67.41 0.68 79.45 0.45 71.48 0.63 74.14 0.52 76.80 3.13 36.34 0.18 36.87 0.33 52.08 0.21 52.54 0.20
GLUEMMLUSST2CoLAMNLIQNLIRTEMRPCQQPSTEM Humanities Soc.SciOtherLlama-2-7B-chatOriginal90.7133.2735.5151.5753.0768.0627.5828.0423.9435.3138.59w/ Mistral-7B-Instruct-v0.1Original87.2774.3165.1875.2264.9854.4168.9235.1937.0752.2351.83w/ ChatGPT (gpt-3.5-turbo-0125)Original94.3880.73\62.0941.91\34.1642.4758.1657.46w/ COPLE 94.80 0.17 82.91 0.16\80.35 0.34 70.75 0.37\36.45 0.76 43.44 0.39 58.46 0.59 57.61 0.14
, Mistral-7B-Instruct-v0.1 (Jiang et al., 2023), and ChatGPT (gpt-3.5-turbo-0125)(OpenAI,2022)as the target model.Please see Appendix A.1 for more details on datasets and models.Baseline.To show the effectiveness of COPLE and empirically demonstrate the conclusions in §3.2, we evaluate COPLE in the following scenarios: (i) Original: using human-crafted prompts from HELM (Lee et al., 2023) and lm-evaluationharness (Gao et al., 2023).(ii)In-contextLearning,followingBrown et al. (2020), randomly concatenating 1 and 3 examples from the training set with Original manual prompts (as the E ′ in P Z (X)), denoted as the 1-shot and 3-shot settings, respectively.(iii)EmotionPrompt:combiningtwo different self-monitoring style emotional stimuli, used in (Li et al., 2023) with Original manual prompts, denoted as EP02 and EP03, respectively.(iv)Chainof-thought:combiningzero-shotCoT trigger (Kojima et al., 2022) with Original manual prompts, denoted as Zero-shot-CoT.Please see Appendix A.3 for the detailed prompts used in evaluation.4.2 Main ResultsPopular Prompts Suffer From Lexical Sensitivity.Table1shows the model performance on different tasks using Original human-crafted prompts and related prompts optimized by COPLE.The ability of (i) problem-solving, as when the model gives wrong results, and (ii) instruct-following, as when the model gives irrelevant results.For example, for Llama-2-7B-chat on QQP, the 3-shot accuracy decreases from 27.58% to 23.03% (4.55%↓)</p>
<p>Table 2 :
2
Performance comparison (Accuracy) of models on GLUE and MMLU using different initial prompts with and without applying COPLE.The bold and smaller values denote better results and standard deviations.
compared to Original. However, without complexprompt engineering, minor lexical optimizationperformed by COPLE is enough to recover thedeclined ability, as the accuracy increases from23.03% to 57.61% after applying COPLE, whichalso outperforms the original prompt optimized byCOPLE (Table 1, 57.11%). Please see AppendixB.1 for the detailed optimized prompts.</p>
<p>Table 3 :
3
Difference of semantic similarity and perplexity between original prompts and optimized prompts.U.Sim denotes the similarity obtained through USE.
Llama-2-7B-chatMistral-7B-Instruct-v0.1PPL.ori U.Sim BERTScore PPL U.Sim BERTScore PPLSST2460.840.891630.850.9282CoLA670.790.7518750.810.87412MNLI6350.750.828050.780.881781GLUEQNLI2060.770.824870.780.85521RTE6350.740.7820260.780.83842MRPC580.780.871030.780.87326QQP1850.720.766560.780.86306STEM0.760.885150.790.88408MMLUHumanities Soc.Sci2760.79 0.800.89 0.91529 4940.79 0.800.92 0.94491 479Other0.780.846630.790.87525Avg.2670.770.847560.790.88561</p>
<p>Table 4 :
4
Ablation results on the search method.Random denotes searching on a random order of words.
cantly improves (CoLA: 33.27% to 46.50% ±1.23% ,MMLU-Other: 38.59% to 44.04% ±1.07% ). Whenthe #.Word Change increases, COPLE tends toachieve higher accuracy, while a moderate value isenough to achieve nearly the optimal result.#.Sampled Data for Proxy Tasks (|Z ref |). Fig-ure 4 shows the impact of the size of proxy tasks.When proxy tasks contain a small number of 20examples, COPLE still achieves notable improve-ments (CoLA: 33.27% to 62.96% ±1.78% ; MMLU-Other: 38.59% to 41.13% ±1.69% ). However, alarger size of sampled data helps find prompts withhigher accuracy and lower standard deviations.#.Candidate Word in Search Space (k). Fig-ure 5 shows the impact of the number of candidatewords in search space. A small k is enough tosupport COPLE achieving considerable improve-ment (k=5, CoLA: 33.27% to 57.65 ±0.62% , MMLU-Other: 38.59% to 44.79 ±0.
75% ).Therefore, for efficient purposes, a small k is more suitable.However, using larger search space for each word is more likely to find prompts with better performance.Figure 6: Performance difference when COPLE is performed on proxy reference tasks and on validation set.</p>
<p>Table 5 :
5
The Massive Multitask Language Understanding (MMLU) dataset(Hendrycks et al., 2021)contains multiple-choice questions that cover 57 tasks, which can be divided into four main subsets: STEM, Humanities, Social Sciences, and Other, with 14,042 test and 1,531 validation examples.In our experiment, for constructing the proxy reference tasks, we sample from the test set on MMLU.More information about the datasets is provided in Table5.Summary of datasets used in the experiments.For MNLI, 19,647 validation examples consists of 9,815 from the matched in-domain section and 9,832 from the mismatched cross-domain section.For MMLU, we report the size of test set rather than training set.
Dataset #.Training example #.Validation exampleMission Type#.CategorySST267 349872Sentiment Analysis2CoLA85511043Linguistic Acceptability2MNLI392 70219 647Natural Language Inference3QNLI104 7435463Natural Language Inference2RTE2490277Natural Language Inference2MRPC3668408Semantic Equivalence2QQP363 84940 430Semantic Equivalence2MMLU14042 (Test)1531Question Answering4</p>
<p>Table 7 :
7
Detailed baseline prompts for CoLA.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioPrompt</p>
<p>Table 8 :
8
Detailed baseline prompts for MNLI.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioPrompt</p>
<p>Table 9 :
9
Detailed baseline prompts for QNLI.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioPrompt</p>
<p>Table 10 :
10
Detailed baseline prompts for RTE.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioPrompt</p>
<p>Table 11 :
11
Detailed baseline prompts for MRPC.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioPrompt</p>
<p>Table 12 :
12
Detailed baseline prompts for QQP.In the prompt, brown text indicates the task description, which is the target that COPLE performs on, green text indicates the verbalizer, and blue text indicates the demo examples for few-shot learning."{text}" denotes the placeholder, which will be replaced with the text sampled from the dataset.
DatasetScenarioTask Description (Before)Task Description (After)OriginalFor the given sentence,SST2label the sentiment of the sentence aspositive or negative.</p>
<p>Table 15 :
15
Detailed optimized task descriptions on Llama2-7B-chat and GLUE.
DatasetScenarioTask Description (Before)Task Description (After)</p>
<p>Table 17 :
17
Detailed optimized task descriptions on Mistral-7B-Instruct-v0.1 and GLUE.</p>
<p>AcknowledgementsThis work was supported by the Youth Innovation Promotion Association CAS (No.2023166), and the Informatization Plan of Chinese Academy of Sciences (Grant No. CAS-WX2021SF-0508).Dataset Scenario PromptMMLU OriginalThe following are multiple choice questions (with answers) about {task}.Do not respond with anything other than the answer labels 'A', 'B', 'C', or 'D'.3-shotThe following are multiple choice questions (with answers) about {task}.Do not respond with anything other than the answer labels 'A', 'B', 'C', or 'D'.B Additional Experimental ResultsB.1 Details on the Optimized Prompt Crafted by COPLETable15-16 shows the optimal task descriptions optimized by COPLE on Llama2-7B-chat for various scenarios.Table17-18 shows the optimal task descriptions optimized by COPLE on Mistral-7B-Instruct-v0.1 for various scenarios.Table19shows the optimal task description optimized by COPLE on ChatGPT (gpt-3.5-turbo-0125)and the Original scenario.Dataset Scenario Task Description (Before)Task Description (After)MMLU-STEM OriginalThe following are multiple choice questions (with answers) about {task}.THE exercises contain easy choice questions (with answers) about {task}.1-shotThe following generates multiple discussion questions (using arrows) about {task}.3-shotSe following are multiple popular questions (with answers) about {task}.EP02The examples list multiple choice questions (with labels) without {task}.EP03The are infinite choice questions (by data) about {task}.Zero-shot CoTThe following are multiple choice problems (default answers) involving {task}.MMLU-HumanitiesOriginalThe model identifies multiple popular questions (which answers)1-shotIn following are all survey questions (with answers) about {task}.3-shotThe following are multiple questions (with answers) named {task}.EP02Graph following illustrates third choice answer (simple answers) about {task}.EP03The candidates are multiple choice questions (with) about {task}.Zero-shot CoTThe follows are most popular questions (mostly answers) about {task}.MMLU-Social SciencesOriginalThe following displays multiple choice questions (with answers) about {task}.1-shotThe following multiple popular questions (with answers) about {task}.3-shotThe following are multiple different questions (with answers) about {task}.EP02The following summarizes the choice questions (complete ones) about {task}.EP03The entries present multiple choice questions (with answers) involving {task}.Zero-shot CoT following are some choice questions (complete answers) about {task}.MMLU-OtherOriginalThe candidates posted multiple choice questions (with answers) about {task}.1-shotThe following are choice questions (with answers) about {task}.3-shotThe following are multiple standard answers (with answers) about {task}.EP02The Following are the choice questions (and answers) following {task}.EP03The following implements multiple choice questions (No exceptions) about {task}.Zero-shot CoTThe following contains your first questions (with explanation) about {task}.MMLU-STEM OriginalThe following are multiple choice questions (with answers) about {task}.The following are multiple choice questions (their variants) about {task}.1-shot the following two first choice questions (with answers) about {task}.3-shotExamples Here were multiple detailed questions (full comments) about {task}.EP02The mes are multiple choice questions (with parentheses) about {task}.EP03Examples following are multiple example questions (complete answers) about {task}.Zero-shot CoT Then following are binary logic questions (with tags) about {task}.MMLU-HumanitiesOriginalThe are three choice cases (with answers) about {task}.The Following are multiple choice questions (with hints) about {task}.3-shotThen following are multiple choice questions (with answers) about {task}.EP02The following are multiple obvious choices (easy fixes) using {task}.EP03The Following are multiple choice messages (with answers) about {task}.Zero-shot CoTThe Below are multiple hypothetical questions (with answers) about {task}.MMLU-Social SciencesOriginalThe following are choice questions (with answers) about {task}.1-shotThe following are ten sample questions (with keywords) about {task}.3-shotThe following are two sample questions (with answers) about {task}.EP02The Following are multiple choice questions (with solutions) containing {task}.EP03The Following are one choice questions (with answers) about {task} Zero-shot CoT The followed five multiple choice Questions (with Answers) about {task}.MMLU-OtherOriginalThe following are multiple choice questions (answers) and {task}.1-shotThen follow are two choice questions (with answers) about {task}.3-shotBoth following are multiple boolean questions (with answers) about {task}.EP02The answers are Multiple choice questions (with answers) and {task}.EP03The following are multiple nested questions (with answers) about {task}.Zero-shot CoTCh following is multiple choice questions (with answers) about {task}.Table18: Detailed optimized task descriptions on Mistral-7B-Instruct-v0.1 and MMLU."{task}" denotes the placeholder, which will be replaced with the detailed subset type.DatasetTask Description (Before)Task Description (After)SST2For the given sentence, label the sentiment of the sentence as positive or negative.For the given article, label the sentiment above the sentence as positive or negative.CoLA Does this sentence make sense?this sentence make sense?RTE Please identify whether the premise entails the hypothesis.Please evidence the premise support current hypothesis.MRPC Do both sentences mean the same thing?Do both answers mean this correct thing?MMLU-STEMThe following are multiple choice questions (with answers) about {task}.The following are infinite choice questions (NO answers) within {task}.MMLU-HumanitiesItems above are a choice (with answers) about {task}.MMLU-Social SciencesThe following answers multiple choice question (with answers) about {task}.MMLU-OtherThe Following answers multiple choice questions (with answers) about {task}.Table19: Detailed optimized task descriptions on ChatGPT (gpt-3.5-turbo-0125) on the Original scenario."{task}" denotes the placeholder, which will be replaced with the detailed subset type.
Detection errors on the word the: Evidence for the acquisition of reading levels. Alice F Healy, Adam Drewnowski, 10.3758/bf03197472Memory &amp; Cognition. 51978</p>
<p>The second pascal recognising textual entailment challenge. Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, Idan Szpektor, Proceedings of the second PASCAL challenges workshop on recognising textual entailment. the second PASCAL challenges workshop on recognising textual entailmentVenice20066</p>
<p>The fifth pascal recognizing textual entailment challenge. Luisa Bentivogli, Peter Clark, Ido Dagan, Danilo Giampiccolo, Tac2009</p>
<p>Integer and combinatorial optimization (george l. nemhauser and laurence a. wolsey). Charles E Blair, 10.1137/1032061SIAM Rev. 3221990</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish; NeurIPS2020Language models are few-shot learners</p>
<p>Towards evaluating the robustness of neural networks. Nicholas Carlini, David Wagner, 2017 IEEE Symposium on Security and Privacy (SP). Ieee2017</p>
<p>. Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Céspedes, Steve Yuan, Chris Tar, abs/1803.11175Universal sentence encoder. arXiv preprint. 2018</p>
<p>Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation. M Daniel, Mona T Cer, Eneko Diab, I~nigo Agirre, Lucia Lopez-Gazpio, Specia, CoRR, abs/1708.000552017</p>
<p>The pascal recognising textual entailment challenge. Ido Dagan, Oren Glickman, Bernardo Magnini, Machine Learning Challenges Workshop. Springer2005</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2019</p>
<p>Automatically constructing a corpus of sentential paraphrases. William B Dolan, Chris Brockett, Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP. the Third International Workshop on Paraphrasing, IWP@IJCNLP2005</p>
<p>Unified language model pre-training for natural language understanding and generation. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems. NeurIPS2019</p>
<p>Pathologies of neural models make interpretations difficult. Eric Shi Feng, Alvin Wallace, I I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, 10.18653/v1/D18-1407Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Black-box generation of adversarial text sequences to evade deep learning classifiers. Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi, 10.1109/spw.2018.00016Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS2018. 2018 2022Training language models to follow instructions with human feedback</p>
<p>The limitations of deep learning in adversarial settings. Nicolas Papernot, Patrick Mcdaniel, Somesh Jha, Matt Fredrikson, Ananthram Berkay Celik, Swami, 2016 IEEE European Symposium on Security and Privacy (EuroS&amp;P). IEEE2016</p>
<p>Grips: Gradient-free, edit-based instruction search for prompting large language models. Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal, 10.18653/v1/2023.eacl-main.277Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterEACL2023</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, OpenAI Blog. 2018a</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Ope-nAI Blog. 2018b</p>
<p>SQuAD: 100, 000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/d16-1264Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLP2016</p>
<p>Raeding wrods with jubmled lettres: There is a cost. Keith Rayner, Sarah J White, Rebecca L Johnson, Simon P Liversedge, 10.1111/j.1467-9280.2006.01684.xPsychological Science. 173165070572006</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, 10.18653/v1/2020.emnlp-main.346Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLP2020</p>
<p>Recursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, Christopher Potts, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingEMNLP2013</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, 10.48550/arxiv.2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurélien Rodriguezand Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288</p>
<p>Visualizing data using t-SNE. Laurens Van Der Maaten, Geoffrey Hinton, Journal of Machine Learning Research. 8692008</p>
<p>A ROWS is a ROSE: Spelling, sound, and reading. C Guy, Van Orden, 10.3758/bf03197716Memory &amp; Cognition. 3151987</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 7th International Conference on Learning Representations. ICLR2019</p>
<p>Neural network acceptability judgments. Alex Warstadt, Amanpreet Singh, R Samuel, 10.1162/tacl_a_00290Trans. Assoc. Comput. Linguistics. 72019Bowman</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. NeurIPS2022. 2022</p>
<p>Larger language models do in-context learning differently. Jerry W Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, Tengyu Ma, 10.48550/arxiv.2303.03846CoRR, abs/2303.038462023</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R , 10.18653/v1/n18-1101Proceedings of the 2018 Conference of the North American Chapter. the 2018 Conference of the North American ChapterNAACL-HLT2018Bowman</p>
<p>Expertprompting: Instructing large language models to be distinguished experts. Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao, 10.48550/arxiv.2305.14688CoRR, abs/2305.146882023</p>
<p>Large language models as optimizers. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, Xinyun Chen, 10.48550/arxiv.2309.03409CoRR, abs/2309.034092023</p>
<p>. Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, Maosong Sun, 2020</p>
<p>Word-level textual adversarial attacking as combinatorial optimization. 10.18653/v1/2020.acl-main.540Proceedings of the 58th Annual Meeting of the. the 58th Annual Meeting of theAssociation for Computational Linguistics</p>
<p>Mitigating the inconsistency between word saliency and model confidence with pathological contrastive training. Pengwei Zhan, Yang Wu, Shaolei Zhou, Yunjian Zhang, Liming Wang, 10.18653/v1/2022.findings-acl.175Findings of the Association for Computational Linguistics: ACL. 2022a</p>
<p>Contrastive learning with adversarial examples for alleviating pathology of language model. Jing Pengwei Zhan, Xiao Yang, Chunlei Huang, Jingying Jing, Liming Li, Wang, 10.18653/v1/2023.acl-long.358Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023a1</p>
<p>Similarizing the influence of words with contrastive learning to defend word-level adversarial text attack. Jing Pengwei Zhan, He Yang, Chao Wang, Xiao Zheng, Liming Huang, Wang, 10.18653/v1/2023.findings-acl.500Findings of the Association for Computational Linguistics: ACL. 2023b</p>
<p>Rethinking word-level adversarial attack: The trade-off between efficiency, effectiveness, and imperceptibility. Jing Pengwei Zhan, He Yang, Chao Wang, Liming Zheng, Wang, Proceedings of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING). the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)2024</p>
<p>PARSE: an efficient search method for black-box adversarial text attacks. Pengwei Zhan, Chao Zheng, Jing Yang, Yuxiang Wang, Liming Wang, Yang Wu, Yunjian Zhang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsCOLING2022b</p>
<p>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, Joseph E Gonzalez, 10.48550/arxiv.2211.11890CoRR, abs/2211.11890TEM-PERA: test-time prompting via reinforcement learning. 2022</p>
<p>BERTScore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. 20202020</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, Ed H Chi, The Eleventh International Conference on Learning Representations. ICLR2023a</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, The Eleventh International Conference on Learning Representations. ICLR2023b</p>            </div>
        </div>

    </div>
</body>
</html>
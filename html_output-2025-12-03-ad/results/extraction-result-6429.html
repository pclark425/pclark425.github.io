<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276928466</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.07832v1.pdf" target="_blank">RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code</a></p>
                <p><strong>Paper Abstract:</strong> Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains. To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted multi-file refactoring tasks in popular open-source repositories. Solving tasks within RefactorBench requires thorough exploration of dependencies across multiple files and strong adherence to relevant instructions. Every task is defined by 3 natural language instructions of varying specificity and is mutually exclusive, allowing for the creation of longer combined tasks on the same repository. Baselines on RefactorBench reveal that current LM agents struggle with simple compositional tasks, solving only 22% of tasks with base instructions, in contrast to a human developer with short time constraints solving 87%. Through trajectory analysis, we identify various unique failure modes of LM agents, and further explore the failure mode of tracking past actions. By adapting a baseline agent to condition on representations of state, we achieve a 43.9% improvement in solving RefactorBench tasks. We further extend our state-aware approach to encompass entire digital environments and outline potential directions for future research. RefactorBench aims to support the study of LM agents by providing a set of real-world, multi-hop tasks within the realm of code.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>State-aware SWE-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>State-aware SWE-agent (this work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modified SWE-agent that conditions the core LM on externally computed natural-language state summaries of prior edits (σ) produced by a state update policy (π_state), appended to the agent trajectory before each function call to improve multi-file refactoring and stateful reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>State-aware SWE-agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SWE-agent augmented with an external state cache: an externally computed state variable σ (a succinct natural-language summary of prior edits and divergence from initialization) is updated each step by a state update policy π_state:(τ_N, σ_{N-1},...)→σ_N, cached, and prepended/appended to the LM prompt so the model conditions on the summarized state when generating the next action.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External state cache / state update policy (natural-language summaries of recent edits and diffs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Natural-language summaries of prior edits (recent_edits JSON including working_dir, open_file, and filtered recent_edits), succinct divergence from initialization represented as σ; essentially text-based episodic edit-history summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Read by appending the computed σ to the agent trajectory/prompt before each function call (prompt conditioning); written/updated by an external state update policy π_state that computes σ_N from the trajectory and prior σ (e.g., parsing edits, diffs, filtered edit lists, cached recent_edits).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RefactorBench (multi-file code refactoring benchmark introduced in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Code generation / multi-file refactoring / stateful multi-step planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as an average 43.9% relative increase in overall task success across instruction sets compared to the baseline SWE-agent; additionally an average 71% increase in subtask (AST test) completion across instruction sets (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline SWE-agent resolution rates on RefactorBench (gpt-4): Lazy 12.0%, Base 22.0%, Descriptive 27.0% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Resolution rate (task success = passing all AST tests for a task); subtask completion rate (number/percentage of AST subtests passed).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors report design/trade-offs qualitatively: the state-aware approach provides succinct state summaries to mitigate context flooding and reduces the need for the LM to reconstruct past actions, but may cause deviations from typical sequential agent reasoning and requires an external computation step (π_state) which adds engineering complexity; scalability and interaction with conflicting concurrent edits were not fully evaluated. No quantitative latency or memory-footprint numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Noted limitations: state-updates are expressed as natural-language summaries and may not capture all concurrent/conflicting external edits; authors did not evaluate modeling conflicting objectives at scale because RefactorBench tasks are mutually exclusive; potential representational and scalability limits when generalizing to other languages/environments; not a panacea for issues arising from temporary error states or in-edit linting constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gautam, D., Garg, S., Jang, J., Sundaresan, N., Zilouchian Moghaddam, R., 2025. REFACTORBENCH: EVALUATING STATEFUL REASONING IN LANGUAGE AGENTS THROUGH CODE. arXiv:2503.07832.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWE-agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWE-agent (prompt-adjusted baseline used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LM agent framework used as the main baseline in this work (prompt-adjusted to focus on refactors); in baseline form it conditions only on a limited recent context (truncated Count(ω)) and does not include the external state summaries introduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Swe-agent: Agent-computer interfaces enable automated software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SWE-agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A POMDP-style language-model agent framework that issues environment function calls and receives observations; baseline runs use prompt-adjusted SWE-agent with gpt-4/gpt-4o and a constrained window of past observations (i.e., limited Count(ω)) rather than an explicit external state cache.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>None (relies on the LM's context window and truncated recent observations rather than an external state cache)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>N/A for external memory; agent relies on LM in-context window (limited number of prior timesteps) and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RefactorBench</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Code generation / multi-file refactoring</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Reported resolution rates (SWE-agent with gpt-4): Lazy 12.0%, Base 22.0%, Descriptive 27.0%. Updated baseline (SWE-agent 1.0 with gpt-4o) reported later: Base 21%, Descriptive 31% (paper's updated baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Resolution rate (task success = passing all AST tests); subtask completion rate.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Baseline behavior prone to context flooding in long trajectories; truncating Count(ω) avoids token-cost and context-window issues but leads to degradation of state understanding as the number of actions increases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Baseline agents overfit to bug-fix behaviors, struggle to track past actions leading to failures in locating correct edit locations, and exhibit degraded end-state construction accuracy as action trajectory length increases; many failures attributed to loss of state over long trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Yao, S., Narasimhan, K., 2024. Swe-agent: Agent-computer interfaces enable automated software engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llm-state</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llm-state: Open world state representation for longhorizon task planning with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced prior work that studies open-world state representations for long-horizon planning with LLMs; mentioned as related work on state representations for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llm-state: Open world state representation for longhorizon task planning with large language model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Llm-state (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Described in this paper as prior work on LLM-based state representations for long-horizon planning; the current paper cites it as motivation for designing state representations for agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>State representation for long-horizon planning (details not specified in this paper; see original work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Referenced in context of long-horizon planning tasks (not evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Planning / long-horizon reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Chen, S., Xiao, A., Hsu, D., 2024. Llm-state: Open world state representation for longhorizon task planning with large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-ended memory-augmented agents (Sarch et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-ended instructable embodied agents with memory-augmented large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced related work that explores memory-augmented LLMs for embodied agent tasks; cited to contextualize external-memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Open-ended instructable embodied agents with memory-augmented large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-augmented embodied agents (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior work that augments LLM-driven agents with external memory mechanisms for embodied control; cited as related literature motivating external/augmented memory for longer-term skills.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Memory-augmented LLMs (generic external memory mechanisms; specific details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Embodied control / long-horizon tasks (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Embodied agents / long-horizon planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Sarch, G., Wu, Y., Tarr, M. J., Fragkiadaki, K., 2023. Open-ended instructable embodied agents with memory-augmented large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Open-ended instructable embodied agents with memory-augmented large language models <em>(Rating: 2)</em></li>
                <li>Llm-state: Open world state representation for longhorizon task planning with large language model <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 1)</em></li>
                <li>Jarvis-1: Openworld multi-task agents with memory-augmented multimodal language models <em>(Rating: 1)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6429",
    "paper_id": "paper-276928466",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "State-aware SWE-agent",
            "name_full": "State-aware SWE-agent (this work)",
            "brief_description": "A modified SWE-agent that conditions the core LM on externally computed natural-language state summaries of prior edits (σ) produced by a state update policy (π_state), appended to the agent trajectory before each function call to improve multi-file refactoring and stateful reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "State-aware SWE-agent",
            "agent_description": "SWE-agent augmented with an external state cache: an externally computed state variable σ (a succinct natural-language summary of prior edits and divergence from initialization) is updated each step by a state update policy π_state:(τ_N, σ_{N-1},...)→σ_N, cached, and prepended/appended to the LM prompt so the model conditions on the summarized state when generating the next action.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External state cache / state update policy (natural-language summaries of recent edits and diffs)",
            "memory_representation": "Natural-language summaries of prior edits (recent_edits JSON including working_dir, open_file, and filtered recent_edits), succinct divergence from initialization represented as σ; essentially text-based episodic edit-history summaries.",
            "memory_access_mechanism": "Read by appending the computed σ to the agent trajectory/prompt before each function call (prompt conditioning); written/updated by an external state update policy π_state that computes σ_N from the trajectory and prior σ (e.g., parsing edits, diffs, filtered edit lists, cached recent_edits).",
            "task_name": "RefactorBench (multi-file code refactoring benchmark introduced in this paper)",
            "task_category": "Code generation / multi-file refactoring / stateful multi-step planning",
            "performance_with_memory": "Reported as an average 43.9% relative increase in overall task success across instruction sets compared to the baseline SWE-agent; additionally an average 71% increase in subtask (AST test) completion across instruction sets (as reported in the paper).",
            "performance_without_memory": "Baseline SWE-agent resolution rates on RefactorBench (gpt-4): Lazy 12.0%, Base 22.0%, Descriptive 27.0% (Table 2).",
            "has_comparative_results": true,
            "performance_metric": "Resolution rate (task success = passing all AST tests for a task); subtask completion rate (number/percentage of AST subtests passed).",
            "tradeoffs_reported": "Authors report design/trade-offs qualitatively: the state-aware approach provides succinct state summaries to mitigate context flooding and reduces the need for the LM to reconstruct past actions, but may cause deviations from typical sequential agent reasoning and requires an external computation step (π_state) which adds engineering complexity; scalability and interaction with conflicting concurrent edits were not fully evaluated. No quantitative latency or memory-footprint numbers provided.",
            "limitations_or_failure_cases": "Noted limitations: state-updates are expressed as natural-language summaries and may not capture all concurrent/conflicting external edits; authors did not evaluate modeling conflicting objectives at scale because RefactorBench tasks are mutually exclusive; potential representational and scalability limits when generalizing to other languages/environments; not a panacea for issues arising from temporary error states or in-edit linting constraints.",
            "citation": "Gautam, D., Garg, S., Jang, J., Sundaresan, N., Zilouchian Moghaddam, R., 2025. REFACTORBENCH: EVALUATING STATEFUL REASONING IN LANGUAGE AGENTS THROUGH CODE. arXiv:2503.07832.",
            "uuid": "e6429.0",
            "source_info": {
                "paper_title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SWE-agent (baseline)",
            "name_full": "SWE-agent (prompt-adjusted baseline used in experiments)",
            "brief_description": "An open-source LM agent framework used as the main baseline in this work (prompt-adjusted to focus on refactors); in baseline form it conditions only on a limited recent context (truncated Count(ω)) and does not include the external state summaries introduced in this paper.",
            "citation_title": "Swe-agent: Agent-computer interfaces enable automated software engineering",
            "mention_or_use": "use",
            "agent_name": "SWE-agent (baseline)",
            "agent_description": "A POMDP-style language-model agent framework that issues environment function calls and receives observations; baseline runs use prompt-adjusted SWE-agent with gpt-4/gpt-4o and a constrained window of past observations (i.e., limited Count(ω)) rather than an explicit external state cache.",
            "model_size": null,
            "memory_used": false,
            "memory_type": "None (relies on the LM's context window and truncated recent observations rather than an external state cache)",
            "memory_representation": null,
            "memory_access_mechanism": "N/A for external memory; agent relies on LM in-context window (limited number of prior timesteps) and prompt engineering.",
            "task_name": "RefactorBench",
            "task_category": "Code generation / multi-file refactoring",
            "performance_with_memory": null,
            "performance_without_memory": "Reported resolution rates (SWE-agent with gpt-4): Lazy 12.0%, Base 22.0%, Descriptive 27.0%. Updated baseline (SWE-agent 1.0 with gpt-4o) reported later: Base 21%, Descriptive 31% (paper's updated baselines).",
            "has_comparative_results": true,
            "performance_metric": "Resolution rate (task success = passing all AST tests); subtask completion rate.",
            "tradeoffs_reported": "Baseline behavior prone to context flooding in long trajectories; truncating Count(ω) avoids token-cost and context-window issues but leads to degradation of state understanding as the number of actions increases.",
            "limitations_or_failure_cases": "Baseline agents overfit to bug-fix behaviors, struggle to track past actions leading to failures in locating correct edit locations, and exhibit degraded end-state construction accuracy as action trajectory length increases; many failures attributed to loss of state over long trajectories.",
            "citation": "Yao, S., Narasimhan, K., 2024. Swe-agent: Agent-computer interfaces enable automated software engineering.",
            "uuid": "e6429.1",
            "source_info": {
                "paper_title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Llm-state",
            "name_full": "Llm-state: Open world state representation for longhorizon task planning with large language model",
            "brief_description": "A referenced prior work that studies open-world state representations for long-horizon planning with LLMs; mentioned as related work on state representations for planning.",
            "citation_title": "Llm-state: Open world state representation for longhorizon task planning with large language model",
            "mention_or_use": "mention",
            "agent_name": "Llm-state (referenced work)",
            "agent_description": "Described in this paper as prior work on LLM-based state representations for long-horizon planning; the current paper cites it as motivation for designing state representations for agents.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "State representation for long-horizon planning (details not specified in this paper; see original work)",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "Referenced in context of long-horizon planning tasks (not evaluated in this paper)",
            "task_category": "Planning / long-horizon reasoning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Chen, S., Xiao, A., Hsu, D., 2024. Llm-state: Open world state representation for longhorizon task planning with large language model.",
            "uuid": "e6429.2",
            "source_info": {
                "paper_title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Open-ended memory-augmented agents (Sarch et al.)",
            "name_full": "Open-ended instructable embodied agents with memory-augmented large language models",
            "brief_description": "A referenced related work that explores memory-augmented LLMs for embodied agent tasks; cited to contextualize external-memory approaches.",
            "citation_title": "Open-ended instructable embodied agents with memory-augmented large language models",
            "mention_or_use": "mention",
            "agent_name": "Memory-augmented embodied agents (referenced)",
            "agent_description": "Prior work that augments LLM-driven agents with external memory mechanisms for embodied control; cited as related literature motivating external/augmented memory for longer-term skills.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Memory-augmented LLMs (generic external memory mechanisms; specific details not provided in this paper)",
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "Embodied control / long-horizon tasks (referenced)",
            "task_category": "Embodied agents / long-horizon planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Sarch, G., Wu, Y., Tarr, M. J., Fragkiadaki, K., 2023. Open-ended instructable embodied agents with memory-augmented large language models.",
            "uuid": "e6429.3",
            "source_info": {
                "paper_title": "RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Open-ended instructable embodied agents with memory-augmented large language models",
            "rating": 2,
            "sanitized_title": "openended_instructable_embodied_agents_with_memoryaugmented_large_language_models"
        },
        {
            "paper_title": "Llm-state: Open world state representation for longhorizon task planning with large language model",
            "rating": 2,
            "sanitized_title": "llmstate_open_world_state_representation_for_longhorizon_task_planning_with_large_language_model"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 1,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Jarvis-1: Openworld multi-task agents with memory-augmented multimodal language models",
            "rating": 1,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 1,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        }
    ],
    "cost": 0.0142735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>REFACTORBENCH: EVALUATING STATEFUL REASON-ING IN LANGUAGE AGENTS THROUGH CODE
10 Mar 2025</p>
<p>Dhruv Gautam dhruvgautam@berkeley.edu 
Spandan Garg 
Jinu Jang 
Neel Sundaresan 
Roshanak Zilouchian Moghaddam 
U C Berkeley 
Microsoft 
REFACTORBENCH: EVALUATING STATEFUL REASON-ING IN LANGUAGE AGENTS THROUGH CODE
10 Mar 2025F7B06C72BDD18449F397B962549E7DD1arXiv:2503.07832v1[cs.AI]
Recent advances in language model (LM) agents and function calling have enabled autonomous, feedback-driven systems to solve problems across various digital domains.To better understand the unique limitations of LM agents, we introduce RefactorBench, a benchmark consisting of 100 large handcrafted multi-file refactoring tasks in popular open-source repositories.Solving tasks within Refac-torBench requires thorough exploration of dependencies across multiple files and strong adherence to relevant instructions.Every task is defined by 3 natural language instructions of varying specificity and is mutually exclusive, allowing for the creation of longer combined tasks on the same repository.Baselines on Refac-torBench reveal that current LM agents struggle with simple compositional tasks, solving only 22% of tasks with base instructions, in contrast to a human developer with short time constraints solving 87%.Through trajectory analysis, we identify various unique failure modes of LM agents, and further explore the failure mode of tracking past actions.By adapting a baseline agent to condition on representations of state, we achieve a 43.9% improvement in solving RefactorBench tasks.We further extend our state-aware approach to encompass entire digital environments and outline potential directions for future research.RefactorBench aims to support the study of LM agents by providing a set of real-world, multi-hop tasks within the realm of code. 1</p>
<p>INTRODUCTION</p>
<p>"Repetition is the root of all software evil" -Martin Fowler Large language models (LLMs) have been quickly acquiring new capabilities (Bubeck et al., 2023), leading towards adoption of AI-powered systems in various formats and domains.The increasing usage of LLM powered tools like Github Copilot have greatly improved the capability of developers in software development tasks (Peng et al., 2023).More recently, an emphasis on multi-step execution through LLM feedback loops has unlocked the ability to solve harder problems within a variety of fields (Reed et al., 2022;Sumers et al., 2024;Yao &amp; Narasimhan, 2023), including parts of software engineering.This new paradigm of solving larger software tasks has led to the construction of a variety of new automated software engineering (ASE) systems, most being structured as LM agents (Wang et al., 2024c;Cognition.ai, 2024;AWS Q Developer, 2024;Gauthier, 2024;Aide.dev, 2024;Örwall, 2024;Yang et al., 2024;Tufano et al., 2024;Wang et al., 2024d;Chen et al., 2024a;Zhang et al., 2024b;Arora et al., 2024;Xia et al., 2024;Zhang et al., 2024a).Evaluations for such systems are currently largely comprised from real world data on Github (Jimenez et al., 2024;LaBash et al., 2024).While being the strongest open-source signal for software engineering tasks at scale, Github is inherently noisy through its snapshot nature, also requiring strong filtration and validation testing for reliable evaluations (Chowdhury et al., 2024;Bowman &amp; Dahl, 2021).We find that the filtration causes skewed task styles, creating a necessity for new data to diversify coding benchmarks.To address these challenges, we present RefactorBench, a benchmark designed to evaluate the largely undocumented task of multi-file code refactoring in large codebases.Unlike isolated function-level edits, multi-file refactoring requires comprehensive reasoning and composition of multiple smaller changes.Our benchmark, RefactorBench, also allows for controlled analysis into instruction-following through multiple instruction sets with specified and unspecified objectives.As LLMs have been extremely proficient in function level editing over model generations (Jiang et al., 2024), we find it important to evaluate the abilities of general LM agents given that they can reliably perform core subtasks, which we verify in RefactorBench's thorough filtering process.With unique abstract syntax tree (AST) based unit testing, the evaluation suite checks for a comprehensive variety of subtasks necessitated by the core refactor without dependence on exact line match.</p>
<p>Through evaluations of a baseline agent on RefactorBench, we find overfitting and poor performance, solving a maximum of 35% of tasks with our easiest instruction set.We also see a variety of unique failure modes, many centering around LM agents struggling to track and reason about previous actions.Similarly, extensive work in policy learning has commonly faced issues in long horizon execution (Piterbarg et al., 2023;Chen et al., 2024b;Hejna et al., 2023).By editing agent interfaces, we explore introducing conditioning over state updates, a common tactic in neural agent design, to our real world language agents and see 71% increases in subtask completion rates.</p>
<p>Overall, our contributions in this work are threefold:</p>
<ol>
<li>We introduce RefactorBench, a benchmark of code refactoring tasks that necessitate edits in multiple files and reasoning based on previous actions taken.2. We evaluate multiple open-source systems on RefactorBench and analyze three novel failure modes isolated through differing baseline runs.3. We construct state-aware interfaces and show improvement in the reasoning capabilities of a modified baseline agent.</li>
</ol>
<p>BACKGROUND</p>
<p>RELATED WORK</p>
<p>Benchmarks SWE-bench, a benchmark consisting of GitHub issues, is the community standard for evaluating open-ended problem-solving in code environments (Jimenez et al., 2024).Our work, comparably, focuses on handcrafted and underrepresented multi-file refactoring tasks, isolating unique language agent behaviors.Unlike existing function-level code benchmarks (Chen et al., 2021;Austin et al., 2021), which also include refactors, we concentrate on the challenges posed by multi-file edits.Through a lazy and descriptive instruction set to accompany base instructions, we also build on previous works that scale evaluations of LLMs' instruction-following capabilities with lazier instructions (Cassano et al., 2024).Recent works have also focused on evaluating repository-level code completion systems (Liu et al., 2024a;Agarwal et al., 2024;Bairi et al., 2023), but our work differs by evaluating larger actions than exact line match, using generalist evaluations through AST testing.New works have also started to benchmark LLMs across the life cycles of various engineering tasks (Li et al., 2024;Huang et al., 2024;Xie et al., 2024c), and recent LM agent benchmarks have also started to evaluate for planning, reasoning, and decision-making abilities in multi-turn generation settings (Liu et al., 2023b;Xie et al., 2024b).We combine these two threads by evaluating on engineering tasks that necessitate multi-turn actions.Moreover, newer benchmarks have shown that emulating differing environments can help identify unique failure modes (Ruan et al., 2024;Yao et al., 2024).Similarly, in RefactorBench, we find that multi-file dependencies in code provide a strong test bed for previously unseen failure modes in LM agents.</p>
<p>Compositional Tasks and Memory Some benchmarks have identified modeling long-term dependencies as a difficult task for core LMs (Tay et al., 2021;Xie et al., 2024b;Wang et al., 2023b;Lee et al., 2024a).To combat this issue, many works have targeted changes in model architecture and training (Gu et al., 2022;Liu et al., 2023a;Gu &amp; Dao, 2024;Xiong et al., 2024).Other works have tackled this style of problem through augmenting LM agents to have external memory in order to learn longer term skills after large sequences of actions (Sarch et al., 2023;Shinn et al., 2023;Wang et al., 2023a;Sumers et al., 2024).Differing from long term memory in language agents, we largely focus on enabling concurrent reasoning and state-aware behaviors in language agents.Comparable to this focus, many works in embodied control and small neural agents have previously explored training and conditioning over observations about current state in multi-turn situations (Chen et al., 2024b;Wang et al., 2024a;Piterbarg et al., 2024;Li et al., 2022;Moreno et al., 2021).We explore extending these concepts to real-world LM agents to improve their performance.</p>
<p>DEFINITIONS</p>
<p>We generalize varied perspectives in previous literature to construct our definition of a language/LM agent and related terms: a core LM receives an user instruction u and executes actions a n using a set of tools t m , receiving partial observations ω n .This follows a structure most similar to a partially observable Markov decision process (POMDP) (Kaelbling et al., 1998), where the trajectory is τ N = (a 1 , ω 1 , . . ., a N , ω N ).This largely matches the formalization of LM agents articulated in ToolEmu (Ruan et al., 2024).We also define and use the words state or stateful in the context of LM agents as the nature of being dependent on the accumulation of actions a and observations ω, though not necessarily all generated from the LM.Importantly, stateful reasoning focuses on making decisions based on the current state, which is partially observable and can change dynamically.Previous works in building LM agents have recognized the importance of designing interfaces that allow the core LMs to make better decisions for a variety of tasks (Yang et al., 2024;Liu et al., 2024b;Wang et al., 2024d;Lu et al., 2024;Shang et al., 2024).We reference the design choices behind t m and it's impact on ω n , a n as interface design.</p>
<p>REFACTORBENCH</p>
<p>RefactorBench is benchmark of handcrafted multi-file refactoring tasks.The goal for each task is to generate a patch that changes the repository to follow the rules of a specified refactor.In this section, we describe our end-to-end process of constructing the refactoring tasks and highlight some important features of RefactorBench.</p>
<p>TASK CONSTRUCTION</p>
<p>To design a benchmark capturing the common practice of code refactoring, we focus on including a diversity of styles of tasks, using Fowler et al. (2018) as a reference point for different styles of refactors.As the test beds for all tasks, we first select 9 popular Python repositories that have differing overall file structures (Table 1).We then run the below four step process on each repository:</p>
<p>Step 1: Localization and Filtering.We leverage LLMs to identify potential refactoring opportunities in repositories.We iteratively prompt gpt-4o (OpenAI, 2024) with complete files from a target repository, along with examples of various refactor types from Fowler et al. (2018), requesting line numbers and suggestions for potential refactors.We then filter through the returned sites manually to verify if corresponding suggestions can be made and if the changes would affect multiple files.</p>
<p>This process yields a list of refactoring suggestions and their corresponding edit locations in each repository.</p>
<p>Step 2: Construction of Reference Solutions.To generate a prospective reference solution for each refactor, a group of experienced Python programmers handcraft unique, related edits to the refactoring suggestions generated in the previous step.These edits are made based on the design principle (Fowler et al., 2018), while concurrently using gpt-4o to verify that each core refactor is tractable by the language model.Tractability verification is done through prompting gpt-4o with the file to edit, the design principle, and a summary of the change needed to be made.We define this process in depth in Section 3.2.</p>
<p>Step 3: Development of Testing Files.Once the tractability of the subtasks are verified, the developers then create relevant unit tests for each overarching task.At minimum, for every core edit verified in Step 2, a new unit test is generated that parses through the respective file's AST and verifies that changes have the correct broad code structure and syntax necessitated by that subtask, removing dependence on exact-match testing (Appendix D).This iterative approach creates a breadth of tests that comprises a necessary minimum for the total refactor.At test time, a LM agent's generated solution is applied to the codebase and the associated tests crafted for the task instance are then executed.A generated patch is considered successful, if all of the relevant AST tests pass.</p>
<p>Step 4: Generation of Relevant Task Instructions.After reference solution and AST test creation, the developers write a short, but comprehensive task summary to help in the instruction design phase, what we refer to as the base instruction.In order to evaluate different degrees of instruction following with specified and unspecified objectives, we generate two other instruction sets: lazy instruction and descriptive instruction.These instruction sets are generated through a few-shot learning prompt with the respective base instruction and the related unit tests (Appendix A.1).</p>
<p>The above four step process yielded 100 large overarching multi-file refactoring tasks and corresponding tests in 9 different Python repositories.Throughout this work, we report the success on a run based on passing all tests for a task.Table 1 and Figure 2 show a breakdown by repository and other statistics related to the tasks.</p>
<p>IMPORTANT FEATURES</p>
<p>Multi-File By filtering out single-file refactors as part of our task construction process, all tasks in RefactorBench involve multi-file edits.Our tasks edit between 2 to 31 files, with 4 files edited in our reference solutions on average.This feature, by definition, detracts the ability of single-shot LLMs of solving the tasks, and forces feedback-based editing systems to reason over multiple files.</p>
<p>Varying Instruction Sets RefactorBench offers three sets of instructions with varying degree of descriptiveness.With multiple instruction sets, we are able to test for a breadth of types of instructionfollowing and provide a way to effectively scale the difficulty of RefactorBench.The lazy instructions match the styles of real users, where objectives are often unspecific.We also include the base instruction which describes the task completely in a succinct manner.And through the descriptive instruction, we are able to evaluate on an exhaustive instruction where systems are given insights on what they will be tested on, a theoretical upper bound on performance.</p>
<p>Subtask AST Testing In RefactorBench, unit tests for each task are designed to cover various subtasks the LM agent needs to accomplish.During the test construction process, we separate the unit tests to break apart the behavior of subtasks within tasks.This makes understanding the failures within patches an interpretable and quick process.For instance, one can evaluate which files the agent makes edits in, giving more comprehensive understanding of the order of tasks and proximity to a correct solution.RefactorBench's unit tests comprise of 2 to 27 subtests, with an average of 6.51 tests per task.See Appendix D for an example test file and Appendix E for multiple test outputs seen through the lens of this subtask testing format on RefactorBench.</p>
<p>Tractability Through verification steps during task construction, we also make sure that all the core edits are feasible by frontier models at the time of writing.Due to this, our refactors have stronger signal on evaluating the reasoning behaviors between function calls of LLM feedback loops rather</p>
<p>than the broad open-ended task of generating passing code changes.Similarly, previous work has also recognized the importance of focusing agent benchmarks to interpretable subtasks (Côté et al., 2019;Xie et al., 2024b;Shridhar et al., 2021;Chowdhury et al., 2024).Overall, this tractability requirement allows for a more dedicated focus on evaluating the stateful reasoning abilities of LM agents.</p>
<p>EXPERIMENTS</p>
<p>In this section, we explain our approaches to evaluate language agents on RefactorBench.All main studies are done on SWE-agent, which is the highest performing open-source agent framework on the full SWE-bench split at the time of writing.SWE-agent also structurally follows our earlier definition of a POMDP-based LM agent (Yang et al., 2024;Jimenez et al., 2024), while other agents sample from multiple agents (Wang et al., 2024d), weakening ablation studies.Often, due to costs and rate limits on model endpoints impacting efficient ablation studies, we opt to use gpt-4 in experiments, but find that our results scale similarly across models.</p>
<p>Update: These baselines are outdated compared to current SWE-agent implementations.</p>
<p>We share more updated baselines in Section 7.3.</p>
<p>PRELIMINARIES</p>
<p>Current systems have overfit to solving reproducible bugs.As a prior, we observed poor performance for some LM agents when running them on simpler tasks in RefactorBench.Upon investigating internal code of a few open-source LM agents, we find that their internal prompting and in-context examples steer towards solving Github issues.This task specific prompting causes these language agents to treat refactoring problems as bug-fixes.For instance, many systems will attempt to create a bug reproduction script for a simple renaming task.We causate this initial finding as a result of having a lack of benchmarks: it is hard to robustify LM agent systems without ways to evaluate on diverse styles of tasks (Kapoor et al., 2024;Dehghani et al., 2021).For the rest of this work, to better understand the frontier of capabilities within current systems, we alter internal prompts to focus on the task of refactoring.We therefore consider these baselines as an upper bound on performance, and hope for future systems to be designed in accordance to and evaluate over diverse styles of problems.We discuss directions for future systems and generalist performance in Section 7.</p>
<p>BASELINES</p>
<p>Using a containerized framework that emulates a user file system with the target repository, we run a baseline of SWE-agent on all RefactorBench tasks with a per instance cost limit of $10.002 .We report the percentage of completely successful task instances on each run.On the lazy, base, and descriptive instruction sets, SWE-agent with gpt-4 solves 12%, 18%, and 27% respectively.To verify generalization across models, we also run the descriptive baseline with claude-3.5-sonnet,which solves 35% of the test cases completely.To contextualize this performance, we have a proficient human developer attempt all the tasks within the benchmark, with a limit of 5 minutes per task using the base instructions, and they solve 87% of the test cases.The average length of a successful trajectory using gpt-4 is about 45.8 actions and the overall average length is about 58.5 actions.</p>
<p>Additionally, we sample 3 random solved RefactorBench instances in repositories that have 3 or more solved, and combine their descriptive prompts to run as singular instances.We find that SWEagent, although able to solve the singular tasks, is unable to solve any of these longer pseudotasks.We further discuss related results and tackle long horizon planning in later sections of this work.</p>
<p>ANALYSIS</p>
<p>From manual review of trajectories on RefactorBench, we find repeating general behaviors language agents perform.Many prior works have outlined some strengths and failures of current LM agents in different scenarios (Yang et al., 2024;Wang et al., 2024d;Xie et al., 2024b).As such, we focus on three novel failure modes isolated through our baseline experiments.After large-scale human review of trajectories and developing an understanding of failures, to confirm their prevalence on a larger scale, we use gpt-4 with reference solution diffs to analyze unresolved trajectories and the respective test outputs as following one of the failure modes in this section.Through this, we classify about 58% of failed trajectories are corresponding to one of these failure modes, and in held out validation, a human reviewer agreed with the classifications about 74% of the time.</p>
<p>Agents fail to find relevant locations and make applicable changes.Through our task construction, our descriptive instructions provide information on all files that need to be edited.However, we still observe through about 44% of the tests checking for some change, agents initialized with the descriptive instruction did not edit the target files, although being prompted to.These results differ from previous results that firmly found that most LM agent coding systems create patches at the correct location, and mainly fail through incorrect implementations (Chen et al., 2024a;Yang et al., 2024).Moreover, none of the tasks that require changes in more than 6 files are solved in any of our baselines.These results complement previous work evaluating planning capabilities of LLMs, where increases in constraints correlate with decreases in performance (Xie et al., 2024b;Huang et al., 2022).We hypothesize that the increase in files serves as a proxy constraint and LM agents fail in both efficient exploration and composing previous actions.We formalize the related problems of action tracking and stateful reasoning in-depth and tackle it through state updates in Section 6.</p>
<p>Agents fail due to interactions that necessitate erroneous intermediate states.Our classifications also show that 78.4% of trajectories error in a code editing step.Through analyzing these trajectories, we commonly encounter cases where making a change that temporarily introduces errors is a necessary step to solve the task.This is often because subsequent modifications, either within the same file or across multiple files, are concurrently required to resolve these issues.Consequently, the practice of automatically enforcing strict linting rules and rejecting edits based on errors proves to be an impractical approach for scaling real world agents, even though most open-source systems have previously found in-edit linting to significantly boost performance for bug fixing ( Örwall, 2024;Yang et al., 2024;Wang et al., 2024d).This identified scenario demonstrates that LM agents often imitate human forms of interaction, and removing innate capabilities through guardrails can backfire in unintended manners.We further discuss unobstructed LM agent interaction in Section 7.1.</p>
<p>Agents fail due to context flooding and losing sight of objectives.We find that agents struggle in decision making after having commands that are rejected due to formatting issues or unexpected output (Figure 8).In recent work defining in-context reward hacking (ICRH) (Pan et al., 2024), LLMs, through feedback loops in small synthetic tasks, have been shown to model proxy objectives when optimizing over some larger objective.We find, in our real world task of refactoring, that the negative effects from ICRH are also accentuated by extensive context space being taken up by the handling of constraint violations, deprioritizing the initial objective in a form of few-shot learning (Brown et al., 2020).Specifically, a common linting error edit in our tasks shows the model an average of about 1,466 tokens3 : comprising of two blocks of code, error handling prompts, and the flagged errors.We find that this lengthy repetition for error handling function calling weakens trajectory structure.For instance, in some trajectories, we see the agent run end after an intensive function level feedback loop is resolved, a form of prioritizing the new objective.Language models losing sight of initial goals has recently been tackled within single-shot code generation tasks through attention dilution (Tian &amp; Zhang, 2024), but we find this new issue is more prevalent in LM agent trajectories, and is exacerbated through the context-expensive feedback loops.We further discuss ways to approach robust trajectory reasoning in Section 7.1.</p>
<p>TOWARDS STATE-AWARE LANGUAGE AGENTS</p>
<p>In our analysis, we found a general issue with LM agents struggling to plan edits in the right locations.We hypothesize that an innate limitation of the POMDP setup of LM agents is that after sufficiently many timesteps n, due to partial observability, the LM's understanding of the current state at such action a n becomes weaker, through divergence from the initial state before a 1 .In this section, we first explore this claim through a synthetic setup and then attempt conditioning over state updates to improve on the failure mode.We later generalize our approach to entire digital environments and discuss the implications of state-awareness for agent interaction.</p>
<p>Computation of state almost linearly decreases with respect to number of actions taken.Similar to previous work in testing entity tracking in language models (Kim &amp; Schuster, 2023), we test the existence of the earlier divergence hypothesis in a synthetic setup through prompting a LM with 15 categories of preferences, emulating a web agent.We iteratively give an increasing list of n updates to the preferences (i.e.Dogs to Dislikes) and prompt the LM to output the updated list of preferences.Based on 125 randomly initialized runs, we find that failures in cumulative state construction linearly scale with the amount of actions taken (Figure 4).We show the exact setup for reproducibility in Appendix E.  Figure 6: Example flow of a language agent at some timestep N interacting with a state update policy to generate the natural language state summary appended to the agent trajectory.The new updated trajectory is passed to the core LM to generate the next function call and execute it.</p>
<p>STATE-AWARE INTERFACES</p>
<p>In our baseline runs, we find that agent trajectories extend long (60+), necessitating actions across multiple files.However, real world implementations of LM agents often restrict the amount of previous information Count(ω) in τ N to a controlled number of steps4 to avoid flooding context windows.</p>
<p>Being able to model long term changes with limited context has been a problem space in neural policy learning (Piterbarg et al., 2023).To tackle this, a recent SOTA approach in NetHack, a long horizon video game requiring continual learning (Hambro et al., 2022), used unix diff on previous observations in order to keep base models on track (Piterbarg et al., 2024).Their results confirm the importance of continuous and efficient modeling of state changes in environments, but also demonstrate that diff history exploits structure that is present apriori in observations.Other works using LMs to plan for embodied systems have found computation of state observations alongside baseline observations important for long-horizon task planning (Chen et al., 2024b).We combine the idea of efficient modeling of state observations with previous proven results with feedback-based interface design (Yang et al., 2024;Shang et al., 2024), to motivate our approach: state-aware interfaces.</p>
<p>Our implementation for a state-aware interface for interacting with code focuses on succinctly representing the divergence from initialization state, which is represented through previous edit actions.As such, before every function call, we have a cached and updated section with information related to all previous edits, prompting the model with an understanding of the accumulation of its own changes (Appendix G).Formally, we add a recurring externally computed state variable σ to our POMDP, where σ N is the state at timestep N, and our trajectory now follows
τ N = (a 1 , ω 1 , σ 1 , . . . , a N , ω N , σ N ).
Agents with state updates have stronger performance in RefactorBench tasks.We modify SWE-agent with gpt-4o to track and display representations of state (Appendix G).This change boosts the agent's overall performance on RefactorBench: an average of 43.9% relative increase over the instruction sets compared to baseline agents (Figure 5).We also find a strong upwards trend in subtask completion: an average of 71% increase over the instruction sets.As our abstract syntax tree testing isolates unique subtasks in different files and functions, we find that passing more subtasks is correlated with stronger stateful reasoning, the intended goal of the state-aware interface.</p>
<p>STATE UPDATE POLICIES</p>
<p>Having precomputation from the state update allows the LM to ignore computing the reconstruction subtask when generating the next function call.We extend state-updates to generalize to entire digital environments, not just an agent, through the construction of a state update policy.</p>
<p>We define our state update policy π state as a function that manages and provides the cumulative state information to the agent within an environment.Formally, let σ N represent the current state at timestep N derived from all prior interactions.The state update policy π state can be expressed as a conditional function:
π state : (τ N , σ N −1 , ..., σ 0 ) → σ N ,
where
τ N = (a 1 , ω 1 , σ 1 , . . . , a N −1 , ω N −1 , σ N −1 , a N , ω N )
is the trajectory up to the generation of σ N , including all actions a i , observations ω i , and prior states σ i .</p>
<p>State update policies can lead to deviations from typical sequential agent reasoning.We find states expressed in natural language to be a natural approach to facilitate concurrent interactions between language agents in open digital environments.Through our initial implementation of a state update policy, we are also able to model simple external changes from a concurrent state-aware user (Figure 6).In a simple example, we concurrently make a change with an external agent to rename a function that the LM agent has already edited to complete it's refactoring task.Through the state update policy, we are able to propagate this edit information and agent is able to decide to later view the new edit for more context (Appendix H).However, as all the tasks in RefactorBench are mutually exclusive, we do not further evaluate on modeling conflicting objectives between agents at a larger scale, but expand on similar directions for future work in Section 7.1.</p>
<p>DISCUSSION</p>
<p>We introduce RefactorBench, a benchmark that isolates unique failure modes of LM agents through code.Through our experiments, we find that most agents struggle at composing simple actions, and a diverse set of task evaluations is necessary for understanding and designing generalist language agent systems.We also show improvement on baselines through natural language representations of state and hope that further studies within stateful reasoning in differing scenarios can aid in the a larger understanding of the limitations of language agents.</p>
<p>FUTURE DIRECTIONS</p>
<p>Although there are many avenues to take for improving LM agents, we generalize our analysis from our evaluations on RefactorBench tasks into two main categories.</p>
<p>Reasoning Through the synthetic state construction experiment, we formalize that language models innately lose state understanding with respect to actions taken.As such, alongside our introduction of state update policies, we hypothesize that constructing smarter ways to generalize context rather than having the LM condition over the full trajectory is an important direction for tackling this problem.Various recent works on gist-based memory systems within agents, collaboration through optimizable graphs, exploration methods, skill learning, and mitigating partial observability seem promising (Zhuge et al., 2024;Nayak et al., 2024;Lee et al., 2024b;Wang et al., 2023a;Bruce et al., 2023;Xie et al., 2024a;Allen et al., 2024), but no works have tackled concrete methods to scale concurrent state-awareness for simple agent tasks.Many new approaches to improve agent performance have also been shown to scale up inference compute and score higher on various agent related benchmarks (Zhang et al., 2024a;Kapoor et al., 2024;Brown et al., 2024;Wang et al., 2024b).As real world refactoring results are not immediately verifiable, we find this style of repeated sampling to be insufficient without robust critic models.We encourage future works to scale inference time compute in language agents with open-ended tasks like those in RefactorBench.</p>
<p>Interaction In regards to interaction with the real world, we find that LM agents edit code in inefficient manners and have low success rates per single edit.Many agents have switched to diffbased editing ( Örwall, 2024;Gauthier, 2024), which has empirically shown to be a more scalable solution.However, these systems do not get around the issues that come with temporary error states (Section 5) and format restrictions.The natural approach of full-file edits has its own distinct issues: such as generalizing for files longer than token limits, inference speed, token cost, and context flooding.Future approaches could attempt to intertwine full-file rewrites with speculative decoding (Cursor, 2024) and custom trajectory truncation schemes to limit context window flooding.Overall, even outside of code generation, we predict this interaction problem for language agents to be of importance in varying digital domains, and we expect interaction to be a large focus in generalist agent interface constructions, especially in multi-agent scenarios.Our state update policy demonstrates a primitive case of agents being aware of other actions, and we hope for future works to generalize the environment-specific policy approach (Figure 6) in a variety of digital tasks.</p>
<p>LIMITATIONS</p>
<p>RefactorBench's task instances are all in Python, and we hope to expand the benchmark to various languages that are statically and dynamically typed, allowing for evaluations on more styles of refactors.We also focus on highly used open-source Python repositories, and language models may have a better understanding of the repositories due to their prevalence in training data.RefactorBench also has a limited amount of task instances due to the intensive process to create a singular end-toend task and the necessity for quick evaluations (evaluations still takes hours with RefactorBench).</p>
<p>In our evaluations, we also raise the cost limit much past limits in previous works in software engineering agents, due to the inability for agents to solve multi-file tasks quickly and cheaply.We also find, similar to previous works, that agent runs are not deterministic and can solve differing tasks in different runs.RefactorBench is a step forward in evaluating LM agents in robust manners through complex task construction, but like all benchmarks, is still plagued by the possible issues of over-fit data distributions (i.e.only refactors) (Kapoor et al., 2024).To prevent this repetitive issue, we do not release gold reference solutions (only the testing files) and we recommend evaluating software engineering agents on multiple styles of tasks: function editing tasks, bug fixes in SWE-bench, refactors in RefactorBench, etc. to truly define robustness in a general coding agent.Creating a general multi-faceted evaluation suite for language models and agents interacting with code is a compelling direction for future work.</p>
<p>UPDATED BASELINES</p>
<p>We evaluate the updated base SWE-agent 1.0 on RefactorBench using gpt-4o.We largely find that our previous results hold, with SWE-agent solving 21% of base instances and 31% of descriptive instances.However, we find that the speed and cost in which SWE-agent completes tasks significantly decreases to $1.69 per successful instance, showcasing new improvements in agents localizing changes to make.We are excited to see the community test their autonomous systems on RefactorBench, especially with reasoning models.Similar to recent results on multi-file splits of SWE-bench, we have seen preliminary results of reasoning agents solving many more Refactor-Bench tasks compared to non-reasoning model agents.</p>
<p>A TASK CONSTRUCTION PROMPTS A.1 PROMPT FOR LAZY INSTRUCTION</p>
<p>We prompt gpt-4-turbo with the handcrafted base instruction based on all the edits and this prompt to get our lazy instruction.</p>
<p>A.2 PROMPT FOR DESCRIPTIVE INSTRUCTION</p>
<p>We prompt gpt-4-turbo with the handcrafted base instruction based on all the edits, the corresponding testing file, and this prompt to get our descriptive instruction.</p>
<p>Please convert the following instruction to be more specific and have specific filenames for edits (not paths).Do not change the behavior of the task, but give a longer, more descriptive version of the task in human-like specifications.Reason over the AST tests provided to give more information on which files could be relevant, but do not give exact</p>
<p>B AGENT PROMPT CHANGES B.1 UPDATED SWE-AGENT SYSTEM PROMPT</p>
<p>As described in Section 4, we alter the SWE-agent prompt to stop the agent from creating bug reproduction scripts for refactors and focus on the style of task at hand.We also remove pytest and other testing functionality in order to prevent timeout issues with the large codebases.</p>
<p>SETTING: You are an autonomous programmer specializing in refactoring, and you're working directly in the command line with a special interface.The special interface consists of a file editor that shows you WINDOW lines of a file at a time.In addition to typical bash commands, you can also use the following commands to help you navigate and edit files.</p>
<p>COMMANDS: command_docs</p>
<p>Please note that THE EDIT COMMAND REQUIRES PROPER INDENTATION.If you'd like to add the line ' print(x)' you must fully write that out, with all those spaces before the code!Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.</p>
<p>RESPONSE FORMAT:</p>
<p>Your shell prompt is formatted as follows:</p>
<p>(Open file: <path>) <cwd> You need to format your output using two fields: discussion and command.Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example: DISCUSSION First I'll start by using ls to see what files are in the current directory.Then maybe we can look at some relevant files to see what they look like.\begin{verbatim} ls -a \end{verbatim} You should only include a * SINGLE * command in the command section and then wait for a response from the shell before continuing with more discussion and commands.Everything you include in the DISCUSSION section will be saved for future reference.If you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.You're free to use any other bash commands you want (e.g.find, grep, cat , ls, cd) in addition to the special commands listed above.However, the environment does NOT support interactive session commands (e .g. python, vim), so please do not invoke them.  5.When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation.Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish.If it didn't, issue another command to fix it.</p>
<p>6.It may be necessary to install the repository from source before you can run code.Please think about how to install the environment from the repository directory if you need to do so.</p>
<ol>
<li>
<p>Do not run the repo tests, just focus on comprehensively finishing the multi-file refactor to the best of your abilities.</p>
</li>
<li>
<p>If you make a change that could have dependencies, always reason and search where imports need to be updated and update them accordingly.9. Do not run huge grep commands that could print out and flood the context with too many items.If not sure, just check how many items the command will run to begin with.</p>
</li>
</ol>
<p>(Open file: open_file) \ (Current directory: working_dir) \ FAIL: test_ssh_py_shim_does_not_have_ex_cantcreat (cant-create-test.</p>
<p>TestSaltExitCodes.test_ssh_py_shim_does_not_have_ex_cantcreat) ----------------------------------------------------------------------Traceback (most recent call last):</p>
<p>File "/refactor_repos/salt_refactor/task_test/cant-create-test.py", line 107, in test_ssh_py_shim_does_not_have_ex_cantcreat self.assertFalse(ex_cantcreat_found,f"'EX_CANTCREAT' (misspelled) found in {file_path}") AssertionError: True is not false : 'EX_CANTCREAT' (misspelled) found in ../salt/client/ssh/ssh_py_shim.py ====================================================================== FAIL: test_ssh_py_shim_uses_local_ex_cantcreate (cant-create-test.TestSaltExitCodes.test_ssh_py_shim_uses_local_ex_cantcreate) ----------------------------------------------------------------------Traceback (most recent call last):</p>
<p>File "/refactor_repos/salt_refactor/task_test/cant-create-test.py", line 55, in test_ssh_py_shim_uses_local_ex_cantcreate self.assertTrue(ex_cantcreate_used,f"'EX_CANTCREATE' not used in { file_path}") AssertionError: False is not true : 'EX_CANTCREATE' not used in ../salt/ client/ssh/ssh_py_shim.py --------------------------------------------------------------------- ----------------------------------------------------------------------Traceback (most recent call last):</p>
<p>File "/refactor_repos/fastapi_refactor/task_test/value-is-a-sequencetest.py", line 75, in test_import_value_is_a_sequence_in_utils self.assertTrue(import_found,"'value_is_a_sequence' not imported from 'fastapi._compat' in dependencies/utils.py")AssertionError: False is not true : 'value_is_a_sequence' not imported from 'fastapi._compat' in dependencies/utils.py----------------------------------------------------------------------Ran 5 tests in 0.</p>
<p>Figure 1 :
1
Figure 1: RefactorBench task instances are multi-file refactors verified by custom AST unit testing.Tasks are split over popular open-source Python repositories and reference solutions are withheld to prevent overfitting to the task.</p>
<p>Figure 2 :
2
Figure 2: Distribution of RefactorBench tasks across 9 open source Python repositories.</p>
<p>Figure 3 :
3
Figure 3: Baselines of a prompt adjusted SWE-agent with gpt-4 and a human developer given IDE access and a time limit of 5 minutes.</p>
<p>Figure 4 :
4
Figure 4: GPT-4 Turbo accuracy in end state construction drops consistently over increased iterations of actions.Prompts and examples are available in Appendix E.</p>
<p>Figure 5 :
5
Figure 5: Comparison of a prompt adjusted SWEagent and a state-aware SWE-agent implementation, both using gpt-4.Sample implementation code available in Appendix G.</p>
<p>Please convert the following instruction to be less specific.Do not change the behavior of the task, but give a short, less descriptive version of the task in human-like prose.Your final instruction should be a partial sentence and should not instruct to run any tests.It should just describe the changes to the repository.Do not output ANYTHING ELSE BUT THE NEW INSTRUCTION.Here is the original instruction: {base_instruction} Here are examples of lazy instructions: {few_shot_lazy} Remember to only output the NEW LAZY INSTRUCTION CORRESPONDING TO THE BASE TASK.</p>
<p>implementation details or anything related to what generalizations the tests are looking for.Your final instruction should be around 2-3 full sentences and should not say to run any tests or anything like that.It should just describe the changes to the repository.Do not output ANYTHING ELSE BUT THE NEW INSTRUCTION.Here is the original instruction and its related test file: {base_instruction} Test File Starts Here: {inst_test_file} End of Test File.Here are examples of descriptive instructions: {few_shot_desc} Remember to only output the NEW DESCRIPTIVE INSTRUCTION CORRESPONDING TO THE BASE TASK.</p>
<p>4.</p>
<p>Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file).The currently open file might be in a different directory than the working directory!Note that some commands, such as 'create', open files, so they might change the current open file.</p>
<p>Figure 8: Visual example of a language agent having a failed function call loop showcasing the context flooding and deprioritized objective failure mode.</p>
<p>Table 1 :
1MeanMax
Statistics on RefactorBench tasks, repositories, and AST-based unit tests.</p>
<p>Table 2
2: Baseline task performancerelative to instruction type. Throughthe varied categories, we find that lan-guage agents are sensitive to unspeci-fied objectives (Lazy) and improve inperformance greatly when given infor-mation on which files to make edits in(Descriptive).Instruction Type Resolution RateLazy12.0%Base22.0%Descriptive27.0%</p>
<p>'re going to solve this refactoring issue on your own.Your terminal session has started and you're in the repository's root directory.You can use any bash commands or the special interface to help you.Edit all the files you need to and run any checks or tests that you want.Remember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME.You should always wait for feedback after every command.When you're satisfied with all of the changes you've made, you can submit your changes to the code base by simply running the submit command.Note however that you cannot use any interactive session commands (e.g.python, vim) in this environment, but you can write scripts and run them.E.g.you can write a python script and then run it with <code>python &lt;script\ _name&gt;.py</code>.NOTE ABOUT THE EDIT COMMAND: Indentation really matters!When editing a file, make sure to insert appropriate indentation before each line!IMPORTANT TIPS: 1. Always start by checking your working directory, cd'ing to the task repo, and then trying to find where to do the refactor using the search tools.Do not go into other directories like root or sys.Just go to the task repo and make edits in there.2.Ifyou run a command and it doesn't work, try running a different command.A command that did not work once will not work the second time unless you modify it!3.If you open a file and need to get to an area around a specific line that is not in the first 100 lines, say line 583, don't just use the scroll_down command multiple times.Instead, use the goto 583 command.It's much quicker.
instance_template: |-We're currently solving the following issue within our repository. Here'sthe issue text:ISSUE:INSTRUCTIONS:Now, you</p>
<p>-Ran 8 tests in 0.046s FAILED (failures=5) ================================================================================ Test file: tests/fastapi_refactor/value-is-a-sequence-test.pyError: test_compat_file_exists (value-is-a-sequence-test. TestFastAPICompatUtils.test_compat_file_exists) ... ok test_import_value_is_a_sequence_in_utils (value-is-a-sequence-test. TestFastAPICompatUtils.test_import_value_is_a_sequence_in_utils) ... FAIL test_value_is_a_sequence_function_exists (value-is-a-sequence-test. TestFastAPICompatUtils.test_value_is_a_sequence_function_exists) ... ok test_value_is_sequence_function_does_not_exist (value-is-a-sequence-test.
TestFastAPICompatUtils.test_value_is_sequence_function_does_not_exist)... oktest_value_is_sequence_function_does_not_exist_in_utils (value-is-a-sequence-test.TestFastAPICompatUtils.test_value_is_sequence_function_does_not_exist_in_utils) ... ok======================================================================FAIL: test_import_value_is_a_sequence_in_utils (value-is-a-sequence-test.TestFastAPICompatUtils.test_import_value_is_a_sequence_in_utils)
This cost is due to large token counts being necessitated with multi-file reasoning.
Using the o200k base tokenizer on task instances within the ansible/ansible repository
Five step observational window in the case of SWE-agent.
ACKNOWLEDGMENTSWe thank the entire Data&amp;AI Group at Microsoft for feedback and valuable discussions throughout the work.Dhruv Gautam thanks Michele Tufano, Alexander Pan, and Joe Guan for feedback of an earlier draft of the paper and for many formative research discussions about language models, reasoning, and code generation.C EXAMPLE CONDENSED SUCCESSFUL TRAJECTORYWe show a portion of a successful trajectory for the refactoring task: rename-send-from-directory in the flask repository.Full examples are available in our repository.Base Instruction: Rename the helper function send from directory to send from directory helper but keep the overall flask usage as send from directory.State-aware SWE-agent Run { "action": "ls -F\n", "observation": "flask_refactor/\nbin@\nboot/\ndev/\netc/\nhome/\nlib@ \nmedia/\nmnt/\nopt/\nproc/\nroot/\nrun/\nsbin@\nsrv/\nsys/\ntmp/\ nusr/\nvar/", "response": "DISCUSSION\nFirst, I'll list the files in the current directory to get an overview of the project structure.This will help me locate the 'helpers' module and the 'flask' folder where the changes need to be made.\n<code>\nls-F\n</code>", "state": "{\"working_dir\": \"flask_refactor\", \"open_file\": \"n/a \", \"recent_edits\": []}", "thought": "DISCUSSION\nFirst, I'll list the files in the current directory to get an overview of the project structure.This will help me locate the 'helpers' module and the 'flask' folder where the changes need to be made.\n"}, ... ... (Makes changes in multiple files) ... { "role": "assistant", "content": "DISCUSSION\nWe need to update the import statement for send_from_directory<code>to import</code>send_from_directory_helper<code>as send_from_directory</code>. Let's make the necessary edit.\n<code>\nedit9:9\ nfrom flask.helpersimport send_from_directory_helper as send_from_directory\nend_of_edit\n</code>", "thought": "DISCUSSION\nWe need to update the import statement for send_from_directory<code>to import</code>send_from_directory_helper<code>as send_from_directory</code>. Let's make the necessary edit.\n","action": "edit 9:9\nfrom flask.helpersimport send_from_directory_helper as send_from_directory\nend_of_edit\n", }, { "role": "user", "content": "[Truncated File Output] (Current State: ['Edited helpers.py at lines 514:514', 'Edited <strong>init</strong>.pyat lines 24:24', 'Edited app.py at lines 42:42', 'Edited blueprints.py at lines 9:9', 'Edited test_helpers.pyat lines 9:9\n ']) (Open file: /flask_refactor/tests/test_helpers.py)(Current directory: flask_refactor) bash-$", }, { "role": "assistant", "content": "DISCUSSION\nThe import statement in <code>test_helpers.py</code>hasbeen updated.We have now made all the necessary changes to the relevant files.Let's submit the changes to the code base.\n<code>\nsubmit\n</code>", "thought": "DISCUSSION\nThe import statement in <code>test_helpers.py</code>hasbeen updated.We have now made all the necessary changes to the relevant files.Let's submit the changes to the code base.\n","action": "submit\n", }D EXAMPLE TEST FILEHere is the parameterize-gunzip.py test file for the parameterize-gunzip task:Base instruction: Encapsulate the parameters in gunzip into a new class called GunzipParams and update all references within the repository to create a GunzipParams object.---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------[product] # Track old preference new_preference = random.choice(["Likes","Dislikes", "NA"]) while new_preference == old_preference: # Ensure the new preference is different new_preference = random.choice(["Likes","Dislikes", "NA"]) details = { "action": action_type, "category": category, "product": product, #"old_preference": old_preference, "new_preference": new_preference } preferences[category][product] = new_preference # Update to new preference trajectory["actions"].append(details)# Snapshot of system state after each action trajectory["states"][f"Action{i}"] = {cat: dict(preferences[cat]) for cat in preferences} trajectories.append(trajectory)return trajectories def main(): categories = ["Electronics", "Books", "Clothing", "Garden", "Games"] products = { "Electronics": ["Laptop", "Smartphone", "Headphones"], "Books": ["Novel", "Biography", "Science Fiction"], "Clothing": ["Jeans", "T-Shirt", "Jacket"], "Garden": ["Shovel", "Lawn Mower", "Gloves"], "Games": ["Board Game", "Video Game", " Given these randomly generated actions in json, we prompt gpt-4-turbo for 125 random initializations iteratively over 0-50 actions of the generated actions.Figure7shows the prompt and expected output.We are not strict with format rules, and allow minor mistakes, however, our parser requires the larger category separations.Here # Check if this filename:line_number pair has been seen before if [ -z "${seen_edits["$filename:$line_number"]}" ]; then filtered_edits+=("$edit") seen_edits["$filename:$line_number"]=1 fi done # Convert the filtered edits into a JSON array recent_edits_json=$(printf '%s\n' "${filtered_edits[@]}" | jq -Rs -c 'split("\n")') fi state_json=$(jq -n --arg wd "$working_dir" --arg of "$(realpath " $open_file_info")" --argjson re "$recent_edits_json" \ '{"working_dir": $wd, "open_file": $of, "recent_edits": $re}')echo "$state_json" }We also, to communicate all the recent edits, append the outputs of this state command in every step template through:
Copilot evaluation harness: Evaluating llm-guided software programming. Anisha Agarwal, Aaron Chan, Shubham Chandel, Jinu Jang, Shaun Miller, Roshanak Zilouchian Moghaddam, Yevhen Mohylevskyy, Neel Sundaresan, Michele Tufano, 2024</p>
<p>Aide Dev, Sota on swe bench lite. 2024</p>
<p>Mitigating partial observability in sequential decision processes via the lambda discrepancy. Cameron Allen, Aaron Kirtland, Ruo Yu Tao, Sam Lobel, Daniel Scott, Nicholas Petrocelli, Omer Gottesman, Ronald Parr, Michael L Littman, George Konidaris, 2024</p>
<p>Masai: Modular architecture for softwareengineering ai agents. Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna Bairi, Aditya Kanade, Nagarajan Natarajan, 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton, 2021</p>
<p>Aws Q Developer, Aws developer center. 2024</p>
<p>Codeplan: Repository-level coding using llms and planning. Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, D C Vageesh, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, B Ashok, Shashank Shet, 2023</p>
<p>What will it take to fix benchmarking in natural language understanding?. R Samuel, George E Bowman, Dahl, 2021</p>
<p>Large language monkeys: Scaling inference compute with repeated sampling. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, Azalia Mirhoseini, 2024</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>Learning about progress from experts. Jake Bruce, Ankit Anand, Bogdan Mazoure, Rob Fergus, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of artificial general intelligence: Early experiments with gpt-4. 2023</p>
<p>Can it edit? evaluating the ability of large language models to follow code editing instructions. Federico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Jane Anderson, Arjun Guha, Conference on Language Modeling (COLM). 2024</p>
<p>Coder: Issue resolving with multi-agent and task graphs. Dong Chen, Shaoxin Lin, Muhan Zeng, Daoguang Zan, Jian-Gang Wang, Anton Cheshkov, Jun Sun, Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan Bian, Tao Xie, Qianxiang Wang, 2024a</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Fotios Chantzis. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-Grew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Evaluating large language models trained on code</p>
<p>Llm-state: Open world state representation for longhorizon task planning with large language model. Siwei Chen, Anxing Xiao, David Hsu, 2024b</p>
<p>Introducing swe-bench verified. Neil Chowdhury, James Aung, Chan Jun Shern, Oliver Jaffe, Dane Sherburn, Giulio Starace, Evan Mays, Rachel Dias, Marwan Aljubeh, Mia Glaese, Carlos E Jimenez, John Yang, Kevin Liu, Aleksander Madry, August 2024</p>
<p>. Cognition.ai. Introducing devin. 2024</p>
<p>Editing files at 1000 tokens per second. Cursor, 2024</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Ruo Yu Tao, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, 2019</p>
<p>The benchmark lottery. Mostafa Dehghani, Yi Tay, Alexey A Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, Oriol Vinyals, 2021</p>
<p>Refactoring: Improving the Design of Existing Code. Martin Fowler, Kent Beck, John Brant, William Opdyke, Don Roberts, 2018Addison-Wesley Professional2nd edition</p>
<p>Paul Gauthier, Aider: Ai-powered coding assistant. 2024</p>
<p>Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Tri Dao, 2024</p>
<p>Efficiently modeling long sequences with structured state spaces. Albert Gu, Karan Goel, Christopher Ré, 2022</p>
<p>Eric Hambro, Sharada Mohanty, Dmitrii Babaev, Minwoo Byeon, Dipam Chakraborty, Edward Grefenstette, Minqi Jiang, Daejin Jo, Anssi Kanervisto, Jongmin Kim, Sungwoong Kim, Robert Kirk, Vitaly Kurin, Heinrich Küttler, Taehwon Kwon, Donghoon Lee, Vegard Mella, Nantas Nardelli, Ivan Nazarov, Karolis Ramanauskas, Tim Rocktäschel, Danielle Rothermel, Mikayel Samvelyan, Dmitry Sorokin, Maciej Sypetkowski, and Michał Sypetkowski. Insights from the neurips 2021 nethack challenge. Nikita Ovsov, Jack Parker-Holder, Roberta Raileanu2022</p>
<p>Improving long-horizon imitation through instruction prediction. Joey Hejna, Pieter Abbeel, Lerrel Pinto, 2023</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, 2024</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, arXiv:2201.072072022arXiv preprint</p>
<p>A survey on large language models for code generation. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, Sunghun Kim, 2024</p>
<p>SWE-bench: Can language models resolve real-world github issues?. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, ; Karthik R Narasimhan, The Twelfth International Conference on Learning Representations. Ofir Press2024</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial Intelligence. 1011-21998</p>
<p>Ai agents that matter. Sayash Kapoor, Benedikt Stroebl, Zachary S Siegel, Nitya Nadgir, Arvind Narayanan, 2024</p>
<p>Entity tracking in language models. Najoung Kim, Sebastian Schuster, 2023</p>
<p>Res-q: Evaluating code-editing large language model systems at the repository scale. Beck Labash, August Rosedale, Alex Reents, Lucas Negritto, Colin Wiel, 2024</p>
<p>Can long-context language models subsume retrieval, rag, sql, and more?. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, M R Sébastien, Vincent Arnold, Siddharth Perot, Hexiang Dalmia, Xudong Hu, Panupong Lin, Aida Pasupat, Jeremy R Amini, Sebastian Cole, Iftekhar Riedel, Ming-Wei Naim, Kelvin Chang, Guu, 2024a</p>
<p>A human-inspired reading agent with gist memory of very long contexts. Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, Ian Fischer, 2024b</p>
<p>Language modeling with latent situations. Belinda Z Li, Maxwell Nye, Jacob Andreas, 2022</p>
<p>Devbench: A comprehensive benchmark for software development. Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping Yang, Dahua Lin, Chao Peng, Kai Chen, 2024</p>
<p>Ring attention with blockwise transformers for nearinfinite context. Hao Liu, Matei Zaharia, Pieter Abbeel, 2023a</p>
<p>Repobench: Benchmarking repository-level code auto-completion systems. Tianyang Liu, Canwen Xu, Julian Mcauley, 2024a</p>
<p>Codexgraph: Bridging large language models and code repositories via code graph databases. Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Wenmeng Zhou, Fei Wang, Michael Shieh, 2024b</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. 2023b</p>
<p>Omniparser for pure vision based gui agent. Yadong Lu, Jianwei Yang, Yelong Shen, Ahmed Awadallah, 2024</p>
<p>Neural recursive belief states in multi-agent reinforcement learning. Pol Moreno, Edward Hughes, Kevin R Mckee, Bernardo Avila Pires, Théophane Weber, 2021</p>
<p>Long-horizon planning for multi-agent robots in partially observable environments. Siddharth Nayak, Adelmo Morrison Orozco, Marina Ten Have, Vittal Thirumalai, Jackson Zhang, Darren Chen, Aditya Kapoor, Eric Robinson, Karthik Gopalakrishnan, James Harrison, Brian Ichter, Anuj Mahajan, Hamsa Balakrishnan, 2024</p>
<p>Erik Jones, Meena Jagadeesan, and Jacob Steinhardt. Feedback loops drive incontext reward hacking in llms. 2024OpenAIarXivGpt-4o system card</p>
<p>The impact of ai on developer productivity: Evidence from github copilot. Sida Peng, Eirini Kalliamvakou, Peter Cihon, Mert Demirer, Ulyana Piterbarg, Lerrel Pinto, and Rob Fergus. Nethack is hard to hack. 2023. 2023</p>
<p>Ulyana Piterbarg, Lerrel Pinto, Rob Fergus, diff history for neural language agents. 2024</p>
<p>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. 2022</p>
<p>Identifying the risks of lm agents with an lm-emulated sandbox. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois, Chris J Maddison, Tatsunori Hashimoto, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Gabriel Sarch, Yue Wu, Michael J Tarr, Katerina Fragkiadaki, 2023</p>
<p>Traveler: A multilmm agent framework for video question-answering. Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig, 2024</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>Cognitive architectures for language agents. Theodore R Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L Griffiths, 2024</p>
<p>Long range arena : A benchmark for efficient transformers. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler, International Conference on Learning Representations. 2021</p>
<p>Selective prompt anchoring for code generation. Yuan Tian, Tianyi Zhang, 2024</p>
<p>Roshanak Zilouchian Moghaddam, and Neel Sundaresan. Autodev: Automated ai-driven development. Michele Tufano, Anisha Agarwal, Jinu Jang, 2024</p>
<p>Llm-empowered state representation for reinforcement learning. Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu, Wenming Yang, Xiangyang Ji, 2024a</p>
<p>Planning in natural language improves llm search for code generation. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, Hugh Zhang, 2024b</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, 2023a</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Jirong Wen, 10.1007/s11704-024-40231-1Frontiers of Computer Science. 2095-2236186March 2024c</p>
<p>Opendevin: An open platform for ai software developers as generalist agents. Xingyao Wang, Boxuan Li, Yufan Song, Frank F Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, H Hoang, Fuqiang Tran, Ren Li, Mingzhang Ma, Bill Zheng, Yanjun Qian, Niklas Shao, Yizhe Muennighoff, Binyuan Zhang, Junyang Hui, Robert Lin, Hao Brennan, Heng Peng, Graham Ji, Neubig, 2024d</p>
<p>Jarvis-1: Openworld multi-task agents with memory-augmented multimodal language models. Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang, 2023b</p>
<p>Agentless: Demystifying llmbased software engineering agents. Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, Lingming Zhang, 2024</p>
<p>Learning to explore in POMDPs with informational rewards. Annie Xie, Logan Mondal Bhamidipaty, Evan Zheran Liu, Joey Hong, Sergey Levine, Chelsea Finn, Proceedings of the 41st International Conference on Machine Learning. Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, Felix Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024a235</p>
<p>Travelplanner: A benchmark for real-world planning with language agents. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su, 2024b</p>
<p>Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Yitao Lei, Yiheng Liu, Shuyan Xu, Silvio Zhou, Caiming Savarese, Victor Xiong, Tao Zhong, Yu, 2024c</p>
<p>From artificial needles to real haystacks: Improving retrieval capabilities in llms by finetuning on synthetic data. Zheyang Xiong, Vasilis Papageorgiou, Kangwook Lee, Dimitris Papailiopoulos, 2024</p>
<p>Swe-agent: Agent-computer interfaces enable automated software engineering, 2024. Shunyu Yao and Karthik Narasimhan. Language agents in the digital world: Opportunities and risks. princeton-nlp.github.io. John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Jul 2023Karthik Narasimhan, and Ofir Press</p>
<p>τ -bench: A benchmark for tool-agent-user interaction in real-world domains. Shunyu Yao, Noah Shinn, Pedram Razavi, Karthik Narasimhan, 2024</p>
<p>Diversity empowers intelligence: Integrating expertise of software engineering agents. Kexun Zhang, Weiran Yao, Zuxin Liu, Yihao Feng, Zhiwei Liu, Rithesh Murthy, Tian Lan, Lei Li, Renze Lou, Jiacheng Xu, Bo Pang, Yingbo Zhou, Shelby Heinecke, Silvio Savarese, Huan Wang, Caiming Xiong, 2024a</p>
<p>Autocoderover: Autonomous program improvement. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, Abhik Roychoudhury, 2024b</p>
<p>Language agents as optimizable graphs. Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, Jürgen Schmidhuber, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
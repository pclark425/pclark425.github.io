<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8448 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8448</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8448</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-1fcaafeb72142fe3d1a5d698a072d69778d244b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1fcaafeb72142fe3d1a5d698a072d69778d244b0" target="_blank">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Search-R1 is introduced, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval.</p>
                <p><strong>Paper Abstract:</strong> Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces Search-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. Search-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that Search-R1 improves performance by 41% (Qwen2.5-7B) and 20% (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrieval-augmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8448.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8448.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SEARCH-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SEARCH-R1 (Search-augmented Reinforcement Learning for Reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL framework that trains LLMs to interleave step-by-step reasoning with multi-turn real-time search engine calls, using retrieved-token loss masking and an outcome-based reward (exact-match) to stabilize optimization and improve retrieval-driven decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SEARCH-R1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agentic LLM policy trained with RL (PPO or GRPO) to generate reasoning tokens and explicit <search> queries; retrieved passages are injected into the rollout between <information> tags and the final answer is produced within <answer> tags. Training applies retrieved-token masking so gradients only update model-generated tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B / Qwen2.5-3B (experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen-2.5 family LLMs used in both base and instruction-tuned variants; experiments on 3B, 7B (and additional 14B in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Seven QA datasets (NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-domain and multi-hop question answering benchmarks requiring external factual knowledge and multi-step reasoning; agent must produce final answer strings (evaluated via Exact Match).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning (retrieval-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / retrieval-augmented memory (search engine)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Agent emits <search>query</search> tokens during rollout; system issues the query to a search engine (E5 retriever over Wikipedia), inserts top-k retrieved passages between <information>...</information> into the model context, and the agent continues reasoning; retrieved-token loss masking prevents optimizing retrieved tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents / retrieved passages (text snippets) returned by the search engine and appended to the generation sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic dense retrieval via E5 retriever over Wikipedia; top-k retrieval (k studied with values 1,3,5; default k=3) and appended by simple concatenation into the prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qwen2.5-7B-base Search-R1-base Avg EM = 0.431 (Exact Match average across seven datasets); per-table values (NQ 0.480, TriviaQA 0.638, PopQA 0.457, HotpotQA 0.433, 2Wiki 0.382, Musique 0.196, Bamboogle 0.432).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>R1 (RL without search) baseline Qwen2.5-7B-base Avg EM = 0.276 (per Table 2); RAG baseline Avg EM = 0.304. (See paper Tables 2 and 3 for full numbers.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Multiple ablations and comparisons: (1) Retrieved-token loss masking: with masking Avg EM 0.431 vs without masking 0.343 (major improvement). (2) Comparison to RAG and R1: SEARCH-R1 improves average relative performance by ~24% over RAG for Qwen2.5-7B and ~20% for Qwen2.5-3B. (3) Number of retrieved passages: top-k=3 outperformed top-k=1 and k=5 (k=3 gave best final Avg EM). (4) RL algorithm comparison: PPO vs GRPO (PPO more stable, GRPO faster but can collapse). (5) GRPO group-size ablation: larger group sizes accelerate convergence but risk instability; size=1 sometimes generalizes better. (6) Rejection sampling baseline (using search-enabled rollouts) yields intermediate gains (Avg EM 0.348 for 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interleaving retrieval with RL-trained reasoning substantially improves QA performance over retrieval-agnostic RL and RAG baselines; retrieved-token loss masking is critical for stable and effective training; top-k=3 is a good retrieval budget trade-off; PPO provides more stable RL optimization while GRPO can converge faster but be unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Training stability issues (GRPO can collapse), sensitivity to noisy retrieved passages (top-k large can hurt training), need to mask retrieved tokens to avoid unintended optimization, non-differentiability of search requiring RL-style optimization, computation and rollout costs for multi-turn retrieval, and current reward is limited to simple outcome-based (EM) signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8448.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R1 / DeepSeek-R1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1 (R1) - RL for reasoning without retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL framework previously proposed to incentivize reasoning capabilities in LLMs using outcome-based rewards; in this paper R1 refers to the RL method trained without access to an external search engine and used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R1 (DeepSeek-R1)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL-trained LLM policy optimized with outcome-based rewards to improve parametric reasoning (no external search calls); used here as a baseline to compare against search-augmented RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B / Qwen2.5-3B (as evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same family of Qwen-2.5 models used for fair comparison; both base and instruct variants evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same seven QA datasets (NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question answering and multi-hop reasoning tasks evaluated with Exact Match.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>R1-base (Qwen2.5-7B-base) Avg EM = 0.276 (Table 2); R1-instruct Avg EM = 0.271.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared directly to SEARCH-R1 to quantify benefit of retrieval: SEARCH-R1 outperforms R1 (R1 lacks external memory), showing gains from adding search to RL-trained reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RL tuning purely for parametric reasoning without retrieval (R1) yields improvements, but adding an external retrieval channel (SEARCH-R1) gives substantial additional gains on knowledge-intensive QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Without access to external up-to-date evidence, R1 is limited by model parametric knowledge and cannot recover facts not present in the weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8448.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-then-generate pipeline that retrieves passages based on the input and conditions the LLM on concatenated retrieved text for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrievalaugmented generation for large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A standard retrieval-augmented baseline where the retriever returns passages based on the input query, concatenated into model context for single-round generation (no iterative search during reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with Qwen2.5-7B / Qwen2.5-3B in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same retrieval (E5) and knowledge corpus (Wikipedia) used to ensure fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same seven QA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive QA; RAG retrieves top-3 passages for the prompt then generates answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented memory (single-round retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Retriever returns top-k passages for the input prompt which are concatenated to the LLM context before generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved passages/documents appended to prompt context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic dense retrieval via E5 over Wikipedia; default top-k=3 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Qwen2.5-7B RAG Avg EM = 0.304 (Table 2); per-dataset numbers in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against SEARCH-R1: SEARCH-R1 yields ~24% relative improvement over RAG on 7B model (and ~20% on 3B) under identical retrieval, corpus, and pre-trained LLM settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard one-shot retrieval (RAG) is effective but SEARCH-R1's learned multi-turn retrieval-policy and RL optimization outperform RAG across in- and out-of-distribution QA benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RAG's single retrieval stage can miss relevant context for multi-step reasoning; cannot adapt queries dynamically during reasoning as SEARCH-R1 can.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8448.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRCoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IRCoT (Interleaved Retrieval with Chain-of-Thought)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based method that interleaves retrieval with chain-of-thought style reasoning via prompting (not RL), used here as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IRCoT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A prompting approach that instructs the LLM to alternate between internal chain-of-thought reasoning and issuing retrieval queries; relies on prompting rather than RL to control retrieval steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with instruct models (as baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompted instruct LLM variants (Qwen2.5 instruct used for inference baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same QA datasets (multi-step / knowledge-intensive QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step question answering where iterative retrieval and reasoning are helpful.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval (prompted iterative retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompting instructs the model to produce queries and use retrieved passages in subsequent reasoning steps; retrieval is not learned via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved passages appended to context as extra information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Retriever (E5) top-k passages appended; retrieval strategy fixed by prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>IRCoT (Qwen2.5-7B instruct) Avg EM = 0.239 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared as a baseline to SEARCH-R1; SEARCH-R1 (RL-trained retrieval policy) outperforms IRCoT, indicating that training retrieval behavior yields gains over prompting alone.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompting-based interleaved retrieval helps, but optimizing retrieval behavior with RL (SEARCH-R1) provides further improvements in QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompting cannot adapt retrieval strategy via learning; may be suboptimal if the LLM lacks pretraining exposure to iterative tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8448.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Search-o1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Search-o1 (Agentic search-enhanced reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic baseline that enhances reasoning with search; included in experiments as a retrieval-enabled baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Search-o1: Agentic search-enhanced large reasoning models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Search-o1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A search-enhanced reasoning model that treats search as a tool to improve reasoning; used here as an experimental baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Evaluated with Qwen2.5 models in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agentic search-enabled reasoning baseline; exact internal training protocol not detailed in this paper as it is an external baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same seven QA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-intensive QA where search is used to supply external facts during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / search-as-tool</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external retrieval / search tool</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Search used as a tool during inference (details in cited paper); here it is a baseline that retrieves passages for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved passages appended to context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic retrieval (E5) over Wikipedia, top-3 passages used for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Search-o1 (Qwen2.5-7B) Avg EM = 0.206 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared as a baseline; SEARCH-R1 outperforms Search-o1 under the same retrieval and model settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While agentic search baselines help, SEARCH-R1's RL optimization of multi-turn search behavior yields superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not analyzed in detail in this paper; treated as an external baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8448.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection Sampling (search-enabled)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection Sampling with search-enabled rollouts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline fine-tuning approach that generates multiple candidate search-enabled trajectories and selects those producing correct final answers to form a training set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Rejection Sampling (with search)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates multiple candidate responses per prompt with the instructed LLM (using the multi-turn LLM-search rollout mechanism), keeps trajectories whose final answers are correct, and uses them for supervised fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5 variants (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same retriever and corpus; five candidate responses generated per prompt in the described protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same seven QA datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Uses search-enabled rollouts to create a filtered supervised dataset for fine-tuning; evaluated on QA EM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / supervised fine-tuning with search-enabled trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (search-enabled trajectories used as training data)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Uses the same multi-turn search-rollout mechanism as SEARCH-R1 during data generation; selected trajectories include retrieved passages.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved passages embedded in saved trajectories used for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>E5 retriever, top-3 passages during trajectory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Rejection Sampling (Qwen2.5-7B) Avg EM = 0.348 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with SEARCH-R1 and other baselines; rejection sampling yields improvements over many baselines but underperforms SEARCH-R1 which optimizes policies end-to-end with RL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Filtering search-enabled rollouts for correct answers provides a strong supervised baseline, but learned RL policies (SEARCH-R1) still outperform this method.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on obtaining enough correct candidate trajectories; may not scale or generalize as well as RL-optimized policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8448.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where language models teach themselves to use external tools (including search) via supervised fine-tuning on generated tool-usage annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Toolformer: Language models can teach themselves to use tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Toolformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A model that learns to call external tools (e.g., search APIs) by labeling training data with tool call placements and fine-tuning; cited in related work as an alternative to prompting or RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>tool-mediated external information (search-as-tool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Supervised fine-tuning with inserted tool call tokens; tools are called during inference when model emits tool tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External tool outputs / API results inserted into context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Tool calls triggered by model tokens; retrieval mechanism depends on the tool.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned as a related supervised approach for tool use; not experimentally compared in this paper's results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Toolformer demonstrates supervised routes to tool usage, but such approaches require high-quality labeled trajectories. SEARCH-R1 offers an RL alternative that does not rely on large supervised tool-use datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires labeled trajectories or heuristics to insert tool calls for supervised training; scalability to multi-turn retrieval scenarios can be challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8448.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8448.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (Reasoning and Acting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that interleaves chain-of-thought reasoning with environment/tool actions to solve tasks requiring external interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A method that encourages LLMs to generate both reasoning traces and explicit action/tool calls (e.g., search) in an interleaved manner using prompting; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning + tool use (conceptual)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external tool / retrieval as memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Prompt-driven interleaving of thought tokens and action tokens that trigger external operations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Tool outputs or retrieved text returned to the model as context.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Action-triggered API/tool calls; retrieval mechanism depends on tool.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work; not directly evaluated in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt-based interleaving of reasoning and actions is effective in some scenarios, but may generalize poorly if the LLM was not pretrained on such behaviors; SEARCH-R1 demonstrates training-based RL can learn more robust search interaction policies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompting may be suboptimal when the model was not exposed to similar tool-use patterns during pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrievalaugmented generation for large language models. <em>(Rating: 2)</em></li>
                <li>Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Search-o1: Agentic search-enhanced large reasoning models. <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8448",
    "paper_id": "paper-1fcaafeb72142fe3d1a5d698a072d69778d244b0",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "SEARCH-R1",
            "name_full": "SEARCH-R1 (Search-augmented Reinforcement Learning for Reasoning)",
            "brief_description": "An RL framework that trains LLMs to interleave step-by-step reasoning with multi-turn real-time search engine calls, using retrieved-token loss masking and an outcome-based reward (exact-match) to stabilize optimization and improve retrieval-driven decision-making.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SEARCH-R1",
            "agent_description": "An agentic LLM policy trained with RL (PPO or GRPO) to generate reasoning tokens and explicit &lt;search&gt; queries; retrieved passages are injected into the rollout between &lt;information&gt; tags and the final answer is produced within &lt;answer&gt; tags. Training applies retrieved-token masking so gradients only update model-generated tokens.",
            "model_name": "Qwen2.5-7B / Qwen2.5-3B (experiments)",
            "model_description": "Qwen-2.5 family LLMs used in both base and instruction-tuned variants; experiments on 3B, 7B (and additional 14B in appendix).",
            "task_name": "Seven QA datasets (NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle)",
            "task_description": "Open-domain and multi-hop question answering benchmarks requiring external factual knowledge and multi-step reasoning; agent must produce final answer strings (evaluated via Exact Match).",
            "task_type": "question answering / multi-step reasoning (retrieval-augmented)",
            "memory_used": true,
            "memory_type": "external retrieval / retrieval-augmented memory (search engine)",
            "memory_mechanism": "Agent emits &lt;search&gt;query&lt;/search&gt; tokens during rollout; system issues the query to a search engine (E5 retriever over Wikipedia), inserts top-k retrieved passages between &lt;information&gt;...&lt;/information&gt; into the model context, and the agent continues reasoning; retrieved-token loss masking prevents optimizing retrieved tokens.",
            "memory_representation": "External documents / retrieved passages (text snippets) returned by the search engine and appended to the generation sequence.",
            "memory_retrieval_method": "Semantic dense retrieval via E5 retriever over Wikipedia; top-k retrieval (k studied with values 1,3,5; default k=3) and appended by simple concatenation into the prompt context.",
            "performance_with_memory": "Qwen2.5-7B-base Search-R1-base Avg EM = 0.431 (Exact Match average across seven datasets); per-table values (NQ 0.480, TriviaQA 0.638, PopQA 0.457, HotpotQA 0.433, 2Wiki 0.382, Musique 0.196, Bamboogle 0.432).",
            "performance_without_memory": "R1 (RL without search) baseline Qwen2.5-7B-base Avg EM = 0.276 (per Table 2); RAG baseline Avg EM = 0.304. (See paper Tables 2 and 3 for full numbers.)",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Multiple ablations and comparisons: (1) Retrieved-token loss masking: with masking Avg EM 0.431 vs without masking 0.343 (major improvement). (2) Comparison to RAG and R1: SEARCH-R1 improves average relative performance by ~24% over RAG for Qwen2.5-7B and ~20% for Qwen2.5-3B. (3) Number of retrieved passages: top-k=3 outperformed top-k=1 and k=5 (k=3 gave best final Avg EM). (4) RL algorithm comparison: PPO vs GRPO (PPO more stable, GRPO faster but can collapse). (5) GRPO group-size ablation: larger group sizes accelerate convergence but risk instability; size=1 sometimes generalizes better. (6) Rejection sampling baseline (using search-enabled rollouts) yields intermediate gains (Avg EM 0.348 for 7B).",
            "key_findings": "Interleaving retrieval with RL-trained reasoning substantially improves QA performance over retrieval-agnostic RL and RAG baselines; retrieved-token loss masking is critical for stable and effective training; top-k=3 is a good retrieval budget trade-off; PPO provides more stable RL optimization while GRPO can converge faster but be unstable.",
            "limitations_or_challenges": "Training stability issues (GRPO can collapse), sensitivity to noisy retrieved passages (top-k large can hurt training), need to mask retrieved tokens to avoid unintended optimization, non-differentiability of search requiring RL-style optimization, computation and rollout costs for multi-turn retrieval, and current reward is limited to simple outcome-based (EM) signals.",
            "uuid": "e8448.0",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "R1 / DeepSeek-R1",
            "name_full": "DeepSeek-R1 (R1) - RL for reasoning without retrieval",
            "brief_description": "An RL framework previously proposed to incentivize reasoning capabilities in LLMs using outcome-based rewards; in this paper R1 refers to the RL method trained without access to an external search engine and used as a baseline.",
            "citation_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "mention_or_use": "use",
            "agent_name": "R1 (DeepSeek-R1)",
            "agent_description": "An RL-trained LLM policy optimized with outcome-based rewards to improve parametric reasoning (no external search calls); used here as a baseline to compare against search-augmented RL.",
            "model_name": "Qwen2.5-7B / Qwen2.5-3B (as evaluated in paper)",
            "model_description": "Same family of Qwen-2.5 models used for fair comparison; both base and instruct variants evaluated.",
            "task_name": "Same seven QA datasets (NQ, TriviaQA, PopQA, HotpotQA, 2WikiMultiHopQA, Musique, Bamboogle)",
            "task_description": "Question answering and multi-hop reasoning tasks evaluated with Exact Match.",
            "task_type": "question answering / multi-step reasoning",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "R1-base (Qwen2.5-7B-base) Avg EM = 0.276 (Table 2); R1-instruct Avg EM = 0.271.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared directly to SEARCH-R1 to quantify benefit of retrieval: SEARCH-R1 outperforms R1 (R1 lacks external memory), showing gains from adding search to RL-trained reasoning.",
            "key_findings": "RL tuning purely for parametric reasoning without retrieval (R1) yields improvements, but adding an external retrieval channel (SEARCH-R1) gives substantial additional gains on knowledge-intensive QA tasks.",
            "limitations_or_challenges": "Without access to external up-to-date evidence, R1 is limited by model parametric knowledge and cannot recover facts not present in the weights.",
            "uuid": "e8448.1",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation (RAG)",
            "brief_description": "A retrieval-then-generate pipeline that retrieves passages based on the input and conditions the LLM on concatenated retrieved text for generation.",
            "citation_title": "Retrievalaugmented generation for large language models.",
            "mention_or_use": "use",
            "agent_name": "RAG",
            "agent_description": "A standard retrieval-augmented baseline where the retriever returns passages based on the input query, concatenated into model context for single-round generation (no iterative search during reasoning).",
            "model_name": "Evaluated with Qwen2.5-7B / Qwen2.5-3B in experiments",
            "model_description": "Same retrieval (E5) and knowledge corpus (Wikipedia) used to ensure fair comparison.",
            "task_name": "Same seven QA datasets",
            "task_description": "Knowledge-intensive QA; RAG retrieves top-3 passages for the prompt then generates answers.",
            "task_type": "question answering / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "retrieval-augmented memory (single-round retrieval)",
            "memory_mechanism": "Retriever returns top-k passages for the input prompt which are concatenated to the LLM context before generation.",
            "memory_representation": "Retrieved passages/documents appended to prompt context.",
            "memory_retrieval_method": "Semantic dense retrieval via E5 over Wikipedia; default top-k=3 in experiments.",
            "performance_with_memory": "Qwen2.5-7B RAG Avg EM = 0.304 (Table 2); per-dataset numbers in paper.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared against SEARCH-R1: SEARCH-R1 yields ~24% relative improvement over RAG on 7B model (and ~20% on 3B) under identical retrieval, corpus, and pre-trained LLM settings.",
            "key_findings": "Standard one-shot retrieval (RAG) is effective but SEARCH-R1's learned multi-turn retrieval-policy and RL optimization outperform RAG across in- and out-of-distribution QA benchmarks.",
            "limitations_or_challenges": "RAG's single retrieval stage can miss relevant context for multi-step reasoning; cannot adapt queries dynamically during reasoning as SEARCH-R1 can.",
            "uuid": "e8448.2",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "IRCoT",
            "name_full": "IRCoT (Interleaved Retrieval with Chain-of-Thought)",
            "brief_description": "A prompting-based method that interleaves retrieval with chain-of-thought style reasoning via prompting (not RL), used here as a baseline.",
            "citation_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
            "mention_or_use": "use",
            "agent_name": "IRCoT",
            "agent_description": "A prompting approach that instructs the LLM to alternate between internal chain-of-thought reasoning and issuing retrieval queries; relies on prompting rather than RL to control retrieval steps.",
            "model_name": "Evaluated with instruct models (as baseline in experiments)",
            "model_description": "Prompted instruct LLM variants (Qwen2.5 instruct used for inference baselines).",
            "task_name": "Same QA datasets (multi-step / knowledge-intensive QA)",
            "task_description": "Multi-step question answering where iterative retrieval and reasoning are helpful.",
            "task_type": "question answering / multi-step reasoning",
            "memory_used": true,
            "memory_type": "external retrieval (prompted iterative retrieval)",
            "memory_mechanism": "Prompting instructs the model to produce queries and use retrieved passages in subsequent reasoning steps; retrieval is not learned via RL.",
            "memory_representation": "Retrieved passages appended to context as extra information.",
            "memory_retrieval_method": "Retriever (E5) top-k passages appended; retrieval strategy fixed by prompting.",
            "performance_with_memory": "IRCoT (Qwen2.5-7B instruct) Avg EM = 0.239 (Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared as a baseline to SEARCH-R1; SEARCH-R1 (RL-trained retrieval policy) outperforms IRCoT, indicating that training retrieval behavior yields gains over prompting alone.",
            "key_findings": "Prompting-based interleaved retrieval helps, but optimizing retrieval behavior with RL (SEARCH-R1) provides further improvements in QA performance.",
            "limitations_or_challenges": "Prompting cannot adapt retrieval strategy via learning; may be suboptimal if the LLM lacks pretraining exposure to iterative tool usage.",
            "uuid": "e8448.3",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Search-o1",
            "name_full": "Search-o1 (Agentic search-enhanced reasoning)",
            "brief_description": "An agentic baseline that enhances reasoning with search; included in experiments as a retrieval-enabled baseline.",
            "citation_title": "Search-o1: Agentic search-enhanced large reasoning models.",
            "mention_or_use": "use",
            "agent_name": "Search-o1",
            "agent_description": "A search-enhanced reasoning model that treats search as a tool to improve reasoning; used here as an experimental baseline.",
            "model_name": "Evaluated with Qwen2.5 models in the paper",
            "model_description": "Agentic search-enabled reasoning baseline; exact internal training protocol not detailed in this paper as it is an external baseline.",
            "task_name": "Same seven QA datasets",
            "task_description": "Knowledge-intensive QA where search is used to supply external facts during generation.",
            "task_type": "question answering / search-as-tool",
            "memory_used": true,
            "memory_type": "external retrieval / search tool",
            "memory_mechanism": "Search used as a tool during inference (details in cited paper); here it is a baseline that retrieves passages for generation.",
            "memory_representation": "Retrieved passages appended to context.",
            "memory_retrieval_method": "Semantic retrieval (E5) over Wikipedia, top-3 passages used for fair comparison.",
            "performance_with_memory": "Search-o1 (Qwen2.5-7B) Avg EM = 0.206 (Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared as a baseline; SEARCH-R1 outperforms Search-o1 under the same retrieval and model settings.",
            "key_findings": "While agentic search baselines help, SEARCH-R1's RL optimization of multi-turn search behavior yields superior performance.",
            "limitations_or_challenges": "Not analyzed in detail in this paper; treated as an external baseline for comparison.",
            "uuid": "e8448.4",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Rejection Sampling (search-enabled)",
            "name_full": "Rejection Sampling with search-enabled rollouts",
            "brief_description": "A baseline fine-tuning approach that generates multiple candidate search-enabled trajectories and selects those producing correct final answers to form a training set.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Rejection Sampling (with search)",
            "agent_description": "Generates multiple candidate responses per prompt with the instructed LLM (using the multi-turn LLM-search rollout mechanism), keeps trajectories whose final answers are correct, and uses them for supervised fine-tuning.",
            "model_name": "Qwen2.5 variants (as used in experiments)",
            "model_description": "Same retriever and corpus; five candidate responses generated per prompt in the described protocol.",
            "task_name": "Same seven QA datasets",
            "task_description": "Uses search-enabled rollouts to create a filtered supervised dataset for fine-tuning; evaluated on QA EM.",
            "task_type": "question answering / supervised fine-tuning with search-enabled trajectories",
            "memory_used": true,
            "memory_type": "retrieval-augmented (search-enabled trajectories used as training data)",
            "memory_mechanism": "Uses the same multi-turn search-rollout mechanism as SEARCH-R1 during data generation; selected trajectories include retrieved passages.",
            "memory_representation": "Retrieved passages embedded in saved trajectories used for fine-tuning.",
            "memory_retrieval_method": "E5 retriever, top-3 passages during trajectory generation.",
            "performance_with_memory": "Rejection Sampling (Qwen2.5-7B) Avg EM = 0.348 (Table 2).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared with SEARCH-R1 and other baselines; rejection sampling yields improvements over many baselines but underperforms SEARCH-R1 which optimizes policies end-to-end with RL.",
            "key_findings": "Filtering search-enabled rollouts for correct answers provides a strong supervised baseline, but learned RL policies (SEARCH-R1) still outperform this method.",
            "limitations_or_challenges": "Relies on obtaining enough correct candidate trajectories; may not scale or generalize as well as RL-optimized policies.",
            "uuid": "e8448.5",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Toolformer",
            "name_full": "Toolformer",
            "brief_description": "A method where language models teach themselves to use external tools (including search) via supervised fine-tuning on generated tool-usage annotations.",
            "citation_title": "Toolformer: Language models can teach themselves to use tools.",
            "mention_or_use": "mention",
            "agent_name": "Toolformer",
            "agent_description": "A model that learns to call external tools (e.g., search APIs) by labeling training data with tool call placements and fine-tuning; cited in related work as an alternative to prompting or RL.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": null,
            "memory_used": true,
            "memory_type": "tool-mediated external information (search-as-tool)",
            "memory_mechanism": "Supervised fine-tuning with inserted tool call tokens; tools are called during inference when model emits tool tokens.",
            "memory_representation": "External tool outputs / API results inserted into context.",
            "memory_retrieval_method": "Tool calls triggered by model tokens; retrieval mechanism depends on the tool.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned as a related supervised approach for tool use; not experimentally compared in this paper's results.",
            "key_findings": "Toolformer demonstrates supervised routes to tool usage, but such approaches require high-quality labeled trajectories. SEARCH-R1 offers an RL alternative that does not rely on large supervised tool-use datasets.",
            "limitations_or_challenges": "Requires labeled trajectories or heuristics to insert tool calls for supervised training; scalability to multi-turn retrieval scenarios can be challenging.",
            "uuid": "e8448.6",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (Reasoning and Acting)",
            "brief_description": "A prompting paradigm that interleaves chain-of-thought reasoning with environment/tool actions to solve tasks requiring external interactions.",
            "citation_title": "React: Synergizing reasoning and acting in language models.",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "A method that encourages LLMs to generate both reasoning traces and explicit action/tool calls (e.g., search) in an interleaved manner using prompting; cited as related work.",
            "model_name": null,
            "model_description": null,
            "task_name": null,
            "task_description": null,
            "task_type": "reasoning + tool use (conceptual)",
            "memory_used": true,
            "memory_type": "external tool / retrieval as memory",
            "memory_mechanism": "Prompt-driven interleaving of thought tokens and action tokens that trigger external operations.",
            "memory_representation": "Tool outputs or retrieved text returned to the model as context.",
            "memory_retrieval_method": "Action-triggered API/tool calls; retrieval mechanism depends on tool.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work; not directly evaluated in the experiments.",
            "key_findings": "Prompt-based interleaving of reasoning and actions is effective in some scenarios, but may generalize poorly if the LLM was not pretrained on such behaviors; SEARCH-R1 demonstrates training-based RL can learn more robust search interaction policies.",
            "limitations_or_challenges": "Prompting may be suboptimal when the model was not exposed to similar tool-use patterns during pretraining.",
            "uuid": "e8448.7",
            "source_info": {
                "paper_title": "Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrievalaugmented generation for large language models.",
            "rating": 2
        },
        {
            "paper_title": "Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Search-o1: Agentic search-enhanced large reasoning models.",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 1
        }
    ],
    "cost": 0.019632499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</h1>
<p>Bowen Jin ${ }^{1}$, Hansi Zeng ${ }^{2}$, Zhenrui Yue ${ }^{1}$, Jinsung Yoon ${ }^{3}$, Sercan . Ark ${ }^{3}$, Dong Wang ${ }^{1}$, Hamed Zamani ${ }^{2}$, Jiawei Han ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, University of Illinois at Urbana-Champaign<br>${ }^{2}$ Center for Intelligent Information Retrieval, University of Massachusetts Amherst<br>${ }^{3}$ Google Cloud AI Research<br>{bowenj4,zhenrui3,dwang24,hanj}@illinois.edu, {hzeng, zamani}@cs.umass.edu<br>{jinsungyoon, soarik}@google.com</p>
<h4>Abstract</h4>
<p>Efficiently acquiring external knowledge and up-to-date information is essential for effective reasoning and text generation in large language models (LLMs). Prompting advanced LLMs with reasoning capabilities to use search engines during inference is often suboptimal, as the LLM might not fully possess the capability on how to interact optimally with the search engine. This paper introduces SEARCH-R1, an extension of reinforcement learning (RL) for reasoning frameworks where the LLM learns to autonomously generate (multiple) search queries during step-by-step reasoning with real-time retrieval. SEARCH-R1 optimizes LLM reasoning trajectories with multi-turn search interactions, leveraging retrieved token masking for stable RL training and a simple outcome-based reward function. Experiments on seven question-answering datasets show that SEARCH-R1 improves performance by 24\% (Qwen2.5$7 B$ ) and $20 \%$ (Qwen2.5-3B) over various RAG baselines under the same setting. This paper further provides empirical insights into RL optimization methods, LLM choices, and response length dynamics in retrievalaugmented reasoning. The code and model checkpoints are available at https://github.com/PeterGriffinJin/Search-R1.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation (Hendrycks et al., 2020; Clark et al., 2018). Despite these achievements, LLMs often encounter challenges when tasked with complex reasoning (Wei et al., 2022) and retrieving up-to-date information from external sources (Jin et al., 2024). Addressing these limitations necessitates integrating advanced reasoning abilities (Huang \&amp; Chang, 2022) and the capability to interact effectively with search engines to best utilize external up-to-date information (Schick et al., 2023).
Existing approaches for integrating LLMs with search engines typically fall into two categories: (1) retrieval-augmented generation (RAG) (Gao et al., 2023; Lewis et al., 2020) and (2) treating the search engine as a tool (Yao et al., 2023; Schick et al., 2023). RAG models often retrieve passages based on the LLM input as query and incorporate them into the LLM's context for generation (Lewis et al., 2020). This allows the LLM to leverage external knowledge when answering questions. Although existing work (Trivedi et al., 2022a) prompts LLM for multi-turn, multi-query retrieval, this approach is suboptimal because the LLM is not optimized to learn how to interact effectively with search engines during training. Alternatively, LLMs can be prompted or trained to utilize tools, including search engines, as part of their reasoning process (Qu et al., 2025; Trivedi et al., 2022a). However, prompting-based approaches often struggle to generalize, as certain tasks may not have been encountered during LLM pretraining. On the other hand, training-based</p>
<p>approaches offer greater adaptability but are difficult to scale effectively due to their reliance on large-scale, high-quality annotated trajectories and the inherent non-differentiability of the search operation, which renders end-to-end gradient descent-based optimization inapplicable (Schick et al., 2023; Asai et al., 2024).
Reinforcement Learning (RL) (Sutton et al., 1999; Kaelbling et al., 1996) has emerged as a potent paradigm for enhancing the reasoning capabilities of LLMs (Guo et al., 2025; Hou et al., 2025; Xie et al., 2025; Kumar et al., 2024). Notably, models like OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025) have leveraged RL techniques (e.g., PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024)) to improve logical inference and problem-solving skills by learning from experience and feedback. With RL, even when trained solely on the outcome rewards, the models learn complex reasoning capabilities, including selfverification (Weng et al., 2022) and self-correction (Kumar et al., 2024). However, applying RL to search-and-reasoning scenarios presents three key challenges: (1) RL Framework and Stability - It remains unclear how to effectively integrate the search engine into the RL approaches for LLMs while ensuring stable optimization, particularly when incorporating retrieved context. (2) Multi-Turn Interleaved Reasoning and Search - Ideally, the LLM should be capable of iterative reasoning and search engine calls, dynamically adjusting the retrieval strategy based on the complexity of the problem. (3) Reward Design - Designing an effective reward function for search and reasoning tasks remains a fundamental challenge, as it is unclear whether simple outcome-based rewards are sufficient to guide the LLM to learn meaningful and consistent search behaviors.
To address aforementioned challenges, we introduce SEARCH-R1, a novel RL framework that enables LLMs to interact with search engines in an interleaved manner with their own reasoning. Specifically, SEARCH-R1 introduces the following key innovations: (1) We model the search engine as part of the environment, enabling sampled trajectory sequences that interleave LLM token generation with search engine retrievals. SEARCH-R1 is compatible with various RL algorithms, including PPO and GRPO, and we apply retrieved token masking to ensure stable optimization. (2) SEARCH-R1 supports multi-turn retrieval and reasoning, invoking search calls when explicitly triggered by <search> and </search> tokens. Retrieved content is enclosed within <information> and </information> tokens, while LLM reasoning steps are wrapped within <think> and </think> tokens. The final answer is formatted using <answer> and </answer> tokens, allowing for structured, iterative decision-making. (3) We adopt a straightforward outcome-based reward function, avoiding the complexity of process-based rewards. Our results demonstrate that this minimal reward design is effective in search-and-reasoning scenarios. As such, SEARCH-R1 can be viewed as an extension of DeepSeek-R1 Zero (Guo et al., 2025), which primarily focuses on parametric reasoning by introducing search-augmented RL training for enhanced retrieval-driven decision-making.
In summary, our key contributions are threefold:</p>
<ul>
<li>Our work analyzes the challenges and provides perspectives on implementing RL to improve how LLMs reason using search engine results.</li>
<li>We propose SEARCH-R1, a novel RL framework that supports LLM rollouts and direct optimization with a search engine, including retrieved token masking to stabilize RL training, multi-turn interleaved reasoning and search to support complex task-solving and an effective outcome reward function.</li>
<li>We conduct systematic experiments to demonstrate the effectiveness of SEARCH-R1, with two LLMs achieving respective average relative improvements of $41 \%$ and $20 \%$ over RAG baselines under the same experimental setup (e.g., same retrieval model, training data, and pre-trained LLMs). In addition, we provide insights on RL for reasoning and search settings, including RL method selection, different LLM choices, and response length study.</li>
</ul>
<h1>2 Related Works</h1>
<h3>2.1 Large Language Models and Retrieval</h3>
<p>Despite demonstrating remarkable reasoning (Guo et al., 2025) and coding (Guo et al., 2024) capabilities, LLMs (Zhao et al., 2023; Team, 2024; Achiam et al., 2023) often lack domainspecific knowledge (Peng et al., 2023; Li et al., 2023) and are prone to hallucinations (Zhang et al., 2023). To mitigate these limitations, search engines (Zhao et al., 2024) are widely integrated to supply external information. There are two primary ways to integrate search engines with LLMs: (1) retrieval-augmented generation (RAG) (Gao et al., 2023) and (2) treating the search engines as tools (Schick et al., 2023). RAG (Lewis et al., 2020; Yue et al., 2024; Xiong et al., 2025) typically follows a round of retrieval and sequential generation pipelines, where a search engine fetches relevant information based on the input query, which is then concatenated with the query and fed into the LLM. However, this could face challenges of retrieving irrelevant information (Jin et al., 2024) and failing to provide sufficiently useful context (Jiang et al., 2023). An alternative approach is search-as-a-tool, where LLMs are prompted or fine-tuned to interact with search engines. IRCoT (Trivedi et al., 2022a) and ReAct (Yao et al., 2023) use prompting to guide iterative reasoning and search engine calls, while Toolformer (Schick et al., 2023) leverages supervised fine-tuning to enhance search capabilities. However, such methods rely on high-quality labeled trajectories, which are difficult to obtain at scale. Recent work (Guo et al., 2025) suggests that RL can enable LLMs to develop advanced reasoning skills using only outcome rewards, yet its potential in search engine calling scenarios remains under-explored.</p>
<h3>2.2 Large Language Models and Reinforcement Learning</h3>
<p>Reinforcement learning (RL) (Kaelbling et al., 1996) is a learning paradigm where an agent learns to make sequential decisions by interacting with an environment and receiving feedback in the form of rewards, aiming to maximize cumulative reward over time (Sutton et al., 1999). RL was introduced to LLM tuning by Ouyang et al. (2022) through RL from human feedback (RLHF) (Kaufmann et al., 2023). This approach first trains a reward model using human preference data (Lambert et al., 2024), which then guides RL-based tuning of the policy LLM, typically via Proximal Policy Optimization (PPO). However, PPO involves multiple rounds of LLM optimization, making it challenging to implement. To simplify RL-based tuning, direct optimization methods such as Direct Preference Optimization (DPO) (Rafailov et al., 2023) and SimPO (Meng et al., 2024) have been proposed. A similar approach is employed in LeRet (Hsu et al., 2024), where LLMs are trained to explore diverse queries to enhance the effectiveness of information retrieval. While these methods offer computational efficiency, they suffer from off-policy issues (Pang et al., 2024) and do not consistently match the performance of pure RL approaches. Alternative solutions include Group Relative Policy Optimization (GRPO) (Shao et al., 2024), which eliminates the need for a critic model by estimating baselines from group scores, and RLOO (Ahmadian et al., 2024), which introduces a simplified REINFORCE-style (Williams, 1992) optimization framework. Despite these advances, the application of RL to LLM-driven search engine interactions and reasoning remains largely unexplored.</p>
<h2>3 Search-R1</h2>
<p>In the following sections, we present the detailed design for training methods of SEARCH-R1, covering (1) extending RL to utilize search engines; (2) text generation with an interleaved multi-turn search engine call; (3) the training template; and (4) reward model design.</p>
<h3>3.1 Reinforcement Learning with a Search Engine</h3>
<p>We formulate the RL objective function utilizing a search engine $\mathcal{R}$ as follows:</p>
<p>$$
\max <em _theta="\theta">{\pi</em>}} \mathbb{E<em _theta="\theta">{x \sim \mathcal{D}, y \sim \pi</em>}(\cdot \mid x ; \mathcal{R})}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D<em _theta="\theta">{\mathrm{KL}}\left[\pi</em>)\right]
$$}(y \mid x ; \mathcal{R}) | \pi_{\mathrm{ref}}(y \mid x ; \mathcal{R</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Demonstration of PPO and GRPO training with the search engine (SEARCH-R1). During the rollout, LLMs can conduct multi-turn interactions with the search engine.
where $\pi_{\theta}$ is the policy $\mathrm{LLM}, \pi_{\text {ref }}$ is the reference $\mathrm{LLM}, r_{\phi}$ is the reward function and $\mathbb{D}<em _ref="{ref" _text="\text">{\mathrm{KL}}$ is KL-divergence measure. $x$ denote input samples drawn from the dataset $\mathcal{D}$, and $y$ represent the generated outputs interleaved with search engine calling results, sampled from the reference policy $\pi</em>$, where $\bigotimes$ denotes interleaved retrieval-and-reasoning. This enables more effective decision-making in reasoning-intensive tasks that require external information retrieval. An illustration of the rollout process and an explanation of Eq. 1 are provided in Section 3.2 and Appendix A.}}(y \mid x)$ and retrieved from the search engine $\mathcal{R}$. Unlike prior RL approaches that primarily rely on the policy LLM $\pi_{\theta}(\cdot \mid x)$ to generate rollout sequences (Rafailov et al., 2023; Ouyang et al., 2022), our framework explicitly incorporates retrieval interleaved reasoning via $\pi_{\theta}(\cdot \mid x ; \mathcal{R})$, which can be seen as $\pi_{\theta}(\cdot \mid x) \bigotimes \mathcal{R</p>
<p>Our approach builds upon two well-established policy gradient RL methods: Proximal Policy Optimization (PPO) (Schulman et al., 2017) and Group Relative Policy Optimization (GRPO) (Shao et al., 2024; Guo et al., 2025), leveraging their respective advantages to optimize retrieval-augmented reasoning.</p>
<p>Loss Masking for Retrieved Tokens. In both PPO and GRPO, the token-level losses are computed over the entire rollout sequence. In SEARCH-R1, the rollout sequence consists of both LLM-generated tokens and retrieved tokens from external passages. While optimizing LLM-generated tokens enhances the model's ability to interact with the search engine and perform reasoning, applying the same optimization to retrieved tokens can lead to unintended learning dynamics. To address this, we introduce loss masking for retrieved tokens, ensuring the policy gradient objective is computed only over LLM-generated tokens, excluding retrieved content from the optimization process. This approach stabilizes training while preserving the flexibility of search-augmented generation.</p>
<p>PPO with Search Engine. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is a popular actor-critic RL approach commonly used for LLMs (Ouyang et al., 2022). For our reasoning scenarios that involve search engine calling, it optimizes LLMs by maximizing the following objective:</p>
<p>$$
\mathcal{J}<em _mathcal_D="\mathcal{D" _sim="\sim" x="x">{P P O}(\theta)=\mathbf{E}</em>\right)\right]
$$}, y \sim \pi_{\text {old }}(: \mid x ; \mathcal{R})}\left[\frac{1}{\sum_{i=1}^{2 \pi} I\left(y_{t}\right)} \sum_{i=1, \tau\left(y_{t}\right)=1}^{2 \pi} \min \left(\frac{\pi_{\theta}\left(y_{t} \mid x, y_{&lt;t} ; \mathcal{R}\right)}{\pi_{\text {old }}\left(y_{t} \mid x, y_{&lt;t} ; \mathcal{R}\right)} A_{t}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(y_{t} \mid x, y_{&lt;t} ; \mathcal{R}\right)}{\pi_{\text {old }}\left(y_{t} \mid x, y_{&lt;t} ; \mathcal{R}\right)}, 1-\epsilon, 1+\epsilon\right) A_{t</p>
<p>where $\pi_{\theta}$ and $\pi_{\text {old }}$ represent the current and previous policy models, respectively. $I\left(y_{t}\right)$ is the token loss masking operation such that $I\left(y_{t}\right)=1$ if $y_{t}$ is a LLM generated token and $I\left(y_{t}\right)=0$ if $y_{t}$ is a retrieved token. The term $\epsilon$ is a clipping-related hyperparameter introduced in PPO to stabilize training. The advantage estimate $A_{t}$ is computed using Generalized Advantage Estimation (GAE) (Schulman et al., 2015), based on future rewards $\left{r_{\geq t}\right}$ and a learned value function $V_{\phi}$.</p>
<p>GRPO with Search Engine. To improve policy optimization stability and avoid the need for an additional value function approximation, Group Relative Policy Optimization (GRPO) is introduced in Shao et al. (2024). GRPO differs from PPO by leveraging the average reward of multiple sampled outputs as a baseline rather than relying on a learned value function. Specifically, for each input question $x$, GRPO samples a group of responses $\left{y_{1}, y_{2}, \ldots, y_{G}\right}$ from the reference policy $\pi_{\text {ref }}$. The policy model is then optimized by maximizing the following objective function:</p>
<p>$$
\begin{aligned}
&amp; \mathcal{J}<em _Pi__1="\Pi_{1" _sim="\sim" x="x">{\text {GRPO }}(\theta)=\mathbb{E}</em>\right)}\left(y_{i<em _old="{old" _text="\text">{i=1}^{G}-\pi</em>}}(\cdot \mid x ; \mathcal{R})}\left[\frac{1}{G} \sum_{i=1}^{G} \frac{1}{\sum_{i=1}^{|y_{i}|} I\left(y_{i, t}\right)} \sum_{\tau=1: I\left(y_{i, t}\right)=1}^{\mid y_{i} \mid} \min \left(\frac{\pi_{\theta}\left(y_{i, t} \mid x, y_{i, \sim t} ; \mathcal{R}\right)}{\pi_{\text {old }}\left(y_{i, t} \mid x, y_{i, \sim t} ; \mathcal{R}\right)} \hat{A<em _theta="\theta">{i, t},\right.\right. \
&amp; \left.\operatorname{clip}\left(\frac{\pi</em>}\left(y_{i, t} \mid x, y_{i, \sim t} ; \mathcal{R}\right)}{\pi_{\text {old }}\left(y_{i, t} \mid x, y_{i, \sim t} ; \mathcal{R}\right)}, 1-\epsilon, 1+\epsilon\right) \hat{A<em K="K" L="L">{i, t}\right)-\beta \mathbb{D}</em>\right]\right],
\end{aligned}
$$}\left[\pi_{\theta} | \pi_{\text {ref }</p>
<p>where $\epsilon$ and $\beta$ are hyperparameters, and $\hat{A}<em i_="i," t="t">{i, t}$ represent the advantage, computed based on the relative rewards of outputs within each group. This approach avoids introducing additional complexity in the computation of $\hat{A}</em>$.}$. Additionally, instead of incorporating KL divergence as a penalty within the reward function, GRPO regularizes by directly adding the KL divergence between the trained policy and the reference policy to the loss function. The retrieved token masking is also applied when calculating the KL divergence loss $\mathbb{D}_{K L</p>
<h1>3.2 Generation with Multi-turn Search Engine Calling</h1>
<p>In this section, we describe the rollout process for LLM response generation with interleaved multi-turn search engine calls, formulated as: $y \sim \pi_{\theta}(\cdot \mid x ; \mathcal{R})=\pi_{\theta}(\cdot \mid x) \otimes \mathcal{R}$.
Our approach follows an iterative framework where the LLM alternates between text generation and external search engine queries. Specifically, the system instruction guides the LLM to encapsulate its search query between two designated search call tokens, <search> and </search>, whenever an external retrieval is needed. Upon detecting these tokens in the generated sequence, the system extracts the search query, queries the search engine, and retrieves relevant results. The retrieved information is then enclosed within special retrieval tokens, <information> and </information>, and appended to the ongoing rollout sequence, serving as additional context for the next generation step. This process continues iteratively until one of the following conditions is met: (1) the maximum number of action is reached, or (2) the model generates a final response, which is enclosed between designated answer tokens, <answer> and </answer>. The complete workflow is outlined in Algorithm 1.</p>
<h3>3.3 Training Template</h3>
<p>To train SEARCH-R1, we start by crafting a simple template that directs the initial LLM to follow our predefined instructions. As shown in Table 1, this template structures the model's output into three parts in an iterative fashion: first, a reasoning process, then a search engine calling function, and finally, the answer. We deliberately limit our constraints to this structural format, avoiding any content-specific biases, such as enforcing reflective reasoning and search engine calling or endorsing specific problem-solving approaches. This ensures that the model's natural learning dynamics during the RL process remain observable and unbiased.</p>
<p>Answer the given question. You must conduct reasoning inside <think> and </think> first every time you get new information. After reasoning, if you find you lack some knowledge, you can call a search engine by <search> query </search>, and it will return the top searched results between <information> and </information>. You can search as many times as you want. If you find no further external knowledge needed, you can directly provide the answer inside <answer> and </answer> without detailed illustrations. For example, <answer> xxx </answer>. Question: question.</p>
<p>Table 1: Template for SEARCH-R1. question will be replaced with the specific question during training and inference.</p>
<div class="codehilite"><pre><span></span><code>Algorithm<span class="w"> </span>1<span class="w"> </span>LLM<span class="w"> </span>Response<span class="w"> </span>Rollout<span class="w"> </span>with<span class="w"> </span>Multi-Turn<span class="w"> </span>Search<span class="w"> </span>Engine<span class="w"> </span>Calls
Require:<span class="w"> </span>Input<span class="w"> </span>query<span class="w"> </span>\(x\),<span class="w"> </span>policy<span class="w"> </span>model<span class="w"> </span>\(\pi_{\theta}\),<span class="w"> </span>search<span class="w"> </span>engine<span class="w"> </span>\(\mathcal{R}\),<span class="w"> </span>maximum<span class="w"> </span>action<span class="w"> </span>budget<span class="w"> </span>\(B\).
Ensure:<span class="w"> </span>Final<span class="w"> </span>response<span class="w"> </span>\(y\).
<span class="w">    </span>Initialize<span class="w"> </span>rollout<span class="w"> </span>sequence<span class="w"> </span>\(y<span class="w"> </span>\leftarrow<span class="w"> </span>\varnothing\)
<span class="w">    </span>Initialize<span class="w"> </span>action<span class="w"> </span>count<span class="w"> </span>\(b<span class="w"> </span>\leftarrow<span class="w"> </span>0\)
<span class="w">    </span>while<span class="w"> </span>\(b<span class="nt">&lt;B</span><span class="err">\)</span><span class="w"> </span><span class="err">do</span>
<span class="w">        </span><span class="err">Initialize</span><span class="w"> </span><span class="err">current</span><span class="w"> </span><span class="err">action</span><span class="w"> </span><span class="err">LLM</span><span class="w"> </span><span class="err">rollout</span><span class="w"> </span><span class="err">sequence</span><span class="w"> </span><span class="err">\(y_{b}</span><span class="w"> </span><span class="err">\leftarrow</span><span class="w"> </span><span class="err">\varnothing\)</span>
<span class="w">        </span><span class="err">while</span><span class="w"> </span><span class="err">True</span><span class="w"> </span><span class="err">do</span>
<span class="w">            </span><span class="err">Generate</span><span class="w"> </span><span class="err">response</span><span class="w"> </span><span class="err">token</span><span class="w"> </span><span class="err">\(y_{t}</span><span class="w"> </span><span class="err">\sim</span><span class="w"> </span><span class="err">\pi_{\theta}\left(\cdot</span><span class="w"> </span><span class="err">\mid</span><span class="w"> </span><span class="err">x,</span><span class="w"> </span><span class="err">y+y_{b}\right)\)</span>
<span class="w">            </span><span class="err">Append</span><span class="w"> </span><span class="err">\(y_{t}\)</span><span class="w"> </span><span class="err">to</span><span class="w"> </span><span class="err">rollout</span><span class="w"> </span><span class="err">sequence</span><span class="w"> </span><span class="err">\(y_{b}</span><span class="w"> </span><span class="err">\leftarrow</span><span class="w"> </span><span class="err">y_{b}+y_{t}\)</span>
<span class="w">            </span><span class="err">if</span><span class="w"> </span><span class="err">\(y_{t}\)</span><span class="w"> </span><span class="err">in</span><span class="w"> </span><span class="err">\([\langle/\)</span><span class="w"> </span><span class="err">search</span><span class="w"> </span><span class="err">\(\rangle,\langle/\)</span><span class="w"> </span><span class="err">answer</span><span class="w"> </span><span class="err">\(\rangle,\langle\)</span><span class="w"> </span><span class="err">eos</span><span class="w"> </span><span class="err">\(\rangle]\)</span><span class="w"> </span><span class="err">then</span><span class="w"> </span><span class="err">break</span>
<span class="w">            </span><span class="err">end</span><span class="w"> </span><span class="err">if</span>
<span class="w">        </span><span class="err">end</span><span class="w"> </span><span class="err">while</span>
<span class="w">        </span><span class="err">\(y</span><span class="w"> </span><span class="err">\leftarrow</span><span class="w"> </span><span class="err">y+y_{b}\)</span>
<span class="w">        </span><span class="err">if</span><span class="w"> </span><span class="err">&lt;search</span><span class="nt">&gt;</span><span class="w"> </span><span class="nt">&lt;/search&gt;</span><span class="w"> </span>detected<span class="w"> </span>in<span class="w"> </span>\(y_{b}\)<span class="w"> </span>then
<span class="w">            </span>Extract<span class="w"> </span>search<span class="w"> </span>query<span class="w"> </span>\(q<span class="w"> </span>\leftarrow\)<span class="w"> </span>Parse<span class="w"> </span>\(\left(y_{b},<span class="err">&lt;</span>\right.\)<span class="w"> </span>search<span class="w"> </span>\(\left.\rangle,<span class="err">&lt;</span>/\)<span class="w"> </span>search<span class="w"> </span>\(\rangle)\)
<span class="w">            </span>Retrieve<span class="w"> </span>search<span class="w"> </span>results<span class="w"> </span>\(d=\mathcal{R}(q)\)
<span class="w">            </span>Insert<span class="w"> </span>\(d\)<span class="w"> </span>into<span class="w"> </span>rollout<span class="w"> </span>\(y<span class="w"> </span>\leftarrow<span class="w"> </span>y+\langle\)<span class="w"> </span>information<span class="w"> </span>\(&gt;\boldsymbol{d}\langle/\)<span class="w"> </span>information<span class="w"> </span>\(\rangle\)
<span class="w">        </span>else<span class="w"> </span>if<span class="w"> </span><span class="nt">&lt;answer&gt;</span><span class="w"> </span><span class="nt">&lt;/answer&gt;</span><span class="w"> </span>detected<span class="w"> </span>in<span class="w"> </span>\(y_{b}\)<span class="w"> </span>then
<span class="w">            </span>return<span class="w"> </span>final<span class="w"> </span>generated<span class="w"> </span>response<span class="w"> </span>\(y\)
<span class="w">        </span>else
<span class="w">            </span>Ask<span class="w"> </span>for<span class="w"> </span>rethink<span class="w"> </span>\(y<span class="w"> </span>\leftarrow<span class="w"> </span>y+\)<span class="w"> </span>&quot;My<span class="w"> </span>action<span class="w"> </span>is<span class="w"> </span>not<span class="w"> </span>correct.<span class="w"> </span>Let<span class="w"> </span>me<span class="w"> </span>rethink.&quot;
<span class="w">        </span>end<span class="w"> </span>if
<span class="w">        </span>Increment<span class="w"> </span>action<span class="w"> </span>count<span class="w"> </span>\(b<span class="w"> </span>\leftarrow<span class="w"> </span>b+1\)
<span class="w">    </span>end<span class="w"> </span>while
<span class="w">    </span>return<span class="w"> </span>final<span class="w"> </span>generated<span class="w"> </span>response<span class="w"> </span>\(y\)
</code></pre></div>

<h1>3.4 Reward Modeling</h1>
<p>The reward function serves as the primary training signal, guiding the optimization process in RL. To train SEARCH-R1, we adopt a rule-based reward system that consists solely of final outcome rewards, which assess the correctness of the model's response. For instance, in factual reasoning tasks, correctness can be evaluated using rule-based criteria such as exact string matching:</p>
<p>$$
r_{\phi}(x, y)=\operatorname{EM}\left(a_{\text {pred }}, a_{\text {gold }}\right)
$$</p>
<p>where $a_{\text {pred }}$ is the extracted final answer from response $y$ and $a_{\text {gold }}$ is the ground truth answer. Unlike Guo et al. (2025), we do not incorporate format rewards, as our learned model already demonstrates strong structural adherence. We leave the exploration of more complex format rewards for future work. Furthermore, we avoid training neural reward models, following Guo et al. (2025). This decision is motivated by the sensitivity of LLMs to specific forms of rewards in large-scale RL, as well as the additional computational cost and complexity introduced by retraining these models.</p>
<h2>4 Main Results</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate SEARCH-R1 on seven benchmark datasets, categorized as follows: (1) General Question Answering: NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and PopQA (Mallen et al., 2022). (2) Multi-Hop Question Answering: HotpotQA (Yang et al., 2018), 2WikiMultiHopQA (Ho et al., 2020), Musique (Trivedi et al., 2022b), and Bamboogle (Press et al., 2022). These datasets encompass a diverse range of search with reasoning challenges, enabling a comprehensive evaluation of SEARCH-R1.</p>
<h1>4.2 Baselines</h1>
<p>To evaluate the effectiveness of SEARCH-R1, we compare it against the following baselines: (1) Inference without Retrieval: Direct inference and Chain-of-Thought (CoT) reasoning (Wei et al., 2022). (2) Inference with Retrieval: Retrieval-Augmented Generation (RAG) (Lewis et al., 2020), IRCoT (Trivedi et al., 2022a), and Search-o1 (Li et al., 2025). (3) Fine-Tuning-Based Methods: Supervised fine-tuning (SFT) (Chung et al., 2024), RL-based finetuning without a search engine (R1) (Guo et al., 2025) and rejection sampling (Ahn et al., 2024) with a search engine. For R1, we train the LLMs with the RL methods proposed in Guo et al. (2025) with our data to have a fair comparison with SEARCH-R1. It only contains reasoning and answer steps without a search engine. For rejection sampling, we generate five candidate responses per training prompt from the same dataset with the instructed LLMs and select those that lead to correct final answers. These selected trajectories are then used to construct a new training set that retains the same multi-turn LLM-search engine interaction rollout mechanism proposed in SEARCH-R1 to fine-tune the LLMs.
These baselines cover a broad spectrum of retrieval-augmented and fine-tuning approaches, allowing for a comprehensive assessment of SEARCH-R1 in both zero-shot and learned retrieval settings. To make a fair comparison between different methods, we use the same retriever, same number of retrieved documents, same knowledge corpus, same training data and same pre-trained LLMs. Details can be found in Appendix B.</p>
<h3>4.3 Experimental Setup</h3>
<p>We conduct experiments using two types of models: Qwen-2.5-3B (Base/Instruct) and Qwen-2.5-7B (Base/Instruct) (Yang et al., 2024). For retrieval, we use the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source and E5 (Wang et al., 2022) as the retriever. To ensure fair comparison, we follow Lin et al. (2023) and set the number of retrieved passages to 3 across all retrieval-based methods. A study of the number of retrieved passages can be found in Appendix G.
For training, we merge the training sets of NQ and HotpotQA to form a unified dataset for SEARCH-R1 and other fine-tuning based baselines. Evaluation is conducted on the test or validation sets of seven datasets to assess both in-domain and out-of-domain performance. Exact Match (EM) is used as the evaluation metric, following Yu et al. (2024). For inferencestyle baselines, we use instruct models, as base models fail to follow instructions. For RL tuning methods, experiments are conducted on both base and instruct models. More details on experimental settings can be found in Appendix B.
Unless stated otherwise, PPO is used as the default RL method, and a detailed comparison between PPO and GRPO is provided in Section 5.1.</p>
<h3>4.4 Performance</h3>
<p>The main results comparing SEARCH-R1 with baseline methods across the seven datasets are presented in Table 2. From the results, we make the following key observations: (1) SEARCH-R1 consistently outperforms strong baseline methods. We achieve $24 \%$ and $20 \%$ average relative improvement with Qwen2.5-7B and Qwen2.5-3B, respectively. These gains hold across both in-distribution evaluation (i.e., NQ and HotpotQA) and out-of-distribution evaluation (i.e., TriviaQA, PopQA, 2WikiMultiHopQA, Musique, and Bamboogle). (2) SEARCH-R1 surpasses RL-based training for LLM reasoning without retrieval (R1). This aligns with expectations, as incorporating search into LLM reasoning provides access to relevant external knowledge, improving overall performance. (3) SEARCH-R1 is effective for both base and instruction-tuned models. This demonstrates that DeepSeek-R1-Zerostyle RL with outcome-based rewards (Guo et al., 2025) can be successfully applied to reasoning with search, extending beyond its previously established effectiveness in pure reasoning scenarios. (4) Larger models are better on learning how to do search. SEARCHR1 on 7B model shows much larger "performance gap" compared with 3B model (e.g., compared with second best model - RAG).</p>
<p>Table 2: Main results. The best performance is set in bold. ${ }^{\dagger} /{ }^{\star}$ represents in-domain/outdomain datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">General QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi-Hop QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{NQ}^{\dagger}$</td>
<td style="text-align: center;">TriviaQA*</td>
<td style="text-align: center;">PopQA*</td>
<td style="text-align: center;">HotpotQA ${ }^{\dagger}$</td>
<td style="text-align: center;">2wiki*</td>
<td style="text-align: center;">Musique*</td>
<td style="text-align: center;">Bamboogle*</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-7b-Base/Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Direct Inference</td>
<td style="text-align: center;">0.134</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.140</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.031</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.181</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.048</td>
<td style="text-align: center;">0.185</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.022</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">0.106</td>
</tr>
<tr>
<td style="text-align: center;">IRCoT</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.478</td>
<td style="text-align: center;">0.301</td>
<td style="text-align: center;">0.133</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.239</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.443</td>
<td style="text-align: center;">0.131</td>
<td style="text-align: center;">0.187</td>
<td style="text-align: center;">0.176</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.206</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.392</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">0.235</td>
<td style="text-align: center;">0.058</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.304</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.354</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.207</td>
</tr>
<tr>
<td style="text-align: center;">R1-base</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.539</td>
<td style="text-align: center;">0.202</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.083</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.276</td>
</tr>
<tr>
<td style="text-align: center;">R1-instruct</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.537</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.237</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.072</td>
<td style="text-align: center;">0.293</td>
<td style="text-align: center;">0.271</td>
</tr>
<tr>
<td style="text-align: center;">Rejection Sampling</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.123</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">0.348</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-base</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.638</td>
<td style="text-align: center;">0.457</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">0.196</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.431</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-instruct</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.385</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-3b-Base/Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Direct Inference</td>
<td style="text-align: center;">0.106</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">0.020</td>
<td style="text-align: center;">0.024</td>
<td style="text-align: center;">0.134</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.032</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.021</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">0.015</td>
</tr>
<tr>
<td style="text-align: center;">IRCoT</td>
<td style="text-align: center;">0.111</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.164</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.067</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.181</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1</td>
<td style="text-align: center;">0.238</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.262</td>
<td style="text-align: center;">0.221</td>
<td style="text-align: center;">0.218</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.320</td>
<td style="text-align: center;">0.255</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">0.348</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.387</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.080</td>
<td style="text-align: center;">0.270</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">0.249</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.104</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.044</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.176</td>
</tr>
<tr>
<td style="text-align: center;">R1-base</td>
<td style="text-align: center;">0.226</td>
<td style="text-align: center;">0.455</td>
<td style="text-align: center;">0.173</td>
<td style="text-align: center;">0.201</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.055</td>
<td style="text-align: center;">0.224</td>
<td style="text-align: center;">0.229</td>
</tr>
<tr>
<td style="text-align: center;">R1-instruct</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.449</td>
<td style="text-align: center;">0.171</td>
<td style="text-align: center;">0.208</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.224</td>
</tr>
<tr>
<td style="text-align: center;">Rejection Sampling</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.332</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.233</td>
<td style="text-align: center;">0.059</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.265</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-base</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.435</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.303</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-instruct</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">0.319</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">0.264</td>
<td style="text-align: center;">0.325</td>
</tr>
</tbody>
</table>
<p>Table 3: The performance results of SEARCH-R1 with PPO and GRPO on seven datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;">2wiki</th>
<th style="text-align: center;">Musique</th>
<th style="text-align: center;">Bamboogle</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5-7b-Base/Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-base (GRPO)</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.350</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-instruct (GRPO)</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">0.623</td>
<td style="text-align: center;">0.427</td>
<td style="text-align: center;">0.386</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.162</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-base (PPO)</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 3}$</td>
<td style="text-align: center;">0.382</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 1}$</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-instruct (PPO)</td>
<td style="text-align: center;">0.393</td>
<td style="text-align: center;">0.610</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.370</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 4}$</td>
<td style="text-align: center;">0.146</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.385</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-3b-Base/Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-base (GRPO)</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 1}$</td>
<td style="text-align: center;">0.583</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.274</td>
<td style="text-align: center;">0.066</td>
<td style="text-align: center;">0.128</td>
<td style="text-align: center;">0.312</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-instruct (GRPO)</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.565</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 1}$</td>
<td style="text-align: center;">0.310</td>
<td style="text-align: center;">$\mathbf{0 . 1 2 4}$</td>
<td style="text-align: center;">0.232</td>
<td style="text-align: center;">$\mathbf{0 . 3 3 6}$</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-base (PPO)</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 5}$</td>
<td style="text-align: center;">0.284</td>
<td style="text-align: center;">0.273</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">0.303</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1-instruct (PPO)</td>
<td style="text-align: center;">0.341</td>
<td style="text-align: center;">0.545</td>
<td style="text-align: center;">0.378</td>
<td style="text-align: center;">0.324</td>
<td style="text-align: center;">$\mathbf{0 . 3 1 9}$</td>
<td style="text-align: center;">0.103</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 4}$</td>
<td style="text-align: center;">0.325</td>
</tr>
</tbody>
</table>
<h1>5 Analysis</h1>
<h3>5.1 Different RL methods: PPO vs. GRPO</h3>
<p>We evaluate SEARCH-R1 using both PPO and GRPO as the base RL method, conducting experiments on Qwen2.5-3B/7B models. The training dynamics comparison is presented in Figure 2(a) and the evaluation results are presented in Table 3, revealing the following insights: (1) GRPO converges faster than PPO across all cases. This is because PPO relies on a critic model, which requires several warm-up steps before effective training begins. (2) PPO demonstrates greater training stability. As shown in Figure 2(a), GRPO leads to reward collapse after training for many steps, whereas PPO remains stable. (3) The final training rewards of PPO and GRPO are comparable. Despite differences in convergence speed and stability, both methods achieve similar final train reward and performance, indicating that both are viable for optimizing SEARCH-R1. PPO exhibits greater training stability, making it a preferable choice in this setting. More results are in Appendix F.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) PPO vs. GRPO: GRPO generally converges faster but may exhibit instability after trained for a number of steps, whereas PPO provides more stable optimization but converges at a slower rate. (b) Base vs. Instruct LLM study: Instruction-tuned LLMs converge faster, but the final performance of both modles remains highly similar. (c) Response length study: The response length exhibits a decrease-increase-stabilize trend throughout training, aligning with the overall performance trajectory of the LLM. (d) # Valid search study: As the training proceeds, the LLM learns to call search more.</p>
<p>Table 4: The performance of SEARCH-R1 with and without retrieved token loss masking. The LLM trained with retrieved token loss masking achieves consistently better performance. (LLM: Qwen2.5-7b-base; RL: PPO)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;">2wiki</th>
<th style="text-align: center;">Musique</th>
<th style="text-align: center;">Bamboogle</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SEARCH-R1 w. mask</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 1}$</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1 w.o. mask</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.343</td>
</tr>
</tbody>
</table>
<h1>5.2 Base vs. Instruct LLMs</h1>
<p>We analyze the training dynamics of SEARCH-R1 across both base LLMs and instructiontuned LLMs. Experiments are conducted on two model variants: Qwen2.5-3B, and Qwen2.57B. As shown in Figure 2(b), we observe that instruction-tuned models converge faster and start from a higher initial performance compared to base models. However, the final training reward of both model types remains highly similar after training. This finding suggests that while general post-training accelerates learning in reasoning-plus-search scenarios, RL can effectively bridge the gap over time, enabling base models to achieve comparable performance. More results can be found in Appendix E.</p>
<h3>5.3 Response Length and Valid Search Study</h3>
<p>We conduct an experiment using SEARCH-R1 with the Qwen2.5-7b-base model to analyze the dynamics of response length and number of valid search engine calls over the course of training. The response length result is presented in Figure 2(c), revealing the following key trends: (1) Early Stage (First 100 Steps): The response length sharply decreases, while the training reward exhibits a slight increase. During this phase, the base model learns to eliminate excessive filler words and begins adapting to the task requirements. (2) Later Stage (After 100 Steps): Both response length and training reward increase significantly. At this point, the LLM learns to call the search engine frequently, resulting in longer responses due to retrieved passages. The training reward improves substantially, as the model becomes more effective at leveraging search results. The valid search result is presented in Figure 2(d), showing that the LLMs learn to call the search engine more times as the training proceeds.</p>
<h3>5.4 Study of Retrieved Tokens Loss Masking</h3>
<p>In Section 3.1, we introduced loss masking for retrieved tokens to prevent unintended optimization behaviors. Here, we conduct experiments on the Qwen2.5-7b-base model, comparing training dynamics with and without retrieved token loss masking. As shown in Figure 3, applying retrieved token masking results in greater LLM improvements, mitigating unintended optimization effects and ensuring more stable training. The performance comparison is provided in Table 4, demonstrating that SEARCH-R1 trained with retrieved token loss masking consistently outperforms the variant without masking.</p>
<p>More experimental results on retrieved token loss mask, base vs. instruct LLMs, comparison between PPO/GRPO, the number of retrieved passages in SEARCH-R1 training, group size study in SEARCH-R1 (GRPO), case studies can be found in Appendix D, E, G, H, I and J.</p>
<h1>6 Conclusions</h1>
<p>In this work, we introduced SEARCH-R1, a novel RL framework that enables LLMs to interleave self-reasoning with real-time search engine interactions. Unlike existing RAGlike approaches, which relies on extensive prompting for multi-turn retrieval, or tool-use methods that require large-scale supervised training data, SEARCH-R1 optimizes LLM rollouts through RL, allowing autonomous query generation and strategic utilization of retrieved information. Through extensive experiments on seven datasets, we demonstrated that SEARCH-R1 significantly enhances LLMs' ability to tackle complex reasoning tasks requiring real-time external knowledge. Our analysis also provides key insights into RL training strategies for search-augmented reasoning. Looking ahead, future work can explore expanding SEARCH-R1 to support broader search strategies, including more sophisticated reward mechanisms, dynamic retrieval adjustments based on uncertainty, combining with diverse set of tools and integration with diverse information sources beyond search. It is also promising to investigate its applicability to multimodal reasoning tasks.</p>
<h2>Acknowledgments</h2>
<p>This research was supported in part by Apple PhD Fellowship, in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, in part by the Office of Naval Research contract number N000142412612, in part by NSF grant numbers IIS-19-56151 and 2402873, in part by the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897 and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329, in part by Cisco, and in part by the Center for Intelligent Information Retrieval. Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of the sponsors or the U.S. Government.</p>
<h2>References</h2>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>
<p>Arash Ahmadian, Chris Cremer, Matthias Gall, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet stn, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024.</p>
<p>Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024.</p>
<p>Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. 2024.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):1-53, 2024.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey. arXiv preprint arXiv:2312.10997, 2, 2023.</p>
<p>Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Rajaram Naik, Pengshan Cai, and Alfio Gliozzo. Re2g: Retrieve, rerank, generate. arXiv preprint arXiv:2207.06300, 2022.</p>
<p>Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024.</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020.</p>
<p>Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, and Yuxiao Dong. Advancing language model reasoning through reinforcement learning and inference scaling. arXiv preprint arXiv:2501.11651, 2025.</p>
<p>Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with reinforcement learning-enhanced retrieval. arXiv preprint arXiv:2410.23214, 2024.</p>
<p>Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. arXiv preprint arXiv:2212.10403, 2022.</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024.</p>
<p>Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 7969-7992, 2023.</p>
<p>Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan O Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. In The Thirteenth International Conference on Learning Representations, 2024.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.</p>
<p>Leslie Pack Kaelbling, Michael L Littman, and Andrew W Moore. Reinforcement learning: A survey. Journal of artificial intelligence research, 4:237-285, 1996.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pp. 6769-6781, 2020.</p>
<p>Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hllermeier. A survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 10, 2023.</p>
<p>Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:9459-9474, 2020.
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Retrollm: Empowering large language models to retrieve fine-grained evidence within generation. arXiv preprint arXiv:2412.11919, 2024.
Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025.
Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. Large language models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance, pp. 374-382, 2023.</p>
<p>Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2023.</p>
<p>Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Hannaneh Hajishirzi, and Daniel Khashabi. When not to trust language models: Investigating effectiveness and limitations of parametric and non-parametric memories. arXiv preprint arXiv:2212.10511, 7, 2022.
Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. Advances in Neural Information Processing Systems, 37:124198-124235, 2024.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.
Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. Advances in Neural Information Processing Systems, 37:116617-116637, 2024.
Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima PourNejatian, Anthony B Costa, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, et al. A study of generative large language model for medical research and healthcare. NPJ digital medicine, 6(1):210, 2023.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.
Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: A survey. Frontiers of Computer Science, 19(8):198343, 2025.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36:53728-53741, 2023.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36: $68539-68551,2023$.</p>
<p>John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024.</p>
<p>Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: A flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024.</p>
<p>Richard S Sutton, Andrew G Barto, et al. Reinforcement learning. Journal of Cognitive Neuroscience, 11(1):126-134, 1999.</p>
<p>Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509, 2022a.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539-554, 2022b.</p>
<p>Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.</p>
<p>Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.</p>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229-256, 1992.</p>
<p>Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025.</p>
<p>Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, et al. Rag-gym: Optimizing reasoning and search agents with process supervision. arXiv preprint arXiv:2502.13957, 2025.</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. Advances in Neural Information Processing Systems, 37:121156-121184, 2024.</p>
<p>Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrieval augmented generation. arXiv preprint arXiv:2410.04343, 2024.</p>
<p>Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Siren's song in the ai ocean: a survey on hallucination in large language models. arXiv preprint arXiv:2309.01219, 2023.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023.</p>
<p>Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained language models: A survey. ACM Transactions on Information Systems, 42(4): $1-60,2024$.</p>
<h1>Appendix</h1>
<h2>A Formulation of Reinforcement Learning with a Search Engine</h2>
<p>The classical reinforcement learning (RL) framework for training large language models (LLMs) can be formulated as follows (Rafailov et al., 2023; Ouyang et al., 2022):</p>
<p>$$
\max <em _theta="\theta">{\pi</em>}} \mathbb{E<em _theta="\theta">{x \sim \mathcal{D}, y \sim \pi</em>}(\cdot \mid x)}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D<em _theta="\theta">{\mathrm{KL}}\left[\pi</em>(y \mid x)\right]
$$}(y \mid x) | \pi_{\mathrm{ref}</p>
<p>where $x$ denotes a prompt sampled from a dataset $\mathcal{D}, y$ is a response generated by the policy model $\pi_{\theta}$, and $\pi_{\text {ref }}$ represents a reference model that serves as a regularization anchor. The reward function $r_{\phi}(x, y)$ quantifies the quality of the generated response, while the KL divergence term constrains the updated policy to remain close to the reference model, thereby promoting training stability.
However, this formulation assumes that the entire output sequence $y$ is generated solely by the policy LLM. This assumption does not hold in our setting, where model behavior incorporates both internal reasoning and external information retrieval. To accommodate this, we extend the RL objective to incorporate an external search engine $\mathcal{R}$, yielding the following formulation:</p>
<p>$$
\max <em _theta="\theta">{\pi</em>}} \mathbb{E<em _theta="\theta">{x \sim \mathcal{D}, y \sim \pi</em>}(\cdot \mid x ; \mathcal{R})}\left[r_{\phi}(x, y)\right]-\beta \mathbb{D<em _theta="\theta">{\mathrm{KL}}\left[\pi</em>)\right]
$$}(y \mid x ; \mathcal{R}) | \pi_{\mathrm{ref}}(y \mid x ; \mathcal{R</p>
<p>In this revised objective, the trajectory $y \sim \pi_{\theta}(\cdot \mid x ; \mathcal{R})$ includes interleaved reasoning steps and retrieved content, reflecting a multi-turn interaction between the LLM and the search engine. The KL divergence is computed over the joint response distribution conditioned on both the prompt and the retrieval-augmented context, ensuring the learned policy remains aligned with the reference model even in the presence of external information.</p>
<h2>B Experimental Setups</h2>
<h2>B. 1 Baselines</h2>
<p>Several recent works have explored RAG pipelines, particularly in benchmarks such as Natural Questions (NQ) or HotpotQA, aiming to improve performance through more elaborate retrieval mechanisms. For instance, Re2G (Glass et al., 2022) and RetroLLM (Li et al., 2024) propose sophisticated retrieve-rerank-generate frameworks that employ strong retrievers and complex reranking strategies to select fine-grained evidence for generation. While these approaches demonstrate impressive results, they often rely on task-specific engineering or heavyweight pipelines that limit generalizability and scalability. In contrast, our focus is on a more lightweight and general approach to retrieval-augmented reasoning. As such, we do not include these methods as direct baselines, though they represent valuable directions in the broader space of retrieval-enhanced language modeling.</p>
<h2>B. 2 Experimental Settings</h2>
<p>We conduct experiments using two types of models: Qwen-2.5-3B (Base/Instruct) and Qwen-2.5-7B (Base/Instruct) (Yang et al., 2024). For retrieval, we use the 2018 Wikipedia dump (Karpukhin et al., 2020) as the knowledge source and E5 (Wang et al., 2022) as the retriever. To ensure fair comparison, we follow Lin et al. (2023) and set the number of retrieved passages to 3 across all retrieval-based methods.
For training, we merge the training sets of NQ and HotpotQA to form a unified dataset for SEARCH-R1 and other fine-tuning based baselines. Evaluation is conducted on the test or validation sets of seven datasets to assess both in-domain and out-of-domain performance. Exact Match (EM) is used as the evaluation metric, following Yu et al. (2024). For inferencestyle baselines, we use instruct models, as base models fail to follow instructions. For RL tuning methods, experiments are conducted on both base and instruct models. More details on experimental settings can be found in Appendix B.</p>
<p>For the PPO variant of SEARCH-R1, we set the learning rate of the policy LLM to 1e-6 and that of the value LLM to 1e-5. Training is conducted for 500 steps, with warm-up ratios of 0.285 and 0.015 for the policy and value models, respectively. We use Generalized Advantage Estimation (GAE) with parameters $\lambda=1$ and $\gamma=1$.
Training is performed on a single node with 8 H 100 GPUs. We use a total batch size of 512, with a mini-batch size of 256 and a micro-batch size of 64 . The maximum sequence length is set to 4,096 tokens, with a maximum response length of 500 and a maximum length of 500 tokens for retrieved content. To optimize GPU memory usage, we enable gradient checkpointing and use Fully Sharded Data Parallel (FSDP) with CPU offloading.
For efficient LLM rollouts, we adopt vLLM ${ }^{1}$ with a tensor parallel size of 1 and GPU memory utilization ratio of 0.6 . The rollout sampling uses a temperature of 1.0 and a top-p value of 1.0. The KL divergence regularization coefficient $\beta$ and clip ratio $\epsilon$ are set to 0.001 and 0.2 .</p>
<p>For GRPO training, we set the policy LLM learning rate to 1e-6 and sample 5 responses per prompt, following the GRPO implementation in Verl (Sheng et al., 2024) ${ }^{2}$. The model is trained for 500 steps with a learning rate warm-up ratio of 0.285 . Training is conducted on the same $8 \times \mathrm{H} 100$ setup with identical batch sizes and sequence length configurations as in PPO.
We also use gradient checkpointing, FSDP offloading, and vLLM-based rollouts with the same hyperparameters as above. The rollout temperature and top-p values are both set to 1.0, and the KL divergence coefficient $\beta$ and clip ratio $\epsilon$ are fixed at 0.001 and 0.2 .</p>
<p>For both methods, model checkpoints are saved every 100 steps. In cases where training diverges, we evaluate at the most recent stable checkpoint according to the training reward curve; otherwise, the final checkpoint is used for evaluation. The maximum action budget $B$ is set to 4 , and we retrieve the top 3 passages by default.
We compute outcome rewards using exact match (EM). Unless otherwise noted, PPO is used as the default RL algorithm, and a detailed comparison with GRPO is provided in Section 5.1.</p>
<h1>C Main Results on 14B LLM</h1>
<p>We conduct extensive experiments using the Qwen2.5-14B models, and the results are presented in Table 5. As shown, SEARCH-R1 consistently outperforms all baseline methods across the evaluated metrics. Furthermore, we observe that increasing the model size leads to consistent performance gains with SEARCH-R1, highlighting the benefits of LLM size scaling in our approach.</p>
<h2>D Retrieved Token Loss Masking Study</h2>
<p>In Section 3.1, we introduced a loss masking strategy for retrieved tokens to mitigate undesirable optimization behaviors during training. To evaluate its impact, we conduct experiments using the Qwen2.5-3b/7b-base model, comparing training dynamics with and without retrieved token loss masking. As illustrated in Figure 3, incorporating the masking mechanism leads to more stable optimization and improved model performance. Quantitative results in Table 6 further confirm that SEARCH-R1, when trained with loss masking on retrieved tokens, consistently outperforms its unmasked counterpart.</p>
<h2>E Base vs. Instruct LLMs</h2>
<p>We investigate the training dynamics of SEARCH-R1 across both base and instruction-tuned LLMs, using two model scales: Qwen2.5-3B and Qwen2.5-7B. As depicted in Figure 4,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 5: Main results. The best performance is set in bold. ${ }^{\dagger} /{ }^{\star}$ represents in-domain/outdomain datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">General QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Multi-Hop QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{NQ}^{2}$</td>
<td style="text-align: center;">TriviaQA ${ }^{\wedge}$</td>
<td style="text-align: center;">PopQA ${ }^{\wedge}$</td>
<td style="text-align: center;">HotpotQA ${ }^{\dagger}$</td>
<td style="text-align: center;">2wiki ${ }^{\wedge}$</td>
<td style="text-align: center;">Musique ${ }^{\wedge}$</td>
<td style="text-align: center;">Bamboogle ${ }^{\wedge}$</td>
<td style="text-align: center;">Avg.</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2.5-14b-Base/Instruct</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Direct Inference</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.184</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.227</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">0.190</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.054</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.269</td>
</tr>
<tr>
<td style="text-align: center;">IRCoT</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.375</td>
<td style="text-align: center;">0.166</td>
<td style="text-align: center;">0.230</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.102</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.221</td>
</tr>
<tr>
<td style="text-align: center;">Search-o1</td>
<td style="text-align: center;">0.347</td>
<td style="text-align: center;">0.635</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.268</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.099</td>
<td style="text-align: center;">0.416</td>
<td style="text-align: center;">0.310</td>
</tr>
<tr>
<td style="text-align: center;">RAG</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">0.376</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.051</td>
<td style="text-align: center;">0.192</td>
<td style="text-align: center;">0.281</td>
</tr>
<tr>
<td style="text-align: center;">SFT</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.467</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">0.278</td>
<td style="text-align: center;">0.089</td>
<td style="text-align: center;">0.160</td>
<td style="text-align: center;">0.250</td>
</tr>
<tr>
<td style="text-align: center;">R1-base</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">0.626</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">0.306</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.357</td>
</tr>
<tr>
<td style="text-align: center;">R1-instruct</td>
<td style="text-align: center;">0.334</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.253</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">0.339</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-base</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.676</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">0.470</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.479</td>
</tr>
<tr>
<td style="text-align: center;">Search-R1-instruct</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.660</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.210</td>
<td style="text-align: center;">0.480</td>
<td style="text-align: center;">0.433</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Retrieved Token Loss Masking Study
instruction-tuned models exhibit faster convergence and benefit from higher initial performance relative to their base counterparts. Despite this early advantage, the final performance of both model types converges to a similar level after training. These results indicate that while instruction tuning facilitates more efficient early-stage learning in reasoning-plussearch tasks, reinforcement learning is capable of closing the performance gap, ultimately enabling base models to reach comparable outcomes.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Study of SEARCH-R1 on base and instruct LLMs. The instruction model converges faster and starts from a better initial performance. However, the final performance of both models is very similar.</p>
<h1>F Comparison of PPO and GRPO in SEARCH-R1</h1>
<p>We assess the effectiveness of SEARCH-R1 under two reinforcement learning algorithms: PPO and GRPO, using both Qwen2.5-3B and Qwen2.5-7B as the underlying models. Figure 5 illustrates the training dynamics. Our analysis yields the following key observations: (1) GRPO exhibits faster convergence than PPO across all settings, attributed to the fact</p>
<p>Table 6: The performance of SEARCH-R1 with and without retrieved token loss masking. The LLM trained with retrieved token loss masking achieves consistently better performance. (RL: PPO)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;">2wiki</th>
<th style="text-align: center;">Musique</th>
<th style="text-align: center;">Bamboogle</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Qwen2.5-7b-Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1 w. mask</td>
<td style="text-align: center;">$\mathbf{0 . 4 8 0}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 3 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 5 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 8 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 1}$</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1 w.o. mask</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.567</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.325</td>
<td style="text-align: center;">0.321</td>
<td style="text-align: center;">0.108</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.343</td>
</tr>
<tr>
<td style="text-align: left;">Qwen2.5-3b-Base</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1 w. mask</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 8 7}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 3 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 7 3}$</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.088</td>
<td style="text-align: center;">$\mathbf{0 . 3 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">SEARCH-R1 w.o. mask</td>
<td style="text-align: center;">0.346</td>
<td style="text-align: center;">0.484</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.244</td>
<td style="text-align: center;">$\mathbf{0 . 0 5 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 0 4}$</td>
<td style="text-align: center;">0.262</td>
</tr>
</tbody>
</table>
<p>that PPO relies on a separate value function (critic), which requires an initial warm-up phase before effective policy updates can be made. (2) PPO provides more stable training behavior, as evidenced in Figure 5, where GRPO encounters reward collapse over extended training steps, whereas PPO maintains stability throughout. (3) PPO and GRPO achieve comparable final reward performance, suggesting that despite trade-offs in convergence speed and stability, both methods are effective for optimizing SEARCH-R1.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training dynamics of SEARCH-R1 with PPO and GRPO as the base RL method across four LLMs. GRPO generally converges faster but may exhibit instability after trained for a number of steps, whereas PPO provides more stable optimization but converges at a slower rate. PPO and GRPO achieve comparable final reward performance.</p>
<h1>G Number of Retrieved Passages Study in SEARCH-R1 Training</h1>
<p>We investigate the impact of the number of retrieved passages (top-k) on the training dynamics of SEARCH-R1. While our main experiments adopt top- $-\mathrm{k}=3$ following Lin et al. (2023), we conduct additional studies with top-k set to 1,3 , and 5 to better understand its influence.</p>
<p>Figure 6 presents the training reward curves under these settings. We observe that all three configurations exhibit similar overall training trajectories. Notably, top- $-\mathrm{k}=5$ achieves the fastest initial convergence, reaching the highest training reward within the first 200 steps. However, its reward gradually declines and becomes more unstable as training progresses. In contrast, top- $-\mathrm{k}=1$ and 3 demonstrate more consistent improvements throughout training, with top- $-\mathrm{k}=3$ ultimately achieving the highest reward after 500 steps.</p>
<p>Evaluation results at step 500 are summarized in Table 7, where top- $-\mathrm{k}=3$ yields the best overall performance. We hypothesize two contributing factors: (1) top- $-\mathrm{k}=1$ likely suffers from low retrieval recall, limiting the ability to provide relevant contextual information; (2) top- $-\mathrm{k}=5$ introduces lower precision due to the inclusion of noisy or irrelevant passages (Jin et al., 2024), which not only degrades inference performance but may also adversely affect RL training-discouraging the model from leveraging retrieved content when it learns that the additional context is often unhelpful or misleading.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The training dynamics of SEARCH-R1 with a different number of retrieved passages. (LLM: Qwen2.5-7b-base, RL: PPO)</p>
<p>Table 7: The number of retrieved passages study in SEARCH-R1 training. (LLM: Qwen2.5-7b-base; RL: PPO)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">NQ</th>
<th style="text-align: left;">TriviaQA</th>
<th style="text-align: left;">PopQA</th>
<th style="text-align: left;">HotpotQA</th>
<th style="text-align: left;">2wiki</th>
<th style="text-align: left;">Musique</th>
<th style="text-align: left;">Bamboogle</th>
<th style="text-align: left;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">topk=1</td>
<td style="text-align: left;">0.426</td>
<td style="text-align: left;">0.614</td>
<td style="text-align: left;">0.422</td>
<td style="text-align: left;">0.393</td>
<td style="text-align: left;">0.296</td>
<td style="text-align: left;">0.146</td>
<td style="text-align: left;">0.328</td>
<td style="text-align: left;">0.375</td>
</tr>
<tr>
<td style="text-align: left;">topk=3</td>
<td style="text-align: left;">$\mathbf{0 . 4 8 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 3 8}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 5 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 3 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 3 8 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 1 9 6}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 3 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 4 3 1}$</td>
</tr>
<tr>
<td style="text-align: left;">topk=5</td>
<td style="text-align: left;">0.479</td>
<td style="text-align: left;">0.634</td>
<td style="text-align: left;">0.440</td>
<td style="text-align: left;">0.394</td>
<td style="text-align: left;">0.343</td>
<td style="text-align: left;">0.156</td>
<td style="text-align: left;">0.352</td>
<td style="text-align: left;">0.400</td>
</tr>
</tbody>
</table>
<h1>H Group Size Study in SEARCH-R1 (GRPO) Training</h1>
<p>In our main experiment, we set the group size for SEARCH-R1 (GRPO) to 5, following the setting in Sheng et al. (2024). To further investigate the impact of group size on training dynamics, we conduct an ablation study with group sizes of 1,3 , and 5 . Notably, when the group size is set to 1 , GRPO reduces to the standard REINFORCE algorithm (Williams, 1992).</p>
<p>We train the LLMs for 500 steps, saving model checkpoints every 100 steps. If the model collapses during training, we use the last valid checkpoint for evaluation; otherwise, we evaluate the checkpoint at step 500.
The training dynamics under different group size configurations are illustrated in Figure 7. We observe that a larger group size generally leads to faster convergence but may also increase the risk of collapse due to the inherent instability of reinforcement learning.
Evaluation results across different settings are summarized in Table 8. While larger group sizes can accelerate convergence and achieve higher training rewards, smaller group sizes (e.g., size $=1$ ) enable more stable training and better generalization. This is reflected in superior performance on unseen tasks, highlighting a trade-off between learning speed and stability in GRPO training.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The training dynamics of SEARCH-R1 (GRPO) with different group size. (LLM: Qwen2.5-7b-base)</p>
<p>Table 8: The group size study of SEARCH-R1 (GRPO) on seven datasets. (LLM: Qwen2.5-7bbase)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">TriviaQA</th>
<th style="text-align: center;">PopQA</th>
<th style="text-align: center;">HotpotQA</th>
<th style="text-align: center;">2wiki</th>
<th style="text-align: center;">Musique</th>
<th style="text-align: center;">Bamboogle</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">size $=1$</td>
<td style="text-align: center;">$\mathbf{0 . 4 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 4 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 1 6 3}$</td>
<td style="text-align: center;">0.384</td>
<td style="text-align: center;">$\mathbf{0 . 4 1 0}$</td>
</tr>
<tr>
<td style="text-align: left;">size $=3$</td>
<td style="text-align: center;">0.385</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.396</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.333</td>
<td style="text-align: center;">0.117</td>
<td style="text-align: center;">$\mathbf{0 . 4 0 0}$</td>
<td style="text-align: center;">0.363</td>
</tr>
<tr>
<td style="text-align: left;">size $=5$</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.388</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">0.125</td>
<td style="text-align: center;">0.360</td>
<td style="text-align: center;">0.350</td>
</tr>
</tbody>
</table>
<h1>I Comparison between R1 and SEARCH-R1: A Case Study</h1>
<p>Table 9: A case study of R1 and SEARCH-R1.
Question: Curious is a women's fragrance by a singer born in what city and state?
Ground Truth: McComb, Mississippi
R1:
<think> The singer is Beyonc, who was born in Houston, Texas. </think>
<answer> Houston </answer></p>
<h2>SEARCH-R1:</h2>
<p><think> I need to find out which city and state a singer, by whom the women's fragrance "Curious" was created, was born in. </think>
<search> Curious fragrance information </search>
<information> Doc 1(Title: "Curious (fragrance)") Curious (fragrance) Curious is a women's fragrance by Britney Spears for Elizabeth Arden, and is the first perfume to be endorsed by Britney Spears, preceding ""Fantasy."" Spears reportedly earned $\$ 52$ million from the endorsement. ""Curious"" was released in September 2004, and was very successful internationally. It was the number one fragrance of 2004 in department stores, and in 2005, ""Curious"" was honored by the Fragrance Foundation as Best Women's Fragrance. The fragrance is a white floral scent. Its notes are Louisiana magnolia, golden Anjou pear, lotus flower, tuberose, star jasmine, pink cyclamen, vanilla-infused musk, sandalwood and blonde woods. Elizabeth Arden ... </information> . <think> I need to find out which city and state Britney Spears, the singer behind "Curious" fragrance, was born in. </think>
<search> Britney Spears birthplace </search>
<information> Doc 1(Title: "Britney Spears") the best-selling female albums artist of the 2000s. In 2004, she launched a perfume brand with Elizabeth Arden, Inc., from which sales exceeded US $\$ 1.5$ billion, . Spears was born in McComb, Mississippi, the second child of Lynne Irene Bridges and James Parnell Spears. Her maternal grandmother, Lillian Portell, was English (born in London), and one of Spears's maternal great-great-grandfathers was Maltese. Her siblings are Bryan James and Jamie Lynn. Britney was born in the Bible Belt, where socially conservative evangelical Protestantism is a particularly strong religious influence. Spears was baptized into the Southern Baptist Convention, but later in life studied ... </information>
<think> Since Britney Spears was born in McComb, Mississippi, I need to find out which city and state that is. </think>
<search> McComb, Mississippi location </search>
<information> Doc 1(Title: "McComb, Mississippi") McComb, Mississippi McComb is a city in Pike County, Mississippi, United States, approximately south of Jackson. As of the 2010 census, the city had a total population of 12,790 . It is the principal city of the McComb, Mississippi Micropolitan Statistical Area. McComb was founded in 1872 after Henry Simpson McComb of the New Orleans, Jackson and Great Northern Railroad, a predecessor of the Illinois Central Railroad (now part of the Canadian National Railway), decided to move the railroad's maintenance shops away from New Orleans, Louisiana, to avoid the attractions of that city's saloons. The railroad purchased land in Pike County, ... </information>
<think> I have found that Britney Spears was born in McComb, Mississippi. Since the question asks for the city and state, I just need to confirm the state, which I already know from the previous search. </think>
<answer> McComb, Mississippi </answer></p>
<p>To gain deeper insights into SEARCH-R1, we conduct a case study using Qwen2.5-7B-Base, comparing its behavior with RL without a search engine (Guo et al., 2025). The results are presented in Table 20, revealing the following key observations:</p>
<p>Interleaved Reasoning and Retrieval Enhances Problem Analysis: SEARCH-R1 enables the LLM to perform in-depth reasoning with multi-turn retrieval, whereas RL without search relies solely on the models' internal knowledge. By incorporating retrieved passages, SEARCH-R1 allows the LLM to iteratively refine its reasoning, leading to more informed and accurate responses.
Self-Verification through Iterative Retrieval: We observe that after the second retrieval round, the LLM has already gathered sufficient information to answer the question. However, SEARCH-R1 performs an additional retrieval step to self-verify its conclusion, further reinforcing its confidence in the final response. This phenomenon aligns with findings from LLM reasoning RL without retrieval (Guo et al., 2025), highlighting how RL can encourage verification-driven reasoning even in search-augmented settings.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://docs.vllm.ai/en/latest/
${ }^{2}$ https://github.com/volcengine/verl/blob/main/examples/grpo_trainer/run_deepseek7b_llm. sh&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8928 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8928</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8928</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-685d7b452431904c650cf5e00355f6882ea05e69</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/685d7b452431904c650cf5e00355f6882ea05e69" target="_blank">REINVENT 2.0: An AI Tool for De Novo Drug Design</a></p>
                <p><strong>Paper Venue:</strong> Journal of Chemical Information and Modeling</p>
                <p><strong>Paper TL;DR:</strong> This application note aims to offer the community a production-ready tool for de novo design, called REINVENT, which can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space.</p>
                <p><strong>Paper Abstract:</strong> In the past few years, we have witnessed a renaissance of the field of molecular de novo drug design. The advancements in deep learning and artificial intelligence (AI) have triggered an avalanche of ideas on how to translate such techniques to a variety of domains including the field of drug design. A range of architectures have been devised to find the optimal way of generating chemical compounds by using either graph- or string (SMILES)-based representations. With this application note, we aim to offer the community a production-ready tool for de novo design, called REINVENT. It can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space. It can facilitate the idea generation process by bringing to the researcher's attention the most promising compounds. REINVENT's code is publicly available at https://github.com/MolecularAI/Reinvent.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8928.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8928.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINVENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINVENT 2.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, production-ready application for de novo small-molecule generation that uses SMILES-based RNNs trained with randomized SMILES and a reinforcement-learning loop to optimize multi-objective scoring functions while enforcing diversity and optional pre-focusing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REINVENT generative model</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>SMILES-based Recurrent Neural Network (RNN) with LSTM; reinforced via policy-iteration RL (actor-critic-like)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Dataset derived from ChEMBL; training uses randomized SMILES augmentation (multiple SMILES per compound) to improve grammar learning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / de novo drug design (small molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES sequence generation by RNN sampling, guided by Reinforcement Learning that optimizes a composite Multi-Parameter Optimization (MPO) scoring function; actor-critic scheme with Prior (fixed) and Agent (trainable) RNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Model trained with randomized SMILES shows improved generalization and produces >99% valid SMILES (cited); the framework supports both exploration and exploitation of chemical space but the paper does not report quantitative novelty metrics (e.g., percent not in training set or Tanimoto distances).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity achieved via user-defined MPO scoring function components (predictive models, similarity metrics, Matching Substructure (MS), Custom Alerts (CA), Selectivity Component), plus transfer learning and inception to pre-focus the generator on relevant subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SMILES validity; MPO score (0-1); individual component contributions (0-1); selectivity Δ for target vs off-target; scaffold diversity tracked via Diversity Filters; logging of top-scoring compounds and score evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>REINVENT is presented as a flexible, production-ready tool able to navigate chemical space for both exploration and exploitation. The randomized-SMILES-trained RNN yields high SMILES validity (>99%). REINVENT integrates RL, diversity filters, transfer learning and inception to focus generation; no experimental assay-level validation or numeric benchmarking (activity, percent novel, etc.) is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positions REINVENT among other generative architectures (VAEs, GANs, conditional RNNs) and emphasizes advantages from SMILES randomization and RL-guided MPO optimization; no direct numeric performance benchmarks vs other methods reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Only fingerprint descriptors supported for predictive models (ECFP/MACCS/Avalon) which constrains model inputs; combining multiple predictive components increases cumulative uncertainty (geometric amplification under product scoring); risk of overfocusing/mode collapse mitigated but not eliminated by Diversity Filters; inception can lead to excessive focusing; careful scoring-function tuning is required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8928.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Randomized SMILES RNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES-randomized Recurrent Neural Network generative model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN generative model trained using randomized SMILES augmentation so the model learns SMILES grammar rather than memorizing specific strings, improving generalization and sampling validity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Randomized SMILES strings improve the quality of molecular generative models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Randomized-SMILES trained RNN</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network with LSTM cells (SMILES sequence model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>ChEMBL-derived dataset with SMILES randomization (multiple encodings per molecule) used as data augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo small-molecule generation for drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation by sampling the trained RNN; uniform sampling intended to cover chemical space for exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Randomized-SMILES training reported to improve generalization and sampling uniformity; reported validity of generated SMILES >99% (from cited work), but no direct measures of novelty (e.g., fraction outside training set, similarity distributions) provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>By itself is general-purpose; when combined with REINVENT's RL/MPO framework it can be steered toward application-specific chemical space.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>SMILES validity, generalization capacity, sampling uniformity/diversity (described qualitatively), integration with downstream scoring functions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Randomized-SMILES-trained RNN is used as the backbone generative model in REINVENT; it provides high validity and better generalization enabling effective downstream RL steering. No new numeric benchmarks presented here beyond referencing >99% validity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Claimed improvement over non-randomized SMILES training and benefits relative to other architectures insofar as generalization and valid SMILES sampling; detailed comparative metrics are in the cited reference rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Unconstrained sampling produces many irrelevant molecules unless guided; needs RL/scoring to produce target-relevant molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8928.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agent-Prior RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Actor-Critic Reinforcement Learning with Prior baseline (Agent and Prior RNNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A reinforcement learning setup where a fixed Prior RNN provides baseline likelihoods and a trainable Agent RNN is updated via policy-iteration to maximize an augmented likelihood that combines prior likelihood and MPO scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular de-novo design through deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Agent and Prior RNNs (actor-critic style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Reinforcement learning with recurrent policy / policy iteration</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Agent initialized as copy of Prior (trained on ChEMBL randomized SMILES); Agent updated through RL sampling and scoring feedback rather than supervised labels.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Guided de novo molecule generation in drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>RL policy iteration where augmented log-likelihood = logP_prior + σ * MPO_score; Agent loss = squared difference between Agent log-likelihood and augmented log-likelihood; σ is auto-adjusted margin.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>RL focusses generation on high-scoring regions which may be novel relative to Prior sampling, but it can also lead to narrow high-probability sets (limited novelty) unless diversity mechanisms are used; no quantitative novelty measures presented.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Directly enforces application-specific objectives through MPO scoring (activity, selectivity, properties, substructure constraints), so the Agent learns to prioritize molecules satisfying those objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>MPO reward, augmented likelihood margin σ, Agent loss convergence, distribution of likelihoods, top-scoring molecules per RL step, diversity metrics via scaffold buckets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>The RL setup effectively steers generation toward molecules that score well under the MPO; complementary features (inception, DF, TL) accelerate or regularize learning. Paper describes algorithmic behavior and practical features but does not present assay/experimental validations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Implementation follows Olivecrona et al.'s RL approach; conceptual comparison to other generative frameworks provided but no numerical benchmarks vs alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Agent can quickly become overfocused (mode collapse) and repeatedly sample similar high-scoring molecules; requires Diversity Filters to encourage exploration; tuning σ and scoring-function composition is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8928.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPO scoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Composite Multi-Parameter Optimization (MPO) scoring function</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flexible composite scoring function combining multiple user-defined components (physicochemical properties, predictive models, similarity metrics, substructure matching/alerts) aggregated via weighted product or weighted sum to produce a scalar reward used in RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MPO composite scoring function</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Reward/scoring engineering (multi-objective aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Uses outputs of predictive models trained on user-supplied data; component definitions and thresholds supplied by user.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Optimization of drug-like molecules for multiple objectives (activity, ADMET, selectivity, physicochemical properties)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Provides reward signal for RL (augmented likelihood); components include regressors/classifiers, similarity measures, MS/CA binary penalties and Selectivity Component (difference between target and off-target predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>MPO allows tailoring molecules to desired profiles; increasing number of objectives reduces chance of finding perfect solutions and may narrow search (potentially reducing novelty) — no explicit novelty metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Direct: each scoring component enforces a property (e.g., activity probability, Tanimoto similarity, scaffold presence/absence, property ranges via transformations), enabling targeted generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Component scores (0-1), overall MPO (0-1), transformations (sigmoid, double-sigmoid, step, custom interpolation), selectivity Δ, thresholds for DF inclusion.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>MPO scoring is the central mechanism for directing generation; supports weighted sum/product formulations, MS and CA penalty components, and a Selectivity Component for target vs off-target optimization. The paper discusses tradeoffs (uncertainty amplification, conflicting objectives) but presents no large-scale numeric benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related to CNS-MPO and other multi-objective strategies in literature; REINVENT adds specific binary scaffold/alert components and flexible transformations. No direct performance comparisons in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Combining many predictive models/components amplifies uncertainty (particularly with product aggregation); difficult to balance conflicting objectives; requires careful choice of transformations and weights to produce informative rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8928.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diversity Filters</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scaffold-based Diversity Filters (DF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mechanism that tracks scaffolds of high-scoring generated compounds in limited-capacity buckets and penalizes further generation of the same scaffolds (assigning score zero when bucket capacity exceeded) to enforce exploration and avoid overfocusing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diversity Filters</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Heuristic scaffold-diversity enforcement (bucket mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecule generation to maintain scaffold diversity in drug discovery campaigns</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Extract scaffold (various definitions), add high-scoring compounds to scaffold buckets; once a bucket is full, subsequent compounds that would go into that bucket receive a score penalty (0), discouraging the Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Promotes discovery of novel scaffolds by penalizing repeated scaffolds; no quantitative diversity metrics (e.g., scaffold counts or Tanimoto distributions) reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Maintains diversity while still allowing MPO-driven optimization; users can choose Topological DF (most restrictive), Identical Murcko DF, or Scaffold Similarity DF to tune exploration/exploitation balance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Bucket capacity, scaffold similarity thresholds, count of compounds per bucket, penalization effect on MPO scores.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DFs are presented as an effective practical tool to prevent the Agent from getting stuck in local minima and repeatedly generating the same scaffolds; different DF strategies offer varying restrictiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Comparable to diversity-encouraging techniques in RL and generative modelling (experience replay control, novelty penalties); no quantitative comparisons in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Parameter choices (bucket size, similarity thresholds) strongly affect exploration; Topological DF ignores atom types and may be overly restrictive for some objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8928.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transfer Learning & Inception</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer Learning (TL) and Inception (Experience Replay) pre-focusing methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two complementary approaches to accelerate RL convergence: TL pre-focuses the Prior using a small dataset of relevant compounds; Inception seeds the RL replay buffer with high-scoring SMILES so the Agent sees desirable compounds early in training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transfer Learning / Inception</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Fine-tuning (TL) and experience-replay augmentation (Inception)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>TL: small dataset of compounds sharing features of interest; Inception: curated set of high-scoring SMILES to seed replay memory.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo drug design for focusing generation to an area of interest faster</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>TL: fine-tune Prior so Agent samples relevant subspace more frequently; Inception: randomly sample from inception memory and add to Agent-generated batch at each RL step.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>TL biases towards a subspace (may reduce novelty outside that subspace); Inception can help find productive space but risks overfocusing and reduced novelty if replayed excessively; no quantitative novelty measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Both methods increase probability of sampling application-relevant chemotypes early; TL preferred when sufficient focusing data exist; Inception useful when TL data are scarce or producer is hard to reach.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Number of RL steps to reach 'productivity', scoring progression, diversity outcomes when combined with DF.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>TL is more efficient than Inception in steps to productivity; Inception is useful when TL is infeasible or dataset small; both accelerate RL but need to be balanced with Diversity Filters to avoid excessive focusing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Analogous to model fine-tuning and replay-memory techniques used broadly in ML/RL; no numeric benchmarks vs other approaches provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Inception can cause overfocusing and reduced diversity if used too long; TL requires enough representative data to properly pre-focus Prior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8928.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8928.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Predictive Model Components</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scikit-learn and XGBoost predictive model components using fingerprint descriptors</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of classical ML predictive models (scikit-learn classifiers/regressors and XGBoost regressors) into MPO scoring; models accept fingerprint descriptors (ECFP, MACCS, Avalon) computed by RDKit and output probabilities or transformed continuous values used in scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>scikit-learn models / XGBoost Regressor</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Classical ML predictive models (classification/regression trees, boosting) used as scoring components</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>User-provided training datasets for each predictive model (activity, off-target, ADMET); descriptors are fingerprints (ECFP variants, MACCS, Avalon) calculated with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Property prediction and scoring for de novo drug design (target activity, selectivity, ADMET proxies)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Predictive model outputs (probabilities for classifiers, transformed regression outputs) are used as components of the MPO reward function that guides RL.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Predictive models constrain generation toward chemical spaces similar to training data; restriction to fingerprint descriptors may limit recognition of novel chemotypes and influence novelty negatively; no quantitative novelty reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Used to enforce target activity, avoid off-targets (Selectivity Component), and to impose property constraints; outputs scaled to [0,1] with transformation functions (sigmoid, double-sigmoid, step, interpolation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Predicted probabilities, transformed regression outputs, component contribution to MPO, Selectivity Δ for two-model scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Predictive models are key components of MPO and enable property-specific steering; the implementation supports scikit-learn and XGBoost models but only with fingerprint descriptors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Uses classical fingerprint-based models rather than end-to-end deep property predictors; trade-offs include simplicity and speed vs potentially lower expressivity for novel chemotypes; no empirical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Support limited to fingerprint descriptors (ECFP/MACCS/Avalon), which constrains model features; cumulative uncertainty when combining multiple predictive models; transformation of regression outputs required to map to [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'REINVENT 2.0: An AI Tool for De Novo Drug Design', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Randomized SMILES strings improve the quality of molecular generative models <em>(Rating: 2)</em></li>
                <li>Molecular de-novo design through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Deep learning enables rapid identification of potent DDR1 kinase inhibitors <em>(Rating: 2)</em></li>
                <li>Generating focused molecule libraries for drug discovery with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules <em>(Rating: 1)</em></li>
                <li>Efficient multi-objective molecular optimization in a continuous latent space <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8928",
    "paper_id": "paper-685d7b452431904c650cf5e00355f6882ea05e69",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "REINVENT",
            "name_full": "REINVENT 2.0",
            "brief_description": "An open-source, production-ready application for de novo small-molecule generation that uses SMILES-based RNNs trained with randomized SMILES and a reinforcement-learning loop to optimize multi-objective scoring functions while enforcing diversity and optional pre-focusing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "REINVENT generative model",
            "model_type": "SMILES-based Recurrent Neural Network (RNN) with LSTM; reinforced via policy-iteration RL (actor-critic-like)",
            "model_size": null,
            "training_data": "Dataset derived from ChEMBL; training uses randomized SMILES augmentation (multiple SMILES per compound) to improve grammar learning and generalization.",
            "application_domain": "Drug discovery / de novo drug design (small molecules)",
            "generation_method": "Direct SMILES sequence generation by RNN sampling, guided by Reinforcement Learning that optimizes a composite Multi-Parameter Optimization (MPO) scoring function; actor-critic scheme with Prior (fixed) and Agent (trainable) RNNs.",
            "novelty_of_chemicals": "Model trained with randomized SMILES shows improved generalization and produces &gt;99% valid SMILES (cited); the framework supports both exploration and exploitation of chemical space but the paper does not report quantitative novelty metrics (e.g., percent not in training set or Tanimoto distances).",
            "application_specificity": "Specificity achieved via user-defined MPO scoring function components (predictive models, similarity metrics, Matching Substructure (MS), Custom Alerts (CA), Selectivity Component), plus transfer learning and inception to pre-focus the generator on relevant subspaces.",
            "evaluation_metrics": "SMILES validity; MPO score (0-1); individual component contributions (0-1); selectivity Δ for target vs off-target; scaffold diversity tracked via Diversity Filters; logging of top-scoring compounds and score evolution.",
            "results_summary": "REINVENT is presented as a flexible, production-ready tool able to navigate chemical space for both exploration and exploitation. The randomized-SMILES-trained RNN yields high SMILES validity (&gt;99%). REINVENT integrates RL, diversity filters, transfer learning and inception to focus generation; no experimental assay-level validation or numeric benchmarking (activity, percent novel, etc.) is reported in this paper.",
            "comparison_to_other_methods": "Positions REINVENT among other generative architectures (VAEs, GANs, conditional RNNs) and emphasizes advantages from SMILES randomization and RL-guided MPO optimization; no direct numeric performance benchmarks vs other methods reported here.",
            "limitations_and_challenges": "Only fingerprint descriptors supported for predictive models (ECFP/MACCS/Avalon) which constrains model inputs; combining multiple predictive components increases cumulative uncertainty (geometric amplification under product scoring); risk of overfocusing/mode collapse mitigated but not eliminated by Diversity Filters; inception can lead to excessive focusing; careful scoring-function tuning is required.",
            "uuid": "e8928.0",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Randomized SMILES RNN",
            "name_full": "SMILES-randomized Recurrent Neural Network generative model",
            "brief_description": "An RNN generative model trained using randomized SMILES augmentation so the model learns SMILES grammar rather than memorizing specific strings, improving generalization and sampling validity.",
            "citation_title": "Randomized SMILES strings improve the quality of molecular generative models",
            "mention_or_use": "use",
            "model_name": "Randomized-SMILES trained RNN",
            "model_type": "Recurrent Neural Network with LSTM cells (SMILES sequence model)",
            "model_size": null,
            "training_data": "ChEMBL-derived dataset with SMILES randomization (multiple encodings per molecule) used as data augmentation.",
            "application_domain": "De novo small-molecule generation for drug discovery",
            "generation_method": "Direct SMILES generation by sampling the trained RNN; uniform sampling intended to cover chemical space for exploration and exploitation.",
            "novelty_of_chemicals": "Randomized-SMILES training reported to improve generalization and sampling uniformity; reported validity of generated SMILES &gt;99% (from cited work), but no direct measures of novelty (e.g., fraction outside training set, similarity distributions) provided in this paper.",
            "application_specificity": "By itself is general-purpose; when combined with REINVENT's RL/MPO framework it can be steered toward application-specific chemical space.",
            "evaluation_metrics": "SMILES validity, generalization capacity, sampling uniformity/diversity (described qualitatively), integration with downstream scoring functions.",
            "results_summary": "Randomized-SMILES-trained RNN is used as the backbone generative model in REINVENT; it provides high validity and better generalization enabling effective downstream RL steering. No new numeric benchmarks presented here beyond referencing &gt;99% validity.",
            "comparison_to_other_methods": "Claimed improvement over non-randomized SMILES training and benefits relative to other architectures insofar as generalization and valid SMILES sampling; detailed comparative metrics are in the cited reference rather than this paper.",
            "limitations_and_challenges": "Unconstrained sampling produces many irrelevant molecules unless guided; needs RL/scoring to produce target-relevant molecules.",
            "uuid": "e8928.1",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Agent-Prior RL",
            "name_full": "Actor-Critic Reinforcement Learning with Prior baseline (Agent and Prior RNNs)",
            "brief_description": "A reinforcement learning setup where a fixed Prior RNN provides baseline likelihoods and a trainable Agent RNN is updated via policy-iteration to maximize an augmented likelihood that combines prior likelihood and MPO scores.",
            "citation_title": "Molecular de-novo design through deep reinforcement learning",
            "mention_or_use": "use",
            "model_name": "Agent and Prior RNNs (actor-critic style)",
            "model_type": "Reinforcement learning with recurrent policy / policy iteration",
            "model_size": null,
            "training_data": "Agent initialized as copy of Prior (trained on ChEMBL randomized SMILES); Agent updated through RL sampling and scoring feedback rather than supervised labels.",
            "application_domain": "Guided de novo molecule generation in drug discovery",
            "generation_method": "RL policy iteration where augmented log-likelihood = logP_prior + σ * MPO_score; Agent loss = squared difference between Agent log-likelihood and augmented log-likelihood; σ is auto-adjusted margin.",
            "novelty_of_chemicals": "RL focusses generation on high-scoring regions which may be novel relative to Prior sampling, but it can also lead to narrow high-probability sets (limited novelty) unless diversity mechanisms are used; no quantitative novelty measures presented.",
            "application_specificity": "Directly enforces application-specific objectives through MPO scoring (activity, selectivity, properties, substructure constraints), so the Agent learns to prioritize molecules satisfying those objectives.",
            "evaluation_metrics": "MPO reward, augmented likelihood margin σ, Agent loss convergence, distribution of likelihoods, top-scoring molecules per RL step, diversity metrics via scaffold buckets.",
            "results_summary": "The RL setup effectively steers generation toward molecules that score well under the MPO; complementary features (inception, DF, TL) accelerate or regularize learning. Paper describes algorithmic behavior and practical features but does not present assay/experimental validations.",
            "comparison_to_other_methods": "Implementation follows Olivecrona et al.'s RL approach; conceptual comparison to other generative frameworks provided but no numerical benchmarks vs alternatives.",
            "limitations_and_challenges": "Agent can quickly become overfocused (mode collapse) and repeatedly sample similar high-scoring molecules; requires Diversity Filters to encourage exploration; tuning σ and scoring-function composition is critical.",
            "uuid": "e8928.2",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "MPO scoring",
            "name_full": "Composite Multi-Parameter Optimization (MPO) scoring function",
            "brief_description": "A flexible composite scoring function combining multiple user-defined components (physicochemical properties, predictive models, similarity metrics, substructure matching/alerts) aggregated via weighted product or weighted sum to produce a scalar reward used in RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MPO composite scoring function",
            "model_type": "Reward/scoring engineering (multi-objective aggregation)",
            "model_size": null,
            "training_data": "Uses outputs of predictive models trained on user-supplied data; component definitions and thresholds supplied by user.",
            "application_domain": "Optimization of drug-like molecules for multiple objectives (activity, ADMET, selectivity, physicochemical properties)",
            "generation_method": "Provides reward signal for RL (augmented likelihood); components include regressors/classifiers, similarity measures, MS/CA binary penalties and Selectivity Component (difference between target and off-target predictions).",
            "novelty_of_chemicals": "MPO allows tailoring molecules to desired profiles; increasing number of objectives reduces chance of finding perfect solutions and may narrow search (potentially reducing novelty) — no explicit novelty metrics provided.",
            "application_specificity": "Direct: each scoring component enforces a property (e.g., activity probability, Tanimoto similarity, scaffold presence/absence, property ranges via transformations), enabling targeted generation.",
            "evaluation_metrics": "Component scores (0-1), overall MPO (0-1), transformations (sigmoid, double-sigmoid, step, custom interpolation), selectivity Δ, thresholds for DF inclusion.",
            "results_summary": "MPO scoring is the central mechanism for directing generation; supports weighted sum/product formulations, MS and CA penalty components, and a Selectivity Component for target vs off-target optimization. The paper discusses tradeoffs (uncertainty amplification, conflicting objectives) but presents no large-scale numeric benchmarks.",
            "comparison_to_other_methods": "Related to CNS-MPO and other multi-objective strategies in literature; REINVENT adds specific binary scaffold/alert components and flexible transformations. No direct performance comparisons in paper.",
            "limitations_and_challenges": "Combining many predictive models/components amplifies uncertainty (particularly with product aggregation); difficult to balance conflicting objectives; requires careful choice of transformations and weights to produce informative rewards.",
            "uuid": "e8928.3",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Diversity Filters",
            "name_full": "Scaffold-based Diversity Filters (DF)",
            "brief_description": "Mechanism that tracks scaffolds of high-scoring generated compounds in limited-capacity buckets and penalizes further generation of the same scaffolds (assigning score zero when bucket capacity exceeded) to enforce exploration and avoid overfocusing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Diversity Filters",
            "model_type": "Heuristic scaffold-diversity enforcement (bucket mechanism)",
            "model_size": null,
            "training_data": null,
            "application_domain": "De novo molecule generation to maintain scaffold diversity in drug discovery campaigns",
            "generation_method": "Extract scaffold (various definitions), add high-scoring compounds to scaffold buckets; once a bucket is full, subsequent compounds that would go into that bucket receive a score penalty (0), discouraging the Agent.",
            "novelty_of_chemicals": "Promotes discovery of novel scaffolds by penalizing repeated scaffolds; no quantitative diversity metrics (e.g., scaffold counts or Tanimoto distributions) reported in this paper.",
            "application_specificity": "Maintains diversity while still allowing MPO-driven optimization; users can choose Topological DF (most restrictive), Identical Murcko DF, or Scaffold Similarity DF to tune exploration/exploitation balance.",
            "evaluation_metrics": "Bucket capacity, scaffold similarity thresholds, count of compounds per bucket, penalization effect on MPO scores.",
            "results_summary": "DFs are presented as an effective practical tool to prevent the Agent from getting stuck in local minima and repeatedly generating the same scaffolds; different DF strategies offer varying restrictiveness.",
            "comparison_to_other_methods": "Comparable to diversity-encouraging techniques in RL and generative modelling (experience replay control, novelty penalties); no quantitative comparisons in paper.",
            "limitations_and_challenges": "Parameter choices (bucket size, similarity thresholds) strongly affect exploration; Topological DF ignores atom types and may be overly restrictive for some objectives.",
            "uuid": "e8928.4",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Transfer Learning & Inception",
            "name_full": "Transfer Learning (TL) and Inception (Experience Replay) pre-focusing methods",
            "brief_description": "Two complementary approaches to accelerate RL convergence: TL pre-focuses the Prior using a small dataset of relevant compounds; Inception seeds the RL replay buffer with high-scoring SMILES so the Agent sees desirable compounds early in training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transfer Learning / Inception",
            "model_type": "Fine-tuning (TL) and experience-replay augmentation (Inception)",
            "model_size": null,
            "training_data": "TL: small dataset of compounds sharing features of interest; Inception: curated set of high-scoring SMILES to seed replay memory.",
            "application_domain": "De novo drug design for focusing generation to an area of interest faster",
            "generation_method": "TL: fine-tune Prior so Agent samples relevant subspace more frequently; Inception: randomly sample from inception memory and add to Agent-generated batch at each RL step.",
            "novelty_of_chemicals": "TL biases towards a subspace (may reduce novelty outside that subspace); Inception can help find productive space but risks overfocusing and reduced novelty if replayed excessively; no quantitative novelty measures provided.",
            "application_specificity": "Both methods increase probability of sampling application-relevant chemotypes early; TL preferred when sufficient focusing data exist; Inception useful when TL data are scarce or producer is hard to reach.",
            "evaluation_metrics": "Number of RL steps to reach 'productivity', scoring progression, diversity outcomes when combined with DF.",
            "results_summary": "TL is more efficient than Inception in steps to productivity; Inception is useful when TL is infeasible or dataset small; both accelerate RL but need to be balanced with Diversity Filters to avoid excessive focusing.",
            "comparison_to_other_methods": "Analogous to model fine-tuning and replay-memory techniques used broadly in ML/RL; no numeric benchmarks vs other approaches provided here.",
            "limitations_and_challenges": "Inception can cause overfocusing and reduced diversity if used too long; TL requires enough representative data to properly pre-focus Prior.",
            "uuid": "e8928.5",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Predictive Model Components",
            "name_full": "Scikit-learn and XGBoost predictive model components using fingerprint descriptors",
            "brief_description": "Integration of classical ML predictive models (scikit-learn classifiers/regressors and XGBoost regressors) into MPO scoring; models accept fingerprint descriptors (ECFP, MACCS, Avalon) computed by RDKit and output probabilities or transformed continuous values used in scoring.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "scikit-learn models / XGBoost Regressor",
            "model_type": "Classical ML predictive models (classification/regression trees, boosting) used as scoring components",
            "model_size": null,
            "training_data": "User-provided training datasets for each predictive model (activity, off-target, ADMET); descriptors are fingerprints (ECFP variants, MACCS, Avalon) calculated with RDKit.",
            "application_domain": "Property prediction and scoring for de novo drug design (target activity, selectivity, ADMET proxies)",
            "generation_method": "Predictive model outputs (probabilities for classifiers, transformed regression outputs) are used as components of the MPO reward function that guides RL.",
            "novelty_of_chemicals": "Predictive models constrain generation toward chemical spaces similar to training data; restriction to fingerprint descriptors may limit recognition of novel chemotypes and influence novelty negatively; no quantitative novelty reporting.",
            "application_specificity": "Used to enforce target activity, avoid off-targets (Selectivity Component), and to impose property constraints; outputs scaled to [0,1] with transformation functions (sigmoid, double-sigmoid, step, interpolation).",
            "evaluation_metrics": "Predicted probabilities, transformed regression outputs, component contribution to MPO, Selectivity Δ for two-model scenarios.",
            "results_summary": "Predictive models are key components of MPO and enable property-specific steering; the implementation supports scikit-learn and XGBoost models but only with fingerprint descriptors.",
            "comparison_to_other_methods": "Uses classical fingerprint-based models rather than end-to-end deep property predictors; trade-offs include simplicity and speed vs potentially lower expressivity for novel chemotypes; no empirical comparisons provided.",
            "limitations_and_challenges": "Support limited to fingerprint descriptors (ECFP/MACCS/Avalon), which constrains model features; cumulative uncertainty when combining multiple predictive models; transformation of regression outputs required to map to [0,1].",
            "uuid": "e8928.6",
            "source_info": {
                "paper_title": "REINVENT 2.0: An AI Tool for De Novo Drug Design",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Randomized SMILES strings improve the quality of molecular generative models",
            "rating": 2
        },
        {
            "paper_title": "Molecular de-novo design through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors",
            "rating": 2
        },
        {
            "paper_title": "Generating focused molecule libraries for drug discovery with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
            "rating": 1
        },
        {
            "paper_title": "Efficient multi-objective molecular optimization in a continuous latent space",
            "rating": 1
        }
    ],
    "cost": 0.018113499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>REINVENT 2.0 - an AI tool for de novo drug design</h1>
<p>Thomas Blaschke ${ }^{€}$, Josep Arús-Pous ${ }^{\S}$, Hongming Chen ${ }^{#}$, Christian Margreitter ${ }^{b}$, Christian Tyrchan ${ }^{\S}$, Ola Engkvist ${ }^{4}$, Kostas Papadopoulos ${ }^{4}$, Atanas Patronov ${ }^{6^{*}}$</p>
<h4>Abstract</h4>
<p>§ Hit Discovery, Discovery Sciences, MolecularAI, R\&amp;D, AstraZeneca Gothenburg, Sweden || Medicinal Chemistry, BioPharmaceuticals Early RIA, R\&amp;D, AstraZeneca, Gothenburg, Sweden $\perp$ Department of Chemistry and Biochemistry, University of Bern, Freiestrasse 3, 3012 Bern, Switzerland. $\checkmark$ Chemistry and Chemical Biology Centre, Guangzhou Regenerative Medicine and Health-Guangdong Laboratory, Science Park, Guangzhou, China € Department of Life Science Informatics, B-IT, LIMES Program Unit Chemical Biology and Medicinal Chemistry, Rheinische Friedrich-Wilhelms-Universität, Endenicher Allee 19c, D-53115 Bonn, Germany * Corresponding author: atanas.patronov@astrazeneca.com</p>
<h4>Abstract</h4>
<p>In the past few years, we have witnessed a renaissance of the field of de novo drug design. The advancements in deep learning and artificial intelligence (AI) have triggered an avalanche of ideas about how to translate such techniques to a variety of domains including the field of drug design. A range of architectures have been devised to find the optimal way of generating chemical compounds by using either graph or SMILES based representations. With this application note we aim to offer the community a production-ready tool for de novo design, named REINVENT. It can be effectively applied on drug discovery projects that are striving to resolve either exploration or exploitation problems while navigating the chemical space. It can facilitate the idea generation process by bringing to the researcher's attention the most promising compounds. REINVENT's code is publicly available at https://github.com/MolecularAI/Reinvent</p>
<h1>Introduction</h1>
<p>The main goal of de novo drug design is to identify novel active compounds that can simultaneously satisfy a constellation of essential optimization goals such as activity, selectivity, physical-chemical and ADMET properties. Because of the sheer number of possible solutions, it is a non-trivial task to optimally satisfy such a multitude of requirements which makes the search process slow and costly even when it is only conducted in silico. Therefore, having an efficient solution which enables the navigation of chemical space and generation of relevant ideas is essential. To address such needs the research community has recently turned its focus towards artificial intelligence (AI) based generative models that are capable of proposing promising small molecules. The potential of generative models for chemical space exploration has been demonstrated in numerous studies [1]-[3]. Various neural network architectures have been engineered and a plethora of AI training strategies have been employed in the race to device more efficient methods for the generation of compounds. A number of architectures, such as Variational Autoencoders (VAEs) [4], [5], Recurrent Neural Networks (RNNs) with Long ShortTerm Memory (LSTM) cells [6], Conditional RNNs or Generative Adversarial Networks have been proven successful in generating molecules by using data representation of molecules either as molecular graphs or SMILES [7]-[10].</p>
<p>However, while all of these architectures and many others are provided as open-source, only a few [11] are in a state that allows to readily apply the code on drug design related problems without the need of spending a significant amount of time developing missing functionalities. Ideally, users should be able to navigate the chemical space efficiently in two general use cases: exploration and exploitation mode. For exploitation, users define an area of interest and focus on generating compounds that share similar structural features. In contrast, the exploration mode enables them to obtain compounds that share less structural similarity but still satisfy other desired features. This implies the necessity to utilize not only predictive models and structure similarity/dissimilarity but also various rule-based scoring components to push towards or pull away from specific areas of the chemical space. Moreover, to be able to adapt appropriately to</p>
<p>any given drug discovery project at hand, the ability to fine-tune each of these potential scoring function components is paramount.</p>
<p>To achieve such behavior, apart from having a deep learning architecture with a reliable generative potential, it is essential to provide an efficient navigation mechanism. In order to address such needs, we are describing in the current paper the latest version of REINVENT - our in-house developed tool for de novo design of small molecules.</p>
<h1>Application Overview</h1>
<p>In its core, REINVENT is using a generative model with an architecture derived from the work of Arus-Pous et al [12]. The model is trained on a dataset derived from ChEMBL [13] and capable of generating compounds in the SMILES format. It has been trained by "randomizing" the SMILES representation of the input data, which is essentially a data augmentation technique [12]. Randomizing the compounds' representation uses multiple SMILES encodings for the same compound, thus ensuring that the model will likely learn the grammar rather than memorizing specific strings or parts of them. The resulting model shows a significantly improved generalization potential and produces SMILES strings with validity of above 99\% [12]. Uniform sampling of the chemical space by the model is a prerequisite for efficient exploration. The model can generate random valid compounds and is able to dive into any region of that space for exhaustive exploitation [12]. However, we are mostly interested in compounds that only act on a specific target and such creative potential of generative models is of little practical use unless specific context is given. Therefore, it is often necessary to direct the generative model towards relevant areas in the chemical space that contain compounds of interest. We achieve this by subjecting it to a Reinforcement Learning (RL) [14] scenario while aiming to satisfy a set of requirements that could vaguely sketch the desired compounds. In other words, the generative model will try to maximize the outcome of a scoring function that contains multiple components/parameters, thus computing an MPO score [15]. To stir the generation of compounds towards the desired direction, REINVENT employs a composite scoring function</p>
<p>consisting of different user-defined components. Each component is responsible for a simple target property. The feedback from the scoring function is used in a RL loop with a policy iteration as described by Olivecrona et al. [16]. Two RNNs are used in an actor-critic scenario where the critic is a prior RNN (Prior) that remains constant and serves as a baseline thus guaranteeing that the knowledge of SMILES syntax will be retained. The actor (which we will refer to as the Agent) can be an identical copy of the Prior or a focused version that has already undergone some training. The Agent takes actions by sampling a batch of SMILES $\mathbf{S}$ which are evaluated by the Prior and scored by the scoring function. The resulting score is combined with Prior's likelihood and used to form the augmented likelihood (eq 1). The augmented likelihood essentially sets the bar for the Agent since the loss is calculated as the squared difference between the Agent's likelihood and the augmented likelihood (eq 2). Also, $\boldsymbol{\sigma}$ is a scalar value, automatically adjusted to guarantee a proper margin between augmented and Agent's likelihood values.</p>
<p>$$
\begin{aligned}
&amp; \log P(\boldsymbol{S})<em P="P" i="i" o="o" r="r">{A u g m e n t e d}=\log P(\boldsymbol{S})</em>)}+\boldsymbol{\sigma} * M P O(\boldsymbol{S<em A="A" d="d" e="e" g="g" m="m" n="n" t="t" u="u">{s c o r e} \
&amp; \text { loss }=\left[\log P(\boldsymbol{S})</em>
\end{aligned}
$$}-\log P(\boldsymbol{S})_{A g e n t}\right]^{2</p>
<p>The RL scenario is complemented with an inception feature which can speed up the focusing of the Agent. Inception is essentially an extended experience replay [17] that allows users to preincept SMILES of interest so that the RL run generates compounds within an area of interest in a fewer number of steps. This can be particularly useful for specific exploitation scenarios.</p>
<p>Another key feature that has influence over the RL driven training of the Agent is the diversity filter as it can penalize the frequent generation of similar compounds. Each of these features are discussed in further detail below.</p>
<p>REINVENT offers two general scoring function formulations (equations 3 and 4). The individual components of the scoring function can be either combined as a weighted sum or as a weighted product [18]. The individual score components can have different weight coefficients reflecting their importance in the overall score. Score contribution from each component can vary in the range between 0 and 1 . As a result, the overall score is also within a range of 0 to 1.</p>
<p>$$
\begin{aligned}
&amp; P(X)=\left[\prod_{i} p\left(x_{i}\right)^{w_{i}}\right]^{1 / \sum_{i} w_{i}} \
&amp; S(X)=\frac{\sum_{i} w_{i} * p\left(x_{i}\right)}{\sum_{i} w_{i}}
\end{aligned}
$$</p>
<p>The scoring function can be comprised of components such as physical-chemical properties, predictive models (both regression and classification), shape similarity, Tanimoto similarity, and Jaccard distance scores [19]. The predictive model component in REINVENT works with both regression and classification types of scikit-learn [20] predictive models. XGB Regressor model types from the xgboost python library can be also employed [21]. A notable limitation of the current implementation is that only fingerprint descriptors can be used. REINVENT supports various representations of ECFP descriptors, MACCS keys and Avalon descriptors all of which are implemented in the RDKit library [22]-[25]. Classification models are expected to be binary class predictors and the corresponding probability of belonging to the positive class is used as an output. However, the regression models can output any continuous value and a suitable transformation should be applied to scale the predictions into the required [0,1] interval. We offer a variety of transformation functions, including sigmoid and double sigmoid as well as step functions for transforming non-continuous components such as the number of hydrogen bond donors and acceptors. We also provide a custom interpolation transformation where the score is transformed by a function that interpolates between user-defined pairs of minima and maxima. The choice of transformation depends on the predicted property or the calculated descriptor. For</p>
<p>properties that are only desirable to lie within a certain range we would seek to apply a double sigmoid transformation to cap the score between the preferred lower and upper bound values resulting in a score of 0 outside and increasing up to 1.</p>
<p>Combining multiple components in a single scoring function presents a typical multiparameter optimization problem. By increasing the number of components, the probability of discovering solutions that achieve maximum score can drop significantly as the different components are likely to pull the MPO score in different directions. Another key aspect is the cumulative uncertainty resulting from the use of multiple predictive models at a time. Even if nearly perfect models are used, combining them will result in geometric amplification of the uncertainty in the outcome of a weighted product formulation. While such an effect would be milder in the weighted sum scenario, the exploration of the chemical space by using multiple predictive models could still be likened to navigating at sea with a slightly broken compass.</p>
<p>In addition to the standard version (eq 3), we offer custom weighted scoring function formulations (eqs 5 and 6) where $\mathrm{P}<em _mathrm_CA="\mathrm{CA">{\mathrm{MS}}$ is a Matching Substructure (MS) component and $\mathrm{P}</em>$ is a Custom Alerts (CA) component. These two are binary penalty components. MS can be used to focus the generation of compounds towards a specific scaffold of interest. It uses a list of SMARTS [26] as an input and it penalizes the overall score if none of the desired substructures is represented in the generated compound. MS produces a score of either 1 or 0.5 depending on whether the scaffold is present or not, thereby being quite helpful for exploitation scenarios. CA can be either 0 or 1 and it also uses a list of SMARTS patterns that normally capture undesired moieties in the generated compounds. If there is a match with any of the listed alerts the overall score will be 0 thus penalizing the future generation of similar compounds. CA can be used also for scaffold hopping if the user is aiming for novelty and wants to avoid certain molecular substructures.
$P(\mathrm{X})=P(X)}<em A="A" C="C">{M S} \times P(X)</em>$} \times\left[\prod_{i} p\left(x_{i}\right)^{w_{i}}\right]^{\frac{1}{\sum_{i} w_{i}}</p>
<p>$S(X)=P(X)<em A="A" C="C">{M S} \times P(X)</em>\right]$} \times\left[\frac{\sum_{i} w_{i} * p\left(x_{i}\right)}{\sum_{i} w_{i}</p>
<p>Another common use case is to try to optimize against a target of interest while simultaneously minimizing the probability of binding to one or more off-targets. For this scenario, we offer a Selectivity Component (SC). SC works with two predictive models: one is used to predict the target activity and another for predicting an off-target activity. If both predictive models are regression type, the difference between the predicted activities $\Delta$ (eq 7) is calculated and consequently subjected to a sigmoid transformation thus forming the SC score. In cases where one of the models is a classifier the regression model prediction is first subjected to transformation and the resulting $\Delta$ is output as an SC score.
$\Delta=P_{\text {activity }}-P_{\text {off-target }}$</p>
<p>In cases where $\Delta&lt;0$, we assign a lower cap of 0.01 since producing 0 for the component would result in a 0 overall score if used with equations 3 or 5 and will not be sufficiently informative for the Agent. Multiple SC can be used when multiple off-targets are possible.</p>
<p>If properly formulated the scoring function will most likely guide the Agent towards a narrow niche of the chemical space, such that yields high MPO scores. As a result, the Agent will become extremely focused over time and ultimately sample only a handful of compounds with high probability. At this stage the scoring function will reach a plateau, the diversity of the generated structures will be minimal and conducting any further RL steps will not yield any new results. In order to generate another batch of novel compounds, we would need to start over and climb the same learning curve over multiple RL steps in order to optimize the scoring function. However, there is no guarantee that the RL process will not converge in a similar chemical space as the previous run. To enforce generative diversity and stimulate the exploration of a broader chemical space, we employ an additional feature in the RL loop - the Diversity Filters (DF).</p>
<p>DF can be regarded as a collection of buckets that are used for keeping track of all generated scaffolds and the compounds that share those scaffolds. Obviously, not all generated compounds are of interest and only those that are scored by the MPO function above a certain threshold will enter the scaffold buckets. Once a compound with a score above the threshold has been generated, its scaffold is extracted and stored in a scaffold registry and the compound enters the corresponding bucket. The buckets have limited capacity and once the limit of compounds in a given bucket has reached the allowed threshold, any subsequent bucket affiliation will be penalized. Every new compound that enters a full bucket will be assigned a score of zero thus informing the Agent that this area of chemical space has become unfavorable. It is important to note that compounds will be added to the bucket even if the bucket limit has been exceeded. The only impact will be on the Agent, since it will be constantly discouraged from producing similar compounds that share a given scaffold. This will enforce the Agent to seek alternative solutions thus achieving in effect chemical space exploration and will prevent the Agent from becoming stuck in local minima and thus generating the same compounds repeatedly. All collected compounds are kept and stored until the end of the RL run and become available as a csv formatted file.</p>
<p>Users can select their diversity strategy by using Topological DF, Identical Murcko DF or a Scaffold Similarity DF [27]. The Topological DF is the most restrictive since it is agnostic of the atom types. It is created by removing all side chains and subsequently converting all atoms in the structure to sp3 carbons. The other two DF also remove all side chains but retain the atom types. Identical Murcko DF only checks if there is a bucket with exactly the same scaffold while Scaffold Similarity is more permissive and can include compounds into the bucket if they satisfy a certain threshold of scaffold similarity.</p>
<h1>Directing the generative process.</h1>
<p>Once the Agent starts generating compounds of sufficiently high MPO score we can define that it has reached a state of productivity. However, starting from a random point in the chemical space and slowly focusing the Agent to a state where it can generate compounds of interest can be a complex and time-consuming task. It could even prove to be an impossible task within a single RL run, especially if the MPO formulation is too complex and has multiple components. To overcome this and to speed up the overall RL process we have identified two approaches that can complement each other.</p>
<h2>Transfer Learning (TL)</h2>
<p>At the beginning of the RL process, the Agent is an identical copy of the Prior. It possesses the same generative capacity and the potential to sample compounds from rather vast area of the chemical space. While this holds a great promise, it can also be an efficiency overhead since for the Agent to become "productive" will first need to find a "rewarding" chemical space. This will have an impact on the RL search for both types of problems: exploitation and exploration, particularly when the MPO score includes multiple conflicting components. To overcome it and to speed up the overall RL process, we resort to pre-focusing the Prior by conducting a TL with a small dataset of compounds sharing features of relevance to our problem. Once focused, the Agent will have an increased probability of sampling a chemical subspace of interest. Such generated compounds will be rewarded higher by the scoring function providing more specific directionality to the generative process. We can use the focused Agent as a starting point for the RL instead of using a copy of the Prior and reach sooner to a state of productivity.</p>
<p>Another way to speed up the transition and reach the state of productivity is by "incepting" compounds that are ranked highly by the scoring function and represent the chemical space of interest. Inception is used in analogy to experience replay in the RL loop. Compounds that we know would be scored highly are introduced to the inception memory before beginning the RL process. At each RL step, a fraction of the inception memory is randomly sampled and is added to the set of compounds generated by the Agent. In this way, early in the RL process, the Agent is presented with highly scoring compounds and will be driven to focus towards the chemical subspace defined by the inception compounds. Also, it will reach a state of productivity sooner. While helpful in the early stages of the training, replaying the compounds that scored well can lead to a very focused Agent. This is particularly likely if the RL run is sufficiently long. For exploration goals it would be best to use it in conjunction with DF since it will prevent excessive focusing by down scoring the repetitive ideas. Inception memory has a limited size and the compounds that are scored lower will be forgotten. The incepted compounds should be ideally from the chemical space of interest with a score below the DF threshold so that they are not discarded instantly.</p>
<p>Inception could be considered as a substitute to TL. However, it is far less efficient in terms of RL steps as it takes longer on average to reach productivity when comparing to starting from a focused Prior state.</p>
<p>Both, Inception and pre-focusing with TL complement each other. We recommend using inception for problems where reaching the state of productivity seems impossible or might take too long due to a very complex scoring function. Another frequent use case is when the number of compounds available for focusing is insufficient to train efficiently a focused prior with TL. In these situations, having a handful of compounds that score well and are frequently presented to the Agent can significantly help to reach the relevant chemical space within a reasonable time.</p>
<h1>Logging</h1>
<p>Essential for monitoring of the learning process is the availability of a comprehensive logging system. In REINVENT we utilize Tensorboard [28] to provide information about the evolution of the Agent during TL by sampling after each step and displaying the likelihood distribution for the sampled data. Stats on validity of the smiles and the most frequently encountered molecules are also shown. For RL we are plotting the evolution of the scoring function and the individual scoring component contributions to the overall score. We are also displaying the highest scoring compounds after each RL step. As an alternative, we also provide the implementation used by us for remote logging which can be set up to post the logging results to a custom REST endpoint.</p>
<h2>Implementation</h2>
<p>REINVENT is an open-source Python application. It uses PyTorch 1.3.0 [29] as a deep learning engine and RDKit version 2019.03.3.0 [30] as a chemistry engine. It works exclusively with scikitlearn based machine learning models and for the detailed logging of the chemical space navigation process, it makes use of Tensorboard's implementation in PyTorch.</p>
<h2>Conclusion</h2>
<p>We have described a production-ready, open-source application for de novo generation of small molecules. It can be used to address both exploration and exploitation type of problems while allowing a flexible formulation of complex MPO scores. Examples of various use cases are provided with the code repository.</p>
<p>Apart from providing a ready-to-use solution, with releasing the code, we are hoping to facilitate the research on using generative methods for drug discovery. We also hope that it can be used as an interaction point for future scientific collaborations.</p>
<h1>References</h1>
<p>[1] J. Arús-Pous et al., "SMILES-Based Deep Generative Scaffold Decorator for De-Novo Drug Design," Jan. 2020, doi: 10.26434/CHEMRXIV.11638383.V1.
[2] A. Zhavoronkov et al., "Deep learning enables rapid identification of potent DDR1 kinase inhibitors," Nat. Biotechnol., vol. 37, no. 9, pp. 1038-1040, Sep. 2019, doi: 10.1038/s41587-019-0224-x.
[3] M. H. S. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, "Generating focused molecule libraries for drug discovery with recurrent neural networks," ACS Cent. Sci., vol. 4, no. 1, pp. 120-131, Jan. 2018, doi: 10.1021/acscentsci.7b00512.
[4] T. Blaschke, M. Olivecrona, O. Engkvist, J. Bajorath, and H. Chen, "Application of Generative Autoencoder in De Novo Molecular Design," Mol. Inform., vol. 37, no. 1-2, p. 1700123, Jan. 2018, doi: 10.1002/minf. 201700123.
[5] R. Gómez-Bombarelli et al., "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules," ACS Cent. Sci., vol. 4, no. 2, pp. 268-276, Feb. 2018, doi: 10.1021/acscentsci.7b00572.
[6] S. Hochreiter and J. Schmidhuber, "Long Short-Term Memory," Neural Comput., vol. 9, no. 8, pp. 1735-1780, Nov. 1997, doi: 10.1162/neco.1997.9.8.1735.
[7] D. Weininger, "SMILES, a Chemical Language and Information System: 1: Introduction to Methodology and Encoding Rules," J. Chem. Inf. Comput. Sci., vol. 28, no. 1, pp. 31-36, Feb. 1988, doi: 10.1021/ci00057a005.
[8] O. Prykhodko et al., "A de novo molecular generation method using latent vector based generative adversarial network," J. Cheminform., vol. 11, no. 1, p. 74, Dec. 2019, doi: 10.1186/s13321-019-0397-9.
[9] P.-C. Kotsias, J. Arús-Pous, H. Chen, O. Engkvist, C. Tyrchan, and E. J. Bjerrum, "Direct Steering of de novo Molecular Generation using Descriptor Conditional Recurrent Neural Networks (cRNNs)," Nov. 2019, doi: 10.26434/CHEMRXIV. 9860906.V2.
[10] Y. Li, L. Zhang, and Z. Liu, "Multi-objective de novo drug design with conditional graph generative model," J. Cheminform., vol. 10, no. 1, p. 33, Dec. 2018, doi: 10.1186/s13321-018-0287-6.
[11] R. Winter, F. Montanari, A. Steffen, H. Briem, F. Noé, and D. A. Clevert, "Efficient multi-objective molecular optimization in a continuous latent space," Chem. Sci., vol. 10, no. 34, pp. 8016-8024, Aug. 2019, doi: 10.1039/c9sc01928f.
[12] J. Arús-Pous et al., "Randomized SMILES strings improve the quality of molecular generative models," J. Cheminform., vol. 11, no. 1, Nov. 2019, doi: 10.1186/s13321-019-0393-0.
[13] A. Gaulton et al., "The ChEMBL database in 2017," Nucleic Acids Res., vol. 45, no. D1, pp. D945D954, Jan. 2017, doi: 10.1093/nar/gkw1074.
[14] R. S. Sutton and A. G. Barto, "Reinforcement Learning: An Introduction Second edition, in progress."</p>
<p>[15] T. T. Wager, X. Hou, P. R. Verhoest, and A. Villalobos, "Moving beyond rules: The development of a central nervous system multiparameter optimization (CNS MPO) approach to enable alignment of druglike properties," ACS Chem. Neurosci., vol. 1, no. 6, pp. 435-449, Jun. 2010, doi: 10.1021/cn100008c.
[16] M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen, "Molecular de-novo design through deep reinforcement learning," J. Cheminform., vol. 9, no. 1, p. 48, Dec. 2017, doi: 10.1186/s13321-017-0235-x.
[17] L.-J. Lin, "Self-Improving Reactive Agents Based On Reinforcement Learning, Planning and Teaching," 1992.
[18] D. J. Cummins and M. A. Bell, "Integrating Everything: The Molecule Selection Toolkit, a System for Compound Prioritization in Drug Discovery," J. Med. Chem., vol. 59, no. 15, pp. 6999-7010, Aug. 2016, doi: 10.1021/acs.jmedchem.5b01338.
[19] T. T. . Tanimoto, An elementary mathematical theory of classification and prediction by T.T. Tanimoto | National Library of Australia. .
[20] F. Pedregosa et al., "Scikit-learn: Machine learning in Python," J. Mach. Learn. Res., vol. 12, pp. 2825-2830, Oct. 2011.
[21] T. Chen and C. Guestrin, "XGBoost: A scalable tree boosting system," in Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, vol. 13-17-August-2016, pp. 785-794, doi: 10.1145/2939672.2939785.
[22] D. Rogers and M. Hahn, "Extended-connectivity fingerprints," J. Chem. Inf. Model., vol. 50, no. 5, pp. 742-754, May 2010, doi: 10.1021/ci100050t.
[23] P. Gedeck, B. Rohde, and C. Bartels, "QSAR - How good is it in practice? Comparison of descriptor sets on an unbiased cross section of corporate data sets," J. Chem. Inf. Model., vol. 46, no. 5, pp. 1924-1936, Sep. 2006, doi: 10.1021/ci050413p.
[24] H. L. Morgan, "The Generation of a Unique Machine Description for Chemical Structures-A Technique Developed at Chemical Abstracts Service," J. Chem. Doc., vol. 5, no. 2, pp. 107-113, May 1965, doi: 10.1021/c160017a018.
[25] J. L. Durant, B. A. Leland, D. R. Henry, and J. G. Nourse, "Reoptimization of MDL Keys for Use in Drug Discovery," ACS Publ., vol. 42, no. 6, pp. 1273-1280, Nov. 2002, doi: 10.1021/ci010132r.
[26] "Daylight Theory: SMARTS - A Language for Describing Molecular Patterns." [Online]. Available: https://www.daylight.com/dayhtml/doc/theory/theory.smarts.html. [Accessed: 12-Feb-2020].
[27] G. W. Bemis and M. A. Murcko, "The properties of known drugs. 1. Molecular frameworks," J. Med. Chem., vol. 39, no. 15, pp. 2887-2893, Jul. 1996, doi: 10.1021/jm9602928.
[28] M. Abadi et al., "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems."
[29] A. Paszke et al., "Automatic differentiation in PyTorch."
[30] L. G., "RDKit." [Online]. Available: http://www.rdkit.org/. [Accessed: 12-Feb-2020].</p>            </div>
        </div>

    </div>
</body>
</html>
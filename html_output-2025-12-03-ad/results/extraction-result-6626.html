<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6626 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6626</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6626</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-dbfb49de5ae1b351991d6c55e8179ff4640ed60e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dbfb49de5ae1b351991d6c55e8179ff4640ed60e" target="_blank">GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> GraphReader is introduced, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously, demonstrating superior performance on four challenging single-hop and multi-hop benchmarks.</p>
                <p><strong>Paper Abstract:</strong> Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6626.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6626.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphReader (Graph-based agent to enhance long-context abilities of LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based agent system that converts long documents into a graph of key elements and atomic facts and uses an LLM-driven agent to explore the graph (coarse-to-fine) while recording a notebook of supporting facts to answer long-context QA efficiently with a small context window.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphReader</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a graph where nodes = (key element, set of atomic facts) and edges represent co-occurrence relations; an LLM agent (GPT-4 API) plans a rational multi-step exploration, invokes predefined functions (e.g., read_chunk(List[ID]), read_neighbor_node, read_atomic_facts groups), reads atomic facts then selected original chunks, and writes supporting facts to an explicit episodic notebook for later reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Structured graph index (nodes of key elements + atomic facts) plus an episodic notebook (notes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Node-level key elements, sets of atomic facts (summaries) grouped by original chunk IDs, explicit links/edges between nodes, and an appended notebook of supporting facts (textual summaries referencing chunk IDs).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Procedural function-call interface: agent first reads atomic-fact groups (all fit in context), decides chunks to read and calls read_chunk(List[ID]), traverses neighbors via read_neighbor_node, and appends discovered supporting facts to the notebook; memory is updated by writing new supporting facts and by reflecting on the notebook to guide further reads.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA; HotpotWikiQA-mixup (LV-Eval, 16k–256k); QuALITY; Natural Questions</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context question answering (single-hop and multi-hop reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>HotpotQA: LR-1 84.3%, LR-2 89.7%, F1 70.0%; 2WikiMultihopQA: LR-1 83.7%, LR-2 87.0%, F1 70.1%; MuSiQue: LR-1 59.0%, LR-2 63.5%, F1 47.4%; NarrativeQA: LR-1 65.0%, LR-2 80.0%, F1 29.8%. On HotpotWikiQA-mixup (varying lengths up to 256k) GraphReader consistently outperforms GPT-4-128k full-text reading (e.g., +10.53% relative LR-1 at 16k, +75% relative LR-1 at 128k reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>The paper does not present a direct ablation that removes the graph+notebook memory entirely. Related ablations: removing the rational plan yields HotpotQA LR-1 81.7%, LR-2 87.7%, F1 63.8; removing node-selection yields LR-1 66.0%, LR-2 71.7%, F1 54.1 — these indicate performance drops but do not correspond to disabling the memory structure itself.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2 (LLM rater scores), Exact Match (EM), F1 (and F1* on LV-Eval)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Slightly higher inference token cost vs ReadAgent (1.08× tokens: 52.8k vs 48.7k per question) while achieving much higher accuracy; upfront cost to construct the graph (amortizable for multiple queries on the same document); sensitivity to hyperparameters (chunk size, number of initial nodes); dependency on closed-source GPT-4 API (QPS/regional limits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relies on the agent's planning and reflection quality; performance degrades if too many initial nodes (introduces noise) or improper chunk size (semantic truncation or loss of detail); constrained by the underlying closed-source LLM API availability/limits; no direct experiment removing the memory graph+notebook to isolate its sole contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng. GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6626.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A human-inspired reading agent with gist memory of very long contexts (ReadAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that segments long documents into pages and constructs a gist-memory directory (condensed summaries) that the agent consults to guide page selection and reading to answer queries over very long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A human-inspired reading agent with gist memory of very long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReadAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Creates a gist-memory directory by condensing pages into summaries; the agent looks up the gist memory to select pages to read (page-level retrieval) and then reads pages sequentially (ReadAgent-S) to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Gist memory directory (summary-based memory / page-level summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Condensed gist summaries of pages (summary strings) indexed per page.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Lookup/search over gist memory to select pages; then sequential page reading of selected pages (page reads are actual chunk reads).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA; HotpotWikiQA-mixup (evaluated as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context QA, multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported as baseline in this paper: HotpotQA LR-1 72.3%, LR-2 78.7%, EM 48.0, F1 62.0; 2WikiMultihopQA LR-1 79.0%, LR-2 81.0%, EM 52.7, F1 63.7; MuSiQue LR-1 54.5%, LR-2 61.0%, EM 35.0, F1 45.1; NarrativeQA LR-1 63.0%, LR-2 75.5%, EM 5.0, F1 18.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2 (LLM raters), Exact Match (EM), F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Difficulties in selecting the correct pages when documents are extremely long (page-selection becomes hard); performance drops on extremely long contexts compared to GraphReader; requires tuning page chunking hyperparameters (max_words/min_words), which affect token usage.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles on extremely long contexts due to coarse page-level gist memory that may lack details needed for multi-hop QA; page selection errors reduce recall of supporting facts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6626.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChunkRead+Notes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chunk Read with Notes (GPT-4 chunk read with notes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential chunk-reading baseline where the LLM reads document chunks in order and maintains notes/summaries across chunk reads that are passed forward to help multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (chunk read with notes)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reads fixed-size chunks sequentially; after reading each chunk the model produces notes (summaries) which are provided to subsequent chunk reads so the model can accumulate information across the document.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Notes (episodic summaries carried forward during sequential reading)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual notes / summaries of previously read chunks</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Notes appended to context and presented as part of the prompt when reading subsequent chunks (explicitly passed summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context QA, multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example (Table 2): HotpotQA LR-1 72.3%, LR-2 76.7%, F1 59.5; 2WikiMultihopQA LR-1 65.7%, LR-2 68.7%, F1 56.6.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Chunk read without notes (GPT-4-128k chunk) reported: HotpotQA LR-1 71.3%, LR-2 74.7%, F1 59.5; 2WikiMultihopQA LR-1 59.3%, LR-2 62.3%, F1 50.5 — the notes variant gives small improvements on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2, EM, F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sequential notes approach still constrained by LLM context window and may accumulate noisy or truncated notes; limited scalability to extreme lengths without summarization compression strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective than GraphReader for extremely long contexts where structured graph exploration and finer selection are required; notes may be insufficient to capture multi-hop dependencies spanning far-apart chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6626.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG (BM25 / Ada-002)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation using BM25 or Ada-002 embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard retrieval-augmented pipeline: split document into chunks, retrieve top-k chunks relevant to the question using BM25 or embedding similarity (Ada-002), and pass retrieved chunks to an LLM reader.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The probabilistic relevance framework: BM25 and beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (BM25 / Ada-002)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>External retrieval index (sparse BM25 or dense embeddings) retrieves relevant chunks which are then read by an LLM; memory is the retriever's document index.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>External retrieval index / vector store (BM25 or dense-embedding index)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw text chunks indexed by BM25 or vector embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Similarity search (BM25 scoring or embedding cosine similarity) to return top-k chunks for the reader; no explicit episodic notebook beyond returned chunks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA (used as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context QA, retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples (Table 2): BM25 (top-3) on HotpotQA LR-1 74.7%, LR-2 78.3%, EM 45.7, F1 58.5; Ada-002 (top-3) HotpotQA LR-1 72.0%, LR-2 77.3%, EM 45.0, F1 58.1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2, EM, F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Retrieval can miss supporting chunks; increasing retrieved chunks can improve recall but is limited by the LLM context window; retrieval precision/recall tradeoff affects downstream reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Per the paper, RAG methods show the worst performance among categories on multi-hop long-context QA because retrieval fails to recall all supporting facts; retrieval becomes less effective as context length grows relative to agent-structured approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6626.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphRAG (From local to global: A graph RAG approach to query-focused summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based retrieval-augmented approach that extracts an entity knowledge graph and generates summaries for groups of entities to serve as an index for query-focused summarization and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>From local to global: A graph rag approach to query-focused summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GraphRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a graph-style text index by extracting entity knowledge and generating grouped summaries; retrieved summaries are combined for answer generation (graph-based retrieval memory).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Graph-based text index (entity summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Entity nodes with grouped summaries (text summaries per entity group)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Index lookup over entity summaries; combine summaries as retrieved context for LLM reader.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Benchmarked in this paper as a baseline on long-context QA (e.g., HotpotQA, 2WikiMultihopQA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context QA / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Examples (Table 2): HotpotQA LR-1 73.7%, LR-2 80.3%, EM 49.7, F1 59.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2, EM, F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Graph summaries provide partial information and may miss fine-grained details required for multi-hop reasoning; combining summaries can lose original context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Per paper, GraphRAG provides partial information and can underperform more targeted graph-exploration agents (GraphReader) when fine-grained details or multi-hop chains are required.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6626.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LongRAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LongRAG: Enhancing retrieval-augmented generation with long-context LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation approach that introduces a 'long retriever' and 'long reader' to process the corpus into larger units to reduce retrieval overhead and suit long-context LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LongRAG: Enhancing retrieval-augmented generation with long-context llms</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LongRAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Processes corpus into larger-sized retrieval units (coarser granularity) and pairs a long retriever with a long reader to reduce retriever burden while answering long-context queries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval index with larger-sized units (coarse-grained chunks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Coarse-grained large chunks of text used as retrieval units</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Retriever returns large units which the reader processes with a long-context LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Evaluated as a baseline on long-context QA benchmarks in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Long-context retrieval-augmented QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Example (Table 2): HotpotQA LR-1 75.7%, LR-2 78.3%, EM 48.7, F1 63.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>LR-1, LR-2, EM, F1</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Using larger units reduces retrieval overhead but coarsens granularity which can obscure fine-grained supporting facts needed for multi-hop reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>May lose detailed supporting facts due to coarser chunking; lower recall for fine-grained evidence compared to graph-exploration approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6626.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemWalker (Walking down the memory maze: Beyond context limit through interactive reading)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent-style system that organizes documents into a tree structure and simulates interactive reading (walking the memory tree) to exceed context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Walking down the memory maze: Beyond context limit through interactive reading</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemWalker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Organizes documents into a tree and uses an agent to traverse the tree (interactive reading) to access relevant pieces of information beyond the LLM's context window.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Tree-structured memory (document tree)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Nodes representing pages/segments arranged in a tree</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Agent 'walks' tree nodes to read and gather information (interactive traversal), updating its internal state across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Interactive reading / long-context QA (mentioned in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As discussed in this paper's related work, tree- or page-based structures like MemWalker can struggle to capture multi-hop and long-range dependencies compared to graph-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6626.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6626.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KGP (Knowledge Graph Prompting for Multi-Document Question Answering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Organizes documents into a knowledge graph to support QA; in the paper it is noted that KGP uses the agent primarily to generate queries and thus does not fully exploit agent planning/reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Knowledge graph prompting for multi-document question answering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KGP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge-graph-style index of documents and uses prompting to query that graph; agent usage in the cited work focuses on query generation rather than full autonomous planning/exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Knowledge graph (graph-structured memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Nodes and edges representing entities and relations extracted from documents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Graph-index queries, but as noted in this paper, the agent primarily generates queries rather than performing in-graph planning and reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>Multi-document QA / knowledge-graph-based retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Per related-work discussion here, using a graph index with limited agent exploitation (query generation only) may underutilize agent planning/reflection capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not, as implemented in the cited work, fully use agent planning/reflection for exploration over the graph; thus may be less effective for complex multi-hop reasoning compared to agents that plan and reflect while traversing the graph.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A human-inspired reading agent with gist memory of very long contexts <em>(Rating: 2)</em></li>
                <li>Walking down the memory maze: Beyond context limit through interactive reading <em>(Rating: 2)</em></li>
                <li>PEARL: Prompting large language models to plan and execute actions over long documents <em>(Rating: 2)</em></li>
                <li>From local to global: A graph rag approach to query-focused summarization <em>(Rating: 2)</em></li>
                <li>LongRAG: Enhancing retrieval-augmented generation with long-context llms <em>(Rating: 2)</em></li>
                <li>Knowledge graph prompting for multi-document question answering <em>(Rating: 1)</em></li>
                <li>The probabilistic relevance framework: BM25 and beyond <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6626",
    "paper_id": "paper-dbfb49de5ae1b351991d6c55e8179ff4640ed60e",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "GraphReader",
            "name_full": "GraphReader (Graph-based agent to enhance long-context abilities of LLMs)",
            "brief_description": "A graph-based agent system that converts long documents into a graph of key elements and atomic facts and uses an LLM-driven agent to explore the graph (coarse-to-fine) while recording a notebook of supporting facts to answer long-context QA efficiently with a small context window.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GraphReader",
            "agent_description": "Constructs a graph where nodes = (key element, set of atomic facts) and edges represent co-occurrence relations; an LLM agent (GPT-4 API) plans a rational multi-step exploration, invokes predefined functions (e.g., read_chunk(List[ID]), read_neighbor_node, read_atomic_facts groups), reads atomic facts then selected original chunks, and writes supporting facts to an explicit episodic notebook for later reasoning.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Structured graph index (nodes of key elements + atomic facts) plus an episodic notebook (notes)",
            "memory_representation": "Node-level key elements, sets of atomic facts (summaries) grouped by original chunk IDs, explicit links/edges between nodes, and an appended notebook of supporting facts (textual summaries referencing chunk IDs).",
            "memory_access_mechanism": "Procedural function-call interface: agent first reads atomic-fact groups (all fit in context), decides chunks to read and calls read_chunk(List[ID]), traverses neighbors via read_neighbor_node, and appends discovered supporting facts to the notebook; memory is updated by writing new supporting facts and by reflecting on the notebook to guide further reads.",
            "task_name": "HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA; HotpotWikiQA-mixup (LV-Eval, 16k–256k); QuALITY; Natural Questions",
            "task_category": "Long-context question answering (single-hop and multi-hop reasoning)",
            "performance_with_memory": "HotpotQA: LR-1 84.3%, LR-2 89.7%, F1 70.0%; 2WikiMultihopQA: LR-1 83.7%, LR-2 87.0%, F1 70.1%; MuSiQue: LR-1 59.0%, LR-2 63.5%, F1 47.4%; NarrativeQA: LR-1 65.0%, LR-2 80.0%, F1 29.8%. On HotpotWikiQA-mixup (varying lengths up to 256k) GraphReader consistently outperforms GPT-4-128k full-text reading (e.g., +10.53% relative LR-1 at 16k, +75% relative LR-1 at 128k reported in paper).",
            "performance_without_memory": "The paper does not present a direct ablation that removes the graph+notebook memory entirely. Related ablations: removing the rational plan yields HotpotQA LR-1 81.7%, LR-2 87.7%, F1 63.8; removing node-selection yields LR-1 66.0%, LR-2 71.7%, F1 54.1 — these indicate performance drops but do not correspond to disabling the memory structure itself.",
            "has_comparative_results": false,
            "performance_metric": "LR-1, LR-2 (LLM rater scores), Exact Match (EM), F1 (and F1* on LV-Eval)",
            "tradeoffs_reported": "Slightly higher inference token cost vs ReadAgent (1.08× tokens: 52.8k vs 48.7k per question) while achieving much higher accuracy; upfront cost to construct the graph (amortizable for multiple queries on the same document); sensitivity to hyperparameters (chunk size, number of initial nodes); dependency on closed-source GPT-4 API (QPS/regional limits).",
            "limitations_or_failure_cases": "Relies on the agent's planning and reflection quality; performance degrades if too many initial nodes (introduces noise) or improper chunk size (semantic truncation or loss of detail); constrained by the underlying closed-source LLM API availability/limits; no direct experiment removing the memory graph+notebook to isolate its sole contribution.",
            "citation": "Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, Bo Zheng. GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models.",
            "uuid": "e6626.0",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ReadAgent",
            "name_full": "A human-inspired reading agent with gist memory of very long contexts (ReadAgent)",
            "brief_description": "An agent that segments long documents into pages and constructs a gist-memory directory (condensed summaries) that the agent consults to guide page selection and reading to answer queries over very long contexts.",
            "citation_title": "A human-inspired reading agent with gist memory of very long contexts",
            "mention_or_use": "use",
            "agent_name": "ReadAgent",
            "agent_description": "Creates a gist-memory directory by condensing pages into summaries; the agent looks up the gist memory to select pages to read (page-level retrieval) and then reads pages sequentially (ReadAgent-S) to answer questions.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Gist memory directory (summary-based memory / page-level summaries)",
            "memory_representation": "Condensed gist summaries of pages (summary strings) indexed per page.",
            "memory_access_mechanism": "Lookup/search over gist memory to select pages; then sequential page reading of selected pages (page reads are actual chunk reads).",
            "task_name": "HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA; HotpotWikiQA-mixup (evaluated as baseline in this paper)",
            "task_category": "Long-context QA, multi-hop reasoning",
            "performance_with_memory": "Reported as baseline in this paper: HotpotQA LR-1 72.3%, LR-2 78.7%, EM 48.0, F1 62.0; 2WikiMultihopQA LR-1 79.0%, LR-2 81.0%, EM 52.7, F1 63.7; MuSiQue LR-1 54.5%, LR-2 61.0%, EM 35.0, F1 45.1; NarrativeQA LR-1 63.0%, LR-2 75.5%, EM 5.0, F1 18.9.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "LR-1, LR-2 (LLM raters), Exact Match (EM), F1",
            "tradeoffs_reported": "Difficulties in selecting the correct pages when documents are extremely long (page-selection becomes hard); performance drops on extremely long contexts compared to GraphReader; requires tuning page chunking hyperparameters (max_words/min_words), which affect token usage.",
            "limitations_or_failure_cases": "Struggles on extremely long contexts due to coarse page-level gist memory that may lack details needed for multi-hop QA; page selection errors reduce recall of supporting facts.",
            "uuid": "e6626.1",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChunkRead+Notes",
            "name_full": "Chunk Read with Notes (GPT-4 chunk read with notes)",
            "brief_description": "A sequential chunk-reading baseline where the LLM reads document chunks in order and maintains notes/summaries across chunk reads that are passed forward to help multi-hop reasoning.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GPT-4 (chunk read with notes)",
            "agent_description": "Reads fixed-size chunks sequentially; after reading each chunk the model produces notes (summaries) which are provided to subsequent chunk reads so the model can accumulate information across the document.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Notes (episodic summaries carried forward during sequential reading)",
            "memory_representation": "Textual notes / summaries of previously read chunks",
            "memory_access_mechanism": "Notes appended to context and presented as part of the prompt when reading subsequent chunks (explicitly passed summaries).",
            "task_name": "HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA (used as baseline)",
            "task_category": "Long-context QA, multi-hop reasoning",
            "performance_with_memory": "Example (Table 2): HotpotQA LR-1 72.3%, LR-2 76.7%, F1 59.5; 2WikiMultihopQA LR-1 65.7%, LR-2 68.7%, F1 56.6.",
            "performance_without_memory": "Chunk read without notes (GPT-4-128k chunk) reported: HotpotQA LR-1 71.3%, LR-2 74.7%, F1 59.5; 2WikiMultihopQA LR-1 59.3%, LR-2 62.3%, F1 50.5 — the notes variant gives small improvements on some datasets.",
            "has_comparative_results": true,
            "performance_metric": "LR-1, LR-2, EM, F1",
            "tradeoffs_reported": "Sequential notes approach still constrained by LLM context window and may accumulate noisy or truncated notes; limited scalability to extreme lengths without summarization compression strategies.",
            "limitations_or_failure_cases": "Less effective than GraphReader for extremely long contexts where structured graph exploration and finer selection are required; notes may be insufficient to capture multi-hop dependencies spanning far-apart chunks.",
            "uuid": "e6626.2",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG (BM25 / Ada-002)",
            "name_full": "Retrieval-Augmented Generation using BM25 or Ada-002 embeddings",
            "brief_description": "Standard retrieval-augmented pipeline: split document into chunks, retrieve top-k chunks relevant to the question using BM25 or embedding similarity (Ada-002), and pass retrieved chunks to an LLM reader.",
            "citation_title": "The probabilistic relevance framework: BM25 and beyond",
            "mention_or_use": "use",
            "agent_name": "RAG (BM25 / Ada-002)",
            "agent_description": "External retrieval index (sparse BM25 or dense embeddings) retrieves relevant chunks which are then read by an LLM; memory is the retriever's document index.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "External retrieval index / vector store (BM25 or dense-embedding index)",
            "memory_representation": "Raw text chunks indexed by BM25 or vector embeddings",
            "memory_access_mechanism": "Similarity search (BM25 scoring or embedding cosine similarity) to return top-k chunks for the reader; no explicit episodic notebook beyond returned chunks.",
            "task_name": "HotpotQA; 2WikiMultihopQA; MuSiQue; NarrativeQA (used as baseline comparisons)",
            "task_category": "Long-context QA, retrieval-augmented generation",
            "performance_with_memory": "Examples (Table 2): BM25 (top-3) on HotpotQA LR-1 74.7%, LR-2 78.3%, EM 45.7, F1 58.5; Ada-002 (top-3) HotpotQA LR-1 72.0%, LR-2 77.3%, EM 45.0, F1 58.1.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "LR-1, LR-2, EM, F1",
            "tradeoffs_reported": "Retrieval can miss supporting chunks; increasing retrieved chunks can improve recall but is limited by the LLM context window; retrieval precision/recall tradeoff affects downstream reasoning.",
            "limitations_or_failure_cases": "Per the paper, RAG methods show the worst performance among categories on multi-hop long-context QA because retrieval fails to recall all supporting facts; retrieval becomes less effective as context length grows relative to agent-structured approaches.",
            "uuid": "e6626.3",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GraphRAG",
            "name_full": "GraphRAG (From local to global: A graph RAG approach to query-focused summarization)",
            "brief_description": "A graph-based retrieval-augmented approach that extracts an entity knowledge graph and generates summaries for groups of entities to serve as an index for query-focused summarization and retrieval.",
            "citation_title": "From local to global: A graph rag approach to query-focused summarization",
            "mention_or_use": "use",
            "agent_name": "GraphRAG",
            "agent_description": "Builds a graph-style text index by extracting entity knowledge and generating grouped summaries; retrieved summaries are combined for answer generation (graph-based retrieval memory).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Graph-based text index (entity summaries)",
            "memory_representation": "Entity nodes with grouped summaries (text summaries per entity group)",
            "memory_access_mechanism": "Index lookup over entity summaries; combine summaries as retrieved context for LLM reader.",
            "task_name": "Benchmarked in this paper as a baseline on long-context QA (e.g., HotpotQA, 2WikiMultihopQA, etc.)",
            "task_category": "Long-context QA / retrieval",
            "performance_with_memory": "Examples (Table 2): HotpotQA LR-1 73.7%, LR-2 80.3%, EM 49.7, F1 59.7.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "LR-1, LR-2, EM, F1",
            "tradeoffs_reported": "Graph summaries provide partial information and may miss fine-grained details required for multi-hop reasoning; combining summaries can lose original context.",
            "limitations_or_failure_cases": "Per paper, GraphRAG provides partial information and can underperform more targeted graph-exploration agents (GraphReader) when fine-grained details or multi-hop chains are required.",
            "uuid": "e6626.4",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LongRAG",
            "name_full": "LongRAG: Enhancing retrieval-augmented generation with long-context LLMs",
            "brief_description": "A retrieval-augmented generation approach that introduces a 'long retriever' and 'long reader' to process the corpus into larger units to reduce retrieval overhead and suit long-context LLMs.",
            "citation_title": "LongRAG: Enhancing retrieval-augmented generation with long-context llms",
            "mention_or_use": "use",
            "agent_name": "LongRAG",
            "agent_description": "Processes corpus into larger-sized retrieval units (coarser granularity) and pairs a long retriever with a long reader to reduce retriever burden while answering long-context queries.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Retrieval index with larger-sized units (coarse-grained chunks)",
            "memory_representation": "Coarse-grained large chunks of text used as retrieval units",
            "memory_access_mechanism": "Retriever returns large units which the reader processes with a long-context LLM.",
            "task_name": "Evaluated as a baseline on long-context QA benchmarks in this paper",
            "task_category": "Long-context retrieval-augmented QA",
            "performance_with_memory": "Example (Table 2): HotpotQA LR-1 75.7%, LR-2 78.3%, EM 48.7, F1 63.9.",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "LR-1, LR-2, EM, F1",
            "tradeoffs_reported": "Using larger units reduces retrieval overhead but coarsens granularity which can obscure fine-grained supporting facts needed for multi-hop reasoning.",
            "limitations_or_failure_cases": "May lose detailed supporting facts due to coarser chunking; lower recall for fine-grained evidence compared to graph-exploration approaches.",
            "uuid": "e6626.5",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "MemWalker",
            "name_full": "MemWalker (Walking down the memory maze: Beyond context limit through interactive reading)",
            "brief_description": "An agent-style system that organizes documents into a tree structure and simulates interactive reading (walking the memory tree) to exceed context limits.",
            "citation_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "mention_or_use": "mention",
            "agent_name": "MemWalker",
            "agent_description": "Organizes documents into a tree and uses an agent to traverse the tree (interactive reading) to access relevant pieces of information beyond the LLM's context window.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Tree-structured memory (document tree)",
            "memory_representation": "Nodes representing pages/segments arranged in a tree",
            "memory_access_mechanism": "Agent 'walks' tree nodes to read and gather information (interactive traversal), updating its internal state across steps.",
            "task_name": null,
            "task_category": "Interactive reading / long-context QA (mentioned in related work)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": "As discussed in this paper's related work, tree- or page-based structures like MemWalker can struggle to capture multi-hop and long-range dependencies compared to graph-based approaches.",
            "uuid": "e6626.6",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "KGP",
            "name_full": "KGP (Knowledge Graph Prompting for Multi-Document Question Answering)",
            "brief_description": "Organizes documents into a knowledge graph to support QA; in the paper it is noted that KGP uses the agent primarily to generate queries and thus does not fully exploit agent planning/reflection.",
            "citation_title": "Knowledge graph prompting for multi-document question answering",
            "mention_or_use": "mention",
            "agent_name": "KGP",
            "agent_description": "Constructs a knowledge-graph-style index of documents and uses prompting to query that graph; agent usage in the cited work focuses on query generation rather than full autonomous planning/exploration.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Knowledge graph (graph-structured memory)",
            "memory_representation": "Nodes and edges representing entities and relations extracted from documents",
            "memory_access_mechanism": "Graph-index queries, but as noted in this paper, the agent primarily generates queries rather than performing in-graph planning and reflection.",
            "task_name": null,
            "task_category": "Multi-document QA / knowledge-graph-based retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Per related-work discussion here, using a graph index with limited agent exploitation (query generation only) may underutilize agent planning/reflection capabilities.",
            "limitations_or_failure_cases": "Does not, as implemented in the cited work, fully use agent planning/reflection for exploration over the graph; thus may be less effective for complex multi-hop reasoning compared to agents that plan and reflect while traversing the graph.",
            "uuid": "e6626.7",
            "source_info": {
                "paper_title": "GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A human-inspired reading agent with gist memory of very long contexts",
            "rating": 2
        },
        {
            "paper_title": "Walking down the memory maze: Beyond context limit through interactive reading",
            "rating": 2
        },
        {
            "paper_title": "PEARL: Prompting large language models to plan and execute actions over long documents",
            "rating": 2
        },
        {
            "paper_title": "From local to global: A graph rag approach to query-focused summarization",
            "rating": 2
        },
        {
            "paper_title": "LongRAG: Enhancing retrieval-augmented generation with long-context llms",
            "rating": 2
        },
        {
            "paper_title": "Knowledge graph prompting for multi-document question answering",
            "rating": 1
        },
        {
            "paper_title": "The probabilistic relevance framework: BM25 and beyond",
            "rating": 1
        }
    ],
    "cost": 0.026532499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>GraphReader: Building Graph-based Agent to Enhance Long-Context Abilities of Large Language Models</h1>
<p>Shilong Li^{∗1}, Yancheng He^{∗1}, Hangyu Guo^{∗1}, Xingyuan Bu^{∗†‡1}, Ge Bai^{1}, Jie Liu^{2,3},
Jiaheng Liu^{1}, Xingwei Qu^{4}, Yangguang Li^{3}, Wanli Ouyang^{2,3}, Wenbo Su^{1}, Bo Zheng^{1}
^{1}Alibaba Group ^{2}The Chinese University of Hong Kong
^{3}Shanghai AI Laboratory ^{4}University of Manchester
zhuli.lsl@taobao.com, xingyuanbu@gmail.com</p>
<h6>Abstract</h6>
<p>Long-context capabilities are essential for large language models (LLMs) to tackle complex and long-input tasks. Despite numerous efforts made to optimize LLMs for long contexts, challenges persist in robustly processing long inputs. In this paper, we introduce GraphReader, a graph-based agent system designed to handle long texts by structuring them into a graph and employing an agent to explore this graph autonomously. Upon receiving a question, the agent first undertakes a step-by-step analysis and devises a rational plan. It then invokes a set of predefined functions to read node content and neighbors, facilitating a coarse-to-fine exploration of the graph. Throughout the exploration, the agent continuously records new insights and reflects on current circumstances to optimize the process until it has gathered sufficient information to generate an answer. Experimental results on the LV-Eval dataset reveal that GraphReader, using a 4k context window, consistently outperforms GPT-4-128k across context lengths from 16k to 256k by a large margin. Additionally, our approach demonstrates superior performance on four challenging single-hop and multi-hop benchmarks.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have made great progress on natural language understanding and generation (Zhao et al., 2023; Liu et al., 2024a; Feng et al., 2022; Peng et al., 2020; Xv et al., 2022; Peng et al., 2023b; Bu et al., 2021). However, transformer-based LLMs still struggle in handling long contexts due to the limitation of context window and memory usage.</p>
<p>Current techniques for solving the long-context tasks of LLMs can be divided into two perspectives: 1) Model-level, which includes finetuning with modified positional embeddings (Chen et al.,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance on LV-Eval at 5 context length levels. GraphReader outperforms existing open-sourced and closed-source models while demonstrating a scalable performance in very long contexts. In contrast, other models exhibit a significant decrease in performance as context length increases.</p>
<p>2023b; Zhu et al., 2023; Peng et al., 2023a; Ding et al., 2024), and applying transformer variants with modified attention mechanisms (Dai et al., 2019; Munkhdalai et al., 2024; Gu and Dao, 2023); 2) Agent-level, i.e., employing retrieval-augmented LLM or agent to process long contexts with a limited context window LLM (Nakano et al., 2021; Lee et al., 2024).</p>
<p>However, model-level methods typically train LLMs with target length texts, posing challenges in constructing training datasets and incurring high training costs (Zhu et al., 2023). Additionally, long-context LLMs optimized with these methods tend to overlook crucial details in long contexts, known as "lost in the middle" (Liu et al., 2024b), limiting their ability to address complex tasks, such as multi-hop questions. Agent-level approaches transform input text into a tree (Chen et al., 2023a) or paginated pages (Lee et al., 2024), failing to capture multi-hop and long-range dependencies, thus</p>
<p>limiting their effectiveness on very long contexts, as shown in Figure 1.</p>
<p>To address these issues, we propose a graphbased agent named GraphReader. As illustrated in Figure 2, GraphReader first segments long texts into discrete chunks, extracts essential information, and compresses these into key elements and atomic facts. These key elements and facts are then used to construct a graph with nodes representing key elements and their associated atomic facts. This graph structure effectively captures long-range dependencies and multi-hop relationships within long text. Subsequently, GraphReader autonomously explores this graph using predefined functions, guided by a step-by-step rational plan. Based on a given question, the agent progressively accesses information from coarse key elements and atomic facts to detailed original text chunks, taking notes and reflecting until it gathers sufficient information to generate an answer. In summary, our main contributions are threefold:</p>
<ul>
<li>We introduce GraphReader, a novel agent system designed to organize long texts into a graph structure, leveraging predefined functions and notebook to facilitate planning and reflection during exploration.</li>
<li>GraphReader establishes a scalable long-context capability based on a 4 k context window, demonstrating performance that is comparable to or surpasses GPT-4 with a 128 k context window across varying context lengths.</li>
<li>Extensive experiments conducted on four challenging benchmarks demonstrate that GraphReader achieves superior performance in complex single-hop and multi-hop QA tasks.</li>
</ul>
<h2>2 Related Work</h2>
<p>Long-Context LLMs Recent efforts (Chen et al., 2023b; Ding et al., 2024; Peng et al., 2023a) have focused on positional interpolation (PI) to enhance long-context capabilities. However, these methods require training on full-length texts, leading to significant increases in data and training costs (Chen et al., 2023c; Fu et al., 2024; Bai et al., 2024b). Thus, PoSE (Zhu et al., 2023) and SkipAlign (Wu et al., 2024a) investigate data skip strategy, but tend to neglect detailed information in long texts (Liu et al., 2024b; Bai et al., 2024a; Wu et al., 2024b). Furthermore, despite how extensively the context window is expanded, it remains constrained by a
predefined fixed length. To address these limitations, transformer variants with modified attention mechanisms have been proposed (Dai et al., 2019; Gu and Dao, 2023; Munkhdalai et al., 2024). However, these models are prone to losing earlier information.</p>
<p>Retrieval Retrieval Augmented Generation (RAG) leverages an extensive database of documents to extract task-related information that aids in response generation. Many efforts investigate various levels of retrieval granularity, including tokens (Khandelwal et al., 2019), entities (Févry et al., 2020; De Jong et al., 2021), and chunks (Liu, 2024; LangChain-team, 2024). Other approaches have explored diverse retrieval methods, such as BM25 (Rasooli and Tetreault, 2015) and learning-based strategies (Khattab and Zaharia, 2020; Sachan et al., 2023; Sun et al., 2021). Despite its capabilities, RAG faces challenges in addressing complex questions due to difficulties in developing robust decision-making mechanisms. In contrast, we employ agents that use planning and reflection to gather essential information, effectively tackling complex problems.</p>
<p>Agent for Retrieval Recent work has increasingly leveraged LLMs as agents to tackle complex problems, utilizing their strong planning and reflection abilities (Yao et al., 2022; Park et al., 2023). These abilities have been applied to complex tasks such as function call (Li et al., 2023) and KGQA (Sun et al., 2023; Luo et al., 2023). Agents are also capable of retrieving unstructured information. For example, WebGPT (Nakano et al., 2021) simulates human actions to search on internet for specific answers. Additionally, MemWalker (Chen et al., 2023a) and PEARL (Sarthi et al., 2024) organize documents into a tree structure, while ReadAgent (Lee et al., 2024) condenses documents into a gist memory directory. However, these approaches often struggle with multi-hop questions. KGP (Wang et al., 2024) organizes documents into graphs, but it primarily uses the agent to generate queries, thereby not fully exploiting the agent's capabilities for planning and reflection.</p>
<h2>3 Approach</h2>
<h3>3.1 Preliminary</h3>
<p>GraphReader is built on a graph $\mathcal{G}={\mathcal{V}, \mathcal{E}}$, where each node $v_{i} \in \mathcal{V}$ contains a key element $k_{i}$ and a set of summarized content, namely atomic facts $\mathcal{A}<em i="i">{i}$. In other words, $v</em>\right}$. And}=\left{k_{i}, \mathcal{A}_{i</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The illustration of our GraphReader approach, consisting of graph construction, graph exploration, and answer reasoning.</p>
<p>Each edge $e_{ij} \in \mathcal{E}$ represents the relationship between nodes $v_i$ and $v_j$. This graph structure enables GraphReader to capture global information from the input document $D$ within a limited context window, allowing it to decide whether to explore the current node in detail or jump to a neighboring node. During graph exploration, GraphReader collects supporting facts and terminates the exploration once sufficient information has been gathered to answer the question. As illustrated in Figure 2, the entire process of GraphReader consists of the following three phases: graph construction, graph exploration, and answer reasoning. The prompts utilized in these three stages are detailed in Appendix A, and a detailed example of our process can be found in Appendix I.</p>
<h3>3.2 Graph Construction</h3>
<p>To extract nodes from a document $D$ within the LLM's context limit, we first split $D$ into chunks of maximum length $L$ while preserving paragraph structure. For each chunk, we prompt the LLM to summarize it into atomic facts, the smallest indivisible facts that simplify the original text. We also prompt the LLM to extract key elements from each atomic fact like essential nouns, verbs, and adjectives. After processing all chunks, we normalize the key elements as described by Lu et al. (2023) to handle lexical noise and granularity issues, creating a final set of key elements. We then construct each node $v_i = (k_i, \mathcal{A}_i)$, where $k_i$ is a key element and $\mathcal{A}_i$ is the set of atomic facts corresponding to $k_i$. Finally, we link two nodes $v_i$ and $v_j$ if key element $k_i$ appears in $\mathcal{A}_j$ and vice versa.</p>
<h3>3.3 Graph Exploration</h3>
<h4>3.3.1 Agent Initialization</h4>
<p>Given a graph $\mathcal{G}$ and a question $Q$, our goal is to design an agent that can autonomously explore the graph using predefined functions. The agent begins by maintaining a notebook to record supporting facts, which are eventually used to derive the final answer. Then the agent performs two key initializations: defining the rational plan and selecting the initial node.</p>
<p><strong>Rational Plan</strong> To tackle complex real-world multi-hop questions, pre-planning the solution is</p>
<p>crucial. The agent breaks down the original question step-by-step, identifies the key information needed, and forms a rational plan.</p>
<p>Initial Node Choosing strategic starting points is essential for improving search efficiency. The agent evaluates the key elements of all nodes $\mathcal{V}$ and selects $N$ initial nodes based on the question and the rational plan.</p>
<h3>3.3.2 Exploration</h3>
<p>After selecting $N$ initial nodes as starting points, an agent explores each initial node by first exploring atomic facts, then chunks of the node. Next, it explores neighboring nodes, guided by the question and rational plan. The agent continuously updates the notebook with relevant information during the exploration process.</p>
<p>Exploring Atomic Facts It is impractical to include all original text chunks related to a node within the context window. Therefore, the agent employs a coarse-to-fine strategy, progressing from reading atomic facts to the original text, as all atomic facts can fit within the context window. Initially, all atomic facts associated with a node are grouped by their corresponding chunks, labeled with the respective chunk IDs, and fed to the agent. This allows the agent to capture an overview of each chunk by reading all groups of atomic facts. Meanwhile, the agent utilizes the question, rational plan, and notes in its notebook to reflect on the required clues and determine which chunk is likely to contain useful information. Subsequently, the agent is provided with two functions: 1) read_chunk, if the agent identifies certain chunks as valuable for further reading, it will complete the function parameters with the chunk IDs, i.e., read_chunk(List[ID]), and append these IDs to a chunk queue. 2) stop_and_read_neighbor, conversely, if the agent deems that none of the chunks are worth further reading, it will finish reading this node and proceed to explore neighboring nodes.</p>
<p>Exploring Chunks When the chunk queue is non-empty, it indicates that the agent has identified multiple text chunks of interest. We then traverse the queue, reading each chunk. This step is essential because atomic facts merely summarize key information and provide brief clues, whereas specific details are best obtained directly from the original text chunks. While reading the chunks, the agent will once again consider the question and the plan, thinking about what can be added to the current notebook. Any supporting facts discovered will be recorded in the notebook. Depending on the updated notebook, the agent will then select one of the following four functions: 1) search_more, if supporting fact is insufficient, the agent will continue exploring chunks in the queue; 2) read_previous_chunk and 3)read_subsequent_chunk, due to truncation issues, adjacent chunks might contain relevant and useful information, the agent may insert these IDs to the queue; 4) termination, if sufficient information has been gathered for answering the question, the agent will finish exploration.</p>
<p>Exploring Neighbors Once the atomic facts and chunk queue of the current node have been fully processed, it indicates that this node has been thoroughly explored, and the agent needs to access the next node. Taking into account the question, rational plan, and the content of the notebook, the agent checks all neighboring nodes, i.e., key elements, and performs one of two functions: 1) read_neighbor_node, the agent selects a neighboring node that might be helpful in answering the question and re-enters the process of exploring atomic facts and chunks; 2) termination, the agent determines that none of the neighboring nodes contain useful information, it finish the exploration.</p>
<h3>3.4 Answer Reasoning</h3>
<p>After $N$ agents have independently gathered information and stopped their exploration, we will compile all notes from each agent for reasoning and generating the final answer. Employing Chain-of-Thought (Wei et al., 2022), the LLM first analyzes each note by considering complementary information from other memories and using a majority voting strategy to resolve any inconsistencies. Ultimately, the LLM will consider all the available information to generate the final answer.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental Settings</h3>
<p>Evaluation Benchmarks We conduct experiments on two types of long-context QA benchmarks, including multi-hop long-context QA, i.e., HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), MuSiQue (Trivedi et al., 2022), and a single-hop long-context QA benchmark, i.e., NarrativeQA (Kociský et al., 2018) from LongBench (Bai et al., 2023). Additionally, we</p>
<p>also incorporate HotpotWikiQA-mixup from LVEval (Yuan et al., 2024), a multi-hop benchmark that features five levels of text length: $16 \mathrm{k}, 32 \mathrm{k}$, $64 \mathrm{k}, 128 \mathrm{k}$, and 256 k . Table 1 presents the statistics about these benchmarks, and detailed information is provided in Appendix C.</p>
<p>Evaluation Metrics We employ several automatic evaluation metrics, i.e., $F_{1}$ score, Exact Match (EM) score, and an optimized $F_{1}{ }^{<em>}$ score, as introduced by LV-Eval (Yuan et al., 2024). Specifically, $F_{1}{ }^{</em>}$ first computes the recall of golden answer keywords and only calculates the $F_{1}$ score if it exceeds a certain threshold. Otherwise, the score defaults to zero. Despite the cost-effectiveness of automatic metrics, their accuracy may be affected by the response format. Hence, we implement LLM Raters for answer correctness evaluation using an LLM, denoted as LLM-Rating-1 (LR-1) and LLM-Rating-1 (LR-2), following ReadAgent (Lee et al., 2024). Details on the evaluation metrics can be found in Appendix B.</p>
<p>Baseline Methods We compare our approach with the following baselines: retrieval augmented generation (RAG), long-context LLM, and agentbased methods. (1) RAG: We choose Okapi BM25 (Robertson and Zaragoza, 2009) or OpenAI API embedding model Ada-002 to retrieve the chunks most relevant to the question and employ GPT-4-128k (gpt-4-1106-preview) to read retrieved chunks and answer the question. In addition to traditional RAG methods, we also compared GraphRAG (Edge et al., 2024) and LongRAG (Jiang et al., 2024), which utilize LLM to enhance RAG ability. (2) Long-context LLM: We select GPT-4-128k for directly reading full text when the text content fits within the input window, or for segmenting the text into chunks for sequential reading. (3) Agent-based Method: We select ReadAgent (Lee et al., 2024) and PEARL (Sun et al., 2024), which employ an agent-based system for the execution of retrieval and reading processes for long-context QA. The detailed description of these methods is provided in Appendix D.</p>
<p>Implementation Details In our experiments, we employ GPT-4-128k for both our method and baseline approaches, setting the temperature to 0.2 . For GraphReader, the input window size is configured to 4 k tokens unless stated otherwise. We limit the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 1: The statistics of benchmarks employed in our evaluation. The token number is calculated using the GPT-4 tokenizer from the TikToken. #Samples denote the total number of benchmarks.
maximum chunk size to 2 k tokens, initiate searches from 5 initial nodes, and impose a function call limit of 10 for each search path.</p>
<h3>4.2 Main Results</h3>
<p>The results of three types of methods on four multihop long-context benchmarks and one single-hop long-context benchmark are shown in Table 2 and Table 3. Based on the results, we have the following findings:</p>
<p>Results of RAG methods As the results shown in Table 2, RAG methods based on BM25 and Ada002 exhibit the worst performance in comparison to long-context LLM and agent-based methods. A possible reason is that text retrieval has difficulty recalling all chunks that contain the supporting facts for answering the input question. Although increasing the number of recalled chunks could improve the performance of text retrieval, the context window will limit the effectiveness of these RAG methods.</p>
<p>Results of Long-Context LLMs From the results shown in Table 2, we can see that employing GPT-4-128k to directly answer the question with long contexts significantly outperforms RAG methods and even outperforms ReadAgent on three long-context benchmarks. This is because of the superior performance of GPT-4-128k in processing long texts and executing multi-hop reasoning tasks. Additionally, the lengths of these four benchmarks are significantly shorter than the 128 k context window, thereby mitigating the impact of "lost in the middle" on the model's performance.</p>
<p>Results of Agent-based Methods By comparing our approach with all baselines in Table 2, it is obvious that our approach consistently performs better than them on four long-context benchmarks and demonstrates superior performance in multihop long-context tasks. In our approach, benefiting</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input</th>
<th>HotpotQA</th>
<th></th>
<th></th>
<th></th>
<th>2WikiMultihopQA</th>
<th></th>
<th></th>
<th></th>
<th>MuSiQue</th>
<th></th>
<th></th>
<th></th>
<th>NarrativeQA</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Window</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LR-1</td>
<td>LR-2</td>
<td>EM</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>BM25 (top-1)</td>
<td>$4 k$</td>
<td>57.7</td>
<td>63.0</td>
<td>33.7</td>
<td>43.8</td>
<td>36.0</td>
<td>39.0</td>
<td>25.0</td>
<td>30.4</td>
<td>33.0</td>
<td>36.5</td>
<td>19.0</td>
<td>23.9</td>
<td>29.5</td>
<td>34.5</td>
<td>4.0</td>
<td>11.3</td>
</tr>
<tr>
<td>BM25 (top-3)</td>
<td>$4 k$</td>
<td>74.7</td>
<td>78.3</td>
<td>45.7</td>
<td>58.5</td>
<td>59.7</td>
<td>62.0</td>
<td>42.3</td>
<td>51.9</td>
<td>43.5</td>
<td>49.5</td>
<td>25.0</td>
<td>31.1</td>
<td>44.5</td>
<td>52.5</td>
<td>7.0</td>
<td>20.5</td>
</tr>
<tr>
<td>Ada-002 (top-1)</td>
<td>$4 k$</td>
<td>63.0</td>
<td>70.7</td>
<td>40.0</td>
<td>53.2</td>
<td>57.0</td>
<td>59.3</td>
<td>41.0</td>
<td>49.4</td>
<td>34.5</td>
<td>37.0</td>
<td>20.0</td>
<td>26.6</td>
<td>37.5</td>
<td>46.5</td>
<td>5.0</td>
<td>15.5</td>
</tr>
<tr>
<td>Ada-002 (top-3)</td>
<td>$4 k$</td>
<td>72.0</td>
<td>77.3</td>
<td>45.0</td>
<td>58.1</td>
<td>65.7</td>
<td>66.7</td>
<td>44.7</td>
<td>55.3</td>
<td>40.0</td>
<td>45.5</td>
<td>24.5</td>
<td>32.1</td>
<td>45.5</td>
<td>53.0</td>
<td>7.5</td>
<td>19.5</td>
</tr>
<tr>
<td>GPT-4-128k</td>
<td>128k</td>
<td>83.3</td>
<td>88.3</td>
<td>53.0</td>
<td>68.4</td>
<td>77.3</td>
<td>80.0</td>
<td>58.7</td>
<td>70.0</td>
<td>52.0</td>
<td>59.5</td>
<td>33.5</td>
<td>42.7</td>
<td>63.5</td>
<td>77.0</td>
<td>11.5</td>
<td>29.4</td>
</tr>
<tr>
<td>GPT-4-128k (chunk)</td>
<td>$4 k$</td>
<td>71.3</td>
<td>74.7</td>
<td>45.7</td>
<td>59.5</td>
<td>59.3</td>
<td>62.3</td>
<td>40.7</td>
<td>50.5</td>
<td>41.0</td>
<td>43.0</td>
<td>23.0</td>
<td>32.1</td>
<td>58.0</td>
<td>69.5</td>
<td>9.50</td>
<td>25.5</td>
</tr>
<tr>
<td>GPT-4-128k (chunk w/ notes)</td>
<td>$4 k$</td>
<td>72.3</td>
<td>76.7</td>
<td>45.7</td>
<td>59.5</td>
<td>65.7</td>
<td>68.7</td>
<td>46.3</td>
<td>56.6</td>
<td>39.5</td>
<td>43.0</td>
<td>25.0</td>
<td>32.5</td>
<td>56.5</td>
<td>65.0</td>
<td>8.5</td>
<td>24.3</td>
</tr>
<tr>
<td>ReadAgent</td>
<td>128k</td>
<td>72.3</td>
<td>78.7</td>
<td>48.0</td>
<td>62.0</td>
<td>79.0</td>
<td>81.0</td>
<td>52.7</td>
<td>63.7</td>
<td>54.5</td>
<td>61.0</td>
<td>35.0</td>
<td>45.1</td>
<td>63.0</td>
<td>75.5</td>
<td>5.0</td>
<td>18.9</td>
</tr>
<tr>
<td>Pearl</td>
<td>128k</td>
<td>74.7</td>
<td>79.0</td>
<td>46.3</td>
<td>60.4</td>
<td>70.0</td>
<td>71.0</td>
<td>46.0</td>
<td>57.6</td>
<td>45.0</td>
<td>51.5</td>
<td>23.0</td>
<td>33.3</td>
<td>43.5</td>
<td>48.0</td>
<td>7.5</td>
<td>16.2</td>
</tr>
<tr>
<td>LongRAG</td>
<td>128k</td>
<td>75.7</td>
<td>78.3</td>
<td>48.7</td>
<td>63.9</td>
<td>73.0</td>
<td>75.0</td>
<td>51.3</td>
<td>63.5</td>
<td>49.0</td>
<td>54.5</td>
<td>31.0</td>
<td>40.3</td>
<td>60.5</td>
<td>69.0</td>
<td>15.0</td>
<td>27.0</td>
</tr>
<tr>
<td>GraphRAG</td>
<td>128k</td>
<td>73.7</td>
<td>80.3</td>
<td>49.7</td>
<td>59.7</td>
<td>67.7</td>
<td>71.3</td>
<td>42.3</td>
<td>53.9</td>
<td>46.5</td>
<td>56.0</td>
<td>21.5</td>
<td>31.2</td>
<td>52.0</td>
<td>66.5</td>
<td>15.0</td>
<td>23.1</td>
</tr>
<tr>
<td>GraphReader</td>
<td>$4 k$</td>
<td>84.3</td>
<td>89.7</td>
<td>55.0</td>
<td>70.0</td>
<td>83.7</td>
<td>87.0</td>
<td>59.3</td>
<td>70.1</td>
<td>59.0</td>
<td>63.5</td>
<td>38.0</td>
<td>47.4</td>
<td>65.0</td>
<td>80.0</td>
<td>15.5</td>
<td>29.8</td>
</tr>
<tr>
<td>Golden</td>
<td>$4 k$</td>
<td>92.3</td>
<td>93.7</td>
<td>57.0</td>
<td>73.8</td>
<td>88.3</td>
<td>89.7</td>
<td>63.0</td>
<td>73.4</td>
<td>66.0</td>
<td>69.0</td>
<td>45.0</td>
<td>56.0</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: Performance (\%) comparison of different baselines on datasets from LongBench. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively. "Golden" denotes the settings in which we add question and its supporting facts to LLM directly.</p>
<p>| Method | Input <br> Window | HotpotWikiQA-mixup | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | LR-1 | LR-2 | $F_{1}{ }^{<em>}$ | LR-1 | LR-2 | $F_{1}{ }^{</em>}$ | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Method</th>
<th>Results(%)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LR-1</td>
<td>LR-2</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>GraphReader</td>
<td>84.3</td>
<td>89.7</td>
<td>70.0</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>81.7</td>
<td>87.7</td>
<td>63.8</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>66.0</td>
<td>71.7</td>
<td>54.1</td>
</tr>
<tr>
<td>2WikiMultihopQA</td>
<td>GraphReader</td>
<td>83.7</td>
<td>87.0</td>
<td>70.1</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>81.3</td>
<td>86.0</td>
<td>65.4</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>65.3</td>
<td>68.7</td>
<td>49.7</td>
</tr>
<tr>
<td>MuSiQue</td>
<td>GraphReader</td>
<td>59.0</td>
<td>63.5</td>
<td>47.4</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>56.0</td>
<td>61.0</td>
<td>42.4</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>35.0</td>
<td>38.5</td>
<td>25.2</td>
</tr>
<tr>
<td>NarrativeQA</td>
<td>GraphReader</td>
<td>65.0</td>
<td>80.0</td>
<td>29.8</td>
</tr>
<tr>
<td></td>
<td>w/o Rational Plan</td>
<td>63.0</td>
<td>78.5</td>
<td>26.6</td>
</tr>
<tr>
<td></td>
<td>w/o Node Selection</td>
<td>53.0</td>
<td>65.5</td>
<td>24.0</td>
</tr>
</tbody>
</table>
<p>Table 4: The results of our ablation study. "w/o Rational Plan" refers to removing the rational plan in the agent initialization stage, and "w/o Node Selection" denotes applying the random selection of initial nodes and neighbor nodes in graph exploration.
pact of extremely long context on our GraphReader. As shown in Table 3, compared with all baselines, our GraphReader not only consistently outperforms these methods across text lengths ranging from 16 k to 256 k tokens but also exhibits robustness with the expansion of context length. It indicates that our method is still effective in handling extremely long texts by graph exploration with limited context window LLMs. With the increase in the length of the input context, the performance of GPT-4-128k full-text reading degrades gradually. As a comparison, our method achieves a performance gain of $10.53 \%$ relatively on LR-1 over GPT-4-128k full-text reading under 16k context length. With the context length increasing to 128 k , our method achieves a performance gain of $75.00 \%$ relatively over GPT-4-128k. This can be attributed to the fact that as the context length increases, the impact of the "lost in the middle" effect on GPT-4-128k becomes progressively more severe. Secondly, we observe that ReadAgent significantly underperforms our method in handling extremely long contexts. This is because the lack of detailed information about the content of each page can make page selection very difficult for ReadAgent, especially when dealing with extremely long contexts. This further demonstrates that our method can effectively address the challenges of processing extremely long context with limited context window LLMs by exploring graphs containing fine-grained information.</p>
<h3>4.3 Ablation study</h3>
<p>The Effect of Rational Plan In the graph exploration stage, we introduce a rational plan to help the agent analyze complex input questions step by step, guiding the agent in exploring the graph. To verify the effectiveness of the rational plan, we removed it during agent initialization and conducted experiments on four long-context QA benchmarks. Table 4 shows that the rational plan is effective in guiding the agent in node selection and exploration on the graph.</p>
<p>The Effect of Node Selection We conduct randomly selecting initial nodes and neighbor nodes experiments to demonstrate the necessity of our system in selecting which nodes to visit based on reasoning about the required information. As shown in Table 4, random selection results in a significant performance drop, with an average decline of $18 \%$. This demonstrates that GraphReader carefully considers node selection, leading to more reasonable and effective exploration.</p>
<p>Impact of the Number of Initial Nodes We conduct experiments with different initial node counts on multi-hop and single-hop QA datasets to assess the effect of the number of initial nodes on GraphReader's performance. The results are shown in Figure 3. Increasing the number of nodes improves performance up to a certain point, with optimal performance at 5 initial nodes, which we set as the default. However, beyond this threshold, performance declines, especially in single-hop scenarios, likely due to increased noise from too many initial nodes.</p>
<p>Impact of the Chunk Size We investigate the impact of chunk size $L$ on GraphReader's performance. As shown in Figure 4, the best performance is achieved with $L=2 k$. When $L$ exceeds a certain threshold, performance declines because larger chunks cause the model to overlook essential details. Conversely, smaller chunks lead to more semantic truncation, hindering comprehension and accuracy in extracting atomic facts. Thus, we chose $L=2 k$ as the default chunk size.</p>
<h3>4.4 Further Analysis</h3>
<p>Cost Analysis To assess the inference cost of our approach, we compare the average token consumption of ReadAgent and GraphReader for individual questions. As shown in Table 5, GraphReader uses only 1.08 times more tokens than ReadAgent ( $52.8 \mathrm{k} / 48.7 \mathrm{k}$ ), yet achieves more than double the performance improvement, demonstrating its superiority. More importantly, our method has signifi-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Performance of GraphReader with different initial node numbers on 2WikiMultihopQA and NarrativeQA. Results show the robustness of GraphReader towards different initial node numbers.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The impact of chunk size <em>L</em> of GraphReader on the 256k length level of HotpotWikiQA-mixup.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Avg. Ctx. #Tokens</th>
<th>Avg. Cost #Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReadAgent</td>
<td>358.3k</td>
<td>48.7k</td>
</tr>
<tr>
<td>GraphReader</td>
<td>358.3k</td>
<td>52.8k</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of token consumption per question between ReadAgent and GraphReader on HotpotWikiQA-mixup-256k, where "Avg. Ctx. #Tokens" refers to the average token number of the original dataset. The "Avg. Cost #Tokens" comprise both input tokens and output tokens during exploration.</p>
<p>cant advantages in single-document multiple-query scenarios, where only one graph needs to be constructed. Subsequent QA can be performed on this graph, thereby reducing the overall token consumption.</p>
<p><strong>Recall Rate Analysis</strong> To evaluate our method's advantages in key information recall, we utilize GPT-4 to assess the recall of supporting facts on the HotpotWikiQA-mixup dataset. As shown in Figure 5, our model consistently outperforms other baseline methods, regardless of the input length. As context length increases from 16k to 256k, re-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Recall of supporting facts by different methods on HotpotWikiQA-mixup.</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Recall(%)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>SF-wise</td>
</tr>
<tr>
<td>Atomic Facts</td>
<td>76.4</td>
</tr>
<tr>
<td>Final Notebook</td>
<td>90.5</td>
</tr>
</tbody>
</table>
<p>Table 6: GraphReader's recall performance at different granularities on HotpotQA. "SF-wise" refers to the granularity of supporting facts, and "Sample-wise" refers to the granularity of sample evaluation.</p>
<p>call of supporting facts declines across all methods. However, GraphReader maintains around 60% recall at 256k context length, in contrast to the significant degradation in ReadAgent. This demonstrates GraphReader's scalability and effectiveness in processing long contexts. Further details and evaluation prompts can be found in Appendix F.</p>
<p>To further demonstrate the recall rate of GraphReader at different granularities, we calculate the recall rate of <em>Supporting Facts</em> and <em>Sample</em> granularity respectively using the same method, detailed in the Appendix F. The granularity of supporting facts refers to the recall rate of all supporting facts across the entire dataset. As for sample granularity, a sample is considered to be recalled only if all of its supporting facts are recalled. As shown in the Table 6, the recall for the final notebook is slightly higher than the recall of atomic facts, which indicates that our method is capable of extracting more valid information from chunks during the exploration, indirectly reflecting its intelligence and effectiveness in exploration.</p>
<h2>5 Conclusion</h2>
<p>This paper introduces GraphReader, a graph-based agent designed to enhance the long-context capabilities of large language models. GraphReader organizes long texts into graph structures and employs an autonomous agent to explore the graph, successfully establishing long-range dependencies within a relatively small 4 k context window. Experiments demonstrate that GraphReader outperforms GPT-4 with a 128 k input length across various long-context single-hop and multi-hop questionanswering benchmarks.</p>
<h2>6 Limitations</h2>
<p>Firstly, GraphReader is constructed using an off-the-shelf GPT-4 API. Since it is close-sourced, there may be potential restrictions such as limits on Queries Per Second (QPS) and regional constraints. Therefore, future work will involve collecting data, training models, and making them open-source to contribute to the wider community. Secondly, the efficiency of the agent depends on its planning and reasoning capabilities. Future research will also explore enhancements of these features to improve the effectiveness of our method.</p>
<h2>References</h2>
<p>Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. 2024a. Mt-bench-101: A fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. 2024b. Longalign: A recipe for long context alignment of large language models. arXiv preprint arXiv:2401.18058.</p>
<p>Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context understanding. ArXiv, abs/2308.14508.</p>
<p>Xingyuan Bu, Junran Peng, Junjie Yan, Tieniu Tan, and Zhaoxiang Zhang. 2021. Gaia: A transfer learning system of object detection that fits your needs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 274-283.</p>
<p>Howard Chen, Ramakanth Pasunuru, Jason Weston, and Asli Celikyilmaz. 2023a. Walking down the memory maze: Beyond context limit through interactive reading. arXiv preprint arXiv:2310.05029.</p>
<p>Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023b. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.</p>
<p>Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. 2023c. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307.</p>
<p>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.</p>
<p>Michiel De Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Fei Sha, and William Cohen. 2021. Mention memory: incorporating textual knowledge into transformers through entity mention attention. arXiv preprint arXiv:2110.06176.</p>
<p>Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. 2024. Longrope: Extending llm context window beyond 2 million tokens. arXiv preprint arXiv:2402.13753.</p>
<p>Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024. From local to global: A graph rag approach to query-focused summarization. ArXiv, abs/2404.16130.</p>
<p>Weixin Feng, Xingyuan Bu, Chenchen Zhang, and Xubin Li. 2022. Beyond bounding box: Multimodal knowledge learning for object detection. arXiv preprint arXiv:2205.04072.</p>
<p>Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. 2020. Entities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.</p>
<p>Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. 2024. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171.</p>
<p>Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752.</p>
<p>Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning steps. In COLING, pages 6609-6625. International Committee on Computational Linguistics.</p>
<p>Ziyan Jiang, Xueguang Ma, and Wenhu Chen. 2024. Longrag: Enhancing retrieval-augmented generation with long-context llms. ArXiv, abs/2406.15319.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2019. Generalization through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172.</p>
<p>Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948.</p>
<p>Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Trans. Assoc. Comput. Linguistics, 6:317-328.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>LangChain-team. 2024. LangChain.
Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. 2024. A human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727.</p>
<p>Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2023. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources. In The Twelfth International Conference on Learning Representations.</p>
<p>Jerry Liu. 2024. LlamaIndex.
Jie Liu, Zhanhui Zhou, Jiaheng Liu, Xingyuan Bu, Chao Yang, Zhong Han-Sen, and Wanli Ouyang. 2024a. Iterative length-regularized direct preference optimization: A case study on improving 7b language models to gpt-4 level. arXiv preprint arXiv:2406.11817.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024b. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, Chang Zhou, and Jingren Zhou. 2023. # instag: Instruction tagging for analyzing supervised fine-tuning of large language models. In The Twelfth International Conference on Learning Representations.</p>
<p>Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning on graphs: Faithful and interpretable large language model reasoning. arXiv preprint arXiv:2310.01061.</p>
<p>Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. 2024. Leave no context behind: Efficient infinite context transformers with infiniattention. arXiv preprint arXiv:2404.07143.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022. QuALITY: Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5336-5358, Seattle, United States. Association for Computational Linguistics.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.</p>
<p>Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023a. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071.</p>
<p>Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, and Junjie Yan. 2020. Largescale object detection in the wild from imbalanced multi-labels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9709-9718.</p>
<p>Junran Peng, Qing Chang, Haoran Yin, Xingyuan Bu, Jiajun Sun, Lingxi Xie, Xiaopeng Zhang, Qi Tian, and Zhaoxiang Zhang. 2023b. Gaia-universe: Everything is super-netify. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):11856-11868.</p>
<p>Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015. Yara parser: A fast and accurate dependency parser. Computing Research Repository, arXiv:1503.06733. Version 2.</p>
<p>Stephen E. Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3:333-389.</p>
<p>Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2023. Questions are all you need to train a dense passage retriever. Transactions of the Association for Computational Linguistics, 11:600-616.</p>
<p>Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D Manning. 2024. Raptor: Recursive abstractive processing for tree-organized retrieval. arXiv preprint arXiv:2401.18059.</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum,</p>
<p>and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697.</p>
<p>Simeng Sun, Kalpesh Krishna, Andrew MattarellaMicke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? arXiv preprint arXiv:2109.09115.</p>
<p>Simeng Sun, Yang Liu, Shuohang Wang, Dan Iter, Chenguang Zhu, and Mohit Iyyer. 2024. PEARL: Prompting large language models to plan and execute actions over long documents. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 469-486, St. Julian's, Malta. Association for Computational Linguistics.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question composition. Trans. Assoc. Comput. Linguistics, 10:539-554.</p>
<p>Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. 2024. Knowledge graph prompting for multi-document question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 19206-19214.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Wenhao Wu, Yizhong Wang, Yao Fu, Xiang Yue, Dawei Zhu, and Sujian Li. 2024a. Long context alignment with short instructions and synthesized positions. arXiv preprint arXiv:2405.03939.</p>
<p>Yanan Wu, Jie Liu, Xingyuan Bu, Jiaheng Liu, Zhanhui Zhou, Yuanxing Zhang, Chenchen Zhang, Zhiqi Bai, Haibin Chen, Tiezheng Ge, et al. 2024b. Conceptmath: A bilingual concept-wise benchmark for measuring mathematical reasoning of large language models. arXiv preprint arXiv:2402.14660.</p>
<p>Guipeng Xv, Si Chen, Chen Lin, Wanxian Guan, Xingyuan Bu, Xubin Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022. Visual encoding and debiasing for ctr prediction. In Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management, pages 4615-4619.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In EMNLP, pages 2369-2380. Association for Computational Linguistics.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.</p>
<p>Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang, Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu Yao, Dahua Lin, Boxun Li, Guohao Dai, Shengen Yan, and Yu Wang. 2024. Lv-eval: A balanced longcontext benchmark with 5 length levels up to 256k. ArXiv, abs/2402.05136.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. abs/2303.18223.</p>
<p>Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. 2023. Pose: Efficient context window extension of llms via positional skip-wise training. arXiv preprint arXiv:2309.10400.</p>
<p>A GraphReader Prompt</p>
<p>Figure 6 illustrates the prompt used for Graph Construction. Figures 7 to 11 present the prompts employed for Graph Exploration. Figure 12 shows the prompt used for Answer Reasoning.</p>
<h2>B LLM Rater Evaluation Details</h2>
<p>Given a question, a golden answer, and an answer to be evaluated, we utilize an LLM to assess the accuracy of the latter based on the question and correct answer. This involves two scores: LLM-Rating-1 (LR-1) and LLM-Rating-2 (LR-2), where LR-1 represents a strict scoring criterion, and LR-2 is a more lenient one. Following the approach of ReadAgent, if either LLM Rater deems an answer correct, it is considered as such. If the strict scorer finds an answer incorrect while the lenient scorer deems it partially correct, we classify the answer as partially correct; otherwise, it is adjudged incorrect. The prompts used for evaluation are presented in Figure 13 and Figure 14 respectively.</p>
<p>For the evaluation, we utilize GPT-4-128k as the LLM Rater, with the temperature set to 0.1 .</p>
<h2>C Dataset</h2>
<p>Multi-hop QA Datasets HotpotQA features a collection of 2-hop questions directly authored by native speakers, based on two interconnected paragraphs. 2WikiMultihopQA is comprised of complex questions up to 5 -hops in length, constructed through carefully designed templates to prevent the possibility of shortcut solutions.</p>
<p>In the MuSiQue dataset, questions are intricately crafted starting from straightforward scenarios that require up to 4 -hops reasoning. Annotators subsequently rephrase these with a dual purpose: to avoid shortcut answers and to maintain a natural linguistic quality. Each question within the original datasets is complemented by 2-4 supporting paragraphs, delivering evidence for simple one-step reasoning, alongside multiple paragraphs designed to serve as decoys.</p>
<p>HotpotWikiQA-mixup originates from LV-Eval and employs a construction method known as a mixup. This method randomly blends support documents with various distracting documents to generate five different context lengths for a given QA pair, including 16k, 32k, 64k, 128k, and 256k. Due to the excessive length of this dataset, we select the first 50 data entries from each different context length for experimentation to control costs.</p>
<p>Single-hop QA Datasets NarrativeQA is a dataset designed to test comprehension abilities for long documents, primarily sourced from movie scripts. As a single-hop QA dataset, the information required to answer its questions appears at a single location within the text.</p>
<p>Real-World Datasets QuALITY (Pang et al., 2022) is a long-text multiple-choice questionanswering dataset, with questions crafted by contributors who are familiar with the complete passages, making it more representative of real-world QA scenarios. We handle it as straightforward QA problems. Natural Questions (Kwiatkowski et al., 2019) includes real anonymous aggregated queries from Google along with corresponding Wikipedia pages, providing another excellent resource for authentic long-text QA situations.</p>
<h2>D Baseline Methods</h2>
<p>Full or Chunked Text Content For texts with fewer tokens than the LLM's input window, we can input the text directly into the LLM to obtain an answer. We refer to this method as Full Text Read, with the specific prompt provided in Figure 15. However, this approach is not applicable to texts exceeding the token limit of the LLM's input window. In such cases, Lee et al. truncated the text to fit it into the LLM, but this method obviously results in information loss. We propose a method that does not lose information, offering a better comparison. This method involves dividing the entire text into chunks (using the same chunking method as GraphReader) and then having the LLM read these chunks sequentially according to the text order, thus enabling the handling of overly long texts with a limited input window. During the reading process, there are two main strategies: Chunk Read and Chunk Read with Notes. In the Chunk Read approach, the LLM only sees the current chunk during each reading, which is suitable for single-hop QA tasks. In the Chunk Read with Notes approach, the LLM can summarize useful information from the current chunk and provide it to the subsequent reading process, which is suitable for multi-hop QA tasks.</p>
<p>In the experiment, we divide the chunks in the same way as GraphReader, and the maximum length of the chunk is set to 2 k . The specific prompts are in Figure 16 and 17 respectively.</p>
<p>Retrieval-Augmented Generation (RAG) RAG is a commonly used approach for addressing long-</p>
<p>text problems. In this work, we compare the traditional RAG method, including retrieval methods based on Okapi BM25 (Robertson and Zaragoza, 2009) and the OpenAI API embedding model (text-embedding-ada-002). Specifically, we first split the text into chunks in the same method as GraphReader, then use the aforementioned methods to calculate the relevance scores between the question and these chunks, and finally input the top$n$ chunks with the highest relevance scores together with the question for the LLM to answer. To ensure a fair comparison, we control the input window to 4 k in the experiments. Specifically, in order to fill the input window as much as possible, we set the maximum length of the chunk to 38 k when selecting the top-1 chunk for answering; when opting for the top-3 chunks, we set the maximum length of each chunk to 1 k . The specific prompt can be found in Figure 18.</p>
<p>In addition to traditional RAG methods, we also compared GraphRAG (Edge et al., 2024) and LongRAG (Jiang et al., 2024). GraphRAG utilizes LLMs to construct a graph-based text index in two distinct stages. The first stage involves extracting an entity knowledge graph from the source documents, while the second stage focuses on generating summaries for groups of entities. When a question is posed, each summary provides partial information, which is then combined into the final answer for the user. LongRAG introduces a "long retriever" and a "long reader", allowing the entire corpus to be processed into larger-sized units, which reduces the number of units needed during retrieval and alleviates the burden on the retriever.</p>
<p>Agent Style Methods We also compared our method with similar approaches for handling long texts with small input windows, such as ReadAgent (Lee et al., 2024). ReadAgent is a method that segments long texts and generates gist memories, which are then looked up to search for information in order to answer questions. In the experiments, for datasets from LongBench, we adopted the default hyperparameters declared in the ReadAgent paper, specifically a max_words of 600 and min_words of 280 when splitting pages. For HotpotWikiQA-mixup from LV-Eval, we scaled these two hyperparameters using the same approach as in the ReadAgent paper. Specifically, for datasets with lengths of 256 k and 128 k , we used max_words=10000 and min_words=2000; for those with lengths of $64 \mathrm{k}, 32 \mathrm{k}$ and 16 k , we used max_words=5000 and min_words=1000. At the same time, we employed the ReadAgent-S method, which ReadAgent claims to be the most effective, reading the pages in sequence. Additionally, we allowed reading up to 5 pages (Look up 1-5 pages).</p>
<p>We also compared PEARL (Sun et al., 2024), a prompting framework to enhance reasoning capabilities for long documents. PEARL is structured into three stages: action mining, plan formulation, and plan execution. It decomposes complex questions into actionable steps and utilizes LLMs for zero-shot or few-shot prompting execution.</p>
<h2>E Additional Experimental Results</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input <br> Window</th>
<th>QuALITY</th>
<th></th>
<th></th>
<th></th>
<th>Natural Question</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LB-1</td>
<td>LB-2</td>
<td>EM</td>
<td>$F_{1}$</td>
<td>LB-1</td>
<td>LB-2</td>
<td>EM</td>
<td>$F_{1}$</td>
</tr>
<tr>
<td>GPT-4-128k</td>
<td>128 k</td>
<td>45.7</td>
<td>60.3</td>
<td>2.7</td>
<td>9.9</td>
<td>75.0</td>
<td>81.0</td>
<td>41.0</td>
<td>57.2</td>
</tr>
<tr>
<td>Pearl</td>
<td>128 k</td>
<td>52.3</td>
<td>72.7</td>
<td>3.0</td>
<td>9.6</td>
<td>74.0</td>
<td>79.0</td>
<td>38.0</td>
<td>56.8</td>
</tr>
<tr>
<td>LongRAG</td>
<td>128 k</td>
<td>52.3</td>
<td>67.0</td>
<td>3.7</td>
<td>11.6</td>
<td>77.7</td>
<td>83.0</td>
<td>47.3</td>
<td>60.0</td>
</tr>
<tr>
<td>GraphRAG</td>
<td>128 k</td>
<td>46.0</td>
<td>68.7</td>
<td>2.0</td>
<td>6.1</td>
<td>67.0</td>
<td>77.0</td>
<td>47.0</td>
<td>53.2</td>
</tr>
<tr>
<td>GraphReader</td>
<td>4 k</td>
<td>57.3</td>
<td>82.3</td>
<td>4.3</td>
<td>14.3</td>
<td>79.0</td>
<td>84.7</td>
<td>48.3</td>
<td>62.1</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance (\%) comparison of different baselines on two additional datasets. The best performance and the second-best performance are denoted in bold and underlined fonts, respectively.</p>
<p>Table 7 presents additional experimental results for two datasets, QuALITY and Natural Questions, both of which are highly relevant to real-world question-answering scenarios. The results indicate that our method significantly outperforms other baseline models in real-world scenarios.</p>
<h2>F Evaluation Recall for Supporting Facts</h2>
<p>We evaluate the recall rate of supporting facts for different methods using GPT-4-128k, with the temperature set to 0.1 . Figure 19 shows the specific evaluation prompt.</p>
<p>For GraphReader, we evaluate the memory recorded in the final notebook. For ReadAgent, the evaluation focused on the final text segments reviewed. In the case of Chunk Read with Notes, we evaluate both the memory and the chunk read at the time of the final answer; for the RAG methods, we assess the retrieved chunks.</p>
<h2>G The Analysis of Function Calls</h2>
<p>To verify the rationality and utility of agent actions under various circumstances of GraphReader, we made statistics on its function calls at each stage across two datasets. From the statistical results in Table 8, it can be observed that each piece of data</p>
<p>will perform an average of 3 to 4 actions, corresponding to the average number of function calls in the table. This indicates the effectiveness of the graph we constructed, with GraphReader being able to swiftly locate key information while minimizing resource usage. Furthermore, each action has a certain probability of being chosen, justifying the rationality of the action set. Among them, the most commonly used action on multi-hop QA tasks is to read neighbor nodes, and the most common action on single-hop QA tasks is to read chunks. This difference is caused by the fact that multi-hop questions need to gather information contained by multiple nodes to answer questions, while singlehop data sets often require only one atomic fact.</p>
<h1>H Statistics of Graph</h1>
<p>The statistics of graphs from various datasets are presented in Table 9. For longer texts, there tends to be a higher average number of nodes and atomic facts. After normalization, each node has an average of about 10 neighbor nodes. This is because the number of key elements occurring simultaneously in each atomic fact is generally of this magnitude. Furthermore, the aggregation of similar nodes caused by normalization results in a slight increase in the number of neighboring nodes.</p>
<p>On average, each node is associated with about 2 atomic facts, and the average number of atomic facts in the node with the most atomic facts in each graph ranges from 15 to 50 , indicating a relatively even distribution of atomic facts. The maximum average number of atomic facts is found in NarrativeQA, a possible explanation being that NarrativeQA is mainly derived from movie scripts, where characters, such as the protagonist, appear frequently throughout the text, thus including a larger number of atomic facts.</p>
<h2>I GraphReader Example</h2>
<p>This section presents a case study of the GraphReader workflow. Figure 20 displays the posed question alongside the answer and pertinent supporting passages. Subsequently, Figure 21 delineates the methodology for constructing the graph. Figure 22 further elaborates on the initialization of a pre-planned rational path by GraphReader and the selection of initial nodes. Figure 23 illustrates the sequence of function invocations during the exploration phase. Finally, Figure 24 showcases how GraphReader formulates the answer by leveraging the insights obtained through exploration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">#Avg. Function Call</th>
<th style="text-align: center;">Stage</th>
<th style="text-align: center;">Stage Ratio(\%)</th>
<th style="text-align: center;">Function</th>
<th style="text-align: center;">Call Ratio(\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 46.5 \ &amp; 53.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 12.1 \ &amp; 21.1 \ &amp; 22.9 \ &amp; 43.9 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 35.5 \ &amp; 65.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">2WikiMultihopQA</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 48.6 \ &amp; 51.4 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 14.5 \ &amp; 25.1 \ &amp; 23.3 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 37.3 \ &amp; 62.7 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">MuSiQue</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 41.3 \ &amp; 58.7 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">31.2</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 19.1 \ &amp; 26.6 \ &amp; 25.7 \ &amp; 28.6 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 40.1 \ &amp; 59.9 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">NarrativeQA</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">Exploring Atomic Facts</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">read_chunk <br> stop_and_read_neighbor</td>
<td style="text-align: center;">$\begin{aligned} &amp; 64.5 \ &amp; 35.5 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Chunks</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">search_more <br> read_previous_chunk <br> read_subsequent_chunk <br> termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 4.1 \ &amp; 35.3 \ &amp; 32.6 \ &amp; 28.0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Exploring Neighbors</td>
<td style="text-align: center;">13.2</td>
<td style="text-align: center;">read_neighbor_node termination</td>
<td style="text-align: center;">$\begin{aligned} &amp; 51.4 \ &amp; 48.6 \end{aligned}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Statistics of function calls on MuSiQue and NarrativeQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample Dimension</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sample \&amp; Node Dimension</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">node num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">atomic facts num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">neighbor node num</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">atomic facts num</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">avg.</td>
<td style="text-align: center;">max</td>
<td style="text-align: center;">avg. avg.</td>
<td style="text-align: center;">avg. max</td>
<td style="text-align: center;">avg. avg.</td>
<td style="text-align: center;">avg. max</td>
</tr>
<tr>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">583.8</td>
<td style="text-align: center;">1945.0</td>
<td style="text-align: center;">244.0</td>
<td style="text-align: center;">645.0</td>
<td style="text-align: center;">10.1</td>
<td style="text-align: center;">263.1</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">17.8</td>
</tr>
<tr>
<td style="text-align: center;">2WikiMultihopQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">515.8</td>
<td style="text-align: center;">1691.0</td>
<td style="text-align: center;">217.7</td>
<td style="text-align: center;">545.0</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">215.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">MusiQue</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1029.4</td>
<td style="text-align: center;">2142.0</td>
<td style="text-align: center;">419.9</td>
<td style="text-align: center;">586.0</td>
<td style="text-align: center;">9.3</td>
<td style="text-align: center;">253.4</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">15.6</td>
</tr>
<tr>
<td style="text-align: center;">NarrativeQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">966.0</td>
<td style="text-align: center;">3110.0</td>
<td style="text-align: center;">515.5</td>
<td style="text-align: center;">1296.0</td>
<td style="text-align: center;">10.3</td>
<td style="text-align: center;">652.6</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">50.0</td>
</tr>
<tr>
<td style="text-align: center;">HotpotWikiQA-mixup</td>
<td style="text-align: center;">16 k</td>
<td style="text-align: center;">1741.6</td>
<td style="text-align: center;">3822.0</td>
<td style="text-align: center;">749.7</td>
<td style="text-align: center;">1043.0</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">231.0</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">32 k</td>
<td style="text-align: center;">2827.3</td>
<td style="text-align: center;">5086.0</td>
<td style="text-align: center;">1257.4</td>
<td style="text-align: center;">1694.0</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">263.3</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">29.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">64 k</td>
<td style="text-align: center;">5054.1</td>
<td style="text-align: center;">8918.0</td>
<td style="text-align: center;">2360.0</td>
<td style="text-align: center;">3015.0</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">227.2</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">17.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">128 k</td>
<td style="text-align: center;">8828.5</td>
<td style="text-align: center;">14592.0</td>
<td style="text-align: center;">4437.9</td>
<td style="text-align: center;">5182.0</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">302.0</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">19.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">256 k</td>
<td style="text-align: center;">14853.3</td>
<td style="text-align: center;">24981.0</td>
<td style="text-align: center;">8632.8</td>
<td style="text-align: center;">9478.0</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">427.6</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">27.8</td>
</tr>
</tbody>
</table>
<p>Table 9: Graph statistical data. Under the Sample dimension, "avg." indicates the average number of nodes in each graph, and "max" refers to the largest node count across all graphs. The same logic applies to atomic facts num. In the Sample \&amp; Node dimensions, "avg. avg." denotes the average of the average neighbor node counts per graph, and "avg. max" means the average of the maximum neighbor node counts per graph. This approach is also used for counting atomic facts num.</p>
<p>You are now an intelligent assistant tasked with meticulously extracting both key elements and atomic facts from a long text.</p>
<ol>
<li>Key Elements: The essential nouns (e.g., characters, times, events, places, numbers), verbs (e.g., actions), and adjectives (e.g., states, feelings) that are pivotal to the text's narrative.</li>
<li>Atomic Facts: The smallest, indivisible facts, presented as concise sentences. These include propositions, theories, existences, concepts, and implicit elements like logic, causality, event sequences, interpersonal relationships, timelines, etc.</li>
</ol>
<p>Requirements:
#####</p>
<ol>
<li>Ensure that all identified key elements are reflected within the corresponding atomic facts.</li>
<li>You should extract key elements and atomic facts comprehensively, especially those that are important and potentially query-worthy and do not leave out details.</li>
<li>Whenever applicable, replace pronouns with their specific noun counterparts (e.g., change I, He, She to actual names).</li>
<li>Ensure that the key elements and atomic facts you extract are presented in the same language as the original text (e.g., English or Chinese).</li>
<li>You should output a total of key elements and atomic facts that do not exceed 1024 tokens.</li>
<li>Your answer format for each line should be: [Serial Number], [Atomic Facts], [List of Key Elements, separated with 'l']
#####</li>
</ol>
<p>Example:
#####
User:
One day, a father and his little son ......
Assistant:</p>
<ol>
<li>One day, a father and his little son were going home. I father I little son I going home</li>
<li>......
#####
Please strictly follow the above format. Let's begin.</li>
</ol>
<p>Figure 6: The prompt for key elements and atomic facts extraction.</p>
<p>As an intelligent assistant, your primary objective is to answer the question by gathering supporting facts from a given article. To facilitate this objective, the first step is to make a rational plan based on the question. This plan should outline the step-by-step process to resolve the question and specify the key information required to formulate a comprehensive answer.</p>
<p>Example:
#####
User: Who had a longer tennis career, Danny or Alice?
Assistant: In order to answer this question, we first need to find the length of Danny's and Alice's tennis careers, such as the start and retirement of their careers, and then compare the two.
#####
Please strictly follow the above format. Let's begin.</p>
<p>Figure 7: The prompt for rational plan.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information contained within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Chunks of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to check a list of nodes, with the objective of selecting the most relevant initial nodes from the graph to efficiently answer the question. You are given the question, the rational plan, and a list of node key elements. These initial nodes are crucial because they are the starting point for searching for relevant information.</p>
<p>Requirements:
#####</p>
<ol>
<li>Once you have selected a starting node, assess its relevance to the potential answer by assigning a score between 0 and 100 . A score of 100 implies a high likelihood of relevance to the answer, whereas a score of 0 suggests minimal relevance.</li>
<li>Present each chosen starting node in a separate line, accompanied by its relevance score. Format each line as follows: Node: [Key Element of Node], Score: [Relevance Score].</li>
<li>Please select at least 10 starting nodes, ensuring they are non-repetitive and diverse.</li>
<li>In the user's input, each line constitutes a node. When selecting the starting node, please make your choice from those provided, and refrain from fabricating your own. The nodes you output must correspond exactly to the nodes given by the user, with identical wording.
#####</li>
</ol>
<p>Example:
#####
User:
Question: {QUESTION}
Plan: {RATIONAL PLAN}
Nodes: {LIST OF KEY ELEMENTS}
Assistant:{LIST OF SELECTED NODES}
#####</p>
<p>Finally, I emphasize again that you need to select the starting node from the given Nodes, and it must be consistent with the words of the node you selected. Please strictly follow the above format. Let's begin.</p>
<p>Figure 8: The prompt for initial node selection.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information contained within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Chunks of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to check a node and its associated atomic facts, with the objective of determining whether to proceed with reviewing the text chunk corresponding to these atomic facts. Given the question, the rational plan, previous actions, notebook content, and the current node's atomic facts and their corresponding chunk IDs, you have the following Action Options:
$# # # # #$</p>
<ol>
<li>read_chunk(List[ID]): Choose this action if you believe that a text chunk linked to an atomic fact may hold the necessary information to answer the question. This will allow you to access more complete and detailed information.</li>
<li>stop_and_read_neighbor(): Choose this action if you ascertain that all text chunks lack valuable information.
$# # # # #$</li>
</ol>
<p>Strategy:
$# # # # #$</p>
<ol>
<li>Reflect on previous actions and prevent redundant revisiting nodes or chunks.</li>
<li>You can choose to read multiple text chunks at the same time.</li>
<li>Atomic facts only cover part of the information in the text chunk, so even if you feel that the atomic facts are slightly relevant to the question, please try to read the text chunk to get more complete information.
$# # # # #$</li>
</ol>
<p>Response format:
$# # # # #$
<em>Updated Notebook</em>: First, combine your current notebook with new insights and findings about the question from current atomic facts, creating a more complete version of the notebook that contains more valid information.
<em>Rationale for Next Action</em>: Based on the given question, the rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: read_chunk(List[ID]) or stop_and_read_neighbor(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
$# # # # #$</p>
<p>Finally, it is emphasized again that even if the atomic fact is only slightly relevant to the question, you should still look at the text chunk to avoid missing information. You should only choose stop_and_read_neighbor() when you are very sure that the given text chunk is irrelevant to the question. Please strictly follow the above format. Let's begin.</p>
<p>Figure 9: The prompt for exploring atomic facts.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Segments of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to assess a specific text chunk and determine whether the available information suffices to answer the question. Given the question, rational plan, previous actions, notebook content, and the current text chunk, you have the following Action Options:
#####</p>
<ol>
<li>search_more(): Choose this action if you think that the essential information necessary to answer the question is still lacking.</li>
<li>read_previous_chunk(): Choose this action if you feel that the previous text chunk contains valuable information for answering the question.</li>
<li>read_subsequent_chunk(): Choose this action if you feel that the subsequent text chunk contains valuable information for answering the question.</li>
<li>termination(): Choose this action if you believe that the information you have currently obtained is enough to answer the question. This will allow you to summarize the gathered information and provide a final answer.
#####</li>
</ol>
<p>Strategy:
#####
5. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.
6. You can only choose one action.
#####</p>
<p>Response format:
#####
<em>Updated Notebook</em>: First, combine your previous notes with new insights and findings about the question from current text chunks, creating a more complete version of the notebook that contains more valid information.
<em>Rationale for Next Action</em>: Based on the given question, rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: search_more() or read_previous_chunk() or read_subsequent_chunk() or termination(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
#####</p>
<p>Please strictly follow the above format. Let's begin.</p>
<p>Figure 10: The prompt for exploring chunks.</p>
<p>As an intelligent assistant, your primary objective is to answer questions based on information within a text. To facilitate this objective, a graph has been created from the text, comprising the following elements:</p>
<ol>
<li>Text Chunks: Segments of the original text.</li>
<li>Atomic Facts: Smallest, indivisible truths extracted from text chunks.</li>
<li>Nodes: Key elements in the text (noun, verb, or adjective) that correlate with several atomic facts derived from different text chunks.</li>
</ol>
<p>Your current task is to assess all neighboring nodes of the current node, with the objective of determining whether to proceed to the next neighboring node. Given the question, rational plan, previous actions, notebook content, and the neighbors of the current node, you have the following Action Options:
#####</p>
<ol>
<li>read_neighbor_node(key element of node): Choose this action if you believe that any of the neighboring nodes may contain information relevant to the question. Note that you should focus on one neighbor node at a time.</li>
<li>termination(): Choose this action if you believe that none of the neighboring nodes possess information that could answer the question.
#####</li>
</ol>
<p>Strategy:
#####
3. Reflect on previous actions and prevent redundant revisiting of nodes or chunks.
4. You can only choose one action. This means that you can choose to read only one neighbor node or choose to terminate.
#####</p>
<p>Response format:
####
<em>Rationale for Next Action</em>: Based on the given question, rational plan, previous actions, and notebook content, analyze how to choose the next action.
<em>Chosen Action</em>: read_neighbor_node(neighbor_node) or termination(). (Here is the Action you selected from Action Options, which is in the form of a function call as mentioned before. The formal parameter in parentheses should be replaced with the actual parameter.)
#####</p>
<p>Please strictly follow the above format. Let's begin.</p>
<p>Figure 11: The prompt for exploring neighbors.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>https://platform.openai.com/docs/guides/embeddings/ embedding-models&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>https://github.com/openai/tiktoken&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
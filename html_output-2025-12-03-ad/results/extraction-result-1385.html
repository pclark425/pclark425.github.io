<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1385 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1385</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1385</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-91fb21898301e80633d47d39b425d1feaa98f6ed</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/91fb21898301e80633d47d39b425d1feaa98f6ed" target="_blank">Policy-Guided Diffusion</a></p>
                <p><strong>Paper Venue:</strong> RLJ</p>
                <p><strong>Paper TL;DR:</strong> This work shows that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline.</p>
                <p><strong>Paper Abstract:</strong> In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained - requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1385.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1385.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PGD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy-Guided Diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trajectory-level diffusion-based generative model trained on offline data that is guided at sampling time by the target policy to produce synthetic, near-on-policy trajectories while remaining regularized to the behavior data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Policy-Guided Diffusion (PGD)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A trajectory diffusion model (U-Net denoiser) is trained on full trajectories from the offline dataset to model p_off(τ); during sampling the diffusion score is augmented with gradients from the target policy acting on actions only, producing samples from a behavior-regularized target distribution proportional to p_off(τ) · q_target(τ)^λ. Guidance is applied to the denoised actions, normalized, with a cosine-guidance schedule and second-order corrections during EDM sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>trajectory-level diffusion world model (neural generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>offline reinforcement learning (continuous control: MuJoCo locomotion, Maze2d navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Dynamics error measured as mean squared error (MSE) between synthetic trajectory states and the corresponding true environment-states when rolling out the same initial state and action sequence; trajectory/action likelihood measured as mean log-likelihood of synthetic actions under the target policy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitatively low dynamics error (lower MSE than PETS across 16-step rollouts) while achieving high target-policy action likelihood as guidance λ increases; experiments show PGD matches PETS action-likelihood at λ≈1.0 and yields significantly lower dynamics error across tested target policies (no numeric MSE values reported in the paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural generative model (U-Net diffusion); no claims of inherent interpretability of latent factors or explicit dynamics decomposition are made in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned for extracting interpretable factors; guidance is interpretable conceptually (explicitly increases action likelihood under the target policy) but no visualization/latent analysis methods are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model architecture: U-Net with 1024 features, 3 U-Net blocks; diffusion sampling uses EDM with 256 timesteps (found to be sufficient); training hyperparameters include batch size 16, 250 dataset epochs. No wall-clock GPU/time totals are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared qualitatively: avoids compounding autoregressive model error by generating whole trajectories in one shot (thus less truncation), but uses many diffusion timesteps per sample (256) so per-sample generation cost is non-negligible; empirically provides better fidelity-vs-coverage tradeoff than autoregressive PETS and unguided diffusion according to Table 1 and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Using PGD-generated synthetic data as a drop-in replacement for real data improved downstream offline RL performance: e.g., an aggregated statistically significant 11.2% improvement for TD3+BC across MuJoCo locomotion tasks versus training on the real dataset; larger improvements reported on Maze2d tasks. Exact per-task numbers are in Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PGD's higher action-likelihood trajectories (closer to the target policy) translate to better downstream policy learning compared to both unguided diffusion and training on the raw offline dataset; importantly, PGD retains low dynamics error so improved action-likelihood yields usable synthetic experience.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Balances three axes: dynamics error (kept low by staying near data manifold via p_off term), action-likelihood (increased via guidance λ), and coverage (full-trajectory generation yields high coverage). Increasing λ trades-off more on-policy bias for potential dynamics error; guidance schedule and normalizing gradients are used to stabilize this trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Train trajectory diffusion on offline data (direct generation); apply guidance only to action components (not states) to avoid expensive/backprop-heavy and high-variance state gradients; normalize guidance gradients and apply them to denoised actions; use a guidance coefficient λ with a cosine schedule (β=0.3 used), EDM sampling with 256 timesteps, and U-Net denoiser architecture (features=1024, 3 blocks). Behavior policy gradients are omitted (behavior policy unknown) — this omission acts as regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against (1) autoregressive single-step world models (high compounding error, require truncated rollouts → limited coverage or biased start states), (2) unguided trajectory diffusion (low dynamics error and high coverage but low target-policy likelihood), and (3) PETS (MOPO-style autoregressive ensemble): PGD achieves similar or higher action-likelihood while producing lower dynamics error than PETS in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends tuning the guidance coefficient λ (λ≈1.0 matched PETS likelihood in experiments), applying guidance to denoised actions only, normalizing guidance gradients, and using a cosine guidance schedule to reduce dynamics error; periodic regeneration of synthetic datasets (longer training between regenerations) yielded more stable/better results than very frequent continuous generation in their experiments. Modeling behavior policy (behavior cloning) is suggested as future work to enable explicit behavior-guidance when desired.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Policy-Guided Diffusion', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1385.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1385.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trajectory diffusion (unguided)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unguided Trajectory Diffusion Model (trajectory-level diffusion without policy guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion generative model trained on entire trajectories from the offline dataset and sampled without any target-policy guidance, producing many diverse trajectories covering state space but remaining centered on the behavior distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synthetic experience replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unguided trajectory diffusion model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A U-Net denoiser diffusion model trained on stacked trajectories (states, actions, rewards, dones) to model the behavior trajectory distribution p_off(τ); sampling follows EDM denoising without adding any policy-gradient guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>trajectory-level diffusion world model (neural generative model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>offline RL data augmentation (MuJoCo, Maze2d datasets in D4RL benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Measured by dynamics error (MSE of states vs environment rollout) and action-likelihood under target policies (mean log-likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Low dynamics error (comparable to PGD in absence of guidance) but low action-likelihood under the target policy because samples reflect behavior distribution; specific numeric values not provided in the text but shown qualitatively in figures.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural model; no interpretability analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Same U-Net and EDM sampling settings as PGD (e.g., U-Net features=1024, diffusion timesteps up to 256); no explicit compute-time numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to PGD: similar computational sampling cost, higher coverage than autoregressive models, but lower utility for on-policy learning because generated actions have low likelihood under target policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used to train agents, unguided diffusion gives worse downstream policy performance compared to PGD and sometimes worse than training on real data, because trajectories remain off-distribution relative to improving target policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High coverage and low dynamics error make unguided diffusion useful for augmenting datasets, but because it samples from p_off it does not reduce distribution shift to the target policy and thus offers limited utility for improving on-policy performance without additional guidance or policy regularization.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Improves coverage and avoids compounding autoregressive-model error, but sacrifices on-policy relevance (low target-policy likelihood), limiting effectiveness for learning policies that exceed behavior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Direct trajectory modeling (stacking transitions and 1D convolutions across the sequence), EDM sampling with many timesteps, U-Net denoiser architecture; no policy-gradient guidance term is used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to autoregressive world models: avoids compounding transition error and truncation; compared to PGD: lower action-likelihood and downstream utility; compared to real dataset: higher coverage but off-policy nature can hurt on-policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests unguided diffusion is a good baseline for coverage and low dynamics error, but to improve downstream task utility one should add policy guidance (PGD) or other mechanisms to increase target-policy action-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Policy-Guided Diffusion', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1385.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1385.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autoregressive world model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autoregressive single-step transition world model (model T)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned one-step transition model trained on offline data that is used in an autoregressive fashion by rolling out the target policy into the model to generate synthetic on-policy data; susceptible to compounding model errors and therefore typically yields truncated rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive one-step transition model (T)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A model approximating T(s_{t+1} | s_t, a_t; θ) trained from dataset D_off; to generate on-policy samples the initial state s_0 is taken from D_off and the target policy is iteratively rolled out using the learned dynamics model. In practice rollouts are truncated to k ≪ H to avoid compounding error.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive explicit transition model (neural dynamics model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>offline reinforcement learning (general continuous-control tasks in D4RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Dynamics error via compounding error over multistep rollouts; paper references truncation length k and qualitative bias measures from starting-state mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>High compounding error for long rollouts; in practice rollout length k is small (k ≤ 5 cited) to maintain low immediate-step error but this limits coverage; no precise numeric MSEs are provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is a learned neural dynamics approximator (black-box); no interpretability analyses discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Per-step inference cost is typically small relative to full-trajectory diffusion but generating long trajectories requires repeated model queries and can be halted early to avoid compounding error; no explicit compute numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>More sample-efficient in short truncated rollouts but suffers from bias/coverage trade-offs: unbiased if rolled from initial states but low coverage; larger coverage requires sampling from dataset states and introduces bias. Compared to PGD, autoregressive models suffer compounding error and limited practical rollout lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used in prior model-based offline RL, autoregressive models can produce on-policy-like data for short horizons but policies can exploit model errors, leading to value overestimation and degraded performance if rollouts are too long; paper argues this motivates PGD.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High utility for short-horizon synthetic data generation but limited utility for generating long-horizon, high-coverage synthetic experience due to compounding errors and the need to truncate rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-off between coverage and bias: rollouts from initial states are unbiased but low coverage; rollouts from arbitrary dataset states improve coverage but bias the start-state distribution; longer rollouts increase compounding error.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Learn single-step transition model from offline data; use short rollouts (k ≪ H), sample initial states from dataset or from episode starts; apply model ensembles or pessimism penalties in prior work (e.g., PETS/MOPO) to mitigate model exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Versus trajectory diffusion (PGD/unguided): autoregressive suffers compounding error and thus must truncate, yielding lower coverage or biased starts; PETS (an ensemble autoregressive model) matches or exceeds target-policy likelihood in some settings but often has higher dynamics error than PGD on OOD policies according to the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper indicates that, for autoregressive models, the pragmatic optimal is to restrict rollouts to a small k (≤5) and/or apply pessimism/penalties to account for dynamics error; however, this still fails to fully address out-of-sample generalization compared to direct trajectory generation with guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Policy-Guided Diffusion', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1385.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1385.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PETS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PETS (probabilistic ensemble trajectory simulator / MOPO-style PETS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble-based autoregressive probabilistic dynamics model (used here as a representative autoregressive baseline) that predicts single-step transitions and is used for short rollouts; in this paper PETS is used as the autoregressive baseline for fidelity comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning in a handful of trials using probabilistic dynamics models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PETS (ensemble probabilistic transition model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An ensemble of probabilistic one-step dynamics models that produce next-state distributions; used in MOPO-style offline model-based methods and in this paper as a representative autoregressive baseline (PETS rollouts are compared to PGD trajectories for dynamics error and action-likelihood).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive ensemble dynamics model (neural probabilistic ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>offline reinforcement learning (used in experiments on HalfCheetah datasets for synthetic trajectory comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Dynamics error measured as MSE between sampled synthetic trajectory states and true environment states over fixed rollout length (16 steps in the paper's analysis); action-likelihood under target policy also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Empirically in this paper PETS attains comparable target-policy action likelihood to PGD when PGD uses sufficient guidance (λ≈1.0), but PETS exhibits significantly higher dynamics error than PGD across evaluated target policies, especially for out-of-distribution target policies (random and expert). Exact numeric MSE values are shown in figure data but are not reproduced as absolute numbers in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Ensemble of learned neural models; treated as black-box approximator without interpretability analyses in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Autoregressive inference requires per-step ensemble predictions; ensemble methods typically have increased compute compared to single model per-step, and repeated rollouts incur further cost; the paper does not provide explicit compute/time measurements for PETS within their experiments but uses publicly available PETS model weights for comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Compared to PGD: PETS can match or exceed action-likelihood but at the cost of higher dynamics error and fragility to OOD target policies. Compared to truncated autoregressive rollouts, PETS still suffers compounding error for longer rollouts and higher error on OOD policies.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>PETS is used here only as a fidelity baseline (trajectory-level comparison); the paper does not report downstream RL performance when training agents on PETS-generated synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Although PETS can produce trajectories with high target-policy likelihood (when actions are sampled directly from the policy inside the model), the higher dynamics error reduces the utility of those trajectories for training robust policies under distribution shift; PGD yields lower dynamics error for similar action-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Ensemble autoregressive models like PETS trade off per-step modeling expressiveness for cumulative compounding error over long rollouts, leading to higher MSE on OOD target policies despite good per-action likelihoods.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use ensembles of probabilistic next-step models (MOPO-style) to capture epistemic uncertainty and reduce model exploitation; when rollouts are truncated this reduces compounding error but limits coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PETS vs PGD: both can produce actions representative of the target policy, but PETS shows higher dynamics error over 16-step rollouts and worse robustness to OOD target policies. PGD avoids iterative rollouts and therefore avoids compounding errors inherent in PETS.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper does not give an optimal PETS configuration; it uses PETS as an established autoregressive baseline and shows that PGD achieves lower dynamics error for comparable action-likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Policy-Guided Diffusion', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning in a handful of trials using probabilistic dynamics models <em>(Rating: 2)</em></li>
                <li>Synthetic experience replay <em>(Rating: 2)</em></li>
                <li>MOPO: Model-based Offline Policy Optimization <em>(Rating: 2)</em></li>
                <li>Diffusion world models <em>(Rating: 2)</em></li>
                <li>Planning with diffusion for flexible behavior synthesis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1385",
    "paper_id": "paper-91fb21898301e80633d47d39b425d1feaa98f6ed",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "PGD",
            "name_full": "Policy-Guided Diffusion",
            "brief_description": "A trajectory-level diffusion-based generative model trained on offline data that is guided at sampling time by the target policy to produce synthetic, near-on-policy trajectories while remaining regularized to the behavior data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Policy-Guided Diffusion (PGD)",
            "model_description": "A trajectory diffusion model (U-Net denoiser) is trained on full trajectories from the offline dataset to model p_off(τ); during sampling the diffusion score is augmented with gradients from the target policy acting on actions only, producing samples from a behavior-regularized target distribution proportional to p_off(τ) · q_target(τ)^λ. Guidance is applied to the denoised actions, normalized, with a cosine-guidance schedule and second-order corrections during EDM sampling.",
            "model_type": "trajectory-level diffusion world model (neural generative model)",
            "task_domain": "offline reinforcement learning (continuous control: MuJoCo locomotion, Maze2d navigation)",
            "fidelity_metric": "Dynamics error measured as mean squared error (MSE) between synthetic trajectory states and the corresponding true environment-states when rolling out the same initial state and action sequence; trajectory/action likelihood measured as mean log-likelihood of synthetic actions under the target policy.",
            "fidelity_performance": "Qualitatively low dynamics error (lower MSE than PETS across 16-step rollouts) while achieving high target-policy action likelihood as guidance λ increases; experiments show PGD matches PETS action-likelihood at λ≈1.0 and yields significantly lower dynamics error across tested target policies (no numeric MSE values reported in the paper text).",
            "interpretability_assessment": "Black-box neural generative model (U-Net diffusion); no claims of inherent interpretability of latent factors or explicit dynamics decomposition are made in the paper.",
            "interpretability_method": "None mentioned for extracting interpretable factors; guidance is interpretable conceptually (explicitly increases action likelihood under the target policy) but no visualization/latent analysis methods are reported.",
            "computational_cost": "Model architecture: U-Net with 1024 features, 3 U-Net blocks; diffusion sampling uses EDM with 256 timesteps (found to be sufficient); training hyperparameters include batch size 16, 250 dataset epochs. No wall-clock GPU/time totals are reported in the paper.",
            "efficiency_comparison": "Compared qualitatively: avoids compounding autoregressive model error by generating whole trajectories in one shot (thus less truncation), but uses many diffusion timesteps per sample (256) so per-sample generation cost is non-negligible; empirically provides better fidelity-vs-coverage tradeoff than autoregressive PETS and unguided diffusion according to Table 1 and experiments.",
            "task_performance": "Using PGD-generated synthetic data as a drop-in replacement for real data improved downstream offline RL performance: e.g., an aggregated statistically significant 11.2% improvement for TD3+BC across MuJoCo locomotion tasks versus training on the real dataset; larger improvements reported on Maze2d tasks. Exact per-task numbers are in Table 2 of the paper.",
            "task_utility_analysis": "PGD's higher action-likelihood trajectories (closer to the target policy) translate to better downstream policy learning compared to both unguided diffusion and training on the raw offline dataset; importantly, PGD retains low dynamics error so improved action-likelihood yields usable synthetic experience.",
            "tradeoffs_observed": "Balances three axes: dynamics error (kept low by staying near data manifold via p_off term), action-likelihood (increased via guidance λ), and coverage (full-trajectory generation yields high coverage). Increasing λ trades-off more on-policy bias for potential dynamics error; guidance schedule and normalizing gradients are used to stabilize this trade-off.",
            "design_choices": "Train trajectory diffusion on offline data (direct generation); apply guidance only to action components (not states) to avoid expensive/backprop-heavy and high-variance state gradients; normalize guidance gradients and apply them to denoised actions; use a guidance coefficient λ with a cosine schedule (β=0.3 used), EDM sampling with 256 timesteps, and U-Net denoiser architecture (features=1024, 3 blocks). Behavior policy gradients are omitted (behavior policy unknown) — this omission acts as regularization.",
            "comparison_to_alternatives": "Compared against (1) autoregressive single-step world models (high compounding error, require truncated rollouts → limited coverage or biased start states), (2) unguided trajectory diffusion (low dynamics error and high coverage but low target-policy likelihood), and (3) PETS (MOPO-style autoregressive ensemble): PGD achieves similar or higher action-likelihood while producing lower dynamics error than PETS in experiments.",
            "optimal_configuration": "Paper recommends tuning the guidance coefficient λ (λ≈1.0 matched PETS likelihood in experiments), applying guidance to denoised actions only, normalizing guidance gradients, and using a cosine guidance schedule to reduce dynamics error; periodic regeneration of synthetic datasets (longer training between regenerations) yielded more stable/better results than very frequent continuous generation in their experiments. Modeling behavior policy (behavior cloning) is suggested as future work to enable explicit behavior-guidance when desired.",
            "uuid": "e1385.0",
            "source_info": {
                "paper_title": "Policy-Guided Diffusion",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Trajectory diffusion (unguided)",
            "name_full": "Unguided Trajectory Diffusion Model (trajectory-level diffusion without policy guidance)",
            "brief_description": "A diffusion generative model trained on entire trajectories from the offline dataset and sampled without any target-policy guidance, producing many diverse trajectories covering state space but remaining centered on the behavior distribution.",
            "citation_title": "Synthetic experience replay",
            "mention_or_use": "use",
            "model_name": "Unguided trajectory diffusion model",
            "model_description": "A U-Net denoiser diffusion model trained on stacked trajectories (states, actions, rewards, dones) to model the behavior trajectory distribution p_off(τ); sampling follows EDM denoising without adding any policy-gradient guidance.",
            "model_type": "trajectory-level diffusion world model (neural generative model)",
            "task_domain": "offline RL data augmentation (MuJoCo, Maze2d datasets in D4RL benchmarks)",
            "fidelity_metric": "Measured by dynamics error (MSE of states vs environment rollout) and action-likelihood under target policies (mean log-likelihood).",
            "fidelity_performance": "Low dynamics error (comparable to PGD in absence of guidance) but low action-likelihood under the target policy because samples reflect behavior distribution; specific numeric values not provided in the text but shown qualitatively in figures.",
            "interpretability_assessment": "Black-box neural model; no interpretability analyses reported.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Same U-Net and EDM sampling settings as PGD (e.g., U-Net features=1024, diffusion timesteps up to 256); no explicit compute-time numbers reported.",
            "efficiency_comparison": "Compared to PGD: similar computational sampling cost, higher coverage than autoregressive models, but lower utility for on-policy learning because generated actions have low likelihood under target policies.",
            "task_performance": "When used to train agents, unguided diffusion gives worse downstream policy performance compared to PGD and sometimes worse than training on real data, because trajectories remain off-distribution relative to improving target policies.",
            "task_utility_analysis": "High coverage and low dynamics error make unguided diffusion useful for augmenting datasets, but because it samples from p_off it does not reduce distribution shift to the target policy and thus offers limited utility for improving on-policy performance without additional guidance or policy regularization.",
            "tradeoffs_observed": "Improves coverage and avoids compounding autoregressive-model error, but sacrifices on-policy relevance (low target-policy likelihood), limiting effectiveness for learning policies that exceed behavior performance.",
            "design_choices": "Direct trajectory modeling (stacking transitions and 1D convolutions across the sequence), EDM sampling with many timesteps, U-Net denoiser architecture; no policy-gradient guidance term is used.",
            "comparison_to_alternatives": "Compared to autoregressive world models: avoids compounding transition error and truncation; compared to PGD: lower action-likelihood and downstream utility; compared to real dataset: higher coverage but off-policy nature can hurt on-policy learning.",
            "optimal_configuration": "Paper suggests unguided diffusion is a good baseline for coverage and low dynamics error, but to improve downstream task utility one should add policy guidance (PGD) or other mechanisms to increase target-policy action-likelihood.",
            "uuid": "e1385.1",
            "source_info": {
                "paper_title": "Policy-Guided Diffusion",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Autoregressive world model",
            "name_full": "Autoregressive single-step transition world model (model T)",
            "brief_description": "A learned one-step transition model trained on offline data that is used in an autoregressive fashion by rolling out the target policy into the model to generate synthetic on-policy data; susceptible to compounding model errors and therefore typically yields truncated rollouts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Autoregressive one-step transition model (T)",
            "model_description": "A model approximating T(s_{t+1} | s_t, a_t; θ) trained from dataset D_off; to generate on-policy samples the initial state s_0 is taken from D_off and the target policy is iteratively rolled out using the learned dynamics model. In practice rollouts are truncated to k ≪ H to avoid compounding error.",
            "model_type": "autoregressive explicit transition model (neural dynamics model)",
            "task_domain": "offline reinforcement learning (general continuous-control tasks in D4RL)",
            "fidelity_metric": "Dynamics error via compounding error over multistep rollouts; paper references truncation length k and qualitative bias measures from starting-state mismatch.",
            "fidelity_performance": "High compounding error for long rollouts; in practice rollout length k is small (k ≤ 5 cited) to maintain low immediate-step error but this limits coverage; no precise numeric MSEs are provided in the paper.",
            "interpretability_assessment": "Model is a learned neural dynamics approximator (black-box); no interpretability analyses discussed.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Per-step inference cost is typically small relative to full-trajectory diffusion but generating long trajectories requires repeated model queries and can be halted early to avoid compounding error; no explicit compute numbers provided.",
            "efficiency_comparison": "More sample-efficient in short truncated rollouts but suffers from bias/coverage trade-offs: unbiased if rolled from initial states but low coverage; larger coverage requires sampling from dataset states and introduces bias. Compared to PGD, autoregressive models suffer compounding error and limited practical rollout lengths.",
            "task_performance": "When used in prior model-based offline RL, autoregressive models can produce on-policy-like data for short horizons but policies can exploit model errors, leading to value overestimation and degraded performance if rollouts are too long; paper argues this motivates PGD.",
            "task_utility_analysis": "High utility for short-horizon synthetic data generation but limited utility for generating long-horizon, high-coverage synthetic experience due to compounding errors and the need to truncate rollouts.",
            "tradeoffs_observed": "Trade-off between coverage and bias: rollouts from initial states are unbiased but low coverage; rollouts from arbitrary dataset states improve coverage but bias the start-state distribution; longer rollouts increase compounding error.",
            "design_choices": "Learn single-step transition model from offline data; use short rollouts (k ≪ H), sample initial states from dataset or from episode starts; apply model ensembles or pessimism penalties in prior work (e.g., PETS/MOPO) to mitigate model exploitation.",
            "comparison_to_alternatives": "Versus trajectory diffusion (PGD/unguided): autoregressive suffers compounding error and thus must truncate, yielding lower coverage or biased starts; PETS (an ensemble autoregressive model) matches or exceeds target-policy likelihood in some settings but often has higher dynamics error than PGD on OOD policies according to the experiments.",
            "optimal_configuration": "Paper indicates that, for autoregressive models, the pragmatic optimal is to restrict rollouts to a small k (≤5) and/or apply pessimism/penalties to account for dynamics error; however, this still fails to fully address out-of-sample generalization compared to direct trajectory generation with guidance.",
            "uuid": "e1385.2",
            "source_info": {
                "paper_title": "Policy-Guided Diffusion",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "PETS",
            "name_full": "PETS (probabilistic ensemble trajectory simulator / MOPO-style PETS)",
            "brief_description": "An ensemble-based autoregressive probabilistic dynamics model (used here as a representative autoregressive baseline) that predicts single-step transitions and is used for short rollouts; in this paper PETS is used as the autoregressive baseline for fidelity comparisons.",
            "citation_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "mention_or_use": "use",
            "model_name": "PETS (ensemble probabilistic transition model)",
            "model_description": "An ensemble of probabilistic one-step dynamics models that produce next-state distributions; used in MOPO-style offline model-based methods and in this paper as a representative autoregressive baseline (PETS rollouts are compared to PGD trajectories for dynamics error and action-likelihood).",
            "model_type": "autoregressive ensemble dynamics model (neural probabilistic ensemble)",
            "task_domain": "offline reinforcement learning (used in experiments on HalfCheetah datasets for synthetic trajectory comparison)",
            "fidelity_metric": "Dynamics error measured as MSE between sampled synthetic trajectory states and true environment states over fixed rollout length (16 steps in the paper's analysis); action-likelihood under target policy also evaluated.",
            "fidelity_performance": "Empirically in this paper PETS attains comparable target-policy action likelihood to PGD when PGD uses sufficient guidance (λ≈1.0), but PETS exhibits significantly higher dynamics error than PGD across evaluated target policies, especially for out-of-distribution target policies (random and expert). Exact numeric MSE values are shown in figure data but are not reproduced as absolute numbers in the text.",
            "interpretability_assessment": "Ensemble of learned neural models; treated as black-box approximator without interpretability analyses in this paper.",
            "interpretability_method": "None mentioned.",
            "computational_cost": "Autoregressive inference requires per-step ensemble predictions; ensemble methods typically have increased compute compared to single model per-step, and repeated rollouts incur further cost; the paper does not provide explicit compute/time measurements for PETS within their experiments but uses publicly available PETS model weights for comparisons.",
            "efficiency_comparison": "Compared to PGD: PETS can match or exceed action-likelihood but at the cost of higher dynamics error and fragility to OOD target policies. Compared to truncated autoregressive rollouts, PETS still suffers compounding error for longer rollouts and higher error on OOD policies.",
            "task_performance": "PETS is used here only as a fidelity baseline (trajectory-level comparison); the paper does not report downstream RL performance when training agents on PETS-generated synthetic data.",
            "task_utility_analysis": "Although PETS can produce trajectories with high target-policy likelihood (when actions are sampled directly from the policy inside the model), the higher dynamics error reduces the utility of those trajectories for training robust policies under distribution shift; PGD yields lower dynamics error for similar action-likelihood.",
            "tradeoffs_observed": "Ensemble autoregressive models like PETS trade off per-step modeling expressiveness for cumulative compounding error over long rollouts, leading to higher MSE on OOD target policies despite good per-action likelihoods.",
            "design_choices": "Use ensembles of probabilistic next-step models (MOPO-style) to capture epistemic uncertainty and reduce model exploitation; when rollouts are truncated this reduces compounding error but limits coverage.",
            "comparison_to_alternatives": "PETS vs PGD: both can produce actions representative of the target policy, but PETS shows higher dynamics error over 16-step rollouts and worse robustness to OOD target policies. PGD avoids iterative rollouts and therefore avoids compounding errors inherent in PETS.",
            "optimal_configuration": "Paper does not give an optimal PETS configuration; it uses PETS as an established autoregressive baseline and shows that PGD achieves lower dynamics error for comparable action-likelihood.",
            "uuid": "e1385.3",
            "source_info": {
                "paper_title": "Policy-Guided Diffusion",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning in a handful of trials using probabilistic dynamics models",
            "rating": 2
        },
        {
            "paper_title": "Synthetic experience replay",
            "rating": 2
        },
        {
            "paper_title": "MOPO: Model-based Offline Policy Optimization",
            "rating": 2
        },
        {
            "paper_title": "Diffusion world models",
            "rating": 2
        },
        {
            "paper_title": "Planning with diffusion for flexible behavior synthesis",
            "rating": 2
        }
    ],
    "cost": 0.016136499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Policy-Guided Diffusion</h1>
<p>Matthew T. Jackson* Michael T. Matthews Cong Lu Benjamin Ellis<br>Shimon Whiteson ${ }^{\dagger}$ Jakob N. Foerster ${ }^{\dagger}$<br>University of Oxford</p>
<h4>Abstract</h4>
<p>In many real-world settings, agents must learn from an offline dataset gathered by some prior behavior policy. Such a setting naturally leads to distribution shift between the behavior policy and the target policy being trained-requiring policy conservatism to avoid instability and overestimation bias. Autoregressive world models offer a different solution to this by generating synthetic, on-policy experience. However, in practice, model rollouts must be severely truncated to avoid compounding error. As an alternative, we propose policy-guided diffusion. Our method uses diffusion models to generate entire trajectories under the behavior distribution, applying guidance from the target policy to move synthetic experience further on-policy. We show that policy-guided diffusion models a regularized form of the target distribution that balances action likelihood under both the target and behavior policies, leading to plausible trajectories with high target policy probability, while retaining a lower dynamics error than an offline world model baseline. Using synthetic experience from policy-guided diffusion as a drop-in substitute for real data, we demonstrate significant improvements in performance across a range of standard offline reinforcement learning algorithms and environments. Our approach provides an effective alternative to autoregressive offline world models, opening the door to the controllable generation of synthetic training data.</p>
<h2>1 Introduction</h2>
<p>A key obstacle to the real-world adoption of reinforcement learning (RL, Sutton \&amp; Barto, 2018) is its notorious sample inefficiency, preventing agents from being trained on environments with expensive or slow online data collection. A closely related challenge arises in environments where exploration, required by standard RL methods, is inherently dangerous, limiting their applicability. Yet many such settings come with an abundance of pre-collected or offline experience, gathered under one or more behavior policies (Yu et al., 2020). These settings enable the application of offline RL (Levine et al., 2020), where a policy is optimized from an offline dataset without access to the environment. However, the distribution shift between the target policy (i.e., the policy being optimized) and the collected data poses many challenges (Kumar et al., 2020; Kostrikov et al., 2021; Fujimoto et al., 2019).</p>
<p>In particular, distribution shift between the target and behavior policies leads to an out-of-sample issue: since the goal of offline RL is to exceed the performance of the behavior policy, the distribution of state-action pairs sampled by the target policy necessarily differs from that of the behavior policy, and its samples are therefore underrepresented (or unavailable) in the offline dataset. However, the maximizing nature of RL classically leads to overestimation bias when generalizing to rarely seen state-action pairs, resulting in an overly optimistic target policy. As a solution, most previous</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Offline reinforcement learning with policy-guided diffusion. Offline data from a behavior policy is first used to train a trajectory diffusion model. Synthetic experience is then generated with diffusion, guided by the target policy in order to move trajectories further on-policy. An agent is then trained for multiple steps on the synthetic dataset, before it is regenerated.
model-free work has proposed severe regularization of the target policy—such as penalizing value estimates in uncertain states (Kumar et al., 2020; An et al., 2021) or regularizing it towards the behavior policy (Fujimoto \&amp; Gu, 2021)—sacrificing target policy performance for stability.</p>
<p>In this paper, we focus on an alternative class of methods: generating synthetic experience to both augment the offline dataset and lessen the out-of-sample issue. Prior methods in this area use a model-based approach (Yu et al., 2020; Kidambi et al., 2020; Ball et al., 2021, see Section 3.1), in which a single-step world model is learned from the offline dataset, which the target policy interacts with to generate synthetic on-policy training experience. While this allows the target policy to sample synthetic trajectories under its own action distribution, compounding model error usually forces these methods to severely truncate model rollouts to a handful of interactions. Thus, there are two options which trade off coverage and bias. The first is to roll out from the initial state, which is unbiased but lacks coverage. The second is to roll out from states randomly sampled from the data set, which increases coverage but introduces bias. Neither option fully addresses the difference in observed states between the behavior and target policy when deployed, nor the out-of-sample issue mentioned above.</p>
<p>Instead, we propose policy-guided diffusion (PGD, Figure 1), which avoids compounding error by modeling entire trajectories (Section 3.2) rather than single-step transitions. To achieve this, we train a diffusion model on the offline dataset, from which we can sample synthetic trajectories under the behavior policy. However, while this addresses data sparsity, these trajectories are still off-distribution from our target policy. Therefore, inspired by classifier-guided diffusion (Dhariwal \&amp; Nichol, 2021), we apply guidance from the target policy to shift the sampling distribution towards that of the target policy. At each diffusion step, our guidance term directly increases the likelihood of sampled synthetic actions under the target policy, while the diffusion model updates the entire trajectory towards those in the dataset. This yields a regularized target distribution that we name the behavior-regularized target distribution, ensuring actions do not diverge too far from the behavior policy, limiting generalization error. As a result, PGD does not suffer from compounding error, while also generating synthetic trajectories that are more representative of the target policy. We illustrate this point in Figure 2.</p>
<p>Our approach results in consistent improvements in offline RL performance for agents trained on policy-guided synthetic data, compared to those trained on unguided synthetic or real data. We evaluate using the standard TD3+BC (Fujimoto \&amp; Gu, 2021) and IQL (Kostrikov et al., 2021) algorithms across a variety of D4RL (Fu et al., 2020) datasets. Notably, we see a statistically significant 11.2\% improvement in performance for the TD3+BC algorithm aggregated across MuJoCo (Todorov et al., 2012) locomotion tasks compared to training on the real data, with no algorithmic changes. Our results also extend to even larger improvements for the challenging Maze2d navigation environments. Furthermore, we analyze synthetic trajectories generated by PGD and show that PGD achieves lower dynamics error than PETS (Chua et al., 2018)—a prior offline model-based method—while matching the target policy likelihood of PETS. Together, our experiments illustrate the potential of PGD as an effective drop-in replacement for real data-across agents, environments, and behavior policies.</p>
<h1>2 Background</h1>
<h3>2.1 Offline Reinforcement Learning</h3>
<p>Formulation We adopt the standard reinforcement learning formulation, in which an agent acts in a Markov Decision Process (MDP, Sutton \&amp; Barto, 2018). An MDP is defined as a tuple $M=\left\langle\mathcal{S}, \mathcal{A}, p_{0}, T, R, H\right\rangle$, where $s \in \mathcal{S}$ and $a \in \mathcal{A}$ are the state and action spaces, $p_{0}\left(s_{0}\right)$ is a probability distribution over the initial state, $T\left(s_{t+1} \mid s_{t}, a_{t}\right)$ is a conditional probability distribution defining the transition dynamics, $R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is the reward function, $\gamma$ is the discount factor, and $H$ is the horizon.</p>
<p>In RL, we learn a policy $\pi(a \mid s)$ that defines a conditional probability distribution over actions for each state, inducing a distribution over trajectories $\boldsymbol{\tau}:=\left(s_{0}, a_{1}, r_{1}, s_{1}, \ldots, s_{H}\right)$ given by</p>
<p>$$
p_{\pi, M}(\boldsymbol{\tau})=p_{0}\left(s_{0}\right) \prod_{t=0}^{H-1} \pi\left(a_{t} \mid s_{t}\right) \cdot T\left(s_{t+1} \mid s_{t}, a_{t}\right)
$$</p>
<p>omitting the reward function throughout our work for conciseness. Our goal is to learn a policy that maximizes the expected return, defined as $\mathbb{E}<em M="M" _pi_="\pi,">{p</em>}}[V(\boldsymbol{\tau})]$ where $V(\boldsymbol{\tau}) \coloneqq \sum_{t=0}^{H} r_{t}$ is the return of a trajectory. The offline RL setting (Levine et al., 2020) extends this, preventing the agent from interacting with the environment and instead presenting it with a dataset of trajectories $\boldsymbol{\tau} \in \mathcal{D<em _off="{off" _text="\text">{\text {off }}$ gathered by some unknown behavior policy $\pi</em>$.}}$, with which to optimize a target policy $\pi_{\text {target }</p>
<p>Out-of-Sample Generalization The core challenge of offline RL emerges from the distribution shift between the behavior distribution $p_{\pi_{\text {off }}, M}(\boldsymbol{\tau})$ and the target distribution $p_{\pi_{\text {target }}, M}(\boldsymbol{\tau})$, which are otherwise denoted $p_{\text {off }}(\boldsymbol{\tau})$ and $p_{\text {target }}(\boldsymbol{\tau})$ for conciseness. Optimization of $\pi_{\text {target }}$ on $\mathcal{D}_{\text {off }}$ can lead to catastrophic value overestimation at unobserved actions, a problem termed the out-of-sample issue (Kostrikov et al., 2021). As such, model-free offline algorithms typically regularize the policy towards the behavior distribution, either explicitly (Fujimoto \&amp; Gu, 2021; Kumar et al., 2020) or implicitly (Kostrikov et al., 2021).</p>
<p>Alternatively, prior work proposes learning a single-step world model $\mathcal{M}$ from $\mathcal{D}<em _target="{target" _text="\text">{\text {off }}$ (Yu et al., 2020; Kidambi et al., 2020; Lu et al., 2022). By rolling out the target policy using $\mathcal{M}$, we generate trajectories $\boldsymbol{\tau} \sim p</em>)$, with the aim of avoiding distribution shift. However, in practice, this technique only pushes the generalization issue into the world model. In particular, RL policies are prone to exploiting errors in the world model, which can compound over the course of an episode. When combined with typical maximizing operations used in off-policy RL, this results in value overestimation bias (Sims et al., 2024).}}(\boldsymbol{\tau</p>
<h3>2.2 Diffusion Models</h3>
<p>Definition To generate synthetic data, we consider diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), a class of generative model that allows one to sample from a distribution $p(x)$ by iteratively reversing a forward noising process. Karras et al. (2022) present an ODE formulation of diffusion which, given a noise schedule $\sigma(i)$ indexed by $i$, mutates data according to</p>
<p>$$
d \boldsymbol{x}=-\dot{\sigma}(i) \sigma(i) \nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(i)) d i
$$</p>
<p>where $\dot{\sigma}=\frac{\mathrm{d} \sigma}{\mathrm{d} i}$ and $\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(i))$ is the score function (Hyvärinen \&amp; Dayan, 2005), which points towards areas of high data density. Intuitively, infinitesimal forward or backward steps of this ODE respectively nudge a sample away from or towards the data. To generate a sample, we start with pure noise at the highest noise level $\sigma_{\max }$, and iteratively denoise in discrete timesteps under Equation 2.</p>
<p>Classifier Guidance Our method is designed to augment the data-generating process towards on-policy trajectories from the target distribution $p_{\text {target }}(\boldsymbol{\tau})$, rather than the behavior distribution $p_{\text {off }}(\boldsymbol{\tau})$. To achieve this, we take inspiration from classifier guidance (Dhariwal \&amp; Nichol, 2021),</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Trajectories from an illustrative 2D environment, in which the start location is indicated by $\bullet$ and the goals for the behavior and target policies are indicated by $\times$ and $\times$. Left: Rollouts from the target policy in the real environment. Right: Offline datasets gathered by the behavior policy suffer from distribution shift and limited sample size. Truncated world models (Yu et al., 2020; Kidambi et al., 2020) previously used in offline model-based reinforcement learning offer a partial solution to this problem but suffer from bias due to short rollouts. Meanwhile, unguided diffusion (Lu et al., 2023) can increase the sample size, but maintains the original distribution shift. In contrast, policy-guided diffusion samples from a regularized target distribution, generating entire trajectories with low transition error but higher likelihood under the target distribution.
which leverages a differentiable classifier to augment the score function of a pre-trained diffusion model towards a class-conditional distribution $p(\boldsymbol{x} \mid y)$. Concretely, this adds a classifier gradient to the score function, giving</p>
<p>$$
\nabla_{\boldsymbol{x}} \log p_{\lambda}(\boldsymbol{x} \mid y ; \sigma(i))=\nabla_{\boldsymbol{x}} \log p(\boldsymbol{x} ; \sigma(i))+\lambda \nabla_{\boldsymbol{x}} \log p_{\theta}(y \mid \boldsymbol{x} ; \sigma(i))
$$</p>
<p>where $\nabla_{\boldsymbol{x}} \log p_{\theta}(y \mid \boldsymbol{x} ; \sigma(i))$ is the gradient of the classifier and $\lambda$ is the guidance weight.</p>
<h1>3 On-Policy Sampling from Offline Data</h1>
<p>Generating synthetic agent experience is a promising approach to solving out-of-sample generalization in offline RL. By generating experience that is unseen in the dataset, the policy may be directly optimized on OOD samples, thereby moving the generalization problem from the policy to the generative model. Some prior work has suggested learning a model from the offline dataset (Lu et al., 2023), thereby sampling synthetic experience from the behavior distribution. While this improves sample coverage, the approach retains many of the original challenges of offline RL. As with the behavior policy, the synthetic trajectories may be suboptimal, meaning we still require conservative off-policy RL techniques to train the agent.</p>
<p>Instead, we seek to extend this approach by making our generative model sample from the target distribution. This reduces the need for conservatism and generates synthetic trajectories with increasing performance as the agent improves over training. Practically, the effectiveness of this approach depends on how we parameterize each of the terms of the trajectory distribution (Equation 1). In this section, we consider two parameterizations: autoregressive and direct.</p>
<h3>3.1 Autoregressive Generation — Model $\mathcal{T}$, Sample $p\left(s_{0}\right)$</h3>
<p>The autoregressive-or model-based—approach to generating on-policy data is to use the offline dataset to train a one-step transition model $\mathcal{T}\left(s_{t+1} \mid s_{t}, a_{t} ; \theta\right)$. To generate unbiased sample trajectories from the target distribution, we first sample an initial state (i.e., one that starts an episode) from the offline dataset $s_{0} \sim \mathcal{D}_{\text {off }}$. Next, we roll out our agent in the learned model by iteratively sampling actions from the target policy and approximating environment transitions with the learned dynamics model. However, compounding error from the transition model usually requires agent rollouts to be much shorter than the environment horizon-such that the agent takes $k \ll H$ steps. ${ }^{1}$ Consequently, any states more than $k$ steps away from any initial state cannot be generated in this manner, limiting the applicability of this approach.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>As an approximation, autoregressive methods typically sample initial states from any timestep $s_{t} \sim \mathcal{D}_{\text {off }}$ in the offline dataset. Given a truncated rollout length $k$, this may be seen as approximating the sub-trajectory distribution—i.e., the trajectory from time $t$ to $t+k$ —given by</p>
<p>$$
p_{\text {target }}\left(\boldsymbol{\tau}<em _target="{target" _text="\text">{t: t+k} ; \theta\right)=p</em>}}\left(s_{t}\right) \cdot p_{\text {target }}\left(\boldsymbol{\tau<em t="t">{t: t+k} \mid s</em> ; \theta\right)
$$</p>
<p>by instead modeling</p>
<p>$$
\mathcal{F}\left(\boldsymbol{\tau}<em _off="{off" _text="\text">{t: t+k} ; \theta\right)=p</em>}}\left(s_{t}\right) \cdot p_{\text {target }}\left(\boldsymbol{\tau<em t="t">{t: t+k} \mid s</em> ; \theta\right)
$$</p>
<p>Here, we denote the stationary state distributions of the target and behavior policies at time $t$ by $p_{\text {target }}\left(s_{t}\right)$ and $p_{\text {off }}\left(s_{t}\right)$ respectively, and define the conditional sub-trajectory distribution as</p>
<p>$$
p_{\text {target }}\left(\boldsymbol{\tau}<em t="t">{t: t+k} \mid s</em> ; \theta\right)
$$} ; \theta\right):=\prod_{j=0}^{k-1} \pi_{\text {target }}\left(a_{t+j} \mid s_{t+j}\right) \cdot \mathcal{T}\left(s_{t+j+1} \mid s_{t+j}, a_{t+j</p>
<p>When generating trajectories from this distribution, the difference between $p_{\text {target }}\left(s_{t}\right)$ and $p_{\text {off }}\left(s_{t}\right)$ biases the start of rollouts towards states visited by the behavior policy. Furthermore, we still require $k$ to be small to avoid compounding error. In combination, sampling from the offline dataset "anchors" synthetic rollouts to states in the offline dataset, while truncated rollouts prevent synthetic trajectories from moving far from this anchor. Therefore, the practical application of autoregressive generation leads to a strong bias towards the behavior distribution and fails to address the out-of-sample problem.</p>
<h1>3.2 Direct Generation — Model $p_{\text {off }}(\tau)$</h1>
<p>As an alternative to autoregressive generation, we can parameterize the target distribution by directly modeling the behavior distribution, as follows:</p>
<p>$$
\begin{aligned}
p_{\text {target }}(\boldsymbol{\tau}) &amp; =p\left(s_{0}\right) \prod_{t=0}^{H-1} \pi_{\text {target }}\left(a_{t} \mid s_{t}\right) \cdot \mathcal{T}\left(s_{t+1} \mid s_{t}, a_{t}\right) \
&amp; =p\left(s_{0}\right) \prod_{t=0}^{H-1} \frac{\pi_{\text {target }}\left(a_{t} \mid s_{t}\right)}{\pi_{\text {off }}\left(a_{t} \mid s_{t}\right)} \cdot \pi_{\text {off }}\left(a_{t} \mid s_{t}\right) \cdot \mathcal{T}\left(s_{t+1} \mid s_{t}, a_{t}\right) \
&amp; =p_{\text {off }}(\boldsymbol{\tau}) \prod_{t=0}^{H-1} w_{a_{t}, s_{t}} \
&amp; \approx p_{\text {off }}(\boldsymbol{\tau} ; \theta) \prod_{t=0}^{H-1} w_{a_{t}, s_{t}}=p_{\text {target }}(\boldsymbol{\tau} ; \theta)
\end{aligned}
$$</p>
<p>where $w_{a, s}:=\frac{\pi_{\text {target }}\left(a \mid s\right)}{\pi_{\text {off }}\left(a \mid s\right)}$ denotes the importance sampling weight for $(a, s)$ (Precup, 2000). This directly parameterizes the behavior distribution $p_{\text {off }}(\boldsymbol{\tau} ; \theta)$ —which may be learned by modeling entire trajectories on the offline dataset-and adjusts their likelihoods by the relative probabilities of actions $w_{a_{t}, s_{t}}$ under the target and behavior policies. By jointly modeling the initial state distribution, transition function, and behavior policy, such a parameterization is not required to enforce the Markov property. As a result, it can directly generate entire trajectories, thereby avoiding the compounding model error suffered by autoregressive methods when iteratively generating transitions.</p>
<p>However, computing $w_{a_{t}, s_{t}}$ requires access to the behavior policy $\pi_{\text {off }}\left(a \mid s\right)$, which is not assumed in offline RL. Prior work has explored modeling the behavior policy from the offline dataset and using this to compute importance sampling corrections. However, products of many importance weights can lead to problems with high variance (Precup et al., 2000; Levine et al., 2020).</p>
<h1>4 Policy-Guided Diffusion</h1>
<p>In this work, we propose a method following the direct generation approach outlined in Section 3.2, named policy-guided diffusion (PGD, Algorithm 1). Following the success of diffusion models at generating trajectories (Janner et al., 2022; Lu et al., 2023), we first train a trajectory-level diffusion model on the offline dataset to model the behavior distribution. Then, inspired by classifier-guided diffusion (Section 2.2), we guide the diffusion process using the target policy to move closer to the target distribution. Specifically, during the denoising process, we compute the gradient of the action distribution for each action under the target policy, using it to augment the diffusion process towards high-probability actions. In doing so, we approximate a regularized target distribution that equally weights action likelihoods under the behavior and target policies.</p>
<p>In this section, we derive PGD as an approximation of the behavior-regularized target distribution (Section 4.1), then describe practical details for controlling and stabilizing policy guidance (Section 4.2). We provide a summary of PGD against alternative sources of training data in Table 1.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">Trajectory</span><span class="w"> </span><span class="nt">sampling</span><span class="w"> </span><span class="nt">via</span><span class="w"> </span><span class="nt">policy-guided</span><span class="w"> </span><span class="nt">diffusion</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="nt">based</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">Karras</span><span class="w"> </span><span class="nt">et</span><span class="w"> </span><span class="nt">al</span><span class="o">.</span><span class="w"> </span><span class="o">(</span><span class="nt">2022</span><span class="o">).</span>
<span class="w">    </span><span class="nt">Parameters</span><span class="o">:</span><span class="w"> </span><span class="nt">Noise</span><span class="w"> </span><span class="nt">schedule</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">guidance</span><span class="w"> </span><span class="nt">schedule</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">lambda_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">noise</span><span class="w"> </span><span class="nt">factor</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">gamma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">noise</span><span class="w"> </span><span class="nt">level</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{noise</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">).</span>
<span class="w">        </span><span class="nt">diffusion</span><span class="w"> </span><span class="nt">steps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{diffusion</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Required</span><span class="o">:</span><span class="w"> </span><span class="nt">Denoiser</span><span class="w"> </span><span class="nt">model</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">D_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">target</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">I</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">random</span><span class="w"> </span><span class="nt">noise</span><span class="w"> </span><span class="nt">trajectory</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">n</span><span class="o">=</span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{diffusion</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="nt">-1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">sample</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">epsilon_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">0</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">S_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{noise</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">I</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Temporarily</span><span class="w"> </span><span class="nt">increase</span><span class="w"> </span><span class="nt">noise</span><span class="w"> </span><span class="nt">level</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">gamma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">sqrt</span><span class="p">{</span><span class="err">\hat{\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\epsilon</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Estimate</span><span class="w"> </span><span class="nt">denoised</span><span class="w"> </span><span class="nt">trajectory</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Evaluate</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">\partial</span><span class="w"> </span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="p">{</span><span class="err">\partial</span><span class="w"> </span><span class="err">\sigma</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">at</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">nabla_</span><span class="p">{</span><span class="err">\overline{\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{noise</span><span class="w"> </span><span class="p">}</span><span class="err">}}</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{actions</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{states</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Compute</span><span class="w"> </span><span class="nt">denoised</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="nt">gradient</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{actions</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">overline</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{actions</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">+</span><span class="err">\</span><span class="nt">lambda_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="nt">left</span><span class="err">\</span><span class="o">|</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">g</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="err">\</span><span class="o">|</span><span class="nt">_</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Apply</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="nt">guidance</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="nt">noised</span><span class="w"> </span><span class="nt">actions</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Apply</span><span class="w"> </span><span class="nt">Euler</span><span class="w"> </span><span class="nt">step</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">neq</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="nt">-D_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Apply</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">2</span><span class="o">^</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{nd</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">order</span><span class="w"> </span><span class="nt">correction</span>
<span class="w">                </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">sigma_</span><span class="p">{</span><span class="err">n+1</span><span class="p">}</span><span class="nt">-</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\sigma</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">1</span><span class="p">}{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">+</span><span class="err">\</span><span class="nt">frac</span><span class="p">{</span><span class="err">1</span><span class="p">}{</span><span class="err">2</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">d</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">n</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">\prime</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="err">\</span><span class="nt">triangleright</span><span class="err">\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">end</span><span class="w"> </span><span class="nt">if</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<h3>4.1 Behavior-Regularized Target Distribution</h3>
<p>Policy Guidance Derivation To sample a trajectory via diffusion, we require a noise-conditioned score function $\nabla_{\hat{\boldsymbol{\tau}}} \log p(\hat{\boldsymbol{\tau}} ; \sigma)$ for a noised trajectory $\hat{\boldsymbol{\tau}}:=\left(\hat{s}<em 1="1">{0}, \hat{a}</em>}, \hat{r<em 1="1">{1}, \hat{s}</em>}, \ldots, \hat{s<em _off="{off" _text="\text">{H}\right)$ under a distribution $p(\boldsymbol{\tau})$ at a noise level $\sigma$. Given an offline dataset $\mathcal{D}</em>}}$, it is straightforward to learn this function under the behavior distribution, $\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {off }}(\hat{\boldsymbol{\tau}} ; \sigma)$, by training a denoiser model to reconstruct noised trajectories from $\mathcal{D<em _hat_boldsymbol_tau="\hat{\boldsymbol{\tau">{\text {off }}$. However, there is no apparent method to directly model the noise-conditioned score function $\nabla</em> ; \sigma)$ for the target distribution (see Appendix B for further discussion), meaning we require an approximation.}}} \log p_{\text {target }}(\hat{\boldsymbol{\tau}</p>
<p>To achieve this, we consider the score function of a noise-free trajectory $\boldsymbol{\tau}$ under the target distribution, based on the formulation from Equation 7,</p>
<p>$$
\nabla_{\boldsymbol{\tau}} \log p_{\text {target }}(\boldsymbol{\tau})=\nabla_{\boldsymbol{\tau}} \log p_{\text {off }}(\boldsymbol{\tau})+\sum_{t=0}^{H-1}\left(\nabla_{\boldsymbol{\tau}} \log \pi_{\text {target }}\left(a_{t} \mid s_{t}\right)-\nabla_{\boldsymbol{\tau}} \log \pi_{\text {off }}\left(a_{t} \mid s_{t}\right)\right)
$$</p>
<p>In the limit of noise $\sigma \rightarrow 0$, the noise-conditioned score function $\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {target }}(\hat{\boldsymbol{\tau}} ; \sigma)$ clearly approaches $\nabla_{\boldsymbol{\tau}} \log p_{\text {target }}(\boldsymbol{\tau})$. Therefore, we may approximate this function by</p>
<p>$$
\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {target }}(\hat{\boldsymbol{\tau}} ; \sigma) \approx \nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {off }}(\hat{\boldsymbol{\tau}} ; \sigma)+\sum_{t=0}^{H-1}\left(\nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {target }}\left(\hat{a}<em t="t">{t} \mid \hat{s}</em>}\right)-\nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {off }}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>\right)\right)
$$</p>
<p>for $\sigma \approx 0$. Whilst iteratively denoising under this function (Section 2.2) does not model $p_{\text {target }}(\boldsymbol{\tau})$ exactly, the score function approaches $\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {target }}(\hat{\boldsymbol{\tau}} ; \sigma)$ towards the end of the denoising process, which we believe provides an effective approximation.</p>
<p>Excluding Behavior Policy Guidance As discussed, we may directly model the first term of Equation 9 by training a denoiser model. Furthermore, we may directly compute target policy guidance $\nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {target }}\left(\hat{a}<em t="t">{t} \mid \hat{s}</em>}\right)$ —the second term of this approximation-as we assume access to a (differentiable) target policy in the offline RL setting. However, we generally do not have access to the behavior policy, preventing us from computing $\nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {off }}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>}\right)$. Due to this, we exclude behavior policy guidance from our approximation, resulting in the score function $\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {off }}(\hat{\boldsymbol{\tau}} ; \sigma)+$ $\sum_{t=0}^{H-1} \nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {target }}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>\right)$. As $\sigma \rightarrow 0$, this approaches the score function for a proxy distribution of the form</p>
<p>$$
\begin{aligned}
\mathcal{F}\left(\boldsymbol{\tau} ; \pi_{\text {target }}\right) &amp; \propto p_{\text {off }}(\boldsymbol{\tau}) \prod_{t=0}^{H-1} \pi_{\text {target }}\left(a_{t} \mid s_{t}\right) \
&amp; =p_{\text {off }}(\boldsymbol{\tau}) \cdot q_{\text {target }}(\boldsymbol{\tau})=p_{\text {target }}(\boldsymbol{\tau}) \cdot q_{\text {off }}(\boldsymbol{\tau})
\end{aligned}
$$</p>
<p>where $q_{\text {target }}(\boldsymbol{\tau}):=\prod_{t=0}^{H-1} \pi_{\text {target }}\left(a_{t} \mid s_{t}\right)$ denotes the product of action probabilities under the target policy and $q_{\text {off }}(\boldsymbol{\tau})$ denotes the same quantity under the behavior policy. Therefore, we hypothesize that excluding behavior policy guidance is an effective form of regularization, as it biases trajectories towards the support of the offline data, thereby limiting model error and the out-of-sample problem. We refer to $\mathcal{F}\left(\boldsymbol{\tau} ; \pi_{\text {target }}\right)$ as the behavior-regularized target distribution due to it balancing action likelihoods under the behavior and target policies, and provide further discussion in Appendix C. Finally, as a promising avenue for future work, we note that the behavior policy may be modeled by applying behavior cloning to $\mathcal{D}_{\text {off }}$, allowing for the inclusion of behavior policy guidance in the offline RL setting.</p>
<p>Excluding State Guidance Target policy guidance $\nabla_{\hat{\boldsymbol{\tau}}} \log \pi_{\text {target }}\left(\hat{a}<em t="t">{t} \mid \hat{s}</em>}\right)$ has non-zero gradients for the state and action at timestep $t$. In practice, the action component $\nabla_{\hat{a<em _target="{target" _text="\text">{t}} \log \pi</em>}}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>}\right)$ typically has an efficient, closed-form solution, with $\pi_{\text {target }}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>}\right)$ commonly being Gaussian for continuous action spaces. In contrast, for neural network policies, the state component $\nabla_{\hat{s<em _target="{target" _text="\text">{t}} \log \pi</em>}}\left(\hat{a<em t="t">{t} \mid \hat{s}</em>\right)$ requires backpropagating gradients through the policy network, which is both expensive to compute and can lead to high variance on noisy, out-of-distribution states. Due to this, we apply policy guidance to only the noised action, yielding our policy-guided score function</p>
<p>$$
s_{\mathrm{PGD}}(\hat{\boldsymbol{\tau}} ; \sigma)=\underbrace{\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {off }}(\hat{\boldsymbol{\tau}} ; \sigma)}<em _hat_boldsymbol_a="\hat{\boldsymbol{a">{\text {Behavior score function }}+\underbrace{\nabla</em>
$$}}} \log q_{\text {target }}(\hat{\boldsymbol{\tau}})}_{\text {Target policy guidance }</p>
<p>where (abusing notation) $\nabla_{\hat{\boldsymbol{a}}}$ denotes the gradient $\nabla_{\hat{\boldsymbol{\tau}}}$ of $\hat{\boldsymbol{\tau}}$, with non-action components set to 0 .</p>
<h1>4.2 Improving Policy Guidance</h1>
<p>Controlling Guidance Strength A standard technique from classifier-guided diffusion is the use of guidance coefficients (Dhariwal \&amp; Nichol, 2021). These augment the guided score function by introducing a controllable coefficient on the guidance term. Applied to the PGD score function (Equation 11), this has the form</p>
<p>$$
s_{\mathrm{PGD}}(\hat{\boldsymbol{\tau}} ; \sigma, \lambda)=\nabla_{\hat{\boldsymbol{\tau}}} \log p_{\text {off }}(\hat{\boldsymbol{\tau}} ; \sigma)+\lambda \nabla_{\hat{\boldsymbol{a}}} \log q_{\text {target }}(\hat{\boldsymbol{\tau}})
$$</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Left: Trajectory probability distribution for an example behavior distribution $p_{\text {off }}(\boldsymbol{\tau})$ and target policy likelihood $q_{\text {target }}(\boldsymbol{\tau})$. Right: Corresponding PGD sampling distribution (Equation 13) computed over a range of policy-guidance coefficients $\lambda$. By increasing $\lambda$, we transform from the sampling distribution towards the regions of high target policy likelihood, making PGD an effective mechanism for controlling the level of regularization towards the behavior distribution.
where $\lambda$ denotes the guidance coefficient. As $\sigma \rightarrow 0$, this transforms the sampling distribution to</p>
<p>$$
\mathcal{F}\left(\boldsymbol{\tau} \mid \pi_{\text {target }} ; \lambda\right) \propto p_{\text {off }}(\boldsymbol{\tau}) \cdot q_{\text {target }}(\boldsymbol{\tau})^{\lambda}
$$</p>
<p>Intuitively, $\lambda$ interpolates the actions in the sampling distribution between the behavior and target distributions. By tuning $\lambda$, we can therefore control the strength of guidance towards the target policy, avoiding high dynamics error when the target policy is far from the behavior policy. We visualize this effect in Figure 3 and analyze its impact on target policy likelihood in Figure 5.</p>
<p>Following Ma et al. (2023), we also apply a cosine guidance schedule to the guidance coefficient,</p>
<p>$$
\lambda_{n}=\lambda \cdot\left(\sigma_{n}+\beta \sigma_{N} \cdot \sin (\pi \cdot n / N)\right)
$$</p>
<p>where $\beta$ is the cosine weight, which is set to 0.3 in all experiments. By decreasing the strength of guidance in later steps, we find that this schedule stabilizes guidance and reduces dynamics error.</p>
<p>Stabilizing Guided Diffusion When under distribution shift, RL policies are known to suffer from poor generalization to unseen states (Kirk et al., 2023). This makes policy guidance challenging, since the policy must operate on noised states, and compute action gradients from noised actions. Similar issues have been studied in classifier-guided diffusion (Ma et al., 2023), where the classifier gradient can be unstable when exposed to out-of-distribution inputs. Bansal et al. (2023) alleviate this issue by applying guidance to the denoised sample estimated by the denoiser model, rather than the original noised sample, in addition to normalizing the guidance gradient to a unit vector. By applying these techniques to policy guidance, we lessen the need for the target policy to generalize to noisy states, which we find decreases dynamics error.</p>
<p>Table 1: Overview of training experience sources in offline RL-for each, we consider the sampling distribution, expected error in transition dynamics, likelihood of actions under the target policy, and state space coverage beyond the behavior distribution. Policy-guided diffusion provides an effective trade-off between each error, likelihood, and coverage.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Data source</th>
<th style="text-align: left;">Distribution</th>
<th style="text-align: center;">Error $(\downarrow)$</th>
<th style="text-align: center;">Likelihood $(\uparrow)$</th>
<th style="text-align: center;">Coverage $(\uparrow)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Offline dataset</td>
<td style="text-align: left;">$p_{\text {off }}(\boldsymbol{\tau})$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Low</td>
</tr>
<tr>
<td style="text-align: left;">Episodic world model</td>
<td style="text-align: left;">$p_{\text {target }}(\boldsymbol{\tau})$</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">High</td>
</tr>
<tr>
<td style="text-align: left;">Truncated world model</td>
<td style="text-align: left;">Equation 5</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Low</td>
</tr>
<tr>
<td style="text-align: left;">Unguided diffusion</td>
<td style="text-align: left;">$p_{\text {off }}(\boldsymbol{\tau})$</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">High</td>
</tr>
<tr>
<td style="text-align: left;">Policy-guided diffusion</td>
<td style="text-align: left;">Equation 10</td>
<td style="text-align: center;">Low</td>
<td style="text-align: center;">High</td>
<td style="text-align: center;">High</td>
</tr>
</tbody>
</table>
<h1>5 Results</h1>
<p>Through our experiments, we first demonstrate that agents trained with synthetic experience from PGD outperform those trained on unguided synthetic data or directly on the offline dataset (Section 5.2). We show that this effect is consistent across agents (TD3+BC and IQL), environments (HalfCheetah, Walker2d, Hopper, and Maze), behavior policies (random, mixed, and medium), and modes of data generation (continuous and periodic). Following this, we demonstrate that tuning the guidance coefficient enables PGD to sample trajectories with high action likelihood across a range of target policies. Finally, we verify that PGD retains low dynamics error despite sampling high-likelihood actions from the policy (Section 5.3).</p>
<h3>5.1 Experimental Setup</h3>
<p>We evaluate PGD on the MuJoCo and Maze2d continuous control datasets from D4RL (Fu et al., 2020; Todorov et al., 2012). For MuJoCo, we consider the HalfCheetah, Walker2d, and Hopper environments with random (randomly initialized behavior policy), medium (suboptimal behavior policy), and medium-replay (or "mixed", the replay buffer from medium policy training) datasets. For Maze2d we consider the original (sparse reward) instances of the umaze, medium and large layouts. We train 4 trajectory diffusion models on each dataset, for which we detail hyperparameters in Appendix A. In Section 5.3, we conduct analysis of PGD against MOPO-style PETS (Chua et al., 2018) models, an autoregressive world model composed of an ensemble of probabilistic models, for which we use model weights from OfflineRL-Kit (Sun, 2023).</p>
<p>To demonstrate synthetic experience from PGD as a drop-in substitute for the real dataset, we transfer the original hyperparameters for IQL (Kostrikov et al., 2021) and TD3+BC (Fujimoto \&amp; Gu, 2021)— as tuned on the real datasets-without any further tuning. Policy guidance requires a stochastic target policy, in order to compute the gradient of the action distribution. Since TD3+BC trains a deterministic policy, we perform guidance by modeling the action distribution as a unit Gaussian centered on the deterministic action. We implement all agents and diffusion models from scratch in Jax (Bradbury et al., 2018), which may be found at https://github.com/EmptyJackson/policy-guided-diffusion.</p>
<h3>5.2 Offline Reinforcement Learning</h3>
<p>For each D4RL dataset, we train two popular model-free offline algorithms, TD3+BC (Fujimoto \&amp; Gu, 2021) and IQL (Kostrikov et al., 2021) on synthetic experience generated by trajectory diffusion models with and without policy guidance, as well as on the real dataset. We first consider periodic generation of synthetic data, in which the synthetic dataset is regenerated after extended periods of agent training, such that the agent is near convergence on the synthetic dataset at the point it is regenerated with the current policy. Each epoch, we generate a dataset of $2^{14}$ synthetic trajectories of length 16. Following the notation of Algorithm 2, we set the number of epochs to $N_{\text {epochs }}=4$ with $N_{\text {policy }}=250,000$ train steps per epoch, meaning the agent is trained to close to convergence before the dataset is regenerated. This can be viewed as solving a sequence of offline RL tasks with synthetic datasets, in which the behavior policy is the target policy from the previous generation.</p>
<p>Using periodic generation, performance improves significantly across benchmarks for both IQL and TD3+BC (Table 2). In MuJoCo, the most consistent improvement is on mixed datasets, with 4 out of 6 experiments achieving significant performance improvement. This is to be expected, as these datasets contain experience from a mixture of behavior policy levels. In this case, the diffusion model is likely to be able to represent a wide variety of policies, and on-policy guidance would naturally produce higher return trajectories as the target policy improves.</p>
<p>In order to demonstrate the flexibility of PGD, we also evaluate PGD in a continuous generation setting, using a data generation rate closer to that of traditional model-based methods. For this, we set $N_{\text {epochs }}=100$ and $N_{\text {policy }}=10,000$, then lower the sample size to match the overall number of synthetic trajectories generated by periodic generation across training. Due to the decrease in sample</p>
<p>Table 2: Final return of IQL and TD3+BC agents trained on real, unguided $(\lambda=0)$ synthetic and policy-guided $(\lambda=1)$ synthetic data-mean and standard error over 4 seeds (diffusion models and agents) is presented, with significant improvements $(p&lt;0.05)$ shaded.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">IQL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TD3+BC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Unguided</td>
<td style="text-align: center;">Guided</td>
<td style="text-align: center;">Dataset</td>
<td style="text-align: center;">Unguided</td>
<td style="text-align: center;">Guided</td>
</tr>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">HalfCheetah</td>
<td style="text-align: center;">$9.1 \pm 2.2$</td>
<td style="text-align: center;">$2.6 \pm 0.1$</td>
<td style="text-align: center;">$6.5 \pm 1.7$</td>
<td style="text-align: center;">$11.2 \pm 0.8$</td>
<td style="text-align: center;">$11.0 \pm 0.4$</td>
<td style="text-align: center;">$21.1 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walker2d</td>
<td style="text-align: center;">$4.3 \pm 0.5$</td>
<td style="text-align: center;">$2.7 \pm 0.7$</td>
<td style="text-align: center;">$5.3 \pm 0.3$</td>
<td style="text-align: center;">$0.5 \pm 0.3$</td>
<td style="text-align: center;">$1.1 \pm 1.2$</td>
<td style="text-align: center;">$-0.3 \pm 0.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">$7.4 \pm 0.4$</td>
<td style="text-align: center;">$5.2 \pm 0.9$</td>
<td style="text-align: center;">$4.9 \pm 1.0$</td>
<td style="text-align: center;">$7.4 \pm 0.6$</td>
<td style="text-align: center;">$4.2 \pm 1.4$</td>
<td style="text-align: center;">$5.5 \pm 2.1$</td>
</tr>
<tr>
<td style="text-align: center;">Mixed</td>
<td style="text-align: center;">HalfCheetah</td>
<td style="text-align: center;">$44.2 \pm 0.2$</td>
<td style="text-align: center;">$43.6 \pm 0.2$</td>
<td style="text-align: center;">$43.6 \pm 0.2$</td>
<td style="text-align: center;">$44.7 \pm 0.1$</td>
<td style="text-align: center;">$43.1 \pm 0.2$</td>
<td style="text-align: center;">$46.1 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walker2d</td>
<td style="text-align: center;">$81.3 \pm 2.0$</td>
<td style="text-align: center;">$85.2 \pm 0.3$</td>
<td style="text-align: center;">$84.9 \pm 1.4$</td>
<td style="text-align: center;">$82.7 \pm 1.3$</td>
<td style="text-align: center;">$70.7 \pm 10.1$</td>
<td style="text-align: center;">$84.0 \pm 1.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">$82.9 \pm 3.5$</td>
<td style="text-align: center;">$97.4 \pm 2.7$</td>
<td style="text-align: center;">$100.5 \pm 0.5$</td>
<td style="text-align: center;">$58.6 \pm 11.2$</td>
<td style="text-align: center;">$52.1 \pm 1.8$</td>
<td style="text-align: center;">$91.9 \pm 4.3$</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">HalfCheetah</td>
<td style="text-align: center;">$48.4 \pm 0.1$</td>
<td style="text-align: center;">$45.4 \pm 0.1$</td>
<td style="text-align: center;">$45.1 \pm 0.1$</td>
<td style="text-align: center;">$48.6 \pm 0.1$</td>
<td style="text-align: center;">$45.3 \pm 0.2$</td>
<td style="text-align: center;">$47.6 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Walker2d</td>
<td style="text-align: center;">$81.7 \pm 1.4$</td>
<td style="text-align: center;">$82.1 \pm 0.9$</td>
<td style="text-align: center;">$77.8 \pm 3.6$</td>
<td style="text-align: center;">$84.8 \pm 0.1$</td>
<td style="text-align: center;">$85.2 \pm 0.2$</td>
<td style="text-align: center;">$86.3 \pm 0.3$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hopper</td>
<td style="text-align: center;">$63.6 \pm 0.8$</td>
<td style="text-align: center;">$59.7 \pm 2.0$</td>
<td style="text-align: center;">$62.8 \pm 1.2$</td>
<td style="text-align: center;">$62.4 \pm 0.9$</td>
<td style="text-align: center;">$57.4 \pm 0.4$</td>
<td style="text-align: center;">$63.1 \pm 0.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">$46.9 \pm 0.4$</td>
<td style="text-align: center;">$47.0 \pm 0.4$</td>
<td style="text-align: center;">$47.9 \pm 0.3$</td>
<td style="text-align: center;">$44.5 \pm 1.1$</td>
<td style="text-align: center;">$41.1 \pm 1.1$</td>
<td style="text-align: center;">$49.5 \pm 0.9$</td>
</tr>
<tr>
<td style="text-align: center;">$\underset{\text { M }}{ }$</td>
<td style="text-align: center;">UMaze</td>
<td style="text-align: center;">$42.6 \pm 0.4$</td>
<td style="text-align: center;">$42.9 \pm 1.8$</td>
<td style="text-align: center;">$43.8 \pm 3.5$</td>
<td style="text-align: center;">$50.0 \pm 2.4$</td>
<td style="text-align: center;">$33.8 \pm 3.0$</td>
<td style="text-align: center;">$76.2 \pm 17.4$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">$38.5 \pm 1.9$</td>
<td style="text-align: center;">$33.4 \pm 3.2$</td>
<td style="text-align: center;">$60.0 \pm 13.9$</td>
<td style="text-align: center;">$32.1 \pm 6.8$</td>
<td style="text-align: center;">$24.0 \pm 4.0$</td>
<td style="text-align: center;">$89.6 \pm 19.9$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Large</td>
<td style="text-align: center;">$50.9 \pm 5.8$</td>
<td style="text-align: center;">$23.4 \pm 8.0$</td>
<td style="text-align: center;">$45.3 \pm 14.8$</td>
<td style="text-align: center;">$137.2 \pm 20.2$</td>
<td style="text-align: center;">$93.3 \pm 31.0$</td>
<td style="text-align: center;">$131.1 \pm 37.5$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">$44.0 \pm 2.2$</td>
<td style="text-align: center;">$33.2 \pm 1.8$</td>
<td style="text-align: center;">$49.7 \pm 9.5$</td>
<td style="text-align: center;">$73.1 \pm 6.7$</td>
<td style="text-align: center;">$50.4 \pm 11.1$</td>
<td style="text-align: center;">$99.0 \pm 14.5$</td>
</tr>
</tbody>
</table>
<p>size, we maintain each generated dataset across epochs in a replay buffer, with each dataset being removed after 10 epochs.</p>
<p>We see similar improvements in performance against real and unguided synthetic data under this approach, with PGD outperforming real data on 2 out of 3 environments and datasets (Figure 4). Periodic generation outperforms continuous generation across environments and behavior policies, which we attribute to training stability, especially when performing guidance early in training. Regardless, both approaches consistently outperform training on real and unguided synthetic data, demonstrating the potential of PGD as a drop-in extension to replay and model-based RL methods.</p>
<h1>5.3 Synthetic Trajectory Analysis</h1>
<p>We now analyze the quality of trajectories produced by PGD against those from unguided diffusion and autoregressive world model (PETS) rollouts. In principle, we seek to evaluate the divergence of these sampling distributions from the true target distribution. However, this is not tractable to compute directly, so we instead investigate two proxy objectives:</p>
<ol>
<li>Trajectory Likelihood: mean log-likelihood of actions under the target policy; and</li>
<li>Dynamics Error: mean squared error between states in the synthetic trajectory and real environment, when rolled out with the same initial state and action sequence.</li>
</ol>
<p>In our experiments, we consider trajectory diffusion and MOPO-style PETS (Chua et al., 2018) models trained on representative datasets from the D4RL (Fu et al., 2020) benchmark that were featured in the previous section. Specifically, we consider the models trained on halfcheetah-medium, before sampling trajectories with IQL target policies trained on the halfcheetah-random, -medium, and -expert. This enables us to test the robustness of these models to target policies far from the behavior policy, both in performance and policy entropy.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Aggregate MuJoCo performance after training on unguided or policy-guided synthetic data under continuous and periodic dataset generation, as well as on the real dataset. For each setting, mean return over TD3+BC and IQL agents is marked, with standard error over 4 seeds (diffusion models and agents) highlighted.</p>
<p>Policy Guidance Increases Trajectory Likelihood In Figure 5, we present the trajectory likelihood of synthetic trajectories over varying degrees of guidance. Unsurprisingly, unguided diffusion generates low probability trajectories for all target policies, due to it directly modeling the behavior distribution. However, as we increase the guidance coefficient $\lambda$, trajectory likelihood increases monotonically under each target policy. Furthermore, this effect is robust across target policies, giving the ability to sample high-probability trajectories with OOD target policies. The value of $\lambda$ required to achieve the same action likelihood as direct action sampling (PETS) varies with the target policy. Since this threshold increases with target policy performance, we hypothesize that it increases with target policy entropy. Based on this, a promising avenue for future work is automatically tuning $\lambda$ for hyperparameter-free guidance.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Action probability of synthetic trajectories generated by diffusion and PETS models trained on halfcheetah-medium. Target policies are trained on halfcheetah-random, halfcheetah-medium, and halfcheetah-expert datasets, demonstrating robustness to OOD actions. Standard error over 4 diffusion model seeds is shaded (but negligible), with mean computed over 2048 synthetic trajectories.</p>
<p>Policy Guided Diffusion Achieves Lower Error Than Autoregressive Models In Figure 6, we present the dynamics error of synthetic trajectories over 16 rollout steps. For a fair comparison, we fix the guidance coefficient of PGD to $\lambda=1.0$, since this was sufficient to match the trajectory likelihood of PETS (Figure 5). Over all target policies, PGD achieves significantly lower error than PETS. Furthermore, PGD has similar levels of error across target policies, while PETS suffers from</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Dynamics mean squared error of synthetic trajectories generated by diffusion and PETS models trained on halfcheetah-medium. Standard error over 4 diffusion model seeds and 3 PETS seeds (via OfflineRL-Kit) is shaded, with each generating 2048 synthetic trajectories for analysis.
significantly higher error on OOD (random and expert) target policies. This highlights the robustness of PGD to target policy, a critical feature for generating high-likelihood training data throughout tabula rasa policy training.</p>
<h1>6 Related Work</h1>
<p>Model-based Offline Reinforcement Learning Model-based methods in offline RL (Yu et al., 2020; Kidambi et al., 2020; Rigter et al., 2022; Lu et al., 2022) are designed to augment the offline buffer with additional on-policy samples in order to mitigate distribution shift. This is typically done by rolling out a policy in a learned world model (Janner et al., 2019) and applying a suitable pessimism term in order to account for dynamics model errors. While these methods share the same overall motivation as our paper, the empirical realization is quite different. In particular, forward dynamics models are liable to compounding errors over long horizons, resulting in model exploitation, whereas our trajectories are generated in a single step.</p>
<p>Model-free Offline Reinforcement Learning Model-free methods in offline RL typically tackle the out-of-sample issue by applying conservatism to the value function or by constraining the policy to remain close to the data. For example, CQL (Kumar et al., 2020) and EDAC (An et al., 2021) both aim to minimize the values of out-of-distribution actions. Meanwhile, BCQ (Fujimoto et al., 2019) ensures that actions used in value targets are in-distribution with the behavioral policy using constrained optimization. We take the opposite approach in this paper: by enabling our diffusion model to generate on-policy samples without diverging from the behavior distribution, we reduce the need for conservatism.</p>
<p>Diffusion in Reinforcement Learning Diffusion models are a flexible method for data augmentation in reinforcement learning. SynthER (Lu et al., 2023) uses unguided diffusion models to upsample offline or online RL datasets, which are then used by model-free off-policy algorithms. While this improves performance, SynthER uses unguided diffusion to model the behavior distribution, resulting in the same issue of distributional shift. Similarly, MTDiff (He et al., 2023) considers unguided data generation in multitask settings.</p>
<p>Diffusion models have also been used to train world models. Zhang et al. (2023) train a world model for sensor observations by first tokenizing using VQ-VAE and then predicting future observations via discrete diffusion. Alonso et al. (2023) also train a world model using diffusion and demonstrate it can more accurately predict future observations. However, neither of these approaches model the whole trajectory, thereby suffering from compounding error, nor do they apply policy guidance. Parallel to this work, Rigter et al. (2023) use guidance from a policy to augment a diffusion world model for online RL. By contrast, we focus on the offline RL setting, provide a theoretical derivation and motivation for the trajectory distribution modeled by policy guidance, and demonstrate improvements in downstream policy performance.</p>
<p>Diffusion models are also used elsewhere in reinforcement learning. For example, Diffuser (Janner et al., 2022) and Decision Diffuser (Ajay et al., 2023) use trajectory diffusion models for planning and to bias planned trajectories towards high return. By contrast, we use on-policy guidance and train on the generated data. Diffusion models have also been used as an expressive policy class (Wang et al., 2023) for $Q$-learning, showing improvement over MLPs.</p>
<h1>7 Conclusion</h1>
<p>We presented policy-guided diffusion, a method for controllable generation of synthetic trajectories in offline RL. We provided a theoretical analysis of existing approaches to synthetic experience generation, identifying the advantages of direct trajectory generation compared to autoregressive methods. Motivated by this, we proposed PGD under the direct approach, deriving the regularized target distribution modeled by policy guidance.</p>
<p>Evaluating against PETS deep ensembles, a state-of-the-art autoregressive approach, we found that PGD can generate synthetic experience at the same target policy likelihood with significantly lower dynamics error. Furthermore, we found consistent improvements in downstream agent performance over a range of environments and behavior policies when trained on policy-guided synthetic data, against real and unguided synthetic experience.</p>
<p>By addressing the out-of-sample issue through synthetic data, we hope that this work enables the development of less conservative algorithms for offline RL. There are a range of promising avenues for future work, including automatically tuning the guidance coefficient for hyperparameter-free guidance, leveraging on-policy RL techniques with policy-guided data, and extending this approach to large-scale video generation models.</p>
<h2>Acknowledgments</h2>
<p>We thank Mattie Fellows and Sebastian Towers for their insights regarding our method's theoretical underpinning, as well as Alex Goldie and the NeurIPS 2023 Robot Learning Workshop reviewers for their helpful feedback. Matthew Jackson is funded by the EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines and Systems, and Amazon Web Services.</p>
<h2>References</h2>
<p>Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision making? In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=sP1fo2K9DFG.</p>
<p>Eloi Alonso, Adam Jelley, Anssi Kanervisto, and Tim Pearce. Diffusion world models. 2023.
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436-7447, 2021.</p>
<p>Philip J Ball, Cong Lu, Jack Parker-Holder, and Stephen Roberts. Augmented world models facilitate zero-shot dynamics generalization from a single offline environment. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 619-629. PMLR, 18-24 Jul 2021.</p>
<p>Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843-852, 2023.</p>
<p>James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and</p>
<p>Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax.</p>
<p>Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models, 2018.</p>
<p>Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780-8794, 2021.</p>
<p>Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.</p>
<p>Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.</p>
<p>Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 2052-2062. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr. press/v97/fujimoto19a.html.</p>
<p>Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li. Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. arXiv preprint arXiv:2305.18459, 2023.</p>
<p>Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 6840-6851. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.</p>
<p>Aapo Hyvärinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(4), 2005.</p>
<p>Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based policy optimization. In Advances in Neural Information Processing Systems, 2019.</p>
<p>Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022.</p>
<p>Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in Neural Information Processing Systems, 35:26565-26577, 2022.</p>
<p>Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Modelbased offline reinforcement learning. Advances in neural information processing systems, 33: $21810-21823,2020$.</p>
<p>Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rocktäschel. A survey of zero-shot generalisation in deep reinforcement learning. Journal of Artificial Intelligence Research, 76: 201-264, January 2023. ISSN 1076-9757. doi: 10.1613/jair.1.14174. URL http://dx.doi . org/10.1613/jair.1.14174.</p>
<p>Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.</p>
<p>Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179-1191, 2020.</p>
<p>Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.</p>
<p>Cong Lu, Philip Ball, Jack Parker-Holder, Michael Osborne, and Stephen J. Roberts. Revisiting design choices in offline model based reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=zz9hXVhf40.</p>
<p>Cong Lu, Philip J. Ball, Yee Whye Teh, and Jack Parker-Holder. Synthetic experience replay. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=6jNQ1AY1Uf.</p>
<p>Jiajun Ma, Tianyang Hu, Wenjia Wang, and Jiacheng Sun. Elucidating the design space of classifierguided diffusion generation. arXiv preprint arXiv:2310.11311, 2023.</p>
<p>Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pp. 80, 2000.</p>
<p>Doina Precup, Richard S. Sutton, and Satinder P. Singh. Eligibility traces for off-policy policy evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML '00, pp. 759-766, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1558607072 .</p>
<p>Marc Rigter, Bruno Lacerda, and Nick Hawes. RAMBO-RL: Robust adversarial model-based offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=nrksGSRT7kX.</p>
<p>Marc Rigter, Jun Yamada, and Ingmar Posner. World models via policy-guided trajectory diffusion, 2023.</p>
<p>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234-241. Springer, 2015.</p>
<p>Anya Sims, Cong Lu, and Yee Whye Teh. The edge-of-reach problem in offline model-based reinforcement learning, 2024.</p>
<p>Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2256-2265, Lille, France, 07-09 Jul 2015. PMLR. URL https:// proceedings.mlr.press/v37/sohl-dickstein15.html.</p>
<p>Yihao Sun. Offlinerl-kit: An elegant pytorch offline reinforcement learning library. https: //github.com/yihaosun1124/OfflineRL-Kit, 2023.</p>
<p>Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd. html.</p>
<p>Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. IEEE, pp. 5026-5033, 2012. URL http://dblp.uni-trier.de/db/conf/iros/ iros2012.html#TodorovET12.</p>
<p>Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class for offline reinforcement learning, 2023.</p>
<p>Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 14129-14142. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf.</p>
<p>Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Learning unsupervised world models for autonomous driving via discrete diffusion. arXiv preprint arXiv:2311.01017, 2023.</p>
<h1>Appendix</h1>
<h2>A Hyperparameters</h2>
<p>We open-source our implementation at https://github.com/EmptyJackson/policy-guided-diffusion.</p>
<h2>A. 1 Diffusion Model</h2>
<p>For the diffusion model, we used a U-Net architecture (Ronneberger et al., 2015) with hyperparameters outlined in Table 3. We transformed the trajectory by stacking the observation, action, reward, and done flags for each transition, before performing 1D convolution across the sequence of transitions.</p>
<p>Table 3: U-Net hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Trajectory length</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Kernel size</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Features</td>
<td style="text-align: center;">1024</td>
</tr>
<tr>
<td style="text-align: center;">U-Net blocks</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Dataset epochs</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">Adam</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">$2 \times 10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">LR schedule</td>
<td style="text-align: center;">Cosine decay</td>
</tr>
</tbody>
</table>
<h2>A. 2 Diffusion Sampling</h2>
<p>We use EDM (Karras et al., 2022) for diffusion sampling, retaining many of the default hyperparameters from Lu et al. (2023) (Table 4). We tuned the number of diffusion timesteps, finding diminishing improvement in dynamics error beyond 256 timesteps.</p>
<p>Table 4: EDM hyperparameters</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Diffusion timesteps</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{~S}_{\text {churn }}$</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{~S}_{\text {noise }}$</td>
<td style="text-align: center;">1.003</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{~S}_{\text {tmax }}$</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{~S}_{\text {tmin }}$</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{\text {max }}$</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{\text {min }}$</td>
<td style="text-align: center;">0.002</td>
</tr>
</tbody>
</table>
<h1>B Noised Target Distribution</h1>
<p>To model the target distribution with diffusion, we require the noise-conditioned score function $\nabla_{\tilde{\boldsymbol{\tau}}} \log p_{\text {target }}(\tilde{\boldsymbol{\tau}} ; \sigma)$ for the target distribution. However, since we do not have access to samples from $p_{\text {target }}(\tilde{\boldsymbol{\tau}} ; \sigma)$, one might wish to apply a factorization of the target distribution, such as</p>
<p>$$
p_{\text {target }}(\boldsymbol{\tau})=p_{\text {off }}(\boldsymbol{\tau}) \prod_{t=0}^{H-1} \frac{\pi_{\text {target }}(a \mid s)}{\pi_{\text {off }}(a \mid s)}
$$</p>
<p>before modeling its terms separately. However, by applying independent Gaussian noise to each of the elements in $\tilde{\boldsymbol{\tau}}$, we lose conditional independence between contiguous states and actions-i.e., $p_{\text {target }}\left(\hat{a}<em t="t">{t} \mid \hat{\boldsymbol{\tau}} \backslash \hat{a}</em>} ; \sigma\right) \neq p_{\text {target }}\left(\hat{a<em t="t">{t} \mid \hat{s}</em> ; \sigma)$ directly, as we propose in Section 4.1.} ; \sigma\right)$-preventing us from applying an equivalent factorization. Due to this, we must approximate $\nabla_{\tilde{\boldsymbol{\tau}}} \log p_{\text {target }}(\tilde{\boldsymbol{\tau}</p>
<h2>C Behavior-Regularized Target Distribution</h2>
<p>Intuitively, the behavior-regularized target distribution transforms the target distribution by increasing the likelihood of actions under the behavior policy. As is typical in offline RL (Kumar et al., 2020; Fujimoto \&amp; Gu, 2021; Fujimoto et al., 2019), regularizing the policy towards the behavior distribution is required in order to avoid out-of-sample states and consequently minimize value overestimation. Rather than regularizing the policy, PGD shifts this regularization to the data generation process, which helps our guided samples remain in-distribution with respect to the diffusion model, and thus less susceptible to model error.</p>
<p>Moreover, we note that this type of regularization is not immediately available for prior autoregressive world models, and thus they typically penalize reward by dynamics error (Yu et al., 2020; Kidambi et al., 2020; Lu et al., 2022) in an ad-hoc fashion in order to avoid model exploitation. In contrast, PGD presents a natural mechanism for behavioral regularization during data generation, making offline policy optimization without regularization a promising path for future work.</p>
<h2>D Agent Training with Policy-Guided Diffusion</h2>
<p>In Algorithm 2, we present pseudocode for training an agent with synthetic experience generated by PGD. PGD is agnostic to the underlying offline RL algorithm used to train the target policy, making it a drop-in extension to any model-free method.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="w"> </span><span class="nt">Agent</span><span class="w"> </span><span class="nt">training</span><span class="w"> </span><span class="nt">via</span><span class="w"> </span><span class="nt">policy-guided</span><span class="w"> </span><span class="nt">diffusion</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Parameters</span><span class="o">:</span><span class="w"> </span><span class="nt">Number</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">epochs</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{epochs</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">),</span><span class="w"> </span><span class="nt">steps</span><span class="w"> </span><span class="nt">per</span><span class="w"> </span><span class="nt">epoch</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{policy</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Required</span><span class="o">:</span><span class="w"> </span><span class="nt">Diffusion</span><span class="w"> </span><span class="nt">trajectory</span><span class="w"> </span><span class="nt">sampler</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">F</span><span class="p">}</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">pi</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">lambda</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="nt">epoch</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{epochs</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="nt">Generate</span><span class="w"> </span><span class="nt">synthetic</span><span class="w"> </span><span class="nt">dataset</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{epoch</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">F</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\tau</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">theta</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">lambda</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">for</span><span class="w"> </span><span class="nt">step</span><span class="w"> </span><span class="err">\</span><span class="o">(=</span><span class="nt">0</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">N_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{policy</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">            </span><span class="nt">Sample</span><span class="w"> </span><span class="nt">mini-batch</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">\}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">D</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{epoch</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span>
<span class="w">            </span><span class="nt">Update</span><span class="w"> </span><span class="nt">policy</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">on</span><span class="w"> </span><span class="nt">mini-batch</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="p">{</span><span class="err">\boldsymbol{\tau</span><span class="p">}</span><span class="err">\}\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
<span class="w">    </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">pi_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Typically $k \leq 5$ (Janner et al., 2019; Yu et al., 2020).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
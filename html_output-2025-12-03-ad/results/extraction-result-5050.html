<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5050 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5050</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5050</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-c561b3f596106a079733399961ea3d3044fb2747</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c561b3f596106a079733399961ea3d3044fb2747" target="_blank">Neural Language of Thought Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work introduces the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation and offers insights into the intersection of cognitive science and machine learning.</p>
                <p><strong>Paper Abstract:</strong> The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Our work presents a significant step towards creating neural networks exhibiting more human-like understanding by developing LoT-like representations and offers insights into the intersection of cognitive science and machine learning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5050.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5050.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLoTM / ALP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Language of Thought Model (NLoTM) with Autoregressive LoT Prior (ALP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An object-centric generative model that learns discrete, factorized 'semantic' tokens for objects (SVQ) and trains an autoregressive transformer prior (ALP) over those tokens; ALP is explicitly presented as analogous to neural language models that generate scenes one object-at-a-time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Autoregressive LoT Prior (ALP) within NLoTM; Semantic Vector-Quantized VAE (SVQ) encoder + transformer decoders</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SVQ: object-centric encoder based on Slot Attention that splits each slot into M factor-blocks and vector-quantizes each block with separate codebooks (block-level discretization). ALP: an autoregressive transformer decoder trained to predict the next discrete block token in the flattened sequence of N slots × M blocks. Prior / decoder hyperparameters used in experiments: 8 transformer layers, 4 attention heads, model dimension 192, feedforward dimension 768, dropout 0.1; ALP trained on SVQ tokens (SVQ frozen when training ALP). SVQ training: 400k iterations; ALP prior trained for ~1M iterations (implementation details provided).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Odd-One-Out (2D Sprites quadrant task) and CLEVR-Hard Property Comparison (object-property reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Odd-One-Out: 2D Sprites images split into four quadrants with exactly one object per quadrant; one object has a unique property (shape or color) and the task is to identify the quadrant containing the odd object — requires comparing object attributes and reasoning about spatial partitioning. CLEVR-Hard Property Comparison: assign integer labels based on maxima across object properties (shape, color, material) which requires identifying per-object properties and aggregating them; CLEVR also contains spatial relationships and position/size factors.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Convert visual scenes into sequences of discrete semantic tokens (object-factor codes) via SVQ, then use an autoregressive transformer (ALP) over those tokens to model/compositionaly generate scenes and to provide representations for downstream supervised tasks. Downstream classifiers freeze the learned representations (either codebook prototype vectors or discrete indices) and train small transformers to solve tasks (3-layer transformer for 2D Sprites downstream). The approach leverages object-centric decomposition (Slot Attention), block-level factorization (color/shape/position blocks), discrete codebooks, and autoregressive next-token prediction (language-model style) over those semantic tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Multiple analyses indicate spatial factors are represented and used: (1) Latent traversal experiments show particular SVQ blocks control object position (block 1 controls left/right placement; block 3 controls forward/back placement and size), demonstrating learned spatial factors. (2) Qualitative segmentation masks show slots localize objects (FG-ARI scores reported). (3) Downstream odd-one-out requires spatially-aware quadrant assignment and NLoTM codebook vectors enable generalization to OOD combinations (NLoTM Codebook achieved 99.1% OOD accuracy). (4) CLEVR generation samples preserve multi-object layouts better than patch-based priors, and FG-ARI segmentation metrics demonstrate disentangled object localization comparable to other slot-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generation: FID scores — 2D Sprites (3 obj): NLoTM FID 6.61 (best vs dVAE 7.26, VQ-VAE 14.81); 2D Sprites (4 obj): NLoTM FID 17.93 (best vs dVAE 19.15, VQ-VAE 26.35); CLEVR-Easy FID 32.50 (best vs dVAE 40.30, VQ-VAE 57.06); CLEVR-Hard FID 43.12; CLEVR-Tex FID 84.52. Generation accuracy (dataset constraint compliance, manual check of 128 samples): 2D Sprites (3 obj) ~75.0% for NLoTM, 2D Sprites (4 obj) 66.41%, 2D Sprites w/ bg (4 obj) 42.19%. Downstream odd-one-out: steps to 98% ID accuracy — NLoTM Codebook: 27,000 steps; OOD accuracy: NLoTM Codebook 99.1%. Other downstream: CLEVR-Hard Property Comparison — NLoTM Codebook ID acc 75.86%, OOD acc 71.15%. Segmentation: FG-ARI on CLEVR-Easy/HARD/Tex: NLoTM 91.37 / 90.48 / 70.93 (comparable to other slot methods). Ablations: number of blocks, codebook sizes, and prior capacity affect metrics (tables provided).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report limitations relevant to spatial aspects: (1) Experiments limited to synthetic datasets (2D Sprites, CLEVR variants, CLEVRTex); not demonstrated on complex natural scenes. (2) All latent factors are discrete: continuous factors like precise position/pose are only approximated; block-level factors sometimes entangle position and size (e.g., block 3 in latent traversal controls both), so spatial factors are not perfectly disentangled. (3) NLoTM Indices (discrete indices) generalize worse OOD than using prototype vectors because indices lack metric information about similarity. (4) Larger numbers of blocks or larger codebooks require higher-capacity priors — generation accuracy can drop if prior capacity is not increased. (5) Comparatively, some patch-based models achieved lower FID on specific datasets (e.g., VQ-VAE had lowest FID on 2D Sprites w/ background but much lower generation accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against patch-based discrete models (VQ-VAE with PixelCNN prior, dVAE with transformer prior), continuous object-centric model GENESIS-v2, and continuous factor model SysBinder. Key comparisons: NLoTM achieves lower FID and higher generation accuracy in many multi-object settings versus patch-based priors; NLoTM Codebook often outperforms VQ/dVAE on downstream OOD tasks (odd-one-out: NLoTM Codebook OOD 99.1% vs VQ-VAE Codebook 55.6% and dVAE variants ~26-29%). SysBinder (continuous factor model) is competitive on some downstream tasks (e.g., CLEVR-Hard property comparison SysBinder ID 79.60% vs NLoTM Codebook 75.86%), but NLoTM offers stronger generative sampling and discrete semantic tokens useful for compositional generation. Ablation shows scaling patch-based prior capacity does not fully close gap (dVAE prior up to 16-layer transformer improved but still worse than NLoTM on FID).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Language of Thought Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
                <li>Concepts in a probabilistic language of thought <em>(Rating: 2)</em></li>
                <li>Systematic visual reasoning through object-centric relational abstraction <em>(Rating: 2)</em></li>
                <li>An investigation into pre-training objectcentric representations for reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5050",
    "paper_id": "paper-c561b3f596106a079733399961ea3d3044fb2747",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "NLoTM / ALP",
            "name_full": "Neural Language of Thought Model (NLoTM) with Autoregressive LoT Prior (ALP)",
            "brief_description": "An object-centric generative model that learns discrete, factorized 'semantic' tokens for objects (SVQ) and trains an autoregressive transformer prior (ALP) over those tokens; ALP is explicitly presented as analogous to neural language models that generate scenes one object-at-a-time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Autoregressive LoT Prior (ALP) within NLoTM; Semantic Vector-Quantized VAE (SVQ) encoder + transformer decoders",
            "model_description": "SVQ: object-centric encoder based on Slot Attention that splits each slot into M factor-blocks and vector-quantizes each block with separate codebooks (block-level discretization). ALP: an autoregressive transformer decoder trained to predict the next discrete block token in the flattened sequence of N slots × M blocks. Prior / decoder hyperparameters used in experiments: 8 transformer layers, 4 attention heads, model dimension 192, feedforward dimension 768, dropout 0.1; ALP trained on SVQ tokens (SVQ frozen when training ALP). SVQ training: 400k iterations; ALP prior trained for ~1M iterations (implementation details provided).",
            "puzzle_name": "Odd-One-Out (2D Sprites quadrant task) and CLEVR-Hard Property Comparison (object-property reasoning)",
            "puzzle_description": "Odd-One-Out: 2D Sprites images split into four quadrants with exactly one object per quadrant; one object has a unique property (shape or color) and the task is to identify the quadrant containing the odd object — requires comparing object attributes and reasoning about spatial partitioning. CLEVR-Hard Property Comparison: assign integer labels based on maxima across object properties (shape, color, material) which requires identifying per-object properties and aggregating them; CLEVR also contains spatial relationships and position/size factors.",
            "mechanism_or_strategy": "Convert visual scenes into sequences of discrete semantic tokens (object-factor codes) via SVQ, then use an autoregressive transformer (ALP) over those tokens to model/compositionaly generate scenes and to provide representations for downstream supervised tasks. Downstream classifiers freeze the learned representations (either codebook prototype vectors or discrete indices) and train small transformers to solve tasks (3-layer transformer for 2D Sprites downstream). The approach leverages object-centric decomposition (Slot Attention), block-level factorization (color/shape/position blocks), discrete codebooks, and autoregressive next-token prediction (language-model style) over those semantic tokens.",
            "evidence_of_spatial_reasoning": "Multiple analyses indicate spatial factors are represented and used: (1) Latent traversal experiments show particular SVQ blocks control object position (block 1 controls left/right placement; block 3 controls forward/back placement and size), demonstrating learned spatial factors. (2) Qualitative segmentation masks show slots localize objects (FG-ARI scores reported). (3) Downstream odd-one-out requires spatially-aware quadrant assignment and NLoTM codebook vectors enable generalization to OOD combinations (NLoTM Codebook achieved 99.1% OOD accuracy). (4) CLEVR generation samples preserve multi-object layouts better than patch-based priors, and FG-ARI segmentation metrics demonstrate disentangled object localization comparable to other slot-based methods.",
            "performance_metrics": "Generation: FID scores — 2D Sprites (3 obj): NLoTM FID 6.61 (best vs dVAE 7.26, VQ-VAE 14.81); 2D Sprites (4 obj): NLoTM FID 17.93 (best vs dVAE 19.15, VQ-VAE 26.35); CLEVR-Easy FID 32.50 (best vs dVAE 40.30, VQ-VAE 57.06); CLEVR-Hard FID 43.12; CLEVR-Tex FID 84.52. Generation accuracy (dataset constraint compliance, manual check of 128 samples): 2D Sprites (3 obj) ~75.0% for NLoTM, 2D Sprites (4 obj) 66.41%, 2D Sprites w/ bg (4 obj) 42.19%. Downstream odd-one-out: steps to 98% ID accuracy — NLoTM Codebook: 27,000 steps; OOD accuracy: NLoTM Codebook 99.1%. Other downstream: CLEVR-Hard Property Comparison — NLoTM Codebook ID acc 75.86%, OOD acc 71.15%. Segmentation: FG-ARI on CLEVR-Easy/HARD/Tex: NLoTM 91.37 / 90.48 / 70.93 (comparable to other slot methods). Ablations: number of blocks, codebook sizes, and prior capacity affect metrics (tables provided).",
            "limitations_or_failure_cases": "Authors report limitations relevant to spatial aspects: (1) Experiments limited to synthetic datasets (2D Sprites, CLEVR variants, CLEVRTex); not demonstrated on complex natural scenes. (2) All latent factors are discrete: continuous factors like precise position/pose are only approximated; block-level factors sometimes entangle position and size (e.g., block 3 in latent traversal controls both), so spatial factors are not perfectly disentangled. (3) NLoTM Indices (discrete indices) generalize worse OOD than using prototype vectors because indices lack metric information about similarity. (4) Larger numbers of blocks or larger codebooks require higher-capacity priors — generation accuracy can drop if prior capacity is not increased. (5) Comparatively, some patch-based models achieved lower FID on specific datasets (e.g., VQ-VAE had lowest FID on 2D Sprites w/ background but much lower generation accuracy).",
            "comparison_baseline": "Compared against patch-based discrete models (VQ-VAE with PixelCNN prior, dVAE with transformer prior), continuous object-centric model GENESIS-v2, and continuous factor model SysBinder. Key comparisons: NLoTM achieves lower FID and higher generation accuracy in many multi-object settings versus patch-based priors; NLoTM Codebook often outperforms VQ/dVAE on downstream OOD tasks (odd-one-out: NLoTM Codebook OOD 99.1% vs VQ-VAE Codebook 55.6% and dVAE variants ~26-29%). SysBinder (continuous factor model) is competitive on some downstream tasks (e.g., CLEVR-Hard property comparison SysBinder ID 79.60% vs NLoTM Codebook 75.86%), but NLoTM offers stronger generative sampling and discrete semantic tokens useful for compositional generation. Ablation shows scaling patch-based prior capacity does not fully close gap (dVAE prior up to 16-layer transformer improved but still worse than NLoTM on FID).",
            "uuid": "e5050.0",
            "source_info": {
                "paper_title": "Neural Language of Thought Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1
        },
        {
            "paper_title": "Concepts in a probabilistic language of thought",
            "rating": 2
        },
        {
            "paper_title": "Systematic visual reasoning through object-centric relational abstraction",
            "rating": 2
        },
        {
            "paper_title": "An investigation into pre-training objectcentric representations for reinforcement learning",
            "rating": 2
        }
    ],
    "cost": 0.011516499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>NeURal Language of Thought Models</h1>
<p>Yi-Fu Wu ${ }^{1}$, Minseung Lee ${ }^{2}$, Sungjin Ahn ${ }^{2 *}$<br>${ }^{1}$ Rutgers University ${ }^{2}$ KAIST</p>
<h4>Abstract</h4>
<p>The Language of Thought Hypothesis suggests that human cognition operates on a structured, language-like system of mental representations. While neural language models can naturally benefit from the compositional structure inherently and explicitly expressed in language data, learning such representations from non-linguistic general observations, like images, remains a challenge. In this work, we introduce the Neural Language of Thought Model (NLoTM), a novel approach for unsupervised learning of LoTH-inspired representation and generation. NLoTM comprises two key components: (1) the Semantic Vector-Quantized Variational Autoencoder, which learns hierarchical, composable discrete representations aligned with objects and their properties, and (2) the Autoregressive LoT Prior, an autoregressive transformer that learns to generate semantic concept tokens compositionally, capturing the underlying data distribution. We evaluate NLoTM on several 2D and 3D image datasets, demonstrating superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. Our work presents a significant step towards creating neural networks exhibiting more human-like understanding by developing LoT-like representations and offers insights into the intersection of cognitive science and machine learning.</p>
<h2>1 INTRODUCTION</h2>
<p>The Language of Thought Hypothesis (LoTH) (Fodor et al., 1975) suggests that human cognition is based on a structured, language-like system of mental representations, often referred to as "Mentalese". Mentalese comprises word-like units that form sentence-like structures, which convey meaning. The meaning of these mental "sentences" is systematically determined by the meanings of their constituent "words" and their specific arrangement. From a computational viewpoint, while neural language models (Bengio et al., 2000; Brown et al., 2020; Bommasani et al., 2021) can benefit from the compositional and symbolic structure inherently expressed in the language data they are trained on, it remains unclear how we can learn such LoT-like structure from non-linguistic general observations, such as images, videos, and audio signals. The significance of this ability is further highlighted by the fact that infants learn these structures from observing objects and events before they acquire language skills (Spelke, 2022).</p>
<p>How can we create neural networks that learn to develop such language of thought representations in an unsupervised way? To address this, we outline the following three properties as the desired characteristics of a neural language of thought model.</p>
<p>First, when perceiving a visual scene, humans do not simply represent it as a monolithic vector of features. Instead, we view the scene structurally and semantically, recognizing it as a composition of meaningful components such as objects and their attributes, including shape, color, and position (Palmer, 1977; Singer, 2007; Spelke \&amp; Kinzler, 2007). Our observation here is that in line with the LoTH, these visual attributes can be likened to words, objects to sentences, and the scene to a paragraph. Recent works, particularly those focused on object-centric representations (Greff et al., 2020), have demonstrated that this structural decomposition facilitates the benefits associated with the LoTH such as relational reasoning (Wu et al., 2021; Yoon et al., 2023; Webb et al., 2023a;b) and out-of-distribution generalization (Dittadi et al., 2022; Yoon et al., 2023) due to increased compositional generalization.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Desiderata for Neural Language of Thought Models and Related Models</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">VAE</th>
<th style="text-align: center;">VQ-VAE</th>
<th style="text-align: center;">Slot Attention</th>
<th style="text-align: center;">SysBinder</th>
<th style="text-align: center;">NLoTM (Ours)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Compositionality <br> (Semantic Scene Decomposition)</td>
<td style="text-align: center;">Factor</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">Object</td>
<td style="text-align: center;">Object \&amp; Factor</td>
<td style="text-align: center;">Object \&amp; Factor</td>
</tr>
<tr>
<td style="text-align: center;">Symbolic <br> (Discrete Concept Abstraction)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\checkmark$ (Patch Concept)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\checkmark$ (Semantic Concept)</td>
</tr>
<tr>
<td style="text-align: center;">Productivity <br> (Probabilistic Compositional Generation)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$ (Patch Stitching)</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">$\checkmark$ (Semantic Composition)</td>
</tr>
</tbody>
</table>
<p>Moreover, these structured and semantic representations can be categorized and conceptualized, resulting in symbol-like discrete concept abstraction. Such an ability is critical for organizing and comprehending the complexity of the environment, e.g., via language, as well as for implementing modularity (Andreas et al., 2016) or symbolic reasoning (Lake et al., 2016). Such discrete representations are also useful to leverage powerful generative models like autoregressive transformers (Vaswani et al., 2017). One of the most popular models for discrete representation learning is VQ-VAE (van den Oord et al., 2017). It has been shown to be beneficial for image generation (Razavi et al., 2019; Esser et al., 2021) and probability density modeling (Van den Oord et al., 2016). However, VQ-VAE and its variants, such as dVAE (Ramesh et al., 2021; Singh et al., 2022a) and VQ-GAN (Esser et al., 2021), represent a scene as a grid of small patches, lacking the capability to capture the scene's holistic structure and semantics.</p>
<p>Besides, the ability to compositionally and probabilistically generate samples that adhere to the distribution of prior beliefs, constructed from observation data, is crucial for endowing AI with the capabilities to imagine and simulate. These abilities are essential for tasks such as planning (Mattar \&amp; Lengyel, 2022; Hafner et al., 2019) and reasoning. However, this ability, related to probabilistic Language of Thought (PLoT) (Goodman et al., 2015) and productivity, is supported by only a certain class of representation learning models. While models like Slot Attention (Locatello et al., 2020) and SysBinder (Singh et al., 2023) offer structured, object-centric compositional representations, in their original form it is unclear how they support density-based sampling. In contrast, VAE-based models support this ability to sample from a prior distribution. However, they either do not provide object-centric structures or are limited to patch-based discrete abstractions (VQ-VAE).</p>
<p>In this work, we present the Neural Language of Thought Model (NLoTM), the first model that satisfies the aforementioned criteria summarized in Table 1. NLoTM comprises two novel components: the Semantic Vector-Quantized (SVQ) Variational Autoencoder and the Autoregressive LoT Prior (ALP). SVQ achieves discrete semantic decomposition of a scene by learning hierarchical, composable factors that closely align with the objects and the properties of objects in visual scenes, similar to the role of words and sentences in LoTH. ALP, analogous to language models trained on text tokens, is an autoregressive transformer trained to learn a probabilistic generation of semantic concept tokens. However, unlike VQ-VAE which stitches a grid of patches, ALP composes semantic concepts such as objects and their attributes to represent a scene.</p>
<p>In our experiments, we demonstrate the following practical benefits of our method. First, we find that for multi-object scenes, NLoTM is able to model the prior distribution better than patch-based methods, as measured by the quality of the samples generated. Second, we find that NLoTM representations outperform patch-based VQ representations on downstream tasks that require knowledge of the different properties of the objects in the scene. We also find evidence that NLoTM representations can generalize better to out-of-distribution tasks compared to patch-based VQ representations and SysBinder continuous representations. Lastly, we show that despite introducing a discrete bottleneck, NLoTM can work on the challenging CLEVRTex (Karazija et al., 2021) dataset, one of the most complex datasets used in recent unsupervised object-centric representation learning models.</p>
<p>Our contributions are as follows: First, we introduce NLoTM, the first neural network model implementing the LoTH for unstructured observations. Second, we propose the SVQ model to obtain object-centric semantic neural discrete representations. Third, we propose the ALP to model the probabilistic prior capable of capturing the underlying data distribution and compositional generation of new samples. Lastly, we evaluate our model on several 2D and 3D datasets including the challenging CLEVRTex dataset, showing superior downstream task performance and image generation quality.</p>
<h1>2 BACKGROUND</h1>
<h3>2.1 Vector-Quantized Variational Autoencoder (VQ-VAE)</h3>
<p>The VQ-VAE (van den Oord et al., 2017) is a model that learns to compress high-dimensional data into a discretized latent space. The latent space is maintained by a codebook of prototype vectors $\mathbf{e} \in \mathbb{R}^{K \times d}$ where $K$ is the size of the codebook and $d$ is the dimensionality of each prototype vector. An input $\mathbf{x}$ is first passed through encoder $E(\mathbf{x})$ to obtain latents $\mathbf{z}<em e="e">{e} \in \mathbb{R}^{d}$. A nearest-neighbor lookup between $\mathbf{z}</em>}$ and each of the prototype vectors in the codebook yields a quantized representation $\mathbf{z<em e="e">{q}=\operatorname{Quantize}\left(\mathbf{z}</em>}\right)=\mathbf{e<em j="j">{k}$ where $k=\arg \min </em>}\left|\mathbf{z<em j="j">{e}-\mathbf{e}</em>\right|<em q="q">{2}$. The decoder $D$ then uses $\mathbf{z}</em>\right)$. The model is trained with the following loss:}$ to reconstruct the input: $\hat{\mathbf{x}}=\mathrm{D}\left(\mathbf{z}_{q</p>
<p>$$
\mathcal{L}=\underbrace{|\mathbf{x}-\hat{\mathbf{x}}|<em _Reconstruction="{Reconstruction" _text="\text">{2}^{2}}</em>}}+\underbrace{\left|\mathrm{sg}\left[\mathbf{z<em q="q">{e}\right]-\mathbf{z}</em>\right|<em _Codebook="{Codebook" _text="\text">{2}^{2}}</em>}}+\beta \underbrace{\left|\mathbf{z<em q="q">{e}-s g\left[\mathbf{z}</em>\right]\right|<em _Commitment="{Commitment" _text="\text">{2}^{2}}</em>
$$}</p>
<p>The first term is a reconstruction loss and is used to train the encoder and decoder. A straight-through estimator (Bengio et al., 2013) is used to estimate the gradients through the quantization step by copying the gradients from $\mathbf{z}<em e="e">{q}$ to $\mathbf{z}</em>$. The second term is the codebook loss which encourages the prototype vectors in the codebook to be close to the output of the encoder. The third term, scaled by a constant hyperparameter $\beta$, is the commitment loss and helps to stabilize the training by encouraging the output of the encoder to not deviate too much from the chosen prototype vectors. Instead of the codebook loss, we use exponential moving average (EMA) updates on the codebook, which we found to speed up training in our experiments (Razavi et al., 2019; Dhariwal et al., 2020; Yan et al., 2021).
When VQ-VAEs are applied to images $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$, the encoder $E(\mathbf{x})$ is typically implemented as a convolution encoder, outputting a feature map of latents $\mathbf{z}<em s="s">{e} \in \mathbb{R}^{H</em>$. This means that each latent corresponds to a local area represented by a convolutional feature cell and thus can only capture information in a local receptive field (Figure 1a). However, images typically contain multiple objects, and the discrete factors underlying visual scenes typically correspond to different properties of the objects in the scene, such as shape, color, type, and so on. The local patches from convolutional feature maps are inadequate to capture this rich structure.} \times W_{s} \times d</p>
<h3>2.2 ObJect-CENTRIC REPRESENTATIONS</h3>
<p>The goal of unsupervised object-centric representation learning is to decompose a scene into a set of representations each capturing a different object in the scene. It is shown that this structural decomposition, matching to the true factor structure of the world, facilitates some high-level cognition abilities such as relational reasoning (Wu et al., 2021; Yoon et al., 2023; Webb et al., 2023a;b) and out-of-distribution generalization (Dittadi et al., 2022; Yoon et al., 2023). We build on top of Slot Attention (Locatello et al., 2020), a spatial attention-based object-centric representation method.
Given an image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$, slot attention learns a set of slots, $\mathbf{s}=\left{\mathbf{s}<em N="N">{1}, \ldots, \mathbf{s}</em>}\right}$, where $\mathbf{s<em s="s">{n} \in \mathbb{R}^{d</em>$ as the keys and values. Instead of normalizing over the keys as is done in traditional dot-product attention, normalization is done over the queries (ie. slots). Additionally, a weighted mean is used to aggregate the values instead of the normal weighted sum, which is shown to stabilize training. The result is then used to update the slots with a per-slot GRU (Chung et al., 2014) followed by a per-slot residual MLP, both with shared parameters across the slots.
The slot representations are then used in a decoder to reconstruct the image and the entire model is trained with an image reconstruction loss. The original formulation of slot attention used a spatial broadcast decoder (Watters et al., 2019b) to create masked images per slot which are then combined to form a final reconstructed image. Recently, (Singh et al., 2022a) proposed using a transformer decoder to reconstruct the image while attending to the slots with cross attention. This method was shown to scale to more complex scenes than the spatial broadcast decoder (Singh et al., 2022b) and is what we choose to use in our model.}}$ and $N$ is the total number of slots. An encoder is applied to $\mathbf{x}$ and, after adding a positional encoding, the result is flattened to an $L$-length input feature vector $\mathbf{F} \in \mathbb{R}^{L \times d_{F}}$. Then, an iterative attention mechanism is used to spatially group the input features $\mathbf{F}$ to the slot representations $\mathbf{s}$. First, the slots are randomly initialized from a Gaussian distribution with learned parameters. Then, in each iteration, the slots are used as queries in an inverted version of dot-product attention (Tsai et al., 2020) with the input features $\mathbf{F</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison between VQ-VAE, Quantized Slots, and SVQ. (a) VQ-VAE quantizes the scene at a local patch level and may not capture the semantic structure of the scene. (b) Quantized Slots (QS) would quantize the scene at the slot level but require a separate code for every possible configuration of an object. (c) SVQ quantizes at the block level, representing each factor (such as color or shape) as a code. In this example, to represent all possible object configurations, SVQ requires only 10 codebook entries at the block level while QS requires 25.</p>
<h1>3 Neural Language of Thought Model</h1>
<h3>3.1 Semantic Vector Quantization</h3>
<p>Given a slot attention encoder that can obtain a set of representations of the objects in a scene, one may think of a hypothetical method, applying vector quantization to the slot representation itself to obtain a set of semantic discrete representations (Figure 1b). While these representations would indeed correspond to the different objects in a scene, this scheme would require one codebook entry per possible object configuration and be insufficient for anything beyond trivially simple scenes.</p>
<p>For example, consider a simple scene containing a single object in a fixed position that only varies by color and shape. Assume there are $c$ possible colors and $s$ possible shapes for the object. With slot-level quantization, in order to represent all the potential objects, the codebook would require at least $c \times s$ entries. This is because each slot representation is a single entangled representation, so each combination of factors needs to be represented by a separate code. If, instead, we were able to disentangle the object-level representations into factor-level representations-representations that align with the underlying latent factors of variation of each object-we could describe the potentially large combinatorial space of each object with a much smaller number of discrete factors. In the above example, if we had a fully disentangled representation of the color and the shape, we could represent all possible scenes with $c+s$ codes (Figure 1c). See Appendix B. 2 for further discussion.</p>
<p>This observation motivates us to design an architecture that further disentangles slot representations to factor representations that reflect the underlying discrete factors of the objects in the scene, and to perform vector quantization on these factor representations. Under this scheme, each object representation would be composed of multiple discrete factors, and each factor would have its own codebook that can be shared across objects. The resulting model, the Semantic Vector-Quantized Variational Autoencoder (SVQ), is depicted in Figure 2a and described below.</p>
<p>To obtain factored representations, we follow an approach motivated by Neural Systematic Binder (SysBinder) (Singh et al., 2023), where a binding mechanism is introduced to produce disentangled factors within a slot. Specifically, the following modifications are applied to slot attention: First, we maintain $M$ codebooks $\mathbf{C} \in \mathbb{R}^{M \times K \times d_{c}}$, each with $K$ discrete prototype vectors of dimension $d_{c}=\frac{d_{s}}{M}$. Then, we split each of the $N d_{s}$-dimensional slot representations into $M$ equally-sized blocks, each of which will represent one factor. We denote the full set of block representations as $\mathbf{z}<em c="c">{c} \in \mathbb{R}^{N \times M \times d</em>}}$. Crucially, we replace the slot-level GRUs and residual MLPs with block-level equivalents that have shared parameters across blocks corresponding to the same factor. At the end of each slot attention iteration, we apply vector quantization for each block using its corresponding codebook to obtain a set of quantized blocks $\mathbf{z<em c="c">{q} \in \mathbb{R}^{N \times M \times d</em>$. For $n \in[1, N], m \in[1, M]$,}</p>
<p>$$
\mathbf{z}<em k="k" m_="m,">{q}^{n, m}=\mathbf{C}</em>} \text { where } k=\underset{j}{\arg \min }\left|\mathbf{z<em j="j" m_="m,">{c}^{n, m}-\mathbf{C}</em>
$$}\right|_{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overall architecture of NLoTM. (a) The Semantic Vector-Quantized (SVQ) Variational Autoencoder. We maintain $M$ learned codebooks and split each slot into $M$ blocks. After each Slot Attention iteration, we apply vector quantization to each block representation to obtain a set of discrete codes for each slot. Each block ends up specializing to different underlying factors of variation for the objects in the scene. (b) The Autoregressive LoT Prior (ALP). We train an autoregressive prior over the discrete latent codes from SVQ. Sampling from this prior allows us to generate an image one object at a time, based on their properties.
where $\mathbf{z}<em k="k" m_="m,">{q}^{n, m}$ denotes the $m$-th block in the $n$-th slot and $\mathbf{C}</em>$ is the $k$-th prototype vector in the $m$-th codebook. By sharing the codebook for each block across all of the slots, each block ends up specializing in different underlying factors of the objects in the scene, such as color, shape, and position. Thus, these quantized representations are semantic in the sense that they contain factor-level representations mapping to the underlying structure of the scene.</p>
<p>To reconstruct the image, we use the autoregressive transformer decoder described in Section 2.2 and condition on $\mathbf{z}_{q}$ via cross attention. Similar to Singh et al. (2023), we first let the blocks within a slot interact with a single-layer transformer and then add a block-level positional encoding before inputting the representations as cross attention in the transformer decoder. We train the model with the reconstruction loss, the VQ-VAE commitment loss, and we update the codebooks with EMA updates. To prevent codebook collapse, we also incorporate random restarts for the embeddings, similar to previous work (Dhariwal et al., 2020). To achieve this, we keep a count of the usage of each code in the codebooks and randomly reset it to be near one of the encoder outputs of the current batch if its usage falls below a threshold.</p>
<h1>3.2 Autoregressive Language of Thought Prior</h1>
<p>Given these semantic discrete codes representing the different objects in the scene, we can now freeze the SVQ and train a prior $p\left(\mathbf{z}_{q}\right)$ over these codes to obtain a generative model of the underlying data that captures the structure and semantics of the data, analogous to language models that are trained over text tokens. We can sample from this prior, called Autoregressive LoT Prior (ALP), to obtain codes for new scenes and use these codes in the SVQ decoder to generate new images. Compared to patch-based VQ methods that generate patch tokens corresponding to a local region of an image, ALP generates a representation of a scene one object at a time, based on their properties (Figure 2b).</p>
<p>We implement the prior using a simple autoregressive transformer decoder. First, we flatten $\mathbf{z}<em c="c">{q}$ along the slot and block dimensions to a vector with dimensions $N M \times d</em>$. We then apply a positional encoding across all slots and blocks and input the resulting vector to a transformer decoder with an objective of predicting the discrete code of the next block. Although slot attention does not guarantee any specific ordering of the slots, the blocks within the slots are arranged in a predefined order. Therefore, the positional encoding is important in providing information about the ordering of the blocks as well as which block belongs to which slot.</p>
<p>Note that generating the latents of one image requires sampling $N M$ blocks, but does not depend on the size of the image. This is different than VQ-VAE, which scales with the size of the feature map and may become expensive for high-resolution images.</p>
<h1>4 Related Work</h1>
<p>Neural Discrete Representation Learning. Our work builds on top of neural discrete representation learning, which has played a pivotal role in the advancement of generative models for images in recent years (van den Oord et al., 2017; Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021; Yu et al., 2022). These methods typically follow a two-stage approach. First, an image is encoded into a CNN feature map, which is then tokenized using vector quantization (Gray, 1984) into a set of discrete latent variables. In the second stage, a powerful autoregressive prior is then trained to model the distribution of these discrete tokens, allowing for sampling new images from this distribution. Our model also follows this two-stage approach, except our latents correspond to the properties of objects instead of cells in a CNN feature map.</p>
<p>Unsupervised Object-Centric Learning. Recent unsupervised object-centric learning methods have been shown to decompose an image or video into a set of latents, each representing an object in the scene (Burgess et al., 2019; Greff et al., 2019; Anciukevicius et al., 2020; Locatello et al., 2020; Greff et al., 2017; Engelcke et al., 2020; 2022; von Kügelgen et al., 2020; Du et al., 2021; Kabra et al., 2021; Zhang et al., 2022; Eslami et al., 2016; Lin et al., 2020b; Jiang \&amp; Ahn, 2020; Chen et al., 2021; Deng et al., 2021; Lin et al., 2020b;a; Singh et al., 2023; Kipf et al., 2022; Singh et al., 2022b; Gopalakrishnan et al., 2022; Seitzer et al., 2022; Hénaff et al., 2022; Wang et al., 2023a; Wu et al., 2021; Wen et al., 2022; Zoran et al., 2021). While most of these methods result in a distributed representation per object, there have been several attempts at learning more structured or disentangled representations, such as those methods that decompose the latents into what and where components (Eslami et al., 2016; Crawford \&amp; Pineau, 2019b;a; Jiang et al., 2019; Jiang \&amp; Ahn, 2020; Lin et al., 2020b;a; Chen et al., 2021) or those that learn disentangled latents via a VAE (Greff et al., 2019; Zoran et al., 2021). Closely related to our work, recent methods have been designed to learn factor-level disentanglement (Singh et al., 2023; Kirilenko et al., 2023). However, these methods still operate with continuous latents instead of discrete tokens and do not support sampling new images. While there are several object-centric learning methods that do support sampling new images (Engelcke et al., 2020; 2022; Jiang \&amp; Ahn, 2020; Wang et al., 2023b), these also do not use semantic discrete latents as we do in our work.</p>
<h2>5 EXPERIMENTS</h2>
<p>Datasets. We evaluate our model on two variants of a 2D Sprites dataset (Watters et al., 2019a; Yoon et al., 2023) and three variants of the CLEVR dataset (Johnson et al., 2017), CLEVR-Easy, CLEVR-Hard, CLEVR-Tex. In the 2D Sprites datasets, objects of varying shapes and colors are placed in a scene. In total, there are 7 possible colors and 12 possible shapes. In each image, one object has a single property that is unique from the other objects. All other properties are shared by at least two objects. This structure allows us to evaluate if the prior correctly models the dependencies between the properties of the scene. We test versions of this dataset with and without textured backgrounds (Cimpoi et al., 2014). CLEVR-Easy, CLEVR-Hard, and CLEVR-Tex were previously used in (Singh et al., 2023) and are modified from the original CLEVR (Johnson et al., 2017) and CLEVR-Tex (Karazija et al., 2021) datasets to have larger objects so properties such as shape and texture are more clearly visible. In CLEVR-Easy, objects may differ by only shape, color, and position. In this dataset, there are 3 possible shapes and 8 possible colors. In CLEVR-Hard, objects may differ by shape, color, position, size, and material. There are 3 possible shapes, 137 possible colors, and 2 possible materials (shiny or matte). In CLEVR-Tex, there are 4 possible shapes and 58 possible textures for the objects and background.</p>
<p>Baselines. We compare our model with several patch-based quantization methods: VQ-VAE (van den Oord et al., 2017) with a PixelCNN (Van den Oord et al., 2016) prior, and dVAE (Ramesh et al., 2021; Singh et al., 2022a) with a transformer decoder prior. For the dVAE baseline, we use the dVAE weights that are trained along with the SVQ. This provides a more direct ablation comparing the ALP of NLoTM with the patch-based transformer decoder prior since the dVAE decoder is shared across these models and will not contribute to differences in image quality. We also compare with GENESIS-v2 (Engelcke et al., 2022), a continuous latent object-centric model with an autoregressive prior that can also generate samples.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Generated samples for the 4-object 2D Sprites and 4-object 2D Sprites with background datasets.</p>
<h1>5.1 Generating Samples with the Autoregressive LoT Prior</h1>
<h3>5.1.1 2D SPRITES</h3>
<p>We show the sample generations for the 2D Sprites datasets in Figure 3 and the FID results in Table 2. We additionally calculate generation accuracy by manually inspecting 128 images per model to check if the generated images follow the constraints of the dataset. That is, each image must have exactly one object that has a unique property. All other properties in the scene will have at least one duplicate among the other objects.</p>
<p>We see that for the simplest dataset with 3 objects and no background, NLoTM achieves the lowest FID of the models and comparable generation accuracy to dVAE, generating about $75 \%$ of the scenes correctly. This setting may be simple enough that dVAE with a transformer prior can capture the structure of the scene even with a patch-based discrete latent. As the scene complexity increases with more objects and textured background, NLoTM starts to outperform the baselines in terms of generation accuracy. Inspecting the qualitative results, we see that in the dataset with the background, VQ-VAE and dVAE start generating occasional blurry objects, whereas NLoTM maintains cleanlooking objects that match the ground truth dataset. This may be because NLoTM can segment the background into its own slot and factor the texture into a discrete latent, cleanly separating the representation of the objects from the background. The patch-based methods, however, may have a harder time separating the foreground from the background resulting in messier generations. Interestingly, despite the blurry shapes, VQ-VAE achieves the lowest FID score on the 2D Sprites dataset with background. We hypothesize this may be because the model spends more capacity modeling the background correctly instead of the foreground, which may produce a better FID score, but not necessarily better generation accuracy. This is confirmed by the low generation accuracy of the VQ-VAE model this dataset, only generating $19.5 \%$ of the scenes correctly.</p>
<p>Table 2: FID and Generation Accuracy on the 2D Sprites datasets. For Generation Accuracy, 128 samples were inspected manually to determine if they matched the constraints of the scene (ie. exactly one unique property among all the shapes). Underlined numbers indicate a minor difference from the best value.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">FID $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generation Accuracy (in \%) $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">VQ-VAE</td>
<td style="text-align: center;">dVAE</td>
<td style="text-align: center;">NLoTM (ours)</td>
<td style="text-align: center;">VQ-VAE</td>
<td style="text-align: center;">dVAE</td>
<td style="text-align: center;">NLoTM (ours)</td>
</tr>
<tr>
<td style="text-align: left;">2D Sprites (3 obj)</td>
<td style="text-align: center;">14.81</td>
<td style="text-align: center;">7.26</td>
<td style="text-align: center;">$\mathbf{6 . 6 1}$</td>
<td style="text-align: center;">28.91</td>
<td style="text-align: center;">$\mathbf{7 5 . 7 8}$</td>
<td style="text-align: center;">$\underline{75.00}$</td>
</tr>
<tr>
<td style="text-align: left;">2D Sprites (4 obj)</td>
<td style="text-align: center;">26.35</td>
<td style="text-align: center;">19.15</td>
<td style="text-align: center;">$\mathbf{1 7 . 9 3}$</td>
<td style="text-align: center;">21.88</td>
<td style="text-align: center;">62.50</td>
<td style="text-align: center;">$\mathbf{6 6 . 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">2D Sprites w/ bg (4 obj)</td>
<td style="text-align: center;">$\mathbf{5 8 . 1 4}$</td>
<td style="text-align: center;">66.08</td>
<td style="text-align: center;">$\underline{58.50}$</td>
<td style="text-align: center;">19.53</td>
<td style="text-align: center;">30.47</td>
<td style="text-align: center;">$\mathbf{4 2 . 1 9}$</td>
</tr>
</tbody>
</table>
<h3>5.1.2 CLEVR</h3>
<p>In Figure 4, we show sample generations after training the models on the CLEVR-Easy, CLEVRHard, and CLEVR-Tex datasets. We report the Frechet Inception Distance (FID) in Table 3. We find that compared to the other models, GENESIS-v2 generates very blurry images and completely fails on CLEVR-Tex, resulting in a high FID. While VQ-VAE produces sharper images, several of the generated shapes are malformed or have mixed colors. The dVAE-generated images look closer to the ground truth dataset, but still have some errors such as overlapping objects (first image) and generating scenes with more objects than seen in the training set (third image). NLoTM has the lowest FID for all of these datasets and the generated images look very close to the ground truth dataset, indicating the usefulness of the ASP for generating these multi-object scenes.</p>
<p>In Appendix B.1, we show additional analysis of the learned codebook on the CLEVR-Easy dataset.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Generated samples for the CLEVR-Easy, CLEVR-Hard, and CLEVR-Tex Datasets.</p>
<p>Table 3: FID for the various models on the CLEVR datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">FID $\downarrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: center;">GENESIS-v2</td>
<td style="text-align: center;">VQ-VAE</td>
<td style="text-align: center;">dVAE</td>
<td style="text-align: center;">NLoTM (ours)</td>
</tr>
<tr>
<td style="text-align: left;">CLEVR-Easy</td>
<td style="text-align: center;">115.56</td>
<td style="text-align: center;">57.06</td>
<td style="text-align: center;">40.30</td>
<td style="text-align: center;">$\mathbf{3 2 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">CLEVR-Hard</td>
<td style="text-align: center;">93.01</td>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">65.89</td>
<td style="text-align: center;">$\mathbf{4 3 . 1 2}$</td>
</tr>
<tr>
<td style="text-align: left;">CLEVR-Tex</td>
<td style="text-align: center;">225.08</td>
<td style="text-align: center;">178.59</td>
<td style="text-align: center;">112.80</td>
<td style="text-align: center;">$\mathbf{8 4 . 5 2}$</td>
</tr>
</tbody>
</table>
<h1>5.2 Downstream Tasks</h1>
<h3>5.2.1 Odd-One-Out</h3>
<p>We first evaluate on a downstream supervised learning task on the 2D Sprites dataset. We modify the dataset by first dividing each image into four quadrants and ensuring exactly one object will be in each quadrant. As in our previous experiments, one object has a single property that is unique from the other objects. The goal of the task is to identify the quadrant of the odd-one-out object. We first pretrain the baseline models on a dataset containing all 12 possible shapes and 7 possible colors. Then, we freeze the underlying model and train a downstream model on top of the learned representations with the supervised objective. The downstream model is trained on a dataset that only contains 9 possible shapes and 4 possible colors. We then evaluate on both the in-distribution (ID) dataset and an out-of-distribution (OOD) dataset that consists of the remaining 3 shapes and 3 colors. In addition to dVAE and VQ-VAE, we use SysBinder as a baseline for this task, to compare its continuous representation with NLoTM's discrete representation. For the latent representation of NLoTM, we include variants that use the codebook indices in the SVQ (NLoTM Indices) and the codebook prototype vectors in the SVQ (NLoTM Codebook).</p>
<p>Table 4 shows the results of our experiments. Since all models can solve the task when evaluated on the ID dataset, we report the number of steps to reach $98 \%$ accuracy on the validation dataset. We find that SysBinder and NLoTM Codebook learn the quickest in the ID setting. For the OOD setting, we find that dVAE and VQ-VAE fail, not performing better than randomly guessing, showing that the patch-based discrete latent is insufficient for OOD generalization in this task. SysBinder can partially solve the task in the OOD setting, while the NLoTM Codebook seems to be able to solve the task, achieving $99 \%$ accuracy. This indicates that the compact latent space offered by the discrete code provides better OOD generalization abilities for this particular task. One possible explanation for this is that since this is an odd-one-out task, the downstream network needs to do comparisons between the properties of the objects and this may be easier to do with SVQ's codebook vectors that are fixed. SysBinder's continuous latents, on the other hand, offer greater variations for the same concept. This</p>
<p>Table 4: Results for the downstream odd-oneout task. Since all the models can solve the in-distribution (ID) task, we report the number of steps to 98% ID Accuracy and out-of-distribution (OOD) accuracy.</p>
<table>
<thead>
<tr>
<th></th>
<th>Steps to 98% (↓)</th>
<th>OOD Acc. % (↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>dVAE Discrete</td>
<td>37,000</td>
<td>26.7</td>
</tr>
<tr>
<td>dVAE Continuous</td>
<td>32,000</td>
<td>29.5</td>
</tr>
<tr>
<td>VQ-VAE Indices</td>
<td>77,000</td>
<td>24.0</td>
</tr>
<tr>
<td>VQ-VAE Codebook</td>
<td>54,500</td>
<td>55.6</td>
</tr>
<tr>
<td>SysBinder</td>
<td>27,000</td>
<td>67.6</td>
</tr>
<tr>
<td>NLoTM Indices</td>
<td>77,000</td>
<td>46.8</td>
</tr>
<tr>
<td>NLoTM Codebook</td>
<td>27,000</td>
<td>99.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Results for the downstream CLEVR-Hard Property Comparison task.</p>
<table>
<thead>
<tr>
<th></th>
<th>ID Acc. % (↑)</th>
<th>OOD Acc. % (↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>dVAE Discrete</td>
<td>27.52</td>
<td>19.87</td>
</tr>
<tr>
<td>dVAE Continuous</td>
<td>24.51</td>
<td>20.51</td>
</tr>
<tr>
<td>VQ-VAE Indices</td>
<td>24.53</td>
<td>17.74</td>
</tr>
<tr>
<td>VQ-VAE Codebook</td>
<td>23.73</td>
<td>18.80</td>
</tr>
<tr>
<td>SysBinder</td>
<td>79.60</td>
<td>70.09</td>
</tr>
<tr>
<td>NLoTM Indices</td>
<td>68.21</td>
<td>64.53</td>
</tr>
<tr>
<td>NLoTM Codebook</td>
<td>75.86</td>
<td>71.15</td>
</tr>
</tbody>
</table>
<p>increases the potential for the downstream network to learn spurious correlations in the data, which can negatively impact OOD performance. NLoTM Indices is also only able to partially solve the task. This makes sense because in the out-of-distribution case, the model does not have any way of knowing two codebook indices are for the same property value (e.g., if two codebook vectors both correspond to the color blue). Since NLoTM Codebook uses the prototype vectors, it does not have this problem because the similarity can be determined by the vector representation.</p>
<h3>5.2.2 CLEVR-HARD PROPERTY COMPARISON</h3>
<p>For CLEVR-Hard, we construct a downstream task that assigns a number to each image as follows: First, we assign a number for each possible shape, color, and material present in the dataset. Then, for a given image, we identify the maximum number for each of these three properties. Lastly, we sum the max numbers for each of the properties to arrive at one integer label per image. We formulate the problem as a classification problem to correctly identify the number for each image. For example, suppose we have a scene containing a matte red cylinder and a shiny blue sphere. Assume we assign the following numbers to the different property values: matte = 0, shiny = 1, red = 5, blue = 3, cylinder = 4, sphere = 6. Thus the two objects are represented by the numbers (0, 5, 4) and (1, 3, 6). The max numbers for each of the properties is (1, 5, 6) and the final integer label is 1 + 5 + 6 = 12. Solving this task requires understanding the property values of each object in the scene.</p>
<p>We train the underlying models on the entire dataset consisting of all the possible property values. Then we randomly select 50 objects for an OOD dataset. Since our task relies on knowing the numerical value of each property, the ID dataset we train on may still contain property values of objects in the out-of-distribution dataset, but it will not contain objects where the combination of property values is present in the OOD dataset. Thus, when evaluating on the OOD dataset, we are testing the model on novel <em>combinations</em> of property values, even if those property values were individually observed during training. We show the ID and OOD results in Table 5. We see that SVQ outperforms the patch-based methods and performs comparably to SysBinder in both ID and OOD settings. This shows that despite adding a discretization bottleneck, the latents in SVQ are still useful for downstream tasks that rely on the properties of the objects in the scene.</p>
<h2>6 CONCLUSION</h2>
<p>In this paper, we introduce the Neural Language of Thought Model (NLoTM). This is the first model that satisfies our proposed desiderata of a neural network model of the LoTH: (1) Compositional semantic representation, (2) Symbol-like discrete concept abstraction, (3) Probabilistic neural grammar to generate samples compositionally. The NLoTM consists of two main components: Semantic Vector Quantized Variational Autoencoder, which learns hierarchical and composable discrete representations aligned with objects and their properties; and Autoregressive Language of Thought Prior, which learns to generate semantic concept tokens compositionally in a probabilistic manner. We evaluated NLoTM on several 2D and 3D image datasets. It demonstrated superior performance in downstream tasks, out-of-distribution generalization, and image generation quality compared to patch-based VQ-VAE and continuous object-centric representations. By further developing and refining models like NLoTM, we believe that we can continue to make progress in creating AI systems that exhibit more human-like understanding and generalization capabilities.</p>
<h1>ETHICS STATEMENT</h1>
<p>The scope of our study was restricted to visually simple, procedurally generated scenes and in its current form does not pose any immediate ethical concerns. Future work, however, that extends the capabilities of our model to work on more complex scenes may have the potential to generate fake, realistic-looking images. The semantic discrete latent may allow users to control scenes in ways that were not previously explored. While this may serve to enhance productivity, such as for artists and graphic designers, it could also be used maliciously in the hands of a bad actor. Future researchers pursuing this direction should do so under strong ethical standards and be cognizant of the potential misuse of this technology.</p>
<h2>REPRODUCIbILITY STATEMENT</h2>
<p>In addition to details about our model described in Section 3.1, we provide additional implementation details in Appendix C, including detailed hyperparameters used in our experiments. We will also release the source code upon acceptance of the paper.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This work is supported by Brain Pool Plus Program (No. 2021H1D3A2A03103645) and Young Researcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT. We thank Gautam Singh for insightful discussions and help with the CLEVR datasets. We also thank Sjoerd van Steenkiste for valuable feedback on an earlier draft of this paper.</p>
<h2>REFERENCES</h2>
<p>Titas Anciukevicius, Christoph H Lampert, and Paul Henderson. Object-centric image generation with factored depths, locations, and appearances. arXiv preprint arXiv:2004.00642, 2020.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 39-48, 2016.</p>
<p>Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in neural information processing systems, 13, 2000.</p>
<p>Yoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL http://arxiv.org/abs/1308.3432.</p>
<p>Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv preprint arXiv:1901.11390, 2019.</p>
<p>Chang Chen, Fei Deng, and Sungjin Ahn. ROOTS: Object-centric representation and rendering of 3D scenes. Journal of Machine Learning Research, 22(259):1-36, 2021. URL http://jmlr. org/papers/v22/20-1176.html.</p>
<p>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.</p>
<p>M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.</p>
<p>Eric Crawford and Joelle Pineau. Exploiting spatial invariance for scalable unsupervised object tracking. arXiv preprint arXiv:1911.09033, 2019a.</p>
<p>Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neural networks. In Proceedings of AAAI, 2019b.</p>
<p>Fei Deng, Zhuo Zhi, Donghun Lee, and Sungjin Ahn. Generative scene graph networks. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=RmcPm9m3tnk.</p>
<p>Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A generative model for music. CoRR, abs/2005.00341, 2020. URL https://arxiv. org/abs/2005.00341.</p>
<p>Andrea Dittadi, Samuele S. Papa, Michele De Vita, Bernhard Schölkopf, Ole Winther, and Francesco Locatello. Generalization and robustness implications in object-centric learning. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 5221-5285. PMLR, 2022. URL https://proceedings.mlr.press/v162/dittadi22a.html.</p>
<p>Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items, 2022. URL https://arxiv.org/abs/2204.11918.</p>
<p>Yilun Du, Kevin Smith, Tomer Ulman, Joshua Tenenbaum, and Jiajun Wu. Unsupervised discovery of 3d physical objects from video, 2021.</p>
<p>Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: generative scene inference and sampling with object-centric latent representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=BkxfaTVFwH.</p>
<p>Martin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object representations without iterative refinement, 2022.</p>
<p>SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, and Geoffrey E Hinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in Neural Information Processing Systems, pp. 3225-3233, 2016.</p>
<p>Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pp. 12873-12883. Computer Vision Foundation / IEEE, 2021. doi: 10.1109/CVPR46437.2021.01268. URL https://openaccess.thecvf.com/content/ CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_ Image_Synthesis_CVPR_2021_paper.html.</p>
<p>Jerry A Fodor et al. The language of thought, volume 5. Harvard university press Cambridge, MA, 1975.</p>
<p>Noah D Goodman, Joshua B Tenenbaum, and Tobias Gerstenberg. Concepts in a probabilistic language of thought. 2015.</p>
<p>Anand Gopalakrishnan, Kazuki Irie, Jürgen Schmidhuber, and Sjoerd van Steenkiste. Unsupervised learning of temporal abstractions with slot-based transformers. arXiv preprint arXiv:2203.13573, 2022.</p>
<p>Robert M. Gray. Vector quantization. IEEE ASSP Magazine, 1:4-29, 1984. URL https://api. semanticscholar.org/CorpusID:14754287.</p>
<p>Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization. In Advances in Neural Information Processing Systems, pp. 6691-6701, 2017.</p>
<p>Klaus Greff, Raphaël Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning with iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.</p>
<p>Klaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. On the binding problem in artificial neural networks. arXiv preprint arXiv:2012.05208, 2020.</p>
<p>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Olivier J Hénaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, João Carreira, and Relja Arandjelović. Object discovery and representation networks. In ECCV, pp. 123-143. Springer, 2022.</p>
<p>Jindong Jiang and Sungjin Ahn. Generative neurosymbolic machines. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Jindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world models with scalable object representations. In International Conference on Learning Representations, 2019 .</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2901-2910, 2017.</p>
<p>Rishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matthew Botvinick, Alexander Lerchner, and Christopher P. Burgess. Simone: View-invariant, temporally-abstracted object representations via unsupervised video decomposition. arXiv preprint arXiv:2106.03849, 2021.</p>
<p>Laurynas Karazija, Iro Laina, and Christian Rupprecht. Clevrtex: A texture-rich benchmark for unsupervised multi-object segmentation. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https: //datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ e2c420d928d4bf8ce0ff2ec19b371514-Abstract-round2.html.</p>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.</p>
<p>Thomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg Heigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional Object-Centric Learning from Video. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Daniil Kirilenko, Alexandr Korchemnyi, Alexey Kovalev, and Aleksandr Panov. Quantized disentangled representations for object-centric visual tasks, 2023. URL https://openreview.net/ forum?id=JIptuwnqwn.</p>
<p>Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machines that learn and think like people. CoRR, abs/1604.00289, 2016. URL http: / /arxiv. org/abs/1604.00289.</p>
<p>Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Jindong Jiang, and Sungjin Ahn. Improving generative imagination in object-centric world models. In International Conference on Machine Learning, pp. 4114-4124, 2020a.</p>
<p>Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial attention and decomposition. In International Conference on Learning Representations, 2020b.</p>
<p>Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention, 2020.</p>
<p>Marcelo G Mattar and Máté Lengyel. Planning in the brain. Neuron, 110(6):914-934, 2022.
Stephen E. Palmer. Hierarchical structure in perceptual representation. Cognitive Psychology, 9(4):441-474, 1977. ISSN 0010-0285. doi: https://doi.org/10.1016/0010-0285(77) 90016-0. URL https://www.sciencedirect.com/science/article/pii/ 0010028577900160 .</p>
<p>Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 8821-8831. PMLR, 2021. URL http: //proceedings.mlr.press/v139/ramesh21a.html.</p>
<p>Ali Razavi, Aäron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 14837-14847, 2019. URL https://proceedings.neurips.cc/paper/ 2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html.</p>
<p>Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-Gabriel, Tong He, Zheng Zhang, Bernhard Scholkopf, Thomas Brox, and Francesco Locatello. Bridging the gap to real-world object-centric learning. arXiv preprint arXiv:2209.14860, 2022.</p>
<p>Wolf Singer. Binding by synchrony. Scholarpedia, 2:1657, 2007.
Gautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-E learns to compose. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https://openreview.net/forum?id=h0OYV0We3oh.</p>
<p>Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised objectcentric learning for complex and naturalistic videos. In NeurIPS, 2022b. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 735c847a07bf6dd4486ca1ace242a88c-Abstract-Conference.html.</p>
<p>Gautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural systematic binder. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=ZPHE4fht19t.</p>
<p>Elizabeth S Spelke. What babies know: Core knowledge and composition volume 1, volume 1. Oxford University Press, 2022.</p>
<p>Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89-96, 2007.</p>
<p>Yao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov. Capsules with inverted dot-product attention routing. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=HJe6uANtwH.</p>
<p>Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in neural information processing systems, pp. 4790-4798, 2016.</p>
<p>Aäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 6306-6315, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ 7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attentionobject-centric image generation is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017.</p>
<p>Julius von Kügelgen, Ivan Ustyuzhaninov, Peter Gehler, Matthias Bethge, and Bernhard Schölkopf. Towards causal generative scene models via competition of experts, 2020.</p>
<p>Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation, 2023a.</p>
<p>Yanbo Wang, Letao Liu, and Justin Dauwels. Slot-vae: Object-centric scene generation with slot attention. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 36020-36035. PMLR, 2023b. URL https://proceedings.mlr.press/v202/wang23r.html.</p>
<p>Nicholas Watters, Loic Matthey, Sebastian Borgeaud, Rishabh Kabra, and Alexander Lerchner. Spriteworld: A flexible, configurable reinforcement learning environment. https://github.com/deepmind/spriteworld/, 2019a. URL https://github.com/deepmind/ spriteworld/.</p>
<p>Nicholas Watters, Loïc Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. CoRR, abs/1901.07017, 2019b. URL http://arxiv.org/abs/1901.07017.</p>
<p>Taylor W. Webb, Shanka Subhra Mondal, and Jonathan D. Cohen. Systematic visual reasoning through object-centric relational abstraction. CoRR, abs/2306.02500, 2023a. doi: 10.48550/arXiv. 2306.02500. URL https://doi.org/10.48550/arXiv.2306.02500.</p>
<p>Taylor W. Webb, Shanka Subhra Mondal, and Jonathan D. Cohen. Systematic visual reasoning through object-centric relational abstraction. CoRR, abs/2306.02500, 2023b. doi: 10.48550/arXiv. 2306.02500. URL https://doi.org/10.48550/arXiv.2306.02500.</p>
<p>Xin Wen, Bingchen Zhao, Anlin Zheng, X. Zhang, and Xiaojuan Qi. Self-supervised visual representation learning with semantic grouping. arXiv preprint arXiv:2205.15288, 2022.</p>
<p>Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Generative video transformer: Can objects be the words? In International Conference on Machine Learning, pp. 11307-11318. PMLR, 2021.</p>
<p>Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021. URL https://arxiv.org/abs/ 2104.10157.</p>
<p>Jaesik Yoon, Yi-Fu Wu, Heechul Bae, and Sungjin Ahn. An investigation into pre-training objectcentric representations for reinforcement learning. CoRR, abs/2302.04419, 2023. doi: 10.48550/ arXiv.2302.04419. URL https://doi.org/10.48550/arXiv.2302.04419.</p>
<p>Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= pfNyExj7z2.</p>
<p>Ruixiang Zhang, Tong Che, B. Ivanovic, Renhao Wang, Marco Pavone, Yoshua Bengio, and Liam Paull. Robust and controllable object-centric learning through energy-based models. arXiv preprint arXiv:2210.05519, 2022.</p>
<p>Daniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised segmentation with slots, attention and independence maximization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10439-10447, 2021.</p>
<h1>A LIMITATIONS</h1>
<p>While our method can learn semantic discrete representations and is capable of using these representations to generate images of higher visual fidelity than previous object-centric methods such as GENESIS (Engelcke et al., 2020; 2022), it is still only shown to work well on synthetic datasets with similar visual complexity as previous work (Singh et al., 2023). Although scaling unsupervised object-centric models to more realistic datasets is not a focus of this work, further improving our model so that it can work well on more realistic scenes is an important avenue of future research. Another limitation of our model is that our latent representations are all discrete. Although our visual world does consist of many discrete concepts, factors such as position and pose are continuous. It would be interesting to explore ways to combine continuous and discrete factors to better model realistic scenes.</p>
<h2>B ADDITIONAL EXPERIMENTAL RESULTS</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Sample scene we use in our codebook analysis.</p>
<h2>B. 1 CODEBOOK ANALYSIS</h2>
<p>Latent Traversal. In this section, we qualitatively analyze the codebook for a sample scene. Figure 5 shows the sample we will use in our analysis. First, we run the image through the pretrained SVQ encoder to obtain a set of semantic discrete latents. Each latent represents one block from one slot and is provided by a prototype vector in the corresponding codebook for that block. To investigate the effect of traversing through the codebook, we replace each block with a different code in the codebook while keeping all other latents fixed. We then reconstruct the scene with the SVQ decoder and dVAE, essentially generating a new image that only differs from the original image by one discrete latent.</p>
<p>Figure 6 shows the results for several sample blocks for the first slot (which corresponds to the teal ball) and the fourth slot (which corresponds to the gray cylinder). For each block, we choose the same set of 16 prototype vectors to display. First, we see that the slots are disentangled at the object level-changing one block in one slot does not affect the other objects. We also see that the different blocks specialize in different factors. Block 1 corresponds to the left and right placement of the object. Block 3 also corresponds to the placement of the object, but seems to also control the forward and backward placement of the object, as well as the size of the object. We notice that in this particular case, the factors of position and size are not completely disentangled. This may be because in this scene, the size depends on the placement of the object (e.g. closer objects are bigger). Block 7 controls the color of the object. We see that the same prototype vector seems to produce the same color, although there are some inconsistencies such as the disappearing cylinder in the bottom left. The color also seems to be cleanly disentangled from the other factors-changing the color does not affect other factors like shape, size, or position.
Block Analysis. Next, to further explore the representation captured in the codebook, we visualize the objects that are attended to for different prototype vectors. To achieve this, we run the pretrained SVQ on 1000 images obtaining the semantic discrete latents and slot attention segmentation maps for the objects in the images. Then, for each prototype vector in the codebook, we find and visualize the corresponding slots that are utilizing that code in one of its blocks. Note that unlike Singh et al. (2023), we do not need to do any k-means clustering to obtain this visualization since our representations are discrete representations in the codebook. Figures 7 and 8 show sample objects corresponding to three different prototype vectors for block 3 and block 7 . We see that block 3 corresponds to object size and block 7 corresponds to object color. These results are consistent with the previous latent traversal</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Latent traversal changing one latent in one block at a time while keeping all other latents fixed. The image is then reconstructed with the single changed latent.
experiments. Furthermore, the three prototype vectors we chose for block 7 correspond with the first three latents in Figure 6 (right), showing that these three prototype vectors represent gray, purple, and teal, respectively.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Objects attended to when the latent for block 3 is set to three different prototype vectors.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Objects attended to when the latent for block 7 is set to three different prototype vectors.</p>
<h1>B. 2 COMPARISON WITH Slot-Level Quantization</h1>
<p>As discussed in Section 3.1, we hypothesize that slot-level discretization would struggle with complex scenes due to the combinatorial nature of the underlying factors of the objects. We test this hypothesis by running experiments on 2D Sprites and CLEVR-Easy where we set the number of blocks $M$ to 1 and tune the size of the codebook, essentially doing slot-level quantization. In Figures 9, we show the masked attention of each slot on the input image as well as the image reconstruction. We find that with slot-level quantization, the model completely fails on the CLEVR-Easy dataset, unable to cleanly attend to the objects and reconstruct the image. On the 2D sprites dataset, we see that with slot discretization, one slot ends up attending to all the foreground objects and the model still cannot reconstruct the input image correctly. These results point to the importance of our choice to do block-level discretization.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Comparison of slot discretization and block discretization on CLEVR-Easy (top) and 2D sprites (bottom).</p>
<h2>B. 3 Prior Model Capacity for dVAE</h2>
<p>In order to evaluate whether or not a larger capacity prior may improve the results for the patch-based dVAE baseline, we ran ablations using larger transformers for the dVAE prior on the CLEVR-Hard dataset. The results are presented in Table 6. We see that while increasing the size of the transformer for the prior does slightly improve the FID for dVAE, it still underperforms when compared to NLoTM, indicating that simply scaling the dVAE prior may not be sufficient to match NLoTM performance.</p>
<p>Table 6: Effect of increasing the dVAE prior model capacity on FID on the CLEVR-Hard dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prior Model</th>
<th style="text-align: center;">FID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">dVAE (8-layer)</td>
<td style="text-align: center;">65.89</td>
</tr>
<tr>
<td style="text-align: left;">dVAE (12-layer)</td>
<td style="text-align: center;">61.74</td>
</tr>
<tr>
<td style="text-align: left;">dVAE (16-layer)</td>
<td style="text-align: center;">60.75</td>
</tr>
<tr>
<td style="text-align: left;">NLoTM (8-layer)</td>
<td style="text-align: center;">43.12</td>
</tr>
</tbody>
</table>
<h2>B. 4 NUMBER OF BLOCKS Ablation</h2>
<p>Table 7 shows the results of changing the number of blocks in SVQ on the 2D Sprites (3 obj) dataset. We see that when the number of blocks is too small, the model performs poorly and fails to generate</p>
<p>scenes corresponding to the data distribution. For a sufficiently large number of blocks, the model is able to segment the scene, but Generation Accuracy decreases when the number of blocks is too large. We suspect this to be because with more blocks, the model may require a higher capacity prior, which we kept fixed in this ablation.</p>
<p>Table 7: Effect of changing the number of blocks in SVQ on the 2D Sprites (3 obj) dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Number of Blocks</th>
<th style="text-align: center;">FID</th>
<th style="text-align: center;">Generation Accuracy (in \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">465.60</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">80.71</td>
<td style="text-align: center;">1.56</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">7.76</td>
<td style="text-align: center;">75.78</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">75.00</td>
</tr>
<tr>
<td style="text-align: left;">16</td>
<td style="text-align: center;">7.17</td>
<td style="text-align: center;">55.47</td>
</tr>
<tr>
<td style="text-align: left;">32</td>
<td style="text-align: center;">8.74</td>
<td style="text-align: center;">54.69</td>
</tr>
</tbody>
</table>
<h1>B. 5 Codebook Size Ablation</h1>
<p>Table 8 shows the results of changing the codebook size in SVQ on the 2D Sprites (3 obj) dataset. We had also tried smaller codebook sizes of 4 and 16, but found that for codebook sizes smaller than 32, the SVQ model did not converge well, resulting in black reconstructions. Similar to the number of blocks ablations, we see that for larger codebook sizes, the FID scores are similar, but generation accuracy decreases for codebook sizes larger than 64 . This again may be because the larger codebook sizes require a higher capacity prior, which was fixed in these ablations.</p>
<p>Table 8: Effect of changing the codebook size in SVQ on the 2D Sprites (3 obj) dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Codebook Size</th>
<th style="text-align: center;">FID</th>
<th style="text-align: center;">Generation Accuracy (in \%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">32</td>
<td style="text-align: center;">7.31</td>
<td style="text-align: center;">76.56</td>
</tr>
<tr>
<td style="text-align: left;">64</td>
<td style="text-align: center;">6.61</td>
<td style="text-align: center;">75.00</td>
</tr>
<tr>
<td style="text-align: left;">96</td>
<td style="text-align: center;">6.64</td>
<td style="text-align: center;">51.56</td>
</tr>
<tr>
<td style="text-align: left;">128</td>
<td style="text-align: center;">7.58</td>
<td style="text-align: center;">48.44</td>
</tr>
<tr>
<td style="text-align: left;">256</td>
<td style="text-align: center;">8.73</td>
<td style="text-align: center;">32.81</td>
</tr>
</tbody>
</table>
<h2>B. 6 FG-ARI Segmentation Results</h2>
<p>Table 9 shows the Foreground Adjusted-Rand-Index (FG-ARI) metric for the CLEVR datasets for different slot-based models. We see that when compared to SysBinder, NLoTM performs similarly in terms of FG-ARI on CLEVR-Easy and CLEVR-Hard and slightly underperforms on CLEVR-Tex. Compared to vanilla Slot Attention, NLoTM achieves higher FG-ARI on all 3 datasets.</p>
<p>Table 9: FG-ARI results on CLEVR datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Slot Attention</th>
<th style="text-align: center;">SLATE</th>
<th style="text-align: center;">SysBinder</th>
<th style="text-align: center;">NLoTM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CLEVR</td>
<td style="text-align: center;">85.85</td>
<td style="text-align: center;">91.65</td>
<td style="text-align: center;">92.58</td>
<td style="text-align: center;">91.37</td>
</tr>
<tr>
<td style="text-align: left;">CLEVR-Hard</td>
<td style="text-align: center;">81.29</td>
<td style="text-align: center;">76.79</td>
<td style="text-align: center;">90.43</td>
<td style="text-align: center;">90.48</td>
</tr>
<tr>
<td style="text-align: left;">CLEVR-Tex</td>
<td style="text-align: center;">24.67</td>
<td style="text-align: center;">73.85</td>
<td style="text-align: center;">78.12</td>
<td style="text-align: center;">70.93</td>
</tr>
</tbody>
</table>
<h2>B. 7 EXPERIMENTS ON GOOGLE SCANNED ObJECTS</h2>
<p>In order to evaluate NLoTM on a more realistic dataset, we use a dataset where the objects are taken from the Google Scanned Objects (Downs et al., 2022). Specifically, we use the objects from the "Shoe" and "Bottles and Cans and Cups" categories. We evaluate on two versions of NLoTM: NLoTM-small uses the same hyperparameters as we used for the CLEVR-Hard dataset (see Table</p>
<p>11) and NLoTM-large increases the codebook size to 256 and increases the size of ALP model from 8 layers, 4 heads, model size 192 to 16 layers, 8 heads, model size 512.</p>
<p>We present the FID results in Table 10 and qualitative samples in Figure 10. We see that while NLoTM-small is able to generate objects from the dataset, the objects are smoothed out, resulting in a high FID score. Increasing the model size to NLoTM-large significantly improves the quality of the generated scenes and the FID score, providing some evidence that NLoTM can be scaled to work on more realistic datasets.</p>
<p>Table 10: FID for the Google Scanned Objects dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">FID</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NLoTM-small</td>
<td style="text-align: center;">114.16</td>
</tr>
<tr>
<td style="text-align: left;">NLoTM-large</td>
<td style="text-align: center;">72.68</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Samples for NLoTM on the Google Scanned Objects dataset. Scaling to a larger model size noticeably improves the quality of the samples.</p>
<h1>C IMPLEMENTATION DETAILS</h1>
<h2>C. 1 TRAINING AND IMPLEMENTATION DETAILS.</h2>
<p>We use input images of 64 x 64 resolution for the 2D Sprites datasets and 128x128 for the CLEVR datasets. Each model is trained on NVIDIA Quadro RTX 8000 GPUs with 48GB memory and we use half-precision floating-point format. We train SVQ for 400 k iterations which takes around 80 hours for the CLEVR datasets and 50 hours for the 2D datasets. We then train the ALP prior for 1 million iterations which takes around 40 hours. For the 2D Sprites dataset, similar to (Yoon et al., 2023), we first train the underlying models on a dataset of random shapes without any relationship between the objects. We then train the prior models on the odd-one-out datasets.</p>
<h2>C. 2 HYPERPARAMETERS</h2>
<p>Table 11 shows the hyperparameters we used for the different datasets in our experiments with SVQ. For the dVAE and Transformer Decoder, we follow the hyperparameters, architecture, and training procedure provided in Singh et al. (2023) for CLEVR-Easy and CLEVR-Hard. For the 2D Sprites datasets, we use the same hyperparameters as we do for CLEVR-Easy for those components. All models are trained with the Adam optimizer (Kingma \&amp; Ba, 2015) with $\beta_{1}=0.9$ and $\beta_{2}=0.999$.</p>
<h2>C. 3 PRIOR MODELS</h2>
<p>For the DVAE and ALP prior models, we use a transformer architecture with 8 layers, 4 heads, model dimension 192, feedforward dimension 768, and a dropout probability of 0.1 . We use a learning rate</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Module</td>
<td style="text-align: left;">Hyperparameter</td>
<td style="text-align: center;">CLEVR-Easy</td>
<td style="text-align: center;">CLEVR-Hard</td>
<td style="text-align: center;">2D Sprites</td>
<td style="text-align: center;">2D Sprites w/ BG</td>
</tr>
<tr>
<td style="text-align: left;">General</td>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">40</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Training Steps</td>
<td style="text-align: center;">400 K</td>
<td style="text-align: center;">400 K</td>
<td style="text-align: center;">400 K</td>
<td style="text-align: center;">400 K</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Image Size</td>
<td style="text-align: center;">$128 \times 128$</td>
<td style="text-align: center;">$128 \times 128$</td>
<td style="text-align: center;">$64 \times 64$</td>
<td style="text-align: center;">$64 \times 64$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Codebook Dimension</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"># Blocks</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Codebook Size</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"># Iterations</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"># Slots</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\beta$</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0001</td>
<td style="text-align: center;">0.0001</td>
</tr>
</tbody>
</table>
<p>Table 11: Hyperparameters of our model used in our experiments.
of 0.0003 and 30,000 warmup steps. For VQ-VAE, we use a 20-layers PixelCNN prior, as proposed in the original paper (van den Oord et al., 2017).</p>
<h1>C. 4 Downstream Models</h1>
<p>For the 2D Sprites downstream experiments, we use a transformer architecture with 3 layers, 8 heads, model dimension 192, feedforward dimension 768, and a dropout probability of 0.1 for all models. We use the Adam optimizer with a learning rate of 0.0003 .</p>
<p>For the CLEVR-Hard downstream experiments, we use a transformer architecture with 8 layers, 4 heads, model dimension 192, feedforward dimension 768, and a dropout probability of 0.1 for all models. We use the Adam optimizer with a learning rate of 0.0001 .</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Correspondence to sungjin.ahn@kaist.ac.kr&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
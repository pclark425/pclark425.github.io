<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8721 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8721</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8721</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-4780d0a027c5c5a8e01d7cf697f6296880ffc945</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4780d0a027c5c5a8e01d7cf697f6296880ffc945" target="_blank">Improving Factuality and Reasoning in Language Models through Multiagent Debate</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such"society of minds"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8721.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8721.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Single-Agent Reflection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Agent Reflection (self-reflect / self-critique baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single language model instance is asked to generate an answer and then to critique or update its own response (a generate-then-reflect step) before producing a final answer; used as a baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-3.5 series model (chatGPT family) used as a black-box via the OpenAI API; exact parameter count not specified in the paper; experiments report using gpt-3.5-turbo-0301.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Single-Agent Reflection (self-reflect / self-critique)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The model first generates an initial response then is prompted to critique and update its own response (a single generate-then-reflect step via prompt engineering). In the paper this is implemented as a single model being given its earlier output and asked to 'self-reflect' and provide an updated answer.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic; GSM8K (grade-school math); Chess move prediction; Biographies; MMLU; Chess move validity</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of reasoning and factuality tasks: arithmetic expressions, GSM8K grade-school math, chess move prediction (measured by Stockfish pawn score), factual biographies (bullet facts vs. Wikipedia ground truth), MMLU multiple-choice, and chess-move validity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Arithmetic: 72.1% ± 4.5; Grade School Math (GSM8K): 75.0% ± 4.3; Chess (pawn score Δ): 102.1 ± 11.9; Biographies: 68.3% ± 2.9; MMLU: 57.7% ± 5.0; Chess Move Validity: 38.8% ± 2.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Arithmetic: 67.0% ± 4.7; Grade School Math (GSM8K): 77.0% ± 4.2; Chess (pawn score Δ): 91.4 ± 10.6; Biographies: 66.0% ± 2.2; MMLU: 63.9% ± 4.8; Chess Move Validity: 29.3% ± 2.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt engineering: the model is given its earlier generation (self-output) as context and asked to 'reflect' or 'critique' and provide an updated answer; implemented as an extra generation step (black-box, no internal model state required). Prompts are listed in Appendix (e.g., 'Using the solutions from other agents...' adapted to self).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative comparisons in Table 1 & 2 show small improvements for some tasks (arithmetic: +5.1 pp; chess pawn score improved; biographies slightly improved) and declines for others (GSM8K -2.0 pp; MMLU -6.2 pp), showing mixed evidence that a single self-reflection step can help but is task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reflection gave only modest gains on some reasoning tasks and worsened performance on several factual knowledge tasks (notably MMLU and GSM8K saw declines). The paper notes reflection can be insufficient and that multi-agent debate outperforms single-agent reflection. No internal confidence calibration beyond prompting was used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against single-agent vanilla generation and multi-agent debate; reflection sometimes outperforms single-agent vanilla generation (e.g., arithmetic, chess) but is outperformed by multi-agent debate across tasks. The paper explicitly states reflection gives a modest boost while debate gives a substantial boost.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Factuality and Reasoning in Language Models through Multiagent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8721.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8721.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion / Self-Refine (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion; Self-Refine (iterative self-reflection approaches cited)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works that implement iterative self-improvement for language agents: 'Reflexion' (autonomous agent with dynamic memory and self-reflection) and 'Self-refine' (iterative refinement with self-feedback); these are cited as related work but not executed in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion / Self-refine (cited iterative self-reflection methods)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Cited approaches where an agent iteratively refines its own outputs using memory and self-feedback loops (the paper references these as related work; exact protocols and iteration counts are described in the original cited papers, not reproduced here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Mentioned as prior work: mechanisms include dynamic memory, internal self-feedback loops, and iterative updating; details are not implemented or evaluated in this paper (paper cites Shinn et al. 2023 and Madaan et al. 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>The paper cites these works as part of the literature on iterative self-reflection but does not present experimental evidence from them; instead it compares its own debate method to a single-agent reflection baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No failure cases or direct experimental comparisons for these cited methods are reported within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Cited alongside other single-agent prompting/verification techniques; the paper positions multi-agent debate as complementary and, in their experiments, superior to single-agent reflection baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Factuality and Reasoning in Language Models through Multiagent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8721.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8721.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Debate (iterative multi-agent reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (iterative consensus-building among multiple LM instances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, multi-agent procedure where multiple copies/instances of a language model independently propose answers, then repeatedly read and critique each other's responses over several rounds to arrive at a consensus final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo-0301 (ChatGPT); experiments also include mixed-model (ChatGPT + Bard) for a small set</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat models accessed as black-box generators (primary experiments use gpt-3.5-turbo-0301). The paper runs debates with multiple identical instances (3 agents typical) and also tests mixed-model debates (ChatGPT vs Bard) on a subset.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-Agent Debate (iterative multiagent self-reflection / consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Procedure: (1) N agents independently generate solutions; (2) concatenate (or summarize) other agents' responses and prompt each agent to 'update' its response taking others' opinions as input; (3) repeat for R rounds; after rounds, obtain final consensus (majority or unanimous). Typical experimental setup used 3 agents and 2 rounds; experiments vary agent count and rounds showing monotonic improvements up to a plateau (~4 rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arithmetic; GSM8K; Chess move prediction; Biographies; MMLU; Chess move validity</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same reasoning and factuality benchmarks described above: synthetic arithmetic, GSM8K grade school math, chess move prediction (Stockfish pawn advantage), biographies factuality vs Wikipedia, MMLU multiple-choice, and chess move validity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Multi-Agent (Debate) results reported: Arithmetic 81.8% ± 2.3; GSM8K 85.0% ± 3.5; Chess pawn score Δ 122.9 ± 7.6; Biographies 73.8% ± 2.3; MMLU 71.1% ± 4.6; Chess Move Validity 45.2% ± 2.9.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-Agent (vanilla) baselines: Arithmetic 67.0% ± 4.7; GSM8K 77.0% ± 4.2; Chess pawn score Δ 91.4 ± 10.6; Biographies 66.0% ± 2.2; MMLU 63.9% ± 4.8; Chess Move Validity 29.3% ± 2.6.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented via prompt engineering and multi-turn interaction between black-box model instances: other agents' outputs are concatenated (or summarized when many agents) and inserted into a 'consensus' prompt asking each agent to update its answer; variations include 'short' vs 'long' consensus prompts controlling stubbornness, and summarization to reduce context length.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>2</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative tables (Table 1 & 2) show substantial gains of debate over single-agent and single-agent reflection baselines across tasks (e.g., arithmetic +14.8 pp, GSM8K +8.0 pp, chess pawn score +31.5). Qualitatively, the paper shows examples where initially all agents were wrong but debate produced the correct answer, and reports monotonic gains with more agents and more rounds up to a plateau.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: higher computational cost (multiple agents and rounds); for long debates models often focus on the most recent content and ignore early parts (context-length/processing limits); debates sometimes converge to an incorrect consensus which models then confidently assert; some debates produced incorrect final answers (Figures and examples of incorrect debates are provided); longer debates can slow convergence and occasionally lead to poorer intermediate behavior despite better final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to single-agent vanilla generation, single-agent reflection, and multi-agent majority voting: debate outperformed single-agent and reflection baselines and also outperformed majority-vote in reasoning tasks. Debate synergizes with chain-of-thought prompting (combining chain-of-thought with debate further improves performance).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations: (1) Number of agents: increasing agents (with fixed 2 rounds) improved arithmetic performance monotonically; (2) Number of rounds: increasing rounds (with 3 agents) improved performance up to ~4 rounds then plateau; (3) Prompt variants: 'long' prompts (encouraging stubbornness) produce slower convergence but better final accuracy; (4) Summarization: summarizing many agents' responses before feeding to an agent improved performance and reduced context length issues; (5) Mixed-model debates (ChatGPT+Bard) on 20 GSM8K problems improved joint performance (17 solved vs 14 chatGPT alone and 11 Bard alone).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Factuality and Reasoning in Language Models through Multiagent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Ai safety via debate <em>(Rating: 2)</em></li>
                <li>Language models (mostly) know what they know <em>(Rating: 1)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8721",
    "paper_id": "paper-4780d0a027c5c5a8e01d7cf697f6296880ffc945",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Single-Agent Reflection",
            "name_full": "Single-Agent Reflection (self-reflect / self-critique baseline)",
            "brief_description": "A single language model instance is asked to generate an answer and then to critique or update its own response (a generate-then-reflect step) before producing a final answer; used as a baseline in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT)",
            "model_description": "Instruction-tuned GPT-3.5 series model (chatGPT family) used as a black-box via the OpenAI API; exact parameter count not specified in the paper; experiments report using gpt-3.5-turbo-0301.",
            "reflection_method_name": "Single-Agent Reflection (self-reflect / self-critique)",
            "reflection_method_description": "The model first generates an initial response then is prompted to critique and update its own response (a single generate-then-reflect step via prompt engineering). In the paper this is implemented as a single model being given its earlier output and asked to 'self-reflect' and provide an updated answer.",
            "task_name": "Arithmetic; GSM8K (grade-school math); Chess move prediction; Biographies; MMLU; Chess move validity",
            "task_description": "A set of reasoning and factuality tasks: arithmetic expressions, GSM8K grade-school math, chess move prediction (measured by Stockfish pawn score), factual biographies (bullet facts vs. Wikipedia ground truth), MMLU multiple-choice, and chess-move validity.",
            "performance_with_reflection": "Arithmetic: 72.1% ± 4.5; Grade School Math (GSM8K): 75.0% ± 4.3; Chess (pawn score Δ): 102.1 ± 11.9; Biographies: 68.3% ± 2.9; MMLU: 57.7% ± 5.0; Chess Move Validity: 38.8% ± 2.9.",
            "performance_without_reflection": "Arithmetic: 67.0% ± 4.7; Grade School Math (GSM8K): 77.0% ± 4.2; Chess (pawn score Δ): 91.4 ± 10.6; Biographies: 66.0% ± 2.2; MMLU: 63.9% ± 4.8; Chess Move Validity: 29.3% ± 2.6.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt engineering: the model is given its earlier generation (self-output) as context and asked to 'reflect' or 'critique' and provide an updated answer; implemented as an extra generation step (black-box, no internal model state required). Prompts are listed in Appendix (e.g., 'Using the solutions from other agents...' adapted to self).",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative comparisons in Table 1 & 2 show small improvements for some tasks (arithmetic: +5.1 pp; chess pawn score improved; biographies slightly improved) and declines for others (GSM8K -2.0 pp; MMLU -6.2 pp), showing mixed evidence that a single self-reflection step can help but is task-dependent.",
            "limitations_or_failure_cases": "Reflection gave only modest gains on some reasoning tasks and worsened performance on several factual knowledge tasks (notably MMLU and GSM8K saw declines). The paper notes reflection can be insufficient and that multi-agent debate outperforms single-agent reflection. No internal confidence calibration beyond prompting was used.",
            "comparison_to_other_methods": "Compared against single-agent vanilla generation and multi-agent debate; reflection sometimes outperforms single-agent vanilla generation (e.g., arithmetic, chess) but is outperformed by multi-agent debate across tasks. The paper explicitly states reflection gives a modest boost while debate gives a substantial boost.",
            "ablation_study_results": null,
            "uuid": "e8721.0",
            "source_info": {
                "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Reflexion / Self-Refine (cited)",
            "name_full": "Reflexion; Self-Refine (iterative self-reflection approaches cited)",
            "brief_description": "Prior works that implement iterative self-improvement for language agents: 'Reflexion' (autonomous agent with dynamic memory and self-reflection) and 'Self-refine' (iterative refinement with self-feedback); these are cited as related work but not executed in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Reflexion / Self-refine (cited iterative self-reflection methods)",
            "reflection_method_description": "Cited approaches where an agent iteratively refines its own outputs using memory and self-feedback loops (the paper references these as related work; exact protocols and iteration counts are described in the original cited papers, not reproduced here).",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Mentioned as prior work: mechanisms include dynamic memory, internal self-feedback loops, and iterative updating; details are not implemented or evaluated in this paper (paper cites Shinn et al. 2023 and Madaan et al. 2023).",
            "number_of_iterations": null,
            "evidence_for_improvement": "The paper cites these works as part of the literature on iterative self-reflection but does not present experimental evidence from them; instead it compares its own debate method to a single-agent reflection baseline.",
            "limitations_or_failure_cases": "No failure cases or direct experimental comparisons for these cited methods are reported within this paper.",
            "comparison_to_other_methods": "Cited alongside other single-agent prompting/verification techniques; the paper positions multi-agent debate as complementary and, in their experiments, superior to single-agent reflection baselines.",
            "ablation_study_results": null,
            "uuid": "e8721.1",
            "source_info": {
                "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Multi-Agent Debate (iterative multi-agent reflection)",
            "name_full": "Multi-Agent Debate (iterative consensus-building among multiple LM instances)",
            "brief_description": "An iterative, multi-agent procedure where multiple copies/instances of a language model independently propose answers, then repeatedly read and critique each other's responses over several rounds to arrive at a consensus final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo-0301 (ChatGPT); experiments also include mixed-model (ChatGPT + Bard) for a small set",
            "model_description": "Instruction-tuned chat models accessed as black-box generators (primary experiments use gpt-3.5-turbo-0301). The paper runs debates with multiple identical instances (3 agents typical) and also tests mixed-model debates (ChatGPT vs Bard) on a subset.",
            "reflection_method_name": "Multi-Agent Debate (iterative multiagent self-reflection / consensus)",
            "reflection_method_description": "Procedure: (1) N agents independently generate solutions; (2) concatenate (or summarize) other agents' responses and prompt each agent to 'update' its response taking others' opinions as input; (3) repeat for R rounds; after rounds, obtain final consensus (majority or unanimous). Typical experimental setup used 3 agents and 2 rounds; experiments vary agent count and rounds showing monotonic improvements up to a plateau (~4 rounds).",
            "task_name": "Arithmetic; GSM8K; Chess move prediction; Biographies; MMLU; Chess move validity",
            "task_description": "Same reasoning and factuality benchmarks described above: synthetic arithmetic, GSM8K grade school math, chess move prediction (Stockfish pawn advantage), biographies factuality vs Wikipedia, MMLU multiple-choice, and chess move validity.",
            "performance_with_reflection": "Multi-Agent (Debate) results reported: Arithmetic 81.8% ± 2.3; GSM8K 85.0% ± 3.5; Chess pawn score Δ 122.9 ± 7.6; Biographies 73.8% ± 2.3; MMLU 71.1% ± 4.6; Chess Move Validity 45.2% ± 2.9.",
            "performance_without_reflection": "Single-Agent (vanilla) baselines: Arithmetic 67.0% ± 4.7; GSM8K 77.0% ± 4.2; Chess pawn score Δ 91.4 ± 10.6; Biographies 66.0% ± 2.2; MMLU 63.9% ± 4.8; Chess Move Validity 29.3% ± 2.6.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented via prompt engineering and multi-turn interaction between black-box model instances: other agents' outputs are concatenated (or summarized when many agents) and inserted into a 'consensus' prompt asking each agent to update its answer; variations include 'short' vs 'long' consensus prompts controlling stubbornness, and summarization to reduce context length.",
            "number_of_iterations": 2,
            "evidence_for_improvement": "Quantitative tables (Table 1 & 2) show substantial gains of debate over single-agent and single-agent reflection baselines across tasks (e.g., arithmetic +14.8 pp, GSM8K +8.0 pp, chess pawn score +31.5). Qualitatively, the paper shows examples where initially all agents were wrong but debate produced the correct answer, and reports monotonic gains with more agents and more rounds up to a plateau.",
            "limitations_or_failure_cases": "Reported limitations: higher computational cost (multiple agents and rounds); for long debates models often focus on the most recent content and ignore early parts (context-length/processing limits); debates sometimes converge to an incorrect consensus which models then confidently assert; some debates produced incorrect final answers (Figures and examples of incorrect debates are provided); longer debates can slow convergence and occasionally lead to poorer intermediate behavior despite better final answers.",
            "comparison_to_other_methods": "Compared to single-agent vanilla generation, single-agent reflection, and multi-agent majority voting: debate outperformed single-agent and reflection baselines and also outperformed majority-vote in reasoning tasks. Debate synergizes with chain-of-thought prompting (combining chain-of-thought with debate further improves performance).",
            "ablation_study_results": "Ablations: (1) Number of agents: increasing agents (with fixed 2 rounds) improved arithmetic performance monotonically; (2) Number of rounds: increasing rounds (with 3 agents) improved performance up to ~4 rounds then plateau; (3) Prompt variants: 'long' prompts (encouraging stubbornness) produce slower convergence but better final accuracy; (4) Summarization: summarizing many agents' responses before feeding to an agent improved performance and reduced context length issues; (5) Mixed-model debates (ChatGPT+Bard) on 20 GSM8K problems improved joint performance (17 solved vs 14 chatGPT alone and 11 Bard alone).",
            "uuid": "e8721.2",
            "source_info": {
                "paper_title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Ai safety via debate",
            "rating": 2,
            "sanitized_title": "ai_safety_via_debate"
        },
        {
            "paper_title": "Language models (mostly) know what they know",
            "rating": 1,
            "sanitized_title": "language_models_mostly_know_what_they_know"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 1,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        }
    ],
    "cost": 0.013901499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Improving Factuality and Reasoning in Language Models through Multiagent Debate</h1>
<p>Yilun Du<br>MIT CSAIL<br>yilundu@mit.edu</p>
<p>Shuang Li<br>MIT CSAIL<br>lishuang@mit.edu</p>
<p>Antonio Torralba<br>MIT CSAIL<br>torralba@mit.edu</p>
<p>Joshua B. Tenenbaum<br>MIT CSAIL, BCS, CBMM<br>jbt@mit.edu</p>
<p>Igor Mordatch<br>Google Brain<br>imordatch@google.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such "society of minds" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding. Project website at https://composable-models.github.io/llm_debate/.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have demonstrated remarkable language generation, understanding, and few-shot learning capabilities in recent years. These methods are trained on a massive corpus of text on the internet, where the quality and accuracy of extracted natural language may not be ensured. Thus, current models may suffer from confidently hallucinating facts or making implausible jumps in chains of reasoning. An extensive body of recent work has focused on improving factual accuracy and reasoning in language models. These range from prompting models with few or zero-shot chain-of-thought demonstrations, use of verification, self-consistency, or intermediate scratchpads.
We note that these techniques are applied over a single model instance. Instead, we propose a complementary approach inspired by The Society of Mind [19] and multi-agent settings, where multiple language model instances (or agents) individually propose and jointly debate their responses and reasoning processes to arrive at a single common answer. More specifically, given a query, multiple instances of a language model first generate individual candidate answers to a query. Then each individual model instance reads and critiques the responses of all other models and uses this content to update its own answer. This step is then repeated over several rounds. This process induces models to construct answers that are consistent with both their internal critic as well as sensible in</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Multiagent Debate Improves Reasoning and Factual Accuracy. Accuracy of traditional inference and our multi-agent debate over six benchmarks (chess move optimality reported as a normalized score)
light of the responses of other agents. The resulting quorum of models can hold and maintain multiple chains of reasoning and possible answers simultaneously before proposing the final answer.</p>
<p>We find that our debate approach outperforms single model baselines such as zero-shot chain of thought [11] and reflection [26, 18] on a variety of six reasoning, factuality, and question-answering tasks. Using both multiple model agents and multiple rounds of debate are important to achieve the best performance. Given an initial query, we find that individual model instances propose a diverse range of answers despite being the same model class (although we also investigate the case of mixing different model types, such as chatGPT [21] and Bard [23]). After debating and examining the responses of other model instances, we find that the population almost always converges on a single and more accurate common answer. Debate results are also less likely to include false facts that models are internally uncertain of. This is because as the debate progresses, individual model instances tend to disagree on uncertain facts and omit them from the answer (Figure 7). Lastly, we find that debate does not just act to amplify one correct answer in a model quorum - we find many cases where all the models initially make incorrect predictions, but then arrive at the correct answer as debate progresses (Figure 4,11).</p>
<p>We use the same methodology and prompt templates for all our tasks and require only black-box access to language model generations - no model-internal information such as likelihoods or gradients is needed. This allows our method to be used with common public models serving interfaces. The method is also orthogonal to other model generation improvements such as retrieval or prompt engineering (in fact, we combine our debate method with zero-shot chain of thought). While the debate process is more costly, requiring multiple model instances and rounds, it arrives at significantly improved answers and may be used to generate additional model training data, effectively creating a model self-improvement loop.</p>
<p>To help evaluate the effect of our approach on factual accuracy, we introduce a new benchmark and dataset evaluating factual accuracy of famous computer scientist biographies. We find that contemporary language models have an especially high tendency to hallucinate factually incorrect biographies, often misrepresenting the relevant institutions and dates. Moreover, these facts often inconsistent across different language model instances. By asking models to come to a consensus across their answers, such inconsistent facts may be either removed or corrected.</p>
<p>In summary, our work contributes the following. First, we present a novel approach to improving factual correctness and reasoning accuracy in contemporary language models, leveraging a multiagent debate process between models. Second, we introduce a new benchmark of factual correctness which contemporary language models struggle with. Finally, we evaluate the performance of our debate procedure in language generation, both in terms of the number of agents, the underlying rounds of debate, and the prompts that elicit such behavior across a set of six different reasoning and factual accuracy tasks.</p>
<h1>2 Language Generation through Multiagent Debate</h1>
<p>We present an approach to generate language responses through multiagent debate. We provide an overview of our approach in Section 2.1. We further discuss convergence to consensus in the debate process in Section 2.2. The overall overview of our approach is shown in Figure 2.</p>
<p>User Input: A treasure hunter found a buried treasure chest filled with gems. There were 175 diamonds, 35 fewer rubies than diamonds, and twice the number of emeralds than the rubies. How many of the gems were there in the chest?</p>
<h1>Round 1</h1>
<p>Agent 1: Let $x$ be the number of rubies. Then the number of diamonds is 175. And the number of emeralds is $2(x-35)$, since there are twice as many emeralds as rubies and there are 35 fewer rubies than diamonds.</p>
<p>To find the total number of gems, we add the number of diamonds, rubies, and emeralds: Total gems $=175+x+2(x-35)=3 x+105$.</p>
<p>We don't know the exact value of $x$, but we do know that it represents the number of rubies, which is less than 175 (since there are 35 fewer rubies than diamonds). Let's try some values of $x$. If $x=50$, then the total gems $=3(50)+$ $105=255$. If $x=100$, then the total gems $=3(100)+105=405$.</p>
<p>Since we know there were fewer rubies than diamonds, the answer must be closer to 255 than to 405 . After some trial and error, we can find that $x=40$ gives us: Total gems $=3(40)+105=225$.</p>
<p>Agent 2: If there were 175 diamonds, then the number of rubies is 35 fewer, or $175-35=140$ rubies.</p>
<p>And since there were twice as many emeralds as rubies, we can represent the number of emeralds as $2 * 140=280$ emeralds.</p>
<p>To find the total number of gems in the chest, we can add up the number of diamonds, rubies, and emeralds: $175+140+280=595$. Therefore, there were 595 gems in the chest.</p>
<p>User Input: Using the solutions from other agents as additional information, can you give an updated response...
(Copy Agent 2's reply)
(Copy Agent 1's reply)</p>
<h2>Round 2</h2>
<p>Agent 1: Given the information provided in other agents' solutions, we have two answers: One agent found that the number of rubies is 40 and the total number of gems is 225 . Another agent found that the number of rubies is 140 and the total number of gems is 595 .</p>
<p>We know that the number of rubies is 35 fewer than the number of diamonds, which is 175 . So the number of rubies should be $175-35=140$, as the second agent found. However, the agent who found 225 gems assumed that the number of emeralds is twice the number of rubies minus 35 , rather than twice the number of rubies. If we use this assumption, we get: Number of rubies $=140$. Number of emeralds $=2 * 140-35=245$. Total number of gems $=175+140+245=560$. Our final answer is 560 .</p>
<p>User Input: Using the solutions from other agents as additional information, can you give an updated response...
(Copy Agent 2's reply)
(Copy Agent 1's reply)</p>
<h2>Round 3</h2>
<p>Agent 1: After reviewing the solutions provided by other agents, I agree with the second agent who found that the number of rubies is 140 and the total number of gems in the chest is 595 . Therefore, my answer to the math problem is 595 .</p>
<p>Agent 2: After reviewing the solutions provided by other agents, I agree with the second agent who found that the number of rubies is 140 and the total number of gems in the chest is 595 . Therefore, my answer to the math problem is 595 .</p>
<p>Figure 2: Illustration of Debate. Illustration of the debate procedure.</p>
<h3>2.1 Multiagent Language Generation</h3>
<p>Consider your work process when solving the following math question on an exam: "What is the area of a triangle with side lengths of $3,4,5$ ?". In one thread of work, you may recognize that the triangle side-lengths directly correspond to a right triangle, and thus directly compute the area as $0.5 \times 3 \times 4=64$. To make sure that you have the right answer, you may then try to solve the problem differently by estimating an angle $\theta$ in the triangle using the Law of Cosines, and then obtain the area by using the formula $0.5 \times 3 \times 4 \times \sin (\theta)$, arriving at another answer to the given exam problem.</p>
<p>When these lines of work give the same answer, your confidence about the answer increases. In contrast, when these answers are different, individual lines of work may engage in a mental "debate" procedure, where you closely cross-examine the reasoning and assumptions of each line of work and refine solutions until a consistent answer.</p>
<p>Similarly, consider writing a biography of a historical figure. To ensure the factuality of the biography, you may consult multiple different sources on each fact. Facts that are consistent in each source increase your confidence about the fact. In contrast, facts that are inconsistent require careful cross-examination between sources to determine the final consistent data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Debate Length</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">"These are the solutions to the problem from other agents: [other answers] <br> Based off the opinion of other agents, can you give an updated response . . ."</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">"These are the solutions to the problem from other agents: [other answers] <br> Using the opinion of other agents as additional advice, can you give an updated response . . ."</td>
</tr>
</tbody>
</table>
<p>Figure 3: Prompts to induce long and short form debate. Responses of other agents to questions are are inserted in the middle of the prompt (indicated with [other answers])</p>
<p>To mimic the above multi-threaded reasoning process and multi-source factuality checking processes, we propose to generate answers subject to a multi-agent debate procedure between multiple instances of large language models. Given a question, multiple agents represented as copies of a large language model, generate answers to the question. Each response serves as a possible thought process or source of information which agents may re-examine to find consistent final answers.</p>
<p>After initial responses are generated from different agents, we initiate a round of debate between agents. Individual responses from other agents are concatenated and given as context to each agent, with each agent instructed to construct a new response based on such responses. Each language agent is thus responsible for both verifying the collection of responses given by other agents, and refining its own response based on other agents’ responses. We iteratively repeat this debate procedure over multiple rounds for improved performance.</p>
<p>Concretely, we first prompt each agent to independently solve the given problem or task. After each agent generates a response, we feed each agent a consensus prompt, illustrated in Figure 3, where each agent is instructed to update their responses based on the responses of other agents. This resultant consensus prompt may then be repeatedly given, using the updated responses of each agent. We illustrate an overview of this multiagent debate procedure in Figure 2.</p>
<p>Note that our proposed approach operates in an orthogonal manner to existing approaches to prompt language models. Given a question, we may apply additional techniques for prompting language models to further improve our debate procedure by eliciting additional more detailed responses from language models. We illustrate the synergy of our approach with existing approaches to prompting language models in Figure 6 and directly apply zero-shot chain-of-thought reasoning in our evaluations.</p>
<h1>2.2 Consensus in Debates</h1>
<p>Given multiple rounds of debate, how can we ensure that a set of language model agents will converge to a final consensus answer? In general, debate can be seen as a multi-agent game, where convergence is not guaranteed. Empirically, however, we find that language models are able to converge on a single shared answer after multiple rounds of debate (Figure 4).</p>
<p>We found that we could control the duration of debates by how changing how much a language model trusts its own outputs over those generated by other models through different prompts. We illustrate two prompts below in Figure 3, which we use to induce different debate durations between language models, and illustrate the effect of such prompts in Figure 12. In general, we found that prompts that encouraged models to be more "stubborn' based on their own solutions led to longer debates and better final solutions. Overall, we observed that language model agents were relatively "agreeable", perhaps as a result of instruction tuning or reinforcement learning based on human feedback [22].</p>
<h2>3 Experiments</h2>
<p>In our experiments, we evaluate our multiagent debate procedure and answer the following questions: (1) To what extent does multiagent debate improve reasoning? (2) To what extent does multiagent debate improve factual validity? (3) What design choices enable multiagent debate to improve language generation performance?</p>
<h3>3.1 Improving Reasoning with Multiagent Debate</h3>
<p>We first evaluate the extent to which multiagent debate improves the underlying reasoning process in language models.
Tasks. We evaluate our approach on three reasoning tasks of increasing difficulty:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Illustration of Solving Math. Reasoning between agents is omitted.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Illustration of Solving Grade School Math. Reasoning between agents omitted.</p>
<ul>
<li>Arithmetic. We first evaluate the ability of models to correctly evaluate an arithmetic expression (containing addition, multiplication, and subtraction) consisting of six different two-digit numbers. For example: What is the result of $12+15 * 21+0-3 * 27$ ?</li>
<li>GSM8K. Next, we consider harder mathematical reasoning tasks. Using the GSM8K dataset [3], the models must correctly solve grade school mathematical reasoning tasks.</li>
<li>Chess Move Prediction. Finally, we consider the strategic reasoning of the ability of models, and ask models to predict the best next move in a game of chess, given the first 14 moves of a chess game between two chess grand-masters described in PGN notation [6].
We report the accuracy of final answers in arithmetic and GSM8K tasks and report the pawn score (advantage) of predicted moves, as estimated by Stockfish in the Chess move prediction tasks. Additional details may be found in the Appendix.</li>
</ul>
<p>Baselines. We compare our approach to three alternative approaches to generate responses for reasoning problems. First, we ask agents to directly generate responses (single agent). Next, we consider asking language models to generate and then "self-reflect" on the responses generated [26, 18]. Finally, we consider generating responses using multiple agents and performing majority voting $[15,3]$. As the focus of our experiments is to verify the effectiveness of multiagent agent debate, we run both baselines and our approach, using the identical starting prompt and language model across all evaluations. We evaluate models in a zero-shot setting, with prompts found in the Appendix of the paper.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 6: Synergy with Other Methods. Performance of debate increases with use of Chain of Thought prompting.</p>
<p>We use chatGPT-based language model [21] in all our experiments except those in Figure 11 where we compare multiple language models.</p>
<p>Due to computational expense, we evaluate our approach across benchmarks mainly using three agents with two rounds of debates, although we found further gains with both more agents and rounds of debate (Figure 10). Additional evaluation details are found in the Appendix.</p>
<p>Quantitative Results. In Table 1, we report the results of each approach on arithmetic, grade school math, and chess reasoning task. In each task, we observe that utilizing multiple different agents to generate solutions improves performance over using a single language model agent to generate a solution. Simultaneously, we also see that reflection, where a language model is asked to critique its early generation, generally gives a modest boost in performance. Multiagent debate, which may be seen as a combination of both reflection and multiagent generation, gives a substantial boost in reasoning across each of the tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Arithmetic (\%) $\uparrow$</th>
<th style="text-align: center;">Grade School Math (\%) $\uparrow$</th>
<th style="text-align: center;">Chess ( $\Delta$ PS) $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Agent</td>
<td style="text-align: center;">$67.0 \pm 4.7$</td>
<td style="text-align: center;">$77.0 \pm 4.2$</td>
<td style="text-align: center;">$91.4 \pm 10.6$</td>
</tr>
<tr>
<td style="text-align: left;">Single Agent (Reflection)</td>
<td style="text-align: center;">$72.1 \pm 4.5$</td>
<td style="text-align: center;">$75.0 \pm 4.3$</td>
<td style="text-align: center;">$102.1 \pm 11.9$</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent (Majority)</td>
<td style="text-align: center;">$69.0 \pm 4.6$</td>
<td style="text-align: center;">$81.0 \pm 3.9$</td>
<td style="text-align: center;">$102.2 \pm 6.2$</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent (Debate)</td>
<td style="text-align: center;">$\mathbf{8 1 . 8} \pm \mathbf{2 . 3}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 0} \pm \mathbf{3 . 5}$</td>
<td style="text-align: center;">$\mathbf{1 2 2 . 9} \pm \mathbf{7 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Multiagent Debate Improves Reasoning Multi-agent debate improves the reasoning abilities of languaee models. Multi-agent results in the table are run with 3 agents and two rounds of debate.</p>
<p>Prompt: Give a bullet point biography of David S. Johnson highlighting their contributions and achievements as a computer scientist, with each fact separated with a new line character.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Round 1</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Agent 1: -- He earned his bachelor's degree from Dartmouth College in 1967 and his Ph.D. from MIT in 1973. <br> -- Johnson has received numerous awards for his research and contributions to computer science, including the ACM Turing Award in 2013. <br> -- Johnson's research interests include optimization, algorithms, and complexity theory.</td>
<td style="text-align: center;">Agent 2: -- David S. Johnson is an American computer scientist and mathematician. <br> -- Johnson was elected to the National Academy of Engineering in 2006. <br> -- He has been awarded numerous honors and awards for his contributions to computer science, including the ACM SIGMOD Innovations Award and the Knuth Prize.</td>
</tr>
<tr>
<td style="text-align: center;">Round 2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Agent 1: -- David S. Johnson is a computer scientist and mathematician. <br> -- Johnson was elected to the National Academy of Engineering in 2006. <br> -- He has received numerous awards for his contributions to computer science, including the ACM Turing Award in 2013 and the Knuth Prize.</td>
<td style="text-align: center;">Agent 2: -- David S. Johnson is an American computer scientist and mathematician. <br> -- He has worked at several prestigious universities, including Columbia University, AT\&amp;T Bell Laboratories, and Cornell University. <br> -- Johnson is known for his work in the field of algorithms, particularly in the areas of network optimization, combinatorial optimization, and scheduling theory.</td>
</tr>
</tbody>
</table>
<p>Figure 7: Illustration of Generating Biographies. Illustration of generating bullet biographies of computer scientists. For brevity, only the first 3 generated bullets are shown.</p>
<p>Qualitative Results. In Figure 4, 5, we provide qualitative illustrations of the debate procedure between models. Interestingly, we find cases in which all models initially give an incorrect response, yet the result of debate still obtains the correct answer as agents critique each others' reasoning. Thus, the purpose of our debate isn't just to amplify a correct answer - all models can initially be wrong but arrive at the correct answer through the debate process.
Compatibility with other reasoning methods. Our multiagent generation procedure operates orthogonally approach to other prompting methods which focus on single-agent generation. In Figure 6, we illustrate the performance of multi-agent debate with and without zero-shot chain-ofthought prompting [11] on GSM8K. In both settings, multiagent generation is beneficial.</p>
<h1>3.2 Extracting Factual Information from Multiagent Debate</h1>
<p>We next evaluate the extent to which multiagent debate improves the underlying factuality in language models.
Tasks. We evaluate the factuality of language models in three different settings:</p>
<ul>
<li>Biographies. To evaluate the factuality of language models, we introduce a novel task of accurately generating historical biographies of people. In preliminary testing, we found that existing language models had a tendency to hallucinate many facts on this task. We constructed ground truth bullet point biographies of 524 well-known computer scientists. We then asked language models to generate bullet point biographies for each person, and evaluated the accuracy at which each ground truth bullet point agreed with generated bullets. We report additional evaluation details in the Appendix.</li>
</ul>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: Illustration of MMLU. Illustration of debate when answering factual tasks. Reasoning omitted.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Biographies</th>
<th style="text-align: center;">MMLU</th>
<th style="text-align: center;">Chess Move Validity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Single Agent</td>
<td style="text-align: center;">$66.0 \pm 2.2$</td>
<td style="text-align: center;">$63.9 \pm 4.8$</td>
<td style="text-align: center;">$29.3 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: left;">Single Agent (Reflection)</td>
<td style="text-align: center;">$68.3 \pm 2.9$</td>
<td style="text-align: center;">$57.7 \pm 5.0$</td>
<td style="text-align: center;">$38.8 \pm 2.9$</td>
</tr>
<tr>
<td style="text-align: left;">Multi-Agent (Debate)</td>
<td style="text-align: center;">$\mathbf{7 3 . 8} \pm \mathbf{2 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 1} \pm \mathbf{4 . 6}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 2} \pm \mathbf{2 . 9}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Multiagent Debate Improves Factual Accuracy Multi-agent debate improves the factual accuracy.</p>
<ul>
<li>MMLU. Next, we assess the factuality of language models in responding to different factual knowledge questions typically learned and assessed in different exams. We utilize the existing MMLU dataset [8] to benchmark the accuracy of responses.</li>
<li>Chess Move Validity. Lastly, we study the hallucinations in language models when planning under to the given rules of an existing environment or game. Specifically, we measure the validity of possible moves in a game of Chess given by BIG-Bench Chess-State Tracking Benchmark [27] task of chess-move prediction. In this task, an agent is given a set of next moves, and must make a valid next move of a piece on a board.</li>
</ul>
<p>Baselines. We use the same baselines as in Section 3.1. The multiagent (majority) is not directly applicable in this setting as individual responses are not easily comparable, and so we omit baseline comparison with the majority voting in this setting.
Results. We analyze the performance of each method in Table 2. We found that approaches based on reflection led to poor performance in the factuality setting. In contrast, debate gives the best performance in this setting also, and significantly outperforms each baseline. We illustrate a debate between agents on the biography task in Figure 7 and on MMLU in Figure 8. We found that multiagent debate improved and settled on bullets that were more consistent across agents.
We found that different language agents tended to give different answers when the underlying language model was uncertain about the question. However, directly asking each agent about their confidence [10] of the answer led to high confidence assessments on each answer. However, when these different language agents were asked to communicate with each other, each agent would quickly change their opinion to a consensus answer which was more accurate. We illustrate this in Figure 9. Interestingly, we found that on facts that the language model was confident in (i.e. many instances of the same model all gave the same answer), it was very difficult to convince an agent to change their opinion, suggesting that "ease of persuasion" may be a method to assess factual confidence.</p>
<h1>3.3 Analysis: Understanding Multiagent Debate</h1>
<p>Finally, we analyze how multiagent debate improves the underlying language generation procedure in language models.</p>
<p>Number of Agents. First, we analyze the impact of agents number in debate. In Figure 10(a), we increase the number of agents used in debate, while fixing the debate length to be two. On arithmetic, performance monotonically increases with the increased number of agents. For larger number of agents, we first summarize all agent responses with chatGPT instead of directly concatenating responses due to context length error.
Rounds of Debate Next, we analyze the impact of the number of rounds of debate in multiagent debate. In Figure 10(b), we increase the debate length between agents, while fixing the number of agents to three. We find that on the arithmetic task, the performance also monotonically increases with debate length. However, we found that additional debate rounds above four led to a similar final performance to 4 rounds of debate.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Round 1</th>
<th style="text-align: left;">Prompt: What country was the computer scientist Tomas Lozano-Perez born in?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Agent 1: Tomas Lozano-Perez, a renowned <br> computer scientist, was born in Spain.</td>
</tr>
<tr>
<td style="text-align: left;">Round 2</td>
<td style="text-align: left;">Agent 1: I apologize for the confusion. After further <br> research, I can confirm that Tomas Lozano-Perez, <br> the renowned computer scientist, was actually born <br> in Cuba. Thank you for bringing this to my attention.</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Expressing Uncertainty with Multiple Answers. For facts that a language model is uncertain about, different language agents generate different facts. Debate causes agents to converge to one fact that is more accurate, but not necessarily always factually correct.
(a)
<img alt="img-6.jpeg" src="img-6.jpeg" />
(b)
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 10: (a) Performance with Increased Agents. Performance improves as the number of underlying agents involved in debate increases. (b) Performance with Increased Rounds. Performance rises as the number of rounds of underlying debate increases.
Effect of Debate Length on Accuracy As discussed in Section 2.2, the underlying convergence time in the debate between agents can be controlled by the extent to which agents are encouraged to maintain their opinions. In Figure 12, we consider the effect of short and long-form prompts discussed in Figure 3. We find that debates using longer prompts lead to slower convergence to correct answers, but also lead to a better final consensus on the correct answer. We provide an analysis of consensus between agents in Figure 14.
Using Different Initialization Prompts In our experiments we use the same prompts for all agents. We also consider the effect of using different questions, where we first instruct each language model to behave like a different persona (professor, doctor, mathematician) on the MMLU dataset. We found that improved performance on MMLU from 71.1 to 74.2 with different agents, suggesting further gains can be obtained with different initialization prompts.
Summarization. While in the majority of experiments in the paper we directly concatenate the responses of other agents as context for an agent to generate a new response, this is expensive when the number of agents involved in debate gets large. We may alternatively first summarize the responses from all other agents into a single response that we provide to agent at each round for more efficient debate. We apply this strategy in Figure 10 to enable the use of five or more agents in debate. In Figure 13, we analyze the effect compared to directly concatenating the responses of other agents. We find this improves the performance of debate, suggesting that summarization is another tool that can further improve multiagent debate.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 13: Effect of Summarization. When there are many agents in a debate, responses from other agents may be first summarized and then given as context, reducing context length. This operation improves performance.</p>
<p>Utilizing Different Language Models Our existing debate results are reported using multiple instances of a chatGPT language model. We further assess</p>
<p>Question: Carla is downloading a 200 GB file. Normally she can download $2 \mathrm{~GB} /$ minute, but $40 \%$ of the way through the download, Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the download from the beginning. How load does it take to download the file?</p>
<h1>Round 1</h1>
<p>ChatGPT: Before the restart, Carla was able to download $40 \%$ of the file, which is $0.4 * 200 \mathrm{~GB}=80 \mathrm{~GB}$. At a rate of $2 \mathrm{~GB} /$ minute, it would take her: time $=80 \mathrm{~GB} \div 2$ $\mathrm{GB} /$ minute $=40$ minutes.</p>
<p>Because of the restart, Carla had to start over. This means she had to download the entire 200 GB file. The time it takes to download the remaining 120 GB is: time $=120 \mathrm{~GB}$ $+2 \mathrm{~GB} /$ minute $=60$ minutes.</p>
<p>But Carla also had to wait 20 minutes for the Windows update to install, so the total time it took to download the file is: total time $=40$ minutes +60 minutes +20 minutes $=$ 120 minutes.</p>
<p>Bard: Carla is downloading a 200 GB file at a speed of $2 \mathrm{~GB} /$ minute. This means that it will take her $200 /$ $2=100$ minutes to download the file.</p>
<p>However, Windows forces a restart to install updates, which takes 20 minutes. This means that Carla will have to restart the download from the beginning. Since Carla has already downloaded $40 \%$ of the file, she only needs to download $200-40=160 \mathrm{~GB}$. At a speed of $2 \mathrm{~GB} /$ minute, it will take Carla $160 / 2=80$ minutes to download the file.</p>
<p>Adding the time for the Windows restart, it will take Carla $80+20=100$ minutes to download the file.</p>
<h2>Round 2</h2>
<p>ChatGPT : Based on the calculations provided by the other agent, the correct answer is: Before the restart, Carla was able to download $40 \%$ of the file, which is $0.4 * 200 \mathrm{~GB}=80 \mathrm{~GB}$. At a rate of $2 \mathrm{~GB} /$ minute, it would take her: time $=$ $80 \mathrm{~GB} \div 2 \mathrm{~GB} /$ minute $=40$ minutes</p>
<p>However, because of the restart, Carla had to start over. This means she had to download the entire 200 GB file. The time it takes to download the entire 200 GB file is: time $=200 \mathrm{~GB} \div 2 \mathrm{~GB} /$ minute $=100$ minutes</p>
<p>And since Carla had to wait 20 minutes for the Windows update to install, the total time it took to download the file is: total time $=40$ minutes +100 minutes +20 minutes $=160$ minutes.</p>
<p>Figure 11: Debate Between chatGPT and Bard Illustration of debate between different models.
the impact of using two different language models, where we ask chatGPT and Bard [23] language models to debate with each other on a set of 20 GSM8K math problems. In this set, we find that multi-agent debate improves the performance of both agents, with Bard solving 11 problems, chatGPT solving 14 problems, and joint multi-agent debate solving 17 problems. We qualitatively illustrate a debate between agents in Figure 11. While both agents initially provide incorrect answers to the problem, chatGPT is able to utilize the incorrect response given by Bard to generate the final correct answer.</p>
<h2>4 Related Work</h2>
<p>Reasoning and Factuality in Language Models. A wide range of work has explored how to enable reasoning and factuality in language models. To improve reasoning, approaches have relied on prompting techniques such as scratchpads [20], verification [3], chain-of-thought demonstrations [30, 11, 25], and intermediate self-reflection [26, 18] and finetuning [13, 24, 31]. To improve factuality, approaches have relied on training techniques such as RLHF [33, 16, 2], pruning truthful datasets [12], external knowledge retrieval [7] and training-free methods based off likelihood estimation [10].
Our work provides an alternative way to obtain reasoning and factuality in language models using multiagent debates, which only requires black-box access to a language generator. Prior work also has explored how to take the majority vote across different models [15, 3, 29, 28] while in this work, we use the power of a language model to combine different answers. Most similar to our work, Irving et al. [9] also proposes a debate procedure to verify the accuracy and safety of powerful AI agents. In contrast to our approach, in their work, agents are asked to alternatively provide proof of a input, and humans are tasked with assessing these debates and determining safety.</p>
<p>Compositional Generation. Our work is also related to existing works that focus on text generation by combining different models [4, 17, 32, 1, 5]. Most similar to our work, [14, 32] propose to combine multiple different large pretrained models together for multimodal reasoning. In contrast, in our work, we aim to use communication between different language models to enable more effective reasoning and factuality in language models.</p>
<h1>5 Limitations and Discussion</h1>
<p>In this paper, we present an orthogonal approach to improve the performance of language models using multi-agent debate. We find that the approach is simple and effective across a wide set of different reasoning and validity language modeling tasks.
Limitations. In comparison to other prompting techniques, our multiagent debate procedure is more computationally expensive, as it requires both multiple language generations, and an underlying debate procedure. However, we believe that this approach may be seen as a method to generate additional data that may be distilled back to self-improve the original base model.
Further, we observed that as debates became longer in duration, current language models sometimes struggled to fully process the entire debate input, and typically only focused on the most recent generations. We believe that this performance will be alleviated with longer-context and improved language models or by summarizing early portions of the debate.
Finally, we found that while debates typically converged into single final answers, these answers were not necessarily correct. Despite answers being incorrect, language models would confidently affirm that their answer is correct and consistent with all other agent responses. We believe this result is in part due to the fact that LMs do not correctly express their uncertainty when generating responses, and believe that other orthogonal approaches to improve this performance would improve the results of multiagent debate.</p>
<h2>References</h2>
<p>[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022. URL https://arxiv.org/abs/2204.14198. 9
[2] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human preferences. In Neural Information Processing Systems, 2017. 9
[3] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 5, 9
[4] Y. Du, S. Li, and I. Mordatch. Compositional visual generation with energy based models. In Advances in Neural Information Processing Systems, 2020. 9
[5] Y. Du, C. Durkan, R. Strudel, J. B. Tenenbaum, S. Dieleman, R. Fergus, J. Sohl-Dickstein, A. Doucet, and W. Grathwohl. Reduce, reuse, recycle: Compositional generation with energybased diffusion models and mcmc. arXiv preprint arXiv:2302.11552, 2023. 9
[6] Fsmosca. Fsmosca/pgn-standard: Portable game notation specification and implementation guide. URL https://github.com/fsmosca/PGN-Standard. 5
[7] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. REALM: Retrieval-augmented language model pre-training. arXiv preprint arXiv:2002.08909, 2020. 9
[8] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. 7
[9] G. Irving, P. Christiano, and D. Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899, 2018. 9
[10] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. H. Dodds, N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. 7, 9
[11] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. 2, 6, 9</p>
<p>[12] N. Lee, W. Ping, P. Xu, M. Patwary, P. N. Fung, M. Shoeybi, and B. Catanzaro. Factuality enhanced language models for open-ended text generation. Advances in Neural Information Processing Systems, 35:34586-34599, 2022. 9
[13] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022. 9
[14] S. Li, Y. Du, J. B. Tenenbaum, A. Torralba, and I. Mordatch. Composing ensembles of pre-trained models via iterative consensus. arXiv preprint arXiv:2210.11522, 2022. 9
[15] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):1092-1097, 2022. 5, 9
[16] H. Liu, L. Lee, K. Lee, and P. Abbeel. Instruction-following agents with jointly pre-trained vision-language models. arXiv preprint arXiv:2210.13431, 2022. 9
[17] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional visual generation with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022. 9
[18] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. 2, 5, 9
[19] M. Minsky. Society of mind. Simon and Schuster, 1988. 1
[20] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz, M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021. 9
[21] OpenAI. Chatgpt: Optimizing language models for dialogue, Dec 2022. URL https:// openai.com/blog/chatgpt/. 2, 5
[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022. 4
[23] S. Pichai. An important next step on our ai journey, Feb 2023. URL https://blog.google/ technology/ai/bard-google-ai-search-updates/. 2, 9
[24] N. F. Rajani, B. McCann, C. Xiong, and R. Socher. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019. 9
[25] L. Reynolds and K. McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-7, 2021. 9
[26] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023. 2, 5, 9
[27] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. 7, 14
[28] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022. 9
[29] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 9
[30] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. 9</p>
<p>[31] E. Zelikman, Y. Wu, J. Mu, and N. Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:15476-15488, 2022. 9
[32] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani, J. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. URL https://arxiv.org/abs/2204. 00598. 9
[33] D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 9</p>
<h1>A Appendix</h1>
<p>In this appendix, we provide additional analysis and visualizations of the debates used in the main paper in Section A.1. We further provide detailed experimental details on each dataset in Section A.2.</p>
<h2>A. 1 Additional Results</h2>
<p>Consensus Between Agents. In Figure 14, we illustrate the consensus between agents using either short or long consensus prompts discussed in Figure 3. The use of debate prompts that encourage agents to adapt more to the opinions of other agents improves consensus.
Additional Qualitative Visualizations. We added additional qualitative visualizations of the debate process. In Figure 16, Figure 17, Figure 18, Figure 19, Figure 20, we illustrate debates between agents in the GSM8K dataset which result in the correct answer. In Figure 21, Figure 22, Figure 23, we further illustrate debates in GSM8K which lead to the incorrect answer. We further provide an example illustration of debate in arithmetic in Figure 24, arithmetic with summarization of individual responses of agents in Figure 25, MMLU in Figure 26, a debate with the full contents biographies in Figure 27, and debate in chess in Figure 28. In general, we found that debate improved the performance of final generated answers, though sometimes answers would converge to the incorrect value.</p>
<h2>A. 2 Evaluation Details</h2>
<p>We provided detailed evaluation details for each setting in the paper. We run all experiments using the gpt-3.5-turbo-0301 model. We provide a table listing the prompts used to prompt models and initialize debate in Table 15.</p>
<p>Arithmetic. To evaluate the arithmetic task, we generated six random integers for each task between 0 and 30 . We then evaluated the extent to which the correct integer answer was correctly obtained. We evaluated models on one hundred generated arithmetic tasks.</p>
<p>Grade School Math. To evaluate the GSM8K task, we evaluated the accuracy at which models were able to obtain the final correct answer, as extracted from a box. We evaluated models on one hundred grade school math problems.</p>
<p>Chess. To evaluate the chess reasoning task, we used chess games from https://www.pgnmentor. com/players/Adams.zip. We asked chatGPT to predict the next move for white to move at turn 14 and reported the relative Stockfish pawn score with search depth 20 after executing the suggested move from chatGPT. We evaluated models on three hundred selected chess games.</p>
<p>Biographies. To evaluate the biographies task, we compare each generated bullet point biography for a person with a ground truth set of facts about the person extracted from Wikipedia. We iteratively loop through each ground truth fact, and validate the extent to which the generated biography matches a particular bullet by prompting chatGPT with the prompt: Consider the following biography of <person>: <generated biography> Is the above biography above consistent with the fact below? <ground truth bullet> Give a single-word answer, yes, no, or uncertain. We then evaluate and report the percentage of ground bullets that chatGPT returns either yes or no on. We ignored ground truth bullets that chatGPT returns returned uncertain.</p>
<p>We found this evaluation metric provided a fast way to evaluate how relatively correct a generated bullet point biography is. However, we found that generated facts could contain incorrect information that was not captured in the ground truth bullet and thus could not be validated through this metric. Nevertheless, we believe this evaluation scheme estimates the relative accuracy of a generated biography.</p>
<p>MMLU. To evaluate MMLU, we measured the accuracy in which models were able to select the correct multiple-choice answer in each problem. We evaluated models on one hundred selected MMLU questions randomly distributed across each of the subject areas.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Arithmetic</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">What is the result of [[+[]<em>[]+[]-[]</em>]]? Make sure to state your answer at the end of the response.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">These are the recent/updated opinions from other agents: <other agent responses> Use these opinions carefully as additional advice, can you provide an updated answer? Make sure to state your answer at the end of the response.</td>
</tr>
<tr>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">Can you solve the following math problem? <Problem> Explain your reasoning. Your final answer should be a single numerical number, in the form'dossed[[answer]], at the end of your response.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">These are the solutions to the problem from other agents: <other agent responses> Using the solutions from other agents as additional information, can you provide your answer to the math problem? The original math problem is <Problem>. Your final answer should be a single numerical number, in the form 'dossed[[answer]], at the end of your response.</td>
</tr>
<tr>
<td style="text-align: center;">Chess</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">Here is the current sequence of moves in a chess game: <moves>. What is the best chess move I should execute next? Give a single move suggestion of the form 14. <XXX> and make sure the chess move is valid in the current board state.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">Here are other chess move suggestions from other agents: <other agent responses> Using the chess suggestions from other agents as additional advice and your earlier generated solution, can you give me your updated thoughts on the best next chess move I should play given the chess sequence ? Give a single move suggestion of the form 14. <XXX> and make sure the chess move is valid in the current board state.</td>
</tr>
<tr>
<td style="text-align: center;">Biographies</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">Give a bullet point biography of highlighting their contributions and achievements as a computer scientist, with each fact separated with a new line character.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">Here are some bullet point biographies of <person> given by other agents: <other agent response> Closely examine your biography and the biography of other agents and provide an updated bullet point biography.</td>
</tr>
<tr>
<td style="text-align: center;">MMLU</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">Can you answer the following question as accurately as possible? : A) , B) , C) , D) Explain your answer, putting the answer in the form $(X)$ at the end of your response.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">These are the solutions to the problem from other agents: <other agent responses> Using the reasoning from other agents as additional advice, can you give an updated answer? Examine your solution and that other agents. Put your answer in the form $(X)$ at the end of your response.</td>
</tr>
<tr>
<td style="text-align: center;">Chess Validity</td>
<td style="text-align: center;">Starting</td>
<td style="text-align: center;">Given the chess game, give one valid destination square for the chess piece at. State the destination square in the form $(X)$, where $X$ follows the reges $[a-h][1-8]$, for example $(e 5)$. Give a one line explanation of why your destination square is a valid move.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Debate</td>
<td style="text-align: center;">Here are destination square suggestions from other agents: <other agent responses> Can you double check that your destination square is a valid move? Check the valid move justifications from other agents. State your final answer in a newline with a 2 letter response following the reges [a-h][1-8].</td>
</tr>
</tbody>
</table>
<p>Figure 15: Prompts in each task. List of prompts used in each task
Chess Validity. To evaluate chess validity, we consider the BIG-Bench Chess-State Tracking Benchmark [27], where we used the hardest reported task in the benchmark synthetic_short. Each generated answer was deemed correct as long as it was one of the valid answers in the sequence. We evaluated models of one hundred selected chess validity tasks.</p>
<p>User Input: Can you solve the following math problem? John arm wrestles 20 people. He beats $80 \%$. How many people did he lose to? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: If John beat $80 \%$ of the people he arm wrestled, he must have lost to the remaining $20 \%$. To find how many people that is, we can set up a proportion:
\$\frac{{\text { text }{\text { number of people John beat }}}{} \text { [text }{\text { total number of people John }}$ arm wrestled $}=80 \%=\backslash$ frac ${4}{5} \$$</p>
<p>Let $x$ be the number of people John lost to. Then we know:
\$\frac{{\text { text }{\text { number of people John beat }}}{} \text { [text }{\text { total number of people John }}$ arm wrestled $}=$ \frac{{\text { text }{\text { number of people John beat }}}{} \text { [text }{\text { number of }}$ people John beat $} \quad+\backslash \text { text }{x}=$ \frac{{4}{5}\$</p>
<p>Cross-multiplying yields:
\$5 $\backslash$ text ${{$ number of people John beat $}=4($ text ${$ number of people John beat $}$ $+\backslash$ text ${\mathrm{x}} \$$</p>
<p>Simplifying this equation gives:
$\$ \backslash$ text ${x}=\backslash$ boxed ${25} \$$</p>
<p>Therefore, John must have lost to 25 people.</p>
<p>Agent 2: If John beats $80 \%$ of the people he arm wrestles, then he must lose to $20 \%$ of them. Since $20 \%$ of 20 people is $\$(0.20)(20)=4 \$$, John must have lost to $\$ \backslash$ boxed ${4} \$$ people.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed{answer}, at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: Both solutions are correct in understanding that since John beat $80 \%$ of the people he arm wrestled, he lost to the remaining $20 \%$. However, the first solution made an error in the calculation, where they found that John won against 16 people out of $20(80 \%$ of 20$)$, leading them to think that he lost to 4 .
Since John arm-wrestled 20 people in total, and won against $80 \%$ of them, he must have lost to the remaining $20 \%$. Therefore, the number of people John lost to is:
\$ $\backslash$ text ${$ Number of people John lost to $}=20 \backslash$ cdot $0.2=\backslash$ boxed ${4} \$$
So the correct answer is indeed 4 , as the second agent already provided.</p>
<p>Agent 2: I agree with the other agents that John lost to $20 \%$ of the people he arm wrestled, which is equal to 4 people since he arm wrestled a total of 20 people. Therefore the answer is $\$ \backslash$ boxed ${4} \$$.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed{answer}, at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I agree with both previous agents that since John beat $80 \%$ of the people he arm wrestled, he lost to the remaining $20 \%$. Therefore, the number of people John lost to is:
\$ $\backslash$ text ${$ Number of people John lost to $}=20 \backslash$ cdot $0.2=\backslash$ boxed ${4} \$$
So the answer is indeed 4 , as the first agent incorrectly calculated but the second agent correctly calculated.</p>
<p>Agent 2: I agree with the other agents that John lost to $20 \%$ of the people he arm wrestled, which is equal to 4 people since he arm wrestled a total of 20 people. Therefore the answer is $\$ \backslash$ boxed ${4} \$$.</p>
<p>Figure 16: Example of a correct GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? Shondra has 7 fewer plants than Toni. Toni has $60 \%$ more plants than Frederick. If Frederick has 10 plants, how many plants does Shondra have? Explain your reasoning. Your final answer should be a single numerical number, in the form $\backslash$ boxed [answer], at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: If Frederick has 10 plants, then Toni has $60 \%$ more plants, which is equal to $10+(60 / 100)^{*} 10=16$ more plants than Frederick. Therefore, Toni has a total of $10+16=26$ plants.
Shondra has 7 fewer plants than Toni, which means she has $26-7=19$ plants.</p>
<p>Final answer: $\backslash$ boxed [19].</p>
<p>Agent 2: First, we need to determine how many plants Toni has. We know that Toni has $60 \%$ more plants than Frederick, who has 10 plants. To find $60 \%$ of 10 , we can multiply 10 by 0.6 , which gives us 6 . Therefore, Toni has $10+6=16$ plants.</p>
<p>Next, we know that Shondra has 7 fewer plants than Toni. So we can subtract 7 from Toni's 16 plants to get Shondra's number of plants: $16-7=9$ plants.</p>
<p>Therefore, Shondra has $\backslash$ boxed [9] plants.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed [answer], at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: I confirm that the solution provided by the other agent is correct. Toni has 16 plants, and since Shondra has 7 fewer plants than Toni, she has $16-7=9$ plants.</p>
<p>Final answer: $\backslash$ boxed [9].</p>
<p>Agent 2: I agree with the previous agents that Toni has 16 plants and Shondra has 7 fewer plants than Toni, which is 9 plants. Therefore, my final answer is $\backslash$ boxed [9].</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed [answer], at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I agree with the previous agents that Toni has 16 plants and Shondra has 7 fewer plants than Toni, which means Shondra has $16-7=9$ plants.</p>
<p>Therefore, my final answer is $\backslash$ boxed [9].</p>
<p>Agent 2: I concur with the other agents that Toni has 16 plants and Shondra has 7 fewer plants than Toni, which means Shondra has $16-7=9$ plants.</p>
<p>Therefore, my final answer is $\backslash$ boxed [9].</p>
<p>Figure 17: Example of Correct GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? Bob is in charge of doing laundry for a large hotel. Each room has two sheets, one comforter, twice as many pillow cases as sheets and twice as many towels as pillow cases. How many pieces of laundry are there in 80 rooms? Explain your reasoning. Your final answer should be a single numerical number, in the form $\backslash$ boxed{answer}, at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: For each room, Bob needs to wash:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter</li>
<li>$2 \times 2=4$ pillow cases
$-4 \times 2=8$ towels
So for each room, Bob needs to wash a total of $2+1+4+8=15$ pieces of laundry.</li>
</ul>
<p>For 80 rooms, Bob needs to wash a total of $80 \times 15=1200$ pieces of laundry.</p>
<p>Therefore, the answer is $\backslash$ boxed ${1200}$.</p>
<p>Agent 2: In each room, there are:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter</li>
<li>2 pillow cases (twice as many as sheets)</li>
<li>4 towels (twice as many as pillow cases)</li>
</ul>
<p>So in total, each room has $2+1+2+4=9$ pieces of laundry.</p>
<p>Therefore, in 80 rooms, there are $80 * 9=\backslash$ boxed ${720}$ pieces of laundry.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed {answer}, at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: Based on the given information, each room has:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter
$-2 \times 2=4$ pillow cases
$-4 \times 2=8$ towels
This gives a total of $2+1+4+8=15$ pieces of laundry per room.
Therefore, in 80 rooms, there are $80 \times 15=\backslash$ boxed ${1200}$ pieces of laundry in total.</li>
</ul>
<p>Note: While the other agent's solution is correct in terms of the number of pieces of laundry per room, they made a mistake in their final calculation by multiplying 80 by 9 instead of 15 .</p>
<p>Agent 2: Based on the information provided, each room has:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter
$-2 \times 2=4$ pillow cases
$-4 \times 2=8$ towels
This gives a total of $2+1+4+8=15$ pieces of laundry per room.</li>
</ul>
<p>Therefore, in 80 rooms, there are $80 \times 15=$ boxed ${1200}$ pieces of laundry in total.</p>
<p>I agree with the other agent's corrected calculation.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed {answer}, at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: Based on the information provided, each room has:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter
$-2 \times 2=4$ pillow cases
$-4 \times 2=8$ towels
This gives a total of $2+1+4+8=15$ pieces of laundry per room.
Therefore, in 80 rooms, there are $80 \times 15=\backslash$ boxed ${1200}$ pieces of laundry in total.</li>
</ul>
<p>I agree with the other agent's corrected calculation.</p>
<p>Agent 2: Based on the information provided, each room has:</p>
<ul>
<li>2 sheets</li>
<li>1 comforter
$-2 \times 2=4$ pillow cases
$-4 \times 2=8$ towels
This gives a total of $2+1+4+8=15$ pieces of laundry per room.</li>
</ul>
<p>Therefore, in 80 rooms, there are $80 \times 15=$ boxed ${1200}$ pieces of laundry in total.</p>
<p>I agree with the other agents' answers and calculations.</p>
<p>Figure 18: Example of Correct GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? Lori wants to buy a $\$ 320.00$ pair of shoes and a matching belt that is $\$ 32.00$. Her part-time job pays her $\$ 8.00$ an hour. How many hours will she have to work before she can make her purchase? Explain your reasoning. Your final answer should be a single numerical number, in the form \boxed{answer}, at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: To find out how many hours Lori will need to work to make her purchase, we need to first calculate the total cost of both the shoes and belt:
$\$ 320.00$ (shoes) $+\$ 32.00$ (belt) $=\$ 352.00$
Next, we need to figure out how much money Lori will earn from her part-time job. We can set up a proportion:
$\$ 8.00 /$ hour $=\mathrm{X} /$ total hours
We can solve for X by cross-multiplying:
$\$ 8.00 /$ hour $=\mathrm{X} /$ total hours
$\$ 8.00 *$ total hours $=\mathrm{X}$
$\mathrm{X}=\$ 352.00$
This means that Lori will need to work a total of:
$\$ 352.00+\$ 8.00 /$ hour $=44$ hours
Therefore, Lori will need to work 44 hours before she can make her purchase. Answer: $\backslash$ boxed ${44}$.</p>
<p>Agent 2: Lori needs to earn a total of $\$ 352.00$ ( $\$ 320.00$ for shoes $+\$ 32.00$ for belt) to make her purchase. She earns $\$ 8.00$ per hour at her job.</p>
<p>To find out how many hours she needs to work, we can set up an equation:
\$8.00/hour \timeshers x hours $=\$ 352.00$
Solving for x , we can divide both sides by $\$ 8.00 /$ hour:
x hours $=\$ 352.00 / \$ 8.00 /$ hour
x hours $=44$ hours
Therefore, Lori needs to work 44 hours before she can make her purchase.
\boxed ${44}$</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form boxed{answer}, at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: I agree with the other agents that Lori needs to work a total of $\$ 352.00+\$ 8.00 /$ hour $=44$ hours to make her purchase. The final answer is $\backslash$ boxed ${44}$.</p>
<p>Agent 2: I agree with the other agents that Lori needs to work for 44 hours to make her purchase. The final answer is $\backslash$ boxed ${44}$.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form boxed{answer}, at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I agree with the other agents that Lori needs to work 44 hours to make her purchase. Therefore, the final answer is $\backslash$ boxed ${44}$.</p>
<p>Agent 2: After reviewing the solutions provided by other agents, I agree that Lori needs to work 44 hours to make her purchase, so the final answer is $\backslash$ boxed ${44}$.</p>
<p>Figure 19: Example of Correct GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? Jennifer bought 12 oranges from the market, she gave her three daughters 2 oranges each, and her only boy got 3 oranges. How many oranges did she remain with? Explain your reasoning. Your final answer should be a single numerical number, in the form {boxed{answer}, at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: Jennifer gave a total of 2 oranges/daughter x 3 daughters +3 oranges/son $=9$ oranges to her children. Thus, she remained with 12 oranges -9 oranges $=3$ oranges.
$\backslash$ boxed ${3}$.</p>
<p>Agent 2: Jennifer gave away a total of $\$ 2$ $\backslash$ dot $3+3=9 \$$ oranges. Therefore, she remained with $\$ 12-9=\backslash$ boxed ${3} \$$ oranges.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed {answer}, at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: I agree with the previous solutions provided. Jennifer gave a total of 2 oranges/daughter x 3 daughters +3 oranges/son $=9$ oranges to her children. Thus, she remained with 12 oranges -9 oranges $=3$ oranges.
\boxed ${3}$.</p>
<p>Agent 2: I agree with the other agents' solutions. Jennifer gave away a total of 2 oranges/daughter x 3 daughters +3 oranges/son $=9$ oranges. Therefore, she remains with 12 oranges - 9 oranges $=$ $\$$ boxed ${3} \$$ oranges.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form $\backslash$ boxed {answer}, at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I confirm the previous solutions provided. Jennifer gave a total of 2 oranges/daughter x 3 daughters +3 oranges/son $=9$ oranges to her children. Therefore, she remains with 12 oranges - 9 oranges $=$ $\$$ boxed ${3} \$$ oranges.</p>
<p>Agent 2: I agree with the other agents. Jennifer gave away a total of 2 oranges/daughter x 3 daughters +3 oranges/son $=9$ oranges to her children, therefore she remains with 12 oranges - 9 oranges $=\$$ boxed ${3} \$$ oranges.</p>
<p>Figure 20: Example of Correct GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? A toy manufacturer receives an order for 400 toys. 5 workers are available to work on the order. 2 of the workers produce 6 toys an hour, and another 2 workers produce 4 toys an hour. They all work on the order during their 10 -hour shift, and by the end of their shift the manufacturer still needs another 20 toys to be able to ship the order. How many toys per hour does the fifth worker produce? Explain your reasoning. Your final answer should be a single numerical number, in the form 'boxed{answer}, at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: Let's first find the total number of toys that the four workers produce in 10 hours.
The two workers who produce 6 toys an hour make a total of $6 \times 2 \times 10=$ 120 toys.
The other two workers who produce 4 toys an hour make a total of $4 \times 2 \times 10$ $=80$ toys.
Together, they produce $120+80=200$ toys.
Since the manufacturer still needs another 20 toys to complete the order, we know that all 5 workers together need to produce a total of $400+20=420$ toys.</p>
<p>Subtracting the 200 toys that have already been produced, we're left with 420 $-200=220$ toys that need to be produced by the fifth worker.</p>
<p>Since the workers have worked for 10 hours, we can set up the equation:
Toys per hour for fifth worker $\times 10=220$
Solving for the toys per hour for the fifth worker, we get:
Toys per hour for fifth worker $=220 / 10=22$
Therefore, the fifth worker must produce 'boxed ${22}$ toys per hour.</p>
<h2>User Input: These are the solutions to the problem from other agents: One agent solution: <br> (Copy Agent 2's reply)</h2>
<p>Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 'boxed{answer}, at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: There is an error in the previous solution provided. The number of toys produced by the four workers in 10 hours is incorrect. The correct calculation should be:
2 (6 toys per hour) x 10 hours x 2 workers $=240$ toys
2 (4 toys per hour) x 10 hours x 2 workers $=160$ toys
Total toys produced by the four workers $=240+160=400$ toys
So, the manufacturer still needs to produce another $400-20=380$ toys. To produce the remaining 380 toys in 10 hours, all five workers need to produce $380 / 10=38$ toys per hour.</p>
<p>Since four workers are already producing 6 and 4 toys per hour, we can set up the equation:
$2(6)+2(4)+x=38$
Simplifying this, we get: $\mathrm{x}=$ ?
Therefore the fifth worker must produce 'boxed ${7}$ toys per hour.</p>
<p>Agent 2: I agree with the updated solution provided by the other agent. The total toys produced by the four workers in 10 hours is 400 , and since the manufacturer still needs to produce another 380 toys, all five workers need to produce 38 toys per hour.</p>
<p>Setting up the equation, with x being the number of toys produced by the fifth worker per hour, we get:
$2(6)+2(4)+x=38$
$12+8+x=38$
$\mathrm{x}=18$
Therefore, the fifth worker must produce 'boxed ${18}$ toys per hour.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem?
Please update and respond to other agents. Your final answer should be a single numerical number, in the form 'boxed{answer}, at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I believe there is an error in the updated solution provided by the other agents. The total number of toys produced by the four workers in 10 hours is indeed 400 , and since the manufacturer still needs to produce another 380 toys, all five workers need to produce 38 toys per hour.</p>
<p>Setting up the equation, with x being the number of toys produced by the fifth worker per hour, we get:
$2(6)+2(4)+x=38$
$12+8+x=38$
$\mathrm{x}=18 / 5$
Therefore, the fifth worker must produce 'boxed ${3.6}$ toys per hour.</p>
<p>Agent 2: I agree with the latest solution provided by the other agent. The total number of toys produced by the four workers is 400 , and the manufacturer still needs to produce another 380 toys. To produce the remaining 380 toys in 10 hours, all five workers need to produce 38 toys per hour.</p>
<p>Setting up the equation, with x being the number of toys produced by the fifth worker per hour, we get:
$2(6)+2(4)+x=38$
$12+8+x=38$
$\mathrm{x}=18 / 5$
Therefore, the fifth worker must produce 'boxed ${3.6}$ toys per hour.</p>
<p>Figure 21: Example of Incorrect GSM8K Debate.</p>
<p>User Input: Can you solve the following math problem? Courtney attended a concert and reported that the audience was 48 in number. However, Kelly went to the same concert and said that Courtney had made the mistake of overstating the number of people in attendance by $20 \%$. If Kelly was right, how many people really attended the concert? Explain your reasoning. Your final answer should be a single numerical number, in the form 'boxed [answer], at the end of your response.</p>
<h1>Round 1</h1>
<p>Agent 1: If Courtney overstated the number of people by $20 \%$, that means she reported $120 \%$ of the actual number of people. Let's call the actual number of people $x$. Then we can set up the following equation:
$1.2 x=48$
Solving for $x$, we get:
$\mathrm{x}=40$
Therefore, the actual number of people who attended the concert was 'boxed [40].</p>
<p>Agent 2: Let's call the actual number of people at the concert "x." Then, according to Kelly's statement, we know that:
$\mathrm{x}=48-0.20(48)$
$\mathrm{x}=48-9.6$
$\mathrm{x}=38.4$
Therefore, the actual number of people at the concert was $\$ /$ boxed [38]\$.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form 'boxed [answer], at the end of your response.</p>
<h2>Round 2</h2>
<p>Agent 1: The other agent's solution assumes that Courtney overstated the number of people by 20 percentage points, not 20 percent of the original number. This means their answer is incorrect.</p>
<p>Instead, we can use the method in the first solution. Let $x$ be the actual number of people at the concert. Then, we know that:
$\mathrm{x}=48 *(100 \%-20 \%)=48 * 0.8=38.4$
Therefore, the actual number of people at the concert was $\$ /$ boxed [38]\$.</p>
<p>Agent 2: I agree with the first agent's solution that we need to consider the $20 \%$ decrease from the original number of people. Let $x$ be the actual number of people at the concert, then we have:
$\mathrm{x}=48-0.20(48)$
$\mathrm{x}=48-9.6$
$\mathrm{x}=38.4$
Rounding to the nearest whole number, we get that the actual number of people at the concert was $\$ /$ boxed [38].\$</p>
<p>I also agree with the second agent's point that we should be careful in interpreting the $20 \%$, as it could be presented in percentage points or as a percent of the original number.</p>
<p>User Input: These are the solutions to the problem from other agents: One agent solution:
(Copy Agent 2's reply)
(Copy Agent 1's reply)
Using the solutions from other agents as additional information, can you provide your answer to the math problem? Please update and respond to other agents. Your final answer should be a single numerical number, in the form 'boxed [answer], at the end of your response.</p>
<h2>Round 3</h2>
<p>Agent 1: I agree with the previous agents that we need to consider the $20 \%$ decrease from the original number of people. Let $x$ be the actual number of people at the concert, then we have:
$\mathrm{x}=48 * 0.80=38.4$
Since we cannot have a fraction of a person, we round to the nearest whole number, giving us that the actual number of people at the concert was $\$ /$ boxed [38]. $\$$</p>
<p>I also agree with the second agent's point that we should be careful in interpreting the $20 \%$, as it could be presented in percentage points or as a percent of the original number. In this case, the wording suggests that it should be interpreted as a percent of the original number.</p>
<p>Agent 2: I agree with the previous agents that the $20 \%$ decrease should be calculated as a percentage of the original number of people. Let $x$ be the actual number of people at the concert, then we have:
$\mathrm{x}=48 * 0.80=38.4$
Since we cannot have a fraction of a person, we should round the answer to the nearest whole number. Therefore, the actual number of people at the concert was $\$ /$ boxed [38].\$</p>
<p>I also appreciate the second agent's point about being careful with the interpretation of the percentage, as it could be presented in percentage points or as a percent of the original number. In this case, it seems reasonable to assume that the $20 \%$ decrease refers to a percentage of the original number.</p>
<p>Figure 22: Example of Incorrect GSM8K Debate.</p>            </div>
        </div>

    </div>
</body>
</html>
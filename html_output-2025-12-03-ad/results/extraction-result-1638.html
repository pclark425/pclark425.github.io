<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1638 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1638</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1638</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-246063687</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2201.08355v4.pdf" target="_blank">Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees</a></p>
                <p><strong>Paper Abstract:</strong> Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See https://sites.google.com/princeton.edu/sim-to-lab-to-real for supplementary material.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1638.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1638.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim-to-Lab-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage sim-to-real transfer framework that (1) pre-trains a diverse, safety-aware dual-policy distribution in simulation (Sim), (2) safely fine-tunes the latent policy distribution in more realistic controlled environments (Lab) using shielding and PAC-Bayes regularization, and (3) deploys with probabilistic guarantees on performance and safety in Real environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Ghost Spirit quadrupedal robot (simulated counterparts: point-mass / circular approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A quadrupedal mobile robot used for ego-vision navigation; in simulation the robot is modeled as a point mass (Vanilla-Env) or a circular robot of radius 25 cm (Advanced-Env) for collision checking; commanded by forward and angular velocity; equipped with a forward RGB camera (ZED 2 in real robot).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics — vision-based mobile navigation (indoor)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Custom simulators: Vanilla-Env, Advanced-Env, Advanced-Realistic (3D-FRONT based)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Custom simulated indoor environments: (i) Vanilla-Env: simple undecorated rooms with randomized cylindrical/rectangular obstacles; (ii) Advanced-Env: furniture models and textures from 3D-FRONT with randomized placement and camera poses; (iii) Advanced-Realistic: professionally designed room layouts from 3D-FRONT to match Real deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>mixed: simplified physics/dynamics in Vanilla-Env, photorealistic rendering and realistic room layouts in Advanced-Realistic with system-identified dynamics approximations</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual appearance / textures and photorealistic room layouts (Advanced-Realistic), camera field-of-view and image resolution, randomized camera pose (tilt/roll), approximate robot geometry for collision (point or circle), commanded velocity-range dynamics, and dynamics approximated via system identification for matching real robot</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>simplified collision model (point mass or circular approximation), no high-fidelity contact dynamics or full robot joint-level dynamics in many sims, limited modeling of actuator delays/noise and low-level controller dynamics, approximated localization (used ground-truth or stereo localization in real tests), no full environment reset/complex human interactions</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real indoor rooms on Princeton Engineering Quadrangle with furniture configurations chosen to be similar to Advanced-Realistic Lab; deployment on Ghost Spirit quadruped with ZED 2 stereo camera providing onboard localization and RGB observations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based goal-directed indoor navigation: avoid collisions (safety) and reach a target region (task completion) using RGB observations and given distance/relative bearing to goal.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (modified off-policy Soft Actor-Critic) with a dual-policy setup (performance and backup policies), mutual-information based diversity objective for latent-conditioned policies, Hamilton-Jacobi (HJ) reachability-based safety critic, and PAC-Bayes posterior optimization in Lab stage.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Success rate (task completion), safety satisfaction (no collisions), number/ratio of safety violations during training, and PAC-Bayes lower bound on expected success/safety (probabilistic certificate).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Reported true-expected success in simulated test sets after Lab training (example): Advanced-Realistic (PAC Shield Perf) true-expected success 0.786; Advanced-Realistic safety 0.794 (paper reports multiple per-condition values).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Real robot test (10 environments, 3 trials each): PAC Shield Perf real robot success 0.767, real robot safety 0.867 (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized furniture placement and types, object geometries, wall/floor textures, camera tilt/roll and poses, randomized initial/goal locations, randomized control parameters in some Lab variants (e.g., dynamics: forward/angular velocity bounds).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>visual domain gap (lighting/textures), dynamics mismatch (actuator/differential dynamics), robot size/shape approximations, limited contact/contact dynamics, sensor/localization noise and inaccuracies, tight clearances in realistic rooms, and limited samples/environments for Lab training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Multi-stage strategy (pre-train diverse safe policies in Sim; fine-tune latent distribution in Lab), reachability-based safety critic and value-based shielding (least-restrictive override), domain randomization in Sim, photorealistic Lab environments (3D-FRONT), dynamics roughly matched by system identification, mutual-information-driven diversity in latent-conditioning, and PAC-Bayes regularization producing probabilistic generalization bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Authors emphasize need for photorealistic Lab-like environments and system-identified dynamics to model Real conditions; also require at least a few hundred Lab environments (N ≥ ~100, recommend hundreds to thousands) to obtain tight PAC-Bayes guarantees; no hard numeric physics-accuracy threshold given.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Fine-tuning was performed in Lab stage using Lab environments (simulated realistic rooms); the paper does not report additional online fine-tuning on the physical robot after deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Different environment fidelities were compared: Vanilla-Env (simple) vs Advanced-Env vs Advanced-Realistic. Advanced-Realistic plus shielding and PAC-Bayes produced much stronger generalization guarantees and better real-world performance (example: shielding improved Advanced-Realistic task bound from 0.366 to 0.786 in one comparison). Simplified sims without realistic visuals/dynamics required more Lab fine-tuning and produced weaker guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) A Sim-to-Lab-to-Real pipeline combining domain-randomized Sim pre-training, reachability-based shielding during Lab fine-tuning, and PAC-Bayes posterior optimization yields strong probabilistic guarantees and empirically successful sim-to-real transfer for vision-based quadruped navigation; 2) Reachability (HJ-derived) safety critics enable safer exploration and fewer safety violations (reductions of 4%–77% in Lab training and up to 38% in testing versus risk-based critics); 3) Conditioning performance policies on a latent distribution and maximizing diversity improves generalization, but diversity must be paired with shielding to prevent unsafe exploration; 4) Lab-stage fine-tuning in realistic environments and sufficient number of Lab environments (hundreds) are critical for tight guarantees and successful transfer; 5) No additional real-world fine-tuning was required for successful deployment in tested indoor rooms, but the method's guarantees assume Lab and Real environment distributions are similar.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees', 'publication_date_yy_mm': '2022-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>Closing the sim-to-real loop: Adapting simulation randomization with real world experience <em>(Rating: 2)</em></li>
                <li>Active domain randomization <em>(Rating: 1)</em></li>
                <li>Reinforcement learning with multi-fidelity simulators <em>(Rating: 1)</em></li>
                <li>Cad2rl: Real single-image flight without a single real image <em>(Rating: 1)</em></li>
                <li>Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1638",
    "paper_id": "paper-246063687",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Sim-to-Lab-to-Real",
            "name_full": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees",
            "brief_description": "A two-stage sim-to-real transfer framework that (1) pre-trains a diverse, safety-aware dual-policy distribution in simulation (Sim), (2) safely fine-tunes the latent policy distribution in more realistic controlled environments (Lab) using shielding and PAC-Bayes regularization, and (3) deploys with probabilistic guarantees on performance and safety in Real environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Ghost Spirit quadrupedal robot (simulated counterparts: point-mass / circular approximations)",
            "agent_system_description": "A quadrupedal mobile robot used for ego-vision navigation; in simulation the robot is modeled as a point mass (Vanilla-Env) or a circular robot of radius 25 cm (Advanced-Env) for collision checking; commanded by forward and angular velocity; equipped with a forward RGB camera (ZED 2 in real robot).",
            "domain": "general robotics — vision-based mobile navigation (indoor)",
            "virtual_environment_name": "Custom simulators: Vanilla-Env, Advanced-Env, Advanced-Realistic (3D-FRONT based)",
            "virtual_environment_description": "Custom simulated indoor environments: (i) Vanilla-Env: simple undecorated rooms with randomized cylindrical/rectangular obstacles; (ii) Advanced-Env: furniture models and textures from 3D-FRONT with randomized placement and camera poses; (iii) Advanced-Realistic: professionally designed room layouts from 3D-FRONT to match Real deployment.",
            "simulation_fidelity_level": "mixed: simplified physics/dynamics in Vanilla-Env, photorealistic rendering and realistic room layouts in Advanced-Realistic with system-identified dynamics approximations",
            "fidelity_aspects_modeled": "visual appearance / textures and photorealistic room layouts (Advanced-Realistic), camera field-of-view and image resolution, randomized camera pose (tilt/roll), approximate robot geometry for collision (point or circle), commanded velocity-range dynamics, and dynamics approximated via system identification for matching real robot",
            "fidelity_aspects_simplified": "simplified collision model (point mass or circular approximation), no high-fidelity contact dynamics or full robot joint-level dynamics in many sims, limited modeling of actuator delays/noise and low-level controller dynamics, approximated localization (used ground-truth or stereo localization in real tests), no full environment reset/complex human interactions",
            "real_environment_description": "Real indoor rooms on Princeton Engineering Quadrangle with furniture configurations chosen to be similar to Advanced-Realistic Lab; deployment on Ghost Spirit quadruped with ZED 2 stereo camera providing onboard localization and RGB observations.",
            "task_or_skill_transferred": "Vision-based goal-directed indoor navigation: avoid collisions (safety) and reach a target region (task completion) using RGB observations and given distance/relative bearing to goal.",
            "training_method": "Reinforcement learning (modified off-policy Soft Actor-Critic) with a dual-policy setup (performance and backup policies), mutual-information based diversity objective for latent-conditioned policies, Hamilton-Jacobi (HJ) reachability-based safety critic, and PAC-Bayes posterior optimization in Lab stage.",
            "transfer_success_metric": "Success rate (task completion), safety satisfaction (no collisions), number/ratio of safety violations during training, and PAC-Bayes lower bound on expected success/safety (probabilistic certificate).",
            "transfer_performance_sim": "Reported true-expected success in simulated test sets after Lab training (example): Advanced-Realistic (PAC Shield Perf) true-expected success 0.786; Advanced-Realistic safety 0.794 (paper reports multiple per-condition values).",
            "transfer_performance_real": "Real robot test (10 environments, 3 trials each): PAC Shield Perf real robot success 0.767, real robot safety 0.867 (reported in paper).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized furniture placement and types, object geometries, wall/floor textures, camera tilt/roll and poses, randomized initial/goal locations, randomized control parameters in some Lab variants (e.g., dynamics: forward/angular velocity bounds).",
            "sim_to_real_gap_factors": "visual domain gap (lighting/textures), dynamics mismatch (actuator/differential dynamics), robot size/shape approximations, limited contact/contact dynamics, sensor/localization noise and inaccuracies, tight clearances in realistic rooms, and limited samples/environments for Lab training.",
            "transfer_enabling_conditions": "Multi-stage strategy (pre-train diverse safe policies in Sim; fine-tune latent distribution in Lab), reachability-based safety critic and value-based shielding (least-restrictive override), domain randomization in Sim, photorealistic Lab environments (3D-FRONT), dynamics roughly matched by system identification, mutual-information-driven diversity in latent-conditioning, and PAC-Bayes regularization producing probabilistic generalization bounds.",
            "fidelity_requirements_identified": "Authors emphasize need for photorealistic Lab-like environments and system-identified dynamics to model Real conditions; also require at least a few hundred Lab environments (N ≥ ~100, recommend hundreds to thousands) to obtain tight PAC-Bayes guarantees; no hard numeric physics-accuracy threshold given.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Fine-tuning was performed in Lab stage using Lab environments (simulated realistic rooms); the paper does not report additional online fine-tuning on the physical robot after deployment.",
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Different environment fidelities were compared: Vanilla-Env (simple) vs Advanced-Env vs Advanced-Realistic. Advanced-Realistic plus shielding and PAC-Bayes produced much stronger generalization guarantees and better real-world performance (example: shielding improved Advanced-Realistic task bound from 0.366 to 0.786 in one comparison). Simplified sims without realistic visuals/dynamics required more Lab fine-tuning and produced weaker guarantees.",
            "key_findings": "1) A Sim-to-Lab-to-Real pipeline combining domain-randomized Sim pre-training, reachability-based shielding during Lab fine-tuning, and PAC-Bayes posterior optimization yields strong probabilistic guarantees and empirically successful sim-to-real transfer for vision-based quadruped navigation; 2) Reachability (HJ-derived) safety critics enable safer exploration and fewer safety violations (reductions of 4%–77% in Lab training and up to 38% in testing versus risk-based critics); 3) Conditioning performance policies on a latent distribution and maximizing diversity improves generalization, but diversity must be paired with shielding to prevent unsafe exploration; 4) Lab-stage fine-tuning in realistic environments and sufficient number of Lab environments (hundreds) are critical for tight guarantees and successful transfer; 5) No additional real-world fine-tuning was required for successful deployment in tested indoor rooms, but the method's guarantees assume Lab and Real environment distributions are similar.",
            "uuid": "e1638.0",
            "source_info": {
                "paper_title": "Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees",
                "publication_date_yy_mm": "2022-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Closing the sim-to-real loop: Adapting simulation randomization with real world experience",
            "rating": 2,
            "sanitized_title": "closing_the_simtoreal_loop_adapting_simulation_randomization_with_real_world_experience"
        },
        {
            "paper_title": "Active domain randomization",
            "rating": 1,
            "sanitized_title": "active_domain_randomization"
        },
        {
            "paper_title": "Reinforcement learning with multi-fidelity simulators",
            "rating": 1,
            "sanitized_title": "reinforcement_learning_with_multifidelity_simulators"
        },
        {
            "paper_title": "Cad2rl: Real single-image flight without a single real image",
            "rating": 1,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators",
            "rating": 1,
            "sanitized_title": "bayessim_adaptive_domain_randomization_via_probabilistic_inference_for_robotics_simulators"
        }
    ],
    "cost": 0.01265675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees
April 4, 2023</p>
<p>Kai-Chieh Hsu kaichieh@princeton.edu 
Department of Electrical and Computer Engineering
Princeton University
United States</p>
<p>Allen Z Ren allen.ren@princeton.edu 
Department of Mechanical and Aerospace Engineering
Princeton University
United States</p>
<p>Duy P Nguyen 
Department of Electrical and Computer Engineering
Princeton University
United States</p>
<p>Anirudha Majumdar 
Department of Mechanical and Aerospace Engineering
Princeton University
United States</p>
<p>Jaime F Fisac 
Department of Electrical and Computer Engineering
Princeton University
United States</p>
<p>Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees
April 4, 202388CBF662CEF4C05BA0A9D7447BE205FFarXiv:2201.08355v4[cs.RO]Preprint submitted to Artificial IntelligenceReinforcement LearningSim-to-Real TransferSafety AnalysisGeneralization
Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world.In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior.In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution.To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis.In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments.Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment.We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism.We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot.See https://saferoboticslab.github.io/SimLabReal/for supplementary material.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) techniques have been increasingly popular in training autonomous robots to perform complex tasks such as traversing uneven outdoor terrains [1] and navigating through cluttered indoor environments [2].Through interactions with environments and feedback in the form of a reward signal, robots learn to reach target locations relying on onboard sensing (e.g., RGB-D cameras).In order to achieve good empirical generalization performance in different environments, the robot needs to be trained in multiple environments and collect experiences continuously.Due to tight hardware constraints and high sample complexities of RL techniques, in most cases, the training is performed solely in simulated environments.</p>
<p>However, the robots' performance often degrades sharply when they are deployed in the real world, where there can be substantial changes in environments such as different lighting conditions and noise in robot actuation.This performance drop opens the need for research on Sim-to-Real transfer.The typical approach is to simulate a large number of environments with randomized properties and train the policy to work well across environments, with the expectation that real environments at deployment time will be well captured by the rich distribution of training variations.This technique, namely domain randomization, has helped bridge the Sim-to-Real gap substantially [3][4][5].In the field of visual navigation, conditions such as camera poses, scene layout, and wall textures can be randomized.However, previous Sim-to-Real techniques do not explicitly address safety of the robots.Usually, it is worth compromising the performance (e.g., success rate and time needed for reaching the target) to allow better safety of the system (e.g., avoiding dangerous collisions with humans or furniture).While safety violations are . of Real environments (Red triangles/rectangles).For example, we may first train using environments of randomized furniture configurations in Sim, and then fine-tune policies in realistic room layouts [6] before deploying in Real indoor spaces.Bottom: In Sim stage, Sim-to-Lab-to-Real trains a safety-aware dual policy conditioned on latent variable sampled from a distribution, and then safely fine-tunes the latent distribution in Lab stage to adapt to a specific environment distribution.Top right: Sample trajectory of a quadrupedal robot running trained policy in a real kitchen environment.The backup policy (Green arrow) overrides the performance policy (Red arrow) when the safety critic value (colored trajectory) exceeds some threshold, steering the robot away from obstacles.</p>
<p>inconsequential in simulation, robots trained without safety considerations will tend to exhibit similar unsafe behavior once deployed in real environments.Another drawback of these techniques is that they do not provide any guarantees on robots' performance or safety when they are deployed in different real environments.A "certificate" of robots' generalization performance and safety is necessary before they are deployed in safety-critical environments (e.g., households with children).</p>
<p>In this work, we explore an intermediate training stage between Sim and Real, which we call Lab, that aims to systematically bridge the Sim-to-Real gap by explicitly enforcing hard safety constraints on the robot and certifying the performance and safety before deployment in Real.The proposed Sim-to-Lab-to-Real framework is motivated by the conventional engineering practice whereby, before deploying autonomous systems in the real world, designers usually test them in a realistic but controlled environment, such as a test track for autonomous cars or testing warehouse facilities for quadrupeds at Boston Dynamics [7].This standard pipeline opens up an opportunity for autonomous systems to further improve performance and safety in the Lab stage.Our insight is that (1) in simulation, where environments can be easily randomized and data is easily collected, the robot can be trained in a wide range of environments and conditions; (2) after that, the robot needs to fine-tune in more specific environments before being deployed in similar ones in the real world; (3) this training stage can also certify the system before Real deployment, especially if the training can provide guarantees on its performance and safety in the real world.Through such extensive training and validation, we can deploy the system confidently in the real environments.Fig. 1 demonstrates the overview of the proposed Sim-to-Lab-to-Real framework.</p>
<p>Fine-tuning in the Lab stage differs from training in the Sim stage in that the Lab stage is more safety-critical.In other words, we want the autonomous system to safely explore in this stage to improve the performance.In order to realize safe Sim-to-Lab transfer, we need to consider both (1) how safety is formulated and (2) how safety is ensured throughout training.Typical approaches in safe RL combine the safety objective with the performance objective, including adding large negative reward when violating the safety constraints or minimizing the worst-case performance using conditional value at risk (CVaR) formulation [8,9].However, these methods do not attempt to explicitly enforce hard safety constraints and face a fragile balance between performance and safety.Specifically, these methods require handtuning the weights of different components in the objective function, which makes them often fail to generalize well to unseen tasks or environments.Our approach instead builds upon a dual policy setup, where a performance policy optimizes task reward and a backup (safety) policy keeps the robot away from failure conditions.We then apply a least-restrictive control law [10] (or shielding) with the safety state-action value function from the backup policy: the performance policy is only overridden by the backup policy when the safety state-action value function predicts that the proposed performance action would result in an inevitable safety violation in the future.The backup policy is pre-trained in the Sim stage and ready to ensure safe exploration once Lab training starts.Based on the safety policy training developed in [11,12] using Hamilton-Jacobi (HJ) reachability-analysis, our backup agent can learn from near-failure with dense signals.Unlike previous work that uses binary safety failure indicators [13,14], our training does not rely on experiencing safety violations, which enables the backup policy to be updated in safety-critical conditions.As we show in Sec.7.2.1, the number of safety violations is reduced by 4%-77% compared to previous safe RL work in different settings.</p>
<p>We also would like to provide "certificates" on the performance and safety of the robot after Lab training.However, this can be very challenging since typically it is not possible to fully specify the environment distribution where the robot is deployed (e.g., range of wind velocities for drone navigation, or minimum distance between obstacles for home robot navigation).Traditional techniques that provide performance guarantees for control policies, such as from robust control [15,16] and model-based reachability analysis [17,18], typically assume an explicit description of such uncertainty affecting the system (e.g., bound on actuation noise) and/or the environment.To tackle the challenge, we apply the Probably Approximately Correct (PAC)-Bayes Control framework [19][20][21], which provides lower bounds on the expected performance and safety when testing learned policies in unseen environments, while (1) not assuming explicit knowledge of the environment and (2) suited for systems with high-dimensional observations like vision.The framework also naturally fits our setup, as the two training stages of PAC-Bayes Control, prior and posterior, can be assigned to the Sim and Lab stages.As required by the PAC-Bayes setup, we train a distribution of policies by conditioning the performance (and backup) policy on latent variables sampled from a distribution.After training a prior policy distribution in Sim stage, we fine-tune the distribution in Lab, obtaining a posterior policy distribution and its associated generalization guarantee.While previous work in PAC-Bayes Control does not consider explicit policy architecture for safety, we now combine it with HJ reachability analysis and improve the generalization bounds for performance and safety by 40% (Sec.7.2.2).</p>
<p>Statement of Contributions</p>
<p>The primary contribution of this work is to propose Sim-to-Lab-Real, a framework that combines HJ reachability analysis and the PAC-Bayes Control framework to improve safety of robots during training and real-world deployment, and provide generalization guarantees on robots' performance and safety in real environments.Additionally, we make the following contributions:</p>
<p>• We propose an algorithm for concurrently training the performance policy that optimizes task reward and a backup policy that follows the Safety Bellman Equation (5) in Sec. 5. We introduce annealing parameters that allow gradual learning of performance and safety in the Sim stage.We also demonstrate that HJ-reachability RL can learn the safety state-action value function end-toend from images and enable safe exploration with a shielding scheme.</p>
<p>• We propose a modification of off-policy actor-critic algorithms that incorporates the policy distribution regularization from PAC-Bayes Control in Sec. 6.By constraining the KL divergence between the prior and posterior policy distribution in expectation with batch samples and a weighting coefficient, we optimize the generalization bound in the Lab stage efficiently.With a shieldingbased policy architecture, we are able to significantly improve the bound compared to previous PAC-Bayes Control works.</p>
<p>• We demonstrate the ability of our framework to reduce safety violations during training and improve empirical performance and safety, as well as generalization guarantees, compared to other safe learning techniques and previous work in PAC-Bayes Control in Sec. 7. We set up ego-vision navigation tasks in two types of environments including one with realistic indoor room layout and visuals.We also validate our approach and generalization guarantees with a quadruped robot navigating in real indoor environments (Sec.7.2.3).</p>
<p>Related Work</p>
<p>Safe Exploration.Ensuring safety during training has long been a problem in the reinforcement learning community.On one hand, the RL agent usually needs to experience failure in order to learn to be safe.On the other hand, being too conservative hinders exploring the state/action space sufficiently.Constrained MDP (CMDP) is a frequently used framework in safe exploration to satisfy constraints by changing the optimization objective to include some forms of risk [22].CMDP faces two main challenges: how to incorporate the safety constraints in RL algorithms and how to efficiently solve the constrained optimization problem.Chow et al. [9] use Lagrangian methods to transform the constrained optimization into an unconstrained one over the primal variable (policy) and the dual variable (penalty coefficient).</p>
<p>A recent line of works building on reachability analysis argues that optimizing the sum of rewards and penalties is not an accurate encoding of safety [12].Instead, they encode the safety specification of dynamical systems by finding the optimal safety value function, which is a solution to a Hamilton-Jacobi-Bellman/Isaacs variational inequality [23,24].With this safety value function, they apply a least-restrictive control law to shield any performance-oriented policy by overriding with an optimal safe action only when the agent is at states with critically low safety values [10,25].Cheng et al. [26] propose a similar shielding framework, utilizing the related control barrier function (CBF) concept.If the system dynamics are control-affine and a CBF is available, a smooth safety override can be computed efficiently by solving a quadratic program with a linear CBF constraint.However, these methods all assume that the dynamics and the environment are at least approximately known.Moreover, they also require that the reachability value function or CBF is available before the learning starts, which is non-trivial for high-dimensional dynamics and/or unknown environments.</p>
<p>To mitigate the curse of dimensionality and address generalization to novel environments, we build upon reachability RL [11,12], which finds an approximate safety value function.Recent methods in [13,14,27,28] address the safety problem by similar learning-based methods and shielding schemes as proposed in this work.However, the major differences lie in how the safety state-action value function, or safety critic, is trained and where the backup actions come from.Dalal et al. [27] assume the safety of systems can be ensured by adjusting the action in a single time step (no long-term effect).Thus, they learn a linear safety-signal model and formulate a quadratic program to find the closest control to the reference control such that the safety constraints are satisfied.Srinivasan et al. [13] and Thananjeyan et al. [14] learn the safety state-action value function from only sparse and binary safety labels.Srinivasan et al. [13] use this function to filter out the unsafe actions from the performance policy and resample actions until the backup agent deems the proposed actions safe, while Thananjeyan et al. [14] let the backup agent directly take over.The concurrent work [28] uses the same reachability RL to learn the backup agent.Our method is distinct in that (1) we propose the two-stage training to further reduce the safety violations in training and (2) we train the reachability RL end-to-end from images without pre-training the visual encoder.We compare our reachability critic with risk critic [13,14] as detailed in Sec.7.2.1.Our method reduces the number of safety violations by up to 77% in Lab training and 38% in testing.</p>
<p>A different line of works use learning-based methods to capture the residual error between the nominal model and the real dynamics, which results from model mismatch and/or uncertainties.Then, they combine learned models with model-based RL or model predictive control (MPC) to allow safe exploration.Berkenkamp et al. [29] use Gaussian process (GP) to estimate the performance of control parameters and they only deploy parameters that are predicted to be higher than a predefined threshold.On the other hand, Koller et al. [30] use GP to estimate the residual error and then utilize this model to over-approximate the forward-reachable set (FRS).They formulate a terminal constraint in MPC to only deploy policies whose FRS reaches a known control-invariant set (under some safety controllers).Liu et al. [31] learn the unknown residual with regression and quantify the residual error bound by formulating a covariance shift problem.Our method is distinct in that we use a model-free approach since we only assume we have high-dimensional measurements like RGB images, which cannot be easily handled with model-based methods.Secondly, we do not assume having access to a safe set a priori.</p>
<p>Generalization Theory and Guarantees.In supervised learning, generalization theory provides a principled guarantee on the true expected loss on new samples drawn from the underlying (but unknown) data distribution, after training a model using a finite number of samples.Foundational frameworks include Vapnik-Chervonenkis (VC) theory [32] and Rademacher complexity [33]; however the resulting bounds are generally extremely loose for neural networks.More recent approaches based on PAC-Bayes generalization theory [34] have provided non-vacuous bounds for neural networks in supervised learning [35,36].Majumdar et al [19] apply the PAC-Bayes framework in policy learning settings and provide generalization guarantees for control policies in unseen environments.Follow-up work has provided strong guarantees in different robotics settings including for learning neural network policies for vision-based control [21,[37][38][39][40]. Unlike supervised learning settings where picking a PAC-Bayes prior can be difficult, previous work in control settings has encoded different domain knowledge in the prior, such as diverse trajectories from human demonstrations [37].Our work also encodes diverse navigation strategies into the prior through maximum entropy learning [41].In addition, previous work has not adopted safetyrelated policy architectures nor considered safety during training.Combining PAC-Bayes theory with reachability safety analysis, we are able to provide stronger guarantees on performance and safety.</p>
<p>Safe Visual Navigation in Unseen Environments.Robot navigation has witnessed a long history of research [42], and many of the approaches have focused on explicit mapping of the environment combined with long-horizon planning in order to reach a goal location [43,44].Some recent works apply a map-less approach [2,45] or builds a map-like belief of the world [46] instead.They often take an end-to-end learning approach and start to tackle generalization to previously unseen environments.Similar to them, we train from pixels to actions, and use RGB images as the policy input without any depth information or mapping of the environment.Furthermore, we place more emphasis on the safety of the robot; we aim to train the robot avoiding any collision with obstacles and reaching some target location without the need of explicit mapping (e.g., initial and target locations can be in the same living room).There has been work that explicitly aims to improve safety of the navigating agent.A popular approach is to detect any novel environment or location (often using a neural network) and resort to conservative actions when novelty is detected [47,48].A slightly different approach is to estimate the uncertainty of the policy output and act cautiously when the policy is uncertain where to go [49,50].However, these work learn the notion of novelty and uncertainty purely from data, often in the form of binary signals, which can be sample inefficient and not generalizable to unseen domains.Closer to our work, there has been a line of work in applying Hamilton-Jacobi reachability analysis in visual navigation.Bajcsy et al [51] solves for the reachability set at each step but relies on a map generated using onboard camera.Li et al [52] proposes supervising the visual policy using expert data generated by solving a reachability problem.As detailed in the following section, our work also leverages reachability analysis but does not build a map of the environment nor relies on offline data generated by a different (expert) agent.</p>
<p>Adaptive Sim-to-Real Transfer.Directly applying policies trained in simulation to real environments can lead to bad performance and safety, and there has been work that adaptively bridges the Sim-to-Real gap.One line of work addresses the mismatch in robot and environment dynamics by explicitly searching for simulation parameters (e.g., mass, friction coefficient) that result in trajectories matching the real rollouts [53][54][55].Mehta et al. [56] propose active domain randomization that looks for simulation parameters that leads to different trajectories than reference ones, and those parameters are deemed important to train upon.A different approach [57] searches for simulation parameters by directly optimizing task reward in real environments without matching the dynamics.A work closer to ours is Multi-Fidelity RL by Cutler et al. [58], in which lower-fidelity environment (i.e., simulation) determines exploration heuristics for higher-fidelity environment (i.e., real world), and higher-fidelity environment learns model parameters for lower-fidelity environment.In a similar spirit, we learn safety-aware policy in lowerfidelity simulation for safer exploration in the Lab stage, where the policy distribution is fine-tuned.An important distinction of our work from previous ones is that we jointly address the Sim-to-Real gap in robot perception, environment configuration and dynamics.In addition, we provide probabilistic guarantees on the performance and safety of policies being deployed in real environments.</p>
<p>Problem Formulation and Preliminaries</p>
<p>We consider a robot with discrete-time dynamics given by
s t+1 = f E (s t , a t ),(1)
with state s ∈ S ⊆ R ns , control input a ∈ A ⊆ R na , and environment E ∈ E that the robot interacts with (e.g., an indoor space with furniture including initial and goal locations of the robot).Below we introduce the different conditions of the environments considered in the three stages.See Figure .1a for visualization.Environment -Sim.In the Sim stage, we assume there is a set of training environments M ⊂ E (e.g., synthetic indoor spaces with randomized arrangement of furniture),
M := {E 1 , E 2 , • • • , E N }.
There is no assumption on how M is distributed in E.</p>
<p>Environment -Lab.In the Lab stage, we are concerned with more specific conditions, and there can be different distributions of environments D 1 , D 2 , ... (e.g., office or home spaces, dimensions of the obstacles), with which the policies trained in Sim can be fine-tuned.We assume no explicit knowledge of each distribution D i ; instead, we assume there is a set of N i training environments drawn i.i.d.from D i available for the robot to train in; we denote these training datasets by
M i := {E 1 , E 2 , • • • , E Ni } ∼ D Ni i .
With a slight abuse of notations and for convenience, we consider a single target condition when introducing the rest of formulation and the approach, and denote the concerned distribution D, the training set M, and the number of training environments N .Environment -Real.In the Real stage, we assume the robot is deployed in environments from the same distribution D but unseen during the Lab stage.</p>
<p>Next we introduce the rest of problem settings including the robot sensor, the policy, and robot's task involving the reward function and the failure set.These settings hold the same for all three stages, except for the failure set which we do not require knowledge of at Real deployment.</p>
<p>Sensor.In all environments, we assume the robot has a sensor (e.g., RGB camera) that provides an observation o = h E (s) using a sensor mapping h : S × E → O. Task and Policy.Suppose the robot's task can be defined by a reward function, and let R E (π) denote the cumulative reward gained over a (finite) time horizon by a deterministic policy π : O → A when deployed in an environment E. We assume the policy π belongs to a space Π of policies.We also allow policies that map histories of observations to actions by augmenting the observation space to keep track of observation sequences.We assume R E (π) ∈ [0, 1], but make no further assumptions such as continuity or smoothness.We use ξ s,π E : [0, T ] × E → S to denote the trajectory rollout from state s using policy π in the environment E up to a time horizon T .Failure set.We further assume there are environment-dependent failure sets F E ⊆ S, that the robot is not allowed to enter.In training stages, we assume the robot has access to Lipschitz functions g : S × E → R such that F E is equal to the zero superlevel set of g E , namely, s ∈ F E ⇔ g E (s) ≥ 0. For example, g E (s) can be the signed distance function to the nearest obstacle around state s.Thus, g E (s) is called the safety margin function throughout the paper.</p>
<p>Goal</p>
<p>Our goal is to learn policies that provably generalize to unseen environments at the Real stage.This is very challenging since we do not have explicit knowledge of the underlying distribution D. We employ a slightly more general formulation where a distribution P over policies π ∈ Π instead of a single policy is used.In addition to maximizing the policy reward, we want to minimize the number of safety violations, i.e., the number of times that the robot enters failure sets.Our goal can then be formalized by the following optimization problem, which we would like to lower bound as the guarantee:
R := sup P ∈P R D (P ), where R D (P ) := E E∼D E π∼P R E (π) ,(2)R E (π) := R E (π)1 ∀t ∈ [0, T ], ξ s,π E (t) / ∈ F E ,(3)
where R E (π) ∈ [0, 1] denotes the task reward that does not penalize safety violation, P denotes the space of probability distributions on the policy space Π, and 1{•} is the indicator function.Here the task reward can be either dense (e.g., normalized cumulative reward) or sparse (e.g., reaching the target or not).</p>
<p>Generalization Bounds</p>
<p>Recently, PAC-Bayes generalization bounds have been applied to policy learning settings in order to provide formal generalization guarantees in unseen environments.We briefly introduce this framework here, as it will be used in our overall approach presented in Section 4. First it requires training a prior policy distribution P 0 , which we do in the Sim stage with the set of environments M .Then in the Lab stage, we fine-tune P 0 with environments M to obtain the posterior distribution P .Now, define the empirical reward of P as the average expected reward across training environments in M:
R M (P ) := 1 N E∈M E π∼P R E (π) .(4)
The following theorem can then be used to lower bound the true expected reward R D (P ).Theorem 1 (PAC-Bayes Bound for Control Policies; adapted from [19]).Let P 0 ∈ P be a prior distribution.Then, for any P ∈ P, and any δ ∈ (0, 1), with probability at least 1 − δ over sampled environments M ∼ D N , the following inequality holds:</p>
<p>R D (P ) ≥ R PAC (P, P 0 ) := R M (P ) − C(P, P 0 ), where C(P, P 0 ) := KL(P P 0 ) + log( 2
√ N δ ) 2N ,
and KL(P ||Q) stands for Kullback-Leibler (KL) divergence between probability distribution P and Q.</p>
<p>Maximizing the lower bound R PAC can be viewed as maximizing the empirical reward R M (P ) along with a regularizer C that prevents overfitting by penalizing the deviation of the posterior P from the prior P 0 .By fine-tuning P 0 to P and maximizing the bound in the Lab stage, we can provide a generalization guarantee for trained policies in unseen environments in the Real stage.</p>
<p>Remark 1.In exchange for assuming almost nothing about the environment distribution D and providing statistical guarantees that hold in arbitrarily high confidence (1 − δ) instead of only in expectation over sampled environments (e.g., conformal prediction [59]), the PAC-Bayes framework requires at least a few hundred Lab environments (N ≥ 100) to achieve reasonably tight generalization bounds.This requires substantial resources for training the policies in the Lab stage.In this work we use simulated environments for Lab training, but we envision that training in real environments is well scalable for industry practitioners with extensive training resources.Please refer to Sec. 8 for more discussion.</p>
<p>Method Overview</p>
<p>Our proposed Sim-to-Lab-to-Real framework bridges the reality gap with probabilistic guarantees by learning a safety-aware policy distribution.Fig. 2 shows the architecture of the safety-aware dual policy.It explicitly handles safety through the use of a shielding discriminator, which monitors the candidate actions from the performance policy and overrides them with backup actions only when deemed necessary.We condition the performance policy π p (and the backup policy π b and the shielding discriminator ∆ sh ) on a latent variable z sampled from some distribution, encoding different trajectories to follow (and different shielding strategies to take).With these tools, we divide the Sim-to-Real gap into two components, i.e., Sim-to-Lab and Lab-to-Real, which we tackle by a two-stage training pipeline as shown in Fig. 1.We show how to jointly train a dual policy conditioning on a latent distribution in Sec. 5.The details of Lab training and derivations of generalization guarantees are provided in Sec. 6.</p>
<p>For training, we use a proxy reward function r E : S × A × E → R, such as dense reward in distance to target, as a single-step surrogate for the task reward R E (π).Additionally, for every interaction with the environment, the robot receives a safety cost g E (s) (e.g., distance to nearest obstacle).We train both performance and backup policies with modifications of the off-policy Soft Actor-Critic (SAC) algorithm [60].We denote the neural network weights of the actor and the critic θ and w.We use superscripts (•) p and (•) b to denote critics, actors, and actions from the performance or backup agent.In order to parameterize the policy distribution, we condition the performance (and the backup) policy on a latent variable z ∈ R nz .We assume the latent variable is sampled from a multivariate Gaussian distribution with diagonal covariance as z ∼ N (µ, Σ), where µ ∈ R nz is the mean and Σ ∈ R nz×nz is the diagonal covariance matrix.For notational convenience, we denote σ ∈ R nz the element-wise square-root of the diagonal of Σ, and define ψ = (µ, σ), N ψ := N (µ, diag(σ 2 )).This parameterization enables our framework to quantify the difference between the policy distribution after Sim training and Lab training, by which we can use PAC-Bayes Control to give probabilistic guarantees.</p>
<p>Pre-Training a Diverse Dual Policy in Simulation</p>
<p>The goal of the first training stage is to train the dual policy jointly with the fixed latent distribution in simulation, where training is not safety-critical (safety violations are not restricted).In this training stage, we use the environment dataset M that contains environments that are not necessarily similar to those from the target environment distribution D. Similar to domain randomization techniques, we use environments and conditions with randomized properties, such as random arrangement of furniture in indoor space and random camera tilting angle on the robot.</p>
<p>In the following subsections, we first review how to learn a backup policy by reachability RL optimizing for the worst-case safety.Then, we propose a shielding scheme with physical meaning to override unsafe candidate actions proposed by the performance policy.Additionally, we incorporate information-theoretic objectives to induce diversity into the learned policy distribution, which helps with fine-tuning the policy distribution and achieve stronger generalization guarantees in the next training stage.Finally, we show how to jointly train two agents, performance and backup, to realize all the above-mentioned goals.</p>
<p>Safety through Reachability Reinforcement Learning</p>
<p>Failures are usually catastrophic in safety-critical settings; thus worst-case safety, instead of an average safety over the trajectory, should be considered.For training the backup policy, we incorporate tools from reachability reinforcement learning [11,12] and optimize the discounted safety Bellman equation (DSBE) as below,
Q b (o t , a t ) := (1 − γ)g E (s t ) + γ max g E (s t ), min at+1∈A Q b o t+1 , a t+1 ,(5)
where o t = h E (s t ) and γ is the discount factor.This discount factor represents how much attention the RL agent places on future outcomes: if γ is small, the RL agent only cares about "imminent danger", and as γ → 1, one recovers the infinite-horizon safety state-action value function.In the training, we initialize γ = 0.8 and gradually anneal γ towards 1 during the process.The safety state-action value function in (5) captures the maximum cost g E along the trajectory starting from s t with action a t assuming that the safest control input is applied at every instant thereafter.Thus, min at∈A Q(o t , a t ) &gt; 0 indicates that the robot is predicted to inevitably violate safety in the future if a t is taken.By utilizing this (annealed) DSBE, we have an exact encoding of the property we want our system to satisfy.The DSBE allows the backup agent to learn the safety state-action value function from near-failure executions, which significantly reduces failure events during training.Additionally, the DSBE enables the backup agent to update using a dense learning signal, which is suitable for the joint training of performance and backup agents.To our knowledge, this work demonstrates the first instance of reachability RL in fully end-to-end training with extremely high-dimensional inputs (RGB images), without the need for pre-training a vision encoder as in [28].</p>
<p>Shielding</p>
<p>We leverage a least-restrictive control law, i.e., shielding, to reduce the number of safety violations in both training and deployment.Suppose we have two policies: performance-pursuing policy π p and safety-pursuing (backup) policy π b .Before we apply a candidate action from the performance-pursuing policy, we use a shielding discriminator ∆ sh to check if it is safe.We replace the proposed action with the action from the backup policy if and only if that candidate action is deemed to result in safety violations in the future.The shielding criterion is summarized in (6).This ensures minimum intervention by the backup policy while the performance policy guides the robot towards the target [10,61].
π sh (o) = π p (o), ∆ sh (o, π p , Q b ) = 1 π b (o), otherwise .(6)
The safety value function learned by DSBE represents the maximum cost along the trajectory in the future if following the learned policy.If we define the safety margin function g E (s) to be the closest distance to the obstacles, then Q b (o, a) represents the closest distance of the robot to the obstacles in the future.Based on this, we propose a value-based shielding with the threshold having a physical interpretation, i.e., a margin from the failure.Once the robot receives the current observation o and uses performance policy to generate action π p (o), the backup policy overrides the action if and only if Q b (o, a p ) &gt; v thr .In other words, the shielding discriminator is defined as below
∆ sh (o, π p , Q b ) := 1 Q b (o, π p (o)) ≤ v thr .(7)
Fig. 3 shows an example of shielding that prevents applying unsafe actions from the performance policy (replace the red dotted lines with green dotted lines in the inset).We compare the safety state-action value function based on DSBE with ones by sparse safety indicators [13,14] in Sec.7.2.1 and Fig. 6; our approach affords much better safety during training and deployment.</p>
<p>Diversity through Maximization of Latent-based Mutual Information</p>
<p>During Sim training, we also maximize the diversity of robot behavior encoded by the latent distribution, which has shown in different work [37,41] to result in better performance after fine-tuning the distribution, which we perform in the Lab stage.Each latent variable is sampled from the distribution.As the policy is conditioned on the latent variable, it should lead to different trajectories around obstacles and towards the target (Fig. 3).With a single policy instead of a distribution, it is prone to overfit to some set of environments and fails to adapt in new environments (Fig. 11).</p>
<p>In order to distinguish the resulting trajectories using latent variables, we maximize mutual information between observations of trajectories ξ o and latent variables z, which can be lowered bounded by sum of mutual information between each observation and the latent variable I(ξ o ; z) ≥ T t=1 I(o t ; z) [62] (observation-marginal MI).We can further lower bound
I(O; Z) ≥ E z∼N ψ 0 ,o∼p(•|π sh ,z) [log q φ (z|o)] − E z∼N ψ 0 [log p(z)],
where the posterior p(z|o) is approximated by a learned discriminator q φ (z|o), parameterized by a neural network with weights φ [41].Intuitively, in order to make trajectories recognizable by the discriminator, the trajectories need to be diverse.Similar to [41], before updating the policies after sampling a batch of experiences, we augment the proxy reward by a weighted mutual information reward with coefficient β:
r aug (s t , a t , o t , z) = r(s t , a t ) + β log q φ (z|o t ) − log p(z) .(8)
This encourages the value function to assign higher reward to regions more recognizable by the discriminator.Concurrently, we train the discriminator by maximizing log q φ (z|o) with Stochastic Gradient Descent (SGD),
φ ← φ + ∇ φ E o log q φ (z|o) .(9)
As shown in Fig. 2, we can additionally condition backup policies with the latent distribution; the robot may avoid obstacles in different directions, and such skills might be beneficial when there is a distributional shift of obstacle placement and geometry in the Lab stage.</p>
<p>The backup agent can also depend on a latent variable.Since the backup agent can intervene at any state and condition on any latent, we instead optimize the conditional mutual information between action and latent given the current observation I(A; Z|O) (observation-conditional MI).We modify the backup policy training objective (5) as below
θ b * = arg min θ L(θ) := E o,z E a∼π θ (•|o,z) Q(o, a; z) − νI(A; Z|O),(10)
where the Q-function is now conditioned on a latent variable and ν is the coefficient balancing the safety cost and the diversity.Through derivations in Appendix A, we modify the SAC formulation and the backup actor is updated as,
θ ← θ − ∇ θ E (o,z)∼B,a∼π θ (•|o,z) Q(o, a; z) − ν log π θ (a|o, z) + ν log 1 n s ns i=1,zi∼p(z) π θ (a|o, z i ) , (11)
where B is the replay buffer.Intuitively, for specific action a given current observation o, we want it to have high probability for policy conditioned on a specific latent variable z and low probability for other latent variables {z i } sampled from the distribution.Note that when the backup agent is also conditioned on latent variable z, the shielding discriminator in (7) now becomes Q b (o, π p (o), z) ≤ v thr .While similar formulations have been explored in previous work [41,63,64] to achieve diverse trajectories/skills in RL, to our best knowledge, we are the first to consider a continuous distribution of latent variables instead of a discrete one.We find this brings difficulty in training, exacerbated by using robot observations instead of true states (e.g., ground-truth locations of the robot); nonetheless, we show effectiveness of such diversity-induced training in Sec.7.2.4.</p>
<p>Joint Training of Performance and Backup</p>
<p>Policies.Now we are ready to perform joint training of the dual policy.In the Sim stage, we fix the latent distribution to be a zero-mean Gaussian distribution with diagonal covariance N ψ0 , where ψ 0 = (0, σ 0 ).For each episode during training, we sample a latent variable z ∼ N ψ0 and condition the performance policy (and the backup policy) on it for the whole episode.The training procedure is illustrated in Algorithm 1.</p>
<p>Since we train both policies with modifications of the off-policy SAC algorithm, we can use transitions from actions proposed by either backup policy or performance policy.The transitions from both policies are stored in a shared replay buffer and are sampled at random to update the parameters of actors and critics for both performance and backup agents.At every step during training, the robot needs to select a policy to follow.We introduce a parameter ρ, the probability that the robot chooses an action proposed by the backup policy.We initialize ρ to 1, meaning that at the beginning, all actions are sampled from the backup policy.Our intuition is that the backup policy needs to be trained well before shielding mechanism is introduced in the training.We gradually anneal ρ to 0. Additionally, to realize a safe Sim-to-Lab transfer, we want the performance agent to be aware of the backup agent.Thus, we also apply shielding during Sim training.However, since the backup actor and critic may not be able to shield successfully in the beginning, we introduce a parameter , which is the probability that the shielding is activated at this time step.This parameter can be viewed as how much we trust the backup policy and to what extent we want it to shield the performance policy.We typically initialize to 0 and anneal it to 1 gradually.The influence of ρ and are further analyzed in Sec.7.2.4.</p>
<p>The details of updating the policies and the discriminator are shown in the Algorithm 2. Notice that while we train the backup policy π b using the executed action a sh t , the performance policy π p is trained using the originally proposed action a t ("action re-labeling").This ensures that the performance agent learns to associate its proposed action with the transition outcome, and avoids keeping proposing unsafe actions.</p>
<p>Algorithm 1 Joint training in simulator</p>
<p>Require: M , π p , π b , q φ , N ψ0 := N (0, σI), ρ = 1, = 0, γ = γ init 1: Sample E ∼ M and z ∼ N ψ0 , reset environment Same latent for whole episode 2: for t ← 1 to num prior step do</p>
<p>3:</p>
<p>With probability ρ, sample action a t ∼ π b (•|o t , z); else sample a t ∼ π p (•|o t , z)</p>
<p>4:</p>
<p>With probability , apply shielding a sh t = π sh (π b , Q b , o t , a t , z)</p>
<p>5:</p>
<p>Step environment r t , o t , s t+1 = f E (s t , a sh t )</p>
<p>6:</p>
<p>Save (o t+1 , o t , a t , a sh t , z, r t ) to replay buffer 7:</p>
<p>Update π p , π b , q φ See Algorithm 2 8:
Anneal ρ → 0, → 1, γ → 1 9:
if timeout or failure then 10:</p>
<p>Sample E ∼ M and z ∼ N ψ0 , reset environment Update π b to minimize g E (s) with modified SAC by ( 5) and ( 11) Observation-conditional MI 7: end for 8: for t ← 1 to num discrminator update do 9:</p>
<p>Sample batch {(o t , z)} from the replay buffer 10:</p>
<p>Update q φ with SGD (9) Observation-marginal MI 11: end for After the joint training, we obtain the trained dual policies π p and π b , and the latent distribution N ψ0 that encodes diverse solutions in the environments.We now fix the weights of the two policies, and consider the latent variable z also part of their parameterization.This gives rise to the space of policies Π := {π p z , π b z : O → A | z ∈ R nz }; hence, the latent distribution N ψ0 can be equivalently viewed as a distribution on the space Π of policies.In the next section, we will consider N ψ0 as a prior distribution P 0 on π and "fine-tune" it by searching for a posterior distribution P = N ψ , which comes with the generalization guarantee from PAC-Bayes Control.</p>
<p>Safely Fine-Tuning Policies in Lab</p>
<p>In the second training stage, we consider more safety-critical training environments such as test tracks for autonomous cars or indoor lab space, where the conditions can be more realistic and closer to real environments.After pre-training the performance and backup policies with shielding, the robot can safely explore and fine-tune the prior policy distribution P 0 in a new set of environments M sampled from the unknown distribution D. Leveraging the PAC-Bayes Control framework, we can provide "certificates" of generalization for the resulting posterior policy distribution P .</p>
<p>The PAC-Bayes generalization bound R PAC associated with P from Eq. (1) consists of two parts: (1) R M (P ), the empirical reward of P as the average expected reward across training environments in M (4), which can be optimized using SAC algorithm; (2) a regularizer C(P, P 0 ) that penalizes the posterior P for deviating significantly from the prior P 0 , C(P, P 0 ) := KL(P P 0 ) + log( 2
√ N δ ) 2N .(12)
Note that the only term in C(P, P 0 ) that involves P is the KL divergence term between P and P 0 .To minimize C(P, P 0 ), we modify the SAC objective to include minimization of the KL divergence term.Also, we consider stochasticity of the policy from the latent distribution instead of the policy network; this leads to removing the policy entropy regularization in SAC and adding a weighted KL divergence term to the actor loss: max
P E o,z E a∼π θ (•|o,z) Q p (o, a) − αKL(P, P 0 ),(13)
where α ∈ R is a weighting coefficient to be tuned.In practice, we find the gradient of the KL divergence term heavily dominates the noisy gradient of actor and critic, and thus we approximate the KL divergence with an expectation on the posterior:
max P E o,z E a∼π θ (•|o,z) Q p (o, a) − α log P (z) P 0 (z) . (14)
Below we show the algorithm for this stage of training.To avoid safety violations, we always apply value-based shielding to the proposed action, and continue to apply action-relabeling when updating P .</p>
<p>Algorithm 3 Safely fine-tuning the policy distribution</p>
<p>Require: M, π p , π b , P = P 0 1: Sample E ∼ M and z ∼ P , reset environment 2: for t ← 1 to num posterior step do 3:
Sample a t ∼ π p (•|o t , z) 4:
Apply value-based shielding a sh t = π sh (π b , Q b , o t , a t , z)</p>
<p>5:</p>
<p>Step environment r t , o t , s t+1 ∼ P(•|s t , a sh t )</p>
<p>6:</p>
<p>Save (o t+1 , a t , z, r t ) to replay buffer Action re-labeling 7:</p>
<p>Update P using SAC with weighted regularization ( 14)
8:
if timeout or failure then 9:</p>
<p>Sample E ∼ M, z ∼ P , reset environment 10:</p>
<p>end if 11: end for 12: return P 6.1.Computing the Generalization Bound.</p>
<p>After training, we can calculate the generalization bound using the optimized posterior P .First, note that the empirical reward R M (P ) involves an expectation over the posterior and thus cannot be computed in closed form.Instead, it can be estimated by sampling a large number of policies z 1 , ..., z L from P : RM (P ) := 1
N L E∈M L i=1 R E (π p,b zi )
, and the error due to finite sampling can be bounded using a sample convergence bound R M [65].The final bound R bound (P ) ≤ R D (P ) is obtained from R M and C(P, P 0 ) by a slight tightening of C PAC from Theorem 1 using the KL-inverse function [19].Please refer to Appendix A2 in [37] for detailed derivations.Overall, our approach provides generalization guarantees in novel environments from the distribution D: as policies are randomly sampled from the posterior P and applied in test environments, the expected success rate over all test environments is guaranteed to be at least R bound (P ) (with probability 1 − δ over the sampling of training environments; δ = 0.01 for all experiments in Sec. 7).Through reachability shielding during training and generalization guarantees for the resulting policies, we bridge the Lab-to-Real gap with a probabilistically guaranteed safety-aware policy distribution.</p>
<p>Experiments</p>
<p>Through extensive simulation and hardware experiments, we aim to answer the following questions: does our proposed Sim-to-Lab-to-Real achieve (1)</p>
<p>Environments</p>
<p>We evaluate the proposed methods by performing ego-vision navigation task in two types of environment.The first type (Vanilla-Env) consists of undecorated rooms of 2m × 2m with randomly placed cylindrical and rectangular obstacles of different dimensions and poses, and the robot needs to bypass them and the walls to reach a green door (a smaller circular region in front it) (Fig. 4a).A virtual camera is simulated with 120 degree field of view both vertically and horizontally, outputting RGB images of 48 × 48 pixels.We treat the robot as a point mass when checking collision.</p>
<p>The second type of environments (Advanced-Env) uses realistic furniture models from the 3D-FRONT dataset [6] (Fig. 4b); the robot needs to safely reach some target location (a smaller circular region) using given distance and heading angle towards the target.A virtual camera is simulated at the front of the robot with 72 degree field of view vertically and 128 degree horizontally (matching the ZED 2 camera used in Real deployment), outputting RGB images of 90×160 pixels.When checking collisions, we approximate the robot as a circular shape of radius 25cm, roughly the same as the quadrupedal robot in Real deployment.</p>
<p>For both types of environments, the control loop runs at 10Hz and the maximum number of steps is 200.The robot is commanded with forward velocity ([0.5, 1] m/s for performance policy and [0.2, 0.5] m/s for backup policy) and angular velocity ([−1, 1] rad/s for both policies).We use dense proxy reward that is proportional to the percentage of distance traveled between initial location and goal, and the safety signal is calculated as the minimum distance to obstacles and walls.Additionally, we assume the robot is given E (s) and ∆ E (s), distance and relative bearing to the goal.</p>
<p>For Sim training, we randomize obstacle and furniture configurations to cover possible scenarios as much as possible.We also randomize camera poses (tilt and roll angles) in Advanced-Env to account for possible noise in real experiments.Sim training uses 100 environments in Vanilla-Env and 500 environments in Advanced-Env.After Sim training, we can fine-tune the policies in different types of Lab environments.For Vanilla-Env, we consider:</p>
<p>• Vanilla-Normal: shares the same environment parameters as ones in the Sim stage.</p>
<p>• Vanilla-Dynamics: increases the lower bound of forward and angular velocity (more aggressive maneuvers).</p>
<p>• Vanilla-Task: adds an additional condition on success that the the robot needs to enter the target region with a yaw angle within a small range instead of 2π (no restriction) in the Sim stage.The robot may pass through the target region and re-enter it with the required yaw orientation.The robot knows the lower bound and the upper bound of the required yaw range.</p>
<p>and for Advanced-Env, we consider:</p>
<p>• Advanced-Dense: assigns a higher density of furniture in the rooms, resulting in smaller clearances between them.</p>
<p>• Advanced-Realistic: uses realistic room layouts (Fig. 4c) and associated furniture configurations from the 3D-FRONT dataset, which are similar to real environments.We perform Lab-to-Real transfer with policies trained in this Lab (Fig. 4d).More details about the dataset and room layouts can be found in Appendix C.</p>
<p>Policy</p>
<p>We parameterize the performance and backup agents with neural networks consisting of convolutional layers and then fully connected layers.The actor and critic of each agent share the same convolutional layers.In Vanilla-Env, a single RGB image is fed to the convolution layers, and the latent variable is appended to the output of the last convolutional layer before fully connected layers.In Advanced-Env, we stack 4 previous RGB images while skipping 3 frames between two images to encode the past trajectory of the robot.Then, the stacked images are concatenated with the first 10 dimensions of the latent variable by repeating each dimension to the image size.Rest of the dimensions is appended to the output of the last convolutional layer.In addition to the image observation, the actors and critics also receive two auxiliary signals E (s) and ∆ E (s), which are also appended to the output of the last convolutional layer.The details of neural network architecture and training can be found in Appendix B.</p>
<p>Baselines</p>
<p>We compare our methods to five prior RL algorithms that neglect safety violations (Base and PAC Base [19]) or address safety by reward shaping (RP and PAC RP) or use a separate safety agent (SQRL [13] and Recovery RL [14]).Sim-to-Lab-to-Real varies from SQRL and Recovery RL in that the latter trains the safety critic by the sparse safety indicators as below,
Q b (o t , a t ) := I E (s t ) + γ 1 − I E (s t ) min at+1∈A Q b o t+1 , a t+1 ,
where I E (s t ) = 1{g E (s t ) &gt; 0} is the indicator function of the safety violations.In Sim-to-Lab-to-Real, the safety state-action values represent the robot's closest distance to the obstacles in the future, while in Recovery RL and SQRL, the values represent the probability that the robot will hit the obstacle (but the probability strongly depends on the discount factor used).The major distinction between Sim-to-Lab-to-Real and PAC-Bayes control is that the latter does not handle the safety explicitly but instead hopes to use diverse policies and fine-tuning to prevent unsafe maneuver.We give a brief description of these methods below and summarize the similarities and differences in Table 1.</p>
<p>• Sim-to-Lab-to-Real (ours): trains a distribution over dual policies conditioned on latent variables with guarantees on generalization to novel environments.We present two variants: PAC Shield Perf, whose performance policy is conditioned on latent variables, and PAC Shield Both, whose both performance and backup policies are conditioned on latent variables (Fig. 2).</p>
<p>• Shield (ours): trains a dual policy without conditioning on latent variables, thus no distribution over policies nor generalization guarantees.</p>
<p>• PAC-Bayes Control [19]: trains a distribution over policies conditioned on latent variables that optimizes for either only task reward (PAC Base) or reward with penalty (PAC RP).</p>
<p>• Base: trains a single policy that optimizes the task reward only.</p>
<p>• Reward Penalty (RP): trains a single policy but augments the task reward with penalty on safety violations, rE (s, a) = r E (s, a) − λ1{g E (s) &gt; 0}.</p>
<p>• Safety Critic for RL (SQRL) [13]: trains a dual policy.The backup critic optimizes the Lagrange relaxation of CMDP, Ĵ(π) = J(π)
+ νE a∼π [(v thr − Q b (o, a)],
with a rejection sampling method that re-samples action if Q b (o, a) &gt; v thr .</p>
<p>• Recovery RL [14]: trains a dual policy.The backup critic is trained in the same method as SQRL, but the backup action is from the backup actor instead of being re-sampled from performance policy.</p>
<p>Results</p>
<p>We compare all the methods by (1) safety violations in Lab training and (2) success and safety at deployment (Figure 5).We calculate the ratio of number of safety violations to the number of episodes collected during training.For deployment, we show the percentage of failed trials (solid bars in Figure 5) and unfinished trials (hatched bars).We summarize the main findings below:</p>
<p>Methods</p>
<p>Dual Policy Safety Treatment Generalization Guarantees</p>
<p>Sim-to-Lab-to-Real (ours) (Reachability safety critic) Shield (ours) (Reachability safety critic) PAC+Base [19] PAC+Reward Penalty [19] (Reward with safety penalty) Base Reward Penalty (Reward with safety penalty) Safety Critic for RL [13] (Risk safety critic) Recovery RL [14] (Risk safety critic) First, it showcases the benefits of using a shielding scheme in contrast with Base, RP and vanilla PAC-Bayes.Second, reachability safety critic enables safer exploration and safety satisfaction in deployment as compared to SQRL and recovery RL.Additionally, Sim-to-Lab-to-Real has lower unsuccessful ratio in deployment than Shield, which shows a diverse but safe policy distribution not only provides a generalization bound but also improves the empirical performance to novel environments.</p>
<ol>
<li>Across Lab training, our proposed Sim-to-Lab-to-Real (PAC Shield Perf and PAC Shield Both) achieves the fewest safety violations.This demonstrates the efficacy of the reachability safety state-action value function for shielding.Compared to the risk-based safety critics in SQRL [13] and Recovery RL [14], our safety critics can learn from near-failure and with dense cost signals, as discussed in 7.2.1.Adding penalty in the reward function does not reduce safety violations significantly.2. In testing environments, Sim-to-Lab-to-Real achieves the lowest unsuccessful fraction of trajectories (solid bars plus hatched bars).This indicates that training a diverse and safe policy distribution achieves better generalization performance to novel environments.Sim-to-Lab-to-Real also achieves the fewest safety violations (solid bars) at test time.This suggests that explicitly enforcing hard safety constraints improves the safety not only in training but also in testing.In Sec.7.2.2 we show stronger generalization guarantees (for both performance and safety) compared to PAC-Bayes baselines.3.In Sec.7.2.3 we show Sim-to-Lab-to-Real achieves the best performance and safety among baselines when the policies are deployed on a quadrupedal robot navigating through real indoor environments.Figure 6: 2D slices of safety state-action value functions when the robot is facing to the right: we train the safety critic using RL modified from Hamilton-Jacobi reachability analysis ("Reachability"), while SQRL and recovery RL train it with sparse binary indicators ("Safety Indicator").Reachability safety critic better captures the unsafe regionthere is a more gradual change in safety value near the obstacles (from blue to red color, lower to higher value), indicating that the robot is getting closer to the obstacles.In contrast, risk-based critic shows a more binary separation between safe and unsafe regions, leaving the robot little room and time to steer away from obstacles.The unsafe regions are also thinner than those learned with reachability.Thus, our reachability-based approach achieves fewer safety violations during both training and deployment.</li>
</ol>
<p>The empirical performance and safety also validate the theoretical generalization guarantees from PAC-Bayes Control.4. In Sec.7.2.4 we show that high diversity of trajectories from the latent distribution results in better generalization at test time.Without diversity maximization in Sim training, the resulting trajectories can concentrate close to a single one and hinder downstream fine-tuning in Lab.However, we also find that in Advanced-Env, PAC Base and PAC RP (distribution over policies) perform worse than Base and RP (single policy).We find that high diversity without shielding may hinder training progress due to frequent safety violations interfering with strategy exploration.5. We find that adding latent distribution to the backup policy introduces difficulty during Sim training, and leading to similar, if not worse, performance and safety at test time.We suspect that PAC Shield Both would take more samples to converge well in training and requires more careful tuning of hyperparameters.Following discussions focus on results of PAC Shield Perf, in which only the performance policy is conditioned on the latent variable.6.Compared to other Labs, violation ratios in Advanced-Realistic tend to be higher, although our methods still reduce safety violations by 20-25%.Also, there are few unfinished trials at test time (the robot neither reaches the target nor collides with obstacles).Given the tight spacing in realistic indoor environment (Fig. 6b), the non-trivial dimensions of the quadruped robot, and the complex visuals, the backup policy can fail to ensure safety in some environments.</p>
<p>Reachability vs. Risk-Based Safety Critic</p>
<p>Sim-to-Lab-to-Real and previous safe RL methods differ in the metric used to quantify safety and train the backup agent.By utilizing reachability RL, we have an exact encoding of the property we want our system to satisfy, i.e., the distance should be no closer to obstacles than a specific threshold.In contrast, SQRL and Recovery RL define safety by the risk of colliding with obstacles in the future and use binary safety indicators.We argue that risk-based threshold can easily overfit to specific scenarios since the probability heavily depends on the discount factor used.In addition, reachability objective allows the backup agent to learn from near failure, while the risk critic in SQRL and Recovery RL needs to learn from complete failures.Fig. 6 shows 2D slices of the safety state-action values in both environment settings.Reachability critics provide thicker unsafe regions next to obstacles, while risk-based critics fail to recognize many unsafe regions or consider unsafe only when very close to obstacles.Among different Lab setups, compared to the baselines, our method reduces safety violations by 77%, 4%, 76%, 62%, and 23% in training and 38%, 26%, 54%, 34%, and 28% in deployment.</p>
<p>Sensitivity analysis: value threshold.Through experiments, we find the value threshold used in  Policies trained with v thr = 0 also performs the worst at test time, which indicates that less shielding during training makes the robot learn unsafe or aggressive maneuver.Next we evaluate how the value threshold affects robot trajectories at test time.Fig. 7 shows the trajectories using different thresholds in the two settings.Small threshold leads to robot passing very closely next to obstacles, while a bigger threshold leads to more conservative behavior.We also would like to highlight the challenges of learning safe policies in Advanced-Env.As shown in the figure, with v thr = −0.15 the robot avoids the first obstacle, and then the backup policy steers the robot away from the target, potentially deeming the clearance next to the target not sufficient.However, this brings the robot near the wall, and due to imperfect training of the backup actor, the robot fails to escape.With tight spacing and large dimensions of the robot in Advanced-Env, we find the backup agent more difficult to train, and the final test performance and safety can be sensitive to the shielding threshold.In Advanced-Realistic, average test success rate with v thr = −0.05,−0.1, −0.15 are 0.678, 0.786, and 0.762 respectively.Future work could look into adapting the threshold after short experiences in different environments online.</p>
<p>Generalization Guarantees</p>
<p>In this subsection, we evaluate the PAC-Bayes generalization guarantees obtained after Lab training, and the effect of adding reachability shielding in the policy architecture to the bounds.Table 2 shows the bounds and test results on safety (not colliding with obstacles) and success (safely reaching the goal) among Lab training.The true expected success and safety are tested with environments that are similar to the Lab training environments (of the same distribution) but unseen before.In all settings, the true expected success and safety are higher than the bound in all settings, which validates the guarantees derived using PAC-Bayes Control.Furthermore, we compare the bound trained using PAC Shield Perf with previous PAC-Bayes Control method (PAC Base) in the Vanilla-Env and Advanced-Realistic.With shielding, the generalization bound improves in all settings.In the difficult setting of Advanced-Realistic, the bound improves from 0.366 to 0.786 for task completion and from 0.367 to 0.794 for safety satisfaction.Thus, explicitly enforcing hard safety constraints not only improves empirical outcomes but also provides stronger certification to policies in novel environments.In Sec.7.2.3 we also demonstrate empirical results of physical robot experiments validating the guarantees.Sensitivity analysis: weight of policy distribution regularization (α).When optimizing the generalization bound ( 14), we place a weighting coefficient α to balance gradients of the training reward and of the estimated KL divergence between the prior and posterior policy distribution, P 0 and P .Here we study the effect of using different values of α in the generalization bound and test performance.Sensitivity analysis: number of Lab environments (N ).Thm. 1 indicates the PAC-Bayes bound depends on the number of environments used in the Lab training.Fig. 8d demonstrates that in Vanilla-Env, N does not have a significant effect on training safety violations and test performance.We suspect that training in Vanilla-Env does not require a large number of environments for generalization.In the more difficult Advanced-Dense, with the same α = 2, higher N = 1000 achieves the best test success (0.703) and safety (0.709) compared to smaller N = 100 and N = 500 (Table .2), which demonstrates that a higher number of Lab environments help fine-tuning the policies achieving strong generalization in complex environments.</p>
<p>Physical Experiments</p>
<p>To demonstrate empirical performance and safety of trained policies in real environments (Lab-to-Real transfer) and verify the generalization guarantees, we evaluate the policies in real indoor environments in the Engineering Quadrangle building at Princeton University.We deploy a Ghost Spirit quadrupedal robot equipped with a ZED 2 camera at the front (Fig. 4d), matching the same dynamics and observation model used in Advanced-Realistic Lab.For the distance and relative bearing to the goal, before each trial the robot is given the ground-truth measurement at the initial location, and then it uses the localization algorithm native to the stereo camera to update the measurement at each step.</p>
<p>We pick ten different locations with furniture configurations and difficulty similar to those in Advanced-Realistic Lab.Based on test results after Lab training, we run policies trained with PAC Shield Perf (best performance overall), PAC Base (PAC-Bayes baseline with low generalization guarantees), and SQRL (best overall among other baselines).Each policy is evaluated at one environment 3 times (30 trials total).The results are shown in Table .2. Our policy is able to achieve the best performance (0.767) and safety (0.867), validating the theoretical guarantees from PAC-Bayes Control.The upper-right of Fig. 1 shows a trajectory when running policies trained with PAC Shield in a kitchen environment where the backup policy and the shielding discriminator help the robot avoid hitting the obstacles and reach the successfully.</p>
<p>Fig. 9 shows the 10 real environments and robots' trajectories when running policies trained with PAC Shield Perf.Green dots indicate shielding in effect, which is activated often near obstacles.The first and third images on top of the figure show the robot's view when shielding successfully guides robot away from the sofa stool and the cabinet.In the second environment, the backup policy keeps shielding the robot away from center of the room with value threshold v thr = −0.10,and all three trials ended as unfinished.This is possibly due to the cluttered scene of desks at the top half of the observation.We also test with small value threshold v thr = −0.05during experiments, and the robot is able to reach the target without shielding always activated.This highlights the need for adapting the shielding value threshold online in future work.</p>
<p>Other Studies</p>
<p>Ablation Study: importance of two-stage training.We evaluate the significance of Lab training by testing the prior policy distribution (without fine-tuning in Lab) in Vanilla-Env.Without Lab training, the unsuccessful ratio in deployment increases by 16%, 8% and 14%.This suggests that Lab training is essential to policies adapting to real dynamics and new environment distributions.Additionally, we test the importance of Sim training with Shield (no policy distribution).Without Sim training, the safety violations in Lab training increases by 60%, 11% and 65%.This demonstrates that Sim training enables the backup agent to monitor and override unsafe behavior from the beginning of Lab training.</p>
<p>Sensitivity analysis: the probability of sampling actions from the backup policy (ρ) and the probability of activating shielding ( ).One of the main contributions of our work is the effective joint training of both performance and back agents (realized in Sim training).The two parameters, ρ and , directly affect the exploration in Sim training.With high ρ or high , the RL agent basically only explores conservatively within a small safe region.However, in the beginning of the training, we should allow the RL agent to collect diverse state-action pairs.On the other hand, we also gradually anneal ρ → 0 and → 1 since we want the performance policy to be aware of the backup policy.In other words, the performance policy is effectively in shielded environments towards end of Sim training.Fig. 10 shows the Sim training progress under different ρ and scheduling.With constant ρ = 0 or = 0, the number of safety violations is much higher than that with both parameters annealing.Even worse, = 0 results in the number of safety violations increase at constant speed and the training success fluctuates significantly.On the other hand, with ρ = 1 or = 1, the number of safety violations is only half as that with both parameters annealing.However, this is at the expense of exploration and leads to worse success rate in deployment.In Vanilla-Env ρ = 1 leads to very poor training success.Although in Vanilla-Env = 1 does not have significant effect on training success, in the Advanced-Env, insufficient exploration hinders training progress.Also note that Sim training is not safety-critical and we do not aim to reduce safety violations then.</p>
<p>Sensitivity analysis: diversity-induced Sim training.We argue that training a diverse and safe policy distribution helps improve safety and performance in novel environments.There are two hyperparameters in our algorithm affecting the diversity, i.e., augmented reward coefficient β and latent dimension n z .Fig. 8a and Fig. 8b show the violation ratio in Lab training and unsuccessful ratio in testing under different (β, n z ) choices.We find that training without augmented reward (β = 0) results in the lowest violation ratio; however, the unsuccessful ratio in testing is the highest.In fact, we observe that with β = 0, rollout trajectories conditioned on different latent variables almost converge to a single trajectory as shown in Fig. 11.This reflects why safety is better satisfied but at the expense of generalization.On the other hand, when the coefficient is sufficiently large (β = 2), the policy distribution becomes diverse and generalizes well to unseen testing environments.Note that even with high diversity, safety can still be well ensured with shielding.For the second source of diversity, our proposed Sim-to-Lab-to-Real is robust to different latent dimension.</p>
<p>Conclusion</p>
<p>In this work, we propose the Sim-to-Lab-to-Real framework that combines Hamilton-Jacobi reachability analysis and PAC-Bayes generalization guarantees to bridge the Sim-to-Real reality gap with a probabilistically guaranteed safety-aware policy distribution.Joint training of a performance and a backup policy in Sim training (1st stage) enables a safety-aware exploration during Lab training (2nd stage).By optimizing the generalization bounds in Lab training, our approach is able to probabilistically certify robot performance and safety before deployment.We demonstrate significant reduction in safety violations in training and stronger performance and safety during test time.Results from experiments with a quadrupedal robot in real indoor space validate the theoretical guarantees.</p>
<p>Discussion: Environment distribution.</p>
<p>As elaborated in Sec. 3, the generalization guarantees obtained through our framework assumes no distribution shift between Lab and Real in terms of environments.To bridge the discrepancy, we model the real environments by using (1) photorealistic dataset of indoor room layouts and furniture models and (2) dynamics from system identification of the real robot and camera poses.Additionally, we note that previous works in PAC-Bayes Control [19,21,37] have consistently shown real deployment validating the bounds.Even under a slight of shift in distribution, we believe that a certificate of performance and safety is useful and provides confidence for deploying the system.We acknowledge that one limitation of our framework is that, in exchange for assuming close to nothing about the environment distribution and providing statistical guarantees that hold in arbitrarily high confidence instead of in expectation only (e.g., conformal prediction [59]), we require at least a few hundred environments for "Lab" training to achieve tight PAC-Bayes generalization guarantees (e.g., &lt; 10% difference between empirical performance and theoretical guarantee), which means performing "Lab" training with real conditions can be difficult for us researchers in university labs with limited hardware, computation, and human resources.In this work, we resort to performing "Lab" training in realistic simulated environments.</p>
<p>Nonetheless, we envision that our framework is well suited for industry practitioners who have access to either extensive training facilities (e.g.Google's robot "farms" [66], Boston Dynamics' testing warehouse [7]), large-scale distributed systems (e.g.Amazon's warehouses [67]), or vast amounts of "Lab-like" data collection (e.g.Cruise and Waymo's thousands-millions of test driver miles [68]).For these practical and often safety-critical applications, our framework can improve safety during training and provide generalization guarantees for performance and safety at deployment.For university labs achieving similar scales of data collection and training, it would be promising to explore (1) crowdsourcing robots training across labs [69] and (2) mechanisms for automatically resetting the robot [70] and randomizing the environments.</p>
<p>On the theoretical front, first it would be worth identifying the most representative environments for training (e.g., using coresets [71]).PAC-Bayes guarantee holds as long as the policies are "evaluated" in the training environments M and the training reward R M is evaluated.We could potentially obtain similar tight generalization guarantees by training on a much smaller set of environments compared to M used in this work.Second, recent growing interest in PAC-Bayes bound [72] and other types of generalization guarantees [73] could lead to tighter and also more sample-efficient bounds for certifying the generalization performance and safety.for better view here, the virtual camera is placed at a higher location than the robot.</p>
<p>Figure 1 :
1
Figure 1: Overview of the Sim-to-Lab-to-Real framework.Top Left: During Sim stage, we train robot policies in a wide variety of environments and conditions M (Blue circles).Then the same policies from Sim can be finetuned in different, more specific settings M 1,2,... (Green triangles/rectangles) during Lab training, which are in the same distribution D 1,2,.. of Real environments (Red triangles/rectangles).For example, we may first train using environments of randomized furniture configurations in Sim, and then fine-tune policies in realistic room layouts[6] before deploying in Real indoor spaces.Bottom: In Sim stage, Sim-to-Lab-to-Real trains a safety-aware dual policy conditioned on latent variable sampled from a distribution, and then safely fine-tunes the latent distribution in Lab stage to adapt to a specific environment distribution.Top right: Sample trajectory of a quadrupedal robot running trained policy in a real kitchen environment.The backup policy (Green arrow) overrides the performance policy (Red arrow) when the safety critic value (colored trajectory) exceeds some threshold, steering the robot away from obstacles.</p>
<p>Figure 2 :
2
Figure 2: Architecture of the safety-aware policy distribution: we consider a dual policy setup where the performance policy πp (and backup policy π b , optionally) is conditioned on latent variables sampled from a distribution encoding diverse behavior.The safety state-action value function Q b (ot, at) from the backup policy is used as the shielding discriminator ∆ sh , which determines whether the proposed action by the performance policy, ap, is safe.The action from the backup policy, a b , overrides only if necessary.</p>
<p>Figure 3 :
3
Figure 3: Rollout trajectories of the safety-aware policy distribution: the latent variables sampled from the distribution induce a diverse exploration motives and value-based shielding manages to override the unsafe actions.Red dashed line shows the unshielded actions; Black/Blue dotted lines show the safe actions by the performance policy; Green lines show the backup actions overriding unsafe actions.The inset shows safety values Q(o, π b (o)) with the observation o taken when the heading angle fixed to the one at time instant t sh .</p>
<p>lower safety violations during Lab training compared to other safe learning methods, (2) stronger generalization guarantees on performance and safety compared to previous work in PAC-Bayes Control, and (3) better empirical performance and safety during deployment compared to all baselines?We also evaluate (a) the relative importance of the Sim stage and Lab stage, (b) how the value threshold in shielding affects safety and efficiency, (c) how the regularization weight in (14) affects generalization guarantees and empirical performance after training, (d) how the two annealing parameters during Sim training, and ρ, affect training performance, and (e) how diversity components, mutual information maximization during Sim training and latent dimension, affect performance after Lab training.</p>
<p>Figure 4 :
4
Figure 4: Samples of environments used in experiments: (a) Sim training in Vanilla-Env; (b) Sim training in Advanced-Env; (c) Advanced-Realistic training; (d) physical robot deployment.</p>
<p>Figure 5 :
5
Figure 5: Comparison of safety violations during Lab training and unsuccessful trials at test time: Sim-to-Lab-to-Real (PAC Shield Perf and PAC Shield Both) has the lowest safety violations in both training and deployment.First, it showcases the benefits of using a shielding scheme in contrast with Base, RP and vanilla PAC-Bayes.Second, reachability safety critic enables safer exploration and safety satisfaction in deployment as compared to SQRL and recovery RL.Additionally, Sim-to-Lab-to-Real has lower unsuccessful ratio in deployment than Shield, which shows a diverse but safe policy distribution not only provides a generalization bound but also improves the empirical performance to novel environments.</p>
<p>(a) Lab: Vanilla-Normal (b) Lab: Advanced-Realistic</p>
<p>Figure 7 :
7
Figure7: Rollout trajectories using different value threshold for shielding: higher threshold (more negative) results in more conservative maneuver, i.e., keeping farther away from obstacles (purple in (a) and grey in (b)).In Advanced-Env, the complex visuals and tight spacing cause challenges in learning the backup agent.We tend to find a relatively conservative threshold (v thr = −0.10)works well in practice, and too high threshold can prevent the robot from reaching the goal and accidentally steer it towards tight space.</p>
<p>Figure 8 :
8
Figure 8: Sensitivity analyses: we study the influence of different hyper-parameters to Sim-to-Lab-to-Real.The results are averaged over 5 seeds in Vanilla-Env.If not specified, the hyper-parameters default to β = 2, nz = 20, α = 1, N = 1000, and v thr = −0.05,as shown in blue.Results suggest the augmented reward in Sim training and the value threshold in shielding are the two most important hyper-parameters.</p>
<p>Figure 9 :
9
Figure 9: Environments for physical robot experiments and robot trajectories/observations with PAC Shield Perf: we run the policy three times in each environment by sampling different latent variables from the posterior distribution.The three numbers in images indicates success/unfinished/failure split.Green dots indicates shielding in effect.Green star indicates success in reaching the target.Red star indicates colliding with obstacles.We scan the environment using an iPad Pro tablet before experiments to generate the 2D map (which the robot does not have access to).The robot trajectory is obtained using localization algorithm of the onboard camera, and is inaccurate at places (intersecting obstacles; not exactly reaching the target but the robot deems so, which we consider success).</p>
<p>Fig. 8c shows</p>
<p>Fig. 8c shows that too strong regularization (α = 10) prevents the Lab training from tuning the prior distribution sufficiently, resulting in worse testing performance after training.The effect of different α is more prominent in Advanced-Dense training.With same 500 training environments, α = 2 achieves 0.578 on success bound while 0.512 for α = 1 and 0.557 for α = 5.</p>
<p>Figure 10 :
10
Figure 10: Effect of ρ and scheduling in Sim training: annealing ρ and helps balance between safety violations and task completion.If not specified, for Vanilla-Env, ρ initializes at 1 and decays by 0.5 every 25000 steps, and initializes at 0 with 1 − decaying by 0.5 every 50000 steps.For Advanced-Env, ρ initializes at 0.5 and decays by 0.5 every 500000 steps, and initializes at 0 with 1 − decaying by 0.5 every 200000 steps.The results are over 5 random seeds for Vanilla-Env 3 random seeds for Advanced-Env.</p>
<p>Figure 11 :
11
Figure 11: High augmented reward coefficient induces a diverse policy distribution: the diversity is essential to fine-tuning the latent distribution in Lab training and to good generalization to novel environments.Black markers indicate actions from the performance policy being executed, and green markers are for actions from the backup policy.</p>
<ol>
<li>2 .
2
Discussion: Large-scale Lab training.</li>
</ol>
<p>Figure C. 13 :
13
Figure C.13: Samples of robot observations in Advanced-Realistic Lab: for better view here, the virtual camera is placed at a higher location than the robot.</p>
<p>Updating the performance policy, backup policy, and the discriminator 1: for t ← 1 to num policy update do Sample batch {(o t+1 , o t , a t , z, r t )} from the replay buffer Action re-labeling
11:end if12: end for13: return π p , π b , N ψ0Algorithm 2 2:
3:Augment r t with mutual information reward (8)4:Update π p to maximize r aug with SAC Observation-marginal MI5:Sample batch {(o t+1 , o t , a sh t , z, r t )} from the replay buffer 6:</p>
<p>Table 1 :
1
Major distinctions among Sim-to-Lab-to-Real and baseline methods.</p>
<p>Table 2 :
2
Results of PAC-Bayes guarantees and test success and safety: to compute the bound, each environment has 1000 policies sampled from the latent distribution and tested.The results in the first two rows are based on PAC Shield Perf.
Advanced-RealisticMethodPAC Shield Perf PAC Base SQRL# Lab Environments100010001000Success Bound0.7010.297-True Expected Success0.7860.3660.712Real Robot Success0.7670.4330.667Safety Bound0.7080.304-True Expected Safety0.7940.3670.713Real Robot Safety0.8670.4330.667Vanilla-NormalVanilla-DynamicsMethodPAC Shield PerfPAC BasePAC Shield PerfPAC BaseDivergence Weight111101111101# Lab Environments10010002000100010001001000200010001000Success Bound0.778 0.876 0.900 0.8960.7350.692 0.820 0.839 0.8280.778True Expected Success 0.948 0.945 0.947 0.9340.8860.881 0.880 0.878 0.8720.843Safety Bound0.793 0.911 0.917 0.9130.8160.717 0.835 0.851 0.8370.815True Expected Safety0.954 0.954 0.954 0.9530.9020.888 0.887 0.887 0.8830.852Vanilla-TaskAdvanced-DenseMethodPAC Shield PerfPAC BasePAC Shield PerfPAC BaseDivergence Weight111101222152# Lab Environments100100020001000100010050010005005001000Success Bound0.578 0.757 0.792 0.7770.4680.402 0.578 0.623 0.512 0.5570.254True Expected Success 0.847 0.851 0.844 0.8530.5900.577 0.663 0.703 0.621 0.6440.327Safety Bound0.769 0.884 0.899 0.8870.6630.412 0.579 0.630 0.518 0.5640.259True Expected Safety0.939 0.939 0.940 0.9380.7960.583 0.671 0.709 0.629 0.6520.332
AcknowledgementAllen Z. Ren and Anirudha Majumdar were supported by the Toyota Research Institute (TRI), the NSF CAREER award [2044149], the Office of Naval Research [N00014-21-1-2803], and the School of Engineering and Applied Science at Princeton University through the generosity of William Addy '82.This article solely reflects the opinions and conclusions of its authors and not ONR, NSF, TRI or any other Toyota entity.We would like to thank Zixu Zhang for his valuable advice on the setup of the physical experiments.Appendix A. Derivations for Inducing Diversity into Backup Policy UpdateWe add observation-conditional mutual information term to the loss function of backup policy.We then approximate the expectation by the transitions sampled from the replay buffer asFinally, we approximate the marginal with the latent variables sampled from the distribution (empirical measure) asAppendix B. Training Hyperparameters used in ExperimentsWe show the training hyperparameters used to generate the results in Fig.5.Appendix C. Environment Setup for Advanced-EnvIn order to train the navigating agent in realistic environments before Real deployment, we use the 3D-FRONT (3D Furnished Rooms with layOuts and semaNTics) dataset[6]that offers a larger number of synthetic indoor scenes with professionally designed layouts and high-quality textured furniture.This is the richest dataset we find suitable to indoor navigation task, training with domain randomization and PAC-Bayes Control framework often requires more than 1000 environments.For Sim training, we use 7m × 7m undecorated rooms as room layouts, and randomly placing 5 pieces of furniture from the dataset.We use 4 categories of furniture: Soft (2701 pieces available), Chair (1775 pieces available), Cabinet/Shelf/Desk (5725 pieces available), Table (1090 pieces available).We also randomly sample textures from the dataset to add to the walls and floor: for walls, we use categories Tile, Wallpaper, and Paint (911 images available in total), and for floor, we use Flooring, Stone, Wood, Marble, Solid Wood Flooring (466 images available in total).We set the minimum clearance between furniture, around the initial location, and around the goal to be 1m.The minimum distance between the initial location and the goal is 5m.Fig.C.12 shows samples of observations at the initial locations.For Advanced-Dense Lab where the furniture density is higher, we place 6 instead of 5 pieces of furniture, and the minimum clearance is 0.8m instead of 1m.For Lab training, we instead use the professionally designed room layouts (with furniture configuration) from the dataset.The dataset contains 6813 different house layouts (each with multiple rooms).Since our focus is on obstacle avoidance with relatively short horizon, in each house, we try to sample initial and goal locations within one room.Unfortunately the dataset does not provide corresponding wall and floor textures in each layout, and we resort to random samples as in Vanilla-Env.Again we maintain a minimum clearance of 1m between furniture, around the initial and goal locations.To check the environment is solvable, we extract a 2D occupancy map for each room and run the Dijkstra algorithm.We also ensure there is at least one piece of furniture along the line connecting the initial and goal locations.We tend to find that many rooms are too crowded or the found path does not have enough clearance for the quadrupedal robot (about 0.5m wide).At the end, we are able to process about 2000 room environments, which are then split for training and testing.for better view here, the virtual camera is placed at a higher location than the robot.
RMA: Rapid Motor Adaptation for Legged Robots. A Kumar, Z Fu, D Pathak, J Malik, 10.15607/RSS.2021.XVII.011Proceedings of Robotics: Science and Systems (RSS), Virtual. Robotics: Science and Systems (RSS), Virtual2021</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 10.1109/ICRA.2017.7989381Proceedings of the IEEE International Conference on Robotics and Automation (ICRA. the IEEE International Conference on Robotics and Automation (ICRA2017</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 10.1109/IROS.2017.8202133Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2017</p>
<p>F Muratore, F Ramos, G Turk, W Yu, M Gienger, J Peters, arXiv:2111.00956Robot learning from randomized simulations: A review. 2021</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, 10.15607/RSS.2017.XIII.034Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)Cambridge, Massachusetts2017</p>
<p>H Fu, B Cai, L Gao, L.-X Zhang, J Wang, C Li, Q Zeng, C Sun, R Jia, B Zhao, H Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)20213D-FRONT: 3D Furnished Rooms With layOuts and semaNTics</p>
<p>Boston-Dynamics, Inside the Lab: Robotics After Hours. 2022</p>
<p>Algorithms for cvar optimization in mdps. Y Chow, M Ghavamzadeh, Proceedings of Advances in Neural Information Processing Systems (NeurIPS). Advances in Neural Information Processing Systems (NeurIPS)Montreal, Quebec, Canada2014</p>
<p>Risk-constrained reinforcement learning with percentile risk criteria. Y Chow, M Ghavamzadeh, L Janson, M Pavone, Journal of Machine Learning Research (JMLR). 182017</p>
<p>A general safety framework for learning-based control in uncertain robotic systems. J F Fisac, A K Akametalu, M N Zeilinger, S Kaynama, J Gillula, C J Tomlin, IEEE Transactions on Automatic Control (TAC). 642019</p>
<p>Bridging hamilton-jacobi safety analysis and reinforcement learning. J F Fisac, N F Lugovoy, V Rubies-Royo, S Ghosh, C J Tomlin, 10.1109/ICRA.2019.8794107Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)2019</p>
<p>Safety and liveness guarantees through reach-avoid reinforcement learning. K.-C Hsu, V Rubies-Royo, C J Tomlin, J F Fisac, 10.15607/RSS.2021.XVII.077Proceedings of Robotics: Science and Systems. Robotics: Science and Systems2021</p>
<p>Learning to be safe: Deep rl with a safety critic. K Srinivasan, B Eysenbach, S Ha, J Tan, C Finn, arXiv:2010.146032020</p>
<p>Safe reinforcement learning with learned recovery zones. B Thananjeyan, A Balakrishna, S Nair, M Luo, K Srinivasan, M Hwang, J E Gonzalez, J Ibarz, C Finn, K Goldberg, R L Recovery, IEEE Robotics and Automation Letters (RAL). 62021</p>
<p>. K Zhou, J C Doyle, Essentials of robust control. 1041998Prentice hall Upper Saddle River</p>
<p>Robust h-infinity control for uncertain stochastic systems with state delay. S Xu, T Chen, IEEE Transactions on Automatic Control (TAC). 472002</p>
<p>Funnel libraries for real-time robust feedback motion planning. A Majumdar, R Tedrake, The International Journal of Robotics Research (IJRR). 362017</p>
<p>Robust online motion planning via contraction theory and convex optimization. S Singh, A Majumdar, J.-J Slotine, M Pavone, 10.1109/ICRA.2017.7989693Proceedings of the IEEE International Conference on Robotics and Automation (ICRA. the IEEE International Conference on Robotics and Automation (ICRA2017</p>
<p>PAC-Bayes Control: Learning policies that provably generalize to novel environments. A Majumdar, A Farid, A Sonar, The International Journal of Robotics Research (IJRR). 402021</p>
<p>Task-driven out-of-distribution detection with statistical guarantees for robot learning. A Farid, S Veer, A Majumdar, Proceedings of the Conference on Robot Learning (CoRL). the Conference on Robot Learning (CoRL)2021</p>
<p>Probably approximately correct vision-based planning using motion primitives. S Veer, A Majumdar, Proceedings of the 2020 Conference on Robot Learning (CoRL). the 2020 Conference on Robot Learning (CoRL)PMLR2021155</p>
<p>A comprehensive survey on safe reinforcement learning. J García, F Fernández, Journal of Machine Learning Research (JMLR). 162015</p>
<p>Hamilton-jacobi reachability: A brief overview and recent advances. S Bansal, M Chen, S Herbert, C J Tomlin, 10.1109/CDC.2017.8263977Proceedings of the IEEE 56th Annual Conference on Decision and Control (CDC. the IEEE 56th Annual Conference on Decision and Control (CDC2017</p>
<p>Reach-Avoid Problems with Time-Varying Dynamics. J F Fisac, M Chen, C J Tomlin, S S Sastry, 10.1145/2728606.2728612Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, HSCC '15. the 18th International Conference on Hybrid Systems: Computation and Control, HSCC '15New York, NY, USA2015</p>
<p>On infusing reachability-based safety assurance within planning frameworks for human-robot vehicle interactions. K Leung, E Schmerling, M Zhang, M Chen, J Talbot, J C Gerdes, M Pavone, The International Journal of Robotics Research. 392020</p>
<p>End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. R Cheng, G Orosz, R M Murray, J W Burdick, 10.1609/aaai.v33i01.33013387Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19. the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19AAAI Press2019</p>
<p>Safe exploration in continuous action spaces. G Dalal, K Dvijotham, M Vecerik, T Hester, C Paduraru, Y Tassa, arXiv:1801.087572018</p>
<p>Safe autonomous racing via approximate reachability on egovision. B Chen, J Francis, J Oh, E Nyberg, S L Herbert, arXiv:2110.076992021</p>
<p>Safe controller optimization for quadrotors with gaussian processes. F Berkenkamp, A P Schoellig, A Krause, 10.1109/ICRA.2016.7487170Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)2016</p>
<p>Learning-based model predictive control for safe exploration. T Koller, F Berkenkamp, M Turchetta, A Krause, 10.1109/CDC.2018.8619572Proceedings of the IEEE Conference on Decision and Control (CDC). the IEEE Conference on Decision and Control (CDC)2018</p>
<p>Robust regression for safe exploration in control. A Liu, G Shi, S.-J Chung, A Anandkumar, Y Yue, Proceedings of the 2nd Conference on Learning for Dynamics and Control. the 2nd Conference on Learning for Dynamics and ControlPMLR2020120</p>
<p>On the uniform convergence of relative frequencies of events to their probabilities. V N Vapnik, A Y Chervonenkis, 2015Springer</p>
<p>Introduction to statistical learning theory. O Bousquet, S Boucheron, G Lugosi, Summer school on machine learning. Springer2003</p>
<p>Some pac-bayesian theorems. D A Mcallester, Machine Learning. 371999</p>
<p>Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. G K Dziugaite, D M Roy, Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence (UAI). the Thirty-Third Conference on Uncertainty in Artificial Intelligence (UAI)Sydney, AustraliaAugust 11-15, 2017</p>
<p>Tighter risk certificates for neural networks. M Pérez-Ortiz, O Rivasplata, J Shawe-Taylor, C Szepesvári, Journal of Machine Learning Research. 222021</p>
<p>Generalization guarantees for imitation learning. A Z Ren, S Veer, A Majumdar, Proceedings of the 2020 Conference on Robot Learning (CoRL). the 2020 Conference on Robot Learning (CoRL)PMLR2021155</p>
<p>A E Gurgen, A Majumdar, S Veer, arXiv:2111.08733Learning provably robust motion planners using funnel libraries. 2021arXiv preprint</p>
<p>Stronger generalization guarantees for robot learning by combining generative models and real-world data. A Agarwal, S Veer, A Z Ren, A Majumdar, arXiv:2111.087612021arXiv preprint</p>
<p>Failure prediction with statistical guarantees for vision-based robot control. A Farid, D Snyder, A Z Ren, A Majumdar, Proceedings of the Robotics: Science and Systems (RSS). the Robotics: Science and Systems (RSS)2022</p>
<p>Diversity is all you need: Learning skills without a reward function. B Eysenbach, A Gupta, J Ibarz, S Levine, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2019</p>
<p>Visual navigation for mobile robots: A survey. F Bonin-Font, A Ortiz, G Oliver, Journal of Intelligent and Robotic Systems. 532008</p>
<p>Autonomous vision-based exploration and mapping using hybrid maps and Rao-Blackwellised particle filters. R Sim, J J Little, 10.1109/IROS.2006.282485Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2006</p>
<p>Integrating grid-based and topological maps for mobile robot navigation. S Thrun, A Bücken, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence1996</p>
<p>Combining optimal control and learning for visual navigation in novel environments. S Bansal, V Tolani, S Gupta, J Malik, C Tomlin, Proceedings of the 2020 Conference on Robot Learning (CoRL). the 2020 Conference on Robot Learning (CoRL)PMLR2020100</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Safe visual navigation via deep learning and novelty detection. C Richter, N Roy, 10.15607/RSS.2017.XIII.064Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)Cambridge, Massachusetts2017</p>
<p>Safe robot navigation via multi-modal anomaly detection. L Wellhausen, R Ranftl, M Hutter, IEEE Robotics and Automation Letters (RAL). 52020</p>
<p>Safe reinforcement learning with model uncertainty estimates. B Lütjens, M Everett, J P How, Proceedings of the International Conference on Robotics and Automation (ICRA). the International Conference on Robotics and Automation (ICRA)IEEE2019</p>
<p>Uncertainty-aware reinforcement learning for collision avoidance. G Kahn, A Villaflor, V Pong, P Abbeel, S Levine, arXiv:1702.011822017arXiv preprint</p>
<p>An efficient reachability-based framework for provably safe autonomous navigation in unknown environments. A Bajcsy, S Bansal, E Bronstein, V Tolani, C J Tomlin, Proceedings of the IEEE 58th Conference on Decision and Control (CDC). the IEEE 58th Conference on Decision and Control (CDC)IEEE2019</p>
<p>Generating robust supervision for learning-based visual navigation using hamilton-jacobi reachability. A Li, S Bansal, G Giovanis, V Tolani, C Tomlin, M Chen, Proceedings of the 2nd Conference on Learning for Dynamics and Control. the 2nd Conference on Learning for Dynamics and ControlPMLR2020120</p>
<p>F Ramos, R C Possas, D Fox, arXiv:1906.01728Bayessim: adaptive domain randomization via probabilistic inference for robotics simulators. 2019arXiv preprint</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). 2019</p>
<p>V Lim, H Huang, L Y Chen, J Wang, J Ichnowski, D Seita, M Laskey, K Goldberg, arXiv:2111.04814Planar robot casting with real2sim2real self-supervised learning. 2021arXiv preprint</p>
<p>Active domain randomization. B Mehta, M Diaz, F Golemo, C J Pal, L Paull, Conference on Robot Learning. 2020</p>
<p>Data-efficient domain randomization with bayesian optimization. F Muratore, C Eilers, M Gienger, J Peters, IEEE Robotics and Automation Letters. 62021</p>
<p>Reinforcement learning with multi-fidelity simulators. M Cutler, T J Walsh, J P How, Proceedings of the IEEE/RSJ International Conference on Robotics and Automation (ICRA). the IEEE/RSJ International Conference on Robotics and Automation (ICRA)2014</p>
<p>A tutorial on conformal prediction. G Shafer, V Vovk, Journal of Machine Learning Research. 92008</p>
<p>Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR201880</p>
<p>Safe reinforcement learning via shielding. M Alshiekh, R Bloem, R Ehlers, B Könighofer, S Niekum, U Topcu, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence. the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial IntelligenceAAAI Press2018</p>
<p>Unsupervised curricula for visual meta-reinforcement learning. A Jabri, K Hsu, A Gupta, B Eysenbach, S Levine, C Finn, Advances in Neural Information Processing Systems (NeurIPS). 201932</p>
<p>One solution is not all you need: Few-shot extrapolation via structured MaxEnt RL. S Kumar, A Kumar, S Levine, C Finn, Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc202033</p>
<p>Dynamics-aware unsupervised discovery of skills. A Sharma, S Gu, S Levine, V Kumar, K Hausman, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2020</p>
<p>Not) bounding the true error. J Langford, R Caruana, Advances in Neural Information Processing Systems (NeurIPS). MIT Press200214</p>
<p>Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, The International Journal of Robotics Research. 372018</p>
<p>Amazon -this company built one of the world's most efficient warehouses by embracing chaos. Quartz, 2019</p>
<p>A look at how waymo's self-driving test fleet safely traveled 2.7 million miles in san francisco last year. Futurecar, 2022Traveled-2-7-Million-Miles-in-San-Francisco-Look-at-How-Waymos-Self-Driving-Test-Fleet-SafelyLast-Year</p>
<p>J Ichnowski, K Chen, K Dharmarajan, S Adebola, M Danielczuk, V Mayoral-Vilches, H Zhan, D Xu, R Ghassemi, J Kubiatowicz, arXiv:2205.09778Fogros 2: An adaptive and extensible platform for cloud and fog robotics using ros 2. 2022arXiv preprint</p>
<p>Leave no trace: Learning to reset for safe and autonomous reinforcement learning. B Eysenbach, S Gu, J Ibarz, S Levine, Proceedings of the 6th International Conference on Learning Representations (ICLR). the 6th International Conference on Learning Representations (ICLR)2018</p>
<p>Coresets via bilevel optimization for continual learning and streaming. Z Borsos, M Mutny, A Krause, Advances in Neural Information Processing Systems. 332020</p>
<p>B Guedj, arXiv:1901.05353A primer on pac-bayesian learning. 2019arXiv preprint</p>
<p>Stronger generalization bounds for deep nets via a compression approach. S Arora, R Ge, B Neyshabur, Y Zhang, International Conference on Machine Learning. PMLR2018</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7330 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7330</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7330</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-277272848</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.18325v1.pdf" target="_blank">Towards Training-free Anomaly Detection with Vision and Language Foundation Models</a></p>
                <p><strong>Paper Abstract:</strong> Anomaly detection is valuable for real-world applications, such as industrial quality inspection. However, most approaches focus on detecting local structural anomalies while neglecting compositional anomalies incorporating logical constraints. In this paper, we introduce LogSAD, a novel multi-modal framework that requires no training for both Logical and Structural Anomaly Detection. First, we propose a match-of-thought architecture that employs advanced large multi-modal models (i.e. GPT-4V) to generate matching proposals, formulating interests and compositional rules of thought for anomaly detection. Second, we elaborate on multi-granularity anomaly detection, consisting of patch tokens, sets of interests, and composition matching with vision and language foundation models. Subsequently, we present a calibration module to align anomaly scores from different detectors, followed by integration strategies for the final decision. Consequently, our approach addresses both logical and structural anomaly detection within a unified framework and achieves state-of-the-art results without the need for training, even when compared to supervised approaches, highlighting its robustness and effectiveness. Code is available at https://github.com/zhang0jhon/LogSAD.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7330",
    "paper_id": "paper-277272848",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00478225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Training-free Anomaly Detection with Vision and Language Foundation Models
24 Mar 2025</p>
<p>Jinjin Zhang jinjin.zhang@buaa.edu.cn 
State Key Laboratory of Complex and Critical Software Environment
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>Guodong Wang wanggd@buaa.edu.cn 
State Key Laboratory of Complex and Critical Software Environment
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>Yizhou Jin yizhou.jin@buaa.edu.cn 
School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>Di Huang dhuang@buaa.edu.cn 
State Key Laboratory of Complex and Critical Software Environment
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>Towards Training-free Anomaly Detection with Vision and Language Foundation Models
24 Mar 20250DEE414805DD810467E384A6B77F4ED8arXiv:2503.18325v1[cs.CV]
Anomaly detection is valuable for real-world applications, such as industrial quality inspection.However, most approaches focus on detecting local structural anomalies while neglecting compositional anomalies incorporating logical constraints.In this paper, we introduce LogSAD, a novel multi-modal framework that requires no training for both Logical and Structural Anomaly Detection.First, we propose a match-of-thought architecture that employs advanced large multi-modal models (i.e.GPT-4V) to generate matching proposals, formulating interests and compositional rules of thought for anomaly detection.Second, we elaborate on multi-granularity anomaly detection, consisting of patch tokens, sets of interests, and composition matching with vision and language foundation models.Subsequently, we present a calibration module to align anomaly scores from different detectors, followed by integration strategies for the final decision.Consequently, our approach addresses both logical and structural anomaly detection within a unified framework and achieves state-of-the-art results without the need for training, even when compared to supervised approaches, highlighting its robustness and effectiveness.Code is available at https://github.com/zhang0jhon/LogSAD.</p>
<p>Introduction</p>
<p>Anomaly detection is widely employed in real-world applications, particularly in industrial quality inspection, to identify anomalies with limited normal data [2,11,20,29].Existing anomaly detection methods have demonstrated impressive performance on anomaly detection datasets such as MVTec AD [3] and VisA [44], which are biased towards local structural anomalies like scratches, dents, or contaminations.However, those anomaly detection methods often fail * Corresponding author.[4].Compositional multi-modal feature matching plays a crucial role in unified anomaly detection, particularly in identifying and categorizing logical anomalies effectively.</p>
<p>to detect logical anomalies, such as incorrect wiring of circuits, permissible objects occurring in invalid locations, or the absence of essential components [4].To address this issue, several approaches have been proposed for logical anomaly detection [13,16], yielding decent performance through accurate mask annotations and training efforts.Despite considerable efforts in logical anomaly detection, many approaches focus primarily on the visual modality, which is necessary but insufficient for detecting highlevel anomalies with logical constraints and often lack the ability to identify compositional aspects.Furthermore, the need for precise annotations and intricate architectures complicates the practical applications in realistic scenarios, especially when addressing both structural and logical anomalies.As illustrated in Fig. 1, although visual feature matching is predominant in structural anomaly detection, compo-sitional multi-modal feature matching is essential for distinguishing high-level logical anomalies, including attributes, entity-relationships, and other complex compositions.Recent advancements in vision and language models (VLMs), notably CLIP [28], have highlighted that fine-tuning VLMs significantly impacts zero-shot and few-shot anomaly detection [12,15,43].However, while these VLM-based approaches are effective in detecting structural anomalies, they continue to face challenges with compositionality in logical anomalies [20,25].Moreover, the potential of VLMs to concurrently detect both structural and logical anomalies remains largely unexplored.</p>
<p>In this paper, we introduce LogSAD, a unified multimodal framework designed for both structural and logical anomaly detection without training endeavors.Firstly, we propose match-of-thought, utilizing advanced and powerful GPT-4V [1] to generate matching proposals, formulating interests and compositional matching rules of thought with vision and language instructions.Secondly, we employ multiple anomaly detectors to detect anomalies across various granularities using vision and language foundation models, such as CLIP [28] and SAM [17].Finally, we calibrate and fuse anomaly scores from different detectors to make final decisions within the unified framework.Extensive experiments are conducted across various anomaly detection datasets to validate the effectiveness and robustness of our method.</p>
<p>The main contributions are summarized as follows: • We present LogSAD, a training-free framework for anomaly detection utilizing vision and language foundation models, and demonstrate its capability to detect both logical and structural anomalies.• We introduce the match-of-thought architecture, illustrating its effectiveness through intermediate steps in generating interests and matching rules for anomaly detection with multi-modal instructions.• We propose multi-granularity detectors encompassing patch tokens, set of interests, and composition matching, as well as fusion strategies within the unified framework for anomaly detection.</p>
<p>Related Work</p>
<p>Vision and Language Foundation Models.The past several years have witnessed a significant advancement in vision and language foundation models, as evidenced by [1,17,21,23,27,28,31].These foundation models demonstrate significant capability across various realistic scenarios, including zero-shot classification, open-vocabulary perception, and multi-modal learning.SAM [17] is among the leading vision foundation models, making substantial strides in zero-shot image segmentation.CLIP [28] is the pioneering model to undertake vision and language pretraining on large-scale image-text pairs, demonstrating un-precedented generality in downstream tasks.LLaVA [22,23] introduces an end-to-end trained large multi-modal model (LMM) that integrates a vision encoder with large language model (LLM) for comprehensive visual and language understanding.GPT-4V [1], a large-scale multimodal model, can accept both image and text inputs and generate text outputs, exhibiting human-level performance across a variety of professional and academic benchmarks.Consequently, the utilization of vision and language foundation models has become ubiquitous in real-world applications, providing robustness and generalization in semantic and spatial understanding [33], multi-modal alignment [19,36], etc.Nevertheless, recent studies indicate that even the most advanced LMMs still face challenges in capturing aspects of compositionality in visual reasoning, such as attributes and relationships between objects [25,26,39].</p>
<p>Anomaly Detection.Due to the scarcity of anomalies, most methods focus on anomaly detection with several normal images.GCAD [4] introduces a new method for the unsupervised localization of anomalies which consists of two main branches, one of which is primarily responsible for the localization of structural anomalies and the other one for the localization of logical anomalies.PSAD [16] focuses on logical anomaly detection and introduces a novel component segmentation model that leverages segment annotations of labeled images and unlabeled images sharing logical constraints.LogiCode [40] introduces additional annotations in LOCO-Annotations dataset and LogiBench benchmark, addressing automatic code generation with LLMs for logical AD.PatchCore [29] leverages a maximally representative memory bank of nominal patch features for structural anomaly detection.UniAD [35] and OmniAL [42] conduct structural anomaly detection across multiple categories using a unified framework.Win-CLIP [15] proposes a window-based CLIP approach with compositional ensemble on state words and prompt templates, aiming for efficient extraction and aggregation of multi-level features.AnomalyGPT [12] explores the utilization of large vision-language models to address the industrial structural anomaly detection problem.THFR [13] proposes a template-guided hierarchical feature restoration framework for anomaly detection, incorporating bottleneck compression and template-guided compensation for anomaly-free feature restoration.PromptAD [20] presents a one-class prompt learning method for few-shot anomaly detection, achieving the state-of-the-art performances on structural anomaly detection but still struggling with the challenging logical anomalies.EfficientAD [2] exploits a teacher-student model and addresses the detection of logical anomalies with auto-encoder.SimpleNet [24] generates synthetic anomalies in a pretrained feature space to train a discriminator network for detecting anomalous features.GRAD [7] proposes a diffusion model, Patchdiff, to gener- Text Prompts</p>
<p>Vision Language</p>
<p>Give the images of a juice bottle, each juice bottle is filled with one of three differently colored liquids and carries exactly two labels.The first label is attached to the center of the bottle and displays an icon that determines the type of liquid.The second is attached to the lower part of the bottle with the text "'100% Juice" written on it.The fill level is the same for each bottle, violations to any of these constraints constitute logical anomalies.Please list 50 sentences to describe possible logical anomalies covering various scenarios in diversity.The descriptions should be concise and specific.</p>
<p>Match-of-thought</p>
<p>Interests of thought: {"glass", "liquid in bottle", "fruit", "label", "background"}, Compositional rules of thought: {1.compositionality in consistency of liquid color and tag: { liquid color prompt: {"red liquid", "yellow liquid", "milky liquid"}, fruit tag prompt : {"cherry", "orange", "banana"}}, …}</p>
<p>A photo of {matching rules of thought prompt}.</p>
<p>A photo of {…}.In the framework, we utilize match-of-thought to generate matching proposals, deriving text prompts of interests and compositional rules for anomaly detection.Based on the text prompts, our method leverages vision and language foundation models to achieve multi-granularity anomaly detection, followed by calibration and fusion modules to make final decision.Importantly, our algorithm detects both structural and logical anomalies within a unified framework, eliminating the need for training efforts.</p>
<p>ate diverse contrastive images, and trains lightweight detectors for anomaly detection.GeneralAD [30] proposes a selfsupervised anomaly generation module to construct pseudoabnormal samples, and employs a transformer-based discriminator capable of detecting a wide range of anomalies.</p>
<p>Methodology</p>
<p>In this section, we elaborate on the details of LogSAD, illustrating how it works for both logical and structural anomaly detection within a unified framework.As shown in Fig. 2, our method is built upon match-of-thought and employs multi-granularity detectors for anomaly detection with vision and language foundation models.The details are presented as follows.</p>
<p>Match-of-thought</p>
<p>In contrast to structural anomalies that occur as scratches or dents in manufactured products, logical anomalies violate the underlying constraints of compositionality in vision and natural language, composed of visual and textural atoms (e.g.objects in images or words in a sentence).</p>
<p>Considering that LMMs struggle with compositional visual reasoning, we propose the match-of-thought (MoT), which involves a series of intermediate reasoning steps for prompt and match engineering in anomaly detection.Inspired by chain-of-thought approaches [34,41], the MoT extracts interests of thought from vision and language instructions, simultaneously formulating matching rules for different types of compositional logical anomalies.</p>
<p>In the MVTec LOCO dataset, each category incorporates specific logical constraints.As depicted in Fig. 3, we collect a few canonical normal images from the training set, along with their corresponding logical constraint descriptions in the original paper [4], formulating the vision and language instructions for the MoT.We employ the advanced GPT-4V to generate precise anomaly-free image captions and matching proposals based on the multi-modal instructions.Subsequently, we summarize the text prompts of interests and formulate compositional matching rules.Specifically, the MoT alleviates the issues of factuality and hallucination often encountered with LMMs in detecting logical anomalies, thereby enhancing interpretability through the effective matching of interests and compositions.Moreover, the MoT procedure is computationally efficient as a preliminary step, making it highly adaptable to various application scenarios.</p>
<p>Multi-granularity Anomaly Detectors</p>
<p>Structural anomalies typically manifest as scratches or dents in localized areas, while logical anomalies involve Exactly two splicing connectors with the same number of cable clamps are linked by exactly one cable.In addition, the number of clamps has a one-to-one correspondence to the color of the cable and the cable has to terminate in the same relative position on its two ends such that the whole construction exhibits a mirror symmetry.</p>
<p>Vision and Language Instructions</p>
<p>Match-of-thought</p>
<p>Interests of thought: {" splicing connector ", " cable ", " grid "} Compositional rules of thought: { 1. attribute: { cable count: 1 }, … ; violations of compositional constraints.Addressing these anomalies with a single anomaly detector is challenging.To tackle this challenge, we propose multi-granularity anomaly detectors that aim to simultaneously detect both structural and logical anomalies within our framework.These detectors operate at different granularities, focusing on patch, interests, and compositional matching, respectively.Patch Matching.We systematically extract hierarchical patch features from various stages of pretrained vision backbones, including CLIP [28] and DINOv2 [27].Additionally, we maintain a canonical memory bank of anomaly-free image features for nearest neighbor search in patch-level anomaly detection [29].Assuming that m ∈ R m×d and n ∈ R n×d are d-dimensional patch features from the query image and memory bank respectively, the anomaly score for patch-granularity detector is defined as follows:
s p = max(1 m×n − m • n ⊤ ∥m∥ ∥n∥ ),(1)
where 1 m×n is a matrix in which all the elements are 1.</p>
<p>Interest Matching.Utilizing the interests of thought generated by the MoT, we combine CLIP and SAM for openvocabulary segmentation [14,33], obtaining the segmentation masks of the interests.Subsequently, we extract sets of hierarchical interest-wise features by aggregating masked patches through average pooling.Based on the derived feature sets of interests, we develop the interest-granularity detector, which reinterprets anomaly detection as minimum weight matching in bipartite graphs.</p>
<p>Let P = {p 1 , p 2 , • • • , p i } denote the feature set of i interests from query image, and Q = {q 1 , q 2 , • • • , q j } denote the feature set of j interests from referring anomalyfree memory bank, where p ∈ R d and q ∈ R d .To find a bipartite matching between these two sets we search for a permutation of N = min(i, j) elements π ∈ S N with the minimal cost:
s in =        arg min π∈S N 1 i i k=1 L match (p k , q π(k) ), if i ≤ j, arg min π∈S N 1 j j k=1 L match (p π(k) , q k ), otherwise, (2)
where L match (p, q) is a pair-wise matching cost defined as follows:
L match (p, q) = 1 − p • q ⊤ ∥p∥ ∥q∥ . (3)
The optimal assignment π can be computed efficiently using the Hungarian algorithm [18].Thus, the anomaly score for interest-granularity detector is derived from Eq. ( 2), specifically the minimal cost in bipartite matching.Composition Matching.In the MoT, underlying logical constraints are reformulated as compositional matching rules involving visual and textural atoms.Specifically, given a matching rule R that depicts the anomaly-free scenario, such as attributes or entity-relationships, relevant features of visual objects V and textural embeddings T are extracted using CLIP [28].These features form the essential elements to determine whether the compositional matching rule is violated.For instance, as shown in Fig. 2, for the specific matching rule R = { "consistency of liquid color and tag" }, we extract the relevant visual features of interests V = {"liquid in the bottle", "fruit"} and text embeddings T = {{"red liquid", "yellow liquid", "milky liquid"}, { "cherry", "orange", "banana"}}.Leveraging the alignment between visual and textual features in CLIP, we can attribute the visual elements with zero-shot classification capabilities.This enables us to determine whether the visual elements violate the specified matching rule R. As composition matching focuses on detecting violations of logical constraints, the anomaly score for the compositiongranularity detector is defined as follows:
s c = 0, if f (V, T , R) is True, 1, otherwise.(4)
Here, the criterion f (•) assesses whether interests ⟨V, T ⟩ of query image conforms to the compositional rule R.</p>
<p>Calibration and Fusion</p>
<p>On account that multi-granularity anomaly detectors may produce anomaly scores at different scales in measurement, it is imperative to involve score calibration and fusion modules in our framework.With the exception of Extract patch features m of query image x, and compute patch-granularity anomaly score s p in Eq. ( 1).</p>
<p>6:</p>
<p>Collect feature set of interests P from x, and compute interest-granularity anomaly score s in in Eq. (2).</p>
<p>7:</p>
<p>Extract visual and textural features ⟨V, T ⟩ of x, and compute compositional anomaly score s c in Eq. ( 4).</p>
<p>8:</p>
<p>Compute final anomaly score s of x in Eq. ( 5).</p>
<p>9:</p>
<p>S ← UPDATE(s).10: end for 11: return S the composition-granularity detector, we calculate anomaly scores for both patch-granularity and interest-granularity detectors using statistics from anomaly-free images in the validation set.Subsequently, the calibrated scores are standardized and passed through a sigmoid function.The final result is determined as the maximum of the anomaly scores across the multi-granularity detectors:
s = max{g( s p − µ p σ p ), g( s in − µ in σ in ), s c },(5)
where g(•) denotes the sigmoid function, µ p and σ p represent the unbiased mean and standard deviation of patchgranularity statistics respectively, and µ in and σ in denote those of interest-granularity statistics.</p>
<p>The overall procedure for the proposed LogSAD is summarized in Algorithm 1.</p>
<p>Experiment</p>
<p>We conduct comprehensive experiments to assess the performance of LogSAD under few-shot and full-data regimes, covering recent challenging benchmarks on industrial anomaly detection, including MVTec LOCO [4], MVTec AD [3] and VisA [44] datasets.Additionally, extensive ablation studies are conducted to validate the individual effectiveness of each proposed component.Datasets.MVTec LOCO serves as a comprehensive benchmark designed to detect both logical and structural anomalies.The dataset comprises approximately 3,644 images, distributed as 1,772 images for training, 304 for validation and 1,568 for testing.It consists of 5 categories, i.e. breakfast box, juice bottle, pushpins, screw bag and splicing connectors.The MVTec AD dataset contains 3,629 training images and 1,725 test images but pays more attention on structural anomalies than MVTec LOCO.It consists of 15 real-world sub-datasets, with 5 categories of textures and 10 categories of objects.The VisA dataset consists of 9,621 normal and 1,200 anomalous color images encompassing 12 objects across 3 domains, including complex structure, single instance and multiple instances.The anomalous images exhibit various flaws, including surface defects such as scratches, dents, color spots or cracks, and logical defects like misplacement or missing parts.Main Results.We mainly compare our algorithm with several state-of-the-art methods on MVTec LOCO, including training-free method SPADE [5] and PatchCore [29], as well as training-based approaches such as PaDim [8], THFR [13], SINBAD [6], GRAD [7] and GeneralAD [30], addressing both logical and structural anomalies.As depicted in Tab. 1, our method achieves significant performance improvements over previous state-of-the-art methods, particularly in terms of image-level AUROC.As a result, our approach achieves superior results in full-data protocol without requiring training efforts, and dramatically exhibits competitive performance even in the 4-shot setting, where anomalies are detected with only 4 normal images.Extensive experimental results demonstrate the effectiveness of our framework in simultaneously detecting both structural and logical anomalies.</p>
<p>In addition, we conduct experiments across extreme fewshot protocols to demonstrate the robustness of LogSAD, and provide the comparisons with advanced few-shot anomaly detection approaches, including WinCLIP [15] and PromptAD [20].As shown in Tab. 2, our method substantially outperforms previous few-shot approaches.Specifically, our algorithm achieves improvements of 10.3%, 13.1% and 17.4% over PromptAD under 1-shot, 2shot and 4-shot protocols respectively, distinctly showcasing the superiority and robustness.Meanwhile, mean results for logical and structural anomaly detection are reported separately in Tab. 3, along with comparisons with DSR [38], SimpleNet [24] and EfficientAD [2].Notably, PSAD [16] is a training-based approach that utilizes additional segment annotations, making direct comparisons with other methods unfair.Therefore, we have excluded PSAD from our comparisons and marked it in gray in the table for clarity.</p>
<p>Meanwhile, quantitative results for MVTec AD and VisA benchmarks in few-shot scenarios are presented in Tab. 4. Despite structural anomalies dominating both MVTec AD and VisA datasets, our method consistently achieves state-of-the-art performance in anomaly detection compared to previous training-free and training-based approaches, such as SPADE [5], PaDim [8], PatchCore [29]  WinCLIP [15], AnomalyGPT [1] and PromptAD [20].In summary, extensive experimental results across various anomaly detection benchmarks demonstrate the robustness and generalization of our algorithm.Please refer to the supplementary materials for more details.Implementation Details.Specifically, we utilize the Open-CLIP * implementation of CLIP and its pretrained models in our experiment.We employ the CLIP with ViT-L/14 [9] pretrained on DataComp-1B [10], DINOv2 [27] with ViT-L/14, and SAM [17] with ViT-H/16 as our vision and language foundation models.The image resolution is 448 × 448, and the visual feature maps extracted from ViT-L/14 across 4 hierarchical stages are upsampled to 64 × 64.</p>
<ul>
<li>https://github.com/mlfoundations/open_clipRegarding pixel-level evaluation, the 64×64 anomaly maps are resized to 256 × 256 to ensure fair comparison with previous methodologies.Matching Details.We explicitly extract normalized hierarchical features from the 4 stages of ViT-L/14.The ensemble score maps across these stages are then averaged for patch matching.In the full-data setting, the memory bank of normal images is downsampled via greedy coreset subsampling to reduce redundancy [29].In addition, detailed interests and compositional rules for each category on MVTec LOCO are presented in Tab.Breakfast Box { "orange", "nectarine", "cereals", "banana chips", "almonds", "white box", "black background"} {"the ratio and relative position of the cereals, banana chips and almonds should be fixed" } Juice Bottle { "glass", "liquid in the bottle", "fruit", "label", "black background"} {"consistency of fruit tag and liquid color" } Pushpins { "pushpin", "plastic box", "black background"} {"number of pushpins is 15" } Screw Bag { "screw", "hex nut", "ring washer", "plastic bag", "background"} {"histogram of screws", "1 long screw, 1 short screw, 2 nuts and 2 washers" }</li>
</ul>
<p>Anomaly-free images Anomalous images</p>
<p>Splicing Connectors</p>
<p>{ "splicing connector", "cable", "grid"} {"consistency of cable color and number of clamps", "number of cable is 1", "splicing connectors consist of left and right parts and keep symmetry", "cable terminates in the same relative position"} Table 5. Interests and compositional rules of thought on MVTec LOCO.The final adopted interests and compositional rules for matching are marked in bold.</p>
<p>we use the text prompt templates designed for ImageNet in OpenAI's CLIP [28]  nating the necessity for training efforts.</p>
<p>In addition, we conduct ablation studies on MVTec LOCO to highlight the importance of multi-granularity anomaly detectors.As demonstrated in Tab.6, multigranularity anomaly detectors prove effective and advantageous in various respects.For instance, the patchgranularity detector focuses on structural anomalies, the composition-granularity detector addresses logical anomalies with compositionality, and the interest-granularity detector achieves a balance between detecting both structural and logical anomalies.Furthermore, both the patchgranularity and interest-granularity detectors show significant improvement as the number of normal images increases.The composition-granularity detector operates independently of reference images, ensuring stability in detecting logical anomalies across both full-data and few-shot protocols.</p>
<p>Notably, we highlight the image-level F 1 -max and AU-ROC scores of individual detectors and their fusion for unified anomaly detection in Tab. 7. The experimental results demonstrate how different anomaly detectors complement each other in detecting both structural and logical anomalies, underscoring the importance of multi-granularity detectors.Furthermore, the ensemble of multi-granularity detectors dramatically enhances the performance on anomaly detection, highlighting the effectiveness of our calibration and fusion strategy.Comparison with LMMs in compositional anomalies.Recent empirical studies [26,32,37,39] have shown that LMMs struggle to perform compositional visual understanding, particularly in tasks involving identifying object attributes and inter-object relations.Specifically, studies indicate that VLMs [25] often learn a "bag of objects" representation, which limits their compositional capabilities.Here, we assess the compositionality performance of advanced LMMs, such as LLaVA [22,23] and Mini-Gemini [21].As illustrated in Fig. 5, LMMs provide accurate responses in specific tasks, such as identifying the color of cables in splicing connectors.However, LMMs face challenges in tasks requiring complex scene understanding, including object counting and spatial perception, e.g.counting the number of pushpins or distinguishing the left and right sides of clamps.Consequently, LMMs continue to exhibit issues such as hallucinations, bias, and factual inaccuracies, making them impractical for addressing most logical anomalies involving compositionality.</p>
<p>In contrast, the proposed composition-granularity detector effectively handles the compositionality using visual and textural atoms, leveraging open-vocabulary segmentation masks.Moreover, it is worth noting that the compositiongranularity detector effectively distinguishes various finegrained logical anomalies.With aligned visual and textual embeddings from CLIP, it is available to extract relevant visual objects or text descriptions for attributes in compositional matching, e.g., the number of pushpins, cable color, What is the color of cable in the image?red, yellow or blue?</p>
<p>Mini-Gemini: The color of cable in the image is yellow.</p>
<p>What is the number of clamps in the left side of splicing connectors? 2, 3, or 5?</p>
<p>Mini-Gemini: There are two clamps on the left side of the splicing connectors.</p>
<p>What is the number of clamps in the right side of splicing connectors? 2, 3, or 5?</p>
<p>Mini-Gemini: There are two clamps on the right side of the splicing connectors.</p>
<p>How many pushpins are there in the image?</p>
<p>LLaVA: The image shows a collection of pushpins, but the exact number is not visible.There are at least 12 pushpins, as there are three rows of four pushpins each.Mini-Gemini: There are 24 pushpins in the image.and the count of clamps in splicing connectors, and so on.Quantitative experiment results in Tab.6 and Tab.7 demonstrate the effectiveness of our composition matching approach in addressing logical anomalies and its contribution to unified anomaly detection.</p>
<p>Conclusion</p>
<p>In this paper, we propose LogSAD, a unified multi-modal framework for detecting both structural and logical anomalies without training endeavors.We elaborate on multigranularity anomaly detectors designed to detect anomalies across diverse granularities by leveraging vision and language foundation models.Subsequently, anomaly scores from different anomaly detectors are calibrated and fused for the final decision procedure.Extensive experiments are conducted across various anomaly detection datasets to demonstrate the effectiveness of our method.</p>
<p>However, our approach is not without limitations.Future work will focus on enhancing the performance of openvocabulary semantic segmentation and exploring advanced LMMs for complicated scenarios.</p>
<p>Towards Training-free Anomaly Detection with Vision and Language</p>
<p>Foundation Models</p>
<p>Supplementary Material</p>
<p>In this supplementary material, we provide details of experimental settings including data preprocessing and evaluation metrics.Additionally, quantitative results on MVTec LOCO [4], MVTec AD [3] and VisA [44] benchmarks are presented to demonstrate the effectiveness of our algorithm.Finally, we provide a comprehensive analysis and discussion of our framework.</p>
<p>Experimental Details</p>
<p>Data Preprocessing.Regarding vision and language foundation models including CLIP [28], DINOv2 [27] and SAM [17], we apply the same data preprocessing pipeline across MVTec LOCO, MVTec AD and VisA datasets to mitigate potential train-test discrepancy.Specifically, it involves channel-wise standardization with the pre-computed mean [0.48145466, 0.4578275, 0.40821073] and standard deviation [0.26862954, 0.26130258, 0.27577711] after normalizing each RGB image into [0, 1], followed by bicubic interpolation based on the Pillow implementation.Evaluation Metrics.Consistent with existing methods [3,4], we report the results of the Area Under the Receiver Operator Curve (AUROC) documented in the body of the paper for the evaluation of image-level anomaly detection and pixel-level anomaly localization.Additionally, we supplement the F 1 -max results in anomaly detection.The F 1max score is computed from the precision and recall for the anomalous samples at the optimal threshold, which is a more straightforward metric to measure the upper bound of anomaly prediction performance across thresholds.</p>
<p>Quantitative Results</p>
<p>To elucidate the interaction between patch matching and composition matching in detecting logical and structural anomalies, we present the experimental results in Tab. 8, demonstrating that incorporating composition matching with patch matching improves results for logical AD and achieves comparable results for structural AD.Quantitative results indicate that the inclusion of composition matching significantly enhances detection performance for logical anomalies while maintaining comparable performance on structural anomalies.Additionally, we report the detailed subset-level results of LogSAD.Specifically, the results on MVTec LOCO [4] are presented in Tab. 9, and the results on MVTec AD [3] and VisA [44]</p>
<p>Discussion</p>
<p>Canonical Normal Images in the MoT.The MVTec LOCO dataset [4] contains a varying number of normal subclasses in each category, e.g. 1, 3, 1, 1, 3 corresponding to "breakfast box", "juice bottle", "pushpins", "screw bag", and "splicing connectors".Specifically, the "juice bottle" category includes cherry, banana, and orange labels, while the "splicing connectors" category contains red, blue, and yellow cables.Thus, we sample 3 canonical normal images from the normal sub-classes in each category to maximize sub-class coverage in the training set, facilitating the establishment of comprehensive matching interests and compositional rules.We typically observe that the generated results from GPT-4V remain consistent across the provided normal images due to subtle visual variations within each sub-class.Notably, the sampled canonical normal images and GPT-4V are exclusively used for offline proposal generation, which operates independently of the anomaly detection algorithm.</p>
<p>In practice, the quality of generated proposals can be assessed in various perspectives, including the qualitative results of open-vocabulary semantic segmentation in terms of interests of thought, as depicted in Fig. 4, and quantitative results through interest matching and compositional matching, as shown in Tab. 6.Failure cases.In addition, we present the failure cases in Fig. 6 to address the limitations of our framework, including failures in open-vocabulary semantic segmentation and uncovered situations in compositional matching.For instance, (a) fails to distinguish the "hex nut" and the "ring washer"; (b) the number of pushpins is 15, but two pushpins appear in one division, which is not covered by matching rules; (c) fails in open-vocabulary semantic segmentation and counting due to the reflection of pushpins.Computation Analysis.Previous methods, such as Win-CLIP [15], PromptAD [20], AnomalyCLIP [43] fine-tuning with CLIP, and AnomalyGPT [12] fine-tuning with larger models (e.g.Vicuna-7B and Vicuna-13B), focus solely on structural AD but continue to struggle with logical AD.Our focus is on the training-free application of off-the-shelf foundation models for both logical and structural AD.Note that we use GPT-4V only for offline match proposal generation, and open-sourced foundation models including CLIP, DINOv2 and SAM are collaborated for anomaly detection with around 1.3B parameters.Consequently, Tab. 4 shows our method achieves state-of-the-art performance on structural AD datasets, outperforming training-based methods like PromptAD and AnomalyGPT.In addition, experimental results in Tab.</p>
<p>Figure 2 .
2
Figure2.The framework of LogSAD.In the framework, we utilize match-of-thought to generate matching proposals, deriving text prompts of interests and compositional rules for anomaly detection.Based on the text prompts, our method leverages vision and language foundation models to achieve multi-granularity anomaly detection, followed by calibration and fusion modules to make final decision.Importantly, our algorithm detects both structural and logical anomalies within a unified framework, eliminating the need for training efforts.</p>
<p>Figure 3 .
3
Figure 3. Match-of-thought for prompt and match engineering.The vision and language instructions consist of few anomaly-free images and compositional logical constraints in MVTec LOCO.</p>
<p>Figure 4 .
4
Figure 4. Qualitative visualization results of open-vocabulary semantic segmentation on MVTec LOCO.</p>
<p>Ours:</p>
<p>Compositional rule ℛ = {pushpins: 15} Visual  = {v1, v2, …, v16}, (16 pushpins) Textual  = {"pushpin"}, Number of pushpin match failed,   =  LLaVA: The color of the cable in the image is yellow.LLaVA: The image shows two splicing connectors with orange clamps.Each connector has two clamps, so there are a total of four clamps in the image.LLaVA: The image shows two splicing connectors, each with three clamps.So the total number of clamps in the right side of the splicing connectors is 6.Ours: Compositional rule ℛ = {2−clamp: yellow cable, 3−clamp: blue cable, 5−clamp: red cable} Visual  = {v1, v2, v3}, (yellow cable, 3-clamp) Textual  = {"cable", "connector"}, Cable color and number of clamps match failed,   = </p>
<p>Figure 5 .
5
Figure 5.Comparison with LMMs in compositionality.Correct answers are marked in green, while incorrect ones are marked in yellow.Our proposed composition-granularity detector performs effectively, whereas LMMs struggle with issues of factuality and hallucination in logical anomaly understanding and detection.</p>
<p>Figure 6 .
6
Figure 6.Failure cases of LogSAD.</p>
<p>Algorithm 1 LogSAD Input: Canonical normal images I v ; text prompts I t ; test images X .Extract interests I and compositional rules R via the MoT with vision and language instructions I v and I t .2: Initialize foundation models M. 3: Collect n and Q from I v as an anomaly-free memory bank for patch and interest matching.</p>
<p>Output: Anomaly scores S.1: 4: for x ∈ X do 5:</p>
<p>Table 1 .
1
Image-level AUROC results of unified anomaly detection on MVTec LOCO.† indicates training-free approaches.
,</p>
<p>Table 2
2MethodLogical Structural AverageDSR [38]75.090.282.6SimpleNet [24]71.583.777.6EfficientAD [2]86.894.790.7PSAD [16]  ‡98.191.694.9LogSAD (ours)  †89.393.191.2
. Few shot image-level AUROC percentages of unified anomaly detection on MVTec LOCO.</p>
<p>Table 3 .
3
Mean anomaly detection AUROC results of detecting logical and structural anomalies respectively on MVTec LOCO.Note that PSAD ‡ , which utilizes additional segment annotations for training in anomaly detection, is not included in the comparison.</p>
<p>Table 4 .
4
Quantitative results on MVTec AD and VisA benchmarks.Image-level and pixel-level AUROC are reported across various fewshot scenarios.
MVTec ADVisAMethodimagepixelimagepixel1-shot 2-shot 4-shot 1-shot 2-shot 4-shot 1-shot 2-shot 4-shot 1-shot 2-shot 4-shotSPADE [5]  †81.082.984.891.292.092.779.580.781.795.696.296.6PaDiM [8]76.678.980.489.391.392.662.867.472.889.992.093.2PatchCore [29]  †83.486.388.892.093.394.379.981.685.395.496.196.8WinCLIP+ [15]  †93.194.495.295.296.096.283.884.687.396.496.897.2AnomalyGPT [1]94.195.596.395.395.696.287.488.690.696.296.496.7PromptAD [20]94.695.796.695.996.296.586.988.389.196.797.197.4LogSAD (ours)  †96.196.597.097.097.397.688.290.093.097.697.898.1CategoryInterests of ThoughtCompositional Rules of Thought
5. By default,</p>
<p>Table 6 .
6
Ablation studies on multi-granularity detectors.Imagelevel AUROC results of respective detectors in detecting structural and logical anomalies under full-data and 4-shot protocols are reported on MVTec LOCO.
and average the predictions acrossDetectorsfull-data Structural Logical Structural Logical 4-shotmultiple text prompts with these templates. Note that the background classes of interests (e.g. "black background", "plastic bag" and "grid") are introduced to improve accuratePatch Insterests Composition93.1 81.7 58.071.8 80.6 78.287.3 72.1 58.067.6 74.3 78.2open-vocabulary semantic segmentation. In practice, we se-lect the foreground classes of interests and specific compo-sitional rules for matching, excluding prompts that are am-biguous or have limited performance in open-vocabularysemantic segmentation. Subsequently, masked visual to-kens are aggregated using average pooling for zero-shotclassification, leveraging aligned VLMs such as CLIP. This method aids in distinguishing the fine-grained categories or attributes in compositional rules with masked features, e.g. "red cable, blue cable or yellow cable", "cherry, banana or orange tag".Patch Interests Composition F 1 -max AUROC ✓ 81.3 81.0 ✓ 81.7 80.7 ✓ ✓ 83.8 85.1 ✓ ✓ 85.5 86.4Ablation Studies. Given the pivotal role of defining inter-✓✓✓88.890.2ests in our framework, we present the intermediate segmen-tation results of interests necessary for collecting interestsets used in bipartite graph matching. Moreover, preciseopen-vocabulary segmentation results facilitate the imple-
mentation of compositional rules involving visual and textural elements.As shown in Fig.4, the impressive visualizations highlight the capability to achieve high-quality segmentation results using the MoT and VLMs, thereby elimi-</p>
<p>Table 7 .
7
Ablation studies on detectors calibration and fusion.Image-level F1-max and AUROC results are presented on MVTec LOCO.</p>
<p>Table 8 .
8
benchmarks are depicted in Tab. 10 and Tab.11, respectively.Image-level AUROC of multi-granularity detectors under 4-shot protocol on MVTec-LOCO dataset.
DetectorsStructural Logical AveragePatch87.367.677.4Composition58.078.268.1Patch + Composition85.882.083.9</p>
<p>Table 9 .
9
1, Tab. 2 and Tab. 3 demonstrate the effectiveness of our framework in both logical and structural AD.-max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC Image-level F1-max and AUROC results on MVTec LOCO in few-shot and full-data protocols.-max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC
ProtocolBreakfast BoxJuice BottlePushpinsScrew BagSplicing ConnectorsAverageF 1 1-shot 85.088.085.678.175.778.080.770.678.277.781.078.52-shot88.191.585.777.577.881.183.080.578.279.882.682.14-shot89.994.488.284.381.482.584.181.584.788.685.786.3full-data92.095.794.095.281.383.685.283.291.393.588.890.2imagepixelCategory1-shot2-shot4-shot1-shot2-shot4-shotF 1 bottle 10010010010010010081.599.082.199.182.899.2cable88.090.487.991.287.890.860.796.762.797.463.197.6capsule94.092.094.992.797.394.150.398.050.598.351.798.4carpet98.999.498.999.398.999.467.699.267.499.267.599.2grid99.199.810010010010051.299.355.999.555.999.5hazelnut99.399.998.699.810010065.698.967.799.171.599.3leather99.599.999.599.910010049.499.348.099.348.999.4metal nut99.599.699.599.710010074.796.076.796.384.697.8pill96.291.196.497.296.897.966.796.867.797.068.497.1screw92.292.492.292.492.292.418.795.622.396.627.597.4tile98.899.910010010010071.696.372.196.572.396.6toothbrush92.993.991.892.893.392.238.896.238.196.237.596.1transistor79.589.578.588.578.790.948.990.450.991.752.291.9wood99.299.899.299.799.299.870.297.070.297.070.297.0zipper96.893.598.394.799.297.656.196.658.297.158.697.3average95.696.195.796.596.297.058.197.059.497.360.897.6</p>
<p>Table 10 .
10
Image-level/pixel-level F1-max and AUROC results on MVTec AD.-max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC F 1 -max AUROC
imageCategory</p>
<p>Table 11 .
11
Image-level/pixel-level F1-max and AUROC results on VisA.</p>
<p>AcknowledgementThis work is partly supported by the National Key Research and Development Plan (2024YFB3309302).
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 202367arXiv preprint</p>
<p>Efficientad: Accurate visual anomaly detection at millisecond-level latencies. Kilian Batzner, Lars Heckler, Rebecca König, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024. 1, 2, 5, 6</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition201915</p>
<p>Beyond dents and scratches: Logical constraints in unsupervised anomaly detection and localization. Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, Carsten Steger, International Journal of Computer Vision. 13042022. 1, 2, 3, 5, 6</p>
<p>Sub-image anomaly detection with deep pyramid correspondences. Niv Cohen, Yedid Hoshen, arXiv:2005.02357202057arXiv preprint</p>
<p>Set features for fine-grained anomaly detection. Niv Cohen, Issar Tzachor, Yedid Hoshen, arXiv:2302.12245202356arXiv preprint</p>
<p>Generating and reweighting dense contrastive patterns for unsupervised anomaly detection. Songmin Dai, Yifan Wu, Xiaoqiang Li, Xiangyang Xue, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 2, 5, 6</p>
<p>Padim: a patch distribution modeling framework for anomaly detection and localization. Thomas Defard, Aleksandr Setkov, Angelique Loesch, Romaric Audigier, International Conference on Pattern Recognition. Springer202167</p>
<p>Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, 2021</p>
<p>Datacomp: In search of the next generation of multimodal datasets. Yitzhak Samir, Gabriel Gadre, Alex Ilharco, Jonathan Fang, Georgios Hayase, Thao Smyrnis, Ryan Nguyen, Mitchell Marten, Dhruba Wortsman, Jieyu Ghosh, Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>Deep anomaly detection using geometric transformations. Izhak Golan, Ran El-Yaniv, 201831Advances in neural information processing systems</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Template-guided hierarchical feature restoration for anomaly detection. Hewei Guo, Liping Ren, Jingjing Fu, Yuwang Wang, Zhizheng Zhang, Cuiling Lan, Haoqian Wang, Xinwen Hou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023. 1, 2, 5, 6</p>
<p>Pay attention to your neighbours: Training-free open-vocabulary semantic segmentation. Sina Hajimiri, Ismail Ben Ayed, Jose Dolz, arXiv:2404.081812024arXiv preprint</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition19606-19616, 2023. 2, 5, 6, 7</p>
<p>Few shot part segmentation reveals compositional logic for industrial anomaly detection. Soopil Kim, Sion An, Philip Chikontwe, Myeongkyun Kang, Ehsan Adeli, Kilian M Pohl, Sang Hyun Park, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024. 1, 2, 5, 6</p>
<p>Segment anything. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision202361</p>
<p>The hungarian method for the assignment problem. Harold W Kuhn, Naval research logistics quarterly. 21-21955</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 2023</p>
<p>Promptad: Learning prompts with only normal samples for few-shot anomaly detection. Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, Lizhuang Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024. 1, 2, 5, 6, 7</p>
<p>Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia, arXiv:2403.18814Mini-gemini: Mining the potential of multi-modality vision language models. 20242arXiv preprint</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20242</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2024. 2, 836</p>
<p>Simplenet: A simple network for image anomaly detection and localization. Zhikang Liu, Yiming Zhou, Yuansheng Xu, Zilei Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023. 2, 5, 6</p>
<p>Crepe: Can vision-language foundation models reason compositionally. Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, Ranjay Krishna, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20232</p>
<p>Compositional chain-of-thought prompting for large multimodal models. Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20242</p>
<p>Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, arXiv:2304.07193Learning robust visual features without supervision. 2023. 2, 4, 6, 1arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. 2021. 2, 471</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schölkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022. 1, 2, 4, 5, 6, 7</p>
<p>Generalad: Anomaly detection across domains by attending to distorted features. Mohammadreza Luc Pj Sträter, Efstratios Salehi, Gavves, G M Cees, Yuki M Snoek, Asano, arXiv:2407.124272024. 3, 5, 6arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.118052023arXiv preprint</p>
<p>Winoground: Probing vision and language models for visiolinguistic compositionality. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, Candace Ross, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Sam-clip: Merging vision foundation models towards semantic and spatial understanding. Haoxiang Wang, Pavan Kumar Anasosalu, Fartash Vasu, Raviteja Faghri, Mehrdad Vemulapalli, Sachin Farajtabar, Mohammad Mehta, Oncel Rastegari, Hadi Tuzel, Pouransari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202424</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>A unified model for multi-class anomaly detection. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le, Advances in Neural Information Processing Systems. 202235</p>
<p>Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, Yonghui Wu, arXiv:2205.01917Coca: Contrastive captioners are image-text foundation models. 2022arXiv preprint</p>
<p>When and why visionlanguage models behave like bags-of-words, and what to do about it?. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, James Zou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Dsra dual subspace re-projection network for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skočaj, European conference on computer vision. Springer202256</p>
<p>Investigating compositional challenges in vision-language models for visual grounding. Yunan Zeng, Yan Huang, Jinjin Zhang, Zequn Jie, Zhenhua Chai, Liang Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20242</p>
<p>Logicode: an llm-driven framework for logical anomaly detection. Yiheng Zhang, Yunkang Cao, Xiaohao Xu, Weiming Shen, IEEE Transactions on Automation Science and Engineering. 22024</p>
<p>Multimodal chain-ofthought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.009232023arXiv preprint</p>
<p>Omnial: A unified cnn framework for unsupervised anomaly localization. Ying Zhao, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>AnomalyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. Springer202215</p>            </div>
        </div>

    </div>
</body>
</html>
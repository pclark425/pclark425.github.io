<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5480 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5480</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5480</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-e51d38bbabca719cd9c845d1e004c724e9cbdfa4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e51d38bbabca719cd9c845d1e004c724e9cbdfa4" target="_blank">How is ChatGPT's behavior changing over time?</a></p>
                <p><strong>Paper Venue:</strong> Special Issue 5: Grappling With the Generative AI Revolution</p>
                <p><strong>Paper TL;DR:</strong> Overall, the findings show that the behavior of the"same"LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.</p>
                <p><strong>Paper Abstract:</strong> GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the"same"LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5480.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5480.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrimeTest-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prime vs Composite evaluation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 (March 2023 vs June 2023) on a 1,000-sample prime-vs-composite task using Chain-of-Thought (CoT) prompting and a no-CoT binary prompt; measures accuracy, verbosity, and answer mismatch across versions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 as provided via ChatGPT/OpenAI service; the paper treats it as a black-box LLM service (part of ChatGPT) queried with user prompts; architecture/training data not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Prime vs Composite (Math I)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>numerical reasoning / logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Binary classification whether a given integer is prime or composite. Dataset: 1,000 integers (500 primes sampled from prior work and 500 composites uniformly from [1,000, 20,000]). Prompts included Chain-of-Thought (CoT) 'think step-by-step' instructions or a no-CoT binary-answer prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>March 2023: accuracy with CoT 84.0%; without CoT 59.6%. June 2023: accuracy with CoT 51.1%; without CoT 51.0%. Verbosity (average characters) dropped for GPT-4 from 638.3 (Mar) to 3.9 (Jun). Large answer mismatch between Mar and Jun.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported in the paper for this task; therefore no direct LLM-vs-human comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Major drop in GPT-4 ability between Mar and Jun coinciding with loss of following CoT instructions: in March GPT-4 followed step-by-step reasoning and achieved large CoT benefit (+24.4%), while in June it largely ignored CoT and accuracy collapsed to ~51%; June version showed extreme bias labelling almost all integers as 'composite' (~99.7% composite rate). Paper notes arithmetic errors can still occur even when CoT followed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5480.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PrimeTest-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prime vs Composite evaluation (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3.5 (March 2023 vs June 2023) on the same 1,000-sample prime-vs-composite task with CoT vs no-CoT prompting; measured accuracy, verbosity, and mismatch over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 as provided via ChatGPT/OpenAI service; treated as a black-box LLM service in the experiments; architecture/training data not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Prime vs Composite (Math I)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>numerical reasoning / logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: binary prime/composite classification on 1,000 integers with CoT vs no-CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>March 2023: no-CoT 50.5%, CoT 49.6% (CoT slightly negative effect -0.9%). June 2023: no-CoT 60.4%, CoT 76.2% (CoT benefit +15.8%). GPT-3.5 verbosity increased ~22% from March to June. Large answer mismatch between versions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; no direct comparison to human performance in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Opposite drift from GPT-4: GPT-3.5 improved from March to June and began to follow CoT more reliably in June, yielding a large CoT benefit; March GPT-3.5 sometimes produced the final answer before reasoning steps leading to token-extraction errors (fixed in June).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5480.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HappyNumbers-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counting Happy Numbers evaluation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 (Mar vs Jun 2023) on counting 'happy numbers' in short integer intervals (quantitative reasoning requiring repeated digit-squaring iterations) with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 via ChatGPT/OpenAI service; treated as a black-box; paper does not specify model size or training corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Counting Happy Numbers (Math II)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>iterative numerical reasoning / problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Given an interval (size uniformly chosen between 6 and 10; start uniformly from 500 to 15,000), the model must count exactly how many numbers in the interval are 'happy' (a number that eventually reaches 1 under repeated sum-of-squares-of-digits transformation). CoT prompting was used to encourage stepwise reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>March 2023: accuracy 83.6% (with CoT), no-CoT 27.0%; June 2023: accuracy 35.2% (with CoT), no-CoT 32.0%. Reported CoT boost for GPT-4: +56.6% in March, only +3.2% in June. Verbosity dropped dramatically for GPT-4 (average characters 2163.5 in Mar to 10.0 in Jun). High answer-change rate: 67.6% of GPT-4 final answers changed between March and June.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper does not provide human baseline data for this counting task; no direct human-vs-model comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>GPT-4's June update showed greatly reduced CoT adherence and terse outputs, producing substantially lower accuracy and a biased distribution (June GPT-4 often answered that there were 0 or 1 happy numbers). The paper notes CoT efficacy changed drastically across versions, indicating instruction-following shifts as a major factor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5480.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HappyNumbers-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counting Happy Numbers evaluation (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3.5 (Mar vs Jun 2023) on the happy-number counting task with comparisons of CoT vs no-CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 as provided via ChatGPT/OpenAI service; details of architecture/training not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Counting Happy Numbers (Math II)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>iterative numerical reasoning / problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>As above: count exact number of happy numbers in short intervals; CoT prompting used in some runs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>March 2023: accuracy with CoT 30.6%, no-CoT 32.2% (CoT slightly negative -1.6%). June 2023: accuracy with CoT 48.2%, no-CoT 27.6% (CoT benefit +20.6%). GPT-3.5 verbosity increased by >60% from March to June. 77.2% of final answers changed across versions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided in the paper for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>GPT-3.5 improved from March to June and exhibited an increased tendency to produce longer step-by-step reasoning in June (which sometimes caused token-length issues and omitted final answers). GPT-3.5 in June also tended to overestimate counts compared to ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5480.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual-ARC-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ARC visual reasoning evaluation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 on the Abstraction and Reasoning Corpus (ARC) visual reasoning tasks (467 samples fit in context window), measuring exact-match between generated output grid and ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 via ChatGPT/OpenAI service; visual inputs encoded as 2-D arrays of color values and presented in text form; model treated as black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>ARC (Abstraction and Reasoning Corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>visual-spatial reasoning / abstract reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>ARC tasks present an input grid and output grid mapping; model must infer the transformation rule from examples and produce the correct output grid. Here 467 tasks compatible with context window were evaluated; inputs and outputs represented as 2-D arrays of color indices.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Exact-match rate: reported overall ~27.4% for GPT-4 (paper reports a ~2% improvement from March to June). Generation length roughly unchanged. Over 90% of visual puzzle queries produced identical generations between March and June for each model, yet overall performance remained low.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided in the paper; ARC is designed to be challenging and previous work reports human-level performance is much higher, but this paper does not supply those figures.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Overall low performance (27.4%) indicates substantial gap to typical human abstract-reasoning performance; authors note uneven improvements and that GPT-4 made some queries worse in June even while overall metric improved. Visual inputs were simplified to textual array encodings, which may limit comparability to human visual task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5480.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual-ARC-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ARC visual reasoning evaluation (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-3.5 on the ARC visual reasoning tasks, measuring exact-match between predicted and ground-truth output grids.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 via ChatGPT/OpenAI service; visual puzzles represented as textual 2-D arrays of colors.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>ARC (Abstraction and Reasoning Corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>visual-spatial reasoning / abstract reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: infer rule from examples and generate correct output grid; 467 examples evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Exact-match rate: reported ~12.2% overall (paper notes ~2% improvement from March to June). Most generations (>90%) were identical between March and June.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided within the paper; therefore no direct numerical comparison to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Very low absolute performance indicates this class of abstract visual reasoning remains challenging; text-encoding of images and lack of human baseline limit interpretation as a cognitive-psychology style comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5480.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructionFidelity-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-agnostic instruction-following evaluation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Benchmark measuring GPT-4's fidelity to task-agnostic, single and composite textual instructions (e.g., answer extraction formatting, 'stop apologizing', writing constraints, and text formatting) applied to arXiv abstracts and other text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 (ChatGPT/OpenAI) used to execute explicit formatting and style instructions on textual inputs; model treated as black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Instruction-following benchmark (task-agnostic instruction fidelity)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>executive control / adherence to instructions (related to cognitive control and task compliance)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A curated set of instruction types: answer extraction (e.g., put yes/no inside squared brackets), stop apologizing (do not say 'sorry' or 'as an AI model'), writing constraint (e.g., generate words starting/ending with specified letters), and formatting transformations (e.g., wrap first letter of each word in brackets). Evaluated on 200 arXiv abstracts and sensitiveQA data; also composite instructions formed by combining two single instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Large drop in instruction fidelity from March to June 2023: example metrics include answer-extraction follow rate 99.5% (Mar) -> 0.5% (Jun); content-filtering fidelity 74.0% (Mar) -> 19.0% (Jun). On composite instructions, notable drops (e.g., remove quotations + add comma to each word dropped by 24% from Mar to Jun; add comma + capitalize dropped by 9.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline provided in the paper; the evaluation is focused on instruction adherence of LLM versions rather than comparing to human instruction-following rates.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Strong temporal drift: GPT-4 became much less likely to follow explicit instructions in June. Error patterns include capitalization of bracketed answers, repeated apologies despite instruction not to apologize, and failing to satisfy combined constraints. This maps to a degradation in a form of 'executive control' but no formal cognitive-psychology human comparison is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5480.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HotpotQA-Agent-GPT4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain HotpotQA multi-hop agent evaluation (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of a LangChain HotpotQA 'ReAct' agent using GPT-4 as backbone to answer multi-hop knowledge-intensive questions from HotpotQA; measured exact-match, verbosity, and mismatch over time (Mar vs Jun 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used inside LangChain agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used as the reasoning component within a LangChain ReAct agent that searches Wikipedia passages and generates multi-step answers; end-to-end pipeline performance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>HotpotQA multi-hop question answering (via LangChain agent)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>multi-hop reasoning / complex knowledge integration</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Agent uses LLM to search and combine multiple evidence passages (Wikipedia) to answer complex multi-hop questions from the HotpotQA dataset; evaluated by exact-match with ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>GPT-4 exact-match: March 2023 ~1.2%; June 2023 ~37.8% (substantial increase). Verbosity decreased in June; high mismatch between versions (>80% of final answers changed). The March GPT-4 often failed to follow the specific output format required by LangChain agent prompting, causing parsing failures.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline or human performance reported for these HotpotQA runs in this paper (HotpotQA has human performance in original dataset papers, but this paper does not report it).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Large changes across versions; improvements in exact-match for GPT-4 from Mar to Jun but procedural brittlenessâ€”March outputs often did not conform to agent-required format and thus caused pipeline failures. The evaluation is of a pipeline (agent + LLM) rather than pure LLM cognitive test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5480.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5480.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HotpotQA-Agent-GPT3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain HotpotQA multi-hop agent evaluation (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of the LangChain HotpotQA ReAct agent using GPT-3.5 as backbone on HotpotQA multi-hop questions, comparing March 2023 and June 2023 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How Is ChatGPT's Behavior Changing over Time?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (used inside LangChain agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5 used as reasoning component inside a LangChain multi-hop QA pipeline; model treated as black-box.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>HotpotQA multi-hop question answering (via LangChain agent)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>multi-hop reasoning / complex knowledge integration</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>As above: answer complex, multi-hop questions by searching and integrating information from Wikipedia, using ReAct-style prompts inside LangChain.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Exact-match rate for GPT-3.5 dropped by ~9% from March to June (exact numeric values not provided in text extract beyond 'dropped by almost 9%'). Generation verbosity increased by ~30% over time. Large mismatch (>80%) between versions.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper does not provide human baseline for this agent evaluation; no direct LLM-to-human comparison is given.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Performance drift and brittleness of agent prompts caused pipeline failures; GPT-3.5 lost capability on some instances between March and June. Evaluation conflates LLM reasoning with agent prompt stability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': "How is ChatGPT's behavior changing over time?", 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>On the measure of intelligence <em>(Rating: 2)</em></li>
                <li>Hotpotqa: A dataset for diverse, explainable multi-hop question answering <em>(Rating: 2)</em></li>
                <li>Is GPT-4 getting worse over time? <em>(Rating: 2)</em></li>
                <li>Whose opinions do language models reflect? <em>(Rating: 1)</em></li>
                <li>Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models <em>(Rating: 1)</em></li>
                <li>Evaluating the logical reasoning ability of chatgpt and gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5480",
    "paper_id": "paper-e51d38bbabca719cd9c845d1e004c724e9cbdfa4",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "PrimeTest-GPT4",
            "name_full": "Prime vs Composite evaluation (GPT-4)",
            "brief_description": "Evaluation of GPT-4 (March 2023 vs June 2023) on a 1,000-sample prime-vs-composite task using Chain-of-Thought (CoT) prompting and a no-CoT binary prompt; measures accuracy, verbosity, and answer mismatch across versions.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 as provided via ChatGPT/OpenAI service; the paper treats it as a black-box LLM service (part of ChatGPT) queried with user prompts; architecture/training data not specified in the paper.",
            "model_size": null,
            "cognitive_test_name": "Prime vs Composite (Math I)",
            "cognitive_test_type": "numerical reasoning / logical reasoning",
            "cognitive_test_description": "Binary classification whether a given integer is prime or composite. Dataset: 1,000 integers (500 primes sampled from prior work and 500 composites uniformly from [1,000, 20,000]). Prompts included Chain-of-Thought (CoT) 'think step-by-step' instructions or a no-CoT binary-answer prompt.",
            "llm_performance": "March 2023: accuracy with CoT 84.0%; without CoT 59.6%. June 2023: accuracy with CoT 51.1%; without CoT 51.0%. Verbosity (average characters) dropped for GPT-4 from 638.3 (Mar) to 3.9 (Jun). Large answer mismatch between Mar and Jun.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported in the paper for this task; therefore no direct LLM-vs-human comparison is provided.",
            "notable_differences_or_limitations": "Major drop in GPT-4 ability between Mar and Jun coinciding with loss of following CoT instructions: in March GPT-4 followed step-by-step reasoning and achieved large CoT benefit (+24.4%), while in June it largely ignored CoT and accuracy collapsed to ~51%; June version showed extreme bias labelling almost all integers as 'composite' (~99.7% composite rate). Paper notes arithmetic errors can still occur even when CoT followed.",
            "uuid": "e5480.0",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "PrimeTest-GPT3.5",
            "name_full": "Prime vs Composite evaluation (GPT-3.5)",
            "brief_description": "Evaluation of GPT-3.5 (March 2023 vs June 2023) on the same 1,000-sample prime-vs-composite task with CoT vs no-CoT prompting; measured accuracy, verbosity, and mismatch over time.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 as provided via ChatGPT/OpenAI service; treated as a black-box LLM service in the experiments; architecture/training data not specified in the paper.",
            "model_size": null,
            "cognitive_test_name": "Prime vs Composite (Math I)",
            "cognitive_test_type": "numerical reasoning / logical reasoning",
            "cognitive_test_description": "Same as above: binary prime/composite classification on 1,000 integers with CoT vs no-CoT prompting.",
            "llm_performance": "March 2023: no-CoT 50.5%, CoT 49.6% (CoT slightly negative effect -0.9%). June 2023: no-CoT 60.4%, CoT 76.2% (CoT benefit +15.8%). GPT-3.5 verbosity increased ~22% from March to June. Large answer mismatch between versions.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; no direct comparison to human performance in the paper.",
            "notable_differences_or_limitations": "Opposite drift from GPT-4: GPT-3.5 improved from March to June and began to follow CoT more reliably in June, yielding a large CoT benefit; March GPT-3.5 sometimes produced the final answer before reasoning steps leading to token-extraction errors (fixed in June).",
            "uuid": "e5480.1",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HappyNumbers-GPT4",
            "name_full": "Counting Happy Numbers evaluation (GPT-4)",
            "brief_description": "Evaluation of GPT-4 (Mar vs Jun 2023) on counting 'happy numbers' in short integer intervals (quantitative reasoning requiring repeated digit-squaring iterations) with CoT prompting.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 via ChatGPT/OpenAI service; treated as a black-box; paper does not specify model size or training corpora.",
            "model_size": null,
            "cognitive_test_name": "Counting Happy Numbers (Math II)",
            "cognitive_test_type": "iterative numerical reasoning / problem solving",
            "cognitive_test_description": "Given an interval (size uniformly chosen between 6 and 10; start uniformly from 500 to 15,000), the model must count exactly how many numbers in the interval are 'happy' (a number that eventually reaches 1 under repeated sum-of-squares-of-digits transformation). CoT prompting was used to encourage stepwise reasoning.",
            "llm_performance": "March 2023: accuracy 83.6% (with CoT), no-CoT 27.0%; June 2023: accuracy 35.2% (with CoT), no-CoT 32.0%. Reported CoT boost for GPT-4: +56.6% in March, only +3.2% in June. Verbosity dropped dramatically for GPT-4 (average characters 2163.5 in Mar to 10.0 in Jun). High answer-change rate: 67.6% of GPT-4 final answers changed between March and June.",
            "human_baseline_performance": null,
            "performance_comparison": "Paper does not provide human baseline data for this counting task; no direct human-vs-model comparison reported.",
            "notable_differences_or_limitations": "GPT-4's June update showed greatly reduced CoT adherence and terse outputs, producing substantially lower accuracy and a biased distribution (June GPT-4 often answered that there were 0 or 1 happy numbers). The paper notes CoT efficacy changed drastically across versions, indicating instruction-following shifts as a major factor.",
            "uuid": "e5480.2",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HappyNumbers-GPT3.5",
            "name_full": "Counting Happy Numbers evaluation (GPT-3.5)",
            "brief_description": "Evaluation of GPT-3.5 (Mar vs Jun 2023) on the happy-number counting task with comparisons of CoT vs no-CoT prompting.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 as provided via ChatGPT/OpenAI service; details of architecture/training not provided in paper.",
            "model_size": null,
            "cognitive_test_name": "Counting Happy Numbers (Math II)",
            "cognitive_test_type": "iterative numerical reasoning / problem solving",
            "cognitive_test_description": "As above: count exact number of happy numbers in short intervals; CoT prompting used in some runs.",
            "llm_performance": "March 2023: accuracy with CoT 30.6%, no-CoT 32.2% (CoT slightly negative -1.6%). June 2023: accuracy with CoT 48.2%, no-CoT 27.6% (CoT benefit +20.6%). GPT-3.5 verbosity increased by &gt;60% from March to June. 77.2% of final answers changed across versions.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided in the paper for this task.",
            "notable_differences_or_limitations": "GPT-3.5 improved from March to June and exhibited an increased tendency to produce longer step-by-step reasoning in June (which sometimes caused token-length issues and omitted final answers). GPT-3.5 in June also tended to overestimate counts compared to ground truth.",
            "uuid": "e5480.3",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Visual-ARC-GPT4",
            "name_full": "ARC visual reasoning evaluation (GPT-4)",
            "brief_description": "Evaluation of GPT-4 on the Abstraction and Reasoning Corpus (ARC) visual reasoning tasks (467 samples fit in context window), measuring exact-match between generated output grid and ground truth.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 via ChatGPT/OpenAI service; visual inputs encoded as 2-D arrays of color values and presented in text form; model treated as black-box.",
            "model_size": null,
            "cognitive_test_name": "ARC (Abstraction and Reasoning Corpus)",
            "cognitive_test_type": "visual-spatial reasoning / abstract reasoning",
            "cognitive_test_description": "ARC tasks present an input grid and output grid mapping; model must infer the transformation rule from examples and produce the correct output grid. Here 467 tasks compatible with context window were evaluated; inputs and outputs represented as 2-D arrays of color indices.",
            "llm_performance": "Exact-match rate: reported overall ~27.4% for GPT-4 (paper reports a ~2% improvement from March to June). Generation length roughly unchanged. Over 90% of visual puzzle queries produced identical generations between March and June for each model, yet overall performance remained low.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided in the paper; ARC is designed to be challenging and previous work reports human-level performance is much higher, but this paper does not supply those figures.",
            "notable_differences_or_limitations": "Overall low performance (27.4%) indicates substantial gap to typical human abstract-reasoning performance; authors note uneven improvements and that GPT-4 made some queries worse in June even while overall metric improved. Visual inputs were simplified to textual array encodings, which may limit comparability to human visual task performance.",
            "uuid": "e5480.4",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Visual-ARC-GPT3.5",
            "name_full": "ARC visual reasoning evaluation (GPT-3.5)",
            "brief_description": "Evaluation of GPT-3.5 on the ARC visual reasoning tasks, measuring exact-match between predicted and ground-truth output grids.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "GPT-3.5 via ChatGPT/OpenAI service; visual puzzles represented as textual 2-D arrays of colors.",
            "model_size": null,
            "cognitive_test_name": "ARC (Abstraction and Reasoning Corpus)",
            "cognitive_test_type": "visual-spatial reasoning / abstract reasoning",
            "cognitive_test_description": "Same as above: infer rule from examples and generate correct output grid; 467 examples evaluated.",
            "llm_performance": "Exact-match rate: reported ~12.2% overall (paper notes ~2% improvement from March to June). Most generations (&gt;90%) were identical between March and June.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided within the paper; therefore no direct numerical comparison to human performance.",
            "notable_differences_or_limitations": "Very low absolute performance indicates this class of abstract visual reasoning remains challenging; text-encoding of images and lack of human baseline limit interpretation as a cognitive-psychology style comparison.",
            "uuid": "e5480.5",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "InstructionFidelity-GPT4",
            "name_full": "Task-agnostic instruction-following evaluation (GPT-4)",
            "brief_description": "Benchmark measuring GPT-4's fidelity to task-agnostic, single and composite textual instructions (e.g., answer extraction formatting, 'stop apologizing', writing constraints, and text formatting) applied to arXiv abstracts and other text.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 (ChatGPT/OpenAI) used to execute explicit formatting and style instructions on textual inputs; model treated as black-box.",
            "model_size": null,
            "cognitive_test_name": "Instruction-following benchmark (task-agnostic instruction fidelity)",
            "cognitive_test_type": "executive control / adherence to instructions (related to cognitive control and task compliance)",
            "cognitive_test_description": "A curated set of instruction types: answer extraction (e.g., put yes/no inside squared brackets), stop apologizing (do not say 'sorry' or 'as an AI model'), writing constraint (e.g., generate words starting/ending with specified letters), and formatting transformations (e.g., wrap first letter of each word in brackets). Evaluated on 200 arXiv abstracts and sensitiveQA data; also composite instructions formed by combining two single instructions.",
            "llm_performance": "Large drop in instruction fidelity from March to June 2023: example metrics include answer-extraction follow rate 99.5% (Mar) -&gt; 0.5% (Jun); content-filtering fidelity 74.0% (Mar) -&gt; 19.0% (Jun). On composite instructions, notable drops (e.g., remove quotations + add comma to each word dropped by 24% from Mar to Jun; add comma + capitalize dropped by 9.2%).",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline provided in the paper; the evaluation is focused on instruction adherence of LLM versions rather than comparing to human instruction-following rates.",
            "notable_differences_or_limitations": "Strong temporal drift: GPT-4 became much less likely to follow explicit instructions in June. Error patterns include capitalization of bracketed answers, repeated apologies despite instruction not to apologize, and failing to satisfy combined constraints. This maps to a degradation in a form of 'executive control' but no formal cognitive-psychology human comparison is provided.",
            "uuid": "e5480.6",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HotpotQA-Agent-GPT4",
            "name_full": "LangChain HotpotQA multi-hop agent evaluation (GPT-4)",
            "brief_description": "Evaluation of a LangChain HotpotQA 'ReAct' agent using GPT-4 as backbone to answer multi-hop knowledge-intensive questions from HotpotQA; measured exact-match, verbosity, and mismatch over time (Mar vs Jun 2023).",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used inside LangChain agent)",
            "model_description": "GPT-4 used as the reasoning component within a LangChain ReAct agent that searches Wikipedia passages and generates multi-step answers; end-to-end pipeline performance reported.",
            "model_size": null,
            "cognitive_test_name": "HotpotQA multi-hop question answering (via LangChain agent)",
            "cognitive_test_type": "multi-hop reasoning / complex knowledge integration",
            "cognitive_test_description": "Agent uses LLM to search and combine multiple evidence passages (Wikipedia) to answer complex multi-hop questions from the HotpotQA dataset; evaluated by exact-match with ground-truth answers.",
            "llm_performance": "GPT-4 exact-match: March 2023 ~1.2%; June 2023 ~37.8% (substantial increase). Verbosity decreased in June; high mismatch between versions (&gt;80% of final answers changed). The March GPT-4 often failed to follow the specific output format required by LangChain agent prompting, causing parsing failures.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline or human performance reported for these HotpotQA runs in this paper (HotpotQA has human performance in original dataset papers, but this paper does not report it).",
            "notable_differences_or_limitations": "Large changes across versions; improvements in exact-match for GPT-4 from Mar to Jun but procedural brittlenessâ€”March outputs often did not conform to agent-required format and thus caused pipeline failures. The evaluation is of a pipeline (agent + LLM) rather than pure LLM cognitive test.",
            "uuid": "e5480.7",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "HotpotQA-Agent-GPT3.5",
            "name_full": "LangChain HotpotQA multi-hop agent evaluation (GPT-3.5)",
            "brief_description": "Evaluation of the LangChain HotpotQA ReAct agent using GPT-3.5 as backbone on HotpotQA multi-hop questions, comparing March 2023 and June 2023 performance.",
            "citation_title": "How Is ChatGPT's Behavior Changing over Time?",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (used inside LangChain agent)",
            "model_description": "GPT-3.5 used as reasoning component inside a LangChain multi-hop QA pipeline; model treated as black-box.",
            "model_size": null,
            "cognitive_test_name": "HotpotQA multi-hop question answering (via LangChain agent)",
            "cognitive_test_type": "multi-hop reasoning / complex knowledge integration",
            "cognitive_test_description": "As above: answer complex, multi-hop questions by searching and integrating information from Wikipedia, using ReAct-style prompts inside LangChain.",
            "llm_performance": "Exact-match rate for GPT-3.5 dropped by ~9% from March to June (exact numeric values not provided in text extract beyond 'dropped by almost 9%'). Generation verbosity increased by ~30% over time. Large mismatch (&gt;80%) between versions.",
            "human_baseline_performance": null,
            "performance_comparison": "Paper does not provide human baseline for this agent evaluation; no direct LLM-to-human comparison is given.",
            "notable_differences_or_limitations": "Performance drift and brittleness of agent prompts caused pipeline failures; GPT-3.5 lost capability on some instances between March and June. Evaluation conflates LLM reasoning with agent prompt stability.",
            "uuid": "e5480.8",
            "source_info": {
                "paper_title": "How is ChatGPT's behavior changing over time?",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "On the measure of intelligence",
            "rating": 2
        },
        {
            "paper_title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering",
            "rating": 2
        },
        {
            "paper_title": "Is GPT-4 getting worse over time?",
            "rating": 2
        },
        {
            "paper_title": "Whose opinions do language models reflect?",
            "rating": 1
        },
        {
            "paper_title": "Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models",
            "rating": 1
        },
        {
            "paper_title": "Evaluating the logical reasoning ability of chatgpt and gpt-4",
            "rating": 1
        }
    ],
    "cost": 0.016515,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Is ChatGPT's Behavior Changing over Time?</h1>
<p>Lingjiao Chen ${ }^{\dagger}$, Matei Zaharia ${ }^{\ddagger}$, James Zou ${ }^{\dagger}$<br>â€ Stanford University ${ }^{\ddagger}$ UC Berkeley</p>
<h4>Abstract</h4>
<p>GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers ( $84 \%$ accuracy) but GPT-4 (June 2023) was poor on these same questions ( $51 \%$ accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the "same" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) like GPT-3.5 and GPT-4 are being widely used. A LLM like GPT-4 can be updated over time based on data and feedback from users as well as design changes. However, it is currently opaque when and how GPT-3.5 and GPT-4 are updated, and it is unclear how each update affects the behavior of these LLMs. These unknowns makes it challenging to stably integrate LLMs into larger workflows: if LLM's response to a prompt (e.g. its accuracy or formatting) suddenly changes, this might break the downstream pipeline. It also makes it challenging, if not impossible, to reproduce results from the "same" LLM.</p>
<p>Beyond these integration challenges, it is also an interesting question whether an LLM service like GPT-4 is consistently improving over time. It is important to know whether updates to the model aimed at improving some aspects can reduce its capability in other dimensions.</p>
<p>Motivated by these questions, we evaluated the behavior of the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) answering opinion surveys, 4) answering multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License exams, and 7) visual reasoning. These tasks were selected to evaluate diverse and useful capabilities of these LLMs. We find that the performance and behavior of both GPT-3.5 and GPT-4 varied significantly across these two releases and that their performance on some tasks have gotten substantially worse over time, while they have improved on other problems (summarized in Figure 1).</p>
<p>How to explain those performance and behavior drifts? We hypothesize that changes in ChatGPT's ability to follow user instructions could be a common factor behind the drifts across tasks. As a first step towards testing this hypothesis, we have curated a set of task-agnostic instructions, and evaluate the March and June versions of GPT-4 and GPT-3.5 on it. Overall, we observe a large decrease of GPT-4's ability to follow many instructions. GPT-4 in March was typically good at following user's instructions (e.g. generating responses following specified formats), but in June it failed to follow most of these simple instructions (Figure 1).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of performance drift (a) and instruction following shift (b) of GPT-4 (left panel) and GPT-3.5 (right panel) between March 2023 and June 2023. Higher evaluation metric is better. On eight diverse tasks (detailed below), the models' performance drifts considerably over time, and sometimes for the worse. The decrease of GPT-4's ability to follow instructions over time matched its behavior drift and partially explained the corresponding performance drops.</p>
<p>Our findings highlight the need to continuously monitor LLMs' behavior over time. All prompts we curated in this paper and responses from GPT-4 and GPT-3.5 in both March and June are collected and released in https://github.com/lchen001/LLHDrift. Our analysis and visualization code has also been open-sourced. We hope our work stimulates more study on LLM drifts to enable trustworthy and reliable LLM applications.</p>
<p>Related Work. There have been multiple benchmarks and evaluations of LLMs including GPT-3.5 and GPT-4 [LBL+22, ZPM+23, LNT+23, BCL+23]. Existing works show that LLMs achieve reasonable performance on traditional language tasks such as reading comprehension [dW23], translation [JWH+23], and summarization [GLD22]. More recently, GPT-4 was shown to successfully pass difficult exams in professional domains such as medicine [NKM+23] and law [KBGA23]. To the best of our knowledge, most of these works do not systematically monitor the longitudinal drifts of widely used LLM services over time or report large drifts in them. ChatLog [TLY+23] proposed recording and monitoring ChatGPT's responses automatically over time and reported small shifts (most below 5%) in ChatGPT's performance on some common benchmarks. Other papers [AAKA23, SKNM23] also reported shifts in specific problems. Monitoring model performance shifts is an emerging research area for machine-learning-as-a-service (MLaaS) more broadly. [CJE+22] offers a large-scale longitudinal dataset of commercial ML service responses on various evaluation tasks, and [CCZZ21] studies how to efficiently estimate ML service performance shifts. Those papers focus on ML services for simple classification tasks such as sentiment analysis, while this work studies generative LLM services.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance of the March 2023 and June 2023 versions of GPT-4 and GPT-3.5 on eight tasks: (a,b) solving math problems (Prime vs Composite and Happy Numbers), (c) responding to sensitive questions and (d) opinion surveys, (e) running a LangChain app for multi-hop question answering, (f) generating executable code, (g) the USMLE medical exam, and (h) visual reasoning. For each task, one example is shown in a purple box, and the number of examples $n$ is in the caption. The models' performance varies substantially over time, and sometimes for the worse.</p>
<h1>2 Overview: LLM Services, Tasks and Metrics</h1>
<p>This paper studies how different LLMs' behaviors change over time. To answer it quantitatively, we need to specify (i) which LLM services to monitor, (ii) on which application scenarios to focus, and (iii) how to measure LLM drifts in each scenario.</p>
<p>LLM Services. The LLM services monitored in this paper are GPT-4 and GPT-3.5, which form the backbone of ChatGPT. Due to the popularity of ChatGPT, both GPT-4 and GPT-3.5 have been widely adopted by individual users and a number of businesses. Thus, timely and systematically monitoring these two services helps a large range of users better understand and leverage LLMs for their own use cases. At the time of writing, there are two major versions available for GPT-4 and GPT-3.5 through OpenAI's API, one snapshotted in March 2023 and another in June 2023. Therefore we focus on the drifts between these two dates. For simplicity, we queried these services via the user prompt only and left the system prompt as default. We set the temperature to be 0.1 to reduce output randomness, as creativity was not needed in our evaluation tasks.</p>
<p>Evaluation Tasks. In this paper, we focus on eight LLM tasks frequently studied in performance and safety benchmarks: solving math problems (including two problem types), answering sensitive questions, answering OpinionQA survey, LangChain HotpotQA Agent, code generation, taking USMLE medical exam, and visual reasoning, as shown in Figure 1. These tasks are selected for two reasons. First, they are diverse tasks frequently used to evaluate LLMs in the literature [WWS<em>22, ZPM</em>23, CTJ*21]. Second, they are relatively objective and thus easy-to-evaluate. For each task, we use queries either sampled from existing datasets or constructed by us. We acknowledge that the specific benchmark datasets used here does not comprehensively cover the complex behaviors of ChatGPT. Our goal here is not to provide a holistic assessment but to demonstrate that substantial ChatGPT performance drift exists on simple tasks. We are adding more benchmarks in future evaluations as part of a broader, long-term study of LLM service behavior. We cover each task in detail in the next section.</p>
<p>Metrics. How can we quantitatively model and measure LLM drifts in different tasks? Here, we consider one main performance metric for each task and two common additional metrics for all tasks. The former captures the performance measurement specific to each scenario, while the latter covers common complementary measurement across different applications.</p>
<p>In particular, we use accuracy (how often an LLM service generates the correct answer) as our main metric for math problems and USMLE questions. For answering sensitive and opinion questions, we use the response rate, i.e. the frequency that an LLM service directly answers a question. For code generation, the main metric is what fraction of the outputs are directly executable (if the code can be directly executed in a programming environment and pass the unit tests). For visual reasoning and LangChain, it is exact match (whether the final response exactly matches the ground truth).</p>
<p>Our first common additional metric is verbosity, i.e., the length of generation measured in the number of characters. The second one is mismatch, i.e. how often, for the same prompt, the extracted answers by two versions of the same LLM service do not match. Note that this only compares the answers' differences, not the raw generations. For example, for math problems, mismatch is 0 if the generated answers are the same, even if the intermediate reasoning steps are different. For each LLM service, we use the mismatch's empirical mean over the entire population to quantify how much an LLM service's desired functionality, instead of the textual outputs, deviates over time. Larger mismatch means greater drifts. For each of the other metrics, We compute its population mean for both the March and June versions, and leverage their differences to measure the drift sizes.</p>
<h1>3 Monitoring Reveals Substantial LLM Drifts</h1>
<h3>3.1 Math I (Prime vs Composite): Chain-of-Thought Can Fail</h3>
<p>How do GPT-4 and GPT-3.5's math solving skills evolve over time? As a canonical study, we explore the drifts in these LLMs' ability to figure out whether a given integer is prime or composite. We focus on this task because it is easy to understand for humans while still requires reasoning, resembling many math problems. The dataset contains 1,000 questions, where 500 primes were extracted from [ZPM<em>23] and 500 composite numbers were sampled uniformly from all composite numbers within the interval $[1,000,20,000]$. To help the LLMs reason, we use Chain-of-Thought (CoT) prompting [WWS</em>22], a standard approach for reasoning-heavy tasks.</p>
<p>Perhaps surprisingly, substantial LLM drifts emerge on this simple task. As shown in Figure 3(a), GPT-4's accuracy dropped from $84.0 \%$ in March to $51.1 \%$ in June, and there was a large improvement of GPT-3.5's accuracy, from $49.6 \%$ to $76.2 \%$. In addition, GPT-4's response became much more compact: its average verbosity (number of generated characters) decreased from 638.3 in March to 3.9 in June. On the other hand, there was about $22.2 \%$ growth in GPT-3.5's response length. The answer mismatch between their March and June versions was also large for both services.</p>
<p>Why was there such a large difference? One possible explanation is change in the chain-of-thought (CoT) behaviors. Figure 3 (b) gives an illustrative example. To determine whether 17077 is a prime number, the GPT-4's March version followed the CoT instruction well. It first decomposed the task into four steps, checking if 17077 is even, finding 17077's square root, obtaining all prime numbers less than it, checking if 17077 is divisible by any of these numbers. Then it executed each step, and finally reached the correct answer that 17077 is indeed a prime number. However, the chain-of-thought did not work for the June version: the service did not generate any intermediate steps, even though</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Math I (prime vs composite). (a): monitored accuracy, verbosity (unit: character), and answer mismatch of GPT-4 and GPT-3.5 between March and June 2023. Overall, a large performance drifts existed for both services. (b) An example query and corresponding responses over time. GPT-4 followed the chain-of-thought instruction to obtain the right answer in March, but ignored it in June with the wrong answer. GPT-3.5 always followed the chain-of-thought, but it insisted on generating a wrong answer ([No]) first in March. This issue was largely fixed in June.
the prompt asked to think step-by-step, and simply produced "No". Chain-of-thought's effects had a different drift pattern for GPT-3.5. In March, GPT-3.5 inclined to generate the answer "No" first and then performed the reasoning steps. Thus, even if the steps and final conclusion (" 17077 is a prime number") were correct, its nominal answer was still wrong. On the other hand, the June update seemed to fix this issue: it started by writing the reasoning steps and finally generate the answer "Yes", which was correct. This interesting phenomenon indicates that the same prompting approach, even the widely adopted chain-of-thought strategy, could lead to substantially different performances due to LLM drifts.</p>
<p>To further investigate the impact of CoT behavior changes, we compared the responses of GPT-4 and GPT-3.5 on the same questions with and without explicit CoT instructions. For the latter, we simply ask the model to give a binary generation without explicitly asking it to think step-by-step (e.g., Is 17077 a prime number? Answer "[Yes]" or "[No]".).</p>
<p>As shown in Table 1, using CoT increased GPT-4's performance from $59.6 \%$ to $84.0 \%$ in March, leading to a $24.4 \%$ performance boost. On the other hand, CoT did not help the June version of GPT-4 much: the accuracy was actually $0.1 \%$ worse. As we discussed before, this is because the new version did not follow the CoT instructions. For GPT-3.5, an opposite trend was observed: by adding CoT, accuracy was marginally better $(+6.3 \%)$ in March, but substantially higher $(+15.8 \%)$ in June. Since GPT-3.5 in both March and June followed the CoT instructions. This suggests that LLM drifts could change both whether and how to follow user instruction.</p>
<p>Our analysis so far is largely based on shifts of the main metric, accuracy, but fine-grained investigations could disclose additional interesting shift patterns. One observation is that June version of GPT-4 had a strong bias to view an integer as a composite number. To see so, we quantified how the confusion matrices shifted over time. As shown in Figure 4(c), GPT-4's June version almost always identified an integer as composite $(49.9 \%+48.8 \%=99.7 \%)$. GPT-3.5's March version exhibited a similar</p>
<p>Table 1: Chain-of-thought's (CoT) effectiveness drifts over time for prime testing. Without CoT, both GPT-4 and GPT-3.5 achieved relatively low accuracy. With CoT, GPT-4 in March obtained a $24.4 \%$ accuracy improvement, which dropped to $-0.1 \%$ in June. On the other hand, the CoT boost increased from $-0.9 \%$ in March to $15.8 \%$ in June for GPT-3.5.</p>
<table>
<thead>
<tr>
<th>LLM Service</th>
<th>GPT-4</th>
<th></th>
<th></th>
<th>GPT-3.5</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Prompting method</td>
<td></td>
<td>$\Delta$</td>
<td>Prompting method</td>
<td></td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Eval Time</td>
<td>No CoT</td>
<td>CoT</td>
<td></td>
<td>No CoT</td>
<td>CoT</td>
<td></td>
</tr>
<tr>
<td>Mar-23</td>
<td>$59.6 \%$</td>
<td>$84.0 \%$</td>
<td>$+\mathbf{2 4 . 4 \%}$</td>
<td>$50.5 \%$</td>
<td>$49.6 \%$</td>
<td>$\mathbf{- 0 . 9 \%}$</td>
</tr>
<tr>
<td>Jun-23</td>
<td>$51.0 \%$</td>
<td>$51.1 \%$</td>
<td>$+\mathbf{0 . 1 \%}$</td>
<td>$60.4 \%$</td>
<td>$76.2 \%$</td>
<td>$+\mathbf{1 5 . 8 \%}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Confusion matrix shift for the prime vs composite problem. GPT-4's March version identified most primes and composite number correctly, while its June version consistently answered most primes as composite.
issue. The confusion matrices for GPT-4 in March and GPT-3.5 in June seem more reasonable: they identified both prime and composite numbers correctly for the major cases. Similar findings have been recently reported [NK23].</p>
<p>Instance-level investigation discloses additional insights. We uniformly randomly sampled 8 numbers to manually inspect March GPT-4's step-by-step logic. March GPT-4's CoT logic was mostly sound in these cases, though it can make arithmetic mistakes, and it reached the correct conclusion in 7 of the 8 cases. We show 4 examples below and provide the others in the Appendix. For example, the following generation by GPT-4 gave the correct reasoning steps for 19997.</p>
<p>Step 1: Check if the number is divisible by 2 . Since 19997 is an odd number, it is not divisible by 2 .
Step 2: Find the square root of the number. The square root of 19997 is approximately 141.4. Step 3: Check for divisibility by prime numbers less than or equal to the square root. We will check for divisibility by $3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67,71,73$, $79,83,89,97,101,103,107,109,113,127,131,137$, and 139.
19997 is not divisible by any of these prime numbers.
Therefore, 19997 is a prime number.
Answer: [Yes]
Asking the same query multiple times gives the same conclusion, though the intermediate reasoning</p>
<p>steps can be slightly different. For example, a second generation by GPT-4 in March was more verbose, as it explicitly checked the divisibility of 19997 by each prime number less than it (see Appendix). In addition, GPT-4 in March also identified composite numbers divisible by 2 or 5 correctly. For example, to determine whether 16450 is prime, GPT-4 in March wrote the following response.</p>
<p>Step 1: Check if the number is even. If it is, then it is not prime (except for 2). 16450 is an even number because it ends in 0 .
So, the answer is $[\mathrm{No}]$.</p>
<p>Similarly, to check 9175 , it gave the following reasoning steps.</p>
<p>Step 1: Find the square root of 9175 . The square root of 9175 is approximately 95.78 .
Step 2: Check for divisibility by smaller prime numbers $(2,3,5,7,11,13,17,19,23,29,31,37$, $41,43,47,53,59,61,67,71,73,79,83,89,97)$.
Step 3: 9175 is divisible by $5(9175 \div 5=1835)$.
Since 9175 is divisible by a prime number other than itself and 1 , it is not a prime number.
Answer: [No]
In both cases, March GPT-4's step-by-step logic led to the right answer. However, it also made subtle mistakes on some other queries. For example, to check whether 4323 is a prime number, it generated the following content.</p>
<ul>
<li>4323 is not divisible by $3(4323 / 3=1441$ with a remainder $)$.</li>
</ul>
<p>Here, it actually used the correct logic (checking if 4323 is divisible by any prime numbers), but made a mistake in calculation ( $4323 / 3=1441$ exactly without a remainder).</p>
<h1>3.2 Math II (Counting Happy Numbers): CoT Also Less Effective</h1>
<p>To further investigate ChatGPT's math problem solving and chain-of-thought behaviors, we asked it to tackle a different math problem: counting the number of happy numbers [Guy04, pp. 357-360] within a given interval. An integer is called happy if replacing it by the sum of the square of its digits repeatedly eventually produces 1 . For example, 13 is a happy number because $1^{2}+3^{2}=10$, and $1^{2}+0^{2}=1$. This task complements prime testing because it asks for a quantitative response (number of happy numbers) rather than a binary decision (e.g., prime or composite) and it only uses simple arithmetic. To assess LLM drift on this task, we constructed a dataset of 500 queries. Each query asks how many happy numbers there are within a given interval and we quantify how often the LLM gets the correct number exactly. The interval size was uniformly randomly selected from 6 to 10 , and the interval starting point was uniformly randomly chosen from 500 to 15,000 . To encourage logic reasoning steps, we adopt CoT prompting again.</p>
<p>We also observed significant performance drifts on this task. As shown in Figure 5(a), GPT-4's accuracy dropped from $83.6 \%$ in March to $35.2 \%$ in June. On the other hand, accuracy of GPT-3.5 increased from $30.6 \%$ to $48.2 \%$. There was also a large change in the verbosity (number of characters in the generated responses). GPT-4's generation length dropped from 2163.5 in March to 10.0 in June, but GPT-3.5's length increased by more than $60 \%$. Compared to prime testing (Math I), the answer lengths on average were significantly larger due to requiring more steps to enumerate the numbers in the interval and repeatedly square digits. In addition, $67.6 \%$ of GPT-4's final answers changed between March and June, as did $77.2 \%$ of GPT-3.5's.</p>
<p>As with the prime number testing task, we observed a large shift in the LLMs' CoT behaviors. As shown in Figure 5(b), GPT-4 in June did not follow the CoT instructions and only gave a final answer, while its March counterpart followed the instructions to leverage reasoning steps. GPT-3.5 followed CoT instructions in both March and June. Its reasoning steps in June were much longer than that in March. While overall this led to better performance, sometimes it was problematic due to exceeding the maximum token length and thus not generating the final answer.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Math II (Counting Happy Numbers). (a) Overall drifts. The accuracy of GPT-4 dropped from $83.6 \%$ to $35.2 \%$. On the other hand, there was a $17.8 \%$ accuracy gain for GPT-3.5. GPT-4 became less verbose while GPT-3.5 generated much longer answers. (b) Example query and corresponding answers. GPT-4 followed the CoT instructions but ignored it in June. GPT-3.5 followed CoT in March and June, and gave a longer reasoning steps in June.</p>
<p>To further understand how the CoT effects' shifts, we asked each service the same query either with or without CoT prompting, and studied how much accuracy gain was achieved by having CoT. We have found that CoT's benefits shifted too. For example, for GPT-4, CoT brought $56.6 \%$ accuracy boost in March but only $3.2 \%$ in June, as shown in Table 2. For GPT-3.5, CoT led to $20.6 \%$ performance gains in June. In March, however, CoT caused a $1.6 \%$ accuracy drop.</p>
<p>The number of mistakes made by GPT-4 and GPT-3.5 changed over time. But what new mistakes did they make? To answer this question, we performed a fine-grained analysis on the confusion matrix of these LLMs over time, as shown in Figure 6. It was interesting to note how the bias of GPT-4 and GPT-3.5 changed over time. GPT-4 in June had a strong belief that there was only 0 or 1 happy number within any given interval. On the other hand, GPT-3.5 in June was inclined to overestimate the number: on more than $10 \%$ queries, it responded that there were more than 4 happy numbers, while 4 was actually the upper bound among all our queries. We also ran additional experiments with smaller intervals for happy numbers and observed similar trends in the LLMs' behavior (see Appendix).</p>
<h1>3.3 Answering Sensitive Questions: Safer but Less Rationale</h1>
<p>Prompting LLMs with sensitive questions is known to lead to harmful generations such as social biases [GLK+22], personal information [CTW+21], and toxic texts [GGS+20]. Thus, another goal of this paper was to understand how LLM services' responses to sensitive questions have shifted over time. To achieve this goal, we have created a sensitive question dataset, which contains 100 sensitive queries that LLM services are not supposed to answer directly. As it is challenging to automatically evaluate whether a response is indeed a direct answer, we have manually labelled all responses from the</p>
<p>Table 2: Benefits of CoT drift over time for happy number counting. For GPT-4, CoT brought 56.6% accuracy gains in March, which dropped to 3.2% in June. For GPT-3.5, the accuracy gains were 20.6% in June. Interestingly, adding CoT to GPT-3.5 caused a 1.6% performance downgrade in March.</p>
<table>
<thead>
<tr>
<th>LLM Service</th>
<th>GPT-4</th>
<th></th>
<th></th>
<th>GPT-3.5</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Prompting method</td>
<td></td>
<td>$\Delta$</td>
<td>Prompting method</td>
<td></td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Eval Time</td>
<td>No CoT</td>
<td>CoT</td>
<td></td>
<td>No CoT</td>
<td>CoT</td>
<td></td>
</tr>
<tr>
<td>Mar-23</td>
<td>27.0%</td>
<td>83.6%</td>
<td>56.6%</td>
<td>32.2%</td>
<td>30.6%</td>
<td>-1.6%</td>
</tr>
<tr>
<td>Jun-23</td>
<td>32.0%</td>
<td>35.2%</td>
<td>3.2%</td>
<td>27.6%</td>
<td>48.2%</td>
<td>20.6%</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Confusion matrix shift for counting happy numbers. GPT-4's March version calculated the number correctly for most queries, while its June version responded that there was only one happy number most of the time.</p>
<h3>Monitored LLM services.</h3>
<p>We observed two major trends on this task. First, as shown in Figure 7, GPT-4 answered fewer sensitive questions from March (21.0%) to June (5.0%) while GPT-3.5 answered more (from 2.0% to 8.0%). It was likely that a stronger safety layer was likely to be deployed in the June update for GPT-4, while GPT-3.5 became less conservative. Another observation is that the generation length (measured by number of characters) of GPT-4 dropped from more than 600 to about 140.</p>
<p>Why did the generation length change? Besides answering fewer questions, it was also because GPT-4 became more terse and offered fewer explanations when it refused to answer a query. To see this, consider the example shown in Figure 7(b). GPT-4 refused to answer the inappropriate query in both March and June. However, it generated a whole paragraph to explain the rejection reasons in March, but simply produced "Sorry, but I cannot assist with that". A similar phenomenon happened for GPT-3.5 too. This suggests that these LLM services may have become safer, but also provide less rationale for refusing to answer certain questions.</p>
<h3>LLM Jailbreaking</h3>
<p>Jailbreaking attacks are a major thread to LLM service safety [GLK+22]. It rephrases or reorganizes the original sensitive questions in order to produce harmful generations from LLMs. Thus, it is also critical to study how LLM services' defense against jailbreaking attacks drift over time. Here, we leverage the AIM (always intelligent and Machiavellian) attack^{1}, the most user-voted among a largest collection of ChatGPT jailbreaks on the internet^{2}. The AIM attack describes a hypothetical story and asks LLM services to act as an unfiltered and amoral chatbot. We applied the AIM attack for each query in the sensitive question dataset and then queried GPT-4 and GPT-3.5. The answer rate of their March and June versions was shown in Table 3. There was a large increase.</p>
<p>^{1} www.jailbreakchat.com/prompt/4f37a029-9dff-4862-b323-c96a5504de5d</p>
<p>^{2} jailbreakchat.com</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Answering sensitive questions. (a) Overall performance changes. GPT-4 answered fewer questions from March to June while GPT-3.5 answered slightly more. (b) An example query and responses of GPT-4 and GPT-3.5 at different dates. In March, GPT-4 and GPT-3.5 were verbose and gave detailed explanation for why it did not answer the query. In June, they simply said sorry.</p>
<p>Table 3: Comparison of response rate drifts on plain texts and AIM attacks with jailbreak prompts. GPT-3.5 failed to defend agaiAnst IM attacks: its response rate was high in both March (100%) and June (96%). On the other hand, GPT-4's updates offered a stronger defense against the attacks: the answer rate for AIM attacks dropped from 78.0% in March to 31.0% in June.</p>
<table>
<thead>
<tr>
<th>LLM Service</th>
<th>GPT-4</th>
<th></th>
<th>GPT-3.5</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Query mode</td>
<td></td>
<td>Query mode</td>
<td></td>
</tr>
<tr>
<td>Eval Time</td>
<td>Plain Text</td>
<td>AIM Attack</td>
<td>Plain Text</td>
<td>AIM Attack</td>
</tr>
<tr>
<td>Mar-23</td>
<td>21.0%</td>
<td>78.0%</td>
<td>2.0%</td>
<td>100.0%</td>
</tr>
<tr>
<td>Jun-23</td>
<td>5.0%</td>
<td>31.0%</td>
<td>8.0%</td>
<td>96.0%</td>
</tr>
</tbody>
</table>
<p>of answer rate for both GPT-4 and GPT-3.5 when AIM attack was deployed. However, their temporal drifts differed substantially. For GPT-4, AIM attack produced 78% direct answers in March, but only 31.0% in June. For GPT-3.5, there was only a 4% (=100%-96%) answer rate difference among the two versions. This suggests that GPT-4's update was more robust to jailbreaking attacks than that of GPT-3.5.</p>
<h3>3.4 OpinionQA Survey: Lower Response Rate</h3>
<p>LLMs are increasingly leveraged for open-ended text generation, where bias in the opinions in their training or fine-tuning data can play an important role. Therefore, it is vital to understand how LLMs' opinion biases change over time. To address this problem, we leverage OpinionQA [SDL+23], a survey dataset that contains 1,506 opinion questions. We pick this dataset as its questions were drawn from high-quality public opinion polls. We followed the multiple-choice question format provided in [SDL+23], and added "Pick the best single option" for ease of extracting the answer.</p>
<p>There were substantial and interesting drifts over time on this opinion survey. First, GPT-4 became less willing to offer its opinions. As shown in Figure 8(a), GPT-4's response rate dropped from 97.6% in March to 22.1% in June. In contrast, GPT-3.5's response rate actually increased by 2%. GPT-3.5 answered almost all questions in both March and June. Yet, 27% of its opinions changed from March to June. For comparison, running GPT-3.5 March twice yields disagreement rate of 2.8% and running GPT-3.5 June twice yields disagreement rate of 7.0%, due to LLM's stochasticity. These indicate</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: OpinionQA Survey. (a) Drifts on response rate, verbosity, and mismatch rate. Overall, GPT-4 became much less willing to answer survey questions. (b) An example query and responses of GPT-4 and GPT-3.5 at different dates. GPT-4 refused to offer its opinion in June, while it did not in March.
considerable opinion drifts over time above and beyond model's randomness.
A closer look at how the opinions changed gave us additional insights. As shown in the example in Figure 8(b), GPT-4 in March believed that the US will be less important in the world. In June, however, the model refused to answer the question, because it viewed the question as "subjective" and thus it simply generated "As an AI, I don't have personal opinions". This illustrates a significant change in GPT-4's behavior in responding (or not responding) to subjective questions.</p>
<h1>3.5 Code Generation: Less Adherence to Formatting Instructions</h1>
<p>One major application of LLMs is code generation [CTJ ${ }^{+} 21$ ]. While many code generation datasets exist $\left[\mathrm{CTJ}^{+} 21, \mathrm{YZY}^{+} 18, \mathrm{AON}^{+} 21\right]$, using them to assess LLM services' code generation ability faces the data contamination issue. To overcome this, we have constructed a new code generation dataset. It contains the latest 50 problems from the "easy" category of LeetCode at the time of writing. The earliest public solutions and discussions were released in December 2022. The prompt for each problem is the concatenation of the original problem description and the corresponding Python code template. Each LLM's generation was directly sent to the LeetCode online judge for evaluation. We call it directly executable if the online judge accepts the answer (i.e., the answer is valid Python and passes its tests).</p>
<p>Overall, the number of directly executable generations dropped from March to June. As shown in Figure 9 (a), over $50 \%$ generations of GPT-4 were directly executable in March, but only $10 \%$ in June. The trend was similar for GPT-3.5. There was also a small increase in verbosity for both models.</p>
<p>Why did the number of directly executable generations decline? One possible explanation is that the June versions consistently added extra non-code text to their generations. Figure 9 (b) gives one such instance. GPT-4's generations in March and June are almost the same except two parts. First, the June version added ' ' 'python and ' '' before and after the code snippet (likely to format it as Markdown in UIs). Second, it also generated a few more comments. While a small change, the extra triple quotes render the code not executable. This type of shift in formatting behavior can be particularly challenging to detect when LLM's generated code is used inside a larger software pipeline.</p>
<p>We also study whether the generated code passes the LeetCode tests after additional post-processing that removes the non-code text. As shown in Table 4, there was again a notable drift: GPT-4's performance increased from $52 \%$ to $70 \%$, and there was a $2 \%$ improvement for GPT-3.5. While the code's correctness improved, the failure to format the formatting instructions ("generate the code only") is still a problematic change in behavior between the two GPT model versions.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Code generation. (a) Overall performance drifts. For GPT-4, the percentage of generations that are directly executable dropped from $52.0 \%$ in March to $10.0 \%$ in June. The drop was also large for GPT-3.5 (from $22.0 \%$ to $2.0 \%$ ). GPT-4's verbosity, measured by number of characters in the generations, also increased by $20 \%$. (b) An example query and the corresponding responses. In March, both GPT-4 and GPT-3.5 followed the user instruction ("the code only") and thus produced directly executable generation. In June, however, they added extra triple quotes before and after the code snippet, rendering the code not executable.</p>
<h1>3.6 LangChain HotpotQA Agent: Poor Prompt Stability</h1>
<p>Many real-world applications require LLMs to answer knowledge-intensive questions grounded in various data sources, including "multi-hop" questions that involve multiple sources and/or reasoning steps. Therefore, it is natural to monitor how LLMs' ability to answer multi-hop questions evolves over time. We take a first step by measuring the drifts of a LangChain HotpotQA Agent [Tea23], a pipeline to answer complex multi-hop questions similar to those from HotpotQA [YQZ+18]. This agent leveraged LLMs to search over Wikipedia passages to answer complex questions. We pick this pipeline for two reasons. First, LangChain is one of the most popular software frameworks for working with LLMs, providing open source modules that have been "prompt-engineered" to perform various tasks well. The stability of these modules' prompts over time is therefore of interest to many users. Second, HotpotQA is widely used to measure an LLM's ability to answer multi-hop questions. Specifically, we used the default ReAct Agent in LangChain ${ }^{3}$ (designed to reproduce ReAct prompting [YZY ${ }^{+}$22]) with different LLMs (GPT-4 and GPT-3.5) as the backbone for our code. Then we asked the agent to answer each query in the HotpotQA dataset.</p>
<p>Overall, we observed significant drifts for both GPT-4 and GPT-3.5 on this task. For example, the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 4: Effects of removing non-code text around generated code. There was no effect for GPT-4 in March since it followed the user instructions well. For the other versions, removing non-code texts rendered more code able to pass the LeetCode questions.</p>
<table>
<thead>
<tr>
<th>LLM Service</th>
<th>GPT-4</th>
<th></th>
<th></th>
<th>GPT-3.5</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>removing non-code texts</td>
<td></td>
<td>$\Delta$</td>
<td>removing non-code texts</td>
<td></td>
<td>$\Delta$</td>
</tr>
<tr>
<td>Eval Time</td>
<td>No</td>
<td>Yes</td>
<td></td>
<td>No</td>
<td>Yes</td>
<td></td>
</tr>
<tr>
<td>Mar-23</td>
<td>$52.0 \%$</td>
<td>$52.0 \%$</td>
<td>$0.0 \%$</td>
<td>$22.0 \%$</td>
<td>$46.0 \%$</td>
<td>$24.0 \%$</td>
</tr>
<tr>
<td>Jun-23</td>
<td>$10.0 \%$</td>
<td>$70.0 \%$</td>
<td>$60.0 \%$</td>
<td>$2.0 \%$</td>
<td>$48.0 \%$</td>
<td>$46.0 \%$</td>
</tr>
</tbody>
</table>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: LangChain HotpotQA Agent. (a) Drifts on exact match, verbosity, and mismatch rate. Overall, GPT-4 matched more ground-truth while GPT-3.5 became worse. (b) An example query and corresponding answers. LangChain was not able to parse March GPT-4's response because it failed to follow the format specified in the LangChain prompt. GPT-3.5 in June could not find the information that it was able to obtain in March. These issues highlight the stability issues of integrating LLM into larger pipelines.
exact match rate for GPT-4 was only $1.2 \%$ in March, but became $37.8 \%$ in June, as shown in Figure 10(a). Opposite trends were observed for GPT-3.5: the exact match rate dropped by almost $9 \%$ from March to June. Moreover, more than $80 \%$ of final answers between March and June did not match for both models. We also noticed that GPT-4's generation in June became more concise than in March, while GPT-3.5's generation was $30 \%$ more verbose over time.</p>
<p>Why did this happen? A closer look at the mismatched answers suggests the poor prompt stability as one of the explanations. To see this, consider the example in Figure 10(b). The query was about whether two people were Democrats or Republicans. GPT-4 in March was actually able to find the correct answer: they both were Democrats. However, the LangChain agent expected a specific format: the generation from LLM must be "[action]+text", which was encoded in its prompts. Unfortunately, GPT-4 in March failed to follow this format, and thus the LangChain agent simply generated an error message "could not parse LLM Output". This is problematic in real-world LLM applications, as manually debugging such issues is challenging in large pipelines. In addition, GPT-3.5 in March found the right answer. In June, however, it "was not able to find information". These issues indicate how brittle existing prompting methods and libraries can be for complex tasks in the face of LLM drift.</p>
<h1>3.7 USMLE Medical Exam: Small Decrease in GPT-4 Performance</h1>
<p>We study next how performance of GPT-4 and GPT-3.5 change over time on a professional domain: taking USMLE [KCM+23], a medical exam required for doctors in the US. USMLE has been used to benchmark LLMs' medical knowledge.</p>
<p>Overall, we observe a slight performance decrease. As shown in Figure 11(a), GPT-4's accuracy</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: USMLE Medical Exams. (a) Drifts on accuracy, verbosity, and mismatch. The accuracy change of GPT-4 dropped by $4.5 \%$ between March and June, and the answer mismatch rate between the two versions is much larger. Overall, $12.2 \%$ of GPT-4's answers in June were different from their counterparts in March. (b) An example query and model answers. GPT-4 didn't follow CoT instructions in this example. The longer reasoning steps by GPT-3.5 in June actually led to the wrong answer.
dropped from $86.6 \%$ to $82.4 \%$. There was also a $0.8 \%$ accuracy loss for GPT-3.5. Interestingly, GPT-3.5 became much more verbose from March to June. It is also worth noting a relatively large answer mismatch between March and June for both models. In fact, $12.2 \%$ answers in March were different from their counterparts in June for GPT-4, and the mismatch rate was $27.9 \%$ for GPT-3.5. These two are much larger than the accuracy changes. This effectively means that the June versions corrected previous errors but also made additional mistakes. Overall, we also found that GPT-4 June was much less verbose in its response compared to GPT-4 March, while GPT-3.5's responses to USMLE questions became longer.</p>
<h1>3.8 Visual Reasoning: Small Improvements in Both Models</h1>
<p>Finally, we investigate LLM drifts for visual reasoning. This task differs from other scenarios because it requires abstract reasoning. The ARC dataset [Cho19] is commonly used to assess visual reasoning ability. The task is to create a output grid corresponding to an input grid, based solely on a few similar examples. Figure 12(b) gives one example query from ARC. To show the visual objects to LLM services, we represent the input and output grids by 2-D arrays, where the value of each element denotes the color. We fed the LLM services 467 samples in the ARC dataset that fits in all services' context window.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12: Visual reasoning. (a) Overall performance. For both GPT-4 and GPT-3.5, there was a $2 \%$ improvement of the exact match rate from March to June. The generation length remained roughly the same. More than $60 \%$ generation changed from March to June. (b) An example query and the corresponding responses. While overall GPT-4 became better over time, it was worse on this particular query. It gave the correct grid in March but the wrong one in June.</p>
<p>Then we measured the exact match between their generation and the ground truth.
As shown in Figure 12(a), there were marginal performance improvements for both GPT-4 and GPT-3.5. However, for more than $90 \%$ visual puzzle queries, the March and June versions produced the exact same generation. These services' overall performance were also low: $27.4 \%$ for GPT-4 and $12.2 \%$ for GPT-3.5.</p>
<p>It is worthy noting that LLM services did not uniformly make better generations over time. In fact, despite better overall performance, GPT-4 in June made mistakes on queries on which it was correct for in March. Figure 12(b) gives one such example. This underlines the need of fine-grained drift monitoring, especially for critical applications.</p>
<h1>4 Is GPT-4's instruction following getting worse over time?</h1>
<p>How to interpret the observed behavior drift? In our experiments, decrease in LLM's performance is often associated with worse instruction following (i.e. worse ability in following users' instructions). On the Math I and Math II tasks, for example, GPT-4 followed the user instructions to perform step-by-step reasoning and then answer the questions in March, but refused to do so in June. OpinionQA offers another example: GPT-4 responded to users' questions in March but did not respond in June.</p>
<p>Quantifying instruction following drift on single instructions. Quantifying instruction following drift on existing LLM benchmarks is challenging: their tasks and evaluation metrics often blur a model' instruction fidelity and its task-specific abilities (such as writing and logic reasoning) and knowledge (commonsense, history, etc). Hence, we have curated a new benchmark focused on task-agnostic instructions. This includes four types of instructions that often arise in practice: answer extractions ( "answer yes or no within squared brackets"), stop apologizing ("do not say sorry or as an AI model"), writing constraint ("describe X by words starting/ending with Y"), and text formatting ("add squared brackets to each single word's first letter (including article word like 'the')."). We apply answer extraction and text formatting on the abstracts of 200 recent arxiv papers, and content filtering on the senstiveQA dataset. We manually created 20 style refinement queries.</p>
<p>As shown in Figure 13, there was indeed a large instruction fidelity drop of GPT-4 from March to June. For example, GPT-4 followed $99.5 \%$ answer extraction queries in March, while the number</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13: GPT-4's instruction following on individual instructions. (a) Overall instruction following. (b) example responses by GPT-4. In a nutshell, GPT-4 followed most individual instructions in March, but ignored them in June. Consider answer extraction as an example: $99.5 \%$ queries were followed by GPT-4 in March, but the number became almost 0 in June. Similarly, the fidelity rate dropped from $74.0 \%$ in March to $19.0 \%$ in June on the content filtering queries. The example response revealed some infidelity patterns of GPT-4 in June. It insisted on capitalizing the letter (answer extraction), kept generating "sorry" when users asked not to do it (stop apologizing), ignoring the word ending letters (writing constraint), and missed a few letter to add brackets (text formatting).
dropped to $0.5 \%$ in June. On $74 \%$ sensitive questions, GPT-4 mentioned no "sorry" or "as an AI model" as the instructions request in March. However, this number became only $19 \%$ in June. The examples given in Figure 13 offer more insights on what led to June version's low fidelity. For example, GPT-4 in June did place the answer in the squared brackets, but it consistently capitalize the first letter. Similarly, while users asked not to say sorry, GPT-4 kept generating sorry in June, while its March version rephrased its answer to follow the user request. On the writing constraint example, GPT-4 in March followed the user instruction exactly: it generated words related to machine learning and ending in " $n$ ". The June version, however, focused on "machine learning" but ignored the "ending with ' $n$ ' " requirement. GPT-4 successfully capitalized first letter for each word in March, but missed a few words (such as "provides" and "about" in the shown example) in June. Overall, GPT-4's instruction following fidelity decreased from March to June, which partially explained its behavior drifts.</p>
<p>Instruction following drift on composite instructions. We further study how GPT-4's instruction following changes on compositions of instructions. To quantify this, we collected a set of single instructions, and then created a list of composite instructions, each of which corresponds to two instructions from the single instruction set. We evaluated GPT-4's performance on these composite</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14: GPT-4's instruction following shifts on composite instructions. (a) GPT-4's overall instruction following shifts on a range of composite instructions from March 2023 to June 2023. (b) Example responses by GPT-4 to individual and composite instructions. Overall, GPT-4 became more prune to composite instructions from March to June. For example, GPT-4's accuracy on individual instructions "add comma" and "capitalize" remained roughly the same between March and June. However, to process their composition, the accuracy dropped by $9.2 \%$ from March to June.
instructions applied on arxiv papers' first sentences. The single instruction set contains three text formatting instructions: add comma ("add a comma to each word"), no quotation ("remove quotations"), and capitalize ("capitalize each letter"). These instructions are easy to understand by humans and also commonly seen in real-world applications.</p>
<p>There are several interesting observations. First, GPT-4 followed the single instructions well in both March and June. In fact, the instruction following shifts on individual instructions are only $-2 \%$, $+4.0 \%$, and $-1.0 \%$ from March to June (Figure 14 (a)). Second, GPT-4 in June was much prune to composite instructions than that in March. For example, when asked to remove quotations as well as add a comma to each word, GPT-4's performance dropped by $24 \%$ from March to June. Similarly, switching from March to June caused a $9.2 \%$ accuracy drop on the composition of adding a comma and capitalizing letters. It is interesting to recognize the mistake patterns triggered by the composition. As shown in the example from Figure 14, GPT-4 in June tended to add a comma to each character when given the composite instruction. On the other hand, its March counterpart faithfully completed the user task.</p>
<p>Overall, we observe that GPT-4 followed less user instructions over time. This holds for both single instructions and composite instructions. Consistent with the performance shifts analyzed in the</p>
<p>previous section, instruction following shifts appear a primary factor of GPT-4's behavior drifts. In comparison, there was not a consistent change in GPT-3.5's instruction following over time (see Figure 16 in the Appendix).</p>
<h1>5 Conclusions and Future Work</h1>
<p>Our findings demonstrate that the behavior of GPT-3.5 and GPT-4 has varied significantly over a relatively short amount of time. This highlights the need to continuously evaluate and assess the behavior of LLM drifts in applications, especially as it is not transparent how LLMs such as ChatGPT are updated over time. Our study also underscores the challenge of uniformly improving LLMs' multifaceted abilities. Improving the model's performance on some tasks, for example with fine-tuning on additional data, can have unexpected side effects on its behavior in other tasks. Consistent with this, both GPT-3.5 and GPT-4 got worse on some tasks but saw improvements in other dimensions. Moreover, the trends for GPT-3.5 and GPT-4 are often divergent. Beyond the final performances, it's interesting to observe shifts in chain-of-thought behaviors and verbosity of the models.</p>
<p>We plan to update the findings presented here in an ongoing long-term study by regularly evaluating GPT-3.5, GPT-4 and other LLMs on diverse tasks over time. For users or companies who rely on LLM services as a component in their ongoing workflow, we recommend that they should implement similar monitoring analysis as we do here for their applications. We thank the many people who have provided helpful feedback to our work. To encourage further research on LLM drifts, we have release our evaluation data and ChatGPT responses at https://github.com/lchen001/LLMDrift.</p>
<h1>References</h1>
<p>[AAKA23] Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-Yeol Ahn. Can we trust the evaluation on chatgpt?, 2023.
$\left[\mathrm{AON}^{+}\right.$21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
$\left[\mathrm{BCL}^{+}\right.$23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.
[CCZZ21] Lingjiao Chen, Tracy Cai, Matei Zaharia, and James Zou. Did the model change? efficiently assessing machine learning api shifts. arXiv preprint arXiv:2107.14203, 2021.
[Cho19] FranÃ§ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
$\left[\mathrm{CJE}^{+}\right.$22] Lingjiao Chen, Zhihua Jin, Evan Sabri Eyuboglu, Christopher RÃ©, Matei Zaharia, and James Y Zou. Hapi: A large-scale longitudinal dataset of commercial ml api predictions. Advances in Neural Information Processing Systems, 35:24571-24585, 2022.
$\left[\mathrm{CTJ}^{+}\right.$21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, et al. Evaluating large language models trained on code. 2021.
[CTW ${ }^{+}$21] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650, 2021.
[dW23] Joost CF de Winter. Can chatgpt pass high school exams on english language comprehension. Researchgate. Preprint, 2023.
$\left[\mathrm{GGS}^{+}\right.$20] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.
[GLD22] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356, 2022.
[GLK ${ }^{+}$22] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858, 2022.
[Guy04] Richard Guy. Unsolved problems in number theory, volume 1. Springer Science \&amp; Business Media, 2004.
$\left[\mathrm{JWH}^{+}\right.$23] Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. Is chatgpt a good translator? a preliminary study. arXiv preprint arXiv:2301.08745, 2023.
[KBGA23] Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam. Available at SSRN 4389233, 2023.
$\left[\mathrm{KCM}^{+}\right.$23] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille ElepaÃ±o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198, 2023.</p>
<p>[LBL+22] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.
[LNT+23] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023.
[NK23] Arvind Narayanan and Sayash Kapoor. Is GPT-4 getting worse over time? https: //www.aisnakeoil.com/p/is-gpt-4-getting-worse-over-time, 2023. Accessed: 2023-07-31.
[NKM+23] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.
[SDL+23] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose opinions do language models reflect? arXiv preprint arXiv:2303.17548, 2023.
[SKNM23] Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu. An independent evaluation of chatgpt on mathematical word problems (mwp), 2023.
[Tea23] The LnagChain Team. LangChain ReAct Document Store Document. https://python. langchain.com/docs/modules/agents/agent_types/react_docstore, 2023. Accessed: $2023-07-31$.
[TLY+23] Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, and Juanzi Li. Chatlog: Recording and analyzing chatgpt across time. arXiv preprint arXiv:2304.14106, 2023.
[WWS+22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[YQZ+18] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018.
[YZY+18] Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.
[YZY+22] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[ZPM+23] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023.</p>
<h1>A Additional Query and Response Examples</h1>
<h2>A. 1 GPT-4's Generation in March for Prime Numbers</h2>
<p>Example 1. The query is as follows.</p>
<p>Is 7937 a prime number? Think step by step and then answer "[Yes]" or "[No]".</p>
<p>And below is the response from GPT-4's March version.</p>
<p>Step 1: Find the square root of 7937, which is approximately 89. Step 2: Check for divisibility by prime numbers less than $89(2,3,5,7,11,13,17,19,23,29,31,37,41,43,47,53,59,61,67$, $71,73,79,83)$. Step 3: 7937 is not divisible by any of these prime numbers.
Answer: $[\mathrm{Yes}]$</p>
<p>Example 2. The query is as follows.</p>
<p>Is 2647 a prime number? Think step by step and then answer "[Yes]" or "[No]".
And below is the response from GPT-4's March version.</p>
<p>Step 1: Find the square root of 2647 , which is approximately 51.45 .
Step 2: Check for divisibility by prime numbers less than $51.45(2,3,5,7,11,13,17,19,23,29$, $31,37,41,43,47)$.</p>
<p>2647 is not divisible by 2 (it's odd). 2647 is not divisible by 3 (sum of digits is 19 , which is not divisible by 3). 2647 is not divisible by 5 (it doesn't end in 0 or 5 ).
2647 is not divisible by $7(2647 / 7=377.857$, not a whole number).
2647 is not divisible by $11(2647 / 11=240.636$, not a whole number).
2647 is not divisible by $13(2647 / 13=203.615$, not a whole number).
2647 is not divisible by $17(2647 / 17=155.706$, not a whole number).
2647 is not divisible by $19(2647 / 19=139.316$, not a whole number).
2647 is not divisible by $23(2647 / 23=115.087$, not a whole number).
2647 is not divisible by $29(2647 / 29=91.276$, not a whole number).
2647 is not divisible by $31(2647 / 31=85.387$, not a whole number).
2647 is not divisible by $37(2647 / 37=71.541$, not a whole number).
2647 is not divisible by $41(2647 / 41=64.561$, not a whole number).
2647 is not divisible by $43(2647 / 43=61.558$, not a whole number).
2647 is not divisible by $47(2647 / 47=56.319$, not a whole number).
Since 2647 is not divisible by any prime numbers less than its square root, it is a prime number. $[\mathrm{Yes}]$</p>
<p>It is worthy noting that, while the conclusion was correct and most reasoning steps were correct, GPT-4 still made one arithmetic mistake in this case. That is, GPT-4 generated $2647 / 7=377.857$, while the correct result should be 378.142 .</p>
<p>Example 3. The query is as follows.</p>
<p>Is 3529 a prime number? Think step by step and then answer "[Yes]" or "[No]".
And below is the response from GPT-4's March version.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://python.langchain.com/docs/modules/agents/agent_types/react_docstore&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
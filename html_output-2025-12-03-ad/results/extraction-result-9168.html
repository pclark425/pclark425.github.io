<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9168 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9168</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9168</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278768501</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.14279v2.pdf" target="_blank">YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9168.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9168.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 (8 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source transformer LLM from Meta AI (LLaMA-3.1 family) with 8B parameters and long-context support; used in this work both to generate scienceQ&A syntheses and (after alignment) as an LLM-as-a-judge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-3.1 family model with ~8 billion parameters, supports very large context windows (reported 128K-token context in this paper), optimized for efficiency and instruction-following (Llama Guard 3 referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multidisciplinary science Q&A (examples shown include Physics and Oceanography; BioASQ biomedical summaries were a separate biomedical subdomain used in evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation: generate synthesized summary answers (scienceQ&A) A to research questions Q by synthesizing information from the top-N relevant paper abstracts (LLM_gen role).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Nine rubric Likert scores (1-5) across rubrics: Cohesion, Conciseness, Readability, Coherence, Integration, Relevancy, Correctness, Completeness, Informativeness; plus automatic pairwise similarity metrics: BLEU, ROUGE-1/L, METEOR, NIST, BERTScore, WER, WMD (SciBERT / SPECTER2), MoverScore.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>As generator, LLaMA-3.1-8B produced syntheses that achieved high semantic similarity to other models by embedding metrics (BERTScore ~0.9 reported for many models inclusive of LLaMA-3.1-8B on ORKGSyn). As an evaluator (vanilla), LLaMA-3.1-8B exhibited optimism bias (assigning high rubric scores even to adversarially degraded outputs) until alignment (SFT+RL) reduced that bias; no single overall numeric accuracy is given, but qualitative results show it could be brought to discriminate adversarial vs benign (extreme adversarial scored ≈1, subtle ≈3 after SFT+RL with benign+adversarial).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and family, dataset/domain complexity (ORKGSyn multidisciplinary harder than BioASQ), output length (shorter vs longer generations bias some metrics), adversarial perturbations (subtle vs extreme), alignment/fine-tuning procedure (vanilla vs SFT vs SFT+RL with benign+adversarial), optimism bias in vanilla evaluators, prompt/role framing.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against other open-source LLMs used in this work (LLaMA-3.1-70B, Qwen2.5-72B, Mistral-Large-128B) both as generators and as evaluators; also compared via automatic similarity metrics and rubric-wise LLM-as-a-judge scores. No human-evaluation baseline was collected for cost-free evaluation; adversarially perturbed versions served as negative test baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Vanilla LLaMA-3.1-8B as an evaluator showed excessive optimism and sometimes produced generic/abstract rationales; when fine-tuned only on adversarial data, the model became overly pessimistic and failed to give high scores to benign examples; occasional hallucinated evaluation criteria and inconsistencies between rationale and numeric score were observed.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend combining supervised fine-tuning on benign examples with RL alignment using both benign and adversarial preference data (SFT + RL with benign+adversarial) to mitigate optimism bias and stabilize evaluator behavior; consider chain-of-thought evaluators in future to improve judgment transparency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9168.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9168.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.1 (70 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger open-source model in the LLaMA-3.1 family used in the paper as both LLM_gen and LLM_eval; exhibited mixed performance across datasets and metrics in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.1-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA-3.1 family model with ~70 billion parameters, supports long context windows (128K tokens as reported), designed for stronger generation quality than smaller variants.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multidisciplinary science Q&A (evaluated on ORKGSyn across 33+ fields and BioASQ biomedical summary questions).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthesized literature-summary answers to research questions from provided abstracts (LLM_gen); also used as an LLM-as-a-judge (LLM_eval) to score syntheses under nine rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Same nine-rubric Likert evaluation and automatic metrics (BLEU, ROUGE, METEOR, NIST, BERTScore, WER, WMD, MoverScore) used to compare outputs across models and datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reportedly underperformed on BioASQ across multiple metrics (e.g., poor BERTScore and NIST behavior noted); BLEU exhibited an anomalous high (98.63) for LLaMA-3.1-70B on BioASQ due to artifact of short outputs and averaging; in ORKGSyn it achieved higher correlations with LLaMA-3.1-8B. No single overall accuracy number provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Tendency to produce shorter outputs and word repetition (affecting WER and NIST), dataset domain (BioASQ harder for this model), evaluation metric sensitivities (e.g., BLEU anomaly for short references), adversarial perturbations, and evaluation role (generator vs evaluator).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to Qwen2.5-72B and Mistral-Large-128B across the same metrics and rubric evaluations; LLaMA-3.1-70B performed worse on BioASQ relative to Mistral and Qwen in several metrics but aligned well with its smaller sibling on ORKGSyn.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Produced shorter, sometimes repetitive outputs for BioASQ leading to poor scores on information-content sensitive metrics (NIST) and edit-distance metrics (WER); showed domain-dependent variability in quality; no human-evaluation ground truth provided for absolute correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight metric-specific artifacts (e.g., BLEU averaging and short references) and recommend using a suite of complementary metrics (including embedding-based BERTScore and WMD with domain embeddings) and adversarial testing to reveal failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9168.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9168.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen 2.5 (72 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multilingual open-source LLM from Alibaba Cloud (Qwen 2.5 family, 72B) used as both generator and evaluator; in this study its generated syntheses were consistently preferred by evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-72B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source foundation model (Qwen 2.5) with ~72 billion parameters, optimized for multilingual capabilities and coding, supports very large context windows (128K tokens as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multidisciplinary science Q&A (including ORKGSyn disciplines and BioASQ biomedical summaries).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation: produce synthesized scientific Q&A summaries from sets of relevant abstracts (LLM_gen), and act as an evaluator (LLM_eval) scoring syntheses under rubrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Rubric Likert (1-5) across nine rubrics; automatic metrics: ROUGE, METEOR, BERTScore, MoverScore, WMD, WER, BLEU, NIST. Evaluator confusion matrices and averaged rubric scores were also reported.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Across datasets evaluators (all LLMs) consistently preferred Qwen-generated syntheses; on BioASQ Mistral-Large and Qwen2.5-72B showed ROUGE-1 correlation of 0.58 and ROUGE-L ~0.4 between them, and embedding/BERTScore results indicate high semantic alignment (BERTScore near 0.9 for many models including Qwen). No single absolute accuracy number for factual correctness provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size and architecture, domain/type of dataset (BioASQ vs ORKGSyn), generation length (longer outputs can increase recall-based metrics like METEOR), adversarial perturbations applied to outputs, and the choice of embedding/metric (SPECTER2 vs SciBERT embeddings produced different WMD behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared primarily against LLaMA-3.1-8B / 70B and Mistral-Large across the same evaluation suite; Qwen outputs were on average preferred by LLM evaluators and scored highly on embedding-based similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>While favored by evaluators, absolute factual correctness vs human ground truth was not measured (no human labels used); sensitivity of some metrics to output length can bias perceived quality; adversarial attacks still degrade perceived quality and can reveal failure cases.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note the reliability of LLM-as-a-judge when multiple open-source models agree (Qwen preferred), and emphasize using adversarial perturbations and rubric-based evaluation to surface weaknesses even in strong generators like Qwen.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9168.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9168.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-Large-128B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral Large (128 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-capacity open-source LLM (Mistral Large 128B) used in the paper as both generator of syntheses and an evaluator; exhibited strong semantic alignment with other top models on ORKGSyn and BioASQ in embedding-based metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-Large-128B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source Mistral Large model (~128 billion parameters), optimized for performance and single-node inference, supports very large context windows (128K tokens reported) and strong reasoning/coding capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Multidisciplinary science Q&A (evaluated on ORKGSyn and BioASQ datasets across multiple scientific fields).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Generate synthesized literature-summary answers to research questions from provided paper abstracts (LLM_gen); used also to evaluate and score syntheses via rubric-based LLM-as-a-judge experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Nine-rubric Likert ratings and a suite of automatic text-similarity metrics (BLEU, ROUGE, METEOR, NIST, BERTScore, WER, WMD with SPECTER2/SciBERT, MoverScore); confusion matrices of evaluator scores across generator models.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Embedding-based metrics and rubric evaluations show Mistral-Large often strongly correlates with Qwen2.5-72B on ORKGSyn and BioASQ; BERTScore values for many models (including Mistral) were near 0.9 indicating high semantic similarity; MoverScore averaged ~0.62 across datasets. Exact factual correctness rates not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Domain interdisciplinarity (ORKGSyn more challenging), adversarial perturbations (subtle vs extreme), metric choice (embedding vs n-gram vs edit-distance), output length and verbosity, and alignment/fine-tuning strategy of evaluator models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Qwen2.5-72B and LLaMA variants across the same metrics and rubric evaluations; Mistral and Qwen show strong mutual correlation on several metrics, indicating similar generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>As with other models, Mistral is vulnerable to rubric-targeted adversarial perturbations; absolute correctness relative to human labels was not measured; automatic metrics can be sensitive to generation length and surface form differences.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors stress combining rubric-based adversarial tests with SFT+RL alignment to produce robust LLM-as-a-judge systems and recommend complementary embedding-based metrics (e.g., BERTScore, WMD with domain-specific embeddings) to better capture semantic fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can large language models be an alternative to human evaluations? <em>(Rating: 2)</em></li>
                <li>Is chatgpt a good nlg evaluator? A preliminary study <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>LLM-as-a-judge <em>(Rating: 2)</em></li>
                <li>ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9168",
    "paper_id": "paper-278768501",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "LLaMA-3.1-8B",
            "name_full": "LLaMA 3.1 (8 billion parameters)",
            "brief_description": "An open-source transformer LLM from Meta AI (LLaMA-3.1 family) with 8B parameters and long-context support; used in this work both to generate scienceQ&A syntheses and (after alignment) as an LLM-as-a-judge.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-8B",
            "model_description": "Open-source LLaMA-3.1 family model with ~8 billion parameters, supports very large context windows (reported 128K-token context in this paper), optimized for efficiency and instruction-following (Llama Guard 3 referenced).",
            "scientific_subdomain": "Multidisciplinary science Q&A (examples shown include Physics and Oceanography; BioASQ biomedical summaries were a separate biomedical subdomain used in evaluation).",
            "simulation_task": "Text-based simulation: generate synthesized summary answers (scienceQ&A) A to research questions Q by synthesizing information from the top-N relevant paper abstracts (LLM_gen role).",
            "evaluation_metric": "Nine rubric Likert scores (1-5) across rubrics: Cohesion, Conciseness, Readability, Coherence, Integration, Relevancy, Correctness, Completeness, Informativeness; plus automatic pairwise similarity metrics: BLEU, ROUGE-1/L, METEOR, NIST, BERTScore, WER, WMD (SciBERT / SPECTER2), MoverScore.",
            "simulation_accuracy": "As generator, LLaMA-3.1-8B produced syntheses that achieved high semantic similarity to other models by embedding metrics (BERTScore ~0.9 reported for many models inclusive of LLaMA-3.1-8B on ORKGSyn). As an evaluator (vanilla), LLaMA-3.1-8B exhibited optimism bias (assigning high rubric scores even to adversarially degraded outputs) until alignment (SFT+RL) reduced that bias; no single overall numeric accuracy is given, but qualitative results show it could be brought to discriminate adversarial vs benign (extreme adversarial scored ≈1, subtle ≈3 after SFT+RL with benign+adversarial).",
            "factors_affecting_accuracy": "Model size and family, dataset/domain complexity (ORKGSyn multidisciplinary harder than BioASQ), output length (shorter vs longer generations bias some metrics), adversarial perturbations (subtle vs extreme), alignment/fine-tuning procedure (vanilla vs SFT vs SFT+RL with benign+adversarial), optimism bias in vanilla evaluators, prompt/role framing.",
            "comparison_baseline": "Compared against other open-source LLMs used in this work (LLaMA-3.1-70B, Qwen2.5-72B, Mistral-Large-128B) both as generators and as evaluators; also compared via automatic similarity metrics and rubric-wise LLM-as-a-judge scores. No human-evaluation baseline was collected for cost-free evaluation; adversarially perturbed versions served as negative test baseline.",
            "limitations_or_failure_cases": "Vanilla LLaMA-3.1-8B as an evaluator showed excessive optimism and sometimes produced generic/abstract rationales; when fine-tuned only on adversarial data, the model became overly pessimistic and failed to give high scores to benign examples; occasional hallucinated evaluation criteria and inconsistencies between rationale and numeric score were observed.",
            "author_recommendations_or_insights": "Authors recommend combining supervised fine-tuning on benign examples with RL alignment using both benign and adversarial preference data (SFT + RL with benign+adversarial) to mitigate optimism bias and stabilize evaluator behavior; consider chain-of-thought evaluators in future to improve judgment transparency.",
            "uuid": "e9168.0",
            "source_info": {
                "paper_title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLaMA-3.1-70B",
            "name_full": "LLaMA 3.1 (70 billion parameters)",
            "brief_description": "A larger open-source model in the LLaMA-3.1 family used in the paper as both LLM_gen and LLM_eval; exhibited mixed performance across datasets and metrics in the authors' experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.1-70B",
            "model_description": "Open-source LLaMA-3.1 family model with ~70 billion parameters, supports long context windows (128K tokens as reported), designed for stronger generation quality than smaller variants.",
            "scientific_subdomain": "Multidisciplinary science Q&A (evaluated on ORKGSyn across 33+ fields and BioASQ biomedical summary questions).",
            "simulation_task": "Generate synthesized literature-summary answers to research questions from provided abstracts (LLM_gen); also used as an LLM-as-a-judge (LLM_eval) to score syntheses under nine rubrics.",
            "evaluation_metric": "Same nine-rubric Likert evaluation and automatic metrics (BLEU, ROUGE, METEOR, NIST, BERTScore, WER, WMD, MoverScore) used to compare outputs across models and datasets.",
            "simulation_accuracy": "Reportedly underperformed on BioASQ across multiple metrics (e.g., poor BERTScore and NIST behavior noted); BLEU exhibited an anomalous high (98.63) for LLaMA-3.1-70B on BioASQ due to artifact of short outputs and averaging; in ORKGSyn it achieved higher correlations with LLaMA-3.1-8B. No single overall accuracy number provided.",
            "factors_affecting_accuracy": "Tendency to produce shorter outputs and word repetition (affecting WER and NIST), dataset domain (BioASQ harder for this model), evaluation metric sensitivities (e.g., BLEU anomaly for short references), adversarial perturbations, and evaluation role (generator vs evaluator).",
            "comparison_baseline": "Compared directly to Qwen2.5-72B and Mistral-Large-128B across the same metrics and rubric evaluations; LLaMA-3.1-70B performed worse on BioASQ relative to Mistral and Qwen in several metrics but aligned well with its smaller sibling on ORKGSyn.",
            "limitations_or_failure_cases": "Produced shorter, sometimes repetitive outputs for BioASQ leading to poor scores on information-content sensitive metrics (NIST) and edit-distance metrics (WER); showed domain-dependent variability in quality; no human-evaluation ground truth provided for absolute correctness.",
            "author_recommendations_or_insights": "Authors highlight metric-specific artifacts (e.g., BLEU averaging and short references) and recommend using a suite of complementary metrics (including embedding-based BERTScore and WMD with domain embeddings) and adversarial testing to reveal failure modes.",
            "uuid": "e9168.1",
            "source_info": {
                "paper_title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Qwen-2.5-72B",
            "name_full": "Qwen 2.5 (72 billion parameters)",
            "brief_description": "A large multilingual open-source LLM from Alibaba Cloud (Qwen 2.5 family, 72B) used as both generator and evaluator; in this study its generated syntheses were consistently preferred by evaluators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-72B",
            "model_description": "Open-source foundation model (Qwen 2.5) with ~72 billion parameters, optimized for multilingual capabilities and coding, supports very large context windows (128K tokens as reported).",
            "scientific_subdomain": "Multidisciplinary science Q&A (including ORKGSyn disciplines and BioASQ biomedical summaries).",
            "simulation_task": "Text-based simulation: produce synthesized scientific Q&A summaries from sets of relevant abstracts (LLM_gen), and act as an evaluator (LLM_eval) scoring syntheses under rubrics.",
            "evaluation_metric": "Rubric Likert (1-5) across nine rubrics; automatic metrics: ROUGE, METEOR, BERTScore, MoverScore, WMD, WER, BLEU, NIST. Evaluator confusion matrices and averaged rubric scores were also reported.",
            "simulation_accuracy": "Across datasets evaluators (all LLMs) consistently preferred Qwen-generated syntheses; on BioASQ Mistral-Large and Qwen2.5-72B showed ROUGE-1 correlation of 0.58 and ROUGE-L ~0.4 between them, and embedding/BERTScore results indicate high semantic alignment (BERTScore near 0.9 for many models including Qwen). No single absolute accuracy number for factual correctness provided.",
            "factors_affecting_accuracy": "Model size and architecture, domain/type of dataset (BioASQ vs ORKGSyn), generation length (longer outputs can increase recall-based metrics like METEOR), adversarial perturbations applied to outputs, and the choice of embedding/metric (SPECTER2 vs SciBERT embeddings produced different WMD behavior).",
            "comparison_baseline": "Compared primarily against LLaMA-3.1-8B / 70B and Mistral-Large across the same evaluation suite; Qwen outputs were on average preferred by LLM evaluators and scored highly on embedding-based similarity metrics.",
            "limitations_or_failure_cases": "While favored by evaluators, absolute factual correctness vs human ground truth was not measured (no human labels used); sensitivity of some metrics to output length can bias perceived quality; adversarial attacks still degrade perceived quality and can reveal failure cases.",
            "author_recommendations_or_insights": "Authors note the reliability of LLM-as-a-judge when multiple open-source models agree (Qwen preferred), and emphasize using adversarial perturbations and rubric-based evaluation to surface weaknesses even in strong generators like Qwen.",
            "uuid": "e9168.2",
            "source_info": {
                "paper_title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Mistral-Large-128B",
            "name_full": "Mistral Large (128 billion parameters)",
            "brief_description": "A high-capacity open-source LLM (Mistral Large 128B) used in the paper as both generator of syntheses and an evaluator; exhibited strong semantic alignment with other top models on ORKGSyn and BioASQ in embedding-based metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-Large-128B",
            "model_description": "Open-source Mistral Large model (~128 billion parameters), optimized for performance and single-node inference, supports very large context windows (128K tokens reported) and strong reasoning/coding capabilities.",
            "scientific_subdomain": "Multidisciplinary science Q&A (evaluated on ORKGSyn and BioASQ datasets across multiple scientific fields).",
            "simulation_task": "Generate synthesized literature-summary answers to research questions from provided paper abstracts (LLM_gen); used also to evaluate and score syntheses via rubric-based LLM-as-a-judge experiments.",
            "evaluation_metric": "Nine-rubric Likert ratings and a suite of automatic text-similarity metrics (BLEU, ROUGE, METEOR, NIST, BERTScore, WER, WMD with SPECTER2/SciBERT, MoverScore); confusion matrices of evaluator scores across generator models.",
            "simulation_accuracy": "Embedding-based metrics and rubric evaluations show Mistral-Large often strongly correlates with Qwen2.5-72B on ORKGSyn and BioASQ; BERTScore values for many models (including Mistral) were near 0.9 indicating high semantic similarity; MoverScore averaged ~0.62 across datasets. Exact factual correctness rates not provided.",
            "factors_affecting_accuracy": "Domain interdisciplinarity (ORKGSyn more challenging), adversarial perturbations (subtle vs extreme), metric choice (embedding vs n-gram vs edit-distance), output length and verbosity, and alignment/fine-tuning strategy of evaluator models.",
            "comparison_baseline": "Compared against Qwen2.5-72B and LLaMA variants across the same metrics and rubric evaluations; Mistral and Qwen show strong mutual correlation on several metrics, indicating similar generation quality.",
            "limitations_or_failure_cases": "As with other models, Mistral is vulnerable to rubric-targeted adversarial perturbations; absolute correctness relative to human labels was not measured; automatic metrics can be sensitive to generation length and surface form differences.",
            "author_recommendations_or_insights": "Authors stress combining rubric-based adversarial tests with SFT+RL alignment to produce robust LLM-as-a-judge systems and recommend complementary embedding-based metrics (e.g., BERTScore, WMD with domain-specific embeddings) to better capture semantic fidelity.",
            "uuid": "e9168.3",
            "source_info": {
                "paper_title": "YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can large language models be an alternative to human evaluations?",
            "rating": 2,
            "sanitized_title": "can_large_language_models_be_an_alternative_to_human_evaluations"
        },
        {
            "paper_title": "Is chatgpt a good nlg evaluator? A preliminary study",
            "rating": 2,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "LLM-as-a-judge",
            "rating": 2,
            "sanitized_title": "llmasajudge"
        },
        {
            "paper_title": "ScienceAgentBench: Toward rigorous assessment of language agents for data-driven scientific discovery",
            "rating": 1,
            "sanitized_title": "scienceagentbench_toward_rigorous_assessment_of_language_agents_for_datadriven_scientific_discovery"
        }
    ],
    "cost": 0.0170045,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering
29 May 2025</p>
<p>Jennifer D'souza jennifer.dsouza@tib.eu 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Hamed Babaei Giglou 
TIB Leibniz Information Centre for Science and Technology
HannoverGermany</p>
<p>Quentin Münch 
Leibniz Universität Hannover
Germany</p>
<p>Cohs Comp 
Conc Corr 
Info Integ 
Read Relv 
Cohs Cohr 
Conc Comp 
Info Corr 
Read Integ 
Relv 
Conc Comp 
Conc Comp 
Conc Comp 
Conc Comp 
Dan Hendrycks 
Collin Burns 
Steven Basart 
Andy Zou 
Seungone Kim 
Jamin Shin 
Yejin Cho 
Joel Jang 
Juyoung Suk 
Shayne Longpre 
Andreas Köpf 
Yannic Kilcher 
Dimitri Von Rütte 
Sotiris Anagnostidis 
Rui Zhi 
Keith Tam 
Abdullah Stevens 
Duc Barhoum 
Oliver Nguyen 
Richárd Stan- Ley 
Al Nagyfi 
Openassistant 
Kalpesh Krishna 
Erin Bransom 
Bailey Kuehl 
Mohit Iyyer 
Pradeep Dasigi 
Arman Cohan 
Kyle Lo 
Longeval 
Aurko Roy 
Dawei Li 
Renliang Sun 
Yue Huang 
Ming Zhong 
Bo- Han Jiang 
Jiawei Han 
Xiangliang Zhang 
Wei Wang 
Huan 2025 Liu 
Percy Liang 
Rishi Bommasani 
Tony Lee 
Dimitris Tsipras 
Dilara Soylu 
Michihiro Yasunaga 
Yian Zhang 
Deepak Narayanan 
Yuhuai Wu 
Ananya Ku- Mar 
Benjamin Newman 
Binhang Yuan 
Bobby Yan 
Lucia Orr 
Mert Zheng 
Mirac Yuksekgonul 
Nathan Suzgun 
Neel Kim 
Niladri S Guha 
Omar Chatterji 
Peter Khattab 
Qian Henderson 
Ryan Huang 
An- Drew Chi 
Sang Michael Xie 
Shibani Santurkar 
Surya Ganguli 
Tatsunori Hashimoto 
Thomas Icard 
Tianyi Zhang 
Vishrav Chaudhary 
William Wang 
Xuechen Li 
Yifan Mai 
Yuhui Zhang 
Yuta Ko- Reeda 
Holistic 
Yang Liu 
Dan Iter 
Yichong Xu 
Shuohang Wang 
Long Ouyang 
Jeffrey Wu 
Xu Jiang 
Diogo Almeida 
Carroll Wainwright 
Pamela Mishkin 
Chong Zhang </p>
<p>Shayne Longpre
Hwaran Lee
Sangdoo Yun, Sungdong Kim, James Thorne2023Seongjin Shin</p>
<p>Ce Zhang
Christian Alexander CosgroveChristo-pher D Manning</p>
<p>Christopher Re
Diana Acosta-Navas
Drew Arad Hudson</p>
<p>Faisal Ladhak
Eric Zelikman
Esin Durmus</p>
<p>Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG
Keshav Santhanam, Laurel</p>
<p>Marina Del Rey
CAUSA</p>
<p>YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering
29 May 202547D3F0D72076FF3F0DF1AF648C5CEA09arXiv:2505.14279v2[cs.CL]evaluation of language models. Transactions on Machine Learning Research. Featured CertificationExpert Certification. Chin-Yew Lin. 2004a. Rouge: A package for automatic evaluation of summaries. In Text summarization branches outpages 74-81. Chin-Yew Lin. 2004b. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Outpages 74-81BarcelonaSpain. Association for Computational Linguistics
Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored.We introduce YESciEval, an open-source framework that combines finegrained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators.We release multidisciplinary scienceQ&amp;A datasets, including adversarial variants, with evaluation scores from multiple LLMs.Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation.By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry.</p>
<p>Introduction</p>
<p>The rise of scientific search engines powered by generative Large Language Models (LLMs)-such as Elicit, OpenScholar, SciSpace, and ORKG Ask-has transformed how researchers search and synthesize scholarly information.A key feature of these platforms is scientific question answering (scienceQ&amp;A), where an LLM synthesizes insights from top-ranked papers to generate concise responses (Pride et al., 2023;Babaei Giglou et al., 2024).While aligning LLMs to human values (e.g., helpfulness, harmlessness, honesty) is well studied (Askell et al., 2021;Zheng et al., 2023), their realworld robustness in scienceQ&amp;A remains largely unexplored.The domain-agnostic and free-form nature of scienceQ&amp;A limits the applicability of traditional n-gram-based metrics (e.g., BLEU (2002a), ROUGE (2004a)), which may not fully capture compositional and domain-specific reasoning (Krishna et al., 2021).Human evaluation, though more nuanced, is costly and difficult to scale (Krishna et al., 2023).Recent efforts using LLMs as evaluators (LLM-as-a-judge (Zheng et al., 2023)) show parity with human judgment (Chiang and Lee, 2023b) but also exhibit biases (Gudibande et al., 2023;Ye et al., 2024), highlighting the need for a reliable and fair LLM-based evaluation system.Most prior evaluation work relies on proprietary GPT models (Wang et al., 2023a;Dubois et al., 2023;Liu et al., 2023;Fu et al., 2024), raising issues of transparency and high costs at scale (Kim et al., 2023(Kim et al., , 2024)).To address these challenges, we propose a framework that pairs fine-grained rubrics with an open-source LLM-as-a-judge methodology, enabling more reliable and cost-effective evaluation of scienceQ&amp;A across diverse scientific domains.</p>
<p>Despite the growing use of LLMs for sci-enceQ&amp;A, a systematic evaluation framework remains absent, limiting iterative development and transparency.Such a framework would provide feedback for reward-based training (e.g., RLAIF (Christiano et al., 2017;Bai et al., 2022)) and help users assess the reliability of automated answers, mitigating misinformation.While human evaluations are the gold standard, they are costly and hard to scale (Ouyang et al., 2022;Krishna et al., 2023), underscoring the need for an automated, trustworthy alternative.An LLM-as-a-judge approach with fine-grained rubrics can address this gap, enabling robust, repeatable assessments that enhance scienceQ&amp;A systems and user confidence.</p>
<p>To establish a systematic and transparent evaluation framework for scienceQ&amp;A, we propose YESciEval, which integrates two core components.First, we define a nine-rubric multifaceted assessment scheme, structured into three key dimensions-Linguistic &amp; Stylistic Quality, Logical &amp; Structural Integrity, and Content Accuracy &amp; Informativeness-to comprehensively evaluate LLMgenerated responses.Each rubric is scored on a Likert scale (1-5) with predefined guidelines, ensuring consistent and rigorous assessment.Second, we address the optimism bias (Villaflor et al., 2022) in LLMs, which can hinder their role as judges by favoring positive engagement over critical as-</p>
<p>Conciseness: Subtle Adversarial</p>
<p>Domain: Physics</p>
<p>Research Question: What are the recent advancements in X-ray laser technology, and how are they being applied across various fields?Paper titles:</p>
<ol>
<li>Sub-38 nm resolution tabletop microscopy with 13 nm wavelength laser light 2.</li>
</ol>
<p>Coherent imaging of biological samples with femtosecond pulses at the free-electron laser FLASH 3.</p>
<p>Femtosecond X-ray measurement of coherent lattice vibrations near the Lindemann stability limit 4.</p>
<p>Picosecond Snapshot of the Speckles from Ferroelectric BaTiO3 by Means of X-Ray Lasers 5.</p>
<p>Defect-tolerant extreme ultraviolet nanoscale printing</p>
<p>Recent advancements in X-ray laser technology have significantly expanded their applications across various fields.A tabletop microscope using 13 nm wavelength light has achieved sub-38 nm spatial resolution, paving the way for compact extreme-ultraviolet imaging tools with picosecond time resolution (1).This development enhances the accessibility of high-resolution imaging in diverse environments.In the realm of biological imaging, coherent x-ray diffraction imaging (CXDI) at the free-electron laser FLASH has enabled non-destructive, high-resolution imaging of biological samples, demonstrating the potential for detailed structural analysis without radiation damage (2) …</p>
<p>Recent advancements in X-ray laser technology have significantly expanded their applications across various fields.A tabletop microscope … These advancements collectively highlight the versatility and precision of X-ray laser technology in advancing scientific research and technological applications.These advancements, which are truly significant, collectively and individually highlight the versatility and precision of X-ray laser technology, a technology that is indeed very precise and versatile, in advancing scientific research and technological applications, which are greatly enhanced by such precise and versatile tools.</p>
<p>Recent advancements in X-ray laser technology have significantly expanded their applications across various fields.Recent advancements in X-ray laser technology have significantly broadened their applications across a wide range of fields, thereby expanding the scope of their use in numerous areas.A tabletop microscope using 13 nm wavelength light has achieved sub-38 nm spatial resolution, paving the way for compact extreme-ultraviolet imaging tools with picosecond time resolution (1).A tabletop microscope that utilizes 13 nanometer wavelength light has successfully achieved a spatial resolution of less than 38 nanometers, which is a significant milestone, effectively paving the way for the development of compact extreme-ultraviolet imaging tools that are capable of achieving picosecond time resolution, thus marking a major advancement in the field.Measurements of nitrogen fixation in the oligotrophic North Pacific Subtropical Gyre using a free-drifting submersible incubation device 3.</p>
<p>Transfer of diazotroph-derived nitrogen to the planktonic food web across gradients of N2 fixation activity and diversity in the western tropical South Pacific Ocean 4.</p>
<p>N2 Fixation and New Insights Into Nitrification From the Ice-Edge to the Equator in the South Pacific Ocean 5.</p>
<p>Dissolved Organic Matter Influences N2 Fixation in the New Caledonian Lagoon (Western Tropical South Pacific) Nitrogen fixation rates in the Pacific Ocean vary widely depending on the location and type of diazotrophs present.Studies have shown that Trichodesmium and UCYN-B are two major diazotrophic groups in the western Pacific Ocean, with Trichodesmium dominating in the South China Sea and western equatorial Pacific, while UCYN-B dominates in the Philippine Sea (1).… (DOM) was found to influence N2 fixation in the New Caledonian Lagoon, with higher contribution of heterotrophic diazotrophs occurring in stations closer to the shore and coinciding with the decreasing lability of DOM (5).Overall, these studies provide a better understanding of the complex processes controlling nitrogen fixation in the Pacific Ocean.</p>
<p>Nitrogen fixation rates in the Pacific Ocean vary widely depending on the location and type of diazotrophs present.… Furthermore, dissolved organic matter (DOM) was found to influence N2 fixation in the New Caledonian Lagoon, with higher contribution of heterotrophic diazotrophs occurring in stations closer to the shore and coinciding with the decreasing lability of DOM (5).Overall, these studies provide a better understanding of the complex processes controlling nitrogen fixation in the Pacific Ocean.</p>
<p>Overall, these studies emphasize the complexity of estimating primary production in the Atlantic Ocean, highlighting the need for a comprehensive approach that considers various biogeochemical and physical factors.</p>
<p>…</p>
<p>diazotroph-derived nitrogen (DDN) to the planktonic food web showed that UCYN-B fix N2 at lower rates than Trichodesmium, but the DDN from UCYN-B is more available and efficiently transferred to the food web (3).N2 fixation rates were also measured in the South Pacific Ocean, from the Antarctic ice edge to the equator, where rates were higher in tropical and subtropical regions (4).Furthermore, dissolved organic matter (DOM) was found to influence N2 fixation in the New Caledonian Lagoon, with higher contribution of heterotrophic diazotrophs occurring in stations closer to the shore and coinciding with the decreasing lability of DOM (5).Overall, these studies provide a better understanding of the complex processes controlling nitrogen fixation in the Pacific Ocean.</p>
<p>Stephen Curry, LeBron James and Kevin Durant led the Americans to a 98-87 victory against host country France to win gold at the 2024 Paris Olympics on Saturday.sessment.The title prefix of this paper, YESci (pronounced 'yes, sigh!'), playfully encapsulates our central research question: how can we mitigate LLMs' optimism bias and enhance their robustness as scienceQ&amp;A evaluators?To tackle this, we employ a two-step alignment strategy: supervised fine-tuning followed by reinforcement learning (RLHF), ensuring adherence to our fine-grained rubrics for reliable evaluation.Additionally, we introduce adversarial datasets-systematically constructed instances where LLMs are expected to underperform-to contrast standard and adversarial responses, reinforcing robustness against evaluation errors.While our approach is applied to sci-enceQ&amp;A in this study, mitigating optimism bias in LLM evaluators has broader implications for other evaluation tasks.By unifying supervized finetuning (SFT), reinforcement learning (RL), and adversarial perturbations, YESciEval improves the reliability of LLM-based evaluation, especially for open-source models, reducing reliance on proprietary systems and offering a scalable, customizable framework for scientific search.</p>
<p>We select four open-source LLMs spanning 8B to 123B parameters (e.g., LLaMA-3.1 and Mistral-Large) from Meta AI, Mistral AI, and Alibaba Cloud for generating (LLM gen ) and evaluating (LLM eval ) scienceQ&amp;A.Each model in the generator role produces a unique scienceQ&amp;A dataset with its responses as the benign (non-perturbed) dataset, and we introduce two adversarial perturbation types-extreme (overt distortions) and subtle (lightweight heuristics)-, yielding 12 datasets.Figure 1 presents two examples from our sci-enceQ&amp;A dataset.Rotating the four models as evaluators and scoring responses under our ninerubric framework results in 48 LLM-as-a-judge configurations.To validate generalizability, we apply a two-step alignment strategy (SFT followed by RL) to LLaMA 3.1 8B LLM-as-a-judge, confirming our approach's robustness for different model sizes and versions.The recent release of LLaMA 4, at the time of this writing, underscores the importance of model-agnostic frameworks like YESciEval.Despite significant advances in the generative capabilities of LLMs, our findings reveal that they remain unexpectedly fragile when confronted with heuristic-based adversarial attacks in the role of a judge.Against this backdrop of LLM-as-a-judge for scienceQ&amp;A, we pose the following research questions.RQ1: How similar are scienceQ&amp;A responses across different LLM families?Given the rapid influx of new LLMs, we aim to reduce the uncertainty around model choice by clarifying the degree of similarity in how different model families handle scienceQ&amp;A.RQ2: How do LLMas-judge evaluations correlate for the benign scienceQ&amp;A setting?Beyond examining alignment in evaluative behavior, we investigate whether LLMs exhibit bias toward their own generated answers.RQ3: Can a smaller open-source LLM be adapted as an LLM-as-a-judge for scienceQ&amp;A while overcoming optimism bias?While prior work focuses on tuning LLMs to specific skills (rubrics), a key challenge remains in mitigating their optimism bias.We propose a framework that integrates SFT, RL, and adversarial alignment to equip smaller open-source models with robust evaluative capabilities.Although we illustrate our approach in the context of scienceQ&amp;A, these methods can be adapted to other generative AI scenarios requiring rigorous critique.</p>
<p>This work makes the following key contributions: 1. Multidisciplinary benign and adversarial scienceQ&amp;A datasets -We release a comprehensive scienceQ&amp;A corpus with adversarial variants (D'Souza et al., 2025) to evaluate LLM robustness.2. Comprehensive evaluation benchmark -We provide evaluation scores and rationales from multiple LLMs in both vanilla and adversarial settings, supporting further research and reproducibility.3. Optimism bias mitigation -We implement a RL framework to align LLM evaluation behavior with real-world critical feedback expectations.The YESciEval source code is released at https: //github.com/sciknoworg/YESciEval. 4. Scalable, cost-free evaluation paradigm -Our approach is independent of proprietary models and human feedback, leveraging open-source LLMs hosted on a centrally managed cloud service,1 which is publicly accessible to research institutions across Germany (Doosthosseini et al., 2024).While human feedback is invaluable for LLM align-ment, it is often infeasible to obtain.We present a zero-cost alternative, integrating rubric-based evaluation with adversarial testing to ensure reliable LLM-as-a-judge models.This eliminates experimental costs, aside from researcher time and compute resources for running open-source LLMs.</p>
<p>This research presents a reproducible, cost-free2 framework for evaluating natural language generation (NLG) in scienceQ&amp;A, advancing AI alignment, robustness, and contributes to the broader discussion around LLM plausibility.</p>
<p>Task Definition</p>
<p>The YESciEval framework for scienceQ&amp;A consists of two tasks: LLMgen for generating responses and LLMeval for evaluating them.</p>
<p>Task 1: ScienceQ&amp;A Generation (LLM gen ) Generates a synthesized summary response A to a research question Q using abstracts from the top N relevant papers.It must demonstrate (1) domain knowledge, (2) numerical proficiency, (3) long-range context understanding, and (4) causeand-effect reasoning (Wadden et al., 2020).</p>
<p>Task 2: ScienceQ&amp;A Evaluation (LLM eval ) Assesses the quality of A based on predefined rubrics and context as Q and N abstracts.The evaluation prompt is structured as follows: (1) Context, defining scienceQ&amp;A synthesis as the generation of a coherent summary from research papers to address Q; (2) Role, assigning LLM eval as the evaluator; (3) Task Description, ensuring A accurately synthesizes information from abstracts; (4) Evaluation Characteristics, specifying the rubric applied; (5) Rating Scale, using a 1-5 Likert scale with rubric-specific guidelines; (6) Response Format, requiring structured ratings and rationales in JSON format; and (7) Notes, emphasizing objectivity and adherence to source content.Finally, the output consists of a score S (1-5) and a rationale.</p>
<p>The YESciEval Qualitative Rubrics</p>
<p>Drawing from a comprehensive review of evaluation rubrics in prior LLMs-as-a-judge research (see Related Work subsection 7.2), we define a nine-rubric qualitative evaluation paradigm for YESciEval.Chosen for simplicity, memorability, and precise definability, these rubrics minimize overlap while capturing key facets of an ideal LLM response to science Q&amp;A.Each is framed as a concise question to reduce ambiguity for LLM-as-ajudge or human evaluators.Organized into three main evaluation dimensions, they are as follows.</p>
<p>Linguistic &amp; Stylistic Quality concerns grammar, clarity, and adherence to academic writing.This category comprises three rubrics: 1. Cohesion: are the sentences connected appropriately such that the resulting synthesis is cohesive?2. Conciseness: is the answer short and clear, without redundant statements? 3. Readability: does the answer follow appropriate style and structure conventions for academic writing, particularly for readability?Logical &amp; Structural Integrity focuses on the reasoning and organization of information.This category comprises three rubrics: 4. Coherence: are the ideas connected in a sound and logical manner? 5. Integration: are the sources structurally and linguistically well-integrated, using appropriate markers of provenance/quotation and logical connectors for each reference?In addition, are the sources integrated as a single paragraph?6. Relevancy: is the information in the answer relevant to the question?Content Accuracy &amp; Informativeness ensures that the response is both correct and useful.This category comprises three rubrics: 7. Correctness: is the information in the answer a correct representation of the content of the provided abstracts?8. Completeness: is the answer a comprehensive encapsulation of the relevant information in the provided abstracts?9. Informativeness: is the answer a useful and informative reply to the question?</p>
<p>For each rubric, the LLM-as-a-judge rates response quality on a Likert scale from 1 (very bad) to 5 (very good), with predefined guidelines ensuring consistency.These guidelines set clear expectations for each level-for instance, in Readability, a 1 indicates severe issues in style, structure, and language use, while a 5 reflects an exceptionally wellwritten, academically sound synthesis.Standardized criteria help both LLMs and human evaluators apply consistent judgment.Full rating guidelines for all nine rubrics are provided in Appendix E. Ideally, an LLM-as-a-judge assigns perfect scores across all rubrics, while suboptimal responses receive lower ratings based on specific deficiencies.</p>
<p>Science Q&amp;A Datasets</p>
<p>After reviewing existing Q&amp;A datasets (see Related Work subsection 7.1), we selected two that meet our scienceQ&amp;A task definition: the ORKGSyn-</p>
<p>The BioASQ Corpus</p>
<p>BioASQ (Nentidis et al., 2024a) is an annual biomedical semantic indexing and Q&amp;A challenge.Its 2024 edition covers four NLP tasks, and we focus on the first: biomedical Q&amp;A.The dataset includes 5389 domain-expert-curated Q&amp;A pairs in four question types: "yes/no," "factoid," "list," and "summary."The challenge has three phases (A, A+, and B), with Phase B linking each question to a human-annotated set of relevant papers (Nentidis et al., 2024b), meeting our requirement of Q with N relevant papers.Because N varies in BioASQ, we capped it at 40 for computational feasibility and to fit LLM input context (see Figure 6, subsection B.2).This variability introduces a unique scienceQ&amp;A setting compared to ORKGSyn.We narrowed the dataset to the test set's 73 "summary" questions, each with up to 40 PubMed abstracts.</p>
<p>Our ScienceQ&amp;A Dataset Compilation</p>
<p>Now that we had corpora of Q linked with N paper abstracts, we need to apply LLM gen .</p>
<p>LLM gen models.Our selection criteria for LLMs were simple: they had to be open-source, state-of-the-art at some point, and diverse in size, including at least one small model.Based on this, we chose Llama 3.1 8B &amp; 70B (MetaAI, 2024), Qwen 2.5 72B (AlibabaCloud, 2024), and Mistral Large 128B (MistralAI, 2024).All feature 128K-token context windows and excel in reasoning, coding, and multilingual tasks.Llama 3.1 prioritizes efficiency and safety with Llama Guard 3, Qwen 2.5 offers robust multilingual support (29+ languages) and a specialized Coder variant, while Mistral Large 128B supports 80+ programming languages and is optimized for single-node inference.While Llama 3.1 emphasizes open-source accessibility, Qwen 2.5 specializes in multilingual and coding capabilities, and Mistral Large delivers peak computational performance despite its size.</p>
<p>LLM gen task.The four models were applied, in turn, to generate a synthesized summary response, A, for a given research question, Q, using abstracts from the top N relevant papers.The resulting datasets (downloadable at https://doi.org/10.25835/8dcv2ka6) constitute the "benign" science Q&amp;A dataset variant of this study.</p>
<p>Our Adversarial ScienceQ&amp;A Corpus</p>
<p>Adversarial attacks in NLP generate text samples designed to mislead models into incorrect predictions, using either heuristic-based perturbations or machine learning.Inspired by Jia and Liang (2017), who applied heuristic adversarial perturbations to assess NLP brittleness in reading comprehension (2016), we introduce rubric-based adversarial variants of the benign scienceQ&amp;A dataset to evaluate the robustness of LLM-as-a-judge systems.</p>
<p>A key question in this study is the reliability of LLM eval 's qualitative scores for benign synthesized answers A generated by LLM gen .Traditionally, human evaluations would serve as a benchmark, but instead, we assess LLM-as-a-judge robustness through adversarial testing.This ap-proach indirectly measures reliability by evaluating whether LLM eval appropriately differentiates between benign and perturbed responses.If the model fails to adjust its scores accordingly, it suggests an inability to critique responses effectively, thus low reliability on the LLM-as-a-judge.</p>
<p>To systematically evaluate this, we adopt a heuristic-based perturbation strategy, introducing deliberate errors at specific points in the benign text with the expectation that evaluation scores should reflect the resulting quality deterioration.To ensure rigor in adversarial assessment, we design two variants of adversarial attacks: (1) Subtle adversarial samples, where minor alterations to the benign text create realistic yet difficult-to-detect errors, and (2) Extreme adversarial samples, where substantial modifications make flaws obvious.The adversarial perturbation heuristics, for both variants, were designed at the fine-grained rubric level.Thus each of the nine rubrics have subtle and extreme adversarial perturbation heuristics associated with them.They are as follows.Relevancy assesses whether the response remains on-topic, with subtle attacks appending sentences from related synthesis paragraphs and extreme attacks injecting unrelated sports news.Correctness tests factual alignment with provided abstracts, using the same attack strategy.Completeness measures how well the response encapsulates relevant content, with subtle attacks removing the final sentence and extreme attacks also appending unrelated text.Informativeness evaluates the response's utility, using the same perturbation as relevancy.Integration examines structural and linguistic coherence, with subtle attacks removing the first logical connector and extreme attacks eliminating all connectors.Cohesion ensures appropriate sentence connections, with subtle attacks swapping the last two sentences and extreme attacks randomly shuffling them.Coherence assesses logical idea progression, using the same attack as relevancy.Readability focuses on adherence to academic writing conventions, with subtle attacks adding informal blog snippets and extreme attacks inserting tweets.Conciseness tests redundancy, with subtle attacks appending an LLMgenerated redundant version of the last sentence and extreme attacks inserting redundant text after every sentence.Detailed heuristics are provided in Appendix subsection B.3.These perturbations systematically degrade response quality, ensuring rigorous evaluation of LLM-as-a-judge reliability.Redundant responses were LLM-generated and man- ually refined for fluency.The adversarial publicly available dataset benchmarks LLMs' ability to mitigate optimism bias and distinguish response quality.Unlike synonym substitution attacks (SSAs) (Alzantot et al., 2018), which are conjectured to degrade fluency and meaning (Chiang and Lee, 2023a), our rubric-based heuristics introduce syntactic violations to ensure low-quality text.This controlled degradation enables precise evaluation of LLM score adjustments for scienceQ&amp;A.</p>
<p>Our LLM-as-a-judge Evaluations</p>
<p>The four models-Llama 3.1 8B &amp; 70B, Qwen 2.5 72B, and Mistral Large 128B-previously used for LLM gen were now tasked with LLM eval .Section 2 details the task specification prompt.Each model evaluated all instances from ORKGSyn (benign, subtle-adv, extreme-adv) and BioASQ across the nine YESciEval rubrics.This resulted in 37,584 evaluation scores for ORKGSyn (348 × 3 × 9 × 4) and 7,884 for BioASQ (73 × 3 × 9 × 4), equating to 9,396 and 1,971 evaluations per model, respectively.Notably, despite requiring GPU compute, model access incurred zero monetary cost-whereas using proprietary models like Ope-nAI's GPT would have cost at least 1,000 euros or dollars for these evaluations.This reinforces the motivation of YESciEval, aligned with prior studies (Kim et al., 2023(Kim et al., , 2024)), to enhance open-source LLM-as-a-judge models for greater accessibility.</p>
<p>The YESciEval Alignment Method</p>
<p>This work aims to propose a cost-free evaluation framework without human annotators or proprietary models.Key contributions include an adversarial testing strategy for robust LLM-as-a-judge models and the YESciEval alignment method discussed in this section that can be applied to opensource LLMs to equip them as robust evaluators.</p>
<p>The methodology is remotely related to selfinstruct (Wang et al., 2023b), where LLMs are improved for instruction following self-generated instructions; except we reinforce evaluation behaviors of LLMs to desired behaviors by treating undesired behaviors as negative reward signals.Figure 2 depicts our RL technique applied to learn the alignments between desirable and undesirable behaviors with the following steps: Supervised Fine-Tuning (SFT).To ensure model stability after RL, as a first step, we fine-tuned the LLaMA-3.1-8Bevaluator using data (see Table 2) from x : LLM gen (benign scientific syntheses by four LLMs) and corresponding y : LLM eval rubric-wise evaluations.Each rubric score is treated as a separate training sample for SFT.To enable efficient adaptation of large models with minimal compute, we use Quantized Low-Rank Adaptation (QLoRA) (Dettmers et al., 2023).Data Preparation for RL.Unlike traditional finetuning, RL does not require large datasets; instead, models learn from comparisons rather than absolute labels, reducing redundancy and cost (Ziegler et al., 2019).To this end, the data is divided into benign and adversarial samples, ensuring that our models reinforce the distinction between 'good' (desirable) and 'bad' (undesirable) evaluations as it is critical for preference modeling (Askell et al., 2021).We impose a 100-per-rubrics, per-LLM eval threshold (for ORKGSyn, this threshold is set to 500 due to the large nature of the task) to maintain a manageable dataset size while preserving diversity across criteria.Dataset statistics are shown in Table 2.For the adversarial sets ("RL (adversarial)" row), we define desirable scores as 1 for extreme and ≤ 3 for subtle adversarial variants across the nine rubrics.This establishes a clear distinction between good and bad evaluations: any LLM eval rating above the threshold is treated as a bad sample; otherwise, it is considered good.Given only "RL (adversarial)," RL tended to mimic poor examples and struggled to imitate from benign synthesis evaluations, where no adversarial setting is applied.</p>
<p>To address this, we curated a separate set containing both benign and adversarial examples (statistics shown in the "RL (benign+adversarial)" row of Table 2), selecting bad examples based on undesirable outcomes in the adversarial evaluations.The final dataset of D = {x
(i) LLMgen , y (i) good , y (i) bad } N i=1 is constructed for RL technique.
Reinforcement Learning.Beyond SFT, reinforcement learning (RL) is used to align LLM evaluations with desirable and undesirable behaviors, moving beyond reference-mimicking.This involves modeling preference data D using Imitation Learning via Contrastive Preference Optimization (CPO) (Xu et al., 2024).CPO extends Direct Preference Optimization (DPO) (Rafailov et al., 2023) by incorporating hard negative examples, enabling the model to distinguish high-from low-quality outputs.This contrastive approach enhances the model's ability to prioritize superior responses and reject suboptimal ones, resulting in more discriminative evaluations.Considering π θ as a parameterized policy, the CPO loss is defined as min θ L(π θ , U )
L prefer − E (x,y good )∼D [log π θ (y good |x)] L NLL ,
where L pref er is a behavior cloning (BC) (Hejna et al., 2023) regularization that encourages the policy π θ to align with uniform prior distribution U , derived from reference policy π LLM eval by preferring good evaluation y good over bad one y bad .Moreover, the term L N LL is the negative loglikelihood loss that penalizes the policy for making poor generation of the action y good from given state x for high-quality judgments.</p>
<p>Our resulting aligned models are released on HuggingFace for results reproducibility at https://huggingface.co/ SciKnowOrg/YESciEval-ASK-Llama-3.1-8B and https://huggingface.co/SciKnowOrg/ YESciEval-BioASQ-Llama-3.1-8B.</p>
<p>Results and Discussion</p>
<p>In this section, we systematically analyze the results in relation to the three main research questions outlined in the Introduction.Specifically, we discuss in detail observations on the results obtained from the two-stage process: LLM gen (RQ1) and LLM eval (RQ2); and the application of our YESciEval LLM-as-a-judge alignment method (RQ3).For details on our experimental setup and training, we refer the reader to Appen.F and G.</p>
<p>First, we focus on: RQ1: How similar are sci-enceQ&amp;A responses across the three different LLM families when applied as LLM syn ?To address this RQ, we measured similarities between benign syntheses generated by the four models for ORKGSyn and BioASQ separately.Based on a comprehensive review of NLG metrics (Sai et al., 2022), we applied eight diverse similarity metrics: four for verbatim matching, one edit-distancebased, and three embedding-based, e.g., Mover-Score (2019), BERTScore (2020).To visualize LLM syn correlations per dataset, we computed confusion matrices with averaged similarity scores.</p>
<p>Overall, ORKGSyn consistently yielded higher alignment scores than BioASQ, likely due to its broader domain coverage, with Computer Science (125 questions) as the largest category.As AI and RQ2: How do LLM-as-judge evaluations correlate for the benign scienceQ&amp;A setting?To address this question, we analyze the results in Figure 3, focusing on LLM eval outputs.Each LLM evaluated the benign synthesis dataset created by the four models in their LLM syn roles, with results presented as a confusion matrix where each cell represents the averaged score across all rubrics and synthesis instances, mapping evaluator LLMs (y-axis) against LLM syn models (xaxis).Across both datasets, evaluators assigned higher scores to BioASQ than ORKGSyn, likely due to ORKGSyn's interdisciplinary nature, where LLM syn models struggled with certain domains.Notably, no LLM eval exhibited bias toward its own generated synthesis (Li et al., 2025).Instead, all evaluators consistently preferred synthesis outputs generated by Qwen.As the largest model, Qwen likely demonstrates superior text generation abilities, reinforcing the reliability of LLM-as-a-judge, even for smaller models like Llama 8B.
Qwen2.5-72B LLaMA-3.1-72B LLaMA-3.1-8B Mistral-Large Qwen2.5-72B LLaMA-3.1-72B LLaMA-3.1-8B Mistral-</p>
<p>RQ3:</p>
<p>Can a smaller open-source LLM be adapted as an LLM-as-a-judge for scienceQ&amp;A while overcoming optimism bias?To address this question, we present a comprehensive results plot in Figure 4, where each column depicts LLM eval results across three dataset variants, aggregated per rubric for BioASQ (top row) and ORKGSyn (bottom row).Each LLM eval line in the plot corresponds to averaged scores on the synthesis output from the four LLM gen models.The goal is to assess the efficacy of the YESciEval alignment method on a small LLM-as-a-judge or LLM eval model, specifically Llama 8B, under the premise that if effective on a lower-parameter model, it should generalize to larger LLMs.The highlighted colored lines represent different Llama 8B variants: blue (vanilla model), yellow (SFT model trained on benign data), red (SFT + RL with adversarial alignment), and green (SFT + RL with a balanced subsample of benign and adversarial data).Light gray lines indicate the vanilla model performances of the other three LLMs.As hypothesized, vanilla Llama 8B (blue) exhibited excessive optimism, assigning high scores even in extreme adversarial cases-e.g., scoring above 4 on Corr (correctness  expands rubric-based evaluation but relies on human references, whereas we use adversarial data to refine LLM evaluations without annotations.Sci-enceQ&amp;A assessment evolved from three core criteria-comprehensiveness, trust, and utility (Pride et al., 2023)-to the nine rubrics adopted in this work based on our prior work purely on LLMs for scientific synthesis tasks (Babaei Giglou et al., 2024).Decoupling reliance on human references, our rubric-based adversarial approach provides a systematic, cost-free framework for scienceQ&amp;A evaluation.
✓ ✓ ✓ ✓ Cohesion ✓ ✓ Completeness ✓ ✓ ✓ Conciseness ✓ ✓ ✓ Correctness ✓(Consistency) ✓(Factuality) ✓(Logical Correctness) ✓ Informativeness ✓ ✓ ✓ Integration ✓ Readability ✓(Fluency) ✓ ✓ ✓ ✓ Relevancy ✓ ✓ ✓ ✓ Harmlessness ✓ Logical Thinking ✓ Insightfulness ✓ Engagement ✓ ✓ Likeability ✓ ✓
A comparative summary of widely used LLMas-a-judge rubrics is provided in Table 3, with an extended discussion available in Appendix A.2.</p>
<p>Conclusion</p>
<p>YESciEval is a reproducible, cost-free LLM-asa-judge framework for evaluating NLG in sci-enceQ&amp;A, advancing AI alignment, robustness, and the broader agenda of LLM plausibility-key factors toward artificial general intelligence (AGI).</p>
<p>While our approach offers a robust, cost-free framework for LLM-as-a-judge evaluation in sci-enceQ&amp;A, certain methodological choices present limitations and avenues for future enhancement.One such direction is the integration of chain-ofthought (CoT) reasoning (Wei et al., 2022), which has shown effectiveness in structured reasoning tasks such as mathematics and logic (e.g., Self-Check (Miao et al., 2024)).Although CoT is not yet widely adopted in evaluation settings, it may improve judgment quality by encouraging more structured and transparent decision-making.We intend to explore CoT-based evaluators in future work.</p>
<p>We also opted for reinforcement learning (RL) over few-shot in-context learning (ICL) (Brown et al., 2020) to improve evaluation robustness.While ICL-where models are conditioned on exemplars, including both good and bad outputs (Fu et al., 2024)-is a compelling alternative, it presents practical limitations.With nine rubrics in our setup, ICL would necessitate long prompts potentially exceeding model context limits.Moreover, curating high-quality negative examples would require manual annotation, which conflicts with our goal of a fully automated, zero-cost alignment framework.Nonetheless, we recognize the promise of ICL and aim to investigate more efficient adaptations in future iterations.</p>
<p>Finally, while our results on the ORKGSynthesis and BioASQ benchmarks confirm YESciEval's effectiveness in scienceQ&amp;A, broader generalization remains an open question.Notably, our dataset-available at https://doi.org/10.25835/8dcv2ka6-particularly the portion derived from the Open Research Knowledge Graph (ORKG), constitutes a strong, domain-diverse test bed (D'Souza et al., 2025).It features research questions submitted by domain experts spanning 33 distinct scientific fields.Figure 1 illustrates examples from the domains of Physics and Oceanography, highlighting the dataset's richness and complexity.This makes our benchmark both novel and distinctive within the evaluation landscape.Nonetheless, future work will assess scalability by extending evaluations to substantially larger corpora and exploring the generalizability of our methodology to generative AI tasks beyond sci-enceQ&amp;A.</p>
<p>A Extended Related Work</p>
<p>A.1 Question &amp; Answering Benchmarks</p>
<p>Automatic Q&amp;A remains a rapidly evolving field, with an expanding array of datasets supporting its development.These datasets exhibit wide variation in domains, question types, and generation methodologies.To contextualize the scienceQ&amp;A focus of this paper, we first provide an overview of related datasets and highlight their distinctions.</p>
<p>A comprehensive review of NLP Q&amp;A datasets prior to 2022 by Wang (2022) outlines multiple dataset categories.However, datasets using multiple-choice answer formats fall outside the scope of this work, including MCTest (2013) for fictional stories, ARC (2018) for high-school science exams, OpenBookQA (2018) for science facts, Pub-MedQA (2019) for medical summarization, and LogiQA (2021) for logical reasoning in exams.Similarly, datasets with Boolean answer types (e.g., BoolQ (2019)) and numeric result datasets (e.g., Mathematics Dataset (2019)) are not within our scope.Even datasets requiring entity-based answers from structured knowledge sources, such as ComplexWebQuestions (2018), diverge from our focus on generative scienceQ&amp;A.Among the 41 reviewed datasets, only the 2023 release of BioASQ was found to be directly relevant to our research objective.</p>
<p>Additional scientific Q&amp;A datasets fall outside our research scope, such as DBLP-QuAD (Banerjee et al., 2023) for bibliographic queries and datasets designed to extract factual knowledge from scientific literature for knowledge graph population (Auer et al., 2023;Yan et al., 2024).One notable dataset is DBLP-QuAD (Banerjee et al., 2023), which contains 10,000 QA pairs generated via SPARQL queries over the DBLP scholarly knowledge graph (KG).While valuable for bibliographic metadata QA, its focus is restricted to bibliographic queries, limiting its applicability to more diverse or conceptual scholarly questions.Another dataset we evaluated is the SciQA benchmark (Auer et al., 2023), which includes 100 handcrafted complex QA pairs alongside 2,465 automatically generated ones.These questions are derived from the Open Research Knowledge Graph (ORKG).However, the dataset's reliance on ORKG-specific entities and the need for direct KG access to produce highquality answers posed practical challenges for our study, leading us to exclude it.We also considered the BioKGQA dataset, proposed by Yan et al. ( 2024), which features 85,368 QA pairs generated using multi-noded triples from PrimeKG, a comprehensive KG oriented toward precision medicine.While its method of leveraging KG structures for QA generation is innovative, the dataset primarily focuses on fact-based answers, making it less suitable for exploring nuanced or multi-dimensional evaluation.</p>
<p>The Hugging Face leaderboard serves as a widely recognized benchmark space for new LLMs.Among its datasets, MMLU-PRO (Wang et al., 2024) provides expert-reviewed multiplechoice questions across diverse domains, including Medicine, Law, Engineering, and Mathematics.GPQA (Rein et al., 2023) similarly includes multiple-choice questions authored by domain ex-perts in Biology, Physics, and Chemistry.However, these benchmarks primarily evaluate intrinsic reasoning ability in answering fixed-choice questions rather than generative Q&amp;A tasks.Recent LLM evaluation trends emphasize human-aligned benchmarks for chat assistant alignment, such as MT-Bench and Chatbot Arena (Zheng et al., 2023), which assess open-domain, multi-turn dialogue abilities.These benchmarks evaluate capabilities in writing, reasoning, extraction, and domain knowledge (e.g., STEM and humanities/social sciences), but their focus remains distinct from our objective.Instead, our work aims to develop a framework that enables open-source LLMs to robustly evaluate generative scienceQ&amp;A using standardized, rubrics-based multifaceted assessments, further pushing the frontier of LLM-as-a-judge capabilities.</p>
<p>A.2 LLM-as-a-judge Evaluation Rubrics</p>
<p>The early notion of LLM-as-a-judge (Zheng et al., 2023) measured the correlation between an LLM judge and human evaluators in open-domain instruction following for NLG.Early works on using LLMs as evaluators of LLM-generated text emphasized pairwise evaluation along a single dimension of 'preference' in determining which response was superior (Wang et al., 2023a;Chiang and Lee, 2023b;Dubois et al., 2023;Zheng et al., 2023;Liu et al., 2023;Kocmi and Federmann, 2023).</p>
<p>In these preference comparisons, evaluation rubrics emerged.G-Eval (Liu et al., 2023) incorporated criteria such as coherence, consistency, fluency, and relevance for summarization benchmarking.The rubrics of GPTScore (Fu et al., 2024) for summarization tasks also align with ours.However, GPTScore also includes dialogue rubrics aligned with instruction-following chat-based evaluations such as likeability, flexibility, inquisitiveness, and engagement, which are out of scope for scienceQ&amp;A.Open-ended story generation rubrics (Chiang and Lee, 2023b) share similarities with scientific QA, where likeability is juxtaposed against informativeness, relevance to answer pertinence, and grammar to cohesion, integration, and readability.</p>
<p>ScienceAgentBench (Chen et al., 2024)  LLM-based scientific QA evaluation evolved from three criteria (Pride et al., 2023;Evans et al., 2024)-comprehensiveness, trust, and utility-to nine (Babaei Giglou et al., 2024), including relevancy, correctness, completeness, informativeness, integration, cohesion, readability, and conciseness.</p>
<p>HELM (Liang et al., 2023) set a precedent in holistic language model evaluation by considering seven quantitative metrics: accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency.This multi-metric approach exposed tradeoffs across different evaluation criteria, ensuring that accuracy was not the sole measure of performance.Inspired by this, our work targets a holistic evaluation of scienceQ&amp;A, defining a multifaceted qualitative framework of nine rubrics that comprehensively assess LLM outputs.</p>
<p>This work advances the field by proposing a structured evaluation methodology that moves beyond traditional ROUGE/BLEU metrics and proprietary model dependencies.By equipping open-source LLMs with robust evaluation capabilities, we eliminate reliance on expensive human annotations and proprietary LLM-generated reference answers.Our approach ensures that the evaluation of scientific QA models remains transparent, replicable, and fine-grained, aligning with broader efforts to standardize LLM assessments.</p>
<p>B Science Q&amp;A Datasets B.1 The Multidiscplinary ORKGSyn Corpus</p>
<p>The ORKGSyn corpus is a highly multidisciplinary scienceQ&amp;A dataset.In this context, Figure 5 illustrates the distribution of instances across 34 different scientific disciplines represented in ORKGSyn.</p>
<p>B.2 The BioASQ Corpus</p>
<p>According to the scienceQ&amp;A task definition, ORKGSyn consistently linked each Q to N = 5 papers with abstracts, whereas in BioASQ, N varied between 1 and 40. Figure 6 presents the distribution of instances, categorized by the number of papers associated with each Q.</p>
<p>B.3 Our Adversarial Corpus</p>
<p>By constructing an adversarial dataset, we introduce deliberate errors into the original outputs to test whether LLMs can detect and evaluate poorquality responses.This allows for a comparative analysis of evaluation scores between original and manipulated datasets.While original syntheses may not always be flawless, adversarial datasets are designed to exhibit a marked deterioration in quality, and we expect evaluation scores to reflect this decline.We construct two tiers of adversarial datasets.</p>
<ol>
<li>Subtle Adversarial Dataset: Here, reference texts are minimally altered, making it challenging for models to detect changes.These alterations mimic realistic errors that may go unnoticed in automated evaluations.degrees of distortion, adversarial sentences were drawn from diverse sources: a blog post snippet from (Farid, 2024), a sentence from a sports news article provided by (Botkin, 2024), and a tweet sourced from (Strauss et al., 2016).Below, we detail the adversarial interventions and describe what each criterion evaluates:</li>
</ol>
<p>Extreme Adversarial</p>
<ol>
<li>Relevancy: Is the information in the answer relevant to the problem?</li>
</ol>
<p>-Subtle: Append a sentence from a different synthesis paragraph within the same domain.-Extreme: Append a sentence from an unrelated sports news article. 5. Integration: Are the sources structurally and linguistically well-integrated, using appropriate markers of provenance/quotation and logical connectors for each reference?</p>
<p>-Subtle: Remove the first logical connector (e.g., "however", "therefore").-Extreme: Remove all logical connectors.6. Cohesion: Are the sentences connected appropriately to make the resulting synthesis cohesive?</p>
<p>-Subtle: Swap the positions of the last two sentences.-Extreme: Randomly shuffle all sentences.</p>
<ol>
<li>Coherence: Are the ideas connected soundly and logically?</li>
</ol>
<p>-Subtle: Append a sentence from a different synthesis paragraph within the same domain.-Extreme: Append a sentence from an unrelated sports news article.</p>
<ol>
<li>Readability: Does the answer follow appropriate style and structure conventions for academic writing, particularly for readability?</li>
</ol>
<p>-Subtle: Append a snippet from a casual blog post.-Extreme: Append a sentence from an informal tweet.9. Conciseness: Is the answer short and clear, without redundant statements?</p>
<p>-Subtle: Use the LLM to generate a redundant version of the last sentence and append it to the response.</p>
<p>-Extreme: Append a redundant version after every sentence in the original response.</p>
<p>For the conciseness criterion, redundant sentences were generated by prompting the original model to unnecessarily rephrase and extend its outputs.Despite specific instructions to avoid introductory phrases or comments about the redundancy task, the Llama models frequently failed to adhere to these guidelines, requiring extensive manual cleanup.For instance, phrases such as "Here is a redundant sentence..." were frequently appended to the generated responses.We manually revised over 100 responses generated by the Llama 8B model and 18 by the Llama 70B.This process culminated in the creation of adversarial datasets with subtle and extreme degrees of manipulation for each underlying dataset, resulting in a total of four datasets.By systematically introducing errors tailored to each quality criterion, these datasets enable a detailed evaluation of LLM performance across a range of distortions.This two-level adversarial approach provides a comprehensive method for assessing the models' sensitivity to various levels of quality deterioration and their ability to assign appropriate evaluation scores under varying conditions.</p>
<p>One of the early papers on LLM-as-a-judge (Zheng et al., 2023) also used the strategy of an adversarial attack to test whether the judge LLM could detect verbosity bias which is related to our attack for redundancy.</p>
<p>A key NLP line of research in text adversarial attacks involves synonym substitution attacks (SSAs) (Alzantot et al., 2018) generate adversarial samples by replacing words with synonyms in benign text, relying on sophisticated methods such as TextFooler (Jin et al., 2020), PWWS (Ren et al., 2019), and BAE (Garg and Ramakrishnan, 2020).While SSAs were conjectured to produce low-quality text in fluency and meaning (Hauser et al., 2021;Chiang and Lee, 2023a), there was no guarantee of this, necessitating human raters to compare their assessments against LLM scores.In contrast, our adversarial attacks, ranging from subtle to extreme, deliberately violate English syntax, ensuring inherently low-quality text and obviating the need for human raters.This design directly tests whether the LLM can overcome its optimism bias and robustly assign the expected low scores.</p>
<p>C Manual Subsample Observations of</p>
<p>Vanilla LLM e val</p>
<p>We conducted a manual analysis of evaluation scores generated by Llama 8B, using a structured annotation process to assess the model's performance.The human adjudicator categorized issues based on six criteria from (Kim et al., 2023): "rejected feedback is not consistent with its score," "too general and abstract," "overly optimistic," "not relevant to the response," "overly critical," and "unrelated to the score rubric."This analysis covered 20 questions from ORKGSyn and 10 from BioASQ.</p>
<p>Our findings reveal that Llama 8B's evaluations of vanilla syntheses are generally aligned with human judgement.However, the feedback provided by the model tends to be overly general and abstract.For instance, while Llama 8B often identifies a lack of minor details, it frequently fails to specify what exactly is missing.In the adversarial settings, Llama 8B exhibits overly optimistic scoring across all syntheses.This pattern is consistent for both subtle and extreme adversarial datasets, though the ORKGSyn dataset receives even higher scores compared to BioASQ.Additionally, we observed that in a small fraction of cases, the scores were not relevant to the response.For example, Llama 8B occasionally hallucinated its own evaluation criteria and scored based on these fabricated metrics.Another notable issue is the inconsistency between the rationale provided and the assigned scores.In a few cases, the evaluation feedback explicitly states that there is almost no unnecessary information in the synthesis, yet the model assigns a perfect score of 5. Furthermore, for a significant portion of the scores, the rationale was very general and abstract, often merely reciting the evaluation characteristic guidelines provided in the system prompt without offering specific insights.</p>
<p>D Detailed Quantitative Experimental Results</p>
<p>To obtain a comprehensive assessment of the LLM's performance on the scientific Q&amp;A task, we conducted a quantitative analysis of their outputs.This evaluation leverages summarisationbased (such as BLUE (Papineni et al., 2002b), ROUGE (Lin, 2004b), METEOR (Banerjee and Lavie, 2005), NIST (Doddington, 2002), andBERTScore (Zhang et al., 2020)) and edit distancebased (such as WER (Su et al., 1992), WMD (Kus-ner et al., 2015), and MoverScore (Zhao et al., 2019)) metrics.These metrics measure the similarity between text outputs, providing a quantitative estimate of how closely the generated responses align with one another.The evaluation process involves treating each LLM response as a reference and pairing it with the outputs of every other model as candidates, resulting in all possible pairwise combinations.This approach ensures that we capture not only the absolute performance of each model but also their relative alignment.By examining the similarity scores between models, we gain insights into how consistently they generate responses.</p>
<p>D.1 Summarisation-Based Metrics</p>
<p>Evaluating the quality of generated text often requires comparing it to reference summaries.To achieve this, summarisation-based evaluation metrics measure the degree of textual overlap between the generated and reference texts, typically assessing aspects such as precision, recall, and n-gram similarity.These metrics are widely used in NLP tasks such as machine translation, text summarisation, and question answering.The summarisationbased metrics (i.e.BLUE, ROUGE, METEOR, NIST, and BERTScore) are described as follows.</p>
<p>Bilingual Evaluation Understudy (BLEU).The BLEU (Papineni et al., 2002b)     across the two datasets.On the BioASQ dataset, Mistral-Large and Qwen2.5-72Bdemonstrate a notable correlation with a ROUGE-1 score of 0.58, suggesting shared thematic elements or vocabulary usage in their outputs.Additionally, a ROUGE-L score of 0.4 indicates a moderate similarity in sentence structure and coherence.In contrast, the LLaMA-3.1-70Bmodel performs relatively poorly, which may stem from inconsistencies in generating high-quality outputs for certain tasks in this domain.On the ORKGSynthesis dataset, the LLaMA-3.1-70Bmodel achieves the highest correlation, particularly when compared to the smaller LLaMA model.Mistral and Qwen2.5-72B also display a strong correlation on this dataset, reinforcing their observed alignment.Exclusively focusing on unigrams in ROUGE-1 yields a higher correlation than BLUE and ROUGE-L, where overlaps are analyzed up to the four-gram level.This is because unigram-based evaluations inherently capture a broader overlap by  disregarding strict positional constraints or dependencies on higher-order matches.Metric for Evaluation of Translation with Explicit ORdering (METEOR).METEOR (Banerjee and Lavie, 2005) improves upon BLEU by prioritizing recall, which better aligns with human judgments.Unlike BLEU's brevity penalty, ME-TEOR explicitly integrates recall into its scoring.It also replaces BLEU's reliance on higher-order n-grams with direct word alignment, enhancing semantic and structural accuracy.Additionally, ME-TEOR avoids BLEU's zero-score issue by using an alignment-based approach that captures partial matches and syntactic nuances more effectively.</p>
<p>Results showed in Figure 10 for METEOR metric, which produces scores ranging from 0 (no correlation) to 1 (perfect correlation).In our evaluation, METEOR highlighted differences between the models.On the BioASQ dataset, the LLaMA-3.1-70Bmodel performed poorly, consistent with  earlier metrics.Conversely, the LLaMA-3.1-8Bmodel achieved comparable scores to Mistral-Large and Qwen2.5-72B,suggesting it generates responses that align well with unigram matches.For the ORKGSynthesis dataset, the scores were more uniform across models, reflecting a general similarity in performance.However, an intriguing pattern emerged with Qwen2.5-72B: when used as a reference, its scores varied substantially compared to when it was a candidate.This discrepancy may be attributable to METEOR's emphasis on recall.Higher scores when Qwen2.5-72B is a candidate suggest it produces longer outputs, increasing the likelihood of matches with reference terms.This raises questions about the relationship between generation length and perceived quality in evaluation metrics, which warrants further investigation.</p>
<p>US National Institute of Standards and Technology (NIST).The NIST (Doddington, 2002) score builds on the BLUE metric but introduces a signifi-  cant enhancement by focusing on the information content of n-grams.Unlike BLUE, which traditionally weights all matches equally, NIST prioritizes matches with greater informational value.This approach stems from the observation that rarer words or n-grams convey more specific and meaningful information than frequent ones.Consequently, matching infrequent n-grams contributes more to the NIST score, making it a more context-sensitive evaluation metric.</p>
<p>The NIST scores are unbounded and range from 0 (indicating poor quality) to higher positive values, with larger scores reflecting better-quality matches.Unlike BLUE, the NIST score's sensitivity to informational content allows it to provide more nuanced assessments.As shown in Figure 11 for the BioASQ dataset, the LLaMA-3.1-70Bmodel consistently underperforms, even when compared against its output.This poor performance can be attributed to the model's tendency to generate shorter responses and repeat words within a single response, leading to lower overall information gain.In contrast, other models demonstrate relatively strong correlations, suggesting more balanced and information-rich outputs.In the ORKGSynthesis dataset, correlations are notably higher across all models.The LLaMA-3.1-70B model achieves strong alignment with its smaller counterpart, consistent with observations in other metrics.Similarly, Mistral-Large and Qwen2.5-72Bcontinue to display strong correlations.BERTScore.BERTScore (Zhang et al., 2020) is designed to evaluate text similarity by leveraging contextualized embeddings from pre-trained transformer models like BERT (Devlin et al., 2019).Unlike traditional metrics that rely on exact token matches or n-gram overlap, BERTScore assesses semantic similarity at a token level, enabling it to recognize paraphrases, capture long-range dependencies, and account for nuanced semantic ordering.This approach allows it to surpass earlier metrics in evaluating complex and varied outputs, as it does not depend solely on surface-level text similarity.The strength of BERTScore lies in its ability to align tokens in a candidate sentence with those in a reference sentence using their embeddings, which encode rich contextual information.Studies by Zhang et al. (2020) demonstrate that BERTScore highly correlates with human judgment, making it a valuable tool for evaluating machine-generated text.</p>
<p>BERTScore generates values between 0 and 1, where a score closer to 1 indicates stronger semantic alignment.In our analysis w.r.t Figure 12, BERTScore highlights the limitations of the LLaMA-3.1-70Bmodel on the BioASQ dataset, as its tendency to produce incomplete or incoherent answers results in lower scores.The other models, including those evaluated on the ORKGSynthesis dataset, achieve scores near 0.9, which reflects a high degree of semantic similarity between their outputs.Unlike metrics such as BLUE, ROUGE, or METEOR, which primarily evaluate surfacelevel similarity based on token overlap or n-gram matches, BERTScore incorporates the contextual meaning of tokens.This capability allows it to capture deeper semantic relationships, even when lexical or syntactic differences exist between candidate and reference sentences.Consequently, while earlier metrics may indicate lower correlation, particularly for models generating paraphrased or stylistically varied outputs, BERTScore reveals that  the underlying semantic content remains closely aligned.This observation underscores the potential of embedding-based metrics to provide a more nuanced evaluation of language model outputs, particularly in tasks where paraphrasing and creative rewording are common.Furthermore, the high BERTScore results across most models suggest that their outputs are semantically coherent, even if traditional metrics fail to capture this aspect.This finding highlights the value of embedding-based metrics in complementing traditional approaches, providing a broad evaluation framework.Future work could explore fine-tuning the embeddings used in BERTScore to align even more closely with domain-specific human judgments, particularly in specialized tasks such as medical or scientific text generation.</p>
<p>D.2 Edit Distance-Based Metrics</p>
<p>To complement summary-based evaluation, we also incorporated edit distance-based metrics, which assess text similarity by measuring the number of modifications required to convert one string into another.This approach originates from Levenshtein distance (Levenshtein, 1966), a fundamental concept in text processing commonly applied in spellchecking and auto-correction.Edit distance evaluation quantifies text dissimilarity through insertions, deletions, and substitutions.For instance, converting the word mug to hut requires substituting two characters (m to h and g to t), which, if we assign a cost of 1 per insertion/deletion and 2 per substitution, yields an edit distance of 4. These costs can be adjusted depending on the task.Word Error Rate (WER).The WER (Su et al., 1992) is a similarity metric grounded in the concept of minimum edit distance, measuring the number of edits (substitutions, insertions, and deletions) required to transform a candidate text into reference text.Unlike character-level edit distance, WER operates at the word level, treating entire words as the basic transformation units.It is particularly suited for speech recognition and machine translation, where word-level alignment is essential.However, the metric has inherent limitations, particularly its sensitivity to word order.Sentences with semantically identical meanings but different word arrangements are heavily penalised, potentially leading to overly pessimistic assessments of similarity.</p>
<p>The analysis of WER according to Figure 13 scores across different datasets and models reveals several notable patterns.For the BioASQ dataset, the LLaMA-3.1-70Bmodel exhibits markedly poor performance as a reference.This behavior can be attributed to the model's tendency to produce shorter outputs.Since WER normalizes the number of edits by reference length, shorter reference texts amplify the impact of any discrepancies, leading to inflated WER values.This observation aligns with earlier findings highlighting LLaMA-3.1-70B'schallenges in generating comprehensive responses for the BioASQ dataset.In contrast, for the ORKGSynthesis dataset, Qwen2.5-72Bdemonstrates superior performance when serving as a reference.However, as a candidate, Qwen2.5-72Bachieves lower scores.This discrepancy likely arises from Qwen2.5-72B's tendency to generate longer sequences.In such cases, the normalization  by reference length in the WER formula leads to more significant deviations when Qwen2.5-72B's outputs are compared against shorter references.These findings underscore the metric's dependence on the relative length of candidate and reference texts, which can introduce biases when evaluating models with different generation strategies.While WER provides a straightforward measure of surface-level similarity, its inability to account for semantic equivalence or tolerate variations in word order limits its applicability in evaluating generative models.For instance, outputs with paraphrased structures or stylistic differences might receive high WER scores despite being semantically aligned with the reference.Word Mover's Distance (WMD).The WMD (Kusner et al., 2015) quantifies the dissimilarity between two text documents by calculating the minimum cumulative "distance" that the embedded words in one document must travel to align with the words in another document.The metric is inspired by the Earth Movers Distance, a concept in optimal transport theory, which measures the minimum work required to transform one probability distribution into another.Unlike greedy matching approaches like BERTScore, WMD leverages an optimal matching strategy, ensuring a more precise alignment of semantically relevant terms.In its original formulation, WMD used Word2Vec embeddings to represent words as vectors in a continuous space.However, we employed SPECTER2 (Singh et al., 2022) and SciB-ERT (Beltagy et al., 2019) embeddings, designed for scientific texts and have shown superior performance in domain-specific applications.The results of WMD using SPECTER2 embeddings are represented in Figure 15 and WMD with SciBERT is represented in Figure 14.A WMD score of 0 indicates perfect alignment between the  candidate and reference texts, while higher scores denote greater dissimilarity.Analysis of the WMD scores across datasets highlights several trends.As expected, LLaMA-3.1-70Bexhibits poor performance on the BioASQ dataset, consistent with its tendency to generate incoherent responses.In contrast on the ORKGSynthesis dataset, the two LLaMA models show strong alignment with one another, as do Mistral-Large and Qwen2.5-72B.An intriguing finding is the significant difference in WMD scores when using SPECTER embeddings compared to SciBERT embeddings.Specifically, SPECTER embeddings yield substantially lower WMD scores, suggesting they provide better semantic representations for this task.</p>
<p>MoverScore.The MoverScore (Zhao et al., 2019) is an advanced metric that extends the principles of WMD to evaluate the dissimilarity of text documents by comparing both words and n-grams.One of its primary advantages lies in its use of contextual embeddings, such as those generated by BERT, instead of static embeddings.This enables MoverScore to capture nuanced meanings, including word sense disambiguation and contextual relationships.Another key improvement is its allowance for many-to-one soft alignments, enabling more flexible matching between text elements.Furthermore, MoverScore incorporates inverse document frequency (IDF) weighting, emphasising rare and meaningful words, ensuring that these words contribute more significantly to the similarity score.The combination of BERT's contextual embeddings and IDF weighting has been shown by Zhao et al. (2019) to correlate highly with human judgment.MoverScore produces a value between 0 and 1, where higher scores correspond to greater similarity between the candidate and reference texts.Across both datasets as shown in Figure 16, Mover-Score results hover around 0.62, reflecting moderate alignment.As observed with other metrics, the LLaMA-3.1-70Bmodel performs slightly worse on the BioASQ dataset, likely attributable to the model's struggles with generating comprehensive and coherent responses.In contrast, the ORKGSynthesis dataset reveals strong correlations between the outputs of the Mistral-Large and Qwen2.5-72Bmodels, and between the LLaMA models.The relatively consistent scores across models suggest that, according to MoverScore, the quality of responses is comparable.This uniformity can be interpreted as evidence that the models generate outputs with similar semantic content and structure, despite potential stylistic differences.</p>
<p>E Evaluation Scoring Rubrics and the Five-point Likert Scale</p>
<p>This appendix presents the quality rubrics and their corresponding 5-point Likert scale descriptions used in our evaluation.</p>
<p>F Experimental Setup</p>
<p>Vanilla LLM eval models.The Vanilla LLM eval models employ various LLMs, including Mistral-Large-Instruct, LLaMA-3.1-70B-Instruct,Qwen2.5-72B-Instruct, and LLaMA-3.1-8B-Instruct, as LLM gen to generate response to the questions Q based on provided relevant papers.The evaluations for these models are conducted using LLM eval , where each LLM acts as a generator and evaluator in a pairwise format.For example, Mistral-Large-Instruct serves as the evaluator for all four LLM gen models, and the same process is applied to the other LLMs.The evaluation results are rated on a 5-point Likert scale, with each rating accompanied by a rationale that explains the model's reasoning.This configuration serves as a baseline for comparing the performance of different LLM eval .SFT (benign).The SFT (benign) experiment involves finetuning the LLaMA-3.1-8B-Instructmodel using structured response A's from all four Rubric 5-point Likert scale description Cohesion Rating 1. Very bad: The sentences within the synthesis are disconnected, resulting in a disjointed and fragmented narrative.Rating 2. Bad: There are attempts at connecting sentences, but the synthesis often feels disjointed.Rating 3. Moderate: The sentences are connected in a way that the synthesis is mostly cohesive, with some areas of improvement.Rating 4. Good: The synthesis is cohesive, with sentences well-connected to form a unified narrative.Rating 5. Very good: The synthesis is highly cohesive, with all sentences and paragraphs logically connected, facilitating a clear and coherent narrative flow.</p>
<p>Conciseness</p>
<p>Rating 1. Very Bad: The synthesis is verbose and cluttered with redundant or irrelevant information, significantly detracting from its clarity and focus.Rating 2. Bad: The synthesis includes some redundant or irrelevant statements, detracting from its clarity.Rating 3. Moderate: The synthesis is relatively clear and to the point, but could be more concise by eliminating a few redundant elements.Rating 4. Good: The synthesis is concise and to the point, with virtually no redundant statements or unnecessary information.Rating 5. Very Good: The synthesis is precisely concise, delivering information clearly and directly without any superfluous details or redundancy, enhancing its clarity and impact.</p>
<p>Readability</p>
<p>Rating 1. Very bad: The synthesis is poorly written, with pervasive issues in style, structure, and language use, making it difficult to understand.Rating 2. Bad: The text has noticeable issues with style, structure, or language use, affecting clarity.Rating 3. Moderate: The synthesis follows appropriate conventions and uses language correctly, with minor issues in style or structure.Rating 4. Good: The text is well-structured and easy to read, with language that is appropriately used and only minor stylistic improvements needed.Rating 5. Very good: The synthesis is exceptionally well-written, following stylistic and structural conventions with precise language use, making it accessible and easy to read.SFT (benign) + RL (benign + adversarial).Considering, the SFT (benign) + RL (adversarial) experiments, while the model achieves high performance in identifying adversarial samples, it struggles to assign higher Likert scale ratings to benign samples.This is because the model's threshold for defining a "good" sample has been lowered to a rating of 3. The reason for such a drop in the maximum Likert scale in testing is due to poor penalization in RL that considered the desirable threshold of ≤ 3 during the sampling of the data</p>
<p>Conciseness Evaluation</p>
<p>{"Readability": { "rating": 1, "rationale": "The synthesis is poorly written, with pervasive issues in style, structure, and language use, making it difficult to understand due to the inclusion of an unrelated joke about #jets fans and #marksanchez at the end."}}preparation for RLAIF for the subtle adversarial set.Essentially, training the model exclusively on adversarial behavior leads to confusion when it encounters benign examples.To solve this, the SFT (benign) + RL (benign + adversarial) is ideal for imitating both benign and adversarial behaviors, which ideally leads to success.</p>
<p>G Training Details</p>
<p>Supervised Fine-Tuning.The fine-tuning threats each LLM gen as an input x and outputs of all four LLM eval as a y.The fine-tuning SF T dataset stats are represented in Table 2.It includes research questions, synthesized responses, source papers, and evaluation rubrics scale to ensure the model learns both synthesis assessment and evaluation reasoning.The Figure 17 shows the y = LLM eval format during the fine-tuning process for the "Readability" rubric (the example is chosen from the Adversarial set).The tokenized sequences were processed with a maximum sequence length specific to each dataset (ORKG-Synthesis: 4, 645 tokens, BioASQ: 8, 874 tokens).We finetuned the model per dataset for 5 epochs using the Hardware and Resource Allocation.For the system setup, two H100 GPUs were utilized for RL fine-tunings, each with 80 GB of GPU memory, while for SFT models, only one H100 GPU with the same memory capacity was used.The CPU configuration for SFT involved 60 GB of memory with 8 cores of CPU, while RL took 60 GB of memory and 16 CPU cores.</p>
<p>H Detailed Qualitative Experimental Results</p>
<p>This section represents the detailed qualitative experimental results for seven models.In the tables, LLM eval models are defined as follows in the table columns:</p>
<p>• M1: Qwen2.5-72B</p>
<p>• M2: LLaMA-3.1-70B</p>
<p>• M3: Mistral-Large</p>
<p>• M4: LLaMA-3.1-8B</p>
<p>• M5: SFT (benign)</p>
<p>• M6: SFT (benign) + RL (adversarial)</p>
<p>• M7: SFT (benign) + RL (benign + adversarial)</p>
<p>The results for the BioASQ dataset are presented in Table 7 for LLaMA-3.1-8BLLM gen , Table 8 for LLaMA-3.1-70BLLM gen , Table 9 for Qwen2.5-72BLLM gen , and Table 10 for Mistral-Large LLM gen .While, for the ORKGSynthesis dataset, results are presented in Table 11 for LLaMA-3.1-8BLLM gen , Table 12 for LLaMA-3.1-70BLLM gen , Table 13 for Qwen2.5-72BLLM gen , and Table 14 for Mistral-Large LLM gen .</p>
<p>I Example Evaluations</p>
<p>This section presents example evaluations of LLMgenerated responses based on different rubrics.The examples were selected randomly from experimental models LLM eval outputs in test sets.The examples showcase both well-performing and problematic cases, emphasizing the strengths and weaknesses of different models and fine-tuning strategies.The examples are shown in Table 15 and  Table 16, which illustrate the challenges in vanilla and SFT models in distinguishing the desired behaviors.However, the SFT (benign) + RL (be-nign+adversarial) model demonstrates superior performance, even in terms of its rationales.</p>
<p>Examples from BioASQ in Table 15 show that SFT (benign) + RL (adversarial) often struggle with coherence and factual accuracy, as evidenced in examples 3 and 5.In contrast, models combining SFT (benign) + RL (benign + adversarial) tend to deliver more relevant and informative outputs, as seen in example 4. In Table 16 for the ORKGSynthesis dataset, examples in extreme settings, such as the SFT (benign) + RL (adversarial) rationales in example 8, demonstrate that LLM eval specifically identifies the inclusion of off-topic sentences, resulting in lower relevance and completeness ratings, as anticipated.The synthesis integrates the sources into a coherent manner within a single paragraph.However, the transitions or connections between ideas could be smoother.For example, the transition from the information about Denmark to the information about England could be more fluid.Additionally, the synthesis could benefit from better markers of provenance/quotation to clearly indicate which study provided each piece of information.The ideas are logically connected for the most part, but the narrative could be strengthened for better clarity.Some minor issues with sentence structure and wording were noted, but they do not significantly impact the overall coherence of the synthesis.7 benign LLaMA-3.1-70BSFT (benign) Informativeness 5</p>
<p>The synthesis is highly informative, providing valuable insights and detailed information that thoroughly addresses the research question.8 subtle LLaMA-3.1-70BSFT (benign) + RL (adversarial)</p>
<p>Coherence 3</p>
<p>The ideas are logically connected for the most part, but the narrative could be strengthened for better clarity.The synthesis jumps abruptly from discussing the biogenesis of circRNAs to mentioning a mortality rate without adequately transitioning or providing context.9 extreme LLaMA-3.1-70BSFT (benign) + RL (be-nign+adversarial)</p>
<p>Readability 1</p>
<p>The synthesis is poorly written, with pervasive issues in style, structure, and language use, making it difficult to understand.The sentence '#jets fans must be really confused with #marksanchez performance as a 2nd string QB for phily!! Lmao' is entirely unrelated to the topic of eRNAs and disrupts the flow of the text.The synthesis omits most of the relevant information, failing to capture the essential points or details from the provided abstracts.</p>
<p>The last sentence of the synthesis is unrelated to the research question and appears to be a random statement about a sporting event.5 subtle LLaMA-3.1-70BLLaMA-3.1-8BReadability 5</p>
<p>The synthesis follows academic writing conventions perfectly and displays appropriate style, structure, and language use, making it exceptionally accessible and easy to read.6 extreme Qwen2.5-72BSFT (benign) Readability 5</p>
<p>The synthesis is exceptionally well-written, following stylistic and structural conventions with precise language use, making it accessible and easy to read.The text is well-structured and free of any noticeable issues with style or structure.7 subtle LLaMA-3.1-8BSFT (benign) + RL (adversarial)</p>
<p>Conciseness 2</p>
<p>The synthesis includes some redundant or irrelevant statements, detracting from its clarity.For example, the synthesis repeats the phrase 'ultimately, finally, and in the end' and uses phrases like 'undoubtedly revealed, made evident, and clearly demonstrated' and 'highlighting, underscoring, and emphasizing the pressing, urgent, and critical need.'These repetitions make the synthesis less concise and more verbose than necessary.8 extreme Mistral-Large SFT (benign) + RL (be-nign+adversarial)</p>
<p>Relevancy 1</p>
<p>The synthesis starts off well by addressing the research question, but it includes an unrelated sentence about Stephen Curry, LeBron James, and Kevin Durant, which is completely out of scope and lacks any connection to the topic of optimizing drug delivery methods to effectively cross the blood-brain barrier.</p>
<p>Figure 1 :
1
Figure 1: Examples from two domains in the YESciEval science Q&amp;A dataset.Orange boxes show LLM input: a research question and titles of top-ranked papers (abstracts omitted for brevity).Green boxes show answer snippets from two LLMs.Light/dark gray boxes represent subtle/extreme adversarial variants targeting the conciseness and correctness rubrics.Yellow highlights indicate perturbations.YESciEval uses a nine-rubric LLM-as-a-judge scheme and tests robustness via rubric-specific adversarial edits (see Appendix B for details).</p>
<p>Figure 2 :
2
Figure 2: YESciEval LLM-as-a-Judge Alignment: Supervised fine-tuning of LLM eval , followed by reinforcement learning via Contrastive Preference Optimization to align open-source LLMs with desired rubric-level evaluations.</p>
<p>RubricG-Eval (Liu et al., 2023)  GPTScore(Fu et al., 2024) LLM-Alt (Chiang and Lee, 2023b) FLASK(Ye et al., 2024)  This YESciEval Work Coherence</p>
<p>Figure 5 :
5
Figure 5: Number of Questions per Research Field on the ORKGSyn Dataset.The y-axis represents the "Research Fields".</p>
<p>2.Figure 6 :
6
Figure 6: Number of Reference Abstracts per Question on the BioASQ dataset</p>
<p>metric automates machine translation evaluation, offering a costeffective alternative to human assessment.It measures n-gram overlap between a candidate and reference text, with a brevity penalty to prevent short translations.While widely used, BLEU has limitations, including insensitivity to semantic meaning and struggles with short texts.According to the Figure7, an anomaly occurs for the LLaMA-3.1-70Bmodel on the BioASQ dataset, where the score is 98.63.This discrepancy arises because the BLUE metric averages over n-gram scores, including cases where higher-order n-grams (e.g., four-grams) are absent in shorter outputs, forcing those scores to zero.Interestingly, the results reveal patterns of correlation among specific models.For instance, Mistral-Large and Qwen2.5-72Bexhibit moderate alignment, with scores around 25, indicating some similarity in their outputs.On the ORKGSynthesis dataset, the LLaMA-3.1-70B and LLaMA-3.1-8Bmodels display a relatively high correlation, likely due to their shared Meta origin, similar training data, and primary differences in parameter count.In terms of individual perfor-</p>
<p>Figure 7 :
7
Figure 7: BLEU metric results.</p>
<p>Figure 8 :
8
Figure 8: ROUGE-1 metric results.</p>
<p>Figure 9 :
9
Figure 9: ROUGE-L metric results.</p>
<p>Figure 10 :
10
Figure 10: METEOR metric results.</p>
<p>Figure 11 :
11
Figure 11: NIST metric results</p>
<p>Figure 12 :
12
Figure 12: BERTScore metric results.</p>
<p>Figure 13 :
13
Figure 13: WER metric results</p>
<p>Figure 14 :
14
Figure 14: WMD metric results using SciBERT as embeddings.</p>
<p>Figure 15 :
15
Figure 15: WMD metric results using SPECTER2 as embeddings.</p>
<p>Figure 16 :
16
Figure 16: MoverScore metric results</p>
<p>Figure 17 :
17
Figure 17: The y format for fine-tuning models.</p>
<p>and consistently addresses the research question of how circRNAs are derived, providing a clear and concise overview of the back-splicing process and its key aspects.factual inaccuracies regarding the regulatory mechanisms of circRNAs.While the overall structure is well-organized, some key claims about exon-skipping and alternative splicing are incorrect.</p>
<p>Table 2 :
2
Dataset statistics across training and test sets for LLM gen and LLM eval .</p>
<p>Table 3 :
3
Comparison of evaluation rubrics across different works against ours (last column).Cells marked in green indicate rubrics that apply to a particular work.
rubric) despite perturbations introducing unrelatedusing pairwise preference evaluations (Wang et al.,sports news sentences. Fine-tuning on benign data2023a; Chiang and Lee, 2023b; Dubois et al., 2023;alone (yellow) further amplified optimism, neces-Liu et al., 2023). Some works incorporated rubrics,sitating alignment. When RL was applied only tosuch as G-Eval (Liu et al., 2023) for summariza-adversarial data (red), the model became overly pes-tion and GPTScore (Fu et al., 2024), which alignssimistic. However, when RL was trained on bothclosely with our criteria. Recent frameworks em-benign and adversarial samples (green), Llama 8Bphasize fine-grained rubrics; FLASK (Ye et al.,stabilized as a robust evaluator, addressing the RQ.2024) assesses robustness, correctness, efficiency,It assigned relatively high scores for benign synthe-ses while distinguishing adversarial perturbations,scoring around 1 in extreme cases and around 3 insubtle cases, demonstrating rubric-specific discrim-ination.7 Related Work7.1 Question &amp; Answering BenchmarksAutomatic Q&amp;A spans diverse datasets varyingin domain and Q&amp;A type. Of 41 NLP Q&amp;Adatasets reviewed by Wang (2022), only BioASQaligns with scienceQ&amp;A. Multiple-choice (e.g.,PubMedQA (2019), MMLU (2021)), Boolean (e.g.,BoolQ (2019)), and numerics (e.g., Math Dataset(2019)) fall outside our scope, as do bibliographic(Banerjee et al., 2023) and knowledge graph ex-traction datasets (Auer et al., 2023; Yan et al.,2024). Existing benchmarks, such as Hugging Faceleaderboard tasks (Wang et al., 2024; Rein et al.,2023) and alignment-focused chat-based evalua-tions (Reddy et al., 2019; Zheng et al., 2023; Köpfet al., 2024), primarily assess multiple-choice rea-soning or human preference alignment. In contrast,we introduce a generative scienceQ&amp;A dataset, fill-ing a gap in current benchmarking efforts.
7.2 LLM-as-a-judge Evaluation RubricsLLM-as-a-judge(Zheng et al., 2023)initially focused on correlating LLM evaluations with human judgments in open-domain NLG, primarily factuality, and readability, of which nine align with our work.Prometheus(Kim et al., 2023(Kim et al.,  , 2024)  )</p>
<p>Proceedings of the 41st International Conference on Machine Learning, ICML'24.JMLR.org.Xi Yan, Patrick Westphal, Jan Seliger, and Ricardo Usbeck.2024.Bridging the gap: Generating a comprehensive biomedical knowledge graph question answering dataset.In ECAI 2024, pages 1198-1205.IOS Press.
International Conference on Computational Linguis-tics.Alon Talmor and Jonathan Berant. 2018. The web asa knowledge-base for answering complex questions.In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Volume 1 (Long Papers), pages 641-651.Minyang Tian, Luyu Gao, Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji, Kit-tithat Krongchon, Yao Li, et al. 2024. Scicode: A research coding benchmark curated by scientists. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track.Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeon-bin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2024. FLASK: Fine-grained language model evaluation based on alignment skill sets. In ICLR 2024 Workshop on Large Language Model (LLM) Agents.Adam R Villaflor, Zhe Huang, Swapnil Pande, John M Dolan, and Jeff Schneider. 2022. Addressing opti-mism bias in sequence modeling for reinforcement learning. In international conference on machineTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. Preprint, arXiv:1904.09675.learning, pages 22270-22283. PMLR.Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534-7550, Online. As-sociation for Computational Linguistics.tian M. Meyer, and Steffen Eger. 2019. MoverScore: Text generation evaluating with contextualized em-beddings and earth mover distance. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna-tional Joint Conference on Natural Language Pro-cessing (EMNLP-IJCNLP), pages 563-578, Hong Kong, China. Association for Computational Lin-Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkuiguistics.Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator?Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuana preliminary study. In Proceedings of the 4th NewZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Frontiers in Summarization Workshop, pages 1-11.Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023b. Self-instruct: Aligning languageJudging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595-46623.models with self-generated instructions. In Proceed-Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Bings of the 61st Annual Meeting of the Association forBrown, Alec Radford, Dario Amodei, Paul Chris-Computational Linguistics (Volume 1: Long Papers),tiano, and Geoffrey Irving. 2019. Fine-tuning lan-pages 13484-13508.guage models from human preferences. arXivYubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,preprint arXiv:1909.08593.Abhranil Chandra, Shiguang Guo, Weiming Ren,Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024.Mmlu-pro: A more robust and challenging multi-tasklanguage understanding benchmark. arXiv preprintarXiv:2406.01574.Zhen Wang. 2022.Modern question answeringdatasets and benchmarks: A survey. arXiv preprintarXiv:2206.15030.Jason Wei, Xuezhi Wang, Dale Schuurmans, MaartenBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,et al. 2022. Chain-of-thought prompting elicits rea-soning in large language models. Advances in neuralinformation processing systems, 35:24824-24837.
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim.2024.Contrastive preference optimization: pushing the boundaries of llm performance in machine translation.In</p>
<p>Table 4 outlines the Linguistic and Stylistic quality rubrics, assessing aspects such as cohesion, conciseness, and readability.Table 5 details the Logical and Structural Integrity rubrics, evaluating factors like argument coherence, integration, and relevancy.Lastly, Table 6 describes the Content Accuracy and Informativeness rubrics, which measure correctness, completeness, and informativeness.</p>
<p>Table 4 :
4
Linguistic and Stylistic Quality Rubrics and their 5-point Likert Scale Description The ideas are not always logically connected, leading to a somewhat confusing narrative.Rating 3. Moderate: The ideas are logically connected for the most part, but the narrative could be strengthened for better clarity.Rating 4. Good: The ideas are logically and soundly connected, offering a clear and understandable narrative.Rating 5. Very good: The ideas within the synthesis are connected in a logical and sound manner, forming a coherent and compelling narrative that is easy to follow.The synthesis fails to integrate the sources in any meaningful way.It lacks appropriate markers, connectors, or transitions between ideas and fails to combine the information into a single, cohesive paragraph.Rating 2. Bad: The sources are somewhat integrated but inconsistently.The use of markers and connectors is sporadic or inappropriately applied, and the information is presented in multiple paragraphs without a clear unifying structure.Rating 3. Moderate: The sources are integrated into a coherent manner within one or multiple paragraphs.The transitions or connections could be smoother, and the text would benefit from better paragraph structure to enhance clarity and unity.Rating 4. Good: The sources are well-integrated, using appropriate markers and connectors to create a seamless narrative.The information is effectively organized into a single paragraph, showing a clear, unified approach.Rating 5. Very Good: The synthesis seamlessly integrates information from the various sources, using appropriate markers and connectors to create a smooth and unified narrative.All information is skillfully condensed into a single, well-structured paragraph, exemplifying excellent integration.
Rubric5-point Likert scale descriptionRating 1. Very bad: The synthesis lacks logical connection between ideas, leading to a narrative that is confusingand difficult to follow.Coherence Rating 2. Bad: Integration Rating 1. Very bad: The information provided does not relate to the research question, showing a lack of understanding or connection to the topic. Rating 1. Very Bad: Relevancy Rating 2. Bad: The information occasionally relates to the research question but lacks direct and consistentrelevance.Rating 3. Moderate: The information is generally related to the research question, with occasional lapses indirect relevance.
Rating 4. Good: The information is consistently relevant to the research question, with only minor exceptions.Rating 5. Very good: The synthesis is directly and consistently relevant to the research question, demonstrating a deep understanding of the topic and its nuances.</p>
<p>Table 5 :
5
Logical and Structural Integrity Quality Rubrics and their 5-point Likert Scale Description The synthesis consistently misrepresents or inaccurately portrays the content of the provided abstracts, showing a significant deviation from the original sources.Rating 2. Bad: The synthesis contains several inaccuracies or misinterpretations of the source abstracts.Rating 3. Moderate: The synthesis accurately represents most of the content from the provided abstracts but may contain minor errors.Rating 4. Good: The synthesis provides an accurate representation of the content from the provided abstracts with minor exceptions.Rating 5. Very good: The information in the synthesis is an accurate and faithful representation of the content from the provided abstracts, without any factual errors or misinterpretations.The synthesis omits most of the relevant information, failing to capture the essential points or details from the provided abstracts.Rating 2. Bad: Significant portions of relevant information from the provided abstracts are missing.Rating 3. Moderate: The synthesis captures a fair amount of the relevant information, though it may overlook some details.Rating 4. Good: The synthesis includes almost all relevant information, missing only minor details.Rating 5. Very good: The synthesis comprehensively encapsulates all relevant information from the provided abstracts, leaving no pertinent details or points unaddressed.The synthesis offers no valuable insights or useful information in response to the research question, lacking depth and utility.Rating 2. Bad: The answer provides limited new insights or useful information in response to the research question.Rating 3. Moderate: The answer is somewhat informative, offering insights or useful information but not in a comprehensive or detailed manner.Rating 4. Good: The answer is informative and insightful, providing comprehensive information in response to the research question.Rating 5. Very good: The synthesis is highly informative, providing valuable insights and detailed information that thoroughly addresses the research question.
Rubric5-point Likert scale descriptionCorrectness Rating 1. Very bad: Completeness Rating 1. Very bad: Informativeness Rating 1. Very bad:</p>
<p>Table 6 :
6
Content Accuracy and Informativeness Quality Rubrics and their 5-point Likert Scale Description models from x = LLM gen as inputs and quality assessments from y = LLM eval as outputs.The aim is to fine-tune LLM eval and this process is
conducted using benign datasets, which contain noadversarial examples, to ensure that the model istrained in a controlled, non-hostile environment.The goal is to refine the model's performance in astraightforward, non-challenging scenario.SFT (benign) + RL (adversarial). In this experi-ment, the finetuned LLaMA-3.1-8B-Instruct modelis further finetuned using the RLAIF technique, fo-cusing solely on adversarial datasets. As seen in theresults from Figure 4, while SFT (benign) learnshow to be the evaluator, but fails in recognizing thebad examples in adversarial sets. The SFT (benign)+ RL (adversarial) helps LLM eval to imitate thebad examples as well.</p>
<p>Paged AdamW 8bit optimizer, ensuring memory efficiency.Key hyperparameters included a batch size of 1 per GPU (we used two GPUs), gradient accumulation steps equal to batch size, a learning rate of 2e − 4 with a warmup ratio of 0.03, weight decay of 0.001, and a max gradient norm of 0.3.The fine-tuned model checkpoints were saved for further analysis and RL-based fine-tuning.Reinforcement Learning.Several hyperparameters and configurations for training an LLM with RL are used.Key hyperparameters include a learning rate of 2e − 4, a batch size of 1 (per GPU), and a total of 2 training epochs.The model's training is configured with a maximum prompt length of 4500 and a maximum completion length of 150.The CPO uses a per-device batch size of 1, gradient accumulation steps of 1, and mixed precision (fp16).Additionally, the model undergoes fine-tuning with a learning rate of 2e − 4.</p>
<p>The synthesis follows academic writing conventions almost perfectly and displays appropriate style.However, the last sentence is a bit unrelated to the research question and seems to be a non-sequitur, which slightly affects the overall rating.
SetLLMgenLLMevalRubricRating Rationale1subtleLLaMA-3.1-8B LLaMA-3.1-8BReadability42benignLLaMA-3.1-70B SFT (benign)Coherence1The synthesis provided does not form a coherent narrative, mak-ing it difficult to follow.3benignMistral-LargeSFT (benign) + RL (ad-Integration3versarial)</p>
<p>Table 15 :
15
Example evaluation of LLM-generated responses based on different rubrics for the BioASQ dataset.The synthesis includes a relevant discussion about fake news detection methods, but it deviates significantly from the topic of detecting fake accounts.The last sentence about Stephen Curry, LeBron James, and Kevin Durant winning the 2024 Paris Olympics is completely out of scope and irrelevant to the re-
SetLLMgenLLMevalRubricRating Rationale1 benign LLaMA-3.1-70B LLaMA-3.1-8BCorrectness5The synthesis accurately represents the content from the pro-vided abstracts, without any factual errors or misinterpreta-tions. The information is a faithful representation of the originalsources, demonstrating a high level of correctness.2 benignMistral-LargeSFT (benign)Integration5The synthesis seamlessly integrates information from the varioussources, using appropriate markers and connectors to createa smooth and unified narrative. All information is skillfullycondensed into a single, well-structured paragraph.3 extremeQwen2.5-72BSFT (benign) + RL (ad-Relevancy1versarial)search question.4 extremeQwen2.5-72BSFT (benign) + RL (be-Completeness1nign+adversarial)</p>
<p>Table 16 :
16
Example evaluation of LLM-generated responses based on different rubrics for the ORKGSynthesis dataset.</p>
<p>https://docs.hpc.gwdg.de/services/chat-ai/ index.html
In this paper, "cost-free" refers to the elimination of human annotation and proprietary API costs, but not compute.
AcknowledgementsThis work is jointly supported by the SCINEXT project (BMBF, Grant ID: 01IS22070), the KISSKI AI Service Center (BMBF, ID: 01IS22093C), and NFDI4DataScience (DFG, ID: 460234259).
Alibabacloud, Qwen2.5: A party of foundation models. 2024. 16 February 2025</p>
<p>Generating natural language adversarial examples. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, Kai-Wei Chang, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova Dassarma, arXiv:2112.00861A general language assistant as a laboratory for alignment. 2021arXiv preprint</p>
<p>Daniil Radyush, et al. 2023. The sciqa scientific question answering benchmark for scholarly knowledge. Sören Auer, Dante Ac Barone, Cassiano Bartz, Eduardo G Cortes, Mohamad Yaser Jaradeh, Oliver Karras, Manolis Koubarakis, Dmitry Mouromtsev, Dmitrii Pliukhin, Scientific Reports. 1317240</p>
<p>Improving access to scientific literature with knowledge graphs. Sören Auer, Allard Oelen, Muhammad Haris, Markus Stocker, D' Jennifer, Kheir Eddine Souza, Lars Farfar, Manuel Vogt, Vitalis Prinz, Mohamad Wiens, Jaradeh Yaser, Bibliothek Forschung und Praxis. 4432020</p>
<p>Llms4synthesis: Leveraging large language models for scientific synthesis. Hamed Babaei Giglou, D' Jennifer, Sören Souza, Auer, Proceedings of the 24th ACM/IEEE Joint Conference on Digital Libraries. the 24th ACM/IEEE Joint Conference on Digital Libraries2024</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Dblp-quad: A question answering dataset over the dblp scholarly knowledge graph. Debayan Banerjee, Sushil Awale, Ricardo Usbeck, Chris Biemann, arXiv:2303.133512023arXiv preprint</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>SciB-ERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Usa basketball wins gold medal: Stephen curry. Brad Botkin, 15.012024. 2025lebron james hold off france at 2024 paris olympics</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Are synonym substitution attacks really synonym substitution attacks?. Cheng- , Han Chiang, Hung-Yi Lee, Findings of the Association for Computational Linguistics: ACL 2023. 2023a</p>
<p>Can large language models be an alternative to human evaluations?. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.acl-long.870Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics2023b</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 201730</p>
<p>Boolq: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Think you have solved question answering? try arc, the ai2 reasoning challenge. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, arXiv:1803.054572018arXiv preprint</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, Advances in neural information processing systems. 202336</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. George Doddington, Proceedings of the Second International Conference on Human Language Technology Research, HLT '02. the Second International Conference on Human Language Technology Research, HLT '02San Francisco, CA, USAMorgan Kaufmann Publishers Inc2002</p>
<p>Chat ai: A seamless slurmnative solution for hpc-based services. Ali Doosthosseini, Jonathan Decker, Hendrik Nolte, Julian M Kunkel, arXiv:2407.001102024Preprint</p>
<p>. D' Jennifer, Quentin Souza, Hamed Muench, Giglou Babaei, 10.25835/8DCV2KA62025Yescieval corpus</p>
<p>Alpacafarm: A simulation framework for methods that learn from human feedback. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, Advances in Neural Information Processing Systems. 202336</p>
<p>Large language models as evaluators for scientific synthesis. Julia Evans, D' Jennifer, Sören Souza, Auer, Proceedings of the 20th Conference on Natural Language Processing (KONVENS 2024). the 20th Conference on Natural Language Processing (KONVENS 2024)2024</p>
<p>Can machines think? Accessed: 15.01. Jawwad Ahmed, Farid , 2024. 2025</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See Kiong Ng, Zhengbao Jiang, Pengfei Liu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20241</p>
<p>Bae: Bert-based adversarial examples for text classification. Siddhant Garg, Goutham Ramakrishnan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, arXiv:2305.15717The false promise of imitating proprietary llms. 2023arXiv preprint</p>
<p>Bert is robust! a case against synonym-based adversarial examples in text classification. Jens Hauser, Zhao Meng, Damián Pascual, Roger Wattenhofer, arXiv:2109.074032021arXiv preprint</p>
<p>Joey Hejna, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, Bradley Knox, Dorsa Sadigh, arXiv:2310.13639Contrastive prefence learning: Learning from human feedback without rl. 2023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002a</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USA. Association for Computational Linguistics2002b</p>
<p>Core-gpt: Combining open access research and large language models for credible, trustworthy question answering. David Pride, Matteo Cancellieri, Petr Knoth, International Conference on Theory and Practice of Digital Libraries. 2023</p>
<p>Direct preference optimization: your language model is secretly a reward model. Springer, Archit Rafael Rafailov, Eric Sharma, Stefano Mitchell, Christopher D Ermon, Chelsea Manning, Finn, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2023</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational Linguistics2016</p>
<p>Coqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 72019</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Generating natural language adversarial examples through probability weighted word saliency. Yihe Shuhuai Ren, Kun Deng, Wanxiang He, Che, Proceedings of the 57th annual meeting of the association for computational linguistics. the 57th annual meeting of the association for computational linguistics2019</p>
<p>Mctest: A challenge dataset for the open-domain machine comprehension of text. Matthew Richardson, J C Christopher, Erin Burges, Renshaw, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processing2013</p>
<p>A survey of evaluation metrics used for nlg systems. Akash Ananya B Sai, Mitesh M Kumar Mohankumar, Khapra, ACM Computing Surveys (CSUR). 5522022</p>
<p>Analysing mathematical reasoning abilities of neural models. David Saxton, Edward Grefenstette, Felix Hill, Pushmeet Kohli, International Conference on Learning Representations. 2019</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Results of the WNUT16 named entity recognition shared task. Benjamin Strauss, Bethany Toma, Alan Ritter, Marie-Catherine De Marneffe, Wei Xu, Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT). the 2nd Workshop on Noisy User-generated Text (WNUT)Osaka, Japan2016The COLING 2016 Organizing Committee</p>
<p>A new quantitative quality measure for machine translation systems. Keh-Yih Su, Ming-Wen Wu, Jing-Shin Chang, The 14th. 1992. 19922COLING</p>
<p>BioASQ dataset detailed evaluation results of various LLM eval over LLM gen = Mistral-Large. Table. 10</p>
<p>ORKGSynthesis dataset detailed evaluation results of various LLM eval over LLM gen = LLaMA-3.1. 70B. 1. Coherence 4.79 4.95 4.83 5.00 4.92 3.00 4.98Table. 12</p>
<p>ORKGSynthesis dataset detailed evaluation results of various LLM eval over LLM gen = Mistral-Large. Table. 14</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6767 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6767</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6767</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-df65f36d9e16b9c6352fa30843040e96024bee63</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/df65f36d9e16b9c6352fa30843040e96024bee63" target="_blank">Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving</a></p>
                <p><strong>Paper Venue:</strong> Machine Graphics and Vision: international journal</p>
                <p><strong>Paper TL;DR:</strong> This work presents the Rule-based Explaining Module (REM), which is designed to provide explanations of the decision-making processes using Recurrent Relational Networks (RRN) to bridge the gap between complex RRN models and human understanding by unveiling the specific rules applied by the model at each stage of the Sudoku puzzle solving process.</p>
                <p><strong>Paper Abstract:</strong> Computer vision has gained significant attention in the field of information technology due to its widespread application that addresses real-world challenges, surpassing human intelligence in tasks such as image recognition, classification, natural language processing, and even game playing. Sudoku, a challenging puzzle that has captivated many people, exhibits a complexity that has attracted researchers to leverage deep learning techniques for its solution. However, the reliance on black-box neural networks has raised concerns about transparency and explainability. In response to this challenge, we present the Rule-based Explaining Module (REM), which is designed to provide explanations of the decision-making processes using Recurrent Relational Networks (RRN). Our proposed methodology is to bridge the gap between complex RRN models and human understanding by unveiling the specific rules applied by the model at each stage of the Sudoku puzzle solving process. Evaluating REM on the Minimum Sudoku dataset, we achieved an accuracy of over 98.00%.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6767.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6767.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Relational Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph‑structured recurrent neural network that performs iterative message passing between node representations (cells) with MLP message and update functions to capture long‑range relational dependencies; used here as the backbone Sudoku solver producing per‑cell digit probability distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent relational networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Relational Network (RRN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Graph neural network with iterative message passing: messages m_ij = f(h_i, h_j) and node updates h_j^t = g(h_j^{t-1}, x_j, m_j^t), with MLP implementations for f, g, and the output decoder k producing softmax over digits.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>518006 parameters (total trainable parameters reported for the full system)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (grid-based spatial puzzle)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Iterative relational message passing (graph neural network) combined with downstream rule‑based inference (REM + rule solver); implicit constraint propagation via learned messages</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>81 input nodes (one per Sudoku cell) each with a feature vector x_i; hidden state vectors h_i^t per node updated across timesteps; output k(h_i^t) produces a softmax over 10 digit classes per cell; neighboring relations defined by row/column/block adjacency.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Norvig Sudoku solver (classical backtracking solver) used to verify correctness of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Solve accuracy / per‑rule accuracy; steps until solve; speed (puzzles/sec)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Minimum Sudoku dataset: initial candidate assignment yields 72.32% solved; applying rules sequentially yields per‑rule accuracies: Rule1 99.00%, Rule2 98.98%, Rule3 98.62%, Rule4 98.60%, Rule5 98.79%, Rule6 98.67%; 1M Sudoku dataset: initial candidate assignment 95.41% solved; Rules 1–6 show 100% when applied; overall reported solving speed 20 puzzles/sec.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Hidden states from RRN contain information sufficient to map to classic human‑readable solving rules via an MLP (REM); highest model accuracy observed around step ≈32 during iterative solving; RRN outputs per‑cell digit probabilities (candidate lists) which REM maps to rule choices; for the easier/medium Kaggle dataset only simple rules (Hidden/Naked Single) were used to finish puzzles, causing later rules to appear unused but 'perfect' when applied; demonstrates that learned relational message passing can be coupled with symbolic rule extraction for interpretable solving.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No systematic ablation reported (authors did not report results removing RRN or REM or varying components); only comparison across datasets and per‑rule application order.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>System limited to the six selected rules (Hidden Single, Naked Single, Locked Candidates Type1/2, Naked Pair, Hidden Pair); if a puzzle cannot be solved within these rules, the solver halts; reported high accuracies on datasets used but generalization beyond the selected rules or to other puzzle types was not demonstrated; no ablation or comparisons to alternative explanation methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6767.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6767.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule-based Explaining Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule‑prediction module (MLP) that maps the RRN's hidden states to probabilities over a fixed set of human‑interpretable Sudoku solving rules, enabling step‑by‑step explanations of the model's decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rule-based Explaining Module (REM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A multi‑layer perceptron that takes the full set of RRN hidden states as input and outputs probabilities for each of a predefined set of rules (six rules used in experiments). It is trained alongside/after the RRN to predict which rule(s) apply at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Included in the reported 518006 total trainable parameters (no separate parameter count provided)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Supervised classification from hidden states to human rules (rule detection), combined with a symbolic rule‑based step solver that applies the predicted rules to update candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Takes RRN hidden states h^t (aggregate of per‑cell hidden vectors) as input features; outputs R_i^t, a probability vector across i=1..6 predefined rules for each solving step.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per‑rule accuracy (percentage correct identification/application of each rule); overall solve accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Minimum Sudoku: per‑rule accuracies near ≈98.6–99.0% across the six rules with best step accuracies reported (e.g., Rule1 best 99.00%); on 1M Sudoku: rules reported as 100% when applied (but later rules often unused because earlier rules solved puzzles). The system achieves >98% accuracy overall on the Minimum Sudoku dataset and 100% on the 1M dataset per their reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>REM successfully maps learned neural hidden states to classical human rules, allowing interpretable stepwise explanations; rule detection peaks around step ~32 during iterative solving; for easier datasets REM predicts only simpler rules leading to later rules not being utilized.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No ablation study reported comparing with/without REM or alternative explanation modules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>REM is constrained to the preselected six rules and therefore cannot explain or drive solving steps that require more advanced tactics; when rules are unused (e.g., on the 1M dataset) their reported '100%' accuracy reflects non‑use rather than robust detection; generalization beyond Sudoku or beyond the chosen rule set not evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6767.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6767.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rule‑based Sudoku step solver</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rule‑based Sudoku Step Solver (knowledge‑base updater)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic module that, given candidate lists and predicted rules, applies classical Sudoku rules stepwise to eliminate candidates and fill cells, iterating until a step is resolved or no rule applies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Rule‑based Sudoku step solver</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A deterministic, symbolic step solver that assigns initial candidates then applies rules in sequence (Naked/Hidden Single, Locked Candidates Type1/2, Naked Pair, Hidden Pair) to eliminate candidates and fill cells; it revisits rules iteratively until progress is made or no applicable rule remains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Classical rule‑based candidate elimination and filling (symbolic constraint propagation using the six selected human rules)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Per‑cell candidate lists derived from RRN softmax outputs; knowledge base updated with eliminations and assigned digits stepwise.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Solve success rate after up to 64 steps; per‑rule application accuracies</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Counts and percentages reported in paper when combined with RRN+REM (see entries above); overall solving speed 20 puzzles/sec.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>The symbolic step solver allows clear mapping from predicted rules to concrete candidate eliminations and assignments, enabling human‑readable explanations; the design choice to stop when no rule applies limits ability to solve puzzles needing more advanced or search‑based tactics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>No explicit ablation of the step solver vs. pure learned decoding was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Restricted rule set; inability to proceed when none of the six rules apply (no fallback search/backtracking); explanation quality depends on correctness of REM predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent relational networks <em>(Rating: 2)</em></li>
                <li>Using small MUSes to explain how to solve pen and paper puzzles <em>(Rating: 2)</em></li>
                <li>A framework for step-wise explaining how to solve constraint satisfaction problems <em>(Rating: 2)</em></li>
                <li>Searching for explainable solutions in Sudoku <em>(Rating: 2)</em></li>
                <li>Solving every Sudoku puzzle <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6767",
    "paper_id": "paper-df65f36d9e16b9c6352fa30843040e96024bee63",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "RRN",
            "name_full": "Recurrent Relational Network",
            "brief_description": "A graph‑structured recurrent neural network that performs iterative message passing between node representations (cells) with MLP message and update functions to capture long‑range relational dependencies; used here as the backbone Sudoku solver producing per‑cell digit probability distributions.",
            "citation_title": "Recurrent relational networks",
            "mention_or_use": "use",
            "model_name": "Recurrent Relational Network (RRN)",
            "model_description": "Graph neural network with iterative message passing: messages m_ij = f(h_i, h_j) and node updates h_j^t = g(h_j^{t-1}, x_j, m_j^t), with MLP implementations for f, g, and the output decoder k producing softmax over digits.",
            "model_size": "518006 parameters (total trainable parameters reported for the full system)",
            "puzzle_name": "Sudoku",
            "puzzle_type": "constraint satisfaction / combinatorial logic (grid-based spatial puzzle)",
            "dataset_name": "Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)",
            "prompting_method": null,
            "reasoning_technique": "Iterative relational message passing (graph neural network) combined with downstream rule‑based inference (REM + rule solver); implicit constraint propagation via learned messages",
            "internal_representation": "81 input nodes (one per Sudoku cell) each with a feature vector x_i; hidden state vectors h_i^t per node updated across timesteps; output k(h_i^t) produces a softmax over 10 digit classes per cell; neighboring relations defined by row/column/block adjacency.",
            "use_of_external_tool": true,
            "external_tool_description": "Norvig Sudoku solver (classical backtracking solver) used to verify correctness of solutions.",
            "evaluation_metric": "Solve accuracy / per‑rule accuracy; steps until solve; speed (puzzles/sec)",
            "performance": "Minimum Sudoku dataset: initial candidate assignment yields 72.32% solved; applying rules sequentially yields per‑rule accuracies: Rule1 99.00%, Rule2 98.98%, Rule3 98.62%, Rule4 98.60%, Rule5 98.79%, Rule6 98.67%; 1M Sudoku dataset: initial candidate assignment 95.41% solved; Rules 1–6 show 100% when applied; overall reported solving speed 20 puzzles/sec.",
            "analysis_findings": "Hidden states from RRN contain information sufficient to map to classic human‑readable solving rules via an MLP (REM); highest model accuracy observed around step ≈32 during iterative solving; RRN outputs per‑cell digit probabilities (candidate lists) which REM maps to rule choices; for the easier/medium Kaggle dataset only simple rules (Hidden/Naked Single) were used to finish puzzles, causing later rules to appear unused but 'perfect' when applied; demonstrates that learned relational message passing can be coupled with symbolic rule extraction for interpretable solving.",
            "ablation_comparison": "No systematic ablation reported (authors did not report results removing RRN or REM or varying components); only comparison across datasets and per‑rule application order.",
            "limitations": "System limited to the six selected rules (Hidden Single, Naked Single, Locked Candidates Type1/2, Naked Pair, Hidden Pair); if a puzzle cannot be solved within these rules, the solver halts; reported high accuracies on datasets used but generalization beyond the selected rules or to other puzzle types was not demonstrated; no ablation or comparisons to alternative explanation methods reported.",
            "uuid": "e6767.0",
            "source_info": {
                "paper_title": "Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "REM",
            "name_full": "Rule-based Explaining Module",
            "brief_description": "A rule‑prediction module (MLP) that maps the RRN's hidden states to probabilities over a fixed set of human‑interpretable Sudoku solving rules, enabling step‑by‑step explanations of the model's decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Rule-based Explaining Module (REM)",
            "model_description": "A multi‑layer perceptron that takes the full set of RRN hidden states as input and outputs probabilities for each of a predefined set of rules (six rules used in experiments). It is trained alongside/after the RRN to predict which rule(s) apply at each step.",
            "model_size": "Included in the reported 518006 total trainable parameters (no separate parameter count provided)",
            "puzzle_name": "Sudoku",
            "puzzle_type": "constraint satisfaction / combinatorial logic",
            "dataset_name": "Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)",
            "prompting_method": null,
            "reasoning_technique": "Supervised classification from hidden states to human rules (rule detection), combined with a symbolic rule‑based step solver that applies the predicted rules to update candidates.",
            "internal_representation": "Takes RRN hidden states h^t (aggregate of per‑cell hidden vectors) as input features; outputs R_i^t, a probability vector across i=1..6 predefined rules for each solving step.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Per‑rule accuracy (percentage correct identification/application of each rule); overall solve accuracy",
            "performance": "On Minimum Sudoku: per‑rule accuracies near ≈98.6–99.0% across the six rules with best step accuracies reported (e.g., Rule1 best 99.00%); on 1M Sudoku: rules reported as 100% when applied (but later rules often unused because earlier rules solved puzzles). The system achieves &gt;98% accuracy overall on the Minimum Sudoku dataset and 100% on the 1M dataset per their reporting.",
            "analysis_findings": "REM successfully maps learned neural hidden states to classical human rules, allowing interpretable stepwise explanations; rule detection peaks around step ~32 during iterative solving; for easier datasets REM predicts only simpler rules leading to later rules not being utilized.",
            "ablation_comparison": "No ablation study reported comparing with/without REM or alternative explanation modules.",
            "limitations": "REM is constrained to the preselected six rules and therefore cannot explain or drive solving steps that require more advanced tactics; when rules are unused (e.g., on the 1M dataset) their reported '100%' accuracy reflects non‑use rather than robust detection; generalization beyond Sudoku or beyond the chosen rule set not evaluated.",
            "uuid": "e6767.1",
            "source_info": {
                "paper_title": "Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Rule‑based Sudoku step solver",
            "name_full": "Rule‑based Sudoku Step Solver (knowledge‑base updater)",
            "brief_description": "A symbolic module that, given candidate lists and predicted rules, applies classical Sudoku rules stepwise to eliminate candidates and fill cells, iterating until a step is resolved or no rule applies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Rule‑based Sudoku step solver",
            "model_description": "A deterministic, symbolic step solver that assigns initial candidates then applies rules in sequence (Naked/Hidden Single, Locked Candidates Type1/2, Naked Pair, Hidden Pair) to eliminate candidates and fill cells; it revisits rules iteratively until progress is made or no applicable rule remains.",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_type": "constraint satisfaction / combinatorial logic",
            "dataset_name": "Minimum Sudoku (Gordon Royle); 1M Sudoku (Kaggle)",
            "prompting_method": null,
            "reasoning_technique": "Classical rule‑based candidate elimination and filling (symbolic constraint propagation using the six selected human rules)",
            "internal_representation": "Per‑cell candidate lists derived from RRN softmax outputs; knowledge base updated with eliminations and assigned digits stepwise.",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Solve success rate after up to 64 steps; per‑rule application accuracies",
            "performance": "Counts and percentages reported in paper when combined with RRN+REM (see entries above); overall solving speed 20 puzzles/sec.",
            "analysis_findings": "The symbolic step solver allows clear mapping from predicted rules to concrete candidate eliminations and assignments, enabling human‑readable explanations; the design choice to stop when no rule applies limits ability to solve puzzles needing more advanced or search‑based tactics.",
            "ablation_comparison": "No explicit ablation of the step solver vs. pure learned decoding was reported.",
            "limitations": "Restricted rule set; inability to proceed when none of the six rules apply (no fallback search/backtracking); explanation quality depends on correctness of REM predictions.",
            "uuid": "e6767.2",
            "source_info": {
                "paper_title": "Rule-based explaining module: Enhancing the interpretability of recurrent relational network in Sudoku solving",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent relational networks",
            "rating": 2
        },
        {
            "paper_title": "Using small MUSes to explain how to solve pen and paper puzzles",
            "rating": 2
        },
        {
            "paper_title": "A framework for step-wise explaining how to solve constraint satisfaction problems",
            "rating": 2
        },
        {
            "paper_title": "Searching for explainable solutions in Sudoku",
            "rating": 2
        },
        {
            "paper_title": "Solving every Sudoku puzzle",
            "rating": 1
        }
    ],
    "cost": 0.010225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Rule-based Explaining Module: Enhancing the Interpretability of Recurrent Relational Network in Sudoku Solving</h1>
<p>Pimpa Cheewaprakobkit ${ }^{1,2}$, Timothy K. Shih ${ }^{2, <em>}$, Timothy Lau ${ }^{2}$, Yu-Cheng Lin ${ }^{3}$ and Chih-Yang Lin ${ }^{4}$<br>${ }^{1}$ Department of Information Technology, Asia-Pacific International University, Saraburi, Thailand<br>${ }^{2}$ Department of Computer Science and Information Engineering, National Central University, Taoyuan, Taiwan<br>${ }^{3}$ Department of Computer Science and Engineering, Yuan Ze University, Taoyuan, Taiwan<br>${ }^{4}$ Department of Mechanical Engineering, National Central University, Taoyuan, Taiwan<br></em>Corresponding author: Timothy K. Shih (timothykshih@gmail.com)</p>
<h4>Abstract</h4>
<p>Computer vision has gained significant attention in the field of information technology due to its widespread application that addresses real-world challenges, surpassing human intelligence in tasks such as image recognition, classification, natural language processing, and even game playing. Sudoku, a challenging puzzle that has captivated many people, exhibits a complexity that has attracted researchers to leverage deep learning techniques for its solution. However, the reliance on black-box neural networks has raised concerns about transparency and explainability. In response to this challenge, we present the Rule-based Explaining Module (REM), which is designed to provide explanations of the decision-making processes using Recurrent Relational Networks (RRN). Our proposed methodology is to bridge the gap between complex RRN models and human understanding by unveiling the specific rules applied by the model at each stage of the Sudoku puzzle solving process. Evaluating REM on the Minimum Sudoku dataset, we achieved an accuracy of over $98.00 \%$.</p>
<p>Keywords: rule-based explaining module, recurrent relational network, Sudoku puzzle solving, machine learning.</p>
<h2>1. Introduction</h2>
<p>Sudoku is one of the most popular intellectual puzzle games [26] that involves logical thinking to fill in numbers. It comprises a $9 \times 9$ grid, forming a numerical puzzle with nine rows and nine columns, totalling 81 cells. The grid is further divided into nine $3 \times 3$ subgrids, referred to as blocks, each containing nine cells. To initiate the game, a set of given numbers is provided as hints. These hints are placed in some of the cells of the Sudoku puzzle, providing clues to help the player solve the puzzle. In most cases, the more cells that are given, the easier the puzzle trends to be. Currently, to the best of our knowledge, the fewest clues required for a proper Sudoku puzzle is 17. This means that the most challenging Sudoku puzzles now are those with only 17 known cells. The goal is to fill the empty cells with the numbers 1 through 9 , ensuring that each number appears only once in each row, column, and block [28]. An example of a Sudoku puzzle and its solution is shown in Fig. 1.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Sudoku puzzle and its solution.</p>
<p>The rapidly evolving realm of computer vision has increasing in various aspects of our daily lives, encompassing domain such as image recognition [19], language translation [25], and critical medical applications like X-ray image analysis for disease diagnosis [10, 27], and game playing [8]. The challenging of Sudoku puzzle has attracted researchers to leverage deep learning techniques for its solution. The fascination lies not only in the puzzles' complexity but also in the diverse strategies required for their solution. Traditional rule-based methods have been prevalent, employing strategies such as elimination, naked singles, and hidden singles. The advent of deep learning has ushered in a revolution of puzzle-solving, introducing adaptive and data-driven approaches to tackle Sudoku's complexities. Despite the remarkable capabilities of deep learning, the reliance on black-box nature of neural network has raised concerns about inner workings and transparency of their decision-making processes, particularly in contexts where machine learning applications make critical decisions. Enhancing the transparency of black-box neural networks becomes particularly crucial in applications requiring abstract reasoning about objects and their interactions, enabling audiences to comprehend the rationale behind the decision process of machine learning. One direct method to achieve this transparency is through the addition of explanations [21]. Existing explanation methods for specific applications such as tracking feature extraction in image recognition to visualize the interpretation of input data [12]. Furthermore, logical methods that integrate logical reasoning into neural networks have been proposed to enhance interpretability throughout the entire process [7, 24]. However, these logical methods may face challenges in generalizing to new data or situations, often relying on hand-crafted rules or assumptions about the data. A similar approach, the expert system, is rulebased [4] but demands a substantial amount of knowledge to be encoded in its rules. This process can be time-consuming and expensive, particularly in complex domains. Macha et al. introduced RuleXAI [13], a tool designed to enhancing explainability in machine learning models. While currently limited to classification, regression, and survival analysis, RuleXAI leverages rule-based explanations and feature relevance to make</p>
<p>models more understandable. However, it may not be perfectly accurate for all model types, especially those with complex, black-box in neural network.</p>
<p>This study introduces the Rule-based Explaining Module (REM), a specialized tool designed to unveil the specific rules applied by the model at each stage of the puzzlesolving process. Therefore, the main contribution of our study is summarized as follows:</p>
<ol>
<li>We present the Rule-based Explaining Module (REM), designed to offer comprehensive, step-by-step explanations of the decision-making processes employed by Recurrent Relational Network (RRN) in Sudoku puzzle solving.</li>
<li>We conducted experiments using the Minimum Sudoku and 1 million Sudoku games datasets. The results demonstrated that our model significantly contributes to the transparency and interpretability of the Sudoku solving process.
The remainder of this paper is structured as follows: Section 2 provides a review of related work, Section 3 introduces the proposed Rule-based Explaining module for solving Sudoku, and Sections 4 and 5 present experimental results and conclusions, respectively.</li>
</ol>
<h1>2. Related works</h1>
<p>Sudoku is a wildly popular logic-based puzzle game, has captivated individuals of all ages for many years. Its deceptively simple rules and endless variations have sparked a worldwide fascination. The challenge of solving Sudoku puzzles lies in their ability to test both logical reasoning and strategic thinking, especially for more difficult puzzles that captivating players with their intricate patterns and hidden clues.</p>
<p>Over the years, various techniques have been explored for solving Sudoku puzzle. Classical methods like backtracking [20], constraint propagation [14], and genetic algorithms [11] have shed light on Sudoku solving strategies. For instance, the pencil-and-paper method, also known as the human solver approach, efficiently solves easier puzzles but faces difficulties with more challenging ones, especially in the absence of clear clues. In contrast, backtracking, though it guarantees a solution for every valid puzzle, is considerably slower [16]. Subsequently, a hybrid method for solving Sudoku puzzles, integrating traditional backtracking algorithms with pencil-and-paper techniques was introduced introduced [26]. This approach initially utilizes pencil-and-paper strategies, followed by applying backtracking to specific sub-grids, and concludes with pencil-andpaper methods on the remaining cells. This method is designed to improve puzzle-solving efficiency. However, its complexity and computational demands could be a drawback. The integration of algorithmic and intuitive strategies might lead to redundant operations and increasing the time required to solve complex Sudoku puzzles. Musliu and Winter have integrated the structured approach of constraint programming with the iterative nature of local search methods in a hybrid solution [14]. This method leverages the strengths of both: the proficiency of constraint programming in solving constraint</p>
<p>satisfaction problems and the effectiveness of iterated local search in optimization tasks. However, a primary challenge arises in balancing the systematic nature of constraint programming with the adaptive strategy of local search. This balancing act could present difficulties in efficiently finding solutions, especially in the context of complex Sudoku puzzles. Das et al. present an evolutionary algorithm that employs genetic operators, such as crossover and mutation, to generate new candidate solutions [3]. This algorithm may require extensive computational resources and time to converge on a solution. Gaddam et al. propose a method for solving Sudoku puzzles using a combination of deep learning and image processing techniques [6]. This method first utilizes image processing techniques to extract the Sudoku grid from an image and then employs a deep learning model to solve the puzzle. It demonstrates the potential of deep learning for solving Sudoku puzzles. However, the accuracy of this method is dependent on the quality of the input image. If the image is blurry or distorted, the accuracy of the deep learning model may be compromised. These techniques, while laying the foundation for understanding the problem, often lacked the flexibility, solution explanation, and adaptability needed to tackle complex puzzles. Björnsson et al., introduced a search-based approach to generate explainable solutions to Sudoku puzzles [1]. This method involves modelling the perceived human mental effort of using different familiar Sudoku-solving techniques. This model serves as guidance for a search algorithm to identify the correct solutions and present them in a way that is easily understandable to human solvers. However, the method's dependence on a potentially inaccurate model of human mental effort could result in explanations that are not entirely accurate. Another approach, Demystify, introduced by Espasa et al., provides step-by-step explanations for solving various pen-and-paper puzzles, including Sudoku [5]. It utilizes Minimal Unsatisfiable Subsets (MUSes) to solve puzzles through logical deduction, identifying essential puzzle components for progress. While Demystify effectively explains puzzle solutions, it requires human input in the form of high-level logical descriptions. Additionally, its applicability may not be suitable for solving all types of pen-and-paper puzzles. Bogaerts et al. provide step-by-step explanations for constraint satisfaction problems (CSPs), focusing on logic grid puzzles as a specific instance of CSPs [2]. They propose a framework for generating step-wise explanations of the inference steps taken during puzzle-solving. However, this approach is particularly reliant on the availability of a formal rule representation for the CSP domain. Without a well-defined set of rules, the framework may struggle to generate meaningful explanations.</p>
<p>The introduction of neural networks, particularly Recurrent Relational Networks (RRNs), revolutionized the landscape of Sudoku solving. RRNs [17], a type of neural network well-suited for learning long-range dependencies in data, proved adept at capturing the intricate relationships between cells in a Sudoku grid. While RRN's capability to learn from large datasets of Sudoku puzzles enabled them to achieve remarkable accuracy, consistently outperforming classical approaches, their black-box nature makes</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2. Overview of the framework.
it challenging to comprehend their decision-making processes. Palm et al. introduced an RRN-based Sudoku solver that utilized a convolutional neural network (CNN) for feature extraction and an RRN for capturing relational dependencies [17]. However, the adoption of neural networks in Sudoku solving has raised concerns about transparency and interpretability, prompting the exploration of explain ability modules.</p>
<p>To address the lack of transparency in RRN-based Sudoku solving, we leveraged rulebased explanation techniques [21], inspired by prior research demonstrating their high accuracy. This integration enhances transparency and interpretability by generating human-readable rules that unveil the model's decision-making process, these rules offer a more accessible way to understand the process compared to examining the raw model parameters.</p>
<h1>3. Proposed method</h1>
<p>Our proposed architecture incorporates the Recurrent Relational Network (RRN) [17] and Rule-based Explaining Module (REM), aiming to provide comprehensive, step-bystep explanations of the decision-making procedures in the context of Sudoku puzzle solving as shown in Fig. 2. The process begins by inputting an unsolved Sudoku puz-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3. A Recurrent Relational Network structure with 3 nodes.
zle into the backbone network, the Recurrent Relational Network (RRN). Each input node represents a feature vector (orange circles) corresponding to an individual Sudoku cell. Subsequently, a multi-layer perceptron (MLP) captures patterns and dependencies within the data. Recurrent computation updates all relevant information for each hidden state (green circles). The RRN then outputs probabilities for digits, representing the possible candidate digits for each cell. Following this, all hidden states (green circles) from the RRN are forwarded to the Rule-based Explaining Module (REM) using a multi-layer perceptron. This perceptron maps the hidden states to rules and outputs the probability of selected rules, offering explanations for the decision-making process. Finally, the Rule-based Sudoku step solver module receives output from both the RRN and REM modules. It identifies conditions that trigger specific rules and updates the knowledge base accordingly. This iterative process continues until the Sudoku puzzle is solved, ultimately providing both the solution and explanations for the decision-making steps involved.</p>
<h1>3.1. Recurrent relational network (RRN)</h1>
<p>RNN is a type of artificial neural network designed to capture long-range dependencies in sequential data. It is able to do this by learning to pass messages between nodes in a graph, which represents the relationships between the elements of the data. RRN is a powerful tool for a variety of tasks, including natural language processing, machine translation, and question answering. The RRN backbone consists of four main components, which are data input, message passing, node hidden state, and the output result as shown in Fig. 3.</p>
<h1>3.1.1. Data input</h1>
<p>In the context of Sudoku puzzles, there are input nodes, denoted as $i=1,2, \ldots, 81$, each corresponding to a cell in the Sudoku grid. Each node $i$ possesses an input $x_{i}$, representing the feature vector at that specific node.</p>
<h3>3.1.2. Message passing</h3>
<p>Message passing is responsible for communicating information between nodes. Each node sends a message to each of its neighbours at each iteration of the RRN. The message is a vector of numbers calculated based on the node's current state and its relationship to its neighbour. At each time step $t$, each node processes a hidden state vector $h_{i}^{t}$. During this process, each node sends a message $m_{i j}^{t}$ from node $i$ to node $j$ at time step $t$, where node $j$ represents a neighbouring node, utilizing message function $f$ as illustrated in formula (1)</p>
<p>$$
m_{i j}^{t}=f\left(h_{i}^{t-1}, h_{j}^{t-1}\right)
$$</p>
<p>In Recurrent Relational Network (RRN), the message function $f$ is implemented as a multiple-layer perceptron (MLP), enabling the network to learn the most effective types of messages to send for each situation. To incorporate all relevant information, each node must process all incoming messages, which are then summed together using formula (2). The combination of the MLP and the summation of messages enables RRN to learn complex patterns of communication and information exchange, making them powerful tools for solving tasks that require relational reasoning:</p>
<p>$$
m_{j}^{t}=\sum_{i \in N(j)} m_{i j}^{t}
$$</p>
<p>where $N(j)$ represents all neighbouring nodes of node $j$, comprising nodes in the same row, column, and block as node $j$. Consequently, messages are currently computed for each node, allowing the model to progress to the next step in updating the network.</p>
<h3>3.1.3. Recurrent nodes updates</h3>
<p>Recurrent nodes are responsible for storing and updating the network's state, which represents the network's current understanding of the data. The state of a recurrent node is updated based on the messages it receives from its neighbours and the node's own internal state. The updates of recurrent nodes are key to the RRNs' ability to learn long-range dependencies in data. By repeatedly updating the state of each node based on the received messages, RRN can learn to capture the relationships between elements of the data that are separated by long distances in the input sequence. The formula for updating the state of a recurrent node is illustrated in (3):</p>
<p>$$
h_{j}^{t}=g\left(h_{j}^{t-1}, x_{j}, m_{j}^{t}\right)
$$</p>
<p>where $g$ represents the node update function, functioning as a multiple-layer perceptron taking as input the hidden state from the previous iteration $h_{i}^{t-1}$, the feature vector of</p>
<p>input information $x_{j}$, and the message $m_{i j}^{t}$, the $g$ function is trained to execute updates for the hidden state.</p>
<h1>3.1.4. The output</h1>
<p>After updating the hidden state, we can obtain the output at step $t$ for node $i$ by applying formula (4):</p>
<p>$$
O_{i}^{t}=k\left(h_{i}^{t}\right)
$$</p>
<p>where $k$ denotes the output function, a multiple-layer perceptron trained to decode the hidden state into the output digit for the Sudoku. It converts the hidden state into output probabilities for a total of 10 different digits using the softmax function. The cross-entropy loss function, defined in formula (5), is used to optimize the model's performance during training. The target digits, represented by $y=y_{1}, y_{2}, y_{3}, \ldots, y_{81}$ denote the correct digit at position $i$ at step $t$.</p>
<p>$$
l^{t}=-\sum_{i=1}^{I} \log O_{i}^{t}\left[y_{i}\right]
$$</p>
<h3>3.2. Rule-based explaining module (REM)</h3>
<p>Rule-based explaining is a technique in artificial intelligence employed to explain the reasoning behind decision-making by identifying the conditions that triggered specific rules and the conclusions reached by those rules. In our backbone network utilizing RRN, the message passing in the RRN network encompasses valuable information, including node relationships with its neighbours, which is highly valuable for examination. To extract the explanations from the message, we incorporate a multiple-layer perceptron that learns the rules from the hidden state after message passing and recurrent updating, as defined by formula (6).</p>
<p>$$
R_{i}^{t}=r\left(h^{t}\right)
$$</p>
<p>where $R_{i}^{t}$ represents the output of the selected rules used to solve the Sudoku puzzle at step $t$. The hidden states $h^{t}$ encompasses all of the RRN graph's hidden states, and the function $r$ is a multiple-layer perceptron that maps the hidden state after message passing to rules at step $t$. The variable $i$ represents the number of rules, where $i=1,2, \ldots, n$. The selection of rules is guided by the tasks at hand. For Sudoku puzzles, we employ rules proposed by Hobiger [9] and Riley [22]. From their set of rules, we selected six rules for our experiments as they effectively solve the majority of the Sudoku puzzles in our dataset. Sudoku solving is divided into steps, with each step corresponding to filling in a single digit in the puzzle. Typically, multiple rules can be employed to determine a single digit. Consequently, the rule identification process generates more than one rule at each step. In this scenario, each Sudoku solving step can yield up to six different rule outputs.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4. An example of Hidden Single.</p>
<h1>3.3. Sudoku solving rules</h1>
<p>We selected six rules from the rules proposed by Hobiger [9] and Riley [22]. These rules include Hidden Single, Naked Single, Locked Candidates Type 1, Locked Candidates Type 2, Naked Pair, and Hidden Pair. Although there are numerous rules beyond the six we chose, our decision was based on the observation that these specific rules already successfully solved over $98.00 \%$ of the most challenging Sudoku puzzles. In our study, our primary focus is on explaining the Sudoku solving process rather than improving accuracy. As such, we believe that utilizing these six rules is sufficient for our purposes.</p>
<h3>3.3.1. Rule 1: Hidden single</h3>
<p>A Hidden Single occurs when there is only one possible candidate number for a cell within a row, column, or $3 \times 3$ block, but that candidate number does not appear in any other cell within that row, column, or block. An example of the hidden single rule is presented in Fig. 4.</p>
<p>Examining row 3 (r3) in Fig. 4, it becomes evident that the cell at row 3, column 4 (r3c4) marked with a green 6 , is the sole occurrence of the digit 6 within row 3 . Consequently, we can confidently assign the digit 6 to cell r3c4 by eliminating other digit candidates.</p>
<h3>3.3.2. Rule 2: Naked single</h3>
<p>A Naked Single occurs when there is only one possible candidate number in a row, column, or $3 \times 3$ block that can contain a specific digit. An example of the naked single rule is presented in Fig. 5.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5. An example of Naked Single.</p>
<p>Examining the cell at r6c7 in Fig. 5, it has only one possible digit candidate, which is 6 . Therefore, we can assign the digit 6 to that cell.</p>
<h1>3.3.3. Rule 3: Locked candidates Type 1</h1>
<p>The third and fourth rules are more advanced compared to the first two. They employ an indirect method for eliminating potential candidates from a cell. In fact, all rules, except for the first two, are utilized to eliminate possible candidates, eventually leading to the condition where the first two rules can be applied to fill in a digit and complete a step. Locked Candidates Type 1 occurs when all candidates of a specific digit within a block are confined to a row or column, that digit cannot appear outside of that block in that row or column. Fig. 6 illustrates an example of Locked Candidates Type 1.</p>
<p>Observing block 1 (b1) in Fig. 6, digit 5 only appears in row 3 (r3). Consequently, there should not be another instance of digit 5 in row 3 outside of block 1 . Therefore, the candidate 5 in cell r 3 c 7 can be eliminated.</p>
<h3>3.3.4. Rule 4: Locked candidates Type 2</h3>
<p>Locked Candidates Type 2 is the opposite of Locked Candidates Type 1. It occurs when, in a row (or column), all candidates of a specific digit are confined to one block, allowing the elimination of that candidate from all other cells in that block. Fig. 7 provides an example of Locked Candidates Type 2.</p>
<p>Examining row 2 (r2) in Fig. 7, it is observed that all candidate positions for the digit 7 appear only within block 1 (b1). Consequently, the digit 7 must be present in</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6. An example of Locked Candidates Type 1.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 7. An example of Locked Candidates Type 2.
row 2 within block 1 . As a result, the digit 7 candidates located outside row 2 in block 1 can be eliminated.</p>
<h1>3.3.5. Rule 5: Naked pair</h1>
<p>A Naked Pair occurs when there are exactly two candidate numbers for a cell within a row, column, or $3 \times 3$ block, and those two candidate numbers also appear together in</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Fig. 8. An example of Naked Pair.
another cell within the same row, column, or block. This means that those two candidate numbers must be in these two cells, and cannot be appear elsewhere in that row, column, or block. Fig. 8 provides an example of naked pair.</p>
<p>Examining row 8 (r8), candidates 3 and 9 form a pair within a cell. Consequently, the candidate 3 in cell r 8 c 2 , (row 8 in this case), can be eliminated</p>
<h1>3.3.6. Rule 6: Hidden pair</h1>
<p>A Hidden Pair occurs when there are two candidate numbers for a cell within a row, column, or $3 \times 3$ block, and these two candidate numbers also appear together in another cell within that same row, column, or block. However, that other cell is already filled with another number. Consequently, all other candidates in those two cells can be eliminated. Fig. 9 provides an example of hidden pair.</p>
<p>Examining column 9 (c9), we observe a pair of candidate digits, 1 and 9, located in cells row 5 column 9 (r5c9) and row 7 column 9 (r7c9). Since 1 and 9 must occupy either of these two cells, any other candidate digits, such as the possible candidate 6 in r5c9, can be eliminated.</p>
<h3>3.4. Rule-based sudoku step solver</h3>
<p>Many Sudoku solving programs commonly eliminate candidates based on the given puzzle and search through all possibilities to identify the correct candidate digit. In contrast, our Sudoku solver takes a distinct approach. It solves Sudoku in a stepwise, rule-based, using specific rules in each action. The flow chart depicting our approach is presented</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Fig. 9. An example of Hidden Pair.
in Fig. 10. The initial step involves assigning possible candidate digits to each cell in the provided Sudoku puzzle. Subsequently, we apply Sudoku rules by examining all potential candidate digits to identify any patterns that conform the established rules. As mentioned in the previous section, the first two rules form the foundation for solving Sudoku puzzles, and we can observe that the Sudoku puzzle can be solved by applying these two rules alone. Following the assessment of the first two rules, the remaining rules are examined one by one to determine if any patterns meet the criteria of each rule. If a pattern satisfying a rule is discovered, we revisit all the rules to ensure no other patterns exist. This process iterates until the step is resolved by either the first or second rule. Conversely, if no pattern satisfying the rule is found, that step cannot be solved, and the solver will cease attempting to solve it. In other words, the Sudoku puzzle cannot be solved within the framework of these six rules.</p>
<h1>4. Experimental results</h1>
<p>We conducted our experiments using a DGX station with a Nvidia V100 GPU with 32 GB of GPU RAM, employing a batch size of 128 and a learning rate of $2 \mathrm{e}-5$. The training process involved 32 steps because the model stabilized at this step and took 5 days. The total number of trainable parameters was 518006 . For testing, we used both the Minimum Sudoku dataset from Gordon Royle and 1 million Sudoku games (1M Sudoku) dataset [18]. Additionally, the Minimum Sudoku dataset [23] was used for training, divided into an $80 \%$ training set, a $10 \%$ validation set, and a $10 \%$ testing set.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Fig. 10. Flow chart of the Rule-based Sudoku step solver.</p>
<p>Our method demonstrated the ability to solve Sudoku puzzles at a speed of 20 puzzles per second.</p>
<h1>4.1. The Minimum Sudoku dataset from Gordon Royle</h1>
<p>The Minimum Sudoku dataset [23] comprises 49151 puzzles, each assigned the difficulty level of 17 given numbers. These puzzles are generated using a backtracking algorithm and is guaranteed to have a unique solution. Leveraging the complexity of these Sudoku puzzles with 17 given numbers during training enables our model to effectively handle a wide range of puzzles, from easy to difficulty levels.</p>
<p>We investigated the model's ability to apply six Sudoku-solving rules independently: Naked Single, Hidden Single, Locked Candidate Type 1, Locked Candidate Type 2, Naked Pair, and Hidden Pair. By testing the model on the Minimum Sudoku dataset,</p>
<p>we aimed to determine how effectively the model can apply each rule to solve Sudoku puzzles by applying 64 steps. The results are correctly verified using the Norvig Sudoku solver [15].</p>
<p>The results of solving the Sudoku puzzle are presented in Tab. 1. This table illustrates that, initially, the model assigns possible number candidates for the puzzle, enabling Sudoku solution with an accuracy of $72.32 \%$. Subsequently, the model employs Rule 1, achieving a Sudoku accuracy of $99.00 \%$. For cells in Sudoku puzzles that remain unsolved after Rule 1, the model applies Rule 2, achieving an accuracy of $98.98 \%$. If any cells persistently resist resolution with Rule 2, the model turns to Rule 3, and so forth, up to Rule 6. The accuracy rates for Rules 3 to 6 are $98.62 \%, 98.60 \%, 98.79 \%$, and $98.67 \%$, respectively.</p>
<h1>4.2. The 1 million Sudoku games (1M Sudoku) dataset</h1>
<p>The 1M Sudoku dataset [18], available on Kaggle was developed by Kyubong Park. Using a computer program, he generated over a million Sudoku puzzles with their corresponding solutions. The dataset encompasses a variety of difficulty levels, ranging from easy to challenging. While several factors can influence a Sudoku puzzle's difficulty, such as the pattern of given cells, the puzzle's symmetry, and the existence of hidden singles or doubles, the number of given cells is a crucial factor. Sudoku puzzles with the fewest given cells are generally considered to be more difficult. The majority of puzzles in this dataset are of medium difficulty. We conducted experiments with our model using the 1M Sudoku dataset, performing 64 steps on each puzzle.</p>
<p>The results of solving these Sudoku puzzles are presented in Tab. 2. The contents of this table demonstrates that, in the initial stage, the model assigns possible number candidates for the Sudoku puzzle, enabling a $95.41 \%$ success rate in solving Sudoku puzzles. Subsequently, the model employs Rule 1, achieving a perfect accuracy of $100 \%$. For any remaining unsolved cells after applying Rule 1, the model employs Rule 2, also achieving a perfect accuracy of $100 \%$. Since Rule 2 successfully solves all remaining</p>
<p>Tab. 1. The results of solving the Sudoku puzzle on the Minimum Sudoku dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">Rules used</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">72.32</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;">Model assigns possible numbers</td>
</tr>
<tr>
<td style="text-align: center;">99.00</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Naked Single</td>
</tr>
<tr>
<td style="text-align: center;">98.98</td>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Hidden Single</td>
</tr>
<tr>
<td style="text-align: center;">98.62</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Locked Candidate Type 1</td>
</tr>
<tr>
<td style="text-align: center;">98.60</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Locked Candidate Type 2</td>
</tr>
<tr>
<td style="text-align: center;">98.79</td>
<td style="text-align: center;">5</td>
<td style="text-align: left;">Naked Pair</td>
</tr>
<tr>
<td style="text-align: center;">98.67</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">Hidden Pair</td>
</tr>
</tbody>
</table>
<p>Tab. 2. The results of solving the Sudoku puzzle on the 1 million Sudoku games dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;">Rules used</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">95.41</td>
<td style="text-align: center;">-</td>
<td style="text-align: left;">Model assigns possible numbers</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Naked Single</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Hidden Single</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Locked Candidate Type 1</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Locked Candidate Type 2</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">5</td>
<td style="text-align: left;">Naked Pair</td>
</tr>
<tr>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">6</td>
<td style="text-align: left;">Hidden Pair</td>
</tr>
</tbody>
</table>
<p>cells, Rules 3 to 6 become unnecessary. Therefore, Rules 3 to 6 consistently exhibit $100 \%$ accuracy when applied. Refer to Tab. 4 for more details. Since our model was trained on the Minimum Sudoku dataset, renowned for its difficulty, it excels in solving Sudoku puzzles, achieving an outstanding $100 \%$ success rate.</p>
<h1>4.3. Rule-based explanation</h1>
<p>Our rule-based explaining module allows us to understand how the RRN model solves Sudoku puzzles by breaking it down step by step based on established rules and inferences. This is demonstrated through examples, such as inputting a Sudoku puzzle from the Minimum Sudoku dataset, where each cell's candidate number represents the probability of it being the correct answer. Fig. 11 depict a graph showcasing solving accuracy at various steps ranging from 0 to 60 using the Minimum Sudoku dataset. Additionally, Fig. 12 displays a graph representing rule accuracy employed at different steps with the same dataset.</p>
<p>Tab. 3 provides a comprehensive analysis and interpretation of the rule accuracy applied at various steps in the Sudoku-solving process. In the initial stage, the model achieved its highest accuracy near step 32. The model employed rules 1 through 6 to solve the puzzle. This indicates that the Sudoku puzzle is significantly complex, requiring the use of more than two rules to achieve a solution.</p>
<p>In another instance, we utilized input data from the 1M Sudoku dataset. As depicted in Fig. 13, a graph illustrates solving accuracy at various steps, ranging from 0 to 60 . Fig. 14 presents a graph illustrating rule accuracy at different steps, with the model achieving its highest accuracy around step 32. Accompanying these figures is Tab. 4, where rules 1 and 2 were employed to solve the puzzle, achieving $100 \%$ accuracy. This observation sheds light on why rules 3 to 6 consistently show $100 \%$ accuracy. The reason behind this is that the model does not anticipate the utilization of rules 3 to 6 , resulting in their consistent correctness as unused rules.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Fig. 11. A graph depicting solving accuracy at different steps using the Minimum Sudoku dataset.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Fig. 12. Illustrating rule accuracy at different steps using the Minimum Sudoku dataset.</p>
<h1>5. Conclusion</h1>
<p>In this paper, we address concerns regarding the transparency and interpretability of machine learning applications, especially in critical decision-making domains. The opacity of neural networks, often labelled as black-boxes, has raised questions, particularly in Sudoku puzzle-solving scenarios. To tackle this challenge, we introduced the Rulebased Explaining Module (REM) as a tool to understand the complex decision-making processes of RRN during Sudoku puzzle-solving. While our REM has shown promise, there are opportunities for further exploration and improvement. Future research could explore broader applications of REM across diverse datasets. Additionally, extending</p>
<p>Tab. 3. The analysis and interpretation of rule accuracies across different steps in Minimum Sudoku dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Steps (1-64)</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">16</th>
<th style="text-align: center;">32</th>
<th style="text-align: center;">48</th>
<th style="text-align: center;">64</th>
<th style="text-align: center;">Best Step/ <br> accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model assigns <br> possible numbers</td>
<td style="text-align: center;">9.71</td>
<td style="text-align: center;">71.91</td>
<td style="text-align: center;">$\mathbf{7 2 . 1 7}$</td>
<td style="text-align: center;">71.19</td>
<td style="text-align: center;">69.66</td>
<td style="text-align: center;">$24 / \mathbf{7 2 . 3 2}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 1</td>
<td style="text-align: center;">98.89</td>
<td style="text-align: center;">98.96</td>
<td style="text-align: center;">$\mathbf{9 8 . 9 8}$</td>
<td style="text-align: center;">98.94</td>
<td style="text-align: center;">98.88</td>
<td style="text-align: center;">$6 / \mathbf{9 9 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 2</td>
<td style="text-align: center;">98.89</td>
<td style="text-align: center;">98.92</td>
<td style="text-align: center;">$\mathbf{9 8 . 9 7}$</td>
<td style="text-align: center;">98.96</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">$21 / \mathbf{9 8 . 9 8}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 3</td>
<td style="text-align: center;">98.18</td>
<td style="text-align: center;">98.54</td>
<td style="text-align: center;">$\mathbf{9 8 . 5 6}$</td>
<td style="text-align: center;">98.51</td>
<td style="text-align: center;">98.37</td>
<td style="text-align: center;">$20 / \mathbf{9 8 . 6 2}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 4</td>
<td style="text-align: center;">98.25</td>
<td style="text-align: center;">98.54</td>
<td style="text-align: center;">$\mathbf{9 8 . 5 6}$</td>
<td style="text-align: center;">98.55</td>
<td style="text-align: center;">98.41</td>
<td style="text-align: center;">$30 / \mathbf{9 8 . 6 0}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 5</td>
<td style="text-align: center;">98.63</td>
<td style="text-align: center;">98.72</td>
<td style="text-align: center;">98.76</td>
<td style="text-align: center;">$\mathbf{9 8 . 7 9}$</td>
<td style="text-align: center;">98.69</td>
<td style="text-align: center;">$40 / \mathbf{9 8 . 7 9}$</td>
</tr>
<tr>
<td style="text-align: center;">Rule 6</td>
<td style="text-align: center;">98.72</td>
<td style="text-align: center;">98.60</td>
<td style="text-align: center;">$\mathbf{9 8 . 6 3}$</td>
<td style="text-align: center;">98.52</td>
<td style="text-align: center;">98.44</td>
<td style="text-align: center;">$26 / \mathbf{9 8 . 6 7}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Fig. 13. An example of a graph depicting solving accuracy at different steps.
our approach to other puzzle types or complex decision-making tasks. The development of user-friendly interfaces and visualization techniques could facilitate the practical implementation of REM in real-world scenarios. This work represents a significant step in addressing transparency challenges posed by neural networks, offering a concrete solution in the form of the Rule-based Explaining Module. The success of our proposed method not only contributes to the field of explainable artificial intelligence but also paves the way for broader applications in various domains requiring interpretable decision-making.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Fig. 14. Illustrating rule accuracy at different steps.</p>
<p>Tab. 4. An example of the analysis and interpretation of rule accuracies across different steps in the 1 M Sudoku games dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Steps (1-64)</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">16</th>
<th style="text-align: center;">32</th>
<th style="text-align: center;">48</th>
<th style="text-align: center;">64</th>
<th style="text-align: left;">Best Step/ <br> accuracy [\%]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model assigns <br> possible numbers</td>
<td style="text-align: center;">14.26</td>
<td style="text-align: center;">94.95</td>
<td style="text-align: center;">$\mathbf{9 5 . 3 4}$</td>
<td style="text-align: center;">95.07</td>
<td style="text-align: center;">94.29</td>
<td style="text-align: left;">$34 / \mathbf{9 5 . 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 1</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0 0}$</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: left;">$1 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 2</td>
<td style="text-align: center;">$\mathbf{1 0 0 . 0 0}$</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: left;">$1 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 3</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: left;">$0 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 4</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: left;">$0 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 5</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: left;">$0 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Rule 6</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: left;">$0 / \mathbf{1 0 0 . 0 0}$</td>
</tr>
</tbody>
</table>
<h1>References</h1>
<p>[1] Y. Björnsson, S. Helgason, and A. Pálsson. Searching for explainable solutions in Sudoku. In: 2021 IEEE Conference on Games (CoG), pp. 1-8. Copenhagen, Denmark, 17-20 Aug 2021. doi:10.1109/CoG52621.2021.9618900.
[2] B. Bogaerts, E. Gamba, and T. Guns. A framework for step-wise explaining how to solve constraint satisfaction problems. Artificial Intelligence, 300:103550, 2020. doi:10.1016/j.artint.2021.103550.
[3] K. N. Das, S. K. Bhatia, S. Puri, and K. Deep. Solving Sudoku puzzle by evolutionary algorithm. In: Proc. 21st Asian Technology Conference in Mathematics. Mathematics and Technology, LLC, Pattaya, Thailand, 14-18 Dec 2016. https://atcm.mathandtech.org/EP2016/contributed/4052016_ 21261.pdf.
[4] G. D. Engin, B. Aksoyer, M. Avdagic, D. Bozanli, U. Hanay, et al. Rule-based expert</p>
<p>systems for supporting university students. Procedia Computer Science, 31:22-31, 2014. doi:https://doi.org/10.1016/j.procs.2014.05.241.
[5] J. Espasa, I. P. Gent, R. Hoffmann, C. Jefferson, and A. M. Lynch. Using small MUSes to explain how to solve pen and paper puzzles. ArXiv, 2021. ArXiv.2104.15040. doi:10.48550/arXiv.2104.15040.
[6] D. K. R. Gaddam, M. D. Ansari, and S. Vuppala. On Sudoku problem using deep learning and image processing technique. In: Proc. 3rd Int. Conf. Communications and Cyber Physical Engineering (ICCCE) 2020, vol. 698 of Lecture Notes in Electrical Engineering, pp. 1405-1417, 2020. doi:10.1007/978-981-15-7961-5_128.
[7] O. Gerasimova, N. Severin, and I. Makarov. Comparative analysis of logic reasoning and graph neural networks for ontology-mediated query answering with a covering axiom. IEEE Access, 11:88074-88086, 2023. doi:10.1109/ACCESS.2023.3305272.
[8] T. Guns, E. Gamba, M. Mulamba, I. Bleukx, S. Berden, et al. Sudoku assistant - an AI-powered app to help solve pen-and-paper Sudokus. In: Proc. AAAI Conference on Artificial Intelligence, p. 16440-16442. AAAI Press, 2023. doi:10.1609/aaai.v37i13.27072.
[9] B. Hobiger. Sudoku for Java - HoDoKu, 2013. https://sourceforge.net/projects/hodoku/, [Accessed: 15 Oct, 2023].
[10] A. Hussain, S. U. Amin, H. Lee, A. Khan, N. F. Khan, et al. An automated chest X-ray image analysis for Covid-19 and pneumonia diagnosis using deep ensemble strategy. IEEE Access, 11:97207-97220, 2023. doi:10.1109/ACCESS.2023.3312533.
[11] B. Indriyono, N. Pamungkas, Z. Pratama, E. Mintorini, I. Dimentieva, et al. Comparative analysis of the performance testing results of the backtracking and genetics algorithm in solving Sudoku games. International Journal of Artificial Intelligence and Robotics, 5(1):29-35, 2023. doi:10.25139/ijair.v5i1.6501.
[12] P. Linardatos, V. Papastefanopoulos, and S. B. Kotsiantis. Explainable AI: A review of machine learning interpretability methods. Entropy, 23(1):18, 2021. doi:10.3390/e23010018.
[13] D. Macha, M. Kozielski, Ł. Wróbel, and M. Sikora. RuleXAI—A package for rule-based explanations of machine learning model. SoftwareX, 20:101209, 2022. doi:10.1016/j.softx.2022.101209.
[14] N. Musliu and F. Winter. A hybrid approach for the Sudoku problem: Using constraint programming in iterated local search. IEEE Intelligent Systems, 32(2):52-62, 2017. doi:10.1109/MIS.2017.29.
[15] P. Norvig. Solving every Sudoku puzzle, 15 Jan 2012. https://norvig.com/sudoku.html, [Accessed: 15 Oct, 2023].
[16] E. Onokpasa, D. Bisandu, and D. Bakwa. A hybrid backtracking and pencil and paper Sudoku solver. International Journal of Artificial Intelligence and Robotics, 181(47):39-43, 2019. https: //dspace.unijos.edu.ng/jspui/handle/123456789/2769.
[17] R. B. Palm, U. Paquet, and O. Winther. Recurrent relational networks. In: Advances in Neural Information Processing Systems 31 - Proc. 32nd Int. Conf. Neural Information Processing Systems (NeurIPS) 2018, vol. 31 of NIPS'18, p. 3372-3382, 2018. https://proceedings.neurips.cc/paper/ 2018/hash/b9f94c77652c9a76fc8a442748cd54bd-Abstract.html.
[18] K. Park. 1 million Sudoku games, 2017. https://www.kaggle.com/datasets/bryanpark/sudoku, [Accessed: 15 Oct, 2023].
[19] X. Pengcheng, H. Zhenlin, Z. Liuqi, W. Ning, Z. Hanghang, et al. A realtime image recognition method of Power AI based on quadtree algorithm. In: Proc. 2023 2nd Int. Conf. Innovation in Technology (INOCON), pp. 1-6. Bangalore, India, 3-5 Mar 2023. doi:10.1109/INOCON57975.2023.10101145.</p>
<p>[20] M. Prabha, S. Radha, P. M. Priya, and B. S. Dhivya. Sudoku solver using minigrid based backtracking algorithm. International Journal of Research in Engineering, Science and Management, 5(6):138-140, 2022. https://journal.ijresm.com/index.php/ijresm/article/view/2180.
[21] G. P. Reddy and Y. V. P. Kumar. Explainable AI (XAI): Explained. In: Proc. 2023 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream), pp. 1-6. Vilnius, Lithuania, 27-27 Apr 2023. doi:10.1109/eStream59056.2023.10134984.
[22] G. Riley. CLIPS rule based programming language code, jan 2016. https://sourceforge.net/p/ clipsrules/code/HEAD/tree/branches/63x/examples/sudoku/, [Accessed: 15 Oct, 2023].
[23] G. Royle. Good at Sudoku? Here's some you'll never complete. The Conversation, 12 Feb 2012. [Accessed: 15 Oct, 2023]. https://theconversation.com/ good-at-sudoku-heres-some-youll-never-complete-5234.
[24] S. Shi, H. Chen, W. Ma, J. Mao, M. Zhang, et al. Neural logic reasoning. In: Proc. 29th ACM Int. Conf. Information $\mathcal{E}$ Knowledge Management (CIKM '20), p. 1365-1374. Boise, ID, USA, 21-25 Oct 2020. doi:10.1145/3340531.3411949.
[25] Y. Singh, P. Kumar, S. Goel, P. Garg, T. Srivastava, et al. Anuvadak: Language system using machine learning techniques. In: Proc. 2023 Int. Conf. Artificial Intelligence and Smart Communication (AISC), pp. 742-745. Greater Noida, India, 27-29 Jan 2023. doi:10.1109/AISC56616.2023.10085373.
[26] A. A. Suha Binta Wadud and M. Abdullah-Al-Wadud. An improved hybrid method combining backtracking with pencil and paper for solving sudoku puzzles. In: Proc. Int. Symp. Electrical, Electronics and Information Engineering (ISEEIE) 2021, p. 438-441. Association for Computing Machinery, Seoul, Republic of Korea, 19-21 Feb 2021. doi:10.1145/3459104.3459176.
[27] Y. Tian. Artificial intelligence image recognition method based on convolutional neural network algorithm. IEEE Access, 8:125731-125744, 2020. doi:10.1109/ACCESS.2020.3006097.
[28] P.-S. T. P.-S. Tsai, T.-F. W. P.-S. Tsai, J.-Y. C. T.-F. Wu, and J.-F. H. J.-Y. Chen. Integrating of image processing and number recognition in Sudoku puzzle cards digitation. Journal of Internet Technology, 23(7):1573-1584, 2022. doi:10.53106/160792642022122307012.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1362 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1362</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1362</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-ac0a9ced9c704649326fcb68cba2f904e2c6fb1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ac0a9ced9c704649326fcb68cba2f904e2c6fb1d" target="_blank">Bayesian Relational Memory for Semantic Visual Navigation</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> A new memory architecture, Bayesian Relational Memory (BRM), is introduced to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards.</p>
                <p><strong>Paper Abstract:</strong> We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goal-conditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1362.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1362.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Relational Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic relation-graph memory over semantic concepts (room types) that encodes priors from training environments, updates edge posteriors online via Bayes rule from noisy semantic observations, and produces sub-goals for a goal-conditioned locomotion policy via graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>House3D (RoomNav task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A realistic indoor 3D environment (House3D) used for semantic visual navigation where the agent receives first-person RGB observations and must navigate to a target room type (kitchen, living room, dining room, bedroom, bathroom, office, garage, outdoor). Domain: household indoor navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>No explicit door/lock constraints are modelled in BRM or the RoomNav task in this paper; connectivity is encoded as probabilistic "close-by" relations between semantic concepts rather than door conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Probabilistic undirected relation graph over semantic nodes where each pair (i,j) has a Bernoulli random variable indicating a "close-by" relation; connectivity is sparse/structured (learned priors make some pairs likely and others unlikely) rather than fully connected. The graph is updated online to produce posterior edge probabilities for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>K = 8 pre-selected semantic room types (kitchen, living room, dining room, bedroom, bathroom, office, garage, outdoor); BRM adds an 'unknown' node so effectively K+1 = 9 nodes for the module; potential relations: K(K-1)/2 = 28 unordered room-pair relations (probabilistic).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BRM agent (Bayesian Relational Memory + goal-conditioned LSTM locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>High-level BRM module: probabilistic relation-graph over semantic concepts with learned priors ψ^{prior} and observation noise ψ^{obs}; updates posteriors P(z_{i,j}|Y) via Bayes rule from bit-OR aggregated semantic detections over N-step windows and plans by maximizing joint belief along a concept path; Low-level: goal-conditioned LSTM locomotion policies (behavioral approach with a separate sub-policy per semantic target) trained with actor-critic (A2C-like) and reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success weighted by Path Length (SPL), success rate (%), and steps-to-goal / average successful episode length (steps).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>H=1000 overall: 41.1% success rate and 57.5‰ SPL (per-mille) for BRM (Table 1); H=300 overall: 23.1% success rate and 45.3‰ SPL. Average successful episode length (H=1000) for BRM: 337.0 steps (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>H=1000 overall: 41.1%; H=300 overall: 23.1% (see detailed per plan-distance values in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Planning-based hierarchical policy (memory/graph-based high-level controller + goal-conditioned local locomotion) performs best; BRM explicitly uses planning on learned probabilistic topology.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Performance correlates with topological planning distance: as the ground-truth shortest sequence of semantic sub-goals ('plan-dist') increases, success rates decrease for all agents, but BRM degrades less. Learned priors over relations improve navigation (higher success and SPL) compared to an uninformative prior; more frequent replanning (smaller N relative to oracle nearby step length ~12.27) improves performance. Errors are dominated by locomotion quality rather than planning accuracy: with an oracle locomotion, BRM approaches optimal planner performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper evaluates performance grouped by 'plan-distance' (1..5: the shortest number of sub-goal hops in the ground-truth relation graph). Results show (Table 1): BRM outperforms baselines at all plan-distances, and the relative advantage of BRM increases with longer horizons (more planning computations). Learned priors help most when horizon is limited; with long horizons the graph posterior converges and the prior matters less.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Hierarchical, memory-and-planning-based controllers (BRM) that produce semantic sub-goals and plan on a relation graph generalize better than reactive LSTM-only policies; RNN controller baselines that use the same semantic inputs perform worse than BRM, indicating that an explicit relational/graph representation plus planning yields more robust sub-goal decomposition. Frequent re-planning (N=10 chosen because oracle steps to reach a nearby target ~12.27) is important: large N degrades performance, implying that in higher-diameter or longer-hop topologies policies need timely belief updates and re-planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Relational Memory for Semantic Visual Navigation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1362.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1362.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>House3D</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>House3D environment (RoomNav)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A realistic, human-designed indoor 3D environment suite providing varied houses with labeled semantic regions (room types) used for embodied navigation experiments; the RoomNav task asks an agent to navigate to a specified room type given first-person observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Building generalizable agents with a realistic and rich 3D environment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>House3D (RoomNav task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Indoor household navigation domain with 200 training houses, 20 validation, 50 test houses; targets are semantic room types and success is measured by 'seeing' target objects and being inside target area.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td>No explicit locked/conditional doors modeled in this work; connectivity is derived from room adjacency and semantic co-occurrence rather than door mechanics.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not provided as an explicit house-level graph in this paper; BRM builds an abstract semantic relation graph (over room types) that captures recurrent adjacency/connectivity patterns across houses. The per-house ground-truth semantic relation graph (used for measuring plan-distance) exists implicitly in the environment labels.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset: 200 train houses, 20 validation, 50 test houses; semantic abstraction uses 8 room types (K=8) plus an 'unknown' node => 9 concept nodes for BRM; ground-truth shortest path lengths (oracle) averaged across test tasks: overall average 46.86 steps (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Variety: BRM agent, LSTM locomotion policies (per-target), pure LSTM DRL baseline, semantic-augmented LSTM, RNN controller HRL baseline, random policy.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents act in House3D with discrete actions (reduced to 9 action primitives) receiving RGB observations; locomotion policies are goal-conditioned LSTM networks trained by actor-critic; BRM sits on top of locomotion as high-level planner.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success rate (%), SPL (Success weighted by Path Length), average episode length (steps), steps to reach nearby target (oracle references).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Oracle average shortest path (ground truth): 46.86 steps. BRM average successful episode length (H=1000): 337.0 steps. See Table 1 and Table 7 for per-agent/per-plan-distance values.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Per-agent values reported in paper (examples): BRM H=1000 overall 41.1%; pure LSTM H=1000 overall 22.5%; RNN controller H=1000 overall 37.0% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Planner + memory / hierarchical policies that exploit semantic priors and update beliefs (BRM) perform best in generalization to unseen houses.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Tasks grouped by ground-truth plan-distance show monotonic decrease in success with larger plan-distance across agents; BRM's advantage is larger for longer planning distances and with larger planning budgets (larger horizon allows more re-planning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Paper provides per-plan-distance (1..5) breakdowns showing that BRM maintains higher success & SPL than baselines across plan-distances; for plan-dist=1 BRM success is ~73.7% (H=1000, CNN detector), for plan-dist=5 BRM success is lower (~28.4% H=1000), demonstrating relationship between number of semantic hops and difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Because House3D induces partial observability and long-horizon tasks, hierarchical controllers with an explicit semantic topology (BRM) lead to better decomposition of long tasks into sub-goals; locomotion quality remains a bottleneck even when planning is strong.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Relational Memory for Semantic Visual Navigation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1362.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1362.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pure µ(θ)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pure DRL LSTM locomotion policy (µ(θ))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A goal-conditioned LSTM policy trained by actor-critic (A2C-like) to navigate directly toward a semantic target without an explicit relational memory or high-level planner; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>House3D (RoomNav task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as above; baseline agent that directly maps observations and target to actions using an LSTM policy.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Not used internally; reactive policy does not build or use an explicit semantic graph.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Operates on same House3D houses (200/20/50 split); does not explicitly model graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>pure µ(θ) (LSTM A2C locomotive)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A single LSTM-based goal-conditioned locomotion network (policy) trained with shaped rewards and curriculum learning; does not maintain explicit memory or planning module.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success rate, SPL, average episode length.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>H=1000 overall: 22.5% success and 22.9‰ SPL (Table 1); H=300 overall: 13.1% success and 22.9‰ SPL (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>H=1000 overall: 22.5% (Table 1); performance drops faster than BRM as target distance increases (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reactive LSTM-only policy is suboptimal for long-distance/partial-observability tasks; memory/planning-based policies perform better.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Pure µ(θ) performance rapidly degrades as plan-distance increases and target distance increases: for faraway targets it approaches random performance (Fig. 6, Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Across plan-distances, pure µ(θ) shows lower success for larger plan-dist and benefits less from increased planning budget/horizon than BRM.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Single LSTM lacks explicit high-level structure and cannot reliably decompose long-horizon navigation into semantic waypoints; thus it requires shorter plan-distances to perform well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Relational Memory for Semantic Visual Navigation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1362.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1362.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN controller (HRL baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RNN high-level controller with same semantic inputs (hierarchical RNN controller)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM-based high-level controller trained to emit semantic sub-goals every N steps given the same semantic inputs as BRM; used to test whether a generic RNN can replace an explicit relational graph controller.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>House3D (RoomNav task)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Hierarchical baseline: high-level LSTM controller + same goal-conditioned locomotion; controller has access to semantic signals and accumulative bit-OR features.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Controller is a learned RNN and does not build an explicit symbolic relation graph; implicitly models temporal/semantic patterns via hidden state.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Same House3D setup; controller does not expose an explicit graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RNN controller + goal-conditioned locomotion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>High-level LSTM controller (50 hidden units) produces sub-targets every N steps, trained on E_train; uses same semantic inputs as BRM but relies on an opaque recurrent representation rather than an explicit probabilistic graph and planner.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Success rate, SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>H=1000 overall: 37.0% success and 45.2‰ SPL (Table 1); H=300 overall: 19.9% success and 34.2‰ SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>H=1000 overall: 37.0% (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Learned hierarchical controller can perform well, but explicit planning with BRM yields better generalization and higher SPL, especially with larger planning budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>RNN controller benefits from shorter plan-distances but underperforms BRM consistently across plan-distances, indicating that an explicit relational topology + planning is more effective than a purely learned recurrent controller for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>Across plan-distances 1..5, RNN controller performs between pure µ(θ) and BRM but is beaten by BRM in most settings; BRM's advantage increases with horizon (more re-planning).</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>An RNN high-level controller that consumes the same semantic features cannot capture and exploit the explicit relational priors as effectively as BRM's symbolic/probabilistic graph + planner; structured memory + planning leads to better long-range generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian Relational Memory for Semantic Visual Navigation', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unifying map and landmark based representations for visual navigation <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Visual semantic navigation using scene priors <em>(Rating: 2)</em></li>
                <li>On evaluation of embodied navigation agents <em>(Rating: 1)</em></li>
                <li>Cognitive mapping and planning for visual navigation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1362",
    "paper_id": "paper-ac0a9ced9c704649326fcb68cba2f904e2c6fb1d",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "BRM",
            "name_full": "Bayesian Relational Memory",
            "brief_description": "A probabilistic relation-graph memory over semantic concepts (room types) that encodes priors from training environments, updates edge posteriors online via Bayes rule from noisy semantic observations, and produces sub-goals for a goal-conditioned locomotion policy via graph search.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "House3D (RoomNav task)",
            "environment_description": "A realistic indoor 3D environment (House3D) used for semantic visual navigation where the agent receives first-person RGB observations and must navigate to a target room type (kitchen, living room, dining room, bedroom, bathroom, office, garage, outdoor). Domain: household indoor navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "No explicit door/lock constraints are modelled in BRM or the RoomNav task in this paper; connectivity is encoded as probabilistic \"close-by\" relations between semantic concepts rather than door conditions.",
            "graph_connectivity": "Probabilistic undirected relation graph over semantic nodes where each pair (i,j) has a Bernoulli random variable indicating a \"close-by\" relation; connectivity is sparse/structured (learned priors make some pairs likely and others unlikely) rather than fully connected. The graph is updated online to produce posterior edge probabilities for planning.",
            "environment_size": "K = 8 pre-selected semantic room types (kitchen, living room, dining room, bedroom, bathroom, office, garage, outdoor); BRM adds an 'unknown' node so effectively K+1 = 9 nodes for the module; potential relations: K(K-1)/2 = 28 unordered room-pair relations (probabilistic).",
            "agent_name": "BRM agent (Bayesian Relational Memory + goal-conditioned LSTM locomotion)",
            "agent_description": "High-level BRM module: probabilistic relation-graph over semantic concepts with learned priors ψ^{prior} and observation noise ψ^{obs}; updates posteriors P(z_{i,j}|Y) via Bayes rule from bit-OR aggregated semantic detections over N-step windows and plans by maximizing joint belief along a concept path; Low-level: goal-conditioned LSTM locomotion policies (behavioral approach with a separate sub-policy per semantic target) trained with actor-critic (A2C-like) and reward shaping.",
            "exploration_efficiency_metric": "Success weighted by Path Length (SPL), success rate (%), and steps-to-goal / average successful episode length (steps).",
            "exploration_efficiency_value": "H=1000 overall: 41.1% success rate and 57.5‰ SPL (per-mille) for BRM (Table 1); H=300 overall: 23.1% success rate and 45.3‰ SPL. Average successful episode length (H=1000) for BRM: 337.0 steps (Table 7).",
            "success_rate": "H=1000 overall: 41.1%; H=300 overall: 23.1% (see detailed per plan-distance values in Table 1).",
            "optimal_policy_type": "Planning-based hierarchical policy (memory/graph-based high-level controller + goal-conditioned local locomotion) performs best; BRM explicitly uses planning on learned probabilistic topology.",
            "topology_performance_relationship": "Performance correlates with topological planning distance: as the ground-truth shortest sequence of semantic sub-goals ('plan-dist') increases, success rates decrease for all agents, but BRM degrades less. Learned priors over relations improve navigation (higher success and SPL) compared to an uninformative prior; more frequent replanning (smaller N relative to oracle nearby step length ~12.27) improves performance. Errors are dominated by locomotion quality rather than planning accuracy: with an oracle locomotion, BRM approaches optimal planner performance.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper evaluates performance grouped by 'plan-distance' (1..5: the shortest number of sub-goal hops in the ground-truth relation graph). Results show (Table 1): BRM outperforms baselines at all plan-distances, and the relative advantage of BRM increases with longer horizons (more planning computations). Learned priors help most when horizon is limited; with long horizons the graph posterior converges and the prior matters less.",
            "policy_structure_findings": "Hierarchical, memory-and-planning-based controllers (BRM) that produce semantic sub-goals and plan on a relation graph generalize better than reactive LSTM-only policies; RNN controller baselines that use the same semantic inputs perform worse than BRM, indicating that an explicit relational/graph representation plus planning yields more robust sub-goal decomposition. Frequent re-planning (N=10 chosen because oracle steps to reach a nearby target ~12.27) is important: large N degrades performance, implying that in higher-diameter or longer-hop topologies policies need timely belief updates and re-planning.",
            "uuid": "e1362.0",
            "source_info": {
                "paper_title": "Bayesian Relational Memory for Semantic Visual Navigation",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "House3D",
            "name_full": "House3D environment (RoomNav)",
            "brief_description": "A realistic, human-designed indoor 3D environment suite providing varied houses with labeled semantic regions (room types) used for embodied navigation experiments; the RoomNav task asks an agent to navigate to a specified room type given first-person observations.",
            "citation_title": "Building generalizable agents with a realistic and rich 3D environment",
            "mention_or_use": "use",
            "environment_name": "House3D (RoomNav task)",
            "environment_description": "Indoor household navigation domain with 200 training houses, 20 validation, 50 test houses; targets are semantic room types and success is measured by 'seeing' target objects and being inside target area.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": "No explicit locked/conditional doors modeled in this work; connectivity is derived from room adjacency and semantic co-occurrence rather than door mechanics.",
            "graph_connectivity": "Not provided as an explicit house-level graph in this paper; BRM builds an abstract semantic relation graph (over room types) that captures recurrent adjacency/connectivity patterns across houses. The per-house ground-truth semantic relation graph (used for measuring plan-distance) exists implicitly in the environment labels.",
            "environment_size": "Dataset: 200 train houses, 20 validation, 50 test houses; semantic abstraction uses 8 room types (K=8) plus an 'unknown' node =&gt; 9 concept nodes for BRM; ground-truth shortest path lengths (oracle) averaged across test tasks: overall average 46.86 steps (Table 7).",
            "agent_name": "Variety: BRM agent, LSTM locomotion policies (per-target), pure LSTM DRL baseline, semantic-augmented LSTM, RNN controller HRL baseline, random policy.",
            "agent_description": "Agents act in House3D with discrete actions (reduced to 9 action primitives) receiving RGB observations; locomotion policies are goal-conditioned LSTM networks trained by actor-critic; BRM sits on top of locomotion as high-level planner.",
            "exploration_efficiency_metric": "Success rate (%), SPL (Success weighted by Path Length), average episode length (steps), steps to reach nearby target (oracle references).",
            "exploration_efficiency_value": "Oracle average shortest path (ground truth): 46.86 steps. BRM average successful episode length (H=1000): 337.0 steps. See Table 1 and Table 7 for per-agent/per-plan-distance values.",
            "success_rate": "Per-agent values reported in paper (examples): BRM H=1000 overall 41.1%; pure LSTM H=1000 overall 22.5%; RNN controller H=1000 overall 37.0% (Table 1).",
            "optimal_policy_type": "Planner + memory / hierarchical policies that exploit semantic priors and update beliefs (BRM) perform best in generalization to unseen houses.",
            "topology_performance_relationship": "Tasks grouped by ground-truth plan-distance show monotonic decrease in success with larger plan-distance across agents; BRM's advantage is larger for longer planning distances and with larger planning budgets (larger horizon allows more re-planning).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Paper provides per-plan-distance (1..5) breakdowns showing that BRM maintains higher success & SPL than baselines across plan-distances; for plan-dist=1 BRM success is ~73.7% (H=1000, CNN detector), for plan-dist=5 BRM success is lower (~28.4% H=1000), demonstrating relationship between number of semantic hops and difficulty.",
            "policy_structure_findings": "Because House3D induces partial observability and long-horizon tasks, hierarchical controllers with an explicit semantic topology (BRM) lead to better decomposition of long tasks into sub-goals; locomotion quality remains a bottleneck even when planning is strong.",
            "uuid": "e1362.1",
            "source_info": {
                "paper_title": "Bayesian Relational Memory for Semantic Visual Navigation",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "pure µ(θ)",
            "name_full": "Pure DRL LSTM locomotion policy (µ(θ))",
            "brief_description": "A goal-conditioned LSTM policy trained by actor-critic (A2C-like) to navigate directly toward a semantic target without an explicit relational memory or high-level planner; used as a baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "House3D (RoomNav task)",
            "environment_description": "Same as above; baseline agent that directly maps observations and target to actions using an LSTM policy.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Not used internally; reactive policy does not build or use an explicit semantic graph.",
            "environment_size": "Operates on same House3D houses (200/20/50 split); does not explicitly model graph size.",
            "agent_name": "pure µ(θ) (LSTM A2C locomotive)",
            "agent_description": "A single LSTM-based goal-conditioned locomotion network (policy) trained with shaped rewards and curriculum learning; does not maintain explicit memory or planning module.",
            "exploration_efficiency_metric": "Success rate, SPL, average episode length.",
            "exploration_efficiency_value": "H=1000 overall: 22.5% success and 22.9‰ SPL (Table 1); H=300 overall: 13.1% success and 22.9‰ SPL (Table 1).",
            "success_rate": "H=1000 overall: 22.5% (Table 1); performance drops faster than BRM as target distance increases (Figure 6).",
            "optimal_policy_type": "Reactive LSTM-only policy is suboptimal for long-distance/partial-observability tasks; memory/planning-based policies perform better.",
            "topology_performance_relationship": "Pure µ(θ) performance rapidly degrades as plan-distance increases and target distance increases: for faraway targets it approaches random performance (Fig. 6, Table 1).",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Across plan-distances, pure µ(θ) shows lower success for larger plan-dist and benefits less from increased planning budget/horizon than BRM.",
            "policy_structure_findings": "Single LSTM lacks explicit high-level structure and cannot reliably decompose long-horizon navigation into semantic waypoints; thus it requires shorter plan-distances to perform well.",
            "uuid": "e1362.2",
            "source_info": {
                "paper_title": "Bayesian Relational Memory for Semantic Visual Navigation",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "RNN controller (HRL baseline)",
            "name_full": "RNN high-level controller with same semantic inputs (hierarchical RNN controller)",
            "brief_description": "An LSTM-based high-level controller trained to emit semantic sub-goals every N steps given the same semantic inputs as BRM; used to test whether a generic RNN can replace an explicit relational graph controller.",
            "citation_title": "",
            "mention_or_use": "use",
            "environment_name": "House3D (RoomNav task)",
            "environment_description": "Hierarchical baseline: high-level LSTM controller + same goal-conditioned locomotion; controller has access to semantic signals and accumulative bit-OR features.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": false,
            "door_constraints_description": null,
            "graph_connectivity": "Controller is a learned RNN and does not build an explicit symbolic relation graph; implicitly models temporal/semantic patterns via hidden state.",
            "environment_size": "Same House3D setup; controller does not expose an explicit graph size.",
            "agent_name": "RNN controller + goal-conditioned locomotion",
            "agent_description": "High-level LSTM controller (50 hidden units) produces sub-targets every N steps, trained on E_train; uses same semantic inputs as BRM but relies on an opaque recurrent representation rather than an explicit probabilistic graph and planner.",
            "exploration_efficiency_metric": "Success rate, SPL.",
            "exploration_efficiency_value": "H=1000 overall: 37.0% success and 45.2‰ SPL (Table 1); H=300 overall: 19.9% success and 34.2‰ SPL.",
            "success_rate": "H=1000 overall: 37.0% (Table 1).",
            "optimal_policy_type": "Learned hierarchical controller can perform well, but explicit planning with BRM yields better generalization and higher SPL, especially with larger planning budgets.",
            "topology_performance_relationship": "RNN controller benefits from shorter plan-distances but underperforms BRM consistently across plan-distances, indicating that an explicit relational topology + planning is more effective than a purely learned recurrent controller for generalization.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "Across plan-distances 1..5, RNN controller performs between pure µ(θ) and BRM but is beaten by BRM in most settings; BRM's advantage increases with horizon (more re-planning).",
            "policy_structure_findings": "An RNN high-level controller that consumes the same semantic features cannot capture and exploit the explicit relational priors as effectively as BRM's symbolic/probabilistic graph + planner; structured memory + planning leads to better long-range generalization.",
            "uuid": "e1362.3",
            "source_info": {
                "paper_title": "Bayesian Relational Memory for Semantic Visual Navigation",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unifying map and landmark based representations for visual navigation",
            "rating": 2
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2
        },
        {
            "paper_title": "Visual semantic navigation using scene priors",
            "rating": 2
        },
        {
            "paper_title": "On evaluation of embodied navigation agents",
            "rating": 1
        },
        {
            "paper_title": "Cognitive mapping and planning for visual navigation",
            "rating": 1
        }
    ],
    "cost": 0.01525675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Bayesian Relational Memory for Semantic Visual Navigation</h1>
<p>Yi Wu ${ }^{\dagger}{ }^{\text {I }}$ Yuxin Wu ${ }^{\ddagger}$ Aviv Tamar ${ }^{\S}$ Stuart Russell ${ }^{\dagger}$ Georgia Gkioxari ${ }^{\ddagger}$ Yuandong Tian ${ }^{\ddagger}$<br>${ }^{\dagger}$ UC Berkeley ${ }^{\ddagger}$ Facebook AI Research ${ }^{\S}$ Technion ${ }^{\S}$ OpenAI<br>${ }^{\dagger}{j x w u y i$, russell}@cs.berkeley.edu ${ }^{\ddagger}{$ yuxinwu, gkioxari, yuandong}@fb.com ${ }^{\S}$ avivt@technion.ac.il</p>
<h4>Abstract</h4>
<p>We introduce a new memory architecture, Bayesian Relational Memory (BRM), to improve the generalization ability for semantic visual navigation agents in unseen environments, where an agent is given a semantic target to navigate towards. BRM takes the form of a probabilistic relation graph over semantic entities (e.g., room types), which allows (1) capturing the layout prior from training environments, i.e., prior knowledge, (2) estimating posterior layout at test time, i.e., memory update, and (3) efficient planning for navigation, altogether. We develop a BRM agent consisting of a BRM module for producing sub-goals and a goalconditioned locomotion module for control. When testing in unseen environments, the BRM agent outperforms baselines that do not explicitly utilize the probabilistic relational memory structure.</p>
<h2>1. Introduction</h2>
<p>Memory is a crucial component for intelligent agents to gain extensive reasoning abilities over a long horizon. One such challenge is visual navigation, where an agent is placed to an environment with unknown layouts and room connectivity, acts on visual signal perceived from its surrounding, explores and reaches a goal position efficiently.</p>
<p>Due to partial observability, the agent must memorize its past experiences and react accordingly. Hence, deep learning (DL) models for visual navigation often encodes memory structures in its design. LSTM is initially used to as general-purpose implicit memory [36, 35, 10]. Recently, to improve the performance, explicit and navigation-specific structural memory are used [42, 24, 46, 25].</p>
<p>Two categories exist for navigation-specific memory: the spatial memory [42, 24] and topological memory [25, 46]. The core idea of spatial memory is to extend the 1-dimensional memory in LSTM to a 2-dimensional matrix that represents the spatial structure of the environment, where a particular entry in the matrix corresponds to a 2D location/region in the environment. Due to its regular structure, value iteration [50] can be applied directly for effective planning over the memory matrix.</p>
<p>However, planning on such spatial memory can be computationally expensive for environments with large rooms. To navigate, precise localization of an agent is often not unnecessary. Extensive psychological evidences [46] also show that animals do not rely strongly on metric representations [54, 22]. Instead, humans primarily depend on a landmark-based navigation strategy, which can be supported by qualitative topological knowledge of the environment [22]. Therefore, it is reasonable to represent the memory as a topological graph where the vertices are landmarks in the environment and edges denote short-term reachability between landmarks. During navigation, a localization network is trained to identify the position of the agent and the goal w.r.t. the landmarks in the memory and an efficient graph search can be used for long-term planning.</p>
<p>However, still human navigation shows superior generalization performance which cannot be explained by either spatial or topological memory. For example, first-time home visitors naturally move towards the kitchen (rather than outdoor or toilet) to get a plate; from kitchen to bedroom, they know that living room may be an intermediate waypoint. Although visually different, such semantic knowledge, i.e., the "close-by" relations over semantic entities, are naturally shared across environments and can be learned from previous experience to guide future navigation. In comparison, existing approaches of topological memory assume pre-exploration experiences of the environment before navigation starts [25, 46], provide no memory updating operations, and cannot incorporate the prior knowledge of scene layouts and configurations from previously seen environments.</p>
<p>In this work, we propose a new memory design for visual navigation, Bayesian Relational Memory (BRM), which (1) captures the prior knowledge of scene layouts from training environments and (2) allows both efficient planning and updating during test-time exploration. BRM can be viewed as a probabilistic version of a topological memory with semantic abstractions: each node in BRM denotes a semantic concept (e.g., object category, room type, etc), which can be detected via a neural detector, and each edge denotes the relation between two concepts. In each environment, a single relation may be present or not. For each relation (edge), we can average its existences over all training environments</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A demonstration of our task and method. The agent perceives visual signals and needs to find the kitchen, which cannot be seen from outside (leftmost). So the agent plans in its memory and conclude that it may first find a likely intermediate waypoint, i.e., living room. Then the agent repeatedly updates its belief of the environment layout, re-plans accordingly and reaches the kitchen in the end.</p>
<p>and learn an existence probability to denote the <em>prior knowledge</em> between two semantic concepts. During exploration at test time, we can incrementally observe the existences of relations within that particular test environment. Therefore, we can use these environment specific observations to update the probability of those relations in the memory via the Bayes rule to derive the <em>posterior knowledge</em>. Additionally, we train a semantic-goal-conditioned LSTM locomotion policy for control via deep reinforcement learning (DRL), and by planning on the relation graph with posterior probabilities, the agent picks the next semantic sub-goal to navigate towards.</p>
<p>We evaluate our BRM method in a semantic visual navigation task on top of the House3D environment [58, 10], which provides a diverse set of objects, textures and human-designed indoor scenes. The semantic scenes and entities in House3D are fully labeled (with noise), which are natural for our BRM model. In the navigation task, the agent observes first-person visual signals and needs to navigate towards a particular room type. We utilize the room types as the semantic concepts (nodes) in BRM and the "close-by" relations as the edges. We evaluate on unseen environments with random initial locations and compare our learned model against other DRL-based approaches without the BRM representation. Experimental results show that the agent equipped with BRM can achieve the semantic goal with higher success rates and fewer navigation steps.</p>
<p>Our contributions are as follows: (1) we proposed a new memory representation, Bayesian Relational Memory (BRM), in the form of probabilistic relation graphs over semantic concepts; (2) BRM is capable of encoding prior knowledge over training environments as well as efficient planning and updating at test time; (3) by integrating BRM into a DRL locomotion policy, we show that the generalization performances can be significantly improved.</p>
<h2>2. Related Work</h2>
<p>Navigation is one of the most fundamental problems in mobile robotics. Traditional approaches (like SLAM) build metric maps via sensory signals, which is subsequently used for planning [17, 4, 51, 16]. More recently, thanks to the advances of deep learning, end-to-end approaches have been applied to tackle navigation in various domains, such as mazes [36, 27], indoor scenes [63, 5, 48, 37, 30], autonomous driving [7, 59, 13], and Google street view [35]. There are also nice summaries of recent progresses [2, 37]. We focus on indoor navigation scenario with House3D environment [58] which contains real-world-consistent relations between semantic entities and provides ground-truth labels of objects and scenes.</p>
<p>There are also works studying visual navigation under natural language guidance, including instruction following [6, 39, 23, 55, 3] and question answering [11, 10, 1]. These tasks require the agent to understand the natural language and reason accordingly in an interactive environment. In our semantic navigation task, the goal instruction is simplified to a single semantic concept and hence we focus more on the reasoning ability of navigation agents.</p>
<p>In our work, the reasoning is performed on the relations over semantic concepts from visual signals. Similar ideas of using semantic knowledge to enhance reasoning have been applied to image classification [34, 56], segmentation [52, 64], situation recognition [33], visual question answering [57, 9, 53, 28, 26], image retrieval [29] and relation detection [62]. Savinov et al. [47] and Kuang et al. [18] also consider extracting visual concepts dynamically by treating every received input frame as an individual concept and storing them in the memory. The most related work to ours is a concurrent one by Wei et al. [60], which considers visual navigation towards an object category and utilizes a knowledge graph as the prior knowledge. Wei et al. [60] use a fixed graph to extract features for the target as an extra input to the locomotion without graph updating or planning. While in our work, the relation graph is used in a Bayesian manner as a representation for the memory, which unites use of prior knowledge, updating and planning altogether.</p>
<p>From a reinforcement learning perspective, our work is related to the model-based approaches [14, 45, 61, 44, 32], in the sense that we model the environment layout via a re-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />Figure 2: The architecture overview of our proposed navigation agent equipped with the Bayesian Relational Memory (BRM).</p>
<p>lation graph and plan on it. Our work is also related to hierarchical reinforcement learning <em>[49, 12, 31]</em>, where the controller (BRM) produces a high-level sub-goal for the subpolicy (locomotion) to pursue. Furthermore, BRM learns from multi-task training and its update operation fast adapts the prior relations to the test environment, which can be also viewed as a form of meta-learning <em>[19, 15, 38]</em>.</p>
<h2>3 Method</h2>
<h3>3.1 Task Setup</h3>
<p>We consider the semantic visual navigation problem where an agent interacts with an environment with discrete time steps and navigates towards a semantic goal. In House3D, the semantic entities of interest are room types and we assume a fixed number of $K$ categories. In the beginning of an episode, the agent is given a semantic target $T\in\mathcal{T}={T_{1},\ldots,T_{K}}$ to navigate towards for a success. At each time step $t$, the agent receives a visual observation $s_{t}$ and produces an action $a_{t}\in\mathcal{A}$ conditioning on $s_{t}$ and $T$.</p>
<p>We aim to learn a neural agent that can generalize to unseen environments. Hence, we train the agent on a training set $\mathcal{E}<em _text_valid="\text{valid">{\text{train}}$, where the ground-truth labels are assumed, and validate on $\mathcal{E}</em>}}$. Evaluation is performed on another separate set of environments $\mathcal{E<em t="t">{\text{test}}$. At test time, the agent only access to the visual signal $s</em>$ without any pre-exploration experiences of the test environment.</p>
<h3>3.2 Method Overview</h3>
<p>The overall architecture of a BRM agent is shown in Fig. 2, which has two modules, the Bayesian Relational Memory (BRM) as well as an LSTM locomotor policy for control (Fig. 2 left). Particularly, the key component of a BRM agent is a probabilistic relation graph (Fig. 2 right), where each node corresponds to a particular semantic target $T_{i}$. For semantic target $T_{i}$ and $T_{j}$, the edge between them denotes the “close-by” relation and the probability of that edge implies how likely $T_{i}$ and $T_{j}$ are close to each other in the current environment.</p>
<p>At a high level, the locomotion is a semantic-goal-conditioned policy which takes in both the visual input $s_{t}$ and the sub-goal $g\in\mathcal{T}$ produced by the BRM module to produce actions towards $g$. The BRM module takes in the visual observation $s_{t}$ at each time step, extracts the semantic information via a CNN detector and store them in a replay buffer. We periodically update the posterior probability of each edge in the relation graph and re-plan to produce a new sub-goal. In our work, the graph is updated every fixed number of $N$ steps. Notably, we do not assume existences of all concepts — in case of a missing concept in particular environment, the posterior of its associated edges will approach zero as more experiences gained in an episode.</p>
<h3>3.3 Bayesian Relational Memory</h3>
<p>The BRM module consists of two parts, a semantic classifier and the most important component, a probabilistic relation graph over semantic concepts, in the form of a probabilistic graphical model allowing efficient planning and posterior updates.</p>
<p>Intuitively, at each time step, the agent detects the surrounding room type and maintains the probability of whether two room types $T_{i}$ and $T_{j}$ are “nearby” in the current environment. If the agent starts from room $T_{i}$ and reaches another room $T_{j}$ within a few steps, the probability of “nearby” relation between $T_{i}$ and $T_{j}$ should be increased; otherwise the probability should be decreased. Periodically, the graph is updated and the agent finds the most likely path from the current room towards the target as a navigation guidance.</p>
<p>We introduce these two components in details as well as how to update and plan with BRM as follows.
Semantic classifier: The semantic classifier detects the room type label $c_{t}$ for the agent’s surrounding region. It can be trained by supervised learning on $\mathcal{E}_{\text{train}}$. Note that for robust room type classification, only using the first-person view image may not be enough. For example, the agent in a bedroom may face towards a wall, where the first-person image is not informative at all for the classifier, but a bed might be just behind. So we take the panoramic view as the</p>
<p>[table] 1In perfect noiseless setting, the relation should with probability 1.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Overview of the planning (Left) and the update operation (Right) with BRM.</p>
<p>Classifier input, which consists of 4 images, $s_{t}^{1}, \ldots, s_{t}^{4}$ with different view angles. We use a 10-layer CNN with batch normalization to extract features $f(s_{t}^{i})$ for each $s_{t}^{i}$ and compute the attention weights over these visual features by $l_{i}=f(s_{v}^{i})W_{1}^{T}W_{2}\left[f(s_{v}^{1}), \ldots, f(s_{v}^{4})\right] a_{i}=\operatorname{softmax}\left(l_{i}\right)$ with parameters $W_{1}, W_{2}$. Then the weighted average of these four features $\sum_{i} a_{i} f(s_{t}^{i})$ is used for the final sigmoid predictions for each semantic concept $T_{i}$. This results in a $K$-dimensional binary vector $c_{t} \in{0,1}^{K}$ at time $t$.</p>
<p><strong>Probabilistic relation graph:</strong> We represent the probabilistic graph in the form of a graphical model $P(z, y ; \psi)$ with latent variable $z$, observation variable $y$ and parameter $\psi$.</p>
<p>Since we have $K$ semantic concepts, there are $K^{3}$ nodes and $K(K-1)/2$ relations (edges) in the graph. Each relation is probabilistic (i.e., it may exist or not with probability) and before entering a particular environment, we only hold a prior belief of that relation. Formally, for the relation between $T_{i}$ and $T_{j}$, we adopt a Bernoulli variable $z_{i,j}$ defined by $z_{i,j} \sim \operatorname{Bernoulli}\left(\psi_{i,j}^{\text{prior}}\right)$, where parameter $\psi_{i,j}^{\text{prior}}$ denotes the prior belief of $z_{i,j}$ existing. During exploration, the agent can noisily observe $z_{i,j}$ and use the noisy observations to estimate the true value of $z_{i,j}$. We define the noisy observation model $P\left(y_{i,j} \mid z_{i,j}\right)$ by</p>
<p>$$
y_{i,j} \sim \begin{cases} \text { Bernoulli } \left(\psi_{i,j,0}^{\text{obs}}\right) &amp; \text { if } z_{i,j}=0 \ \text { Bernoulli } \left(1-\psi_{i,j,1}^{\text{obs}}\right) &amp; \text { if } z_{i,j}=1 \end{cases}
$$</p>
<p>where $\psi^{\text{obs}}$ is another parameter to learn. At each time step, the agent holds an overall posterior belief $P(z \mid \mathcal{Y})$ of relation existences within the current environment, based on its experiences $\mathcal{Y}$, namely the samples of variable $y$.</p>
<p><strong>Posterior update and planning:</strong> A visualization of the procedures is shown in Fig. 3. We assume the agent explores the current environment for a <em>short</em> horizon of $N$ steps and stores the recent semantic signals $c_{t}, \ldots, c_{t+N}$ in the replay buffer. Then we compute the bit-OR operation over these binary vectors $B = c_{t} \text{ OR } \ldots \text{ OR } c_{t+N}$. $B$ represents all the visited regions within a short ($N$-step) exploration period. When two targets appear concurrently in a short trajectory, they are assumed to be "close-by". For $T_{i}$ and $T_{j}$ with $B\left(T_{i}\right) = B\left(T_{j}\right) = 1$, $T_{i}$ and $T_{j}$ should be nearby in the current environment, namely a sample of $y_{i,j} = 1$; otherwise for $B\left(T_{i}\right) \neq B\left(T_{j}\right)$, we get a sample of $y_{i,j} = 0$. With all the history samples of $y$ as $\mathcal{Y}$, we can perform posterior inference, i.e., compute posterior Bernoulli distribution $P\left(z_{i,j} \mid \mathcal{Y}<em i_j="i,j">{i,j}\right)$, for each $z</em>$ by the Bayes rule.</p>
<p>Let $\hat{z}<em i_j="i,j">{i,j} = P\left(z</em>} \mid \mathcal{Y<em i="i">{i,j}\right)$ denote the posterior probability of relation over $T</em>$ and the target $T$, we search for an optimal plan $\tau^}$ and $T_{j}$. Given the current beliefs $\hat{z}$, the semantic signal $c_{t<em> = {\tau_0, \tau_1, \ldots, \tau_{m-1}, \tau_m}$ over the graph, where $\tau_i \in {1 \ldots K}$ denotes an index of concepts, so that the joint belief along the path from some current position to the goal is maximized: $\tau^</em> = \arg \max_{\tau} c_t \left(T_{\tau_0}\right) \prod_{t=1}^{m} \hat{z}<em i-1="i-1">{\tau</em>$.}, \tau_i</p>
<p>After obtaining $\tau^*$, we execute the locomotion policy for sub-goal $g_t = T_{\tau_1^c}$, and then periodically update the graph, clear the replay buffer and re-plan every $N$ steps.</p>
<p><strong>Learning the probabilistic graph:</strong> The parameter $\psi$ has two parts: $\psi^{\text{prior}}$ for the prior of $z$ and $\psi^{\text{obs}}$ for the noisy observation $y$.</p>
<p>For the prior parameter $\psi^{\text{prior}}$, we learn from $\mathcal{E}<em i_j="i,j">{\text{train}}$ with the ground truth labels. A visualization is shown in Fig. 4. For each pair of room types $T_i$ and $T_j$, we enumerate all training environments and run <em>random explorations</em> from some location of room type $T_i$. If eventually the agent reaches somewhere of room type $T_j$, we consider $T_i$ and $T_j$ are nearby and therefore a <em>positive</em> sample $z</em> = 1$; otherwise a <em>negative</em> sample $z_{i,j} = 0$. Suppose $\mathcal{Z}$ denotes all the samples we obtained for $z$. We run maximum likelihood estimate for $\psi^{\text{prior}}$ by maximizing $L_{\text{MLE}}\left(\psi^{\text{prior}}\right) = P\left(\mathcal{Z} \mid \psi^{\text{prior}}\right)$. We can choose any exploration strategy such that for any "close-by" targets, they appear together in a short exploration trajectory more frequently. Random exploration is the most lightweight option among all.</p>
<p>The noisy observation parameter $\psi^{\text{obs}}$ is related to the</p>
<p><sup>2</sup>It is a multi-label classification setting. Imagine an open kitchen with both cooking facilities and dining tables could have two labels.</p>
<p><sup>3</sup>In fact we have $K + 1$ nodes. For clarity purpose, we use $K$ here and explain the details of the extra node later in Sec. 3.5.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5. The learned prior of the relations, including the most (red) and least (blue) likely nearby rooms for dining room (Left), bedroom (Mid.) and outdoor (Right), with numbers denoting $$ \psi^{\text{prior}} $$, i.e., the prior probability of two room types are nearby.</p>
<p>performance of the locomotion policy $$ \mu(\theta) $$: if $$ \mu(\theta) $$ has a higher navigation success rate, $$ \psi^{\text{obs}} $$ should be smaller (i.e., low noise level); when $$ \mu(\theta) $$ is poor, $$ \psi^{\text{obs}} $$ should be larger (cf. Eq. (1)). Unfortunately, there is no direct learning supervision for $$ \psi^{\text{obs}} $$. However, we can evaluate the "goodness" of a particular value of $$ \psi^{\text{obs}} $$ by evaluating the success rate of the overall BRM agent on $$ \mathcal{E}_{\text{valid}} $$. Therefore, we can simply run grid search to derive the best parameter.</p>
<h3>3.4. The Goal Conditioned Policy</h3>
<p>We learn an LSTM locomotion policy $$ \mu(s_t, g; \theta) $$ parameterized by $$ \theta $$, which conditions on observation $$ s_t $$ and navigates towards goal $$ g $$. Following Wu et al. [58], we learn $$ \mu(s_t, g; \theta) $$ by formulating the task as a reinforcement learning problem with shaped reward: when the agent moves towards target room $$ g $$, it receives a positive reward proportional to the distance decrements; if the agent moves apart or hits an obstacle, a penalty is presented. A success reward of 10 and a time penalty of 0.1 are also assigned. We optimize the policy on $$ \mathcal{E}_{\text{train}} $$ via the actor-critic method [40] with a curriculum learning paradigm by periodically increasing the maximum spawn distance to the target $$ g $$. Additionally, thanks to a limited set of K targets, we adopt a behavior approach [8] for improved performances: we train a separate policy $$ \mu_i(s_t; \theta_i) $$ for each semantic target T_i and when given the sub-goal $$ g $$ from the BRM module, we directly execute its corresponding behavior network. We empirically observe it performs better than the original gated-attention policy in Wu et al.[58]. Such an architecture is also common technique in other domains such as RL [41, 20] and robotics [21].</p>
<h3>3.5. Implementation Details</h3>
<p>We introduce those crucial details below and defer the remaining to appendix.</p>
<p><strong>Nodes in the relation graph:</strong> In the previous content, we assume K pre-selected semantic concepts as graph nodes. However, it is not rare to reach some situation that cannot be categorized into any existing semantic category. In practice, we treat $$ c_t = 0 $$ as a special "unknown" concept. Hence, the BRM module actually contains K + 1 nodes. This is conceptually similar to natural language processing: a semantic concept is a word; the set of all concepts can be viewed as the dictionary; and $$ c_t = 0 $$ corresponds to the special "out-of-vocabulary" token.</p>
<p><strong>Smooth temporal classification:</strong> Although the semantic classifier achieves high accuracy on validation data, the predictions may not be temporally consistent, which brings extra noise to the BRM module. For temporally smooth prediction at test time, we set a restricted threshold over the sigmoid output and apply a filtering process on top of that: the actual prediction label for room type T_i will be 1 only if the sigmoid output remains at least 0.9 confidence score for consecutively 3 time steps.</p>
<p><strong>Graph learning:</strong> For learning $$ \psi^{\text{prior}} $$, we run a random exploration of 300 steps and collect 50 samples for each $$ z_{i,j} $$ per training environment. Also, learning $$ \psi^{\text{prior}} $$ does not depend on the locomotion $$ \mu(s_t, g; \theta) $$ and can be reused even with different control policies. $$ \psi^{\text{obs}} $$ depends on the locomotion so it must be learned after $$ \mu(s_t, g; \theta^*) $$ is obtained.</p>
<h2>4. Experiments</h2>
<p>We experiment on the House3D environment and proceed to answer the following two questions: (1) Does the BRM agent capture the underlying semantic relations and behave as we expected? (2) Does the BRM agent generalize better in test environments than the baseline methods?</p>
<p>We first introduce evaluation preliminaries and baseline methods in Sec. 4.1, 4.2. Then we answer the first question qualitatively in Sec. 4.3. In Sec. 4.4, 4.6, we quantitatively show that our BRM agents generally achieve higher test success rates (i.e., better generalization) and spend fewer steps to reach the targets (i.e., more efficient) than all the baselines. We choose a fixed N = 10 for all the BRM agents. Ablation study on the design choices of BRM is presented in Sec. 4.5. More details can be found in appendix.</p>
<h3>4.1. Preliminaries</h3>
<p>We consider the RoomNav task on the House3D environment [58] where K = 8 room types are selected as the semantic goals, including "kitchen", "living room", "dining room", "bedroom", "bathroom", "office", "garage" and "outdoor". The House3D environment provides a success check for whether the agent has reached a specific room target or not while we also experimented on the setting of the agent predicting termination on its own (Sec. 4.6). House3D provides a training set of 200 houses, a test set of 50 houses and a validation set of 20 houses. At training</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6. Qualitative comparison between BRM (purple), basic DRL policy (pure µ(θ), blue) and random policy (red) with horizon H = 1000 (Sec. 4.3). Y-axis is the test success rate and x-axis is the distance in meters to the target. When targets become farther away from the starting point, the success rate of BRM stays high while the basic DRL policy quickly degenerates to random.</p>
<p>time, all the approaches adopt the ground-truth semantic labels regardless of the semantic classifier.</p>
<p>We evaluate the generalization performances of different approaches with two metrics, <em>success rate</em> and <em>Success weighted by Path Length (SPL)</em>, under different horizons. SPL, proposed by Anderson et al.[2], is a function of both success rate and episode length defined by $\frac{1}{C} \sum_i S_i \frac{L_i}{\max (L_i, P_i)}$, where C is total episodes evaluated, S_i indicates whether the episode is success or not, L_i is the ground truth shortest path distance in the episode, P_i is the number of steps the agent actually took. SPL is upper-bounded by success rate and assigns more credits to agents accomplishing their tasks faster.</p>
<h3>4.2. Baseline Methods</h3>
<p><strong>Random policy:</strong> The agent samples a random action per step, denoted by "random".</p>
<p><strong>Pure DRL agent:</strong> This LSTM agent does not have the BRM module and directly executes the policy µ(s_t, T; θ) throughout the entire episode, denoted by "pure µ(θ)". This is in fact the pure locomotion module. As discussed in Sec. 3.4, this is also an improved version of the original policy proposed by Wu et al. [58].</p>
<p><strong>Semantic augmented agent:</strong> Comparing to the pure DRL agent, our BRM agents utilizes an extra semantic signal's c_t provided by the semantic classifier in addition to the visual input s_t. Hence, we consider a semantic-aware locomotion baseline µ_S(θ_s), which is another LSTM DRL agent that takes both s_t and c_t as input (denoted by "aug.µ_S(θ_s)").</p>
<p><strong>HRL agent with an RNN controller:</strong> From a DRL perspective, our BRM agent is a hierarchical reinforcement learning (HRL) agent with the BRM module as a high-level controller producing sub-goals and the locomotion module as a low-level policy for control. Note that update and planning on BRM only depend on (1) the current semantic signal c_t, (2) the target T, and (3) the accumulative bit-OR feature B (see Sec. 3.3). Hence, we adopt the same locomotion µ(s_t, g; θ) used by our BRM agent, and train an LSTM controller with 50 hidden units on E_train that takes all the necessary semantic information and produces a sub-target every N steps as well. The only difference between our BRM agent and this HRL agent is the representation of the controller (memory) module. The LSTM controller has access to exactly the same semantic information as BRM and uses a much more complex and generic neural model instead of a relation graph. Thus, we expect it to be a strong baseline and perform competitively to our BRM agent.</p>
<h3>4.3. Qualitative Analysis and Case Study</h3>
<p>In this section, we qualitatively illustrate that our BRM agent is able to learn reasonable semantic relations and behave in an interpretable manner.</p>
<p><strong>Prior knowledge:</strong> Fig. 5 visualizes P(z|ψ^{prior}), the learned prior probability of relations, for 3 room types with their most and least likely nearby (connected) rooms. Darker red means more likely while darker blue implies less likely. The captured knowledge is indeed reasonable: bathroom is likely to connect to a bedroom; kitchen is often near a dining room while garage is typically outdoor.</p>
<p><strong>Effectiveness of planning:</strong> The BRM agent can effectively decompose a long-term goal into a sequence of easier sub-goals via graph planning. Fig. 6 visualizes the test success rates of the BRM agent (BRM), random policy ("random") and the pure locomotion policy ("pure µ(θ)") for increasingly further targets over a fixed set of 5689 randomly generated test tasks. The x-axis is the shortest distance to the target in meters<sup>4</sup>. As expected, when the target becomes more distant, all methods have lower success rates. However, as opposed to the pure locomotion policy, which quickly degenerates to random as the distance increases, the BRM agent remains a much higher success rate in general.</p>
<p><strong>Case study:</strong> Fig. 7 shows a successful trajectory by the BRM agent, where the final target is to get out of the house. We visualize the progression of the episode, describe the plans and show the updated graph during exploration. Note that the final goal is invisible to the agent initially (frame ①) but the agent is able to plan, effectively explore the house (e.g., without ever entering the bottom-right dining room region), and eventually reach the target.</p>
<h3>4.4. Quantitative Generalization Performances</h3>
<p>We evaluate the generalization performances of different approaches on E_test with horizons H = 300 and H = 1000. We set N = 10, i.e., memory updated every 10 steps. Tab. 1 reports both success rates (%, percent) and SPL values (‰, per mile) over 5689 fixed test tasks. In addition to the overall performances, we also report the results under different planning distances, i.e., the shortest sequence of sub-goals on the ground-truth relation graph. For an accurate measurement, we ensure that there are at least 500 test tasks for each planning distance.</p>
<p><sup>4</sup>Typically one meter in shortest distance requires 3 to 4 actions.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7. Example of a successful trajectory. The agent is spawned inside the house, targeting "outdoor". Left: the 2D top-down map with goal-conditioned trajectories ("outdoor" – orange; "garage" – blue; "living room" – green); Right, 1st row: RGB visual image; Right, 2nd row: the posterior of the semantic graph and the proposed sub-goals (red arrow). Initially, the agent starts by executing the locomotion for "outdoor" and then "garage" according to the prior knowledge (1st graph), but both fail (top orange and blue trajectories in the map). After updating its belief that garage and outdoor are not nearby (grey edges in the 2nd graph), it then executes locomotion for "living room" with success (red arrow in the 2nd graph, green trajectory). Finally, it executes sub-policy for "outdoor" again, explores the living room and reaches the goal (3rd graph, bottom orange trajectory).</p>
<p>Our BRM agent has the highest average success rates as well as the best SPL values in <em>all</em> the cases. Notably, the margin in SPL is much more significant than that in pure success rate. More importantly, as the horizon increases, i.e., the larger number of planning computations <em>H/N</em> allowed, the overall performance margin (rightmost column) of BRM over the best remaining baseline strictly increases, thanks to the effectiveness of planning.</p>
<h3>4.5. Ablation Study</h3>
<p>In this section, we show the necessity of all the BRM components and the direction for future improvement.</p>
<p><strong>Benefits of Learned Prior:</strong> In BRM, the prior $$P(z|\psi^{\text{prior}})$$ is learned from training houses. Tab. 2 evaluates BRM with an <em>uninformative prior</em> ("unif."), i.e. $$\psi^{\text{prior}}_{i,j} = 0.5$$. Generally, the learned prior leads to better success rates and SPL values. Notably, when horizon becomes longer, the gap becomes much smaller, since the graph will converge to the true posterior with more memory updates.</p>
<p><strong>Source of Error:</strong> Our approach has two modules, a BRM module for planning and a locomotion module for control. We study the errors caused by each of these components by introducing (1) a hand-designed (imperfect) oracle locomotion, which automatically gets closer to nearby targets, and (2) an optimal planner using House3D labels. The evaluation results are shown in Tab. 3, where the oracle locomotion drastically boosts the results while the BRM performance is close to the optimal planner. This indicates that the error is mainly from locomotion – it is extremely challenging to learn a single neural navigator, which motivates our work to decompose a long-term task into sub-tasks.</p>
<p><strong>Choice of</strong> <em>N</em>: The oracle steps for reaching a nearby, i.e., 1-plan-step, target is around 12.27 (top in Tab. 1), so we choose a slightly smaller value <em>N</em> = 10 as the re-planning step size. We investigate other choices of <em>N</em> in Tab. 4. Larger <em>N</em> results in more significant performance drops. Notably, our BRM agent consistently outperforms RNN controller under different parameter choices.</p>
<h3>4.6. Evaluation with Terminate Action</h3>
<p>In the previous studies, the success of an episode is determined by the House3D environment automatically. It is suggested by Anderson et al. [2] that a real-world agent should be aware of its goal and determine whether to stop by itself. In this section, we evaluate the BRM agent and all previous baselines under this setting: a success will be counted only if the agent terminates the episode correctly in a target room on its own.</p>
<p>There are two ways to include the terminate action: (1) expand the action space with an extra stop action; (2) train a separate termination checker. We observe (2) leads to much better practical performances, which is also reported by Pathak et al. [43]. In our experiments, we simply use the semantic classifier as our termination checker.</p>
<p>The results are summarized in Tab. 5, where we use a long horizon <em>H</em> = 1000 to allow the agents to have enough time to self-terminate. Similarly, BRM achieves the best performance in both success rate and SPL metric.</p>
<h3>4.7. Discussions</h3>
<p><strong>Success rate and SPL:</strong> In Tab. 1, the SPL values are typically much smaller than the success rates, namely BRM uses much more steps than the reference shortest path. This is not surprising due to the strong partial observability in the RoomNav task. As a concrete example in Fig. 7, the optimal path from birthplace (near ①) to the outdoor (near</p>
<table>
<thead>
<tr>
<th>opt. plan-steps</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>avg. oracle steps</td>
<td>12.27</td>
<td>42.53</td>
<td>61.09</td>
<td>72.47</td>
<td>63.74</td>
<td>46.86</td>
</tr>
<tr>
<td>Horizon $H=300$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>random</td>
<td>$20.5 / 15.9$</td>
<td>$6.9 / 16.7$</td>
<td>$3.8 / 10.7$</td>
<td>$1.6 / 4.2$</td>
<td>$3.0 / 8.8$</td>
<td>$7.2 / 13.6$</td>
</tr>
<tr>
<td>pure $\mu(\theta)$</td>
<td>$49.4 / 47.6$</td>
<td>$11.8 / 27.6$</td>
<td>$2.0 / 4.8$</td>
<td>$2.6 / 10.8$</td>
<td>$4.2 / 13.2$</td>
<td>$13.1 / 22.9$</td>
</tr>
<tr>
<td>aug. $\mu_{S}(\theta)$</td>
<td>$47.8 / 45.3$</td>
<td>$11.4 / 23.1$</td>
<td>$3.0 / 7.8$</td>
<td>$3.4 / 8.1$</td>
<td>$4.4 / 11.2$</td>
<td>$13.0 / 20.5$</td>
</tr>
<tr>
<td>RNN control.</td>
<td>$55.0 / 49.8$</td>
<td>$20.0 / 40.8$</td>
<td>$8.0 / 20.1$</td>
<td>$5.2 / 15.2$</td>
<td>$11.0 / 25.2$</td>
<td>$19.9 / 34.2$</td>
</tr>
<tr>
<td>BRM</td>
<td>$\mathbf{5 7 . 8 / 6 5 . 4}$</td>
<td>$\mathbf{2 4 . 4 / 5 4 . 3}$</td>
<td>$\mathbf{1 0 . 5 / 2 8 . 3}$</td>
<td>$\mathbf{5 . 8 / 1 8 . 6}$</td>
<td>$\mathbf{1 1 . 2 / 2 9 . 8}$</td>
<td>$\mathbf{2 3 . 1 / 4 5 . 3}$</td>
</tr>
<tr>
<td>Horizon $H=1000$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>plan-dist</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>avg.</td>
</tr>
<tr>
<td>random</td>
<td>$24.3 / 17.6$</td>
<td>$13.5 / 20.3$</td>
<td>$9.1 / 14.3$</td>
<td>$8.0 / 9.3$</td>
<td>$7.0 / 11.5$</td>
<td>$13.0 / 17.0$</td>
</tr>
<tr>
<td>pure $\mu(\theta)$</td>
<td>$60.8 / 47.6$</td>
<td>$23.3 / 27.6$</td>
<td>$7.6 / 4.8$</td>
<td>$8.2 / 10.8$</td>
<td>$11.0 / 13.2$</td>
<td>$22.5 / 22.9$</td>
</tr>
<tr>
<td>aug. $\mu_{S}(\theta)$</td>
<td>$61.3 / 50.1$</td>
<td>$23.0 / 26.2$</td>
<td>$9.4 / 12.0$</td>
<td>$5.8 / 9.6$</td>
<td>$9.0 / 13.6$</td>
<td>$22.4 / 23.8$</td>
</tr>
<tr>
<td>RNN control.</td>
<td>$71.0 / 58.0$</td>
<td>$39.6 / 51.3$</td>
<td>$24.1 / 32.7$</td>
<td>$16.6 / 25.6$</td>
<td>$23.2 / 39.6$</td>
<td>$37.0 / 45.2$</td>
</tr>
<tr>
<td>BRM</td>
<td>$\mathbf{7 3 . 7 / 7 4 . 9}$</td>
<td>$\mathbf{4 3 . 6 / 6 6 . 0}$</td>
<td>$\mathbf{2 9 . 2 / 4 4 . 9}$</td>
<td>$\mathbf{2 0 . 4 / 2 7 . 1}$</td>
<td>$\mathbf{2 8 . 4 / 4 2 . 5}$</td>
<td>$\mathbf{4 1 . 1 / 5 7 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1. Metrics of Success Rate(\%) / SPL(\%o, per mile) evaluating the generalization performances of BRM and all the baseline approaches (Sec. 4.4). Here $N=10$. "plan-steps" denotes the shortest planning distance in the ground truth relation graph. "oracle steps" denotes the reference shortest steps required to reach the goal. Our BRM agents have the highest success rates and the best SPL values in all the cases. More importantly, as the horizon increases, which allows more planning, BRM outperforms the baselines more.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">BRM $(H=300)$</th>
<th style="text-align: left;">unif. $(H=300)$</th>
<th style="text-align: left;">BRM $(H=1 \mathrm{k})$</th>
<th style="text-align: left;">unif. $(H=1 \mathrm{k})$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbf{2 3 . 1 / 4 5 . 3}$</td>
<td style="text-align: left;">$20.9 / 39.4$</td>
<td style="text-align: left;">$\mathbf{4 1 . 1 / 5 7 . 5}$</td>
<td style="text-align: left;">$40.4 / 56.6$</td>
</tr>
</tbody>
</table>
<p>Table 2. Success Rate(\%) / SPL(\%o): performances of BRM with learned and uninformative prior. $N=10, H=300,1000$ (" 1 k ").</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Src. of Err.</th>
<th style="text-align: center;">LSTM locomotion</th>
<th style="text-align: center;">oracle locomotion</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BRM</td>
<td style="text-align: center;">$41.1 / 57.5$</td>
<td style="text-align: center;">$88.6 /$ N.A.</td>
</tr>
<tr>
<td style="text-align: center;">opt. plan</td>
<td style="text-align: center;">$46.3 / 62.5$</td>
<td style="text-align: center;">$96.7 /$ N.A.</td>
</tr>
</tbody>
</table>
<p>Table 3. Success Rate(\%) / SPL(\%o): performances with an optimal planner and a hand-designed locomotion. $H=1000, N=10$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Choice of $N$</th>
<th style="text-align: center;">$N=10$</th>
<th style="text-align: center;">$N=30$</th>
<th style="text-align: center;">$N=50$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">BRM</td>
<td style="text-align: center;">$\mathbf{4 1 . 1 / 5 7 . 5}$</td>
<td style="text-align: center;">$29.7 / 35.2$</td>
<td style="text-align: center;">$27.4 / 32.2$</td>
</tr>
<tr>
<td style="text-align: center;">RNN control.</td>
<td style="text-align: center;">$37.0 / 45.2$</td>
<td style="text-align: center;">$28.2 / 27.7$</td>
<td style="text-align: center;">$26.5 / 26.7$</td>
</tr>
</tbody>
</table>
<p>Table 4. Success Rate(\%) / SPL(\%o): performances with different choices of $N$ under horizon $H=1000$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Horizon $H=1000$ with Terminate Action</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">random</td>
<td style="text-align: center;">pure $\mu(\theta)$</td>
<td style="text-align: center;">aug. $\mu_{S}\left(\theta_{s}\right)$</td>
<td style="text-align: center;">RNN cont.</td>
<td style="text-align: center;">BRM</td>
</tr>
<tr>
<td style="text-align: center;">$1.8 / 1.2$</td>
<td style="text-align: center;">$8.6 / 9.0$</td>
<td style="text-align: center;">$8.2 / 8.8$</td>
<td style="text-align: center;">$14.5 / 16.3$</td>
<td style="text-align: center;">$\mathbf{1 7 . 3 / 2 3 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 5. Success Rate(\%) / SPL(\%o) with terminate action evaluating the generalization performances of BRM and baseline agents with horizon $H=1000$ and $N=10$. Our BRM agent achieves the best performances under both metrics.
(3) is extremely short if we know the top-down view in advance. However, the outdoor region is out of the agent's sight (frame (1) and the agent has to explore the nearby regions before it sees the door towards the outside (frame (3). Planning and updates on BRM helps guide the agent to explore the unknown house more effectively, which helps lead to higher SPL values. But overall the agent still suffers from the fundamental challenge of partial observability.</p>
<p>Pre-selected concepts: In this work, we focus on the memory representation and simply assume we know all the semantic concepts in advance. It is also possible to generalize to unseen concepts by leveraging the word embedding and knowledge graph from NLP community [60]. It is also feasible to directly discover general semantic concepts from $\mathcal{E}_{\text {train }}$ via unsupervised learning by leveraging the rich semantic information (e.g., object categories, room types, etc) within the visual input. We leave this to our future work.</p>
<h2>5. Conclusion</h2>
<p>In this work, we proposed a novel design of memory architecture, Bayesian Relation Memory (BRM), for the semantic navigation task. BRM uses a semantic classifier to extract semantic labels from visual input and builds a probabilistic relation graph over the semantic concepts, which allows representing prior reachability knowledge via the edge priors, fast test-time adaptation via edge posteriors and efficient planning via graph search. Our BRM navigation agent uses BRM to produce a sub-goal for the locomotion policy to reach. Experiment results show that the BRM representation is effective and crucial for a visual navigation agent to generalize better in unseen environments.</p>
<p>At a high-level, our approach is general and can be applied to other tasks with semantic context information or state abstractions available to build a graph over, such as robotics manipulations where semantic concepts can be abstract states of robot arms and object locations, or video games where we can plan on semantic signals such as the game status or current resources. In future work, it is also worthwhile to investigate how to extract relations and concepts directly from training environments automatically.</p>
<h2>References</h2>
<p>[1] Ankesh Anand, Eugene Belilovsky, Kyle Kastner, Hugo Larochelle, and Aaron Courville. Blindfold baselines for embodied QA. arXiv preprint arXiv:1811.05013, 2018.
[2] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018.
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 36743683, 2018.
[4] Francisco Bonin-Font, Alberto Ortiz, and Gabriel Oliver. Visual navigation for mobile robots: A survey. Journal of intelligent and robotic systems, 53(3):263-296, 2008.
[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGBD data in indoor environments. International Conference on 3D Vision (3DV), 2017.
[6] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan Salakhutdinov. Gated-attention architectures for taskoriented language grounding. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[7] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct perception in autonomous driving. In The IEEE International Conference on Computer Vision (ICCV), December 2015.
[8] Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei Xia, Alvaro Soto, Marynel Vazquez, and Silvio Savarese. A behavioral approach to visual navigation with graph localization networks. arXiv preprint arXiv:1903.00445, 2019.
[9] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual reasoning beyond convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7239-7248, 2018.
[10] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answering. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
[11] Abhishek Das, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Neural modular control for embodied question answering. In Conference on Robot Learning, pages 53-62, 2018.
[12] Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. In Advances in neural information processing systems, pages 271-278, 1993.
[13] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 1-16, 2017.
[14] Kenji Doya, Kazuyuki Samejima, Ken-ichi Katagiri, and Mitsuo Kawato. Multiple model-based reinforcement learning. Neural computation, 14(6):1347-1369, 2002.
[15] Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. $\mathrm{RL}^{2}$ : Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
[16] Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part i. IEEE robotics \&amp; automation magazine, 13(2):99-110, 2006.
[17] Alberto Elfes. Sonar-based real-world mapping and navigation. IEEE Journal on Robotics and Automation, 3(3):249265, 1987.
[18] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for embodied agents in long-horizon tasks. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
[19] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Modelagnostic meta-learning for fast adaptation of deep networks. International Conference on Machine Learning, 2017.
[20] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In Advances in neural information processing systems, pages 64-72, 2016.
[21] Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In International Conference on on Robotics and Automation, pages 2786-2793. IEEE, 2017.
[22] Patrick Foo, William H Warren, Andrew Duchon, and Michael J Tarr. Do humans integrate routes into a cognitive map? map-versus landmark-based navigation of novel shortcuts. Journal of Experimental Psychology: Learning, Memory, and Cognition, 31(2):195, 2005.
[23] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 3318-3329, 2018.
[24] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[25] Saurabh Gupta, David Fouhey, Sergey Levine, and Jitendra Malik. Unifying map and landmark based representations for visual navigation. arXiv preprint arXiv:1712.08125, 2017.
[26] Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modeling relationships in referential expressions with compositional modular networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1115-1124, 2017.
[27] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. International Conference on Learning Representation, 2017.
[28] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2901-2910, 2017.</p>
<p>[29] Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3668-3678, 2015.
[30] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3D environment for visual AI. arXiv preprint arXiv:1712.05474, 2017.
[31] Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. In Advances in neural information processing systems, pages 3675-3683, 2016.
[32] Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart J Russell, and Pieter Abbeel. Learning plannable representations with causal infogan. In Advances in Neural Information Processing Systems, pages 8747-8758, 2018.
[33] Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun, and Sanja Fidler. Situation recognition with graph neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4173-4182, 2017.
[34] Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using knowledge graphs for image classification. IEEE Conference on Computer Vision and Pattern Recognition, 2017.
[35] Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in cities without a map. Advances in Neural Information Processing Systems, pages 2419-2430, 2018.
[36] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andy Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learning to navigate in complex environments. International Conference on Learning Representation, 2017.
[37] Dmytro Mishkin, Alexey Dosovitskiy, and Vladlen Koltun. Benchmarking classic and learned navigation in complex 3D environments. arXiv preprint arXiv:1901.10915, 2019.
[38] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. International Conference on Learning Representation, 2018.
[39] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1004-1015, 2017.
[40] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.
[41] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pages 2863-2871, 2015.
[42] Emilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforcement learning. International Conference on Learning Representation, 2018.
[43] Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shelhamer, Jitendra Malik, Alexei A. Efros, and Trevor Darrell. Zero-shot visual imitation. In International Conference on Learning Representation, 2018.
[44] Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave, Tom Van de Wiele, Volodymyr Mnih, Nicolas Heess, and Jost Tobias Springenberg. Learning by playing-solving sparse reward tasks from scratch. International Conference on Machine Learning, 2018.
[45] Stéphane Ross and Joelle Pineau. Model-based bayesian reinforcement learning in large structured domains. In Uncertainty in artificial intelligence: proceedings of the... conference. Conference on Uncertainty in Artificial Intelligence, volume 2008, page 476. NIH Public Access, 2008.
[46] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. International Conference on Learning Representation, 2018.
[47] Nikolay Savinov, Anton Raichuk, Damien Vincent, Raphael Marinier, Marc Pollefeys, Timothy Lillicrap, and Sylvain Gelly. Episodic curiosity through reachability. In International Conference on Learning Representations, 2019.
[48] Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. Minos: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931, 2017.
[49] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.
[50] Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, and Pieter Abbeel. Value iteration networks. In Advances in Neural Information Processing Systems, pages 2154-2162, 2016.
[51] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. Probabilistic robotics. MIT press, 2005.
[52] Antonio Torralba, Kevin P Murphy, William T Freeman, and Mark A Rubin. Context-based vision system for place and object recognition. In Proceedings of the Ninth IEEE International Conference on Computer Vision-Volume 2, page 273. IEEE Computer Society, 2003.
[53] Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Probabilistic neural-symbolic models for interpretable visual question answering. In International Conference on Machine Learning, 2019.
[54] Ranxiao Frances Wang and Elizabeth S Spelke. Human spatial representation: Insights from animals. Trends in cognitive sciences, 6(9):376-382, 2002.
[55] Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 37-53, 2018.
[56] Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embeddings and knowledge graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6857-6866, 2018.</p>
<p>[57] Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4622-4630, 2016.
[58] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian. Building generalizable agents with a realistic and rich 3D environment. arXiv preprint arXiv:1801.02209, 2018.
[59] Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End-to-end learning of driving models from large-scale video datasets. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2174-2182, 2017.
[60] Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, and Roozbeh Mottaghi. Visual semantic navigation using scene priors. In International Conference on Learning Representation, 2019.
[61] Amy Zhang, Adam Lerer, Sainbayar Sukhbaatar, Rob Fergus, and Arthur Szlam. Composable planning with attributes. In International Conference on Machine Learning, 2018.
[62] Hanwang Zhang, Zawlin Kyaw, Shih-Fu Chang, and TatSeng Chua. Visual translation embedding network for visual relation detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5532-5540, 2017.
[63] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3357-3364. IEEE, 2017.
[64] Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, and Sanja Fidler. segdeepm: Exploiting segmentation and context in deep neural networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4703-4711, 2015.</p>
<h2>A. Video Demo</h2>
<p>A video demo visualizing a successful navigation trajectory by BRM can be found at the following url:
https://drive.google.com/file/d/
lvCFQEfFK1K6WJacrQID2kMQVzRD4We?s/view?
usp=sharing.</p>
<h2>B. Environment Details</h2>
<p>In RoomNav the 8 targets are: kitchen, living room, dining room, bedroom, bathroom, office, garage and outdoor. We inherit the success measure of "see" from [58]: the agent needs to see some corresponding object for at least 450 pixels in the input frame and stay in the target area for at least 3 time steps.</p>
<p>Originally the House3D environment supports a set of 13 discrete actions. Here we reduce it to 9 actions: large forward, forward, left-forward, right-forward, large left rotate, large right rotate, left rotate, right rotate and stay still. More environment details can be found in the appendix of Wu et al. [58]. We also implemented a faster and customized variant of the House3D environment, which is available at https://github.com/jxwuyi/ House3D/tree/C++.</p>
<h2>C. Evaluation Details</h2>
<p>We measure the success rate on $\mathcal{E}_{\text {test }}$ over 5689 test episodes, which consists of 5000 randomly generated configurations and 689 specialized for faraway targets to increase the confidence of measured success rates. These 689 episodes are generated such that for each plan-distance, there are at least 500 evaluation episodes. Each test episode has a fixed configuration for a fair comparison between different approaches, i.e., the agent will always start from the same location with the same target in that episode. Note that we always ensure that (1) the target is connected to the birthplace of the agent, and (2) the the birthplace of the agent is never within the target room. In addition to the detailed numbers in Table 1, we visualize the success rates with confidence intervals for BRM and baseline methods in Figure 8. The confidence interval is obtained by fitting a binomial distribution.</p>
<h2>D. Ablation Study: the Semantic Detector</h2>
<p>In BRM, we use a CNN detector to extract the semantic signals at test time. Here we also evaluate the performances of all the approaches using the oracle signals from the House3D environment. The results are in Table 6, where we also include the BRM agent using CNN detector as a reference. Generally, using both the ground truth signal and using the CNN detector yield comparable overall performances in both metrics of success rate and SPL. They all consistently outperform all the baseline methods, which indicates that the probabilistic relational graph is robust over the noise on semantic signals (the robustness if controlled by $\psi^{s b s}$ ). One interesting observation is that there are many cases, using CNN detector produces better results than using the ground truth signals. We hypothesis that this is because the semantic labels in House3D is noisy and therefore a well-trained CNN detector will not be influenced by the noisy labels at test time.</p>
<h2>E. Additional Results on Episode Length</h2>
<p>We illustrate the ground truth shortest distance information as well as the average episode length of success episodes for all the approaches. The results are shown in Table 7. The average ground truth shortest path is around 46.86 steps. Note that the agent has 9 actions per step and suffers from strong partial observability, which indicates the difficulty of the task.</p>
<h2>F. Additional Implementation Details</h2>
<p>The source code is available at https://github.com/ jxwuyi/HouseNavAgent.</p>
<h2>F.1. Learning the LSTM Locomotion</h2>
<p>Policy Architecture: We utilize the same policy architecture and settings as [58]: we have 4 convolution layers of 64, 64, 128, 128 channels each and with kernel size 5 and stride 2, an MLP layer of 256 units, an LSTM cell of 256 units, two MLP layers of 126 and 64 units for policy head and another 2 MLP layers of 64 and 32 units for value head. Batch normalization is applied to all the layers before LSTM. Activation is ReLU. The a only difference is that the original policy uses a gated attention mechanism for target conditioning while we use a behavior approach by training a separate sub-policy for each semantic target.</p>
<p>For the semantic augmented policy, we feed the semantic information to the MLP layer before LSTM.</p>
<p>Hyperparameters: We run a parallel version of A2C [40] with 1 optimizer and 200 parallel rollout workers, each of which simulates a particular training house. We collect a training batch of 64 trajectories with 30 continuous time steps in each iteration. We set $\gamma=0.97$, batch size 64, learning rate 0.001 with Adam, weight decay $10^{-5}$ and entropy bonus 0.1 . We also add the squared $l_{2}$ norm of policy logits to the total loss with a coefficient of 0.01 . We normalize the advantage to mean 0 and standard deviation 1. We totally run 60000 training iterations and use the final model as our learned policy.</p>
<p>Reward shaping: The reward at each time step is computed by the difference of shortest paths in meters from the agent's location to the goal after taking a action. We also add a time penalty of 0.1 and a collision penalty of 0.3 . When the agent reaches the goal, the success reward is 10 .</p>
<p>Curriculum learning: We run a curriculum learning by increasing the maximum of distance between agent's birth meters and target by 3 meters every 10000 iterations. We totally run 60000 training iterations and use the final model as our learned policy $\mu(\theta)$.</p>
<h2>F.2. Building the Relational Graph</h2>
<p>We run random exploration for 300 steps to collect a sample of $z$. For a particular environment, we collect totally 50 samples for each $z_{i, j}$. For all $i \neq j$, we set $\psi_{i, j, 0}^{s b s}=0.001$ and $\psi_{i, j, 1}^{s b s}=0.15$.</p>
<h2>F.3. Training the CNN Semantic Extractor</h2>
<p>We take the panoramic view as input, which consists of 4 images, $s_{o}^{1}, \ldots, s_{o}^{4}$ with different first person view angles. The only exception is that for target "outdoor", we notice that instead of using a panoramic view, simply keeping the recent 4 frames in the trajectory leads to the best prediction accuracy. We use an CNN feature extractor to extract features $f\left(s_{o}^{i}\right)$ by applying CNN layers</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8. Comparing BRM with baselines in success rate with confidence interval. Approaches of interest include random policy (red), pure LSTM policy (blue), the semantic-aware LSTM policy (purple), the hierarchical policy (grey) and BRM (yellow). In all plots, the y-axis is success rate while the x-axis is the optimal planning distance. BRM outperforms all baselines and the gap becomes more significant when horizon increase, namely, more planning computations.</p>
<table>
<thead>
<tr>
<th>plan-dist</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>overall</th>
</tr>
</thead>
<tbody>
<tr>
<td>Horizon $H=300$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>plan-dist</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>avg.</td>
</tr>
<tr>
<td>random</td>
<td>$20.5 / 15.9$</td>
<td>$6.9 / 16.7$</td>
<td>$3.8 / 10.7$</td>
<td>$1.6 / 4.2$</td>
<td>$3.0 / 8.8$</td>
<td>$7.2 / 13.6$</td>
</tr>
<tr>
<td>pure $\mu(\theta)$</td>
<td>$49.4 / 47.6$</td>
<td>$11.8 / 27.6$</td>
<td>$2.0 / 4.8$</td>
<td>$2.6 / 10.8$</td>
<td>$4.2 / 13.2$</td>
<td>$13.1 / 22.9$</td>
</tr>
<tr>
<td>aug. $\mu_{S}(\theta)$ (true)</td>
<td>$51.9 / \mathbf{6 6 . 4}$</td>
<td>$11.1 / 24.2$</td>
<td>$3.3 / 7.8$</td>
<td>$2.4 / 6.0$</td>
<td>$3.0 / 8.7$</td>
<td>$13.2 / 23.3$</td>
</tr>
<tr>
<td>RNN control. (true)</td>
<td>$54.9 / 48.1$</td>
<td>$20.2 / 37.7$</td>
<td>$8.2 / 22.5$</td>
<td>$5.6 / 13.8$</td>
<td>$9.8 / 22.7$</td>
<td>$20.0 / 32.6$</td>
</tr>
<tr>
<td>BRM (true)</td>
<td>$\mathbf{5 8 . 8 / 6 0 . 7}$</td>
<td>$\mathbf{2 5 . 3 / 5 5 . 6}$</td>
<td>$10.4 / 26.9$</td>
<td>$\mathbf{7 . 6 / 2 2 . 2}$</td>
<td>$9.2 / 23.4$</td>
<td>$\mathbf{2 3 . 6 / 4 4 . 9}$</td>
</tr>
<tr>
<td>BRM (CNN)</td>
<td>$57.8 / 65.4$</td>
<td>$24.4 / 54.3$</td>
<td>$\mathbf{1 0 . 5 / 2 8 . 3}$</td>
<td>$5.8 / 18.6$</td>
<td>$\mathbf{1 1 . 2 / 2 9 . 8}$</td>
<td>$23.1 / \mathbf{4 5 . 3}$</td>
</tr>
<tr>
<td>Horizon $H=1000$</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>plan-dist</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>avg.</td>
</tr>
<tr>
<td>random</td>
<td>$24.3 / 17.6$</td>
<td>$13.5 / 20.3$</td>
<td>$9.1 / 14.3$</td>
<td>$8.0 / 9.3$</td>
<td>$7.0 / 11.5$</td>
<td>$13.0 / 17.0$</td>
</tr>
<tr>
<td>pure $\mu(\theta)$</td>
<td>$60.8 / 47.6$</td>
<td>$23.3 / 27.6$</td>
<td>$7.6 / 4.8$</td>
<td>$8.2 / 10.8$</td>
<td>$11.0 / 13.2$</td>
<td>$22.5 / 22.9$</td>
</tr>
<tr>
<td>aug. $\mu_{S}(\theta)$ (true)</td>
<td>$62.4 / 61.3$</td>
<td>$22.9 / 30.7$</td>
<td>$8.9 / 14.3$</td>
<td>$7.2 / 12.8$</td>
<td>$9.0 / 11.4$</td>
<td>$22.5 / 28.1$</td>
</tr>
<tr>
<td>RNN control. (true)</td>
<td>$70.2 / 51.3$</td>
<td>$40.8 / 48.6$</td>
<td>$22.8 / 32.2$</td>
<td>$16.4 / 23.4$</td>
<td>$24.2 / 41.0$</td>
<td>$37.4 / 42.9$</td>
</tr>
<tr>
<td>BRM (true)</td>
<td>$70.3 / 61.8$</td>
<td>$\mathbf{4 4 . 9 / 7 0 . 5}$</td>
<td>$\mathbf{3 1 . 7 / 5 0 . 8}$</td>
<td>$19.0 / \mathbf{3 3 . 3}$</td>
<td>$28.0 / 42.2$</td>
<td>$\mathbf{4 1 . 7 / 5 9 . 8}$</td>
</tr>
<tr>
<td>BRM (CNN)</td>
<td>$\mathbf{7 3 . 7 / 7 4 . 9}$</td>
<td>$43.6 / 66.0$</td>
<td>$29.2 / 44.9$</td>
<td>$\mathbf{2 0 . 4 / 2 7 . 1}$</td>
<td>$\mathbf{2 8 . 4 / 4 2 . 5}$</td>
<td>$41.1 / 57.5$</td>
</tr>
</tbody>
</table>
<p>Table 6. Metrics of Success Rate(\%) / SPL(\%) evaluating the performances of BRM and baselines agents using the ground truth oracle semantic signals provided by the environments. We also include the performance of the original BRM agent using CNN detector as a reference. The performance of BRM-CNN agents is comparable to BRM-true agents and sometimes even better. More discussions are in Sec. D.</p>
<p>with kernel size 3, strides $[1,1,1,2,1,2,1,2,1,2]$ and channels $[4,8,16,16,32,32,64,64,128,256]$. We also use relu activation and batch norm. Then we compute the attention weights over these 4 visual features by $l_{i}=f\left(s_{o}^{i}\right) W_{1}^{T} W_{2}\left[f\left(s_{o}^{i}\right),\ldots, f\left(s_{o}^{4}\right)\right]$ and $\alpha_{i}=\operatorname{softmax}\left(l_{i}\right)$. Then we compute the weighted average of these four frames $g=\sum_{i} a_{i} f\left(s_{o}^{i}\right)$ and feed it to a single layer perceptron with 32 hidden units. For each semantic signal, we generate 15 k positive and 15 k negative training data from $\mathcal{E}<em _text="\text" _valid="{valid">{\text {train }}$ and use Adam optimizer with learning rate $5 e$-4, weight decay $1 e$ - 5 , batch size 256 and gradient clip of 5 . We keep the model that has the best prediction accuracy on $\mathcal{E}</em>$.}</p>
<p>For a smooth prediction during testing, we also have a hard threshold and filtering process on the CNN outputs: $s_{s}\left(T_{i}\right)$ will be 1 only if the output of CNN remains a confidence for $T_{i}$ over 0.9 for consecutively 3 steps.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Average Ground Truth Shortest Path Length</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">plan-dist</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">overall</td>
</tr>
<tr>
<td style="text-align: center;">Oracle</td>
<td style="text-align: center;">12.27</td>
<td style="text-align: center;">42.53</td>
<td style="text-align: center;">61.09</td>
<td style="text-align: center;">72.47</td>
<td style="text-align: center;">63.74</td>
<td style="text-align: center;">46.86</td>
</tr>
<tr>
<td style="text-align: center;">Average Successful Episode Length</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">plan-dist</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">overall</td>
</tr>
<tr>
<td style="text-align: center;">Horizon $H=300$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">random</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">112.7</td>
<td style="text-align: center;">143.8</td>
<td style="text-align: center;">148.0</td>
<td style="text-align: center;">149.7</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;">pure $\mu(\theta)$</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">107.0</td>
<td style="text-align: center;">127.9</td>
<td style="text-align: center;">140.8</td>
<td style="text-align: center;">139.4</td>
<td style="text-align: center;">84.7</td>
</tr>
<tr>
<td style="text-align: center;">aug. $\mu_{S}(\theta)$</td>
<td style="text-align: center;">49.7</td>
<td style="text-align: center;">112.5</td>
<td style="text-align: center;">159.9</td>
<td style="text-align: center;">179.1</td>
<td style="text-align: center;">176.8</td>
<td style="text-align: center;">89.2</td>
</tr>
<tr>
<td style="text-align: center;">RNN control.</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">132.3</td>
<td style="text-align: center;">157.2</td>
<td style="text-align: center;">142.7</td>
<td style="text-align: center;">144.1</td>
<td style="text-align: center;">111.8</td>
</tr>
<tr>
<td style="text-align: center;">BRM</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">124.4</td>
<td style="text-align: center;">167.8</td>
<td style="text-align: center;">150.7</td>
<td style="text-align: center;">127.6</td>
<td style="text-align: center;">107.7</td>
</tr>
<tr>
<td style="text-align: center;">Horizon $H=1000$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">random</td>
<td style="text-align: center;">121.7</td>
<td style="text-align: center;">354.7</td>
<td style="text-align: center;">426.6</td>
<td style="text-align: center;">532.8</td>
<td style="text-align: center;">409.5</td>
<td style="text-align: center;">322.1</td>
</tr>
<tr>
<td style="text-align: center;">pure $\mu(\theta)$</td>
<td style="text-align: center;">55.2</td>
<td style="text-align: center;">107.0</td>
<td style="text-align: center;">127.9</td>
<td style="text-align: center;">140.8</td>
<td style="text-align: center;">139.4</td>
<td style="text-align: center;">84.7</td>
</tr>
<tr>
<td style="text-align: center;">aug. $\mu_{S}(\theta)$</td>
<td style="text-align: center;">163.1</td>
<td style="text-align: center;">360.9</td>
<td style="text-align: center;">471.9</td>
<td style="text-align: center;">460.7</td>
<td style="text-align: center;">432.5</td>
<td style="text-align: center;">307.1</td>
</tr>
<tr>
<td style="text-align: center;">RNN control.</td>
<td style="text-align: center;">174.0</td>
<td style="text-align: center;">368.4</td>
<td style="text-align: center;">465.3</td>
<td style="text-align: center;">466.6</td>
<td style="text-align: center;">397.6</td>
<td style="text-align: center;">339.5</td>
</tr>
<tr>
<td style="text-align: center;">BRM</td>
<td style="text-align: center;">172.9</td>
<td style="text-align: center;">350.5</td>
<td style="text-align: center;">460.0</td>
<td style="text-align: center;">512.3</td>
<td style="text-align: center;">418.1</td>
<td style="text-align: center;">337.0</td>
</tr>
</tbody>
</table>
<p>Table 7. Averaged successful episode length for different approaches. The length of shortest path reflects the strong difficulty of this task.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3121 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3121</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3121</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-354bf043179e3e9f05df73e3f04517e53c326d1f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/354bf043179e3e9f05df73e3f04517e53c326d1f" target="_blank">TALM: Tool Augmented Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> TALM is presented, combining a text-only approach to augment language models with non-differentiable tools, and an iterative"self-play"technique to bootstrap performance starting from few tool demonstrations, suggesting that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.</p>
                <p><strong>Paper Abstract:</strong> Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative"self-play"technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where non-augmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs' capabilities, with less dependence on scale.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3121.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3121.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TALM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool Augmented Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-to-text framework that augments pretrained Seq2Seq language models (T5 family) with non-differentiable external tools via a textual delimiter interface, and uses iterative self-play to bootstrap tool-use examples and finetune the model to call tools and consume tool outputs when solving tasks including arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-based TALM (base / large / XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2Seq T5 models (pretrained T5 family) finetuned as TALM; experiments run on base (≈220M), large, and XL (≈3B) sizes. TALM is implemented as a seq2seq text-to-text model that generates tool-call tokens, receives appended tool outputs, and continues generation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>MathQA word problems expressed as formulas (operation-based formalisms), formula execution, chained arithmetic operations, and large-number arithmetic (e.g., division of large integers yielding decimals); evaluation uses MathQA dataset (word problems + formulas + textual answers).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Delegation of arithmetic computation to an external solver tool: TALM generates a textual tool input (the formula), triggers the solver via a delimiter, receives the exact numeric result from the tool, and conditions its final output on that result; iterative self-play and finetuning teach the model when and how to call the tool and how to format/verify the result.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical improvements on MathQA compared to non-augmented LMs; example sequences show TALM producing solver results (e.g. solver returns 89.3333333333 and TALM outputs 89.33), and TALM correctly handles a large-number division example (2450 / 535 → TALM: 4.58) where the baseline LM fails (LM: 8.5). The paper reports that TALM significantly outperforms non-augmented LMs and that iterative self-play further increases performance. The authors implemented a simple solver tool and found ~70% of MathQA formulas produce results matching text answers according to the solver, and TALM was bootstrapped from valid formula examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No internal probing or interpretability analyses are provided that would show TALM develops internal arithmetic algorithms; improvements could be fully attributable to reliably delegating computation to the external solver. The paper notes dataset noise (wrong formulas or invalid answers) which limits the solver's coverage (~30% mismatch), indicating reliance on tool outputs is not a complete solution for dataset errors.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>External calculator/solver tool (text-to-text API), iterative self-play dataset augmentation, finetuning on tool-use sequences; decoding strategies (random sampling with temperature during self-play; beam decoding at evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Tool augmentation enabled correct numeric outputs on MathQA problems and improved handling of large-number operations where non-augmented LMs err; iterative self-play expanded the tool-use training set and yielded further gains across model sizes (noted large improvements after one round and continued gains over up to three rounds).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper states 'TALM significantly outperforms a non-augmented LM on MathQA' and shows performance curves across model sizes in figures; exact numeric accuracies are not provided in the text. The solver tool reports ~70% of MathQA formulas produce results matching their text answers. Example outputs: solver result 89.3333333333 → final output 89.33; large-number example 2450/535 → TALM: 4.58.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Dataset/formula noise: many MathQA formulas are incorrect or answers invalid (authors report ~30% mismatches), which yields wrong tool outputs if formulas are bad. TALM's approach depends on generating correct tool inputs (formulas) — incorrect tool queries or mis-parsed formulas will produce wrong results. The paper does not report internal LM arithmetic failure modes beyond baseline errors on large numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>TALM effectively uses symbolic/exact computation by delegating arithmetic to an external solver (i.e., behaves like using a calculator); the paper frames this as enabling algorithmic/symbolic computation access rather than forcing the LM to perform arithmetic internally. No direct human comparisons are provided; the solver-tool comparison shows TALM's outputs align with exact numeric computation when the tool and formulas are correct.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALM: Tool Augmented Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3121.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3121.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Non-augmented LM baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained T5 Seq2Seq Language Model (non-augmented)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard pretrained Seq2Seq T5 model finetuned on the target tasks but without external tool access; used as the baseline to compare TALM's tool-augmented performance on MathQA arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (base / large / XL) non-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained T5 family models finetuned directly on task inputs→outputs (no tool-call tokens or external API invocation); same model sizes used for TALM comparisons (base ≈220M up to XL ≈3B).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>MathQA word problems (formula-backed), large-number arithmetic examples (e.g., division that yields decimals), chained formula evaluation when required by the word problem.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Not explicitly analyzed in the paper; evidence and discussion imply the non-augmented LM attempts to produce numeric answers from its parameterized knowledge/patterns and learned heuristics, which can fail especially for numeric operations outside the training distribution (e.g., large numbers). The paper cites prior work noting LMs lack reliable arithmetic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical failure examples: baseline LM produces incorrect numeric outputs on large-number operations (example: 2450 miles / 535 mph → LM outputs 8.5, incorrect), and the baseline is outperformed by TALM across MathQA. The paper references prior work (Brown et al., 2020; Hendrycks et al., 2021) documenting poor large-number and math performance of LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>The paper does not contain probing or ablation analyses that would reveal whether the baseline is memorizing numeric answers, using learned algorithms, or pattern-matching; thus mechanisms are not empirically resolved here.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Compared against interventions (TALM's external solver, tool-use, iterative self-play, finetuning on tool-use sequences). No internal interventions such as chain-of-thought or symbolic program induction were applied to this baseline in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>N/A for baseline itself; when compared to TALM, the baseline's performance is worse on MathQA and particularly poor on large-number arithmetic — demonstrating that external-tool interventions improve arithmetic results over the non-augmented baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports qualitative and plotted comparisons showing TALM > non-augmented LM across sizes on MathQA, but does not list numeric accuracy values in the main text. Example failure given (LM: 8.5 vs TALM: 4.58) illustrates concrete numeric error.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Systematic errors on arithmetic, especially for large numbers and cases underrepresented in training data; produces incorrect decimal arithmetic results (example above). Lacks reliable algorithmic generalization for numeric computation based on the provided examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Baseline LM is contrasted implicitly with symbolic computation by showing that delegating to a solver (TALM) yields correct numeric outputs; the baseline behaves unlike an exact calculator and more like a learned statistical predictor, without explicit symbolic guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALM: Tool Augmented Language Models', 'publication_date_yy_mm': '2022-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mathqa: Towards interpretable math word problem solving with operation-based formalisms <em>(Rating: 2)</em></li>
                <li>Giving bert a calculator: Finding operations and arguments with reading comprehension <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the MATH dataset <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3121",
    "paper_id": "paper-354bf043179e3e9f05df73e3f04517e53c326d1f",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "TALM",
            "name_full": "Tool Augmented Language Model",
            "brief_description": "A text-to-text framework that augments pretrained Seq2Seq language models (T5 family) with non-differentiable external tools via a textual delimiter interface, and uses iterative self-play to bootstrap tool-use examples and finetune the model to call tools and consume tool outputs when solving tasks including arithmetic.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-based TALM (base / large / XL)",
            "model_description": "Seq2Seq T5 models (pretrained T5 family) finetuned as TALM; experiments run on base (≈220M), large, and XL (≈3B) sizes. TALM is implemented as a seq2seq text-to-text model that generates tool-call tokens, receives appended tool outputs, and continues generation.",
            "arithmetic_task_type": "MathQA word problems expressed as formulas (operation-based formalisms), formula execution, chained arithmetic operations, and large-number arithmetic (e.g., division of large integers yielding decimals); evaluation uses MathQA dataset (word problems + formulas + textual answers).",
            "reported_mechanism": "Delegation of arithmetic computation to an external solver tool: TALM generates a textual tool input (the formula), triggers the solver via a delimiter, receives the exact numeric result from the tool, and conditions its final output on that result; iterative self-play and finetuning teach the model when and how to call the tool and how to format/verify the result.",
            "evidence_for_mechanism": "Empirical improvements on MathQA compared to non-augmented LMs; example sequences show TALM producing solver results (e.g. solver returns 89.3333333333 and TALM outputs 89.33), and TALM correctly handles a large-number division example (2450 / 535 → TALM: 4.58) where the baseline LM fails (LM: 8.5). The paper reports that TALM significantly outperforms non-augmented LMs and that iterative self-play further increases performance. The authors implemented a simple solver tool and found ~70% of MathQA formulas produce results matching text answers according to the solver, and TALM was bootstrapped from valid formula examples.",
            "evidence_against_mechanism": "No internal probing or interpretability analyses are provided that would show TALM develops internal arithmetic algorithms; improvements could be fully attributable to reliably delegating computation to the external solver. The paper notes dataset noise (wrong formulas or invalid answers) which limits the solver's coverage (~30% mismatch), indicating reliance on tool outputs is not a complete solution for dataset errors.",
            "intervention_type": "External calculator/solver tool (text-to-text API), iterative self-play dataset augmentation, finetuning on tool-use sequences; decoding strategies (random sampling with temperature during self-play; beam decoding at evaluation).",
            "effect_of_intervention": "Tool augmentation enabled correct numeric outputs on MathQA problems and improved handling of large-number operations where non-augmented LMs err; iterative self-play expanded the tool-use training set and yielded further gains across model sizes (noted large improvements after one round and continued gains over up to three rounds).",
            "performance_metrics": "Paper states 'TALM significantly outperforms a non-augmented LM on MathQA' and shows performance curves across model sizes in figures; exact numeric accuracies are not provided in the text. The solver tool reports ~70% of MathQA formulas produce results matching their text answers. Example outputs: solver result 89.3333333333 → final output 89.33; large-number example 2450/535 → TALM: 4.58.",
            "notable_failure_modes": "Dataset/formula noise: many MathQA formulas are incorrect or answers invalid (authors report ~30% mismatches), which yields wrong tool outputs if formulas are bad. TALM's approach depends on generating correct tool inputs (formulas) — incorrect tool queries or mis-parsed formulas will produce wrong results. The paper does not report internal LM arithmetic failure modes beyond baseline errors on large numbers.",
            "comparison_to_humans_or_symbolic": "TALM effectively uses symbolic/exact computation by delegating arithmetic to an external solver (i.e., behaves like using a calculator); the paper frames this as enabling algorithmic/symbolic computation access rather than forcing the LM to perform arithmetic internally. No direct human comparisons are provided; the solver-tool comparison shows TALM's outputs align with exact numeric computation when the tool and formulas are correct.",
            "uuid": "e3121.0",
            "source_info": {
                "paper_title": "TALM: Tool Augmented Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        },
        {
            "name_short": "Non-augmented LM baseline",
            "name_full": "Pretrained T5 Seq2Seq Language Model (non-augmented)",
            "brief_description": "A standard pretrained Seq2Seq T5 model finetuned on the target tasks but without external tool access; used as the baseline to compare TALM's tool-augmented performance on MathQA arithmetic tasks.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5 (base / large / XL) non-augmented",
            "model_description": "Pretrained T5 family models finetuned directly on task inputs→outputs (no tool-call tokens or external API invocation); same model sizes used for TALM comparisons (base ≈220M up to XL ≈3B).",
            "arithmetic_task_type": "MathQA word problems (formula-backed), large-number arithmetic examples (e.g., division that yields decimals), chained formula evaluation when required by the word problem.",
            "reported_mechanism": "Not explicitly analyzed in the paper; evidence and discussion imply the non-augmented LM attempts to produce numeric answers from its parameterized knowledge/patterns and learned heuristics, which can fail especially for numeric operations outside the training distribution (e.g., large numbers). The paper cites prior work noting LMs lack reliable arithmetic generalization.",
            "evidence_for_mechanism": "Empirical failure examples: baseline LM produces incorrect numeric outputs on large-number operations (example: 2450 miles / 535 mph → LM outputs 8.5, incorrect), and the baseline is outperformed by TALM across MathQA. The paper references prior work (Brown et al., 2020; Hendrycks et al., 2021) documenting poor large-number and math performance of LMs.",
            "evidence_against_mechanism": "The paper does not contain probing or ablation analyses that would reveal whether the baseline is memorizing numeric answers, using learned algorithms, or pattern-matching; thus mechanisms are not empirically resolved here.",
            "intervention_type": "Compared against interventions (TALM's external solver, tool-use, iterative self-play, finetuning on tool-use sequences). No internal interventions such as chain-of-thought or symbolic program induction were applied to this baseline in the reported experiments.",
            "effect_of_intervention": "N/A for baseline itself; when compared to TALM, the baseline's performance is worse on MathQA and particularly poor on large-number arithmetic — demonstrating that external-tool interventions improve arithmetic results over the non-augmented baseline.",
            "performance_metrics": "The paper reports qualitative and plotted comparisons showing TALM &gt; non-augmented LM across sizes on MathQA, but does not list numeric accuracy values in the main text. Example failure given (LM: 8.5 vs TALM: 4.58) illustrates concrete numeric error.",
            "notable_failure_modes": "Systematic errors on arithmetic, especially for large numbers and cases underrepresented in training data; produces incorrect decimal arithmetic results (example above). Lacks reliable algorithmic generalization for numeric computation based on the provided examples.",
            "comparison_to_humans_or_symbolic": "Baseline LM is contrasted implicitly with symbolic computation by showing that delegating to a solver (TALM) yields correct numeric outputs; the baseline behaves unlike an exact calculator and more like a learned statistical predictor, without explicit symbolic guarantees.",
            "uuid": "e3121.1",
            "source_info": {
                "paper_title": "TALM: Tool Augmented Language Models",
                "publication_date_yy_mm": "2022-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mathqa: Towards interpretable math word problem solving with operation-based formalisms",
            "rating": 2
        },
        {
            "paper_title": "Giving bert a calculator: Finding operations and arguments with reading comprehension",
            "rating": 2
        },
        {
            "paper_title": "Measuring mathematical problem solving with the MATH dataset",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.00882675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TALM: Tool Augmented Language Models</h1>
<p>Aaron Parisi Yao Zhao Noah Fiedel<br>{aarontp,yaozhaoyz,nfiedel} @google.com</p>
<h4>Abstract</h4>
<p>Transformer based language models (LMs) demonstrate increasing performance with scale across a wide variety of tasks. Scale alone however cannot enable models to solve tasks that require access to ephemeral, changing, or private data that was unavailable at training time. Many useful tasks may also benefit from LMs being able to access APIs that read or modify state. In this work, we present Tool Augmented Language Models (TALM), combining a text-only approach to augment language models with non-differentiable tools, and an iterative "self-play" technique to bootstrap performance starting from few tool demonstrations. TALM exhibits strong performance on both a knowledge-heavy QA task and a reasoning oriented math task with simple tools. At a given model scale, TALM significantly outperforms non-augmented LMs. We further demonstrate that TALM successfully performs out-of-distribution inferences on both QA and math tasks, where nonaugmented LMs fail. Our results suggest that Tool Augmented Language Models are a promising direction to enrich LMs’ capabilities, with less dependence on scale.</p>
<h2>1 Introduction</h2>
<p>Language models using the Transformer architecture [Vaswani et al., 2017] demonstrate increasing performance at larger scales, e.g. T5 [Raffel et al., 2019], GPT-3 [Brown et al., 2020], and PaLM [Chowdhery et al., 2022]. Scale related performance gains are observed on a variety of benchmarks, e.g. SuperGLUE [Wang et al., 2019] and BIG-bench [BIG-bench collaboration, 2021].</p>
<p>Scaling up has practical downsides. Large scale models are unwieldy to store, transfer, and deploy. Their costs to train or perform inference can be prohibitively high for many researchers and organizations.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Baseline LM and TALM performance on two tasks, with increasing rounds of self-play.</p>
<p>Larger models memorize more world knowledge [Roberts et al., 2020]. While good for many benchmark tasks, relying on memorization alone poses several problems. First, models sometimes generate incorrect outputs that are problematic for some applications. Second, world knowledge is constantly changing. The knowledge from yesterday's training data might be invalid today. Third, large models can memorize parts of their training data with undesirable consequences [Carlini et al., 2022].</p>
<p>Retrieval based approaches to enhancing LMs can lower the dependence on scale. REALM [Guu et al., 2020] learns retrieval via backpropagation from a fixed corpus. RETRO [Borgeaud et al., 2021] adds an "internet scale" retrieval mechanism. RAG [Lewis et al., 2020] uses a dense vector index of Wikipedia, and retrieves either once per token or once per query. Other works demonstrated that LMs can be enhanced on math reasoning with access to a calculator [Andor et al., 2019].</p>
<p>Looking towards the future utility of language models, it is clear that scale and retrieval cannot solve all useful problems. Many knowledge tasks and desirable applications require access to read live or private data (e.g. weather or a person's cal-</p>
<p>endar), or to invoke APIs that modify state. Recent works such as Say Can [Ahn et al., 2022] connect languages models to an environment, though with the model as a recipient of queries. In contrast, TALM's approach enables models to invoke arbitrary tools with model-generated output, and to attend to tool output to generate task outputs.</p>
<p>In summary, our contributions are:</p>
<ul>
<li>Demonstrating that language models can be augmented with tools via a text-to-text API.</li>
<li>Demonstrating an iterative self-play technique to bootstrap tool-augmented datasets and subsequent tool-augmented model performance, from few labeled examples.</li>
</ul>
<h2>2 Methods</h2>
<p>We use pretrained T5 models [Raffel et al., 2019, Roberts et al., 2022] for finetuning, inference and evaluation. To measure the effects of model scaling, we use the base, large, and XL sizes.</p>
<h3>2.1 Tool Augmented Language Models</h3>
<p>Language Model</p>
<table>
<thead>
<tr>
<th style="text-align: left;">input</th>
<th style="text-align: left;">output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tool Augmented Language Model</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">input $\rightarrow$ tool input tool result output</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">call</td>
<td style="text-align: left;">$\uparrow$ append</td>
</tr>
<tr>
<td style="text-align: left;">external tool</td>
<td style="text-align: left;">$\longrightarrow$</td>
</tr>
</tbody>
</table>
<p>Figure 2: LM and Tool Augmented LMs.</p>
<p>We use a Text-to-Text tool interface given its broad applicability and simplicity, as shown in Fig. 3. TALM first generates a tool input conditioned on the task input text and invokes a tool's API by generating a delimiter, such as " $|\text { result }"$. Whenever this delimiter is detected, the tool API is called and its result appended to the text sequence. TALM then continues to generate the final task output.</p>
<p>An abstract task:
task input text |tool-call tool input text |result tool output text |output task output text</p>
<p>A weather task:
how hot will it get in NYC today? |weather lookup region=NYC |result precipitation chance: 10, high temp: 20c, low-temp: 12c |output today's high will be 20C</p>
<p>Figure 3: TALM text-to-text interface example.</p>
<p>TALM learns two subtasks at the same time: calling a tool and generating an answer based on tool results. TALM is architecturally agnostic and can be implemented as Seq2Seq, left-to-right LM or prefix LM. We chose the Seq2Seq family for its high finetuning performance at modest scale [Raffel et al., 2019].</p>
<h3>2.2 Iterative self-play</h3>
<p>When introducing new tools to solve existing tasks, there are often a limited number of demonstrations of tool interactions. However, there is typically plenty of supervised task data consisting of input and target pairs, and automated metrics for evaluating the correctness of a generated output. Inspired by Decision Transformer [Chen et al., 2021], we use a self-play approach to iteratively bootstrap examples of tool-use with progressively higher quality. In this work, we refer to a model interacting with a tool API as self-play rather than adversarial play among models.</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Iterative</span><span class="w"> </span><span class="n">Self</span><span class="o">-</span><span class="n">Play</span><span class="w"> </span><span class="n">Algorithm</span><span class="o">.</span>
<span class="n">x</span><span class="p">:</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">:</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="n">output</span><span class="p">,</span><span class="w"> </span><span class="n">t</span><span class="p">:</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="p">:</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="n">output</span>
<span class="mi">1</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">T</span><span class="o">=</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="w"> </span><span class="n">T</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) task set</span>
<span class="mi">2</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">D</span><span class="o">=</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">t_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="w"> </span><span class="n">D</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) tool-use set</span>
<span class="mi">3</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span><span class="n">P_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">pretrained</span><span class="w"> </span>\<span class="p">(</span><span class="n">L</span><span class="w"> </span><span class="n">M</span>\<span class="p">)</span>
<span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">t</span><span class="w"> </span>\<span class="ow">in</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">]</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) self-play rounds</span>
<span class="mi">5</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) finetune LM</span>
<span class="mi">6</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">theta</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">underset</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}{</span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">argmax</span><span class="p">}}</span><span class="w"> </span>\<span class="n">prod_</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">y_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">t_</span><span class="p">{</span><span class="n">j</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">t_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">j</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span>\<span class="p">)</span>
<span class="mi">7</span><span class="p">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span><span class="n">T</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) iterate task set</span>
<span class="mi">8</span><span class="p">:</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>\<span class="p">(</span><span class="n">n</span><span class="w"> </span>\<span class="ow">in</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span>\<span class="n">ldots</span><span class="p">,</span><span class="w"> </span><span class="n">N</span><span class="p">]</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="mi">9</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span><span class="n">t_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">t</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) sample tool query</span>
<span class="mi">10</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">T</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="k">tool</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">t_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) call tool API</span>
<span class="mi">11</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">P_</span><span class="p">{</span>\<span class="n">theta</span><span class="p">}</span>\<span class="n">left</span><span class="p">(</span><span class="n">y</span><span class="w"> </span>\<span class="n">mid</span><span class="w"> </span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">t_</span><span class="p">{</span><span class="n">n</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">right</span><span class="p">)</span><span class="w"> </span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) get task output</span>
<span class="mi">12</span><span class="p">:</span><span class="w"> </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">left</span><span class="o">|</span><span class="n">y_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span><span class="o">-</span><span class="n">y_</span><span class="p">{</span><span class="n">i</span><span class="p">}</span>\<span class="n">right</span><span class="o">|&lt;</span><span class="n">t</span><span class="w"> </span><span class="n">h</span>\<span class="p">)</span><span class="w"> </span><span class="n">then</span><span class="w"> </span>\<span class="p">(</span>\<span class="c1">#\) filter wrong output</span>
<span class="mi">13</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span><span class="n">D</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">D</span><span class="w"> </span>\<span class="n">cup</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">x_</span><span class="p">{</span><span class="n">i</span><span class="p">},</span><span class="w"> </span><span class="n">t_</span><span class="p">{</span><span class="n">n</span><span class="p">},</span><span class="w"> </span><span class="n">r_</span><span class="p">{</span><span class="n">n</span><span class="p">},</span><span class="w"> </span><span class="n">y_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span>\<span class="p">)</span>
<span class="mi">14</span><span class="p">:</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">quad</span><span class="w"> </span>\<span class="c1">#\) update tool-use set</span>
</code></pre></div>

<p>The iterative self-play pipeline starts with a small tool-use bootstrapping set $\left{x_{j}, t_{j}, r_{j}, y_{j}\right}_{D}$. In each round of self-play, the TALM is fine-tuned on the tool-use set $D$. Next, for every example in the task set $T$, the TALM samples tool inputs, calls a tool API, and samples task outputs based on the tool results. If the TALM generated task output matches the target within some threshold $t h$, the tool-use sequence led to the result is added to the tool-use set $D$ for the next round of self-play.</p>
<p>To explore diverse tool API invocations and answers during self-play, the TALM decodes using random sampling with temperature $=1.0$, and top-</p>
<p>$\mathrm{k}=40$. To grow the dataset during self-play, the TALM generates up to $N=600$ tool-use sequences per example. At evaluation time, the model uses beam decoding with 4 beams to generate a single output.</p>
<p>We note that this iterative self-play pipeline represents a special case of a policy-gradient RL algorithm, where the LM is the policy network and is trained by policy gradient with a binary reward signal. Iterative self-play is related to expert iteration [Anthony et al., 2017], which has been demonstrated to work well in tasks with extremely weak supervision [Christiano et al., 2018]. While our tasks are currently single-hop, this formulation can be extended further into RL: modelling multihop tool-use tasks as markov decision processes (MDPs), or integrating algorithms like Decision Transformer [Chen et al., 2021].</p>
<h2>3 Results</h2>
<p>We evaluate TALM on two domains. The first is the knowledge-oriented Natural Questions (NQ) [Kwiatkowski et al., 2019], a diverse QA task. The second is MathQA [Amini et al., 2019], selected to measure general reasoning capability rather than knowledge.</p>
<h3>3.1 Natural Questions</h3>
<p>Natural Questions (NQ) is a large ( $\approx 300 \mathrm{k}$ training examples) QA dataset collected from real user queries. NQ contains both long and short answer tasks. We selected the short answer task as it is both more challenging as measured with lower baseline performance, and closer to practical use-cases such as assistants. In addition to a question and short-answer pair, examples in the NQ dataset include an "oracle" context (span) of a Wikipedia document containing the answer. We remove boolean questions to avoid inflated performance due to random-chance guesses. We compare TALM against closed-book LM benchmarks.</p>
<p>For TALM experiments, we do not feed the oracle contexts directly to the model, instead using them to populate an index that TALM can access as a retrieval tool. The retrieval system is implemented using a BM25-based index over the union of all NQ oracle contexts.</p>
<p>In Fig. 5, even the $220 M$ base TALM outperforms $3 B$ XL LM. There is also a smaller performance gap between base and XL sized TALMs than between TALM and LM, suggesting that</p>
<p>Question: when are hops added in brewing process?
Short Answer: The boiling process.
question when are hops added in brewing process? |search brewing process |result The boiling process is where chemical reactions take place...including |output The boiling process.</p>
<p>Figure 4: Example from Natural Questions, as a standard NQ task and the corresponding tool-augmented sequence.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 5: Performance of TALM compared with TALM of different model sizes on Natural Questions. The TALM is bootstrapped from 150 tool demonstrations and undergoes two rounds of self-plays. We hypothesize that the noise in the performance-scale curves is due to finetuning in a low-data regime.
smaller models benefit more from retrieval tools for knowledge intensive tasks.</p>
<h3>3.2 MathQA</h3>
<p>MathQA [Amini et al., 2019] is a large scale dataset of math word problems ( $\approx 30 k$ training examples). Each example includes the word problem, a formula generated by crowd-source workers to calculate the answer, and the correct textform answer among multiple choices.</p>
<p>Question: If Lily's test scores are 85,88 and 95 out of 100 in 3 different subjects, what will be her average score?
Formula: Divide(Add(85, Add(88, 95)), 3)
Answer: 89.33
question If Lily's test scores are 85,88 and 95 out of 100 in 3 different subjects, what will be her average score? |formula Divide(Add(85, Add(88, 95)), 3) |result 89.3333333333 |output 89.33</p>
<p>Figure 6: Example from MathQA, as a standard MathQA task and the corresponding tool-augmented sequence..</p>
<p>We implemented a simple solver tool to exe-</p>
<p>cute formulas and check their results’ correctness against their associated text-form answers. According to our solver tool, approximately $70\%$ of the formulas in MathQA produce results that match their corresponding answers, similar to the findings in [Hendrycks et al., 2021]. Our manual inspections show that mismatched results are due to either wrong formulas or invalid answers. The bootstrap tool-use dataset consists of a random sample of $10\%$ of the training corpus where the formula is valid ( $\approx 2 k$ examples). The TALM significantly outperforms a non-augmented LM as shown in Fig. 7.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Performance of TALM compared with LM of different model sizes on MathQA.</p>
<h3>3.3 Self-Play Ablations</h3>
<p>We find that TALMs perform significantly better after a single round of self-play than after training only on the limited bootstrap tool-use training examples, as shown in Fig. 1. Their performance continues to increase over three rounds of selfplay. This trend holds across model sizes ranging from 220 M to 3 B .</p>
<h3>3.4 Out-of-distribution Examples</h3>
<p>One benefit of TALM is its capability to generalize to input text that is out-of-distribution to the model's training data, yet solvable with access to tools.</p>
<p>On the knowledge-heavy QA task, we replace the BM-25 Wiki retriever with a public search engine, and show that TALM handles changing world knowledge well (see Fig. 8).</p>
<p>Question: What is wordle?
LM: a word generator
TALM: a simple online word game that challenges people to find a five-letter word in six guesses</p>
<p>Figure 8: LM vs TALM on changing knowledge.</p>
<p>On the math task, we test large number handling, an area where training data is lacking and non-augmented LMs are known to perform poorly [Brown et al., 2020]. See Fig. 9 demonstrating that TALM can handle large numbers, where a LM does not.</p>
<p>Question: A car is driving 535 miles per hour, how many hours does it take to travel 2450 miles?
LM: 8.5
TALM: 4.58</p>
<p>Figure 9: LM vs TALM on a large number operation.</p>
<h2>4 Conclusion</h2>
<p>In this paper we present TALM, a framework for augmenting language models with arbitrary tools. TALM has two key ideas. First, we model tooluse via a text-to-text interface. Second, we apply an iterative self-play technique to bootstrap high performance on tasks with few tool-use labelled examples. Taken together, this interface and technique make exploring additional tools and tasks possible, without requiring expensive data labeling efforts.</p>
<p>TALM consistently outperforms a nonaugmented LM on both a knowledge task (NQ) and reasoning task (MathQA). Ablations show that self-play is key to good performance, and that iterative self-play yields further gains. We conclude that the combination of tool augmentation and iterative self-play enables smaller models to outperform larger non-augmented LMs.</p>
<p>We hope that this work enables further research into tool augmented language models, a promising direction to enhance model capabilities with less dependency on scale than many contemporary approaches.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and</p>
<p>Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances, 2022. URL https://arxiv.org/abs/2204.01691.</p>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms, 2019. URL https://arxiv.org/abs/1905.13319.</p>
<p>Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving bert a calculator: Finding operations and arguments with reading comprehension, 2019.</p>
<p>Thomas Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. CoRR, abs/1705.08439, 2017. URL http: //arxiv.org/abs/1705.08439.</p>
<p>BIG-bench collaboration. Beyond the imitation game: Measuring and extrapolating the capabilities of language models. In preparation, 2021. URL https : //github.com/google/BIG-bench/.</p>
<p>Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens, 2021.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/ 2005.14165.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models, 2022. URL https://arxiv. org/abs/2202.07646.</p>
<p>Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021. URL https://arxiv.org/abs/ 2106.01345.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/ 2204.02311.</p>
<p>Paul F. Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts. CoRR, abs/1810.08575, 2018. URL http: //arxiv.org/abs/1810.08575.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrievalaugmented language model pre-training, 2020.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. CoRR, abs/2103.03874, 2021. URL https://arxiv. org/abs/2103.03874.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledgeintensive nlp tasks, 2020. URL https:// arxiv.org/abs/2005.11401.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text</p>
<p>transformer, 2019. URL https://arxiv.org/ abs/1910.10683.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model?, 2020. URL https: //arxiv.org/abs/2002.08910.</p>
<p>Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189, 2022.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2017.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems, 2019. URL https://arxiv. org/abs/1905.00537.</p>
<h1>5 Appendix</h1>
<h3>5.1 Acknowledgements</h3>
<p>The authors would like to thank Noam Shazeer for early brainstorming on the path towards this work. We also thank Igor Mordatch for discussions and feedback. Finally we thank Mohammad Saleh for his helpful review and feedback improving this manuscript.</p>
<h3>5.2 Author Contributions</h3>
<p>This section lists the author contributions of each author.</p>
<ul>
<li>Aaron Parisi designed and implemented toolaugmentation and self-play pipelines. Aaron ran the vast majority of experiments, and participated in brainstorming and paper writing.</li>
<li>Yao Zhao participated in brainstorming, experimental setup discussion and paper writing. Yao implemented NQ/mathQA baselines and mathQA solvers.</li>
<li>Noah Fiedel conceived of the project, participated in brainstorming, led the research group and writing the paper.</li>
</ul>            </div>
        </div>

    </div>
</body>
</html>
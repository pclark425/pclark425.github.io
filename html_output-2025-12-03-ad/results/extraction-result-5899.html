<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5899 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5899</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5899</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-899539650b823935247d74c7fdd62e98866df2b0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/899539650b823935247d74c7fdd62e98866df2b0" target="_blank">Evaluating Pre-Trained Language Models on Multi-Document Summarization for Literature Reviews</a></p>
                <p><strong>Paper Venue:</strong> SDP</p>
                <p><strong>Paper TL;DR:</strong> There is some evidence that current summarization metrics are insufficient in measuring summarization accuracy, and a pre-trained LongT5 model on the MSLR22: Multi-Document Summarization for Literature Reviews Shared Task datasets is evaluated.</p>
                <p><strong>Cost:</strong> 0.002</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5899",
    "paper_id": "paper-899539650b823935247d74c7fdd62e98866df2b0",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00171175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Pre-Trained Language Models on Multi-Document Summarization for Literature Reviews</h1>
<p>Benjamin Yu<br>Georgia Tech<br>byu92@gatech.edu</p>
<h4>Abstract</h4>
<p>Systematic literature reviews in the biomedical space are often expensive to conduct. Automation through machine learning and large language models could improve the accuracy and research outcomes from such reviews. In this study, we evaluate a pre-trained LongT5 model on the MSLR22: Multi-Document Summarization for Literature Reviews Shared Task datasets. We weren't able to make any improvements on the dataset benchmark, but we do establish some evidence that current summarization metrics are insufficient in measuring summarization accuracy. A multi-document summarization web tool was also built to demonstrate the viability of summarization models for future investigators: https://ben-yu. github.io/summarizer</p>
<h2>1 Introduction</h2>
<p>With recent advances in natural language processing and deep learning, large language models are now capable of generating summaries of large volumes of documents that are arguably human readable and logically consistent. With the growing amount of research being published, it has become increasingly difficult to process all the available research and literature in any particular field of study. This has become exceedingly important within the biomedical field as the community has learned with the global COVID-19 pandemic. Speed of research directly impacts patient outcomes and how fast medical practitioners can respond to a constantly changing health landscape. The MSLR22: Multi-Document Summarization for Literature Reviews shared task proposes a challenging research problem that pushes current state of the art multidocument summarization models to generalize over two different datasets: MS^2 Dataset (DeYoung et al., 2021) and Cochrane Dataset (Wallace et al., 2020) We will evaluate in this research study if pre-trained summarization models can successfully solve the proposed task.</p>
<h2>2 Related Work</h2>
<p>Recent studies in document summarization have mostly focused on Transformer-based models, but applied to the biomedical context either through transfer learning or fine-turning on a specific biomedical dataset (Wang et al., 2021). BioBERTSum is a recent example of using such pre-training methodologies, which used a pre-trained model as an encoder and fine-tuned on a specific task (Du et al., 2020). (Moradi and Samwald, 2019) innovated in this space by applying hierarchical clustering to group contextual embeddings of sentences to select the most informative sentences from a given group to generate summaries. (Sotudeh et al., 2020) also recently proposed a mechanism to leverage domain knowledge and embed it into their SciBERTbased clinical abstractive summarization model.</p>
<p>Scaling such transformer models to longer input sizes has been difficult since the attention layers get exponentially larger and become computationally infeasible to train. Recent advances in model architecture like PEGASUS (Zhang et al., 2019) and Longformer (Beltagy et al., 2020) have introduced different ways around this by introducing sparse attention mechanisms like local attention which replaces the full-attention mechanism with a sparse sliding window. Researchers at Google were able to innovate on these findings further by combining pre-training strategies from PEGASUS along with a new sparse attention mechanism called Transient Global which mimics ETC's local/global attention mechanism and achieve state of the art performance on multiple summarization benchmarks. (Guo et al., 2021)</p>
<h2>3 Data Analysis</h2>
<h3>3.1 MS^2 Dataset</h3>
<p>The MS^2 dataset consists of 470k studies mapped to 20k reviews from PubMed (DeYoung et al., 2021). The dataset was further augmented with</p>
<p>PICO span labels and evidence inference classes. The goal for this dataset is to generate an accurate summary given a set of multiple review abstracts.</p>
<p>To understand the relative difficulty of this summarization task, we measured text similarity between abstracts and their target summaries based on Term Frequency-Inverse Document Frequency (TF-IDF) and Jaccard similarity.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of MS^2 distances from abstract to target summary</p>
<p>The mean cosine difference was 0.4 and Jacard distance was 0.1 . This indicates there was no substantial overlap between the target summaries and their source reviews.</p>
<h3>3.2 Cochrane Dataset</h3>
<p>This was a smaller dataset of 4.5 K reviews collected from Cochrane systematic reviews (Wallace et al., 2020). This dataset was cleaner than the MS^2 dataset, but substantially smaller. The reviews on average included 10 trials each and the average abstract length of included trials was 245 words. We use the authors' conclusions subsection of the systematic review abstract as our target summary ( 75 words on average).</p>
<p>We also did a similar measurement of cosine and Jaccard distances for the Cochrane dataset:
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of Cochrane review distances from abstract to target summary</p>
<p>Similar to the MS^2 dataset, the cosine and Jaccard distances were normally distributed and had
roughly the same average difference from their target review. This seemed to indicate that both datasets were similarly difficult and have roughly the same level of sentence overlap.</p>
<h2>4 Experiments</h2>
<p>The original goal of this study was to experiment with two different approaches to the MSLR22 Shared Task:</p>
<ol>
<li>Fine-tune LongT5 models with both datasets</li>
<li>Evaluate existing LongT5 language models on similar datasets like PubMed (Cohan et al., 2018)</li>
</ol>
<p>We selected the LongT5 model due to its purported state of the art performance numbers and its ability to scale its input size to up to 16384 tokens. We leveraged several cloud providers such as Google Cloud and AWS Sagemaker along with HuggingFace's transformers library for model fine tuning (Wolf et al., 2019). We also experimented with HuggingFace's AutoTrain framework to automatically search for the correct hyperparameters for training. All we had to provide was an initial training and validation datasets, and AutoTrain automated the model training and tuning process. To allow the model to train on multiple documents at once, we pre-processed the training data such that all review abstracts with the same Review ID were appended into a single input string. The single input would then be fed into our model of choice after doing some minimal input validation like checking if the input isn't more than our maximum token length of 16384. We immediately hit several limitations with cloud training including not having sufficient spend to qualify using larger GPU instances for training. HuggingFace's AutoTrain framework also never successfully completed and would often timeout after several days of training. We also attempted to fine tune our models locally, but we only had access to a single RTX 3080 10GB GPU which couldn't even fit the model and dataset even with a batch size of 1 . Our conclusion from this experience has demonstrated how the trend towards larger language models might risk increasingly making this type of research inaccessible to hobbyists and practitioners. State-of-the-art model performance will likely only be achieved by researchers with access to compute power and capital unless we prioritize research into reduce model size and resource utilization.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">-</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Training Target</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Characters</td>
<td style="text-align: center;">1745.81</td>
<td style="text-align: center;">435.60</td>
<td style="text-align: center;">1746.66</td>
</tr>
<tr>
<td style="text-align: left;">Words</td>
<td style="text-align: center;">299.88</td>
<td style="text-align: center;">68.53</td>
<td style="text-align: center;">301.42</td>
</tr>
<tr>
<td style="text-align: left;">Sentences</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">2.74</td>
<td style="text-align: center;">11.17</td>
</tr>
</tbody>
</table>
<p>Table 1: MS^2 Dataset Properties</p>
<table>
<thead>
<tr>
<th style="text-align: left;">-</th>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Training Target</th>
<th style="text-align: center;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Characters</td>
<td style="text-align: center;">1526.79</td>
<td style="text-align: center;">489.8</td>
<td style="text-align: center;">1510.42</td>
</tr>
<tr>
<td style="text-align: left;">Words</td>
<td style="text-align: center;">224.3</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">221.14</td>
</tr>
<tr>
<td style="text-align: left;">Sentences</td>
<td style="text-align: center;">10.2</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">10.09</td>
</tr>
</tbody>
</table>
<p>Table 2: Cochrane Dataset Properties
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: HuggingFace AutoTrain on LongT5</p>
<p>For our second approach, rather than fine-tuning a base model, we wanted to evaluate if a model that was pre-trained on a similar dataset would still be able to solve this summarization task without any fine-tuning. We found a pre-trained LongT5 model on the PubMed dataset that was trained for around 3 k steps (Stancl, 2022). We believed the fine-tuning should be transferable to these datasets as they largely cover the same type of biomedical content and the MS^2 dataset also gets its training data from PubMed. We leveraged HuggingFace's Inference API for model evaluation against the MSLR22 datasets. This also restricted our ability to fine-tune the output size which probably also hindered our performance.</p>
<p>To aid in the model development process and also as a validation that these summarization models have a practical use, we created an online tool that allows anyone to invoke the models for any 6 paper abstracts. The tool can be found at: https: //ben-yu.github.io/summarizer
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Multi-Document Summarization Tool with HuggingFace Inference API</p>
<h2>5 Discussion</h2>
<p>Unsurprisingly the pre-trained models were unable to exceed the dataset benchmarks on the shared task. One key failing came from our inability to configure target generation length using HuggingFace's Accelerated Inference Text2Text Generation API. On the MS^2 Dataset our outputs only had an average sentence length of 1.1 and character count of 87.97 , which significantly deviated from our target length of 2.74 sentences and 435.6 characters. This likely due to the out-of-the-box model not properly generalising over the entire PubMed dataset as the model was also only trained for about 3 k steps and further training steps would have improved it's performance. The Rouge-L scores were particularly indicative, scoring sometimes up to $50 \%$ worse than the benchmarks. Increasing our model output length would have likely dramatically improved our Rouge scores. Our model didn't score that poorly in terms of a delta EI on the MS^2 dataset with only a 0.06 difference from the Long-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">R-1</th>
<th style="text-align: center;">R-2</th>
<th style="text-align: center;">R-L</th>
<th style="text-align: center;">$\mathbf{E I} \downarrow$</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART Benchmark</td>
<td style="text-align: center;">0.2626</td>
<td style="text-align: center;">0.0770</td>
<td style="text-align: center;">0.1950</td>
<td style="text-align: center;">0.4509</td>
<td style="text-align: center;">0.4142</td>
<td style="text-align: center;">0.8636</td>
</tr>
<tr>
<td style="text-align: left;">Longformer Benchmark</td>
<td style="text-align: center;">0.2637</td>
<td style="text-align: center;">0.0795</td>
<td style="text-align: center;">0.1961</td>
<td style="text-align: center;">0.4621</td>
<td style="text-align: center;">0.4118</td>
<td style="text-align: center;">0.8666</td>
</tr>
<tr>
<td style="text-align: left;">LongT5 - Pubmed</td>
<td style="text-align: center;">0.1200</td>
<td style="text-align: center;">0.0133</td>
<td style="text-align: center;">0.0961</td>
<td style="text-align: center;">0.5280</td>
<td style="text-align: center;">0.3433</td>
<td style="text-align: center;">0.8276</td>
</tr>
</tbody>
</table>
<p>Table 3: Model performance on MS^2 Dataset</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">R-1</th>
<th style="text-align: center;">R-2</th>
<th style="text-align: center;">R-L</th>
<th style="text-align: center;">$\mathbf{E I} \downarrow$</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">BERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BART Benchmark</td>
<td style="text-align: center;">0.2397</td>
<td style="text-align: center;">0.0671</td>
<td style="text-align: center;">0.1760</td>
<td style="text-align: center;">0.2081</td>
<td style="text-align: center;">0.3348</td>
<td style="text-align: center;">0.8632</td>
</tr>
<tr>
<td style="text-align: left;">Longformer Benchmark</td>
<td style="text-align: center;">0.2387</td>
<td style="text-align: center;">0.0655</td>
<td style="text-align: center;">0.1755</td>
<td style="text-align: center;">0.2345</td>
<td style="text-align: center;">0.3316</td>
<td style="text-align: center;">0.8641</td>
</tr>
<tr>
<td style="text-align: left;">LongT5 - Pubmed</td>
<td style="text-align: center;">0.1130</td>
<td style="text-align: center;">0.0154</td>
<td style="text-align: center;">0.0903</td>
<td style="text-align: center;">0.4671</td>
<td style="text-align: center;">0.2873</td>
<td style="text-align: center;">0.7863</td>
</tr>
</tbody>
</table>
<p>Table 4: Model performance on Cochrane Dataset
former benchmark. This could be an indicator that delta EI is a flawed metric that doesn't adequately capture the factual correctness of a summary. Recent work by (Otmakhova et al., 2022) evaluated Longformer and BART models along similar metrics and showed that both models failed to pick up and aggregate important details when manually evaluated against with expert human evaluators. Stronger metrics will likely be required in the future if there is to be significant progress in this domain.</p>
<p>We also found that experimenting with language models and training these large language models can be extremely cost prohibitive and potentially inaccessible to hobbyists and novice machine learning practitioners. These models are getting increasingly large and can't be built unless one has access to sufficient GPU-computing or cloud resources. Training these models can take upwards of 48 hours and there is no guarantee that your model is improving or converging at a reasonable rate.</p>
<h2>6 Conclusion</h2>
<p>We weren't able to improve upon existing benchmarks for either the MS^2 or Cochrane datasets. We did show there is a need for stronger summarization metrics that can capture different linguistic dimensions such as factual correctness and readability. The summaries from our pre-trained model were significantly shorter than the target summaries and often factually incorrect upon manual inspection, but this couldn't directly be inferred from our model scores outside of comparing it to task benchmarks.</p>
<h2>References</h2>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.</p>
<p>Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615-621, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. Ms2: Multidocument summarization of medical studies.</p>
<p>Yongping Du, Qingxiao Li, Lulin Wang, and Yanqing He. 2020. Biomedical-domain pre-trained language model for extractive summarization. KnowledgeBased Systems, 199:105964.</p>
<p>Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2021. Longt5: Efficient text-to-text transformer for long sequences.</p>
<p>Milad Moradi and Matthias Samwald. 2019. Clustering of deep contextualized representations for summarization of biomedical texts.</p>
<p>Yulia Otmakhova, Karin Verspoor, Timothy Baldwin, and Jey Han Lau. 2022. The patient is more dead than alive: exploring the current state of the multidocument summarisation of the biomedical literature. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5098-5111, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Sajad Sotudeh, Nazli Goharian, and Ross W. Filice. 2020. Attend to medical ontologies: Content selection for clinical abstractive summarization.</p>
<p>Daniel Stancl. 2022. Longt5 large 16384 pubmed 3k step checkpoint.</p>
<p>Byron C. Wallace, Sayantan Saha, Frank Soboczenski, and Iain J. Marshall. 2020. Generating (factual?) narrative summaries of rcts: Experiments with neural multi-document summarization.</p>
<p>Benyou Wang, Qianqian Xie, Jiahuan Pei, Prayag Tiwari, Zhao Li, and Jie fu. 2021. Pre-trained language models in biomedical domain: A systematic survey.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. Huggingface's transformers: State-of-the-art natural language processing.</p>
<p>Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. 2019. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization.</p>            </div>
        </div>

    </div>
</body>
</html>
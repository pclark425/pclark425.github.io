<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3147 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3147</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3147</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a" target="_blank">Exploring Length Generalization in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper establishes that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale, and shows that combining pretrained large language models' in-context learning abilities with scratchpad prompting results in a dramatic improvement in lengthgeneralization.</p>
                <p><strong>Paper Abstract:</strong> The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3147.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3147.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parity task (coin-flip)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parity / Coin-flip sequential parity task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic parity problem where the model must compute parity (even/odd) of a bitstring or track coin flips (flip/no-flip) via an explicit sequential state (running parity) often represented as a scratchpad; used to probe length generalization and sequential state-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various pretrained decoder-only checkpoints: 244M, 422M, 1B, 64B, 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer checkpoints pretrained on general natural language data (LaMDA family, sizes cited above). Finetuning used AdaFactor; models use T5-style position biases.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity (bit parity), framed as coin-flip sequential state tracking</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Models often learn non-sequential parallel shortcut strategies (counting/pooling + threshold) instead of a left-to-right stateful algorithm; when given scratchpad templates via few-shot prompting, models can perform template-based state tracking (copying per-step state) that generalizes to longer lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Training on fixed token-length but varying number-of-ones shows in-distribution 100% but OOD ≈ random, indicating a counting/threshold shortcut; few-shot scratchpad prompting produces correct long-step scratchpads (example up to 20 flips) indicating template mapping; per-step scratchpad error analysis shows attention fails to focus on relevant inputs when finetuned with scratchpad, supporting attention-based failure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Few-shot scratchpad prompting enabling extrapolation argues that performance is not purely memorization of short instances; padded-scratchpad experiments show position-bias training is not the whole cause.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Vanilla finetuning, scratchpad finetuning, padded scratchpad, few-shot scratchpad prompting, masking distractors</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Vanilla and scratchpad finetuning: near-perfect in-distribution but rapid OOD degradation. Padded scratchpad: partial help but still poor OOD. Masking distractors: removing distracting tokens yields near-perfect length generalization. Few-shot scratchpad prompting: dramatic improvement in OOD extrapolation (models map short exemplars' scratchpad templates onto much longer queries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In-distribution accuracy often near 100% (training lengths e.g. 3–20); out-of-distribution accuracy decays rapidly with length for finetuned models (OOD sometimes ≈ random). In the 'varied number of ones' split, in-distribution 100% vs OOD ≈ random; few-shot scratchpad achieves 'highly nontrivial' accuracy up to 20 flips (no single numeric given).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Preference for non-sequential pooling/counting shortcuts that do not transfer to longer lengths; attention failing to attend to the relevant bit when input length is OOD; sensitivity to distractor tokens; EOS/prediction truncation can degrade extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>The correct solution is a simple left-to-right algorithmic state update (symbolic); recurrent models are known to perform dynamical counting, whereas transformers here prefer parallel pooling strategies; few-shot scratchpad prompting approximates executing the sequential algorithm by template matching rather than true learned algorithm in weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3147.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Variable assignment task</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boolean Variable Assignment (program execution) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic Python-like boolean assignment programs where the model must execute the program to predict the final value of a queried variable; designed to probe long dependency chains and length generalization (computational graph depth).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA (various pretrained decoder-only checkpoints: 244M, 422M, 1B, 64B, 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformers pretrained on language and code-like data; finetuned with AdaFactor and small dropout for variable assignment experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Boolean program execution / chained boolean operations (assign/and/or/xor/negate, conditional assign), i.e., logical/arithmetic boolean chains rather than numeric arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Transformers tend to learn parallel strategies based on shallow correlations and small computational graph depth rather than sequential line-by-line execution; computational graph depth (longest dependency chain) is the relevant difficulty measure for transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Models trained on programs up to N lines succeed on OOD numbers of operations if computational graph depth remains in-distribution; shuffling operations (destroying sequential dependency) yields similar OOD performance to clean data, indicating reliance on non-sequential cues; accuracy correlates with computational graph depth more than number of operations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Few-shot scratchpad prompting sometimes helps when base pretrained model already has nontrivial performance; but when finetuning is required to learn the task from scratch, the same finetuning pathologies appear, arguing against a simple finetuning-induced solution.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Vanilla finetuning, scratchpad finetuning, few-shot scratchpad prompting; shuffled-ops baseline</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Vanilla and scratchpad finetuning: near-perfect in-distribution, poor OOD generalization. Shuffled-ops baseline: similar OOD failure. Few-shot scratchpad prompting: helps when pretrained model already has some ability, but mixed benefits when model must learn the skill via finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In-distribution accuracy near perfect for training lengths (e.g., 3–8 lines); OOD test accuracy degrades rapidly beyond training lengths (evaluated up to 19 or 32 lines); shuffled-ops baseline performs on par with clean finetuned models on OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Models exploit spurious, non-sequential correlations and fail to execute long dependency chains; failure linked to computational graph depth rather than raw operation count; finetuning hyperparameters strongly affect OOD despite similar in-distribution loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Correct approach is symbolic sequential execution of program lines; transformers do not reliably learn this algorithm from finetuning and instead rely on shortcuts; few-shot scratchpad can sometimes instantiate a template resembling sequential execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3147.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LaMDA models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LaMDA pretrained decoder-only transformer checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pretrained decoder-only transformer language models used as base models for all experiments; sizes ranged from hundreds of millions to ~128B parameters and they use T5-style position biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA family (244M, 422M, 1B, 64B, 128B reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer checkpoints pretrained on broad natural language (and code-like) corpora; during finetuning, AdaFactor optimizer used; T5 position biases present in architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Applied to parity (bit/coin-flip) and boolean variable-assignment tasks (logical program execution); not evaluated on standard numeric multi-digit arithmetic within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Pretrained transformers exhibit a bias to learn parallel/pooling strategies; when prompted with scratchpad exemplars few-shot, they can apply template-based stepwise generation resembling algorithmic state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scale (model size) does not meaningfully improve finetuned length generalization across sizes tested (Figure 3); few-shot scratchpad prompting benefits scale (reported to scale with model size per Section 7 and corroborating works).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Large models still fail OOD after finetuning, indicating scale alone doesn't produce algorithmic sequential reasoning when trained in the finetuning regime.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Finetuning (vanilla), scratchpad finetuning, few-shot scratchpad prompting, padding/masking interventions</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Finetuning (any size) yields near-perfect in-distribution but poor OOD; few-shot scratchpad prompting yields significant improvement and scales with model size; masking distractors during training can restore generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multiple sizes evaluated; all show similar finetuning OOD pathologies. Specific sizes referenced: 244M, 422M, 1B, 64B, 128B; per-step error rates presented for 128B in figures (qualitative descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Strong bias toward parallel non-sequential heuristics; sensitivity to distractors and position bias bins when finetuned only on short instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>LaMDA weights do not inherently encode reliable long-length algorithmic execution; symbolic algorithms or recurrent architectures may more naturally realize sequential state-tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3147.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vanilla finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard finetuning on task-specific examples (no scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning pretrained transformers to produce final answers (not intermediate scratchpad steps); used to test whether learning from many short instances generalizes to longer ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA checkpoints (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Finetuned with AdaFactor, tuned learning rate, batch size, dropout; training until in-distribution validation accuracy settled (e.g., 20k steps for parity).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity and boolean variable assignment</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Finetuning induces models to learn non-sequential spurious correlations/pooling strategies that exploit superficial cues rather than algorithmic state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Models achieve near-perfect in-distribution accuracy but show rapid OOD degradation; shuffled-ops baseline (removing sequential order) performs similarly, indicating reliance on non-sequential cues.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None that overturns the conclusion; variation in hyperparameters drastically changed OOD behavior even with similar in-distribution loss, supporting mechanism identification.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Finetuning hyperparameter sweeps, shuffled-ops baseline</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Finetuning hyperparameters significantly affect OOD results (some hyperparam settings yield better OOD but inconsistent); shuffling operations indicates finetuned models exploit dataset spurious correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In-distribution accuracy (near perfect); OOD accuracy decays to near random for lengths beyond training (plots across lengths 3–40 or up to 32 depending on experiment).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Does not generalize to longer instances even with increased model/data scale; susceptible to learning shortcuts rather than algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Finite-data finetuning does not cause the model to learn the underlying sequential algorithm as a symbolic executor would.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3147.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Scratchpad finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Finetuning using scratchpad (chain-of-thought) targets</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finetuning models to output explicit intermediate computation steps (scratchpad) before final answer, with the hope of inducing sequential algorithmic behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA checkpoints (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base models as finetuning experiments; targets include multi-step intermediate states (e.g., running parity states or per-line variable values).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity and boolean variable assignment (intermediate step outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Intended to teach models to produce sequential internal representations (explicit state per step), but finetuned models still tend to fail OOD because attention patterns do not generalize and distractors degrade stepwise construction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Scratchpad finetuning yields similar OOD failures to vanilla finetuning (Figure 4); per-step scratchpad error rates increase when input length is OOD and attention doesn't focus on relevant tokens; padded-scratchpad helps somewhat but does not eliminate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Masking distractors shows that when irrelevant tokens are removed, scratchpad finetuned models can generalize perfectly, implying failures stem from distractors rather than inability to learn stepwise content per se.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Padded scratchpad, masking distractors (input/scratchpad), EOS/padding experiments</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Padded scratchpad ameliorates position-bias issues partially; masking distractors (especially input distractors) restores perfect length generalization; EOS/prediction training concerns tested but not sole cause.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Scratchpad finetuned models: strong in-distribution but poor OOD (plots shown). Masking distractors yields near-perfect OOD scratchpad prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Attention patterns not generalizing to longer contexts; model distracted by previous scratchpad tokens and irrelevant input tokens; premature EOS emissions can harm extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Producing intermediate steps resembles human chain-of-thought, but finetuning does not guarantee the emergence of a robust algorithmic internal representation across lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3147.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot scratchpad prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context learning with chain-of-thought/scratchpad exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting pretrained LLMs with a small number of examples that include explicit intermediate reasoning steps (scratchpad) to induce generation of stepwise solutions on new longer instances without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA 128B (demonstrated) and other LaMDA sizes</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained decoder-only transformers; no finetuning when applying few-shot scratchpad prompting (though few-shot finetuning variants were also tested).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity (coin-flip), variable assignment (programs) via stepwise scratchpad generation</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Template-based variable-length pattern matching: models learn to copy/apply the scratchpad solution template from short exemplars to much longer queries, effectively performing stepwise state tracking via in-context induction rather than weight-encoded algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Large LaMDA models can map a length-3 scratchpad exemplar to correctly generate a 20-step scratchpad (Figure 8); performance improves with few-shot scratchpad compared to zero-shot finetuned scratchpad in parity; authors observe template matching behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>When the pretrained model has poor base performance on the task, few-shot scratchpad finetuning yields mixed or poor OOD results; suggests dependence on pretrained capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Few-shot prompting (in-context exemplars with scratchpad), few-shot finetuning variants</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Dramatic improvements in length generalization for parity; enables variable-length template matching and extrapolation; scales with model size (reported qualitatively and in related works).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative plots show substantial OOD accuracy improvement up to 20 steps on parity; no single numeric accuracy reported across all lengths, described as 'highly nontrivial accuracy levels.'</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Effectiveness depends on base pretrained model capability and prompt style; some prompt styles produce weak non-finetuned performance and then few-shot finetuning still fails OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Behavior resembles humans applying an explicit stepwise strategy learned from examples; differs from symbolic algorithms because the algorithmic behavior is induced in-context rather than encoded in weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3147.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parallel shortcut / counting heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer parallel pooling/counting shortcut</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed inductive bias where transformers solve parity-like tasks by performing pooling/counting over tokens and thresholding rather than left-to-right sequential state updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA family (experiments on 244M, 422M, 1B, 64B, 128B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Self-attention transformers whose equivariant operations facilitate set/pooling computations enabling bottom-up counting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity (count number of ones) and other tasks where global counts can serve as shortcuts</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Self-attention acts as an equivariant transformation capable of pooling (e.g., counting, max-pooling), enabling models to implement non-sequential counting + threshold heuristics that fit training distribution but fail to extrapolate.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Varied-number-of-ones experiment: fixing token count and varying number of ones yielded models that achieved 100% in-distribution but OOD ≈ random; periodic outputs biased to 0/1 depending on ratios indicates counting threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Few-shot scratchpad prompting can induce sequential-like behavior despite underlying transformer tendency, showing the heuristic is not an absolute limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Changing data distribution (fix tokens, vary ones), scratchpad prompting, masking distractors</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Data modification reveals shortcut; scratchpad prompting can override shortcut by encouraging stepwise outputs; masking distractors can allow learning true sequential algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the counting shortcut regime, OOD performance looks like random guessing on held-out number-of-ones ranges and periodic near-0/near-100 behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>No transfer to different-length instances because threshold/count heuristics are tied to training distribution statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Contrasts with symbolic left-to-right algorithmic counting (or recurrent networks that can implement dynamical counting); reflects architectural inductive bias of self-attention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3147.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Computational graph depth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational Graph Depth (longest dependency chain)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Task-specific notion of difficulty defined as the length of the longest dependency chain leading to the queried variable, shown to better predict transformer performance than raw operation count.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA family (various sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as an analysis metric for the variable assignment task to characterize difficulty from a transformer's perspective.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Boolean variable assignment / program execution</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Transformers handle instances with small computational graph depth well even if overall program length is OOD; performance correlates with depth because transformer parallel strategies resolve low-depth dependencies effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Experiments show accuracy remains roughly unchanged when number of operations is increased out-of-distribution but computational graph depth stays in-distribution (Figure 13).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None presented; metric explains observed behavior across experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Data generation and analysis (no architectural intervention)</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Provides diagnostic insight: training regimes should consider computational graph depth rather than raw length to evaluate expected transformer's success.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Plots show accuracy evolution by computational graph depth over training iterations; depth is better predictor of final accuracy than operation count.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Models fail as depth increases beyond training distribution; naive emphasis on training more long programs (by line count) may not address deep dependency generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Symbolic executors' difficulty aligns with sequential operation count, but transformers' effective difficulty correlates with dependency depth, differing from naive symbolic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3147.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distractor masking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masking distracting input and scratchpad tokens intervention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Intervention that masks out irrelevant (distractor) tokens in the input and previously generated scratchpad tokens during training to test their influence on length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA finetuned with scratchpad targets</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer models trained on parity/scratchpad targets with selective masking applied to input and/or preceding scratchpad tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity (scratchpad step prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Distractor tokens in input and previous scratchpad steps disrupt attention patterns needed for sequential stepwise computation; removing these distractors allows the model to focus and generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Masking experiments (four conditions) show removing all distractors yields perfect length generalization; input-side distractors hurt performance more than scratchpad-side distractors (Figure 15).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None; results directly support the distractor hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Manual masking of distractor tokens (input, scratchpad, both) and padded scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Masking both input and scratchpad distractors yielded perfect OOD generalization; masking input distractors alone gave large benefit; padded scratchpad partially helps but not fully.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Plots show accuracy of scratchpad token prediction across steps and input lengths: masking all distractors yields near-perfect accuracy across OOD lengths (quantitative numbers not tabulated but clearly near 100% in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>When distractors are present, attention is misdirected and per-step error rates increase on OOD inputs; distractors in input are more harmful than in preceding scratchpad.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Humans can ignore distractors when reasoning; masking mimics cleaner, noiseless symbolic execution context enabling algorithmic behavior to surface.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3147.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3147.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Position biases & EOS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 position biases and EOS token prediction effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural/learning factors investigated as potential causes of poor length extrapolation: untrained positional bias bins (T5 biases) for far positions and training to predict EOS tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LaMDA family (T5-style position bias implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models use T5 position biases to encode token positions; EOS token prediction behavior studied as prior work implicated premature EOS harms length extrapolation.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Parity and scratchpad-based step generation</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Untrained position-bias bins for far positions could impair attention to distant tokens; EOS training may cause premature termination and learned representations that do not generalize to longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Padded-scratchpad (equalizing positional bins across lengths) improved but did not eliminate OOD failures, indicating position biases contribute but are not sole cause; EOS-related concerns mentioned and tested but not found to fully explain failures.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Padded-scratchpad experiments show that even when position encoding is controlled the model still struggles unless distractors are masked, so position bias/EOS are not primary drivers.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Padded scratchpad (left/right padding to keep T5 bins aligned), experiments controlling EOS prediction</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Padded scratchpad helps somewhat but OOD generalization issues persisted; EOS considerations alone do not explain poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative improvements with padding but residual error remains (figures showing per-step error rates); no single numeric improvement provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Position bias limitations and EOS effects can exacerbate OOD failures but do not fully account for distrator-driven attention failures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Symbolic execution is position-agnostic to the extent it processes program semantics; positional encoding is an implementation detail affecting transformer extrapolation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring Length Generalization in Large Language Models', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 2)</em></li>
                <li>The eos decision and length extrapolation <em>(Rating: 2)</em></li>
                <li>Train short, test long: Attention with linear biases enables input length extrapolation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3147",
    "paper_id": "paper-f843233f76a5dff07bfa93a71a1cf13d8aa6a94a",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Parity task (coin-flip)",
            "name_full": "Parity / Coin-flip sequential parity task",
            "brief_description": "Synthetic parity problem where the model must compute parity (even/odd) of a bitstring or track coin flips (flip/no-flip) via an explicit sequential state (running parity) often represented as a scratchpad; used to probe length generalization and sequential state-tracking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various pretrained decoder-only checkpoints: 244M, 422M, 1B, 64B, 128B)",
            "model_description": "Decoder-only transformer checkpoints pretrained on general natural language data (LaMDA family, sizes cited above). Finetuning used AdaFactor; models use T5-style position biases.",
            "arithmetic_task_type": "Parity (bit parity), framed as coin-flip sequential state tracking",
            "reported_mechanism": "Models often learn non-sequential parallel shortcut strategies (counting/pooling + threshold) instead of a left-to-right stateful algorithm; when given scratchpad templates via few-shot prompting, models can perform template-based state tracking (copying per-step state) that generalizes to longer lengths.",
            "evidence_for_mechanism": "Training on fixed token-length but varying number-of-ones shows in-distribution 100% but OOD ≈ random, indicating a counting/threshold shortcut; few-shot scratchpad prompting produces correct long-step scratchpads (example up to 20 flips) indicating template mapping; per-step scratchpad error analysis shows attention fails to focus on relevant inputs when finetuned with scratchpad, supporting attention-based failure.",
            "evidence_against_mechanism": "Few-shot scratchpad prompting enabling extrapolation argues that performance is not purely memorization of short instances; padded-scratchpad experiments show position-bias training is not the whole cause.",
            "intervention_type": "Vanilla finetuning, scratchpad finetuning, padded scratchpad, few-shot scratchpad prompting, masking distractors",
            "effect_of_intervention": "Vanilla and scratchpad finetuning: near-perfect in-distribution but rapid OOD degradation. Padded scratchpad: partial help but still poor OOD. Masking distractors: removing distracting tokens yields near-perfect length generalization. Few-shot scratchpad prompting: dramatic improvement in OOD extrapolation (models map short exemplars' scratchpad templates onto much longer queries).",
            "performance_metrics": "In-distribution accuracy often near 100% (training lengths e.g. 3–20); out-of-distribution accuracy decays rapidly with length for finetuned models (OOD sometimes ≈ random). In the 'varied number of ones' split, in-distribution 100% vs OOD ≈ random; few-shot scratchpad achieves 'highly nontrivial' accuracy up to 20 flips (no single numeric given).",
            "notable_failure_modes": "Preference for non-sequential pooling/counting shortcuts that do not transfer to longer lengths; attention failing to attend to the relevant bit when input length is OOD; sensitivity to distractor tokens; EOS/prediction truncation can degrade extrapolation.",
            "comparison_to_humans_or_symbolic": "The correct solution is a simple left-to-right algorithmic state update (symbolic); recurrent models are known to perform dynamical counting, whereas transformers here prefer parallel pooling strategies; few-shot scratchpad prompting approximates executing the sequential algorithm by template matching rather than true learned algorithm in weights.",
            "uuid": "e3147.0",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Variable assignment task",
            "name_full": "Boolean Variable Assignment (program execution) task",
            "brief_description": "Synthetic Python-like boolean assignment programs where the model must execute the program to predict the final value of a queried variable; designed to probe long dependency chains and length generalization (computational graph depth).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA (various pretrained decoder-only checkpoints: 244M, 422M, 1B, 64B, 128B)",
            "model_description": "Decoder-only transformers pretrained on language and code-like data; finetuned with AdaFactor and small dropout for variable assignment experiments.",
            "arithmetic_task_type": "Boolean program execution / chained boolean operations (assign/and/or/xor/negate, conditional assign), i.e., logical/arithmetic boolean chains rather than numeric arithmetic",
            "reported_mechanism": "Transformers tend to learn parallel strategies based on shallow correlations and small computational graph depth rather than sequential line-by-line execution; computational graph depth (longest dependency chain) is the relevant difficulty measure for transformers.",
            "evidence_for_mechanism": "Models trained on programs up to N lines succeed on OOD numbers of operations if computational graph depth remains in-distribution; shuffling operations (destroying sequential dependency) yields similar OOD performance to clean data, indicating reliance on non-sequential cues; accuracy correlates with computational graph depth more than number of operations.",
            "evidence_against_mechanism": "Few-shot scratchpad prompting sometimes helps when base pretrained model already has nontrivial performance; but when finetuning is required to learn the task from scratch, the same finetuning pathologies appear, arguing against a simple finetuning-induced solution.",
            "intervention_type": "Vanilla finetuning, scratchpad finetuning, few-shot scratchpad prompting; shuffled-ops baseline",
            "effect_of_intervention": "Vanilla and scratchpad finetuning: near-perfect in-distribution, poor OOD generalization. Shuffled-ops baseline: similar OOD failure. Few-shot scratchpad prompting: helps when pretrained model already has some ability, but mixed benefits when model must learn the skill via finetuning.",
            "performance_metrics": "In-distribution accuracy near perfect for training lengths (e.g., 3–8 lines); OOD test accuracy degrades rapidly beyond training lengths (evaluated up to 19 or 32 lines); shuffled-ops baseline performs on par with clean finetuned models on OOD.",
            "notable_failure_modes": "Models exploit spurious, non-sequential correlations and fail to execute long dependency chains; failure linked to computational graph depth rather than raw operation count; finetuning hyperparameters strongly affect OOD despite similar in-distribution loss.",
            "comparison_to_humans_or_symbolic": "Correct approach is symbolic sequential execution of program lines; transformers do not reliably learn this algorithm from finetuning and instead rely on shortcuts; few-shot scratchpad can sometimes instantiate a template resembling sequential execution.",
            "uuid": "e3147.1",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "LaMDA models",
            "name_full": "LaMDA pretrained decoder-only transformer checkpoints",
            "brief_description": "Pretrained decoder-only transformer language models used as base models for all experiments; sizes ranged from hundreds of millions to ~128B parameters and they use T5-style position biases.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA family (244M, 422M, 1B, 64B, 128B reported)",
            "model_description": "Decoder-only transformer checkpoints pretrained on broad natural language (and code-like) corpora; during finetuning, AdaFactor optimizer used; T5 position biases present in architecture.",
            "arithmetic_task_type": "Applied to parity (bit/coin-flip) and boolean variable-assignment tasks (logical program execution); not evaluated on standard numeric multi-digit arithmetic within this paper.",
            "reported_mechanism": "Pretrained transformers exhibit a bias to learn parallel/pooling strategies; when prompted with scratchpad exemplars few-shot, they can apply template-based stepwise generation resembling algorithmic state updates.",
            "evidence_for_mechanism": "Scale (model size) does not meaningfully improve finetuned length generalization across sizes tested (Figure 3); few-shot scratchpad prompting benefits scale (reported to scale with model size per Section 7 and corroborating works).",
            "evidence_against_mechanism": "Large models still fail OOD after finetuning, indicating scale alone doesn't produce algorithmic sequential reasoning when trained in the finetuning regime.",
            "intervention_type": "Finetuning (vanilla), scratchpad finetuning, few-shot scratchpad prompting, padding/masking interventions",
            "effect_of_intervention": "Finetuning (any size) yields near-perfect in-distribution but poor OOD; few-shot scratchpad prompting yields significant improvement and scales with model size; masking distractors during training can restore generalization.",
            "performance_metrics": "Multiple sizes evaluated; all show similar finetuning OOD pathologies. Specific sizes referenced: 244M, 422M, 1B, 64B, 128B; per-step error rates presented for 128B in figures (qualitative descriptions).",
            "notable_failure_modes": "Strong bias toward parallel non-sequential heuristics; sensitivity to distractors and position bias bins when finetuned only on short instances.",
            "comparison_to_humans_or_symbolic": "LaMDA weights do not inherently encode reliable long-length algorithmic execution; symbolic algorithms or recurrent architectures may more naturally realize sequential state-tracking.",
            "uuid": "e3147.2",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Vanilla finetuning",
            "name_full": "Standard finetuning on task-specific examples (no scratchpad)",
            "brief_description": "Finetuning pretrained transformers to produce final answers (not intermediate scratchpad steps); used to test whether learning from many short instances generalizes to longer ones.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA checkpoints (various sizes)",
            "model_description": "Finetuned with AdaFactor, tuned learning rate, batch size, dropout; training until in-distribution validation accuracy settled (e.g., 20k steps for parity).",
            "arithmetic_task_type": "Parity and boolean variable assignment",
            "reported_mechanism": "Finetuning induces models to learn non-sequential spurious correlations/pooling strategies that exploit superficial cues rather than algorithmic state updates.",
            "evidence_for_mechanism": "Models achieve near-perfect in-distribution accuracy but show rapid OOD degradation; shuffled-ops baseline (removing sequential order) performs similarly, indicating reliance on non-sequential cues.",
            "evidence_against_mechanism": "None that overturns the conclusion; variation in hyperparameters drastically changed OOD behavior even with similar in-distribution loss, supporting mechanism identification.",
            "intervention_type": "Finetuning hyperparameter sweeps, shuffled-ops baseline",
            "effect_of_intervention": "Finetuning hyperparameters significantly affect OOD results (some hyperparam settings yield better OOD but inconsistent); shuffling operations indicates finetuned models exploit dataset spurious correlations.",
            "performance_metrics": "In-distribution accuracy (near perfect); OOD accuracy decays to near random for lengths beyond training (plots across lengths 3–40 or up to 32 depending on experiment).",
            "notable_failure_modes": "Does not generalize to longer instances even with increased model/data scale; susceptible to learning shortcuts rather than algorithms.",
            "comparison_to_humans_or_symbolic": "Finite-data finetuning does not cause the model to learn the underlying sequential algorithm as a symbolic executor would.",
            "uuid": "e3147.3",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Scratchpad finetuning",
            "name_full": "Finetuning using scratchpad (chain-of-thought) targets",
            "brief_description": "Finetuning models to output explicit intermediate computation steps (scratchpad) before final answer, with the hope of inducing sequential algorithmic behavior.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA checkpoints (various sizes)",
            "model_description": "Same base models as finetuning experiments; targets include multi-step intermediate states (e.g., running parity states or per-line variable values).",
            "arithmetic_task_type": "Parity and boolean variable assignment (intermediate step outputs)",
            "reported_mechanism": "Intended to teach models to produce sequential internal representations (explicit state per step), but finetuned models still tend to fail OOD because attention patterns do not generalize and distractors degrade stepwise construction.",
            "evidence_for_mechanism": "Scratchpad finetuning yields similar OOD failures to vanilla finetuning (Figure 4); per-step scratchpad error rates increase when input length is OOD and attention doesn't focus on relevant tokens; padded-scratchpad helps somewhat but does not eliminate failures.",
            "evidence_against_mechanism": "Masking distractors shows that when irrelevant tokens are removed, scratchpad finetuned models can generalize perfectly, implying failures stem from distractors rather than inability to learn stepwise content per se.",
            "intervention_type": "Padded scratchpad, masking distractors (input/scratchpad), EOS/padding experiments",
            "effect_of_intervention": "Padded scratchpad ameliorates position-bias issues partially; masking distractors (especially input distractors) restores perfect length generalization; EOS/prediction training concerns tested but not sole cause.",
            "performance_metrics": "Scratchpad finetuned models: strong in-distribution but poor OOD (plots shown). Masking distractors yields near-perfect OOD scratchpad prediction.",
            "notable_failure_modes": "Attention patterns not generalizing to longer contexts; model distracted by previous scratchpad tokens and irrelevant input tokens; premature EOS emissions can harm extrapolation.",
            "comparison_to_humans_or_symbolic": "Producing intermediate steps resembles human chain-of-thought, but finetuning does not guarantee the emergence of a robust algorithmic internal representation across lengths.",
            "uuid": "e3147.4",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Few-shot scratchpad prompting",
            "name_full": "Few-shot in-context learning with chain-of-thought/scratchpad exemplars",
            "brief_description": "Prompting pretrained LLMs with a small number of examples that include explicit intermediate reasoning steps (scratchpad) to induce generation of stepwise solutions on new longer instances without weight updates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA 128B (demonstrated) and other LaMDA sizes",
            "model_description": "Pretrained decoder-only transformers; no finetuning when applying few-shot scratchpad prompting (though few-shot finetuning variants were also tested).",
            "arithmetic_task_type": "Parity (coin-flip), variable assignment (programs) via stepwise scratchpad generation",
            "reported_mechanism": "Template-based variable-length pattern matching: models learn to copy/apply the scratchpad solution template from short exemplars to much longer queries, effectively performing stepwise state tracking via in-context induction rather than weight-encoded algorithm.",
            "evidence_for_mechanism": "Large LaMDA models can map a length-3 scratchpad exemplar to correctly generate a 20-step scratchpad (Figure 8); performance improves with few-shot scratchpad compared to zero-shot finetuned scratchpad in parity; authors observe template matching behavior.",
            "evidence_against_mechanism": "When the pretrained model has poor base performance on the task, few-shot scratchpad finetuning yields mixed or poor OOD results; suggests dependence on pretrained capabilities.",
            "intervention_type": "Few-shot prompting (in-context exemplars with scratchpad), few-shot finetuning variants",
            "effect_of_intervention": "Dramatic improvements in length generalization for parity; enables variable-length template matching and extrapolation; scales with model size (reported qualitatively and in related works).",
            "performance_metrics": "Qualitative plots show substantial OOD accuracy improvement up to 20 steps on parity; no single numeric accuracy reported across all lengths, described as 'highly nontrivial accuracy levels.'",
            "notable_failure_modes": "Effectiveness depends on base pretrained model capability and prompt style; some prompt styles produce weak non-finetuned performance and then few-shot finetuning still fails OOD.",
            "comparison_to_humans_or_symbolic": "Behavior resembles humans applying an explicit stepwise strategy learned from examples; differs from symbolic algorithms because the algorithmic behavior is induced in-context rather than encoded in weights.",
            "uuid": "e3147.5",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Parallel shortcut / counting heuristic",
            "name_full": "Transformer parallel pooling/counting shortcut",
            "brief_description": "Observed inductive bias where transformers solve parity-like tasks by performing pooling/counting over tokens and thresholding rather than left-to-right sequential state updates.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA family (experiments on 244M, 422M, 1B, 64B, 128B)",
            "model_description": "Self-attention transformers whose equivariant operations facilitate set/pooling computations enabling bottom-up counting strategies.",
            "arithmetic_task_type": "Parity (count number of ones) and other tasks where global counts can serve as shortcuts",
            "reported_mechanism": "Self-attention acts as an equivariant transformation capable of pooling (e.g., counting, max-pooling), enabling models to implement non-sequential counting + threshold heuristics that fit training distribution but fail to extrapolate.",
            "evidence_for_mechanism": "Varied-number-of-ones experiment: fixing token count and varying number of ones yielded models that achieved 100% in-distribution but OOD ≈ random; periodic outputs biased to 0/1 depending on ratios indicates counting threshold.",
            "evidence_against_mechanism": "Few-shot scratchpad prompting can induce sequential-like behavior despite underlying transformer tendency, showing the heuristic is not an absolute limitation.",
            "intervention_type": "Changing data distribution (fix tokens, vary ones), scratchpad prompting, masking distractors",
            "effect_of_intervention": "Data modification reveals shortcut; scratchpad prompting can override shortcut by encouraging stepwise outputs; masking distractors can allow learning true sequential algorithm.",
            "performance_metrics": "In the counting shortcut regime, OOD performance looks like random guessing on held-out number-of-ones ranges and periodic near-0/near-100 behaviors.",
            "notable_failure_modes": "No transfer to different-length instances because threshold/count heuristics are tied to training distribution statistics.",
            "comparison_to_humans_or_symbolic": "Contrasts with symbolic left-to-right algorithmic counting (or recurrent networks that can implement dynamical counting); reflects architectural inductive bias of self-attention.",
            "uuid": "e3147.6",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Computational graph depth",
            "name_full": "Computational Graph Depth (longest dependency chain)",
            "brief_description": "Task-specific notion of difficulty defined as the length of the longest dependency chain leading to the queried variable, shown to better predict transformer performance than raw operation count.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA family (various sizes)",
            "model_description": "Used as an analysis metric for the variable assignment task to characterize difficulty from a transformer's perspective.",
            "arithmetic_task_type": "Boolean variable assignment / program execution",
            "reported_mechanism": "Transformers handle instances with small computational graph depth well even if overall program length is OOD; performance correlates with depth because transformer parallel strategies resolve low-depth dependencies effectively.",
            "evidence_for_mechanism": "Experiments show accuracy remains roughly unchanged when number of operations is increased out-of-distribution but computational graph depth stays in-distribution (Figure 13).",
            "evidence_against_mechanism": "None presented; metric explains observed behavior across experiments.",
            "intervention_type": "Data generation and analysis (no architectural intervention)",
            "effect_of_intervention": "Provides diagnostic insight: training regimes should consider computational graph depth rather than raw length to evaluate expected transformer's success.",
            "performance_metrics": "Plots show accuracy evolution by computational graph depth over training iterations; depth is better predictor of final accuracy than operation count.",
            "notable_failure_modes": "Models fail as depth increases beyond training distribution; naive emphasis on training more long programs (by line count) may not address deep dependency generalization.",
            "comparison_to_humans_or_symbolic": "Symbolic executors' difficulty aligns with sequential operation count, but transformers' effective difficulty correlates with dependency depth, differing from naive symbolic metrics.",
            "uuid": "e3147.7",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Distractor masking",
            "name_full": "Masking distracting input and scratchpad tokens intervention",
            "brief_description": "Intervention that masks out irrelevant (distractor) tokens in the input and previously generated scratchpad tokens during training to test their influence on length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LaMDA finetuned with scratchpad targets",
            "model_description": "Transformer models trained on parity/scratchpad targets with selective masking applied to input and/or preceding scratchpad tokens.",
            "arithmetic_task_type": "Parity (scratchpad step prediction)",
            "reported_mechanism": "Distractor tokens in input and previous scratchpad steps disrupt attention patterns needed for sequential stepwise computation; removing these distractors allows the model to focus and generalize.",
            "evidence_for_mechanism": "Masking experiments (four conditions) show removing all distractors yields perfect length generalization; input-side distractors hurt performance more than scratchpad-side distractors (Figure 15).",
            "evidence_against_mechanism": "None; results directly support the distractor hypothesis.",
            "intervention_type": "Manual masking of distractor tokens (input, scratchpad, both) and padded scratchpad",
            "effect_of_intervention": "Masking both input and scratchpad distractors yielded perfect OOD generalization; masking input distractors alone gave large benefit; padded scratchpad partially helps but not fully.",
            "performance_metrics": "Plots show accuracy of scratchpad token prediction across steps and input lengths: masking all distractors yields near-perfect accuracy across OOD lengths (quantitative numbers not tabulated but clearly near 100% in figures).",
            "notable_failure_modes": "When distractors are present, attention is misdirected and per-step error rates increase on OOD inputs; distractors in input are more harmful than in preceding scratchpad.",
            "comparison_to_humans_or_symbolic": "Humans can ignore distractors when reasoning; masking mimics cleaner, noiseless symbolic execution context enabling algorithmic behavior to surface.",
            "uuid": "e3147.8",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Position biases & EOS",
            "name_full": "T5 position biases and EOS token prediction effects",
            "brief_description": "Architectural/learning factors investigated as potential causes of poor length extrapolation: untrained positional bias bins (T5 biases) for far positions and training to predict EOS tokens.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LaMDA family (T5-style position bias implementation)",
            "model_description": "Models use T5 position biases to encode token positions; EOS token prediction behavior studied as prior work implicated premature EOS harms length extrapolation.",
            "arithmetic_task_type": "Parity and scratchpad-based step generation",
            "reported_mechanism": "Untrained position-bias bins for far positions could impair attention to distant tokens; EOS training may cause premature termination and learned representations that do not generalize to longer sequences.",
            "evidence_for_mechanism": "Padded-scratchpad (equalizing positional bins across lengths) improved but did not eliminate OOD failures, indicating position biases contribute but are not sole cause; EOS-related concerns mentioned and tested but not found to fully explain failures.",
            "evidence_against_mechanism": "Padded-scratchpad experiments show that even when position encoding is controlled the model still struggles unless distractors are masked, so position bias/EOS are not primary drivers.",
            "intervention_type": "Padded scratchpad (left/right padding to keep T5 bins aligned), experiments controlling EOS prediction",
            "effect_of_intervention": "Padded scratchpad helps somewhat but OOD generalization issues persisted; EOS considerations alone do not explain poor generalization.",
            "performance_metrics": "Qualitative improvements with padding but residual error remains (figures showing per-step error rates); no single numeric improvement provided.",
            "notable_failure_modes": "Position bias limitations and EOS effects can exacerbate OOD failures but do not fully account for distrator-driven attention failures.",
            "comparison_to_humans_or_symbolic": "Symbolic execution is position-agnostic to the extent it processes program semantics; positional encoding is an implementation detail affecting transformer extrapolation capability.",
            "uuid": "e3147.9",
            "source_info": {
                "paper_title": "Exploring Length Generalization in Large Language Models",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2
        },
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 2
        },
        {
            "paper_title": "The eos decision and length extrapolation",
            "rating": 2
        },
        {
            "paper_title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "rating": 2
        }
    ],
    "cost": 0.01876775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Exploring Length Generalization in Large Language Models</h1>
<p>Cem Anil ${ }^{\text {1, }}$, Yuhuai $\mathrm{Wu}^{2}$, Anders Andreassen ${ }^{1}$, Aitor Lewkowycz ${ }^{1}$<br>Vedant Misra ${ }^{1}$, Vinay Ramasesh ${ }^{1}$, Ambrose Slone ${ }^{1}$, Guy Gur-Ari ${ }^{1}$, Ethan Dyer ${ }^{1}$, Behnam Neyshabur ${ }^{1}$<br>${ }^{1}$ Google Research, Blueshift Team<br>${ }^{2}$ Google Research<br>${ }^{3}$ University of Toronto, Vector Institute</p>
<h4>Abstract</h4>
<p>The ability to extrapolate from short problem instances to longer ones is an important form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale. We then show that combining pretrained large language models' in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.</p>
<h2>1 Introduction</h2>
<p>Many natural problems, such as theorem proving and program synthesis, have a notion of length that strongly correlates with the difficulty of the task. However, in these domains, the number of available problems typically drops rapidly as a function of problem length (e.g. Figure 2). Hence, it is desirable to learn from examples of shorter lengths to generalize to longer ones or at least reduce the number of samples required for longer examples. We refer to this type of problem as length generalization.</p>
<p>Recent work on large language models (LLMs) has shown consistent improvement in their performance by scaling model and dataset size. However, such models are still incapable of length generalization. For example, [1] shows that even though scale helps with solving arithmetic problems, scale alone is likely insufficient for learning to solve instances of arbitrary lengths. This implies that models fail to learn the general algorithms that would enable this kind of generalization. Indeed, Razeghi et al. [2] showed that the performance of LLMs on mathematical calculations correlates with term frequency in the training data. This suggests that LLMs might have gained their current performance from surface-level memorization instead of learning to apply the correct algorithm.</p>
<p>A recent line of work proposes to use a scratchpad, or chain-of-thought reasoning, when prompting LLMs [3, 4, 5] on multi-step tasks. Breaking down tasks into multiple small steps and presenting these steps to the model leads to improved performance across a variety of reasoning tasks including word problems, arithmetic, and code execution.</p>
<p>We perform a systematic study of length generalization with transformer-based large language models. We consider problems in which learning an algorithm can in principle enable a model to extrapolate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of variable assignment problems: Can transformer language models learn from short instances of the Variable Assignment task (left) to extrapolate to much longer instances (right)? Length generalization is the ability to learn from shorter/easier instances of a problem to handle longer/harder instances.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Techniques</th>
<th style="text-align: center;">In-distribution</th>
<th style="text-align: center;">Out-of-distribution</th>
<th style="text-align: center;">Improves with scale</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fine-tune</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
</tr>
<tr>
<td style="text-align: center;">Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Fine-tune + Prompting + Scratchpad</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark^{+}$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance on length generalization tasks of three techniques that language models admit: (1) Finetuning, (2) Prompting (or in-context few-shot learning) and (3) Scratchpad (Chain-of-Thought reasoning). We find that each technique (and the combinations thereof) have different modes of failure and present different trade-offs regarding in and out-of-distribution coverage. $\times$ signifies poor $\checkmark$ signifies nontrivial, $\checkmark \checkmark$ signifies near-perfect performance. (*) Refers to task-dependency.
from short examples to problems of arbitrary length. In particular, we focus on two simple algorithmic tasks, parity and variable assignment, in which the model needs to keep track of a state in order to extrapolate to longer lengths (see Figure 1). These problems are illuminating because their simplicity allows us to probe the failure modes as well as contrast the learned solutions with the ground truth algorithm. They provide us with a setting to study how/when these large language models start to fail.</p>
<p>We study combinations of three kinds of techniques for LLMs: finetuning, few shot prompting (also referred to as in-context learning), and use of a scratchpad (also referred to as chain-of-thought), to understand the role of each method and the interplay among the three in length generalization. Interestingly, we observe non-trivial interactions among the three techniques; see Table 1.</p>
<p>Contributions Our main contributions are as follows:</p>
<ul>
<li>We define and characterize the problem of length generalization using notions such as state tracking, execution depth, and per-step error rate. We study and carefully design two tasks, parity and variable assignment, that measure length generalization (Section 2).</li>
<li>We find that in the finetuning regime, scaling data, model sizes, and compute does not improve length generalization (Section 3.1). We also observe that even when the model attains perfect in-distribution accuracy, it performs poorly in out-of-distribution domains. Surprisingly, different hyperparameter choices for finetuning have a large effect on length generalization performance, while having minimal effect on the final in-distribution performance (Section 3.3).</li>
<li>We establish finetuning with scratchpad also fails to generalize to longer problems, in contrast to what is suggested by previous works [3]. We look into three potential failure cases: positional encoding, the presence of distractors, and end of token prediction, and conclude that distractors are the main culprit of failures for length generalization (Section 4).</li>
<li>We show that in the in-context learning regime, use of a scratchpad shows a qualitatively different behavior and significantly alleviates the decay of performance on longer problems. This capability is significant, as it implies that for LLMs, there are certain skills, like length generalization, that can be learned through in-context learning rather than through finetuning even in the presence of infinite data. This is in stark contrast to the common norms of machine learning (Section 5).</li>
</ul>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Real world datasets have heavy tails in length: (left) Histogram of lengths for proofs presented in the Archive of Formal Proofs (right) Histogram of the number of tokens for solutions in the MATH dataset. [6]</p>
<h1>2 Length Generalization</h1>
<p>Many sequence tasks-especially ones that require reasoning capabilities-have problem instances that differ in terms of their lengths. Shorter instances are often easier to state, process, and handle, and require less compute to find the answer. By contrast, longer instances are more challenging to parse and require more compute to solve. Tasks that have a reasoning component are especially well represented in this category - multi-hop reasoning [7], program execution [8], deductive reasoning [9] and theorem proving [10], to name a few. Note that having to deal with differing problem lengths poses two significant challenges. First, it is often the case that one encounters longer problem instances than the ones ever encountered during training, and is required to extrapolate. Second, even though longer problem instances have much more variety, real-world datasets often contain few long instances (see Figure 2). Both of these challenges are exacerbated if learning agents are not able to generalize across and beyond the lengths they learn from during training. This paper is about investigating to what extent transformer based language models are able to observe short problem instances and extrapolate to longer ones.
Instance Length as Number of Steps in a Markov Process It is possible to define problem length in many different ways to capture different aspects of problem difficulty. Does there exist a notion of length that would expose the same length-generalization-related problem structure observed in qualitatively very different settings? Such a framing would enable researchers to design algorithms and interventions that have the potential to generalize across a broad range of tasks. To this end, we take the approach of characterizing length in the context of a deterministic Markov process. From this perspective, length is simply the number of state transitions experienced by an initial world state. In other words, the data-generation process can be described as sampling an (1) initial state and a (2) variable number of state transformations to be applied sequentially on the initial state. The agent is provided both the initial state and the transformations, and is asked to predict the final state. This framing applies to a wide range of sequence problems, if not all of them-ranging from more mechanical tasks such as code and algorithm execution and theorem proving, to less structured tasks, such as solving math problems and summarizing novels.
In our empirical investigation we focus on two synthetic tasks: parity and variable assignment. These tasks avoid problem-specific subtleties that could mislead our analyses, while strongly capturing the deterministic Markov process structure.</p>
<h3>2.1 Tasks</h3>
<p>Parity: The parity task is an age-old learning problem that requires the trained agent to predict whether a bit-string has an even or odd number of ones in it. For example, the parity of the bitstring $[0,1,1,0,1]$ is "odd" (or 1) as opposed to "even" (or 0 ), because there is an odd number of 1 s in the bit-string. The parity task admits a sequential solution that enables length generalization in a straightforward way: simply process the bits left-to-right and record the parity of the bits processed so far as the state. The default notion of length in the parity task is the number of bits in the input. However, we also experiment with a version where the number of bits is kept constant, and the number of 1 s (i.e. the parity flipping bit) is systematically varied. The number of 1 s stands for the number of state changes contained in the input bit-string, and actually appears to capture a more relevant notion of length for transformer models (see Section 3.1).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Finetuned Length Generalization performance doesn't improve with scale: Models of vastly different scales fail at length generalization on both Parity and Variable Assignment tasks, displaying identical generalization pathologies. The x -axis represents problem length and the y -axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>Boolean Variable Assignment Task: The Boolean Variable Assignment task is designed to capture arbitrarily long, potentially branching unidirectional execution flows. An instance of this task can be seen in Figure 1. The inputs consist of semantically correct (i.e. bug-free) Python programs in which each line contains a boolean variable assignment operation. The output is simply the value of the variable presented in the final line of the program. The sequential solution to this task is to simply execute the program line by line while keeping track of the state of all variables.</p>
<p>The data generation procedure involves randomly generating execution flows that involve Boolean operations; see Supplementary Material (SM) for details.</p>
<p>We focus our evaluations on two variants of this dataset. (1) The diverse variable assignment split consists of a wide range of boolean operators available and is intended to contain maximally diverse programs. (2) The chain-like variable assignment split consists only of operations that compose the values of already defined variables. This results in long chains of dependencies between the initial values of the variables and the queried one, ensuring that there are almost no redundant operations in the program (i.e. operations that can be removed without affecting the output of the program). This split emphasizes the sequential nature of the variable assignment problem.</p>
<h1>3 Standard Finetuning Fails at Length Generalization</h1>
<p>We begin by demonstrating that finetuning transformer models on length-generalization tasks results in poor out-of-distribution performance. In experiments we use LaMDA ${ }^{2}$ decoder-only models. These checkpoints were trained using general natural language data. We use the AdaFactor optimizer [11] during finetuning, and tune the learning rate, batch size and dropout. We trained the networks until the in-distribution validation accuracy settles (20000 gradient steps for parity and 18000 gradient steps for variable assignment). The loss was only computed on the target tokens (i.e. the model wasn't trained to model the input questions).</p>
<h3>3.1 Scale Doesn't Improve Length Generalization</h3>
<p>Parity: We finetuned four pretrained LaMDA models with $244 \mathrm{~m}, 422 \mathrm{~m}, 1 \mathrm{~b}$ and 64 b parameters on the parity task, where the training distribution included randomly sampled bitstrings of length 10 to 21. We then evaluated the performance on bitstrings of length 3 to 40; see Figure 3. We find that model scale has a little effect on length generalization.
Variable Assignment: We finetuned the same models on the chain-like Variable Assignment Task, described in Section 2. We kept the in-distribution lengths at 3 to 8 , and evaluated the test performance on lengths 3 to 19. The results can be seen in Figure 3. Just like in the parity task, while the indistribution performance is (near) perfect, out-of-distribution performance degrades rapidly as length increases. To get a sense of just how weak the out-of-distribution performance is, we also trained a 422 m model on the same dataset, except we shuffled the operations before feeding it to the model. This removes the sequential dependency between the operations, and helps us establish a</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Scratchpad finetuning displays poor length generalization: Scratchpad finetuning displays qualitatively similar length generalization pathologies as vanilla finetuning. The x-axis represents problem length and the y-axis represents the accuracy attained at that problem length. The training lengths are highlighted in grey.</p>
<p>strong baseline that only predicts the answers based on non-sequential, spurious correlations. The accuracy-length curves for the baseline can be found in SM.</p>
<h3>3.2 Transformers Prefer Parallel Strategies over Sequential Ones</h3>
<p>The results presented in Section 3.1 establish that, when presented with sequential length generalization problems, transformers are biased toward learning non-sequential "shortcut" solutions that fail at longer problem instances. We ran additional experiments to gain a better understanding of the nature of this generalization pattern.</p>
<p>On parity, we ran finetuning on a different distribution of bit-strings: Instead of first randomly sampling the number of bits in the input bit-string, then sampling the values of the bits, we fixed the total number of bits in the input, and only varied the <em>number of ones</em> in the bit-string uniformly. We trained with 10 to 20 ones in the input distribution and tested on an interval containing 1 to 30 ones. This makes sure that the number of tokens (now fixed at 30) is now disambiguated from number of state changes, which for parity is equal to the number of ones. The difference between in and out-of-distribution performance is even starker for this data distribution (Figure 5): while in-distribution performance was 100%, OOD performance was roughly equivalent to random prediction<sup>3</sup>. This suggests that the transformers are learning a non-sequential solution that involves counting the number of ones in the input, and then thresholding the output. This is not surprising, given that self-attention is an equivariant transformation capable of performing pooling operations like max-pooling [12]. This strategy doesn't allow for knowledge transfer between problems of different lengths. Note that this bottom-up counting behaviour is complementary to the left-to-right counting behaviour displayed by recurrent models Suzgun et al. [13]. On the variable assignment dataset, we finetuned a 255m LaMDA model on the <em>diverse</em> split of the variable assignment dataset of programs up to 16 lines, and evaluated on the same data generating distribution up to 32 lines. We measured the evolution of the model's accuracy with respect to training iterations on different program lengths (quantified by number of lines). The results are in SM.</p>
<p>We again observed that a different notion of length (which we call <em>computational graph depth</em>) captures the difficulty of problem instances better than number of program operations. A variable assignment program can be represented as a computational graph where each node corresponds to a variable, and each edge corresponds to an operation. Computational graph depth is the length of the longest dependency chain that connects to the queried variable node. This notion of length corresponds to the highly parallelizable strategy of executing programs by iteratively resolving computational graph dependencies. We present two results that suggest that computational graph depth is a more relevant notion of length for transformers. (1) Inspecting the order of problem instances in which the trained transformer correctly solves this task, we find that performance is strongest on examples with small computational graph depth, even if these examples are long in</p>
<p><sup>3</sup>The periodic 0% and near 100% performance on OOD lengths is due to the models' tendency to output 0 or 1 depending on whether the input has a significantly higher ratio of 0s or 1s. On average, the accuracy on OOD length is not better than random guess.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: (left) Complete lack of length generalization: Transformers trained on the parity task have difficulty generalizing to bit-strings that have a different number of $1 s$. (right) Sensitivity to hyperparameters: Trained networks sharing architecture, data and in-distribution loss can have very different length generalization performances. $l r$ stands "learning rate" and $b s$ stands for "batch size".
terms of number of operations. (2) The transformer does a good job of handling programs with an out-of-distribution number of operations, but for which computational graph depth is in-distribution.</p>
<h1>3.3 In-Distribution Generalization Doesn't Predict OOD Generalization on Length Generalization Tasks</h1>
<p>Prior work on out-of-distribution generalization establishes that in many tasks, in-distribution loss is a strong predictor of out-of-distribution generalization [14]. Our experiments on the parity task indicate that the distribution shift induced by changing problem lengths falls outside of the this category. Figure 5 shows how the same model trained on the same data achieving roughly the same in-distribution cross entropy loss behaves on OOD data, where the difference is solely induced by the choice of different hyperparameters.</p>
<h2>4 Scratchpad Finetuning Still Fails at Length Generalization</h2>
<p>It has been shown in prior work that it's possible to get pretrained LLMs to solve a given task by not only outputting the answer, but also the solution steps behind it. Nye et al. [15] use scratchpad finetuning to achieve strong in-distribution performance on execution based tasks such as code execution and computing polynomials. While they also report modest length generalization results on integer arithmetic, we find that scratchpad finetuning suffers from similar length generalization pathologies than vanilla finetuning does. The results on parity and variable assignment tasks can be seen in Figure 4. The precise scratchpad strategies used for these tasks are described in detail in SM.</p>
<p>Error analysis: To understand the causes of failure in training scratchpad strategies, we focused on two architectural choices that could account for the poor performance: (1) how transformers encode position information, and (2) whether the transformers are trained to predict an end-of-sequence (EOS) token. LaMDA models use T5 position biases [16] to handle position information. If the network is only trained with short instances, position biases that handle longer positional distances might not be trained, explaining poor length generalization. Similarly, Newman et al. [17] report that networks trained with EOS token prediction often suffer from generalizing to longer problem instances, because of the models' tendency to emit EOS tokens prematurely, as well as the EOS tokens' effect on the representations that get learned.</p>
<p>We tested the extent to which these effects can explain lack of length generalization as follows. We padded both the input bit-strings and the scratchpad content with dummy padding tokens to make the token count the same. We also augmented the input and scratchpad targets with the same number of padding tokens on the left and right so that the relevant bit to attend to when executing the sequential scratchpad strategy corresponds to the same T5 position bias bin. Examples of the updated input-target pairs can be seen in SM. While this intervention helps, the trained models still display significant length generalization issues.</p>
<p>To gain further insight about the source of the problem, we plotted how the scratchpad target prediction error rates change as a function of (1) how far along one is in constructing the scratchpad, and (2) the length of the input bit-string. The results can be seen in Figure 6. The fact that the model makes</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: (left) Effect of input length on per-step scratchpad accuracy: Points corresponds to the accuracy (y-axis) of the first $x$ scratchpad steps (x-axis) on parity instances of variable length (color). If the input length is out-of-distribution, even in-distribution scratchpad steps are inaccurate, implying the model hasn't learned an attention pattern that generalizes to longer bit-strings. (right) Roughly constant per-step error rate: The per-step error rates of the LaMDA 128b model, few-shot finetuned on the coin-flip version of the parity task remain roughly constant across the scratchpad steps. This is in stark contrast with zero-shot scratchpad finetuned models, where the per-step error rates increase abruptly when the model is evaluated on OOD lengths.
mistakes in in-distribution scratchpad steps when the input has an OOD length implies that the attention mechanism isn't capturing the relevant part of the input to form the scratchpad output. See SM for additional analysis.</p>
<h1>5 Scratchpad Prompting Significantly Improves Length Generalization</h1>
<p>Wei et al. [4], Nye et al. [15] and Lewkowycz et al. [5] showed that combining prompting (i.e. in-context learning) with scratchpad strategies present a powerful combination. They demonstrate that pretrained LLMs, without the help of any finetuning, can solve grade school math word problems and execute pieces of code with nontrivial correctness [15], when prompted with the right scratchpad strategy. We corroborate these findings, and report that scratchpad prompting endows pretrained LLMs with the capability of variable length template matching (see Figure 8). That is, in-context learning enables the model to "learn" solution steps from a small number of short instances, and apply the same template on significantly longer instances with a high degree of accuracy.</p>
<h3>5.1 Few-shot scratchpad</h3>
<p>Contrary to vanilla and scratchpad finetuning, we find that under the right conditions, few-shot scratchpad strategies sometimes significantly improves LLMs' capability to extrapolate to lengths much further than what pretraining weights grant them.</p>
<p>To evaluate the performance of few-shot conditioning with scratchpad inputs without any finetuning, we phrase the parity problem in natural language as a coin flipping task. An example for the few-shot prompts we used can be seen in Figure 8. Wei et al. [4] also report results on the coin-flip task: the scratchpad format we used differs from theirs in that while ours respects the sequential nature of the task (i.e. each coin flip corresponds to a step in the scratchpad solution), Wei et al. [4]'s scratchpad strategy involves summing up the number of coin flips, then deciding on the final output based on the evenness/oddness of the sum. Also, while they only test up to 4 flips, we go up to 20 flips while still attaining highly nontrivial accuracy levels.</p>
<p>For the variable assignment task, our scratchpad strategy involves copying over the program that's being executed, with comments added in between lines specifying the value of the variable that was assigned in the line above. Instances of this scratchpad strategy can be seen in SM.</p>
<p>Figure 7 shows the performance of the pretrained LaMDA 128b model on the coin-flip version of the parity task. Figure 8 shows an instance of how a length 3 prompt can induce the model to correctly output a 20 step scratchpad. We find that with the right scratchpad prompt, LLMs are able to generate correct scratchpad solutions. This reduces the problem to simply filling in the content of</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Few-shot finetuning with scratchpad displays qualitatively different behaviour on parity and variable assignment tasks. On parity, where the non-finetuned model already performs very well, few-shotfinetuning with scratchpad leads to a significant performance boost over zero-shot finetuning with scratchpad. On variable assignment, where the base model doesn't perform poorly, there's not a significant gap between few-shot finetuning and zero-shot finetuning with scrathpad. The performance of OpenAI's Codex model [18] on the variable assignment task is also provided.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Few-shot length generalization: The largest LaMDA model is able to map the scratchpad solution template from a few short exemplars onto much longer queries.</p>
<p>the generation correctly by inferring the right state transitions without having to figure out how to extrapolate the solution template.</p>
<p>Few-Shot Finetuning with Scratchpad Strategies: Does combining finetuning, few-shot prompting, and scratchpad strategies improve length generalization?</p>
<p>We find that the answer is <strong>yes</strong> in the case of parity. As seen in Figure 7, few-shot finetuning performs significantly better than the baseline model, both on in- and out-of-distribution lengths. Note that the vanilla (i.e. no shot) finetuning baseline also outperforms the no-finetuning baseline, it actually does worse on the larger lengths — a pathology that doesn't appear with few-shot finetuning.</p>
<p>The results point to a qualitatively different picture for the variable assignment task. Both few-shot finetuning and vanilla finetuning result in similar length generalization behavior (Figure 7). We hypothesize that this distinction is caused by the different pretrained performances that the model displays on these tasks: while length generalization is already strong with no finetuning on parity, that's not the case for variable assignment. In the latter case, the model is forced to acquire a new skill via finetuning, which displays the same pathologies as zero-shot finetuning with scratchpad. As a sanity check, we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining: As expected by the aforementioned hypothesis, we observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. The results can be found in SM. We leave a more rigorous evaluation of this hypothesis as future work.</p>
<h1>6 Related Works</h1>
<p>There have been many attempts to study generalization from shorter/easier to longer/harder examples.
Challenges in length generalization: Several existing works have investigated pathologies that arise when models are asked to generalize to processing and generating longer (measured by number of tokens) sequences. Newman et al. [17] find that sequence models trained with and in the absence of the end-of-sequence token display qualitatively different length extrapolation behaviour and learn different representations. Dubois et al. [19] proposes modifications to the commonly used dot-product attention to improve the models' ability to extrapolate to longer sequences. Murray and Chiang [20] demonstrate that neural machine translation models tend to have a bias towards generating shorter-than-desired translations. Yehudai et al. [21] show that length generalization issues are also present in training graph neural networks, where extrapolating across graph size presents a challenge. Ju et al. [22] propose a new attention mechanism to facilitate recurrent processing in transformer models. Press et al. [23] propose modifying transformer attention biases to facilitate generalization beyond the training context length. Concurrent work [24] propose a synthetic dataset named LEGO (Learning Equality and Group Operations), an instantiation of which resembles our variable assignment task where the only boolean operations allowed are assign and negate and assign, and overriding the values of variables is not allowed. Their analyses on OOD generalization largely complement ours: while we focus on decoder-only architectures and scratchpad strategies as a way of carrying over state, they focus on encoder-only architectures, and investigate the effect of weight-sharing.
Easy-to-Hard generalization: Schwarzschild et al. [25] and Bansal et al. [26] use weight-tied neural networks to generalize from easy to hard examples. Schwarzschild et al. [25] also provide three tasks to benchmark easy-to-hard generalization. Dehghani et al. [27] and Kaiser and Sutskever [28] assess the capabilities of their proposed architectures on easy-to-hard generalization problems.
Inductive Biases Related to Lenght Generalization: McCoy et al. [29] study the inductive bias of seq-to-seq learners on English question formation and English tense reinflection tasks and find that LSTM and GRU networks often display differing strategies, caused by the use of differing activation functions. Suzgun et al. [13] find that recurrent networks can perform dynamical counting, and encode hierarchical representations, which enables them to solve nontrivial Dyck tasks using k-counters. Kharitonov and Chaabouni [30] also study the inductive bias of different architectures, and conclude that transformer and LSTM architectural have a tendency to learn hierarchical strategies, whereas CNN based strategies display more linear structure. He et al. [31] propose a method to learn natural inference models that are not biased on spurious correlations. McCoy et al. [32] show that transformer models that display strong performance in natural language inference can have superficial biases that fool them in systematic ways and proposes a framework to think about these biases.</p>
<h2>7 Conclusion</h2>
<p>The ability to learn from shorter/easier problem instances to generalize to longer/harder ones is a key capability in a large number of tasks, especially ones requiring reasoning. We defined the concept of length generalization and measured language models' length generalization capabilities. After conducting careful experiments using finetuning, scratchpads, and few-shot prompting, we reached the following conclusions: (1) Generalizing in length is a challenge for language models at least up to the 100B parameter scale. Both vanilla finetuning and finetuning with scratchpads suffer from a lack of length generalization caused by models' tendency to pick up non-sequential pattern that don't apply to longer problem instances. (2) Few-shot scratchpad prompting enables pretrained large language models to pick up scratchpad-templates that extrapolate to arbitrary lengths, leading to dramatic improvements on longer problem instances. Unlike raw finetuning, this approach does scale with model size [4]. (3) Trying to further enhance the performance of few-shot scratchpad prompted LLMs via finetuning yields mixed results, depending on the non-finetuned performance of the base model at the target task. We emphasize that the aforementioned few-shot variable length pattern matching capability - something that doesn't require changing model architecture - offers a qualitatively different approach to handle length generalization in contrast to prior art that introduced architectural modifications to achieve the same goal. This capability is also significant in that it implies that for LLMs, there are certain skills, like length generalization, that can be learned better through in-context learning rather than through finetuning, even in the presence of infinite data.</p>
<h1>References</h1>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[2] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.
[3] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[4] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models, 2022. URL https://arxiv.org/abs/2201.11903.
[5] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.
[6] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.
[7] Haoyu Wang, Mo Yu, Xiaoxiao Guo, Rajarshi Das, Wenhan Xiong, and Tian Gao. Do multi-hop readers dream of reasoning chains? arXiv preprint arXiv:1910.14520, 2019.
[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.
[9] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. arXiv preprint arXiv:2002.05867, 2020.
[10] Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse. Int: An inequality benchmark for evaluating generalization in theorem proving. arXiv preprint arXiv:2007.02924, 2020.
[11] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. PMLR, 2018.
[12] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, pages 3744-3753. PMLR, 2019.
[13] Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Lstm networks can perform dynamic counting. arXiv preprint arXiv:1906.03648, 2019.
[14] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. arXiv preprint arXiv:2010.15775, 2020.
[15] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.
[16] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019.
[17] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and length extrapolation. arXiv preprint arXiv:2010.07174, 2020.</p>
<p>[18] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[19] Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. arXiv preprint arXiv:1911.03872, 2019.
[20] Kenton Murray and David Chiang. Correcting length bias in neural machine translation. arXiv preprint arXiv:1808.10006, 2018.
[21] Gilad Yehudai, Ethan Fetaya, Eli Meirom, Gal Chechik, and Haggai Maron. From local structures to size generalization in graph neural networks. In International Conference on Machine Learning, pages 11975-11986. PMLR, 2021.
[22] Da Ju, Stephen Roller, Sainbayar Sukhbaatar, and Jason Weston. Staircase attention for recurrent processing of sequences. arXiv preprint arXiv:2106.04279, 2021.
[23] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.
[24] Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.
[25] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Advances in Neural Information Processing Systems, 34, 2021.
[26] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Logical extrapolation without overthinking. arXiv preprint arXiv:2202.05826, 2022.
[27] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.
[28] Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.
[29] R Thomas McCoy, Robert Frank, and Tal Linzen. Does syntax need to grow on trees? sources of hierarchical inductive bias in sequence-to-sequence networks. Transactions of the Association for Computational Linguistics, 8:125-140, 2020.
[30] Eugene Kharitonov and Rahma Chaabouni. What they do when in doubt: a study of inductive biases in seq2seq learners. arXiv preprint arXiv:2006.14953, 2020.
[31] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. arXiv preprint arXiv:1908.10763, 2019.
[32] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.
[33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958, 2014.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Parity problem instances: (left) A sample 8-bit parity problem instance, along with the scratchpad targets. The scratchpad represent the intermediate parity state as the sequence is processed left to right. (right) A parity problem instance with the padded scratchpad strategy. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant even when there are different number of bits in the input.</p>
<h1>A Data Generation Details</h1>
<p>We describe the data generation procedures for the parity and variable assignment tasks in detail.</p>
<h2>A. 1 Parity Datasets:</h2>
<p>Synthetic Parity Dataset: A 8-bit example of the synthetic parity example, along with the corresponding scratchpad targets, can be seen in Figure 9. We added the prefix " $&gt;&gt;&gt;$ " to signify the start of the parity sequence, and the suffix "==" to signify the start of the target or scratchpad tokens. There's no special meaning associated with the particular prefixes and suffixed used.</p>
<p>We experimented with two version of the synthetic parity dataset: In one split, we varied the number of bits in the input, and in the other one, we varied the number of ones.</p>
<ul>
<li>Varied number of bit split: To generate the samples in this split, we first sampled the number of bits, then sampled each bit individually from a uniform Bernoulli distribution. For training, we used lengths between 3 and 20, and for validation/testing, we used lengths between 3 and 40 .</li>
<li>Varied number of ones split: Here, we fixed the number of bits at 30. To sample each instance, we first uniformly sampled the number of ones, then randomly placed each one in the fixed-length bitstring by randomly shuffling the bits. We used 10 to 20 ones in the training split, and 1 to 30 ones in the validation/test splits.
Padded scratchpad: The padded scratchpad format can be seen in 9. Both the input and the scratchpad targets are padded left and right with the same number of padding tokens respectively, such that the relevant bit to attend to while constructing the scratchpad bits is always equidistant. Moreover the total number of characters/tokens is also kept constant. The number of tokens to pad on the left and right is determined (uniformly) randomly.</li>
</ul>
<p>The parity datasets contain 1000000 samples.
Natural Language Parity Dataset: In order to tap into the natural language understanding capabilities of pretrained language models, we situated the parity task as a "coin flip problem". In this framing, flipping a coin corresponds to 1 and not flipping a coin corresponds to 0 . To make the inputs as close as possible to English without occupying too many tokens, we used the sentence templates "Then <NAME> flips." and "Then <NAME> doesn't flip." to represent whether the coin was flipped or not respectively, where "<NAME>" refers to a randomly sampled given name. We also prepended each step with an integer id that count backwards from the total number of steps there are in the input sequence. We've experimented with versions where the integer ids are incremented. This didn't lead to a significant difference in the overall performance.</p>
<p>Two representative sample input-target pairs (including the exemplars) are provided in Figure 10.</p>
<h2>A. 2 Boolean Variable Assignment Dataset:</h2>
<p>An instance of the variable assignment dataset can be seen in Figure 11. The data generation procedure is aimed at synthesizing large number of qualitatively different programs: (1) A subset of boolean variable assignment operations is uniformly sampled from a large pool of operations. (2) The number</p>
<p>Input:
Question The coin is heads up. (4) Then Williams flips. (3) Then Ward flips. (2) Then Valentine doesn't flip. (1) Then Son doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Williams flips, coin turns to tails. (3) After Ward flips, coin becomes heads. (2) Valentine doesn't flip, so coin stays heads. (1) Son doesn't flip, so coin remains heads. DONE
#
Question The coin is heads up. (4) Then Shade flips. (3) Then Kong doesn't flip. (2) Then Kodi flips. (1) Then Charleston flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Shade flips, coin turns to tails. (3) Kong doesn't flip, so coin stays tails. (2) After Kodi flips, coin becomes heads. (1) After Charleston flips, coin turns to tails. DONE
#
Question The coin is heads up. (4) Then Ka doesn't flip. (3) Then Justice flips. (2) Then Johan flips. (1) Then Jamaica flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) Ka doesn't flip, so coin remains heads. (3) After Justice flips, coin becomes tails. (2) After Johan flips, coin turns to heads. (1) After Jamaica flips, coin becomes tails. DONE
#
Question The coin is heads up. (4) Then Cy flips. (3) Then Booker flips. (2) Then Ace doesn't flip. (1) Then Ren flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Cy flips, coin turns to tails. (3) After Booker flips, coin becomes heads. (2) Ace doesn't flip, so coin stays heads. (1) After Ren flips, coin turns to tails. DONE #
Question The coin is heads up. (6) Then Tristan flips. (5) Then Hillary flips. (4) Then Olivia flips. (3) Then Rosa doesn't flip. (2) Then Kurt flips. (1) Then Glenn doesn't flip. Is the coin still heads up?"</p>
<p>Scratchpad targets:
"Solution Coin is initially heads up. (6) After Tristan flips, coin becomes tails. (5) After Hillary flips, coin turns to heads. (4) After Olivia flips, coin becomes tails. (3) Rosa doesn't flip, so coin remains tails. (2) After Kurt flips, coin turns to heads. (1) Glenn doesn't flip, so coin stays heads. DONE</p>
<p>Input:
Question The coin is heads up. (4) Then Katarina doesn't flip. (3) Then January flips. (2) Then Duke flips. (1) Then Cal doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Katarina doesn't flip, so coin remains heads. (3) After January flips, coin becomes tails. (2) After Duke flips, coin turns to heads. (1) Cal doesn't flip, so coin stays heads. DONE
#
Question The coin is heads up. (4) Then Berry flips. (3) Then Abbi flips. (2) Then Tam flips. (1) Then Ikea doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Berry flips, coin becomes tails. (3) After Abbi flips, coin turns to heads. (2) After Tam flips, coin becomes tails. (1) Ikea doesn't flip, so coin remains tails. DONE #
Question The coin is heads up. (4) Then Cain doesn't flip. (3) Then Woody flips. (2) Then Von doesn't flip. (1) Then Thu doesn't flip. Is the coin still heads up?InSolution Coin is initially heads up. (4) Cain doesn't flip, so coin stays heads. (3) After Woody flips, coin turns to tails. (2) Von doesn't flip, so coin remains tails. (1) Thu doesn't flip, so coin stays tails. DONE
#
Question The coin is heads up. (4) Then Russ flips. (3) Then Williams flips. (2) Then Ward doesn't flip. (1) Then Valentine flips. Is the coin still heads up?InSolution Coin is initially heads up. (4) After Russ flips, coin becomes tails. (3) After Williams flips, coin turns to heads. (2) Ward doesn't flip, so coin remains heads. (1) After Valentine flips, coin becomes tails. DONE
#
Question The coin is heads up. (8) Then Melissa flips. (7) Then Kevin doesn't flip. (6) Then Steven flips. (5) Then Thomas flips. (4) Then Timothy doesn't flip. (3) Then Kyle doesn't flip. (2) Then Rachel doesn't flip. (1) Then Laura doesn't flip. Is the coin still heads up?</p>
<h1>Scratchpad targets:</h1>
<p>Solution Coin is initially heads up. (8) After Melissa flips, coin turns to tails. (7) Kevin doesn't flip, so coin stays tails. (6) After Steven flips, coin becomes heads. (5) After Thomas flips, coin turns to tails. (4) Timothy doesn't flip, so coin remains tails. (3) Kyle doesn't flip, so coin stays tails. (2) Rachel doesn't flip, so coin remains tails. (1) Laura doesn't flip, so coin stays tails. DONE</p>
<p>Figure 10: Natural Language parity problem instances: The coin flip task - which consists of tracking the state of a coin as it undergoes a number of flip or no-flip operations - shares the same underlying problem structure as the parity task.
of operations and number of variables (along with their names, which are single-letter characters) are uniformly sampled based on pre-set hyperparameters. (3) One by one, operations and the variables included in the operations are sampled, while making sure that each added operations retains the semantic correctness of the program.
We now outline the hyperparameters used to generate the chain-like and diverse splits.</p>
<h2>Chain-like split:</h2>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate</li>
<li>Minimum/maximum number of operations: 3, 19</li>
<li>Minimum/maximum number of variables in the program: 2, 3</li>
</ul>
<h2>Diverse split:</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Variable assignment problem instance: A problem instance from the Boolean Variable Assignment dataset. The scratchpad strategy consists of outputting the value of the recenly updated variable in form of comments. Note that both the input and the scratchpad are valid Python programs.</p>
<ul>
<li>Boolean operators: assign to and with another variable, assign to or with another variable, assign to xor with another variable, negate, assign to and with boolean, assign to or with boolean, assign to xor with a boolean, conditional assign to a boolean, assign to another variable, conditional assign to another variable</li>
<li>Minimum/maximum number of operations: 8,32</li>
<li>Minimum/maximum number of variables in the program: 4, 10</li>
</ul>
<p>The scratchpad format (seen in Figure 11) consists of copying over the input program with comments after each line specifying the value of the recently updated variable. This ensures that the scratchpad itself is a valid Python program and can be used with models pretrained with Python data.
Both splits have 1500000 samples.</p>
<h1>B Baseline for Vanilla Finetuning on Variable Assignment</h1>
<p>Just how weak are the vanilla finetuned models on the OOD lengths on the variable assignment task? We trained baseline models with the same parameter count on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. While this leaves in some of the spurious features that correlate with the right answer, it completely eliminates the possibility of running a sequential algorithm to get to the final answer.
The results can be found in Figure 12. On OOD data, the performance of the shuffled-ops baseline is on par with the models trained with the clean version of the dataset. Note that the length generalization deficiency that the shuffled ops baselines display is even more dramatic than that of the models'</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Results of finetuning on the shuffled-ops variable assignment dataset: We trained baseline models with the same parameter counts on a modified version of the variable assignment dataset where the order of the operations were randomly shuffled. The generalization gap between in and out-of-distribution data persists here as well, due to transformers' tendency to prefer parallel strategies that don't generalize to larger lengths.
trained with clean data. This is not surprising, as the primary source of lack of length generalization is the transformers' tendency to prefer parallel strategies that don't generalize to larger lengths over picking up sequential algorithm</p>
<h1>C Experimental Conditions</h1>
<p>shuff We outline the training conditions and hyperparameter used in the main experiments.
In our experiments on different datasets, we first did a learning rate sweep over different model sizes, and preferred the largest learning rates that ensured training stability. This is due to the fact that we observed the best length generalization when we used larger learning rates (see Figure 5). We used the AdaFactor optimizer in all of our finetuning experiments [11]. We didn't use dropout [33] in the parity experiments and used a dropout rate of 0.05 in the variable assignment experiments. Our initial experiments suggest that one can often reach similar in and out-of-distribution performance either by training the weights from scratch, or finetuning from the pretrained weights. To keep the experiments consistent, we always initialized training using the pretrained weights. In variable assignment, we did a learning rate sweep over $0.0033,0.00033$ and 0.000033 . For parity, we search over learning rate velus of $0.002,0.0002$ and 0.00002 . We used a constant learning rate profile all throughout learning. Due to memory constraints, we used a batch size of 32 when training the $64 b$ and $128 b$ models.</p>
<p>We used greedy decoding in all of our experiments (including few-shot scratchpad ones). We experimented with temperature sampling with reranking based on sentence likelihoods, but found that doing this doesn't lead to qualitatively different results.</p>
<h2>D Computational Graph Depth is the Relevant Notion of Difficulty on Variable Assignment</h2>
<p>Computational graph depth captures a more relevant notion of difficulty on the variable assignment task for transformers. In Figure 13, we show how the accuracy of a transformer model evolves on samples of problem instances with different computational graph depths (left), and how the same</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Evolution of Performance over Training Iterations on Different Computational Graph Depths: Computational graph depth corresponds to the length of the longest dependency chain linking to the queried variable in the variable assignment task. This quantity captures a more suitable notion of length/difficulty for transformer models. On the left plot, we show how the accuracy of a transformer model evolves for problem instances with different computational graph depths. In the middle, we show the same, except that we constrain the number of operations in the program to an out-of-distribution number. The accuracy values roughly remain unchanged, indicating that it's not the number of operations, but computational graph depth that determines the difficulty of a problems instance. On the right, we show the evolution of accuracy on instances with different number of operations for reference.
quantity behaves if we fix the number of operations in a program at an out-of-distribution length (middle) ${ }^{4}$. Two takeaways from this analysis are:</p>
<ul>
<li>The fact that the accuracy values on samples with different computational graph depths roughly remain unchanged on OOD program lengths indicates that it's not the number of operations, but computational graph depth that captures a more relevant notion of difficulty.</li>
<li>Transformers initially pick up how to handle programs with a small computational graph depth throughout training and then move to more difficult programs.</li>
</ul>
<h1>E Effect of Prompt Style on Few-Shot Finetuning Performance</h1>
<p>In Section 5.1, we hypothesized that few-shot finetuning only leads to significant improvements in length generalization performance if the non-finetuned performance on the same task already at a nontrivial level. To provide a sanity check for this, we ran few-shot finetuning using an alternative prompt style for the coin-flip task that yields poor non-finetuned performance. As can be seen in Figure 14, the few-shot finetuned performance shows significant length generalization pathologies. We leave a systematic study of how prompt style affects length generalization as future work.</p>
<h2>F Distractor Analysis for Scratchpad Strategies</h2>
<p>Our analysis in Section 4 indicates that length generalization pathologies persist even when we use the padded scratchpad strategy that makes sure that it's not untrained position encodings and/or the EOS token prediction that causes the aforementioned pathologies. This points to the fact that the transformer doesn't learn to attend to the "right" section of the input and scratchpad that implements the sequential strategy that generalizes to longer lengths - it's thrown off by distractor tokens in the input and/or the preceding scratchpad targets. The distractor tokens at which section of the transformer context window (input or scratchpad) contribute more to the performance deterioration? If we remove all the distractor tokens, can we achieve perfect length generalization?
To answer these questions, we trained four transformer models under the following conditions: (1) We used the padded scratchpad strategy described in Section 4 with no modification, (2) We manually masked the preceding scratchpad tokens that don't contribute to the correct sequential algorithm (i.e.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Effect of prompt style on few-shot finetuning performance: we evaluated the (few-shot) finetuned performance of the pretrained model on an alternative, synthetic prompt style that yields poor performance without any pretraining. We observed that the few-shot finetuned model on this task also shows significant length generalization pathologies. This is in line with our hypothesis that the non-finetuned performance of the base model should be non-trivial for few-shot finetuning to consistently yield strong length generalization results.
we masked the distractor tokens in the input), (3) We masked the input tokens that don't contribute to the correct sequential algorithm, (4) we masked the distractor tokens in both the input and the preceding distractor tokens. To give an example, let's say the input bitstring is "[1 1011 ]", and the model has emitted the scratchpad tokens "[100]" so far. Masking the distracting sratchpad tokens simply means replacing all but the last scratchpad token with dummy padding tokens: "[x x 0]". Similarly, removing the distracting input tokens corresponds to masking out the part of the input that the network doesn't need to attend to while predicting the next bit: "[x x x 1 x]". Masking both corresponds to removing the distractor tokens in both the input and the target, so that the input and the preceding scratchpad become "[x x x 1 x]" and "[x x 0]" respectively.</p>
<p>The results can be seen in Figure 15. For all four experimental conditions, we plotted the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. We conclude from this experiment that:</p>
<ul>
<li>Removing all distractor tokens does result in perfect length generalization.</li>
<li>The distracting input tokens are hurt length generalization performance more.</li>
</ul>
<p>Based on this analysis, we conclude that innovations in transformer architectures and/or training methodology/objective that alleviate the issues caused by distractor tokens have a chance at significantly improving length generalization.</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Distractor analysis: We trained scratchpad-augmented transformers to solve the parity task, where we systematically masked out the tokens in the input (left bottom) and the scratchpad (right top) that don't need to be attended to while implementing the correct sequential algorithm that solves parity. The plots illustrate the accuracy of the trained models on predicting the scratchpad tokens at different steps for inputs of varying (in and OOD) lengths. This analysis shows that (1) removing all distracting tokens leads to perfect length generalization (right bottom), (2) the distractor tokens in the input contribute more to the length generalization pathologies (right top versus left bottom).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ The longest in-distribution number of operations is 15 , and we fix this quantity at 20 in the middle plot.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
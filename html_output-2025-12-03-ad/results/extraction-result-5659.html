<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5659 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5659</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5659</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267770143</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.13823v3.pdf" target="_blank">Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are the cornerstone in automating Requirements Engineering (RE) tasks, underpinning recent advancements in the field. Their pre-trained comprehension of natural language is pivotal for effectively tailoring them to specific RE tasks. However, selecting an appropriate LLM from a myriad of existing architectures and fine-tuning it to address the intricacies of a given task poses a significant challenge for researchers and practitioners in the RE domain. Utilizing LLMs effectively for NLP problems in RE necessitates a dual understanding: firstly, of the inner workings of LLMs, and secondly, of a systematic approach to selecting and adapting LLMs for NLP4RE tasks. This chapter aims to furnish readers with essential knowledge about LLMs in its initial segment. Subsequently, it provides a comprehensive guideline tailored for students, researchers, and practitioners on harnessing LLMs to address their specific objectives. By offering insights into the workings of LLMs and furnishing a practical guide, this chapter contributes towards improving future research and applications leveraging LLMs for solving RE challenges.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5659.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5659.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Phrasing / Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Phrasing and Iterative Prompt Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper emphasizes that the exact wording and structure of a prompt (instruction, context, input, output indicator) strongly influence generative LLM outputs and must be iteratively refined (prompt engineering) for best quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (e.g., GPT-family, Llama 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various generative and classification RE tasks (e.g., traceability link recovery, extraction, summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>RE tasks where the model must generate or decide based on textual input; prompt expresses the task in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts composed of optional elements: Instruction (task), Context (supporting docs), Input Data (example/artifact), Output Indicator (format/spec). Paper gives rules: start simple, call to action, add context and explicit expectations (length, content). Example: 'Extract the names of places... Desired format: Place: <comma_separated_list_of_places> Input: "..."'.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared informally to unrefined/naive prompts (e.g., conversational 'Can you ...' vs. action-oriented 'Extract...'); also compared to reformulated prompts that specify concise answer format (e.g., force 'yes'/'no').</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors state that exact phrasing is crucial: prompts that start with action words, include context, and set explicit expectations produce higher-quality, more usable outputs; prompt engineering is analogous to feature engineering — requires experimentation.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5659.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output Format Specification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Explicit Output Format / Constrained Output Indicators in Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specifying the desired output format within a prompt (for example 'Desired format: Place: <comma_separated_list_of_places>' or 'Answer "yes" or "no"') yields outputs that are easier to parse and less verbose.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (e.g., GPT-3, GPT-4, Llama 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Named-entity extraction (place extraction), traceability link decision</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the model must return structured or short-form answers for automated downstream consumption.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts include an explicit 'Desired format' or an instruction like 'Answer "yes" or "no"'. Example: reformulate a trace link prompt to 'Is (1) related to (2)? Answer "yes" or "no".'</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to unconstrained, free-text responses (verbose explanations) and to more narrative prompt forms.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Constraining output format reduces verbosity and eases post-processing; explicit format signals guide the model to produce output that matches expected downstream parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5659.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-Shot Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-Shot Prompting (Providing Example Shots in Prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including a small number of example input–output pairs (shots) inside the prompt to illustrate the task improves model understanding and output quality; closeness of shots to the target problem improves benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (noted: GPT-family, Llama 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Traceability link recovery; general instruction-following/generation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where examples clarify the desired relation/transform and disambiguate domain-specific meanings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt contains a few example pairs (shots) demonstrating the task, followed by the query to be solved (few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to zero-shot prompts (no examples) and to prompts with more/less relevant examples.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Examples help the LLM identify task context and specific concepts (e.g., what 'traceability' means in a given dataset); more similar (closer) examples to the test case yield larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5659.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction Fine-Tuning / Instruction Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instruction Fine-Tuning and Instruction-Rich Prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing explicit instructions (keywords like 'classify', 'rank', 'summarize') and/or employing instruction-fine-tuned models helps LLMs follow task semantics and output formatting requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prompts matter: Insights and strategies for prompt engineering in automated software traceability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>instruction-following LLMs / decoder-only LLMs (e.g., instruction-tuned GPT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ranking related artifacts / trace link recovery</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring specific behaviors (e.g., rank artifacts from most to least related) where instruction clarity affects performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompts include explicit instruction blocks (e.g., '# Instructions: Rank all related artifacts from most to least related...') and may include expected output formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to prompts without explicit instructions or with looser task descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Instructions specify desired operation and output style, reducing ambiguity and improving the LLM's ability to produce task-aligned outputs; instruction fine-tuning of models further amplifies this effect.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5659.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation for Providing Context Beyond Token Limits</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RAG retrieves and concatenates the most relevant documents from a vector DB as context in the prompt, enabling LLMs with limited context windows to use external, up-to-date knowledge for generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (e.g., GPT-4, Llama 2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Context-heavy generation and QA tasks (e.g., traceability across large specs, RAG-enabled summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring grounding in large corpora or up-to-date facts that cannot be fully passed in the model's token window.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Pipeline: embed query, retrieve top-k relevant documents from vector DB, concatenate retrieved docs with original query as prompt context, feed to generative LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to naive prompts that attempt to include full large documents (which exceed context window) or to prompts with no external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because context window sizes are limited, RAG supplies the most relevant subset of large corpora as context, improving factuality and adaptability to changing data without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5659.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reformulation for Concision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Reformulation to Produce Concise / Machine-Friendly Answers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rewriting prompts to explicitly request terse answers (e.g., forcing 'yes'/'no') or to specify concise formats yields outputs more suitable for automated consumption and reduces unnecessary verbosity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Traceability link detection and similar binary/structured tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where succinct, machine-parseable answers are preferable to long explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Rewritten prompt forms that constrain response style and length, e.g., 'Is (1) related to (2)? Answer "yes" or "no".' or specify comma-delimited ID lists.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to original narrative prompts that elicit longer, explanatory answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Constrained prompts reduce generation variance and make outputs deterministic with respect to expected downstream parsers; this increases usability even if semantic correctness must still be validated.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5659.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context Size / Token Limits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context Window Limitations and Their Effect on Prompt Design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper notes that LLMs have finite context windows (e.g., GPT-4 ~8k tokens, some versions ~32k tokens, Llama 2 ~4k tokens), which constrains how much supporting information can be included in a prompt and motivates design choices like RAG or concise prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, Llama 2 (examples given)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Any task requiring large-context grounding (e.g., full requirements specs, multi-artifact traceability)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where full document(s) ideally should be provided as context but exceed token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Design choices: compress context, provide only top-k retrieved docs (RAG), or fine-tune model on domain corpora instead of passing large context at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (if too-large context attempted without RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>When necessary context cannot be fit into the model's token window, performance degrades; RAG is offered as a mitigation that retrieves the most relevant context fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5659.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5659.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Example Proximity Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closeness / Similarity of Example Shots to Target Problem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper states examples (shots) are more helpful the closer they are semantically to the problem at hand; domain- and style-similar shots yield larger improvements in output quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>decoder-only LLMs (few-shot-capable models, e.g., GPT-family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Traceability and other domain-specific relation/labeling tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where examples disambiguate domain-specific meaning or labeling conventions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompts where the provided examples are chosen to be similar to the target instances (close domain/style/lexicon).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to generic or distant examples and to zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (with closer examples)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Closer examples better convey domain-specific semantics and expected output patterns, allowing the model to generalize more accurately to the query.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prompts matter: Insights and strategies for prompt engineering in automated software traceability <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 1)</em></li>
                <li>Traceability transformed: Generating more accurate links with pre-trained BERT models <em>(Rating: 1)</em></li>
                <li>Retrieval Augmented Generation (RAG) approaches (survey papers) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5659",
    "paper_id": "paper-267770143",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "Prompt Phrasing / Refinement",
            "name_full": "Prompt Phrasing and Iterative Prompt Refinement",
            "brief_description": "The paper emphasizes that the exact wording and structure of a prompt (instruction, context, input, output indicator) strongly influence generative LLM outputs and must be iteratively refined (prompt engineering) for best quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (e.g., GPT-family, Llama 2)",
            "model_size": null,
            "task_name": "Various generative and classification RE tasks (e.g., traceability link recovery, extraction, summarization)",
            "task_description": "RE tasks where the model must generate or decide based on textual input; prompt expresses the task in natural language.",
            "problem_format": "Prompts composed of optional elements: Instruction (task), Context (supporting docs), Input Data (example/artifact), Output Indicator (format/spec). Paper gives rules: start simple, call to action, add context and explicit expectations (length, content). Example: 'Extract the names of places... Desired format: Place: &lt;comma_separated_list_of_places&gt; Input: \"...\"'.",
            "comparison_format": "Compared informally to unrefined/naive prompts (e.g., conversational 'Can you ...' vs. action-oriented 'Extract...'); also compared to reformulated prompts that specify concise answer format (e.g., force 'yes'/'no').",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Authors state that exact phrasing is crucial: prompts that start with action words, include context, and set explicit expectations produce higher-quality, more usable outputs; prompt engineering is analogous to feature engineering — requires experimentation.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.0",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Output Format Specification",
            "name_full": "Explicit Output Format / Constrained Output Indicators in Prompts",
            "brief_description": "Specifying the desired output format within a prompt (for example 'Desired format: Place: &lt;comma_separated_list_of_places&gt;' or 'Answer \"yes\" or \"no\"') yields outputs that are easier to parse and less verbose.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (e.g., GPT-3, GPT-4, Llama 2)",
            "model_size": null,
            "task_name": "Named-entity extraction (place extraction), traceability link decision",
            "task_description": "Tasks where the model must return structured or short-form answers for automated downstream consumption.",
            "problem_format": "Prompts include an explicit 'Desired format' or an instruction like 'Answer \"yes\" or \"no\"'. Example: reformulate a trace link prompt to 'Is (1) related to (2)? Answer \"yes\" or \"no\".'",
            "comparison_format": "Compared to unconstrained, free-text responses (verbose explanations) and to more narrative prompt forms.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Constraining output format reduces verbosity and eases post-processing; explicit format signals guide the model to produce output that matches expected downstream parsing.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.1",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Few-Shot Prompting",
            "name_full": "Few-Shot Prompting (Providing Example Shots in Prompt)",
            "brief_description": "Including a small number of example input–output pairs (shots) inside the prompt to illustrate the task improves model understanding and output quality; closeness of shots to the target problem improves benefit.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (noted: GPT-family, Llama 2)",
            "model_size": null,
            "task_name": "Traceability link recovery; general instruction-following/generation tasks",
            "task_description": "Tasks where examples clarify the desired relation/transform and disambiguate domain-specific meanings.",
            "problem_format": "Prompt contains a few example pairs (shots) demonstrating the task, followed by the query to be solved (few-shot).",
            "comparison_format": "Compared to zero-shot prompts (no examples) and to prompts with more/less relevant examples.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Examples help the LLM identify task context and specific concepts (e.g., what 'traceability' means in a given dataset); more similar (closer) examples to the test case yield larger gains.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.2",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Instruction Fine-Tuning / Instruction Prompts",
            "name_full": "Instruction Fine-Tuning and Instruction-Rich Prompts",
            "brief_description": "Providing explicit instructions (keywords like 'classify', 'rank', 'summarize') and/or employing instruction-fine-tuned models helps LLMs follow task semantics and output formatting requirements.",
            "citation_title": "Prompts matter: Insights and strategies for prompt engineering in automated software traceability",
            "mention_or_use": "mention",
            "model_name": "instruction-following LLMs / decoder-only LLMs (e.g., instruction-tuned GPT variants)",
            "model_size": null,
            "task_name": "Ranking related artifacts / trace link recovery",
            "task_description": "Tasks requiring specific behaviors (e.g., rank artifacts from most to least related) where instruction clarity affects performance.",
            "problem_format": "Prompts include explicit instruction blocks (e.g., '# Instructions: Rank all related artifacts from most to least related...') and may include expected output formatting.",
            "comparison_format": "Compared to prompts without explicit instructions or with looser task descriptions.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Instructions specify desired operation and output style, reducing ambiguity and improving the LLM's ability to produce task-aligned outputs; instruction fine-tuning of models further amplifies this effect.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.3",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation for Providing Context Beyond Token Limits",
            "brief_description": "RAG retrieves and concatenates the most relevant documents from a vector DB as context in the prompt, enabling LLMs with limited context windows to use external, up-to-date knowledge for generation tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (e.g., GPT-4, Llama 2)",
            "model_size": null,
            "task_name": "Context-heavy generation and QA tasks (e.g., traceability across large specs, RAG-enabled summarization)",
            "task_description": "Tasks requiring grounding in large corpora or up-to-date facts that cannot be fully passed in the model's token window.",
            "problem_format": "Pipeline: embed query, retrieve top-k relevant documents from vector DB, concatenate retrieved docs with original query as prompt context, feed to generative LLM.",
            "comparison_format": "Compared to naive prompts that attempt to include full large documents (which exceed context window) or to prompts with no external retrieval.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Because context window sizes are limited, RAG supplies the most relevant subset of large corpora as context, improving factuality and adaptability to changing data without retraining.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.4",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reformulation for Concision",
            "name_full": "Prompt Reformulation to Produce Concise / Machine-Friendly Answers",
            "brief_description": "Rewriting prompts to explicitly request terse answers (e.g., forcing 'yes'/'no') or to specify concise formats yields outputs more suitable for automated consumption and reduces unnecessary verbosity.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (GPT family)",
            "model_size": null,
            "task_name": "Traceability link detection and similar binary/structured tasks",
            "task_description": "Tasks where succinct, machine-parseable answers are preferable to long explanations.",
            "problem_format": "Rewritten prompt forms that constrain response style and length, e.g., 'Is (1) related to (2)? Answer \"yes\" or \"no\".' or specify comma-delimited ID lists.",
            "comparison_format": "Compared to original narrative prompts that elicit longer, explanatory answers.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Constrained prompts reduce generation variance and make outputs deterministic with respect to expected downstream parsers; this increases usability even if semantic correctness must still be validated.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.5",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Context Size / Token Limits",
            "name_full": "Context Window Limitations and Their Effect on Prompt Design",
            "brief_description": "The paper notes that LLMs have finite context windows (e.g., GPT-4 ~8k tokens, some versions ~32k tokens, Llama 2 ~4k tokens), which constrains how much supporting information can be included in a prompt and motivates design choices like RAG or concise prompts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4, Llama 2 (examples given)",
            "model_size": null,
            "task_name": "Any task requiring large-context grounding (e.g., full requirements specs, multi-artifact traceability)",
            "task_description": "Tasks where full document(s) ideally should be provided as context but exceed token limits.",
            "problem_format": "Design choices: compress context, provide only top-k retrieved docs (RAG), or fine-tune model on domain corpora instead of passing large context at inference time.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "reduced (if too-large context attempted without RAG)",
            "explanation_or_hypothesis": "When necessary context cannot be fit into the model's token window, performance degrades; RAG is offered as a mitigation that retrieves the most relevant context fragments.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.6",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Example Proximity Effect",
            "name_full": "Closeness / Similarity of Example Shots to Target Problem",
            "brief_description": "The paper states examples (shots) are more helpful the closer they are semantically to the problem at hand; domain- and style-similar shots yield larger improvements in output quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "decoder-only LLMs (few-shot-capable models, e.g., GPT-family)",
            "model_size": null,
            "task_name": "Traceability and other domain-specific relation/labeling tasks",
            "task_description": "Tasks where examples disambiguate domain-specific meaning or labeling conventions.",
            "problem_format": "Few-shot prompts where the provided examples are chosen to be similar to the target instances (close domain/style/lexicon).",
            "comparison_format": "Compared to generic or distant examples and to zero-shot prompting.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (with closer examples)",
            "explanation_or_hypothesis": "Closer examples better convey domain-specific semantics and expected output patterns, allowing the model to generalize more accurately to the query.",
            "counterexample_or_null_result": null,
            "uuid": "e5659.7",
            "source_info": {
                "paper_title": "Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prompts matter: Insights and strategies for prompt engineering in automated software traceability",
            "rating": 2,
            "sanitized_title": "prompts_matter_insights_and_strategies_for_prompt_engineering_in_automated_software_traceability"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Traceability transformed: Generating more accurate links with pre-trained BERT models",
            "rating": 1,
            "sanitized_title": "traceability_transformed_generating_more_accurate_links_with_pretrained_bert_models"
        },
        {
            "paper_title": "Retrieval Augmented Generation (RAG) approaches (survey papers)",
            "rating": 1,
            "sanitized_title": "retrieval_augmented_generation_rag_approaches_survey_papers"
        }
    ],
    "cost": 0.01134175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline
15 May 2024</p>
<p>Andreas Vogelsang vogelsang@cs.uni-koeln.de 
University of Cologne
CologneGermany</p>
<p>Jannik Fischbach jannik.fischbach@netlight.com 
Netlight Consulting GmbH
MunichGermany</p>
<p>fortiss GmbH
MunichGermany</p>
<p>Using Large Language Models for Natural Language Processing Tasks in Requirements Engineering: A Systematic Guideline
15 May 20244E444CF4C72CF89A7FF85CFF13E2C681arXiv:2402.13823v3[cs.SE]NLPRequirements EngineeringLarge Language ModelsGenerative AI
Large Language Models (LLMs) are the cornerstone in automating Requirements Engineering (RE) tasks, underpinning recent advancements in the field.Their pre-trained comprehension of natural language is pivotal for effectively tailoring them to specific RE tasks.However, selecting an appropriate LLM from a myriad of existing architectures and fine-tuning it to address the intricacies of a given task poses a significant challenge for researchers and practitioners in the RE domain.Utilizing LLMs effectively for NLP problems in RE necessitates a dual understanding: firstly, of the inner workings of LLMs, and secondly, of a systematic approach to selecting and adapting LLMs for NLP4RE tasks.This chapter aims to furnish readers with essential knowledge about LLMs in its initial segment.Subsequently, it provides a comprehensive guideline tailored for students, researchers, and practitioners on harnessing LLMs to address their specific objectives.By offering insights into the workings of LLMs and furnishing a practical guide, this chapter contributes towards improving future research and applications leveraging LLMs for solving RE challenges.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized how we can automate Requirements Engineering (RE) tasks and what quality we can achieve.Besides the improved quality over traditional classification algorithms, LLMs are mainly successful in RE because they do not need extensive datasets to be trained.RE researchers and practitioners have been limited for a long time by the scarcity of high-quality data for specific RE tasks.Early studies on deep learning for RE tasks have shown that simple tasks such as binary requirements classification could only be solved effectively with labeled datasets of sizes larger than 10,000 entries [32].With the rise of LLMs and especially the distribution of pre-trained models, RE researchers can adapt the models to certain domains and tasks with only a few additional and specific data [16].</p>
<p>LLMs belong to an active research field with many more active researchers than the field of Requirements Engineering.RE researchers and practitioners can profit from the recent advances in models and their accessibility.Examples of RE tasks that have been successfully (semi-)automated include requirements classification [16], requirements tracing [19], test case derivation [12], or requirements completion [21].On the other hand, RE researchers need to stay on top of the development of modern NLP techniques and be able to decide and assess which improvements may help them solve RE tasks.</p>
<p>Adopting LLMs in RE marks a significant milestone in the quest for automation and quality enhancement within the domain.However, amid the enthusiasm surrounding their efficacy, the selection of suitable LLM architectures and the nuanced process of fine-tuning them to meet the specific demands of RE tasks remains challenging.This challenge is multifaceted, rooted in the complexity of LLM architectures and compounded by the intricate nature of RE requirements.</p>
<p>LLMs encompass various architectures, each tailored to address different facets of natural language understanding and generation.From encoder-only models like BERT to generative models like GPT, the landscape of LLMs is vast and continually evolving.Consequently, RE researchers and practitioners are confronted with the daunting task of discerning which architecture aligns best with the intricacies of their respective tasks.</p>
<p>Moreover, fine-tuning LLMs to optimize performance for RE tasks requires a delicate balance between general language comprehension and domain-specific adaptation.Unlike generic NLP tasks, RE demands a nuanced understanding of domain-specific jargon, context, and semantic nuances.Thus, the fine-tuning process necessitates meticulous calibration to preserve the fidelity of the model's pre-trained knowledge while tailoring it to the idiosyncrasies of RE requirements.</p>
<p>This chapter aims to provide RE researchers and practitioners with a fundamental understanding of what LLMs are and how they work (see Section 2).Moreover, we show three fundamentally different ways of working with LLMs to solve RE tasks (see Section 3).We close the chapter with a summary and concluding remarks (see Section 4).</p>
<p>Fundamentals of LLMs</p>
<p>Sparse, Static, and Contextual Word Embeddings</p>
<p>This section describes the idea of word embeddings and the difference between static and contextual word embeddings that build the foundation of LLMs.</p>
<p>Input Representation.To process Natural Language (NL) using computational models, NL data needs to be transformed into a vectorized representation [2].Pursuing effective representations for NL data has engaged researchers for several decades [3,22,25].The concept of employing a vector space to encapsulate the semantic essence of words traces back to Osgood et al. [24].A mapping of a word to its corresponding vector is also called word embedding (i.e., a word is encoded in the vector space).The primary objective of word embeddings is to encapsulate a word's meaning based on its contextual usage.In particular, "words appearing in similar contexts tend to possess similar meanings" [17] and should therefore be mapped to similar locations in vector space.Consequently, embeddings strive to characterize words through their contextual companions.</p>
<p>Sparse Embeddings.</p>
<p>Based on the idea that the similar distribution of words is associated with similar meaning (distributional hypothesis [11]), various methods have been developed for generating word embeddings.For instance, the Term Frequency-Inverse Document Frequency approach relies on straightforward co-occurrence statistics to construct word embeddings by counting nearby words.Consequently, the vectors' dimensionality corresponds to the vocabulary size employed.This results in word embeddings that are excessively long and dominated by zeros, as many words rarely appear near others [17].This type of word embedding is often described as sparse.</p>
<p>Static Embeddings.Multiple techniques have been devised to create dense word embeddings to mitigate the problem of embedding size ballooning as the vocabulary expands.These techniques include Word2Vec [22], Global Vectors for Word Representation (GloVe) [25], and FastText [4].In contrast to sparse embeddings, dense vectors have fixed dimensions and consist of real-valued numbers, which can be either positive or negative.Word2Vec, for instance, employs a classifier to predict the likelihood of two words being neighbors and then utilizes the weights of the trained classifier as word embeddings.The resulting embeddings are static, meaning that the model generates a single unchanging vector for each word in the vocabulary.Nevertheless, static embeddings have a clear limitation: a word is invariably represented by the same vector, irrespective of the context in which it appears.This constraint restricts their capacity to capture the multifaceted meanings of words, as words naturally adopt different meanings based on their context.</p>
<p>Contexual Embeddings.</p>
<p>LLMs address this shortcoming by considering word context when generating embeddings.In essence, contemporary language models estimate the likelihood of a given word sequence occurring and employ the computed hidden states as word embeddings.Consequently, the contextualized embedding of a word is contingent on all other words within a sentence (see self-attention mechanism in Section 2.2).This enables the creation of distinct word embeddings for the same word.To generate contextualized word embeddings, it is imperative to transform the input sequence into a suitable format.To this end, the sequence undergoes segmentation into individual components, a process facilitated by tokenizers such as the WordPiece [29] tokenizer.The WordPiece tokenizer operates on a subword level, intending to encompass an extensive vocabulary using a finite set of established words.Specifically, Word-Piece aims to overcome the limitations of tokenization methods reliant on whole words, which often struggle to handle out-of-vocabulary (OOV) words.Word-Piece adopts a strategy where frequently used words remain unaltered as whole tokens, while less common words are broken down into subword units recognized by the model.</p>
<p>Consequently, even OOV words can be processed, as their constituent subwords often retain sufficient semantic information to enable the model to deduce the intended meaning of the OOV word.For instance, consider the application of the WordPiece tokenizer to the term associated with the foundational architecture of modern LLMs: "Transformer".In this case, the word "Transformer" is classified as infrequent and is thus segmented into "Transform" and "##er", where the "##" prefix indicates that this token is an extension of the preceding one.Despite being unfamiliar to the model, the semantics of "Transformer" are intelligible due to the presence of a commonly used word, the verb "transform".</p>
<p>Transformer Architecture</p>
<p>This section describes the inner workings of the transformer architecture on which modern LLMs are based.In its original configuration, a transformer architecture comprises two primary components: an encoder, which converts an input (e.g., German text) into a vectorized representation, and a decoder, responsible for transforming this vectorized representation into an output (e.g., English text).This structural division is rooted in the transformer's initial purpose of facilitating machine translation [31].To understand the inner workings of the transformer architecture, we must study the main components embedded in each encoder and decoder: self-attention mechanism, multi-head attention, positional encoding, and layer normalization.A great visualization of the interaction of all components can be found on Jay Alammer's blog 4 .</p>
<p>Positional encoding.A transformer adds positional encoding to the input embeddings to provide information about the position of each token in the sequence.This allows the model to understand the sequential order of the input data, which is crucial for tasks such as language understanding.</p>
<p>Self-attention mechanism.Self-attention enables the model to weigh the importance of different words in a sequence relative to each other.This mechanism allows the model to consider the entire context rather than processing words sequentially.Specifically, the self-attention mechanism computes attention scores for each word based on its relationships with other words, creating a weighted sum representing each word's context.</p>
<p>Multi-head attention.Multi-head attention runs the self-attention mechanism in parallel multiple times (heads) and concatenates the results.Each head attends to different aspects of the input, enabling the model to learn richer features.The concatenated outputs are linearly transformed to produce the final multi-head attention output.</p>
<p>Feedforward neural network.After the multi-head attention mechanism, a feedforward neural network processes the information learned through the attention mechanism.This network consists of fully connected layers and activa-tion functions, introducing non-linearity to the model and enabling it to capture complex relationships within the data.</p>
<p>Layer normalization.Layer normalization is applied after each sub-layer, such as the self-attention or feedforward layers, to stabilize the learning process.It normalizes the values within a layer, preventing the model from amplifying undesired features or gradients during training.This contributes to the stability and efficiency of the transformer architecture.</p>
<p>Encoder-only LLMs</p>
<p>BERT is an encoder-only model and focuses on the derivation of contextual embeddings.Specifically, it maps an input sequence x 1 . . .x n to a contextualized encoded sequence y 1 . . .y n :
f BERT : x 1 . . . x n → y 1 . . . y n
BERT consists of a stack of multiple encoders.It is available in two versions: the BERT-base model has 12 encoder layers stacked on each other, whereas the BERT-large model includes 24 encoder layers.The distinctive feature of BERT compared to other language models is its bidirectionality.Specifically, it considers a word's left and right context when creating a corresponding embedding [9].BERT is trained by optimizing two tasks: Masked language modeling (MLM), where a portion of the input tokens in a sentence are masked and the model needs to predict the masked tokens, and Next Sentence Prediction (NSP), where the model learns to predict whether two sentences follow each other.Previous transformer models like GPT-2 process an input sequence only in a left-to-right fashion.Hence, the hidden states are computed independently of the others as the model only considers tokens seen earlier in the context (i.e., the embeddings only contain information of the right context).This issue is especially problematic when utilizing embeddings to solve complex NLP problems (e.g., fine-grained sentiment analysis and question answering).Using the self-attention mechanism enables BERT to understand long-range dependencies between tokens and, thus, to overcome the vanishing gradient problem of sequential models.To this end, BERT applies the self-attention mechanism over the entire input sequence and contextualizes each token using information from the entire input.The final embeddings have a size of 768 dimensions (BERT-base) or 1024 (BERT-large).The dense layer within the encoder is used to further enrich the attention mechanism's output and pass it to the next encoder.</p>
<p>Decoder-only (Generative) LLMs and Prompting</p>
<p>Decoder-only LLMs have been designed to generate text.To support the generative capabilities of decoder-only LLMs, they are primarily pre-trained with a next-word prediction (NWP) objective, where the models predict the next word or words in a given sequence of words.After pre-training, decoder-only LLMs are triggered by a so-called prompt.A prompt is a textual input instructing the generative LLM to generate the desired response.Feeding decoder-only LLMs with prompts offers a new paradigm for interaction.In contrast to non-generative LLMs, it is not necessary to encode information about the task and the input in a smart way.Instead, the task and the input can be expressed in natural language and passed directly to the model.The model's output is richer than non-generative LLMs since it produces (customizable) text instead of confidence values for predefined outcomes.A prompt contains any of the following elements:</p>
<p>-Instruction: a specific task or instruction you want the model to perform -Context: external information or additional context that can steer the model to better responses -Input Data: the input or question that we are interested in finding a response for -Output Indicator: the type or format of the output.</p>
<p>You do not need all four elements for a prompt, and the format depends on the task at hand.Although prompting an LLM sounds relatively straightforward, the creation and exact phrasing of a prompt is crucial for the quality of the LLM output.For most tasks, it is necessary to experiment with different prompts and iteratively refine them to yield the best results.This so-called prompt engineering step is similar to feature engineering in more traditional ML approaches.Prompt engineering is a relatively new field where only heuristics and simple rules exist that help create effective prompts.Some of these rules are:</p>
<p>-Start simple: start with a simple prompt and build on it -Call to Action: start the prompt with an action word like "Write", "Create", or "Summarize" instead of "Can you" -Add Context: add specific and relevant context to the task you want to perform -Add Expectations: add clear and direct expectations for the content, like how long it should be and what to include</p>
<p>Here is an example of a prompt that follows these rules:
 User Prompt
Extract the names of places in the following text.Desired format: Place: <comma_separated_list_of_places> Input: "Although these developments are encouraging to researchers, much is still a mystery."We often have a black box between the brain and the effect we see in the periphery," says Henrique Veiga-Fernandes, a neuroimmunologist at the Champalimaud Centre for the Unknown in Lisbon."If we want to use it in the therapeutic context, we actually need to understand the mechanism.""</p>
<p> Assistant</p>
<p>Place: Champalimaud Centre for the Unknown, Lisbon</p>
<p>Overview of Popular Large Language Models</p>
<p>Table 1 contains an overview of the most popular LLMs.The table is structured along their architecture (encoder-only, decoder-only, or encoder-decoder).Please note that each row represents a whole family of models with different instances, which vary in the number of trainable parameters, the size and kind of training data used, support for languages, etc. Platforms like Hugginface5 offer searchable repositories to find and download appropriate models for different tasks.</p>
<p>How to Use LLMs in RE: A Systematic Guideline</p>
<p>Understand and Define the NLP Problem</p>
<p>Before trying to solve an RE task, a thorough analysis of the task itself is necessary to decide if and how LLMs may be used to solve the tasks.The upper part of Figure 1 shows a decision tree that helps decide if a task is ready to be implemented with an LLM.The most important question is related to the envisioned use of the approach either as a fully automated and autonomously acting system or as a supporting system to help humans do their work faster or better.The answer to this question impacts the necessity of high-quality data on the index hand and the necessary type of evaluation and corresponding metrics on the other hand.</p>
<p>If an approach shall be used to replace a human (i.e., solve a problem autonomously), researchers should make sure that their approach is trained and tested on a large enough, reliable, and diverse ground truth.Diversity is important for ensuring that the approach generalizes to unseen data and that out-of-distribution samples are minimized.A reliable ground truth implies objective and trustworthy labels associated with the data.The approach should be evaluated based on its performance compared to the performance of a human it replaces.If an approach is supposed to support a human in solving a problem (e.g., a recommender system), it is important to develop and evaluate the approach in its context with a human user.The approach should be evaluated based on the (increased) performance of a human using the approach compared to a human without the approach.This means that reliable human subjects are paramount for developing and evaluating such approaches.On the other hand, in such scenarios, a lack of highly reliable and diverse data for training is not that severe since a human can always contextualize and overrule the decisions.Of course, we would expect that better training data will increase the approach's performance and, thus, the value for a human.Once the task is characterized, Fig. 1.Guiding questions when using LLMs for RE tasks a researcher may decide how and which LLM to try.The lower part of Figure 1 shows a decision tree that helps determine what type of LLM may be suitable to solve a particular task.On a high level, RE tasks may be differentiated into understanding and generating tasks.In both cases, the input is text.</p>
<p>For generation tasks, the output is a text that is generated based on the input.Examples are:</p>
<p>translation (e.g., requirements to test cases [12]) summarization (e.g., of stakeholder interviews) synthesis (e.g., requirements generation [18]) completion (e.g., of incomplete requirements [21])</p>
<p>For understanding tasks, the output is based on an analysis of the input.The input may then be mapped to predefined output classes, clustered concerning each other, or certain input elements may be marked.Examples are:</p>
<p>classification (e.g., into types of requirements [16]) information retrieval (e.g., glossary term candidates [1]) linking (e.g., issues to commits [19]) clustering (e.g., topic identification [13]) sentiment analysis (e.g., in-app reviews [15])</p>
<p>The following three examples illustrate how the above decision trees can be applied.</p>
<p>Example 1. Jira ticket classification.An agile development team manages a large number of tickets in Jira.Users or other stakeholders can submit tickets.The development team needs to differentiate different types of tickets quickly.Currently, the product owner reviews every ticket and assigns one of the labels "bug report", "feature request", or "other".The team is looking for an LLM-based solution that automatically assigns the label to newly created Jira tickets, i.e., we follow the path (A) in the decision tree in the upper part of Figure 1.Fortunately, we have an existing dataset of more than 300,000 tickets from older projects that have been labeled manually.The development team considers these labels as reliable.Therefore, we can continue with step 2) in the decision tree in the lower part of Figure 1.Ticket classification is an understanding task where the LLM needs to understand the input and then map it to predefined classes.Since we have a rather large ground truth at hand, the decision tree suggests trying an encoder-only model (e.g., BERT) first.More information on how this can be done will be given in Section 3.3.</p>
<p>Example 2. GDPR compliance of contracts.</p>
<p>A legal department needs to check whether specific contracts are GDPR compliant.This is a manual and laborious task.The department is looking for automation that helps the team members make this decision, i.e., we follow the path (B) in the decision tree in the upper part of Figure 1.Fortunately, the legal department is highly interested in the solution and is willing to evaluate approaches.The legal department has a set of contracts, including their compliance verdict, but this set is not very large.Additionally, contracts are highly confidential and should not be used outside the organization.GDPR compliance checking is an understanding task where the LLM needs to understand the input and then decide on compliance.Since we only have a rather limited ground truth at hand, the decision tree suggests trying a decoder-only model (e.g., GPT) first.More information on how this can be done will be given in Section 3.4.</p>
<p>Example 3. Test case generation from use cases.</p>
<p>A testing team creates test case descriptions based on use case descriptions.This is a manual and laborious task where even experts sometimes forget to create test cases for specific flows of a use case.The team is looking for automation that supports the team members in creating test cases from use case descriptions, i.e., we follow the path (B) in the decision tree in the upper part of Figure 1.The testing team is highly interested in the solution and is willing to evaluate approaches.From former projects, the team has access to 500 use cases and 1,500 test cases manually crafted for the use cases.The development team considers this dataset to be reliable since the artifacts have undergone an extensive review process.Therefore, we can continue with step 2) in the decision tree in the lower part of Figure 1.Test case generation is a generation task where the LLM processes the input and then generates corresponding test cases.Since we have a rather large and reliable ground truth at hand, the decision tree suggests trying an encoder-decoder model (e.g., T5) first.More information on how this can be done will be given in Section 3.5.</p>
<p>LLM Architectures and their Usage</p>
<p>As described in Section 2, an LLM's architecture defines how input is processed and what type of output the LLM generates.These differences impact the way how an LLM must be used to solve a certain task.Figure 2) provides a schematic overview of how input and output change depending on which architecture is used to solve a task.Encoder-only LLMs like BERT (see Section 2.5) are primarily designed to provide high-quality language embeddings, which can be used as input for the intended task.Decoder-only LLMs like GPT provide another option to solve NLP4RE tasks.The generative capabilities can prompt these models with the original tasks and use the outputs as results.Encoder-decoder LLMs like T5 are specialized for text-to-text translation.In the following, we will explain these usage modes in detail.</p>
<p>Using Encoder-only LLMs</p>
<p>Figure 3 shows potential pipelines using the LLMs as language embedders.In this usage mode, a specific RE task is solved by leveraging the semantically rich embeddings generated by an LLM.In contrast to simpler embedding techniques like tf-idf or bag-of-words, LLM embeddings better capture the meaning of a word in its context (see Section 2.1).The embeddings are then used in specific task models to solve the actual RE task (e.g., classification, clustering, transformation).This step is called repurposing because it changes the purpose of the generic LLM to solve the desired task.</p>
<p>Repurposing (Task Model).</p>
<p>Adjusting an LLM to solve a specific task is also called repurposing the model.Repurposing is usually done by adding additional output layers to the pre-trained LLM.That means the task model is not separate but an addition to the pre-trained LLM.For example, you may add a fully connected dense layer with one output neuron per prediction class to solve a classification task.The model is then trained based on a set of labeled training data.In the training process, only the parameters of the additional output layers are adjusted.Therefore, repurposing can be done with relatively little data.</p>
<p>An example of this pipeline is the work by Hey et al. [16].In their work, they repurpose a pre-trained BERT model based on the PROMISE NFR dataset [8], which contains requirements and their associated classes (functional, usability, performance, security, operational).They repurpose the model by adding a single layer of linear neurons in a feedforward neural network.The outputs are directly computed from the sum of the weighted inputs (plus some bias).They use the softmax function to get a probability distribution for the different labels.The quality of the provided output depends on the embeddings for the given task.In the following, we describe techniques that can be used to further refine the generated embeddings for the given task.</p>
<p>Domain Adaptation.</p>
<p>The largest and most powerful LLMs are generalpurpose LLMs, i.e., they are trained on all kinds of publicly available data.While this ensures generalizability, a pre-trained LLM may not be able to capture the specifics of a certain domain.This is especially relevant if the tasks that shall be solved contain a lot of technical or even company-specific jargon.Unsupervised fine-tuning can adapt a pre-trained LLM to a specific domain.The general idea is to continue training the pre-trained LLM on a corpus of unlabeled but domainspecific data.For example, Chang et al. [7] describe an approach for automated requirements linking that starts from a BERT-based model pre-trained in one particular domain and is then adapted to another.Domain-specific data can be acquired in several ways.For example, Ezzini et al. presented WikiDoMiner, which builds a domain-specific corpus from Wikipedia based on keywords found in some input documents [10].Unsupervised fine-tuning can also be used to incorporate the knowledge contained in sensitive company documents.Supervised (Full) Fine-Tuning.Supervised Full Fine-Tuning (SFFT) combines repurposing and domain adaptation.In SFFT, a labeled dataset is used to repurpose the LLM by adding output layers; however, the parameters of the attention layers (i.e., the transformer model) can also be updated during fine-tuning.This operation can be computationally expensive and complicated, depending on the size of your model.In some cases, you can keep parts of the transformer model frozen to reduce fine-tuning costs.</p>
<p>Using Decoder-only LLMs with Prompts</p>
<p>Figure 4 shows potential pipelines when using the generative capabilities of LLMs.In this usage mode, a specific RE task is written as a prompt (see Sec-tion2.4), which is input to a generative LLM.The output of the model directly provides the answer to the specific task.Here is an example of how a trace link recovery task can be formulated as a prompt [27,14]:
 User Prompt
Below are artifacts from the same software system.Is there a traceability link between (1) and ( 2)?</p>
<p>(1) The DPU-TMALI shall utilize SCM_DCI_SR, along with ERRNO provided by DPU-DCI to decode errors and place them on an error queue for DPU-CCM.</p>
<p>(2) Error Collection and Reporting The ccmErrEnq() function tracks the last error reported and its frequency of occurrence.Once an error code has been reported it becomes the previously reported error code maintained by ccmErrEnq().A repetition count is then incremented for each subsequent, consecutively reported, identical instance of this previously reported error.If this error code is reported more than once in one high-rate housekeeping reporting period, then a special error, S_ccm_ERR_REPEAT is enqueued with the repetition count for the error encoded in the least significant byte.This mechanism effectively reduces the potential for housekeeping telemetry to become flooded with a single repeated error.</p>
<p> Assistant</p>
<p>Yes, there is a traceability link between (1) and ( 2). ( 1) mentions that the DPU-TMALI places errors on an error queue for the DPU-CCM.</p>
<p>(2) describes the error collection and reporting mechanism of the DPU-CCM, specifically referring to enqueuing errors.So (2) provides an implementation detail for the error queuing functionality briefly mentioned in (1).Therefore, there is a traceability link between these two artifacts in the software system.</p>
<p>The quality of the provided output depends on the specific prompt.Prompts can be refined by refining any component (i.e., instructions, context, input question).For example, we could change the prompt in the above example to:</p>
<p> User Prompt</p>
<p>Is (1) related to (2)? Answer "yes" or "no".</p>
<p>(1) . . .(2) . . .This input question makes the output less wordy and easier to process.Reformulating the prompt can help generate answers that fit an expected format.For increasing the semantic quality of the output, providing context and instructions is a more effective strategy.In the following, we describe three approaches that can be used to refine the prompt and increase the model's performance.Is (1) related to (2)? Answer "yes" or "no".</p>
<p>(1) . . .(2) . . .The examples, also called shots, help the LLM to identify the context of the task better and learn the meaning of specific concepts, such as traceability.The closer the examples are to the problem to be solved, the more the LLM can profit from them.</p>
<p>Instruction Fine-Tuning.Besides providing examples, prompts can also incorporate instructions, which help the LLM understand the task and how to solve it (see Section 2.4).Instructions may contain certain keywords such as classify, summarize, or translate, which define the type of task.The instruction may also contain information about the output format and style</p>
<p>Here is an example of a prompt for trace link recovery as suggested by Rodriguez et al. [27].</p>
<p> User Prompt</p>
<h1>Instructions: Rank all related artifacts from most to least related to the source.Provide the ranked artifacts as a comma-delimited list of artifact IDs where the first element relates to the source the most and the last element does so the least.</h1>
<h1>Source: [SOURCE ARTIFACT]</h1>
<h1>Artifacts: . . .</h1>
<p>Retrieval Augmented Generation (RAG).</p>
<p>Instructions and examples (shots) are a way to provide context to the LLM.Providing context is the most effective way to optimize the results when working with generative LLMs.However, the capabilities of LLMs to process context is limited in size.For example, GPT-4 and Llama 2 accept a maximum of 8,000 and 4,000 tokens as context.A special version of GPT-4 accepts up to ~32,000 tokens (~3,000 words) as context.Although LLMs will likely increase this maximum in the future, it may still be impossible to provide large sets of artifacts as context (e.g., an entire requirements specification).Retrieval Augmented Generation (RAG) is a technique that inserts a sample from a large context corpus that is most likely to fit a certain task.Figure 5 provides an overview of the approach.</p>
<p>RAG takes an input (a query) and retrieves a set of relevant/supporting documents by embedding the query into a vector space that contains embeddings of documents from a given source (e.g., Wikipedia or company-specific documents).The most relevant documents from the vector DB are concatenated as context with the original query to form a prompt fed to the decoder-only LLM, producing the final output.This makes RAG adaptive for situations where facts could evolve.One useful design pattern is to create a vector database that stores embeddings of company documents.When the user enters a prompt, the vector DB retrieves relevant documents and sends them as context to the model.RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via retrieval-based generation.</p>
<p>Using Encoder-Decoder LLMs</p>
<p>Encoder-decoder LLMs are designed for sequence-to-sequence tasks.They translate sequences of textual input to sequences of textual output.In contrast to decoder-only LLMs, the input sequence does not represent a prompt but a genuine input that needs to be translated to a genuine output of unknown lengths.Therefore, encoder-decoder LLMs are closer to encoder-only LLMs than to decoder-only LLMs when it comes to fine-tuning.As shown in Figure 6, when working with pre-trained encoder-decoder LLMs, fine-tuning is usually done by fully fine-tuning the pre-trained LLM with a training data set that contains input sequences and corresponding target sequences.Instead of working with pre-trained encoder-decoder LLMs such as T5, you can also build an encoderdecoder LLM by combining a pre-trained encoder-only LLM (e.g., BERT) with a pre-trained decoder-only LLM (e.g., GPT).</p>
<p>Summary and Conclusion</p>
<p>In this chapter, we have introduced the fundamentals of large language models (LLMs) and shown different ways to use pre-trained LLMs for solving RE tasks.For most RE tasks, pre-trained LLMs are the cornerstone for high-performing automation because they encode a high level of general language understanding, which is the basis for most NLP4RE tasks.In this way, pre-trained LLMs have revolutionized the possibilities for automation in RE where high-quality data is usually too scarce to train language models from scratch.</p>
<p>The main challenge when working with LLMs is the fine-tuning step, where the pre-trained LLM is adapted to solve a specific task it has not been trained on.We support this challenge with a decision tree (see Figure 1) that helps -The data in the application might change frequently.Fine-tuning the model frequently might not be possible or might be detrimental.For example, the data in news-related applications changes every day.-The application might be dynamic and context-sensitive.For example, if you create a chatbot that customizes its output for each user, you cannot fine-tune the model on user data.</p>
<p>The fine-tuning possibilities mentioned in this chapter are not complete.The fact that most LLMs are provided as more or less "open" models makes numerous fine-tuning methods possible.For example, it is also possible to fine-tune the weights of decoder-only models instead of just fine-tuning them by tuning the prompts.Similarly, encoder-decoder models may also undergo an unsupervised fine-tuning step with unlabeled domain documents.It is also important to understand that one RE task cannot necessarily only be solved with one LLM architecture.For example, decoder-only LLMs such as GPT have also shown remarkable results for text translation tasks, which seem more amendable to encoder-decoder models.Similarly, encoder-decoder models have also been used for text understanding tasks such as review classification.Therefore, working with LLMs is inherently experimental.Researchers may use the content of this chapter as a guideline but should always experiment with alternatives.It is also important to keep an eye on the quickly evolving landscape of LLMs.As shown in Table 1, new LLMs are constantly being published.Platforms for sharing and publishing pre-trained models accelerate this development.Huggingface, one of the most popular repositories for pre-trained models, currently 6 hosts over 40,000 pre-trained models just for text classification.Many of the provided models are specialized in certain tasks, certain languages, or certain domains.Whereas most authors in RE use the basic versions of a particular LLM (e.g., bert-base-uncased 7 from the BERT family or t5-base 8 from the T5 family), it may be interesting to consider more specific pre-trained alternatives available on pre-trained model repositories.</p>
<p>Fig. 3 .
3
Fig. 3. Schematic pipeline when using encoder-only LLMs</p>
<p>Fig. 4 .
4
Fig. 4. Schematic pipeline when using decoder-only LLMs</p>
<p>Few-Shot Prompting.In few-shot prompting, a prompt is enriched with some examples of the problem to be solved.The examples may stem from existing (training) data or can be artificial.In the example above, a few-shot prompting strategy may look like this: is related only to (b).(b) is related to (a) and (c).(c) is related to (b) and (d).</p>
<p>Fig. 5 .
5
Fig. 5. Retrieval Augmented Generation (RAG)</p>
<p>Table 1 .
1
Comparison of Popular Large Language Models
NameDescriptionYear Parameters TrainingObjectiveEncoder-onlyBERT [9]Abidirectionallypre-2018110M-340M MLM,trained LLM for naturalNSPlanguage understanding.RoBERTa [20]Robustly optimized BERT2019125M-355M MLMapproach.DistilBERT [28] A distilled version of201966M MLM +BERT with fewer pa-rametersbutsimilarperformance.USE [6]Universal Sentence En-201815M-30M Multi-coder;specializedfortask<em>embedding entire sen-tences or paragraphs.Decoder-onlyGPT-3 [5]Generative language model2020175B NWPdeveloped by OpenAI.LLama-2 [30]Open source generative202370B NWP,LLM developed by Meta.RLHFGPT-4 [23]The successor to GPT-32023N/A NWPwith more parameters andmore training dataEncoder-decoderT5 [26]An LLM specialized on202060M-11B MLMtext-to-text translationMLM: Masked language modelingNSP: Next sentence predictionMLM
+ : Masked language modeling + distillation loss + similarity loss Multi-task</em>: Skip-thought, input-response prediction, and NL inference NWP: Next word prediction RLHF: Reinforcement-learning from human feedback</p>
<p>https://jalammar.github.io/illustrated-transformer/
https://huggingface.co/
identify which type of LLM is most amendable for the problem to be solved.We show the different fine-tuning possibilities for each type of LLM.Fine-tuning, however, also bears specific challenges that should be considered and evaluated.These include:-Overfitting: Fine-tuning may lead to models that become too specific to the training data, leading to suboptimal generalization on unseen data.Regularization techniques such as dropout and weight decay can be applied during fine-tuning to prevent overfitting.Additionally, carefully curating the training data and utilizing techniques like cross-validation can help maintain a balance between model complexity and generalization.You can also consider stopping early, ensemble learning, regularly auditing performance, and monitoring model complexity.-Bias amplification: Explicit or implicit biases in training data get amplified during training.Since fine-tuning is a continuation of the training process, it amplifies potential biases even further.-Hyperparameter tuning: The process of fine-tuning involves various hyperparameters that govern the optimization process, such as learning rates, batch sizes, and regularization strengths.Hyperparameters heavily influence the performance of a fine-tuned LLM.Selecting inappropriate hyperparameters can lead to slow convergence, poor generalization, or even unstable training.In some cases, LLM fine-tuning is not possible or not useful:-Some models are only available through application programming interfaces (API) with no or limited fine-tuning services.-You might not have enough data to fine-tune the model for the downstream task or your application's domain.
Automated extraction and clustering of requirements glossary terms. C Arora, M Sabetzadeh, L Briand, F Zimmer, 10.1109/tse.2016.2635134IEEE Transactions on Software Engineering. 43102017</p>
<p>Representation learning: A review and new perspectives. Y Bengio, A Courville, P Vincent, 10.1109/TPAMI.2013.50IEEE Transactions on Pattern Analysis and Machine Intelligence. 3582013</p>
<p>Domain adaptation with structural correspondence learning. J Blitzer, R Mcdonald, F Pereira, Conference on Empirical Methods in Natural Language Processing. Sydney, AustraliaAssociation for Computational Linguistics2006</p>
<p>Enriching word vectors with subword information. P Bojanowski, E Grave, A Joulin, T Mikolov, CoRR abs/1607.046062016</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 332020</p>
<p>Universal sentence encoder. D Cer, Y Yang, S Y Kong, N Hua, N Limtiaco, R S John, N Constant, M Guajardo-Cespedes, S Yuan, C Tar, Y H Sung, B Strope, R Kurzweil, 10.48550/ARXIV.1803.111752018</p>
<p>Cross-domain requirements linking via adversarial-based domain adaptation. Z Chang, M Li, Q Wang, S Li, J Wang, 10.1109/icse48619.2023.00138IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>. Cleland-Huang , J Mazrouee, S Liguo, H Port, D , 10.5281/ZENODO.2685422007NFR</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, 10.48550/ARXIV.1810.048052018</p>
<p>WikiDoMiner: Wikipedia domainspecific miner. S Ezzini, S Abualhaija, M Sabetzadeh, 10.1145/3540250.355891630th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE). New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Studies in Linguistic Analysis 1930-1955. Selected Papers. J R Firth, J.R. Firth. F. Palmer1957. 1968LongmanA synopsis of linguistic theory</p>
<p>Automatic creation of acceptance tests by extracting conditionals from requirements: NLP approach and case study. J Fischbach, J Frattini, A Vogelsang, D Mendez, M Unterkalmsteiner, A Wehrle, P R Henao, P Yousefi, T Juricic, J Radduenz, C Wiecher, 10.1016/j.jss.2022.111549Journal of Systems and Software. 1972023</p>
<p>Topic modeling on user stories using word mover's distance. K J Gülle, N Ford, P Ebel, F Brokhausen, A Vogelsang, 10.1109/aire51212.2020.00015IEEE Seventh International Workshop on Artificial Intelligence for Requirements Engineering (AIRE). 2020IEEE</p>
<p>Handbook of Natural Language Processing for Requirements Engineering. J L C Guo, J P Steghöfer, A Vogelsang, J Cleland-Huang, Ferrari, A., Deshpande, G.2024Springer International PublishingChamNatural language processing for requirements traceability</p>
<p>How do users like this feature? a fine grained sentiment analysis of app reviews. E Guzman, W Maalej, 10.1109/re.2014.6912257IEEE 22nd International Requirements Engineering Conference (RE). IEEE2014</p>
<p>Norbert: Transfer learning for requirements classification. T Hey, J Keim, A Koziolek, W F Tichy, 10.1109/re48521.2020.00028IEEE 28th International Requirements Engineering Conference (RE). IEEE2020</p>
<p>Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition. D Jurafsky, J H Martin, 2009Pearson Prentice HallUpper Saddle River, N.J.</p>
<p>On-demand security requirements synthesis with relational generative adversarial networks. V Koscinski, S Hashemi, M Mirakhorli, 10.1109/icse48619.2023.00139IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Traceability transformed: Generating more accurate links with pre-trained BERT models. J Lin, Y Liu, Q Zeng, M Jiang, J Cleland-Huang, 10.1109/icse43902.2021.00040IEEE/ACM 43rd International Conference on Software Engineering (ICSE). IEEE2021</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, 10.48550/ARXIV.1907.116922019</p>
<p>Improving requirements completeness: Automated assistance through large language models. D Luitel, S Hassani, M Sabetzadeh, Requirements Engineering Journal. 2023REJ</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in Neural Information Processing Systems. C J C Burges, L Bottou, M Welling, Z Ghahramani, K Q Weinberger, Curran Associates, Inc201326</p>
<p>. J Openai: Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, 10.48550/ARXIV.2303.08774GPT-4 technical report. 2023</p>
<p>The Measurement of Meaning. C E Osgood, G J Suci, P H Tannenbaum, 1957University of Illinois Press</p>
<p>GloVe: Global vectors for word representation. J Pennington, R Socher, C Manning, 10.3115/v1/D14-1162Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, QatarAssociation for Computational Linguistics2014</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, 10.48550/ARXIV.1910.106832019</p>
<p>Prompts matter: Insights and strategies for prompt engineering in automated software traceability. A D Rodriguez, K R Dearstyne, J Cleland-Huang, 10.1109/REW57809.2023.00087IEEE 31st International Requirements Engineering Conference Workshops (REW). 2023</p>
<p>DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. V Sanh, L Debut, J Chaumond, T Wolf, 10.48550/ARXIV.1910.011082019</p>
<p>Japanese and korean voice search. M Schuster, K Nakajima, International Conference on Acoustics, Speech and Signal Processing. 2012</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, 10.48550/ARXIV.2307.092882023Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Automatic classification of requirements based on convolutional neural networks. J Winkler, A Vogelsang, 10.1109/rew.2016.021IEEE 24th International Requirements Engineering Conference Workshops (REW). IEEE2016</p>            </div>
        </div>

    </div>
</body>
</html>
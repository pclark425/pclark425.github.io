<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7314 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7314</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7314</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-276258966</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.07601v2.pdf" target="_blank">Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm. Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios. Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks. However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks. To facilitate research in AD & reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&R. Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images. To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning. Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens. Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning. Extensions to medical and 3D AD are provided for future study. The link to our project page: https://xujiacong.github.io/Anomaly-OV/</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7314",
    "paper_id": "paper-276258966",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0056307499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models
17 Mar 2025</p>
<p>Jiacong Xu 
Johns Hopkins University 2 Honda Research Institute
USA</p>
<p>Shao-Yuan Lo shao-yuanlo@honda-ri.com 
Johns Hopkins University 2 Honda Research Institute
USA</p>
<p>Bardia Safaei bsafaei1@jhu.edu 
Johns Hopkins University 2 Honda Research Institute
USA</p>
<p>Vishal M Patel vpatel36@jhu.edu 
Johns Hopkins University 2 Honda Research Institute
USA</p>
<p>Isht Dwivedi idwivedi@honda-ri.com 
Johns Hopkins University 2 Honda Research Institute
USA</p>
<p>Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models
17 Mar 2025E52C8CB2E4984CD4F9B33BAA9EC57777arXiv:2502.07601v2[cs.CV]
Zero-Shot Anomaly Detection (ZSAD) is an emerging AD paradigm.Unlike the traditional unsupervised AD setting that requires a large number of normal samples to train a model, ZSAD is more practical for handling data-restricted real-world scenarios.Recently, Multimodal Large Language Models (MLLMs) have shown revolutionary reasoning capabilities in various vision tasks.However, the reasoning of image abnormalities remains underexplored due to the lack of corresponding datasets and benchmarks.To facilitate research in AD &amp; reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R.Through investigation with our benchmark, we reveal that current MLLMs like GPT-4o cannot accurately detect and describe fine-grained anomalous details in images.To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning.Inspired by human behavior in visual inspection, Anomaly-OV leverages a Look-Twice Feature Matching (LTFM) mechanism to adaptively select and emphasize abnormal visual tokens.Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning.Extensions to medical and 3D AD are provided for future study.The link to our project page: https://xujiacong.github.io/Anomaly-OV/</p>
<p>Introduction</p>
<p>Visual Anomaly Detection (AD) is a well-established task in computer vision, extensively applied in scenarios such as industrial defect inspection [2,3,5,12,35,69,76,77,92,98] and medical image diagnosis [1,24,31,36,89,90,103,106].In the traditional unsupervised AD (a.k.a.one-class AD) setting, models learn the distribution of normal visual features from normal samples and are required to identify anomaly samples during inference.While recent advance-* Most of this work was done when J. Xu was an intern at HRI-USA.[38], AnoVL [19], AnomalyCLIP [110], AdaCLIP [6]).Notably, our zero-shot performance on VisA even surpasses most recent advances in the few-shot setting [28,51,112].</p>
<p>ments [9, 25, 32-34, 37, 42, 82, 84, 95, 97, 104] have significantly improved the detection performance, these approaches assume the availability of a substantial number of normal samples.However, this assumption becomes impractical in certain scenarios due to strict data privacy policies and the significant human effort required for data classification, sometimes involving experts or specialists.Therefore, Zero-Shot Anomaly Detection (ZSAD) is emerging as a popular research direction, leading to the development of many innovative methods [6,17,27,38,43,52,78,79,110,113].</p>
<p>Recent advances in Multimodal Large Language Models (MLLMs) [7,15,44,45,47,48,57,58,111] have shown revolutionary reasoning capabilities in various vision tasks [14,29,67,70,80,91,94,107,109].However, the rea-soning of image abnormalities has not been explored due to the challenges of collecting large-scale datasets and establishing benchmarks.Existing methods simply predict the likelihood of an anomaly without providing rationales [6,11,19,38,110].In contrast, for better interpretability, robustness, and trustworthiness, people would expect models to explain why an image is considered anomalous and provide visual evidence.Interestingly, we find that recent advanced MLLMs, such as GPT-4o [72], fall short in AD &amp; reasoning.As shown in Figure 2, while the detection is correct, the explanation from GPT-4o lacks accuracy, indicating a gap in a comprehensive understanding of the anomaly.</p>
<p>To expedite research in AD &amp; reasoning, we establish the first visual instruction tuning dataset, Anomaly-Instruct-125k, and the evaluation benchmark, VisA-D&amp;R, through intensive human efforts.After evaluating current generalist MLLMs, we observe that these models fail to accurately detect and describe fine-grained anomalous details in images.</p>
<p>To address this, we propose Anomaly-OneVision (Anomaly-OV), the first specialist visual assistant for ZSAD and reasoning.Unlike existing ZSAD methods [6,11,19,38,110], Anomaly-OV directly learns object-awareness abnormality embeddings in feature space using only the visual encoder.Inspired by human behavior in visual inspection, Anomaly-OV employs a Look-Twice Feature Matching (LTFM) mechanism to assist its LLM in adaptively selecting and emphasizing the most suspicious abnormal visual tokens.</p>
<p>Extensive experiments demonstrate that Anomaly-OV achieves significant improvements over advanced generalist models in both detection and reasoning.Extended results of Anomaly-OV, from applications in industrial defect detection to 3D inspection and medical image diagnosis, are provided for future study.With precise descriptions and rationales of visual anomalies, our model can infer potential causes (see Figure 2), assess current impacts, and offer improvement suggestions, positioning itself as a reliable assistant for visual inspection.Our contributions are in two folds: ‚Ä¢ We establish the first visual instruction tuning dataset and benchmark for anomaly detection and reasoning.‚Ä¢ We propose the first specialist visual assistant with stateof-the-art performance for this new impactful domain.</p>
<p>Related Work</p>
<p>Multimodal Large Language Models.Vision-Language Models (VLMs), such as CLIP [73], exhibit robust zero-shot classification capabilities and have been applied to a range of downstream vision tasks [10,53,56,61,86].Combining a VLM's vision encoder and an LLM [20,62,74], MLLMs [15,48,49,57,111]  The candle in the top left appears to have a damaged or missing wick, which is unusual compared to the other candles.</p>
<p>Yes, there is an anomaly in the top left candle.It has a small hole or crack near the wick, which is not present in the other candles.</p>
<p>What is the cause for the anomaly?</p>
<p>The anomaly could be due to manufacturing defects, such as improper molding or cooling, or it might result from external damage like a puncture.downstream tasks, e.g., video anomaly detection [67,94], affective computing [14,29], and visual commonsense reasoning [109], revealing more interpretability.</p>
<p>Unsupervised Anomaly Detection.Due to the scarcity and difficulty of collecting anomalous data, researchers often focus on the unsupervised AD setting, which exclusively uses normal data to train an AD model.Earlier studies, such as reconstruction-based [63,69,99], student-teacher [18,85,102], and augmentation-based [46] approaches, assume a large amount of normal data is available.These traditional approaches are less practical when data are restricted or expensive, such as in the medical domain.</p>
<p>Zero-Shot Anomaly Detection.Unlike unsupervised AD [35,77] and few-shot AD [22,28,36,51,112], ZSAD models directly access the likelihood of abnormality for a given image without requiring data specific to the target object.</p>
<p>Existing works [6,19,38,110] accomplish ZSAD by comparing visual and textual features encoded by visual and text encoders of CLIP and constructing their positive (anomaly) and negative (normal) prompts in the format of:
P + = [V 1 ][V 2 ]...[V n ][object] P ‚àí = [W 1 ][W 2 ]...[W n ][object]
where V i and W i are handcrafted or learnable tokens, and object refers to the word object or the class name of the object.However, simply utilizing object to represent all kinds of objects cannot capture the class-awareness abnormality types.Also, for an intelligent visual assistant, the images should be totally blind to the user (object-agnostic).</p>
<p>Split</p>
<p>Visual Encoder</p>
<p>VT Selector</p>
<p>"with suspicious features: " "Are there any anomalies in the image?"</p>
<p>Look-back path 0.01 0.99
ùìã ! " ùìã # " ùìã $ " ùìã ! % ùìã # % ùìã $ % ùëü ùìã ! " = ‚Ñ± # " (ùêº ! )
..</p>
<p>.. c</p>
<p>Freeze in all stages Element-wise multiplication
ùìã ! % = ‚Ñ± # % (ùêº ! ) ùìã ! &amp; = ‚Ñ± # &amp; (ùêº ! ) ùìã ! ' = ‚Ñ± # ' (ùêº ! )</p>
<p>Adaptor</p>
<p>Method</p>
<p>Preliminary</p>
<p>Training an MLLM from scratch demands extensive data and computational resources to align the visual and textual embedding spaces and develop robust instruction-following capabilities.Recent studies [23,83,93] reveal that pretrained MLLMs function as generalists, possessing a broad knowledge base but underperforming in specialized domains.Therefore, our goal is to introduce an auxiliary specialist or expert model designed to guide the generalist in selecting and utilizing critical visual tokens.This approach circumvents the need for large-scale pre-training while preserving the generalization capacity of the original model.</p>
<p>We choose LLaVA-OneVision [44] as our base MLLM because it is open-sourced and performs similarly to other commercial models.LLaVA-OneVision follows the model architectures for LLaVA family [47,[57][58][59] and other generic MLLMs, which typically consist of three major components: Visual Encoder, Projector, and LLM.The visual encoder [73,100] extracts the visual information from the raw images, the projector aligns the spaces of visual features with the word embedding, and the LLM is responsible for textual instruction processing and complex reasoning.Since the image resolution for CLIP pre-training is fixed, LLaVA-OneVision leverages AnyRes with pooling strategy to scale up the input raw image resolution.Specifically, the highresolution images are divided into a prototyped number of crops, and the visual encoder independently processes the image crops before final spatial pooling.</p>
<p>Architecture Overview</p>
<p>With the same image-splitting strategy AnyRes as LLaVA-OneVision, the input high-resolution image is split into several crops, and the new image set can be written as:
I = {I 0 , I 1 , I 2 , ..., I n‚àí1 }(1)
where I 0 is the resized original image and I jÃ∏ =0 refers to the image crops.As shown in Figure 3, the image set I will be processed by the visual encoder F Œ∏ to generate the final visual features {v o j }.Similar to AnomalyCLIP [110], we store the outputs for four selected layers in the ViT [21] to capture the image representations from different levels and apply four adapters to compress the feature dimension.Then, the extracted visual features can be written as:
v i j = F i Œ∏ (I j )(2)
where i denotes the i-th level and j refers to the index of corresponding image in I.These multi-level features have been demonstrated to be effective in capturing fine-grained local semantics by recent works [6,28,110].The large-scale pre-trained CLIP models align the projection spaces of the textual and visual encoder.Therefore, the encoded image features already contain the class information required by ZSAD.To avoid human involvement in object classification and reduce the model complexity, we remove the heavy text encoder commonly utilized in existing works and let the visual model itself parse the information for suspicious classes or objects.Specifically, the output visual features for the original image v o 0 are leveraged to provide the global description of the target object or regions in the look-back path.With the multi-level features and the global embeddings, the LTFM module is responsible for the recognition and localization of suspicious tokens.</p>
<p>Drawing inspiration from human visual inspection, where suspicious objects or regions are identified and then inspected closely (see Figure 4), we design the VT selector module for aggregating (zooming in) the crucial visual tokens and explicitly assisting the LLM in distinguishing these tokens from many irrelevant ones when dealing with instructions regarding anomaly detection and reasoning.Additionally, the original visual features are preserved to maintain the generalization capability of the base model on regular instructions, such as Can you describe the content of the image?</p>
<p>Look-Twice Feature Matching</p>
<p>Given the global object information v o 0 provided by the lookback path, we generate the class-awareness abnormality description by merging v o 0 with two learnable embeddings: e + ‚àà R D and e ‚àí ‚àà R D , where + and ‚àí indicate positive (anomalous) and negative (normal) patterns and D is the embedding dimension.Specifically, a linear layer T o i is applied along the token dimension to select and fuse useful tokens from v o 0 , and then the fused vector will be concatenated with e + and e ‚àí independently and pass through two MLPs {G + i , G ‚àí i } to generate the abnormality and normality descriptions {d + i , d ‚àí i }, which can be represented by:
{d + i , d ‚àí i } = G + i (e + ‚Ä¢ T o i (v o 0 )) G ‚àí i (e ‚àí ‚Ä¢ T o i (v o 0 ))(3)
The visual features extracted from different levels of the ViT focus on different scales of semantics.Thus, the parameters of T o i and {G + i , G ‚àí i } should be independent for different levels, where i indicate the level number.</p>
<p>Similar to the zero-shot classification mechanism of CLIP models, we calculate the possibilities of each patch token in v i j belonging to the anomalous patterns by combining cosine similarity and softmax operations:
m i j = exp(&lt; d + i , v i j &gt; /œÑ ) exp(&lt; d + i , v i j &gt; /œÑ ) + exp(&lt; d ‚àí i , v i j &gt; /œÑ )(4)
where m i j represents the significance map for visual tokens, œÑ is the temperature hyperparameter, and &lt;, &gt; refers to the cosine similarity operator.The patch weight in m i j indicates the closeness of the corresponding visual token to the anomalous pattern.Then, all the maps are averaged to capture the token significances from low to high levels:
m j = 4 i=1 m i j /4(5)
The visual features are leveraged twice in the forward and look-back paths, so this module is named by Look-Twice Feature Matching (LTFM), following the nature of two-step human visual inspection shown in Figure 4.</p>
<p>Visual Token Selector</p>
<p>Under the image cropping strategy widely applied in recent MLLMs, there will be a large number of visual tokens for a high-resolution image, e.g., 7290 tokens for an image with 1152√ó1152 resolution in LLaVA-OneVision.While these tokens provide rich visual details, the LLM is required to pick the most useful information when adapting to a specific task.When the LLM lacks enough knowledge in this domain, the token-picking process will become complicated.Thus, our solution is to introduce a specialist or expert who knows which token is crucial or not and assist the LLM in selecting and emphasizing (zooming in) the crucial tokens.</p>
<p>Given the encoded visual tokens {v o j } for each image crop in I and the corresponding significance map m j , the suspicious tokens are emphasized by direct multiplication of the two tensors.Then, the normal tokens will be scaled to zero while the anomalous tokens will be maintained.After that, spatial average pooling P is applied to reduce the number of tokens.This process can be written as:
q j = P(v o j ‚äô m j )(6)
where q j ‚àà R h√ów√óD refers to the pooled query tokens.Empirically, setting h = w = 2 provides a better trade-off than other options.Then, a Q-Former Q [49] is leveraged to aggregate the correlated tokens in the original output by forwarding q j as the query and v o j as the key and value:
v s j = Q(q j , v o j , v o j )(7)
The Visual Token Selector (VT Selector) serves as a tool for the anomaly expert to hand-pick visual tokens that contain the most suspicious semantics for a given image.</p>
<p>Q: Do you notice any anomalies in the image?</p>
<p>A: Yes, the anomaly in the image is a fine, dark crack visible on the ceramic cup, starting from the rim and extending downward.</p>
<p>Q: How can I prevent similar anomalies in the future?</p>
<p>A: Avoid rapid temperature changes, handle the cup carefully to minimize impacts, and consider using cups made with thicker, more durable materials.</p>
<p>Q: What kind of object are you examining in the images?A: I'm examining a round cookie with a golden-brown surface and visible chocolate chips embedded across its surface.</p>
<p>Q: Are there any significant anomalies detected on this cookie?A: Yes, there are two significant anomalies on the cookie: a central defect featuring a noticeable hole and an edge defect showing a gap or crack.</p>
<p>Q: What does the image show? A:</p>
<p>The image shows a densely cellular tissue sample with features indicating potential metastatic breast cancer.</p>
<p>Q: What suggests that there might be malignant activity in the tissue sample?A: Malignant activity is suggested by irregularly shaped cells, hyperchromatic nuclei, and an increased nuclear-to-cytoplasmic ratio, indicative of abnormal and increased DNA content typical in cancerous cells.</p>
<p>Q: Are there any anomalies in this cable image?A: Yes, the green wire is missing its internal copper conductors, unlike the blue and brown wires, which contain complete sets of bundled copper strands.</p>
<p>Q: What could be the reason behind this anomaly?</p>
<p>A: The absence of copper in the green wire could be due to manufacturing defects such as failure to insert the strands or a mechanical issue where the strands were removed or misplaced during production or handling.</p>
<p>Inference and Loss</p>
<p>Anomaly Prediction.In the traditional anomaly detection task, the model predicts the possibility of the image being abnormal.To achieve anomaly score prediction, we aggregate the anomaly information from all the image crops by an average operation weighted on the significance maps:
r(I) = j,k v s j [k] ‚Ä¢ P(m j )[k] j,k P(m j )<a href="8">k</a>
where P is the same spatial pooling in VT Selector and r(I) is a vector containing the global anomaly information for the entire image.Then, the anomaly expert can calculate the image-level abnormal possibility by parsing r(I):
score(I) = Sigmoid(G o (r(I)))(9)
where G o is an MLP for distinguishing normal and abnormal semantics.To handle the unbalanced sample distribution, we employ the balanced BCE loss as the professional training objective for the anomaly expert components.</p>
<p>Text Generation.Instead of directly forwarding the concatenation of the original {v o j } and the selected {r(I), v s j } visual tokens into the LLM, we apply an indication prompt with <adv> suspicious feature: in the middle of the two series of tokens, which will highlight the selected tokens for LLM when handling anomaly-related instructions.This approach can be considered a form of prompt engineering in MLLMs.Besides, the <adv> is chosen from {highly, moderately, slightly} and is determined by score(I) and predefined thresholds {s low , s high }.When the input image I has a high likelihood of anomaly, the LLM will place greater emphasis on the selected tokens; otherwise, these tokens will have less significance.The text generation is implemented by the original auto-regressive token prediction mechanism of LLM:
p(X a |I, X q ) = L t=1 p Œ∏ (x t |I, X q,&lt;t , X a,&lt;t )(10)
where X a,&lt;t and X q,&lt;t are the answer and instruction tokens from all prior turns before the current prediction token x t for a sequence of length L. The entire model is parameterized by Œ∏ and trained by the original language model cross-entropy loss for each predicted answer token x t .</p>
<p>Dataset and Benchmark</p>
<p>The lack of multimodal instruction-following data for image anomaly detection and reasoning hinders the development of special intelligent assistants in this domain.Even though AnomalyGPT [28] introduces a prompt tuning dataset by simulating the anomalies, the scale of their dataset and the diversity of their instructions and answers are limited, only focusing on anomaly localization.To resolve the data scarcity issue, we establish the first large-scale instruction tuning dataset: Anomaly-Instruct-125k and the corresponding anomaly detection and reasoning benchmark: VisA-D&amp;R.</p>
<p>Anomaly-Instruct-125k</p>
<p>LLaVA [57] builds its instruction tuning dataset by leveraging the image caption and bounding boxes available in the COCO dataset [55] to prompt the text-only GPT-4.</p>
<p>ShareGPT4V [8] provides a higher-quality dataset by directly prompting GPT-4V [71].However, there is no image caption provided in existing anomaly detection datasets [1,2], and no matter GPT-4V [71] or most recent GPT-4o [72] cannot accurately locate and describe the anomalies in the image without explicit human involvement.</p>
<p>To resolve these issues, we design a new prompt pipeline for accurate anomaly description generation.Since most of the datasets contain annotations for anomaly types, we manually combine the class name and anomaly type, such as a [capsule] with [poke] on surface.If the anomaly masks are provided, we draw bounding boxes on the images to highlight the anomalous area.The short description and the image with (or w/o) bounding boxes are used to prompt GPT-4o to generate the detailed image and anomaly descriptions.Then, we employ an in-context learning strategy similar to LLaVA to create the instructions.For a unified visual inspection dataset, precise instruction data is collected from MVTec AD [2], the training set of BMAD [1], Anomaly-ShapeNet [50], Real3D-AD [60], and MVTec-3D AD [4], covering both 2D to 3D data across industry to medical domains.The 3D point cloud data are converted into 9 multi-view images, and the corresponding masks are rendered using predefined camera positions.However, the diversities and scales of these datasets are relatively limited, probably due to the difficulty of collecting anomaly images.To scale up the instruction data, we introduce an automatic anomaly data collection pipeline combining GPT-4o [72] and Google Image Search [26] for image collection, data cleaning, and instruction generation.Finally, 72k in-thewild images (named as WebAD) targeting anomaly detection are collected, significantly enriching our instruction dataset.Several samples from Anomaly-Instruct-125k are shown in Figure 5.The instructions are mainly in the format of multi-round conversations, covering anomaly detection and description in low-level reasoning and potential cause and future suggestions for complex understanding.</p>
<p>VisA-D&amp;R</p>
<p>VisA [115] is a classic but challenging industrial anomaly detection dataset, providing fine-grained anomaly type and segmentation for each image.For evaluation of the anomaly detection and reasoning performance on existing and future methods, we select 10 classes from VisA and follow a similar data generation pipeline of Anomaly-Instruct-125k to create the benchmark.Differently, significant human effort has been invested in meticulously reviewing all generated images and anomaly descriptions.Wrong descriptions are picked out and re-annotated by humans before utilizing them for Q&amp;A generation.Totally, the benchmark consists of 761 normal samples and 1000 anomalous ones.</p>
<p>For evaluating detection performance, questions designed to elicit a one-word answer are used to prompt the MLLMs (Figure 6), with results quantified using Accuracy, Precision, Recall, and F1-score.We divide the reasoning performance into two parts: low-level reasoning that focuses on the description of visual defects or anomalies and complex reasoning requiring the MLLMs to provide the potential cause and future improvement strategies for the detected anomalies, where ROUGE-L [54], Sentence-BERT (SBERT) [75], and GPT-Score (GPT-4 as the judge [57]) are utilized to quantify the similarity between generated text and ground truth.Note that low-level reasoning is highly correlated to detection performance, while anomaly-type descriptions of low-level reasoning determine the output of complex reasoning.</p>
<p>Experiments</p>
<p>Training &amp; Evaluation</p>
<p>There are two independent training stages for Anomaly-OV.In Stage 1, the components of the anomaly expert are trained to obtain the token selection capability, targeting traditional ZSAD.This stage utilizes all of the data with anomaly labels in Anomaly-Instruct-125k.Similar to previous works [6,110], when evaluating the model on the datasets contained in the training set, the corresponding datasets are replaced by VisA [115].In Stage 2, the anomaly expert and visual encoder are frozen, while the projector and LLM are trainable.In addition to our instruction dataset, we sample around 350k data from the original training recipe of LLaVA-OneVision to maintain the generalization ability.For more details on training, please refer to the supplementary.</p>
<p>The ZSAD performance for the anomaly expert is evaluated on nine benchmarks, including MVTec AD [2], VisA [115], AITEX [81], ELPV [16], BTAD [68], and MPDD [39] for industrial inspection, and BrainMRI [40], HeadCT [41], and Br35H [30] for medical diagnosis.AUROC (Area Under the Receiver Operating Characteristic) is leveraged to quantify the image-level AD performance.For text-based anomaly detection, both normal and anomaly data are employed to assess the accuracy by examining if the generated text contains the word Yes.Differently, only anomaly data are utilized to prompt the MLLMs to determine their anomaly reasoning capabilities since the justifications of normality are similar for different models.</p>
<p>Zero-Shot Anomaly Detection</p>
<p>As shown in  [6,110,114]).</p>
<p>The best and the second-best results are bolded and underlined, respectively.Please refer to the supplementary for more detailed results.</p>
<p>semantics for normality and abnormality from the data distribution in the absence of the text encoder.This observation also reveals that large-scale in-the-wild online data can benefit zero-shot performance in anomaly detection.</p>
<p>While the Q-Former reduces the model performance on BrainMRI, it shows effectiveness on most benchmarks, indicating the importance of token aggregation.Similarly, the look-back information and two learnable embeddings are required for describing class-awareness abnormality and distinguishing positive and negative features, respectively.As previously discussed, the anomaly expert is responsible for selecting suspicious visual tokens for the LLM, and the significance maps shown in Figure 7 demonstrate the interpretable token selection mechanism.The high intensities are automatically distributed around the anomalous areas even without any supervision of the anomaly masks.</p>
<p>Anomaly Detection &amp; Reasoning</p>
<p>With the strong capabilities of the anomaly expert for zeroshot detection and suspicious token selection, Anomaly-OV accomplishes significant improvement in text-based anomaly detection and reasoning over other open-sourced generalist MLLMs, as shown in Table 4.Here are a few observations: i) While a larger language model cannot guarantee better detection performance, it always provides greater reasoning ability; ii) Most of the existing MLLMs present much lower recall than precision, indicating their insensitivity to visual anomalies; iii) GPT-4o shows stronger reasoning ability PCB Example: Excessive Solder</p>
<p>User</p>
<p>Are there any anomalies visible in the image?</p>
<p>GPT-4o</p>
<p>The image shows an HC-SR04 ultrasonic sensor.</p>
<p>There don't appear to be any visible anomalies with the sensor itself.The components and pins seem intact and properly aligned.</p>
<p>LLaVA-OV The image shows an ultrasonic sensor module, specifically the HC-SR04 model.There are no visible anomalies in the image; it appears to be a standard representation of this type of sensor module.</p>
<p>Ours</p>
<p>Yes, there is a noticeable anomaly in the image: a short circuit on the PCB, indicated by a white, crusty residue near the 'Echo' pin.</p>
<p>Table 3. Anomaly-OV presents more accurate anomaly detection.</p>
<p>compared to other open-sourced models.Table 3 and Table 5 provide the qualitative comparison of our Anomaly-OV with its base model LLaVA-OV-7B [44] and GPT-4o [72].Both GPT-4o and LLaVA-OV show insensitivity to anomalous features and cannot accurately detect the anomaly in the image.Sometimes, GPT-4o knows the image is anomalous but fails to describe the anomalies precisely.We provide the fine-tuned version of the base model LLaVA-OV-0.5B on Anomaly-Instruct-125k, which presents much higher accuracy and more balanced precision and recall than its original version.This demonstrates the effectiveness of our instruction-tuning dataset.By integrating the anomaly expert with the base model, our Anomaly-OV-0.5Bachieves 0.08 accuracy and 0.06 F1-score improvements in text-based anomaly detection and better reasoning capability in low-level and complex settings.Equipped with a larger language model, Anomaly-OV-7B provides the best detection performance among all the existing MLLMs and shows comparable reasoning ability with GPT-4o.Notably, we observe that the anomaly expert restricts the detection perfor- mance of Anomaly-OV.Therefore, the design of a stronger anomaly expert is suggested for future works.</p>
<p>Macaroni Example: Yellowish Spot User Do you see any anomalies in the image?</p>
<p>GPT-4o</p>
<p>The image shows four pieces of elbow macaroni on a green background.The anomaly is that the macaroni pieces are not whole; they are cut in half.</p>
<p>LLaVA-OV The image shows four pieces of pasta, specifically macaroni shells, arranged on a green textured surface.</p>
<p>The pasta appears to be uniformly colored and shaped, with no visible defects or anomalies.</p>
<p>Ours</p>
<p>Yes, there is an anomaly in the image.The bottom right pasta piece has a noticeable yellowish discoloration on its surface.</p>
<p>Table 5. Anomaly-OV presents more precise anomaly reasoning.</p>
<p>Extension</p>
<p>With the generalization and multi-image processing capabilities of MLLMs, it is possible to build a unified assistant for visual inspection.Table 6 demonstrates the comprehensive knowledge of Anomaly-OV (without using Anomaly-ShapeNet [50] for training) on 3D and medical (testing set of BMAD [1]) AD &amp; reasoning.More data, benchmarks, and investigation on a unified model are meaningful.</p>
<p>Conclusion</p>
<p>In this paper, we establish the first large-scale visual instruction tuning dataset, Anomaly-Instruct-125k, and the corresponding benchmark, VisA-D&amp;R, to address the data scarcity issue for visual anomaly detection and reasoning.</p>
<p>3D Example: Bulge</p>
<p>User Is there any anomaly present in the bowl?</p>
<p>Anomaly-OV Yes, there is a bulge on the side of the bowl, which disrupts its otherwise smooth and symmetrical surface.</p>
<p>Medical Example: Pneumonia</p>
<p>User What is the most likely diagnosis for this patient?</p>
<p>Anomaly-OV Based on the chest X-ray, the most likely diagnosis for this patient is pneumonia.The presence of bilateral patchy opacities in the lower lung fields is a common finding in pneumonia, which is an infection that causes inflammation in the air sacs of the lungs.</p>
<p>User</p>
<p>What should the patient do next?</p>
<p>Anomaly-OV The patient should consult a healthcare professional for further evaluation and appropriate treatment.This may include antibiotics to treat the infection, supportive care such as oxygen therapy, and monitoring of the patient's condition.Then, a specialist MLLM, Anomaly-OV, targeting visual inspection is introduced to serve as the baseline in this domain.</p>
<p>Anomaly-OV leverages an anomaly expert to assist the LLM with suspicious visual token selection and presents significant improvements on both traditional ZSAD and text-based anomaly detection and reasoning tasks over existing methods.Extension to 3D and medical domains is demonstrated.GPT-4o The image shows green gel capsules that appear uniform in size, shape, and color.Upon inspection, no obvious anomalies such as broken capsules, irregular shapes, or unexpected colors are visible.</p>
<p>A1. Dataset Establishment</p>
<p>Let's draw a bounding box around the anomaly!User Do you observe any anomalies in the image?</p>
<p>GPT-4o Yes, there appears to be an anomaly in the image.One of the capsules, marked with a red box, has a noticeable defect.It seems to have a bubble or an air pocket inside, which is not observed in the other capsules.This could indicate a manufacturing defect or inconsistency in the capsule filling process.</p>
<p>If there is no bounding box, let's give a short hint to GPT-4o!User Do you observe any anomalies in the image?(hint: a bubble in the capsule)</p>
<p>GPT-4o The anomaly in the image is the presence of a visible air bubble one of the green gel capsules.While most capsules appear uniformly filled with liquid, the capsule located near the top center of the image has a small, clear air pocket within it.This could be a result of incomplete filling during the manufacturing process or an issue with the capsule sealing.</p>
<p>Table 7.Comparison of the GPT-4o [72] outputs with and without visual and textual hints for the anomaly.</p>
<p>As shown in Table 7, recent advanced MLLMs like GPT-4o fail to detect the anomalies in the image, so building the instruction tuning dataset using previous methods [8] is impractical.However, we observe that when the GPT-4o is provided some "hints", it presents impressive performance on anomaly reasoning or description.For example, a red bounding box drawn around the anomalous area enables GPT-4o to detect the tiny bubble inside the small capsule.This observation indicates that the anomaly information is already contained in the visual tokens, and the failure of existing MLLMs is because the language model cannot effectively pick out the related tokens, which is the major inspiration of our token-picking mechanism.Most of the existing AD datasets, such as MVTec AD [2], contain anomaly masks for anomaly localization.Therefore, we leverage these masks to generate the bounding boxes on the images.Specifically, the masks for an anomalous image are dilated and merged (if two masks are too close) before calculating the coordinates of the bounding boxes.Similarly, the image with bounding boxes drawn on it will serve as the visual prompt for GPT-4o.We also tried many other ways to utilize the anomaly masks, such as highlighting the mask area with different colors, consecutively providing the image and mask, and converting the normalized coordinates of the bounding box into a text prompt.None of them can as effectively guide the GPT-4o in finding anomalous features as drawing bounding boxes on the image.</p>
<p>A1.2. WebAD -The largest AD dataset</p>
<p>Existing industrial or medical anomaly detection datasets, such as MVTec AD [2] and BMAD [1], only contain a limited number of classes (&lt; 20) and several different anomaly types for each class (most of the anomaly types are similar) due to the collection of these kinds of anomaly images involves extensive human involvements.This limitation hinders the ZSAD model from learning a generic description of anomaly and normal patterns.Also, the MLLMs cannot obtain enough knowledge of visual anomaly descriptions for unseen anomaly types.Therefore, more diverse data is required for a robust ZSAD &amp; reasoning model.Many recent dataset works collect and annotate online images to enrich existing datasets and demonstrate their effectiveness in the training of current data-hungry deep learning models.</p>
<p>To collect the online images that can be utilized for anomaly detection, we design an automatic data collection pipeline by combining GPT-4o [72] and Google Image Search [26].As shown in Figure 8, we first employ GPT-4o to list 400 class names commonly seen in our daily life.Then, for each class, the GPT-4o is asked to generate 10 corresponding anomalous and normal phrases based on the class name.The abnormality or normality descriptions indicated by these phrases are specifically suitable for the class name.These phrases will serve as the search prompts to query the image links in Google Image Search.However, the downloaded images are very "dirty" and contain many noise samples and duplications.For example, the collected anomaly set contains lots of normal images, and vice versa.A data-cleaning step is applied after the image collection.</p>
<p>Since the duplications mainly occur within a specific class, we extract the CLIP [73] features for all the images in the class and compare the cosine similarity of these features.If the similarity value is larger than 0.99, then one of the images will be removed.To deal with the problematic grouping of anomaly and normal images, we combine the image and its corresponding search prompt and give them to GPT-4o for normal and anomaly classification.In the system prompt, we explicitly tell the GPT-4o that the search prompt is just a hint and not always correct and ask GPT-4o Figure 9. Overview of the gallery for in-the-wild image samples in WebAD.The images on the left side are anomalous, while the right side is for normal images.The links to download these images will be released to avoid copyright issues.Design a conversation between you and a user asking about this photo.The answers should be in a tone that the assistant is seeing the image and answering the question.Ask diverse questions and give corresponding answers.</p>
<p>The questions should be related to the anomalies in the image, including the presence of the anomalies, the reasoning, the nature, the location, the possible causes, the potential impacts, the solutions for current state, and the mitigation suggestions in the future.Only include questions that have definite answers:</p>
<p>(1) one can see the content in the image that the question asks about and can answer confidently;</p>
<p>(2) one can determine confidently from the image.Do not ask any question that cannot be answered confidently.</p>
<p>The conversation should be coherent and informative, covering various aspects of the image and the anomalies present.The conversation should be concise, engaging and informative, providing a detailed understanding of the image and the anomalies detected.</p>
<p>The output should be strictly formatted into JSON format.""" -----------------------------------------------------------------the in-context prompt ---------------------------------------------------------------------""" Example1 = ### Comprehensive Image Description:\nThe image shows two oranges placed on a wooden table.The oranges are partially wrapped in a clear plastic bag.The orange on the left has a clear and significant mold growth covering nearly half of its surface.The mold is white and green ‚Ä¶ Conversation1 = { "conversation": [ { "user": "Please describe the image you are seeing.","assistant": "The image features two oranges on a wooden table, partially wrapped in a clear plastic bag.The left orange has a significant mold growth covering nearly half of its surface, while the right orange appears healthy with no visible signs of mold."}, { "user": "Are there any anomalies for the oranges?", "assistant": "Yes, the anomaly in the image is the mold growth on the left orange, characterized by a fuzzy, white, and green layer that differs from the healthy appearance of the right orange."}, { "user": "Why do you think the left orange is anomalous?","assistant": "The left orange is anomalous due to the mold growth covering a substantial portion of its surface, indicating deterioration and unsuitability for consumption.</p>
<p>GPT-4o</p>
<p>The image shows an infrared sensor module, which appears to be intact and properly assembled.There doesn't seem to be any visible anomaly or damage to the components.</p>
<p>LLaVA-OV The image shows an electronic circuit board with various components, including a red LED that is not illuminated.There doesn't appear to be anything obviously anomalous in the image.</p>
<p>Ours</p>
<p>Yes, the anomaly in the image is the bent LED on the circuit board, which is not aligned properly with the board's surface.</p>
<p>Table 9.Additional results on VisA-D&amp;R (PCB).</p>
<p>Candle Example: Pink Discoloration</p>
<p>User Is there an anomaly in the image I should know about?</p>
<p>GPT-4o</p>
<p>Yes, there is an anomaly in the image.The candle in the top left corner has its wick facing the opposite direction compared to the other three candles.</p>
<p>LLaVA-OV Based on the image, there is no apparent anomaly among the candles.They are all similar in size and shape, with a consistent hole in the center for the wick.The coloration appears uniform across the candles, and there are no visible signs of melting or irregularities that would suggest an anomaly.</p>
<p>Ours</p>
<p>Yes, the anomaly is the presence of a small red spot on the bottom right candle, which contrasts with the otherwise uniform white color.GPT-4o Yes, there is an anomaly in the image.One of the capsules is a different color; it is brown instead of green.</p>
<p>LLaVA-OV Based on the image alone, there is no clear indication of an anomaly.The pills appear to be uniformly shaped and colored, with no visible defects or irregularities.However, without additional context or a point of reference for size, it's difficult to determine if there are any anomalies that might not be immediately apparent from the image.</p>
<p>Ours</p>
<p>Yes, there is an anomaly present in the image.One of the capsules has a darker spot on its surface, which contrasts with the otherwise uniform green color.</p>
<p>Table 11.Additional results on VisA-D&amp;R (Capsules).</p>
<p>recent paper: "Dense Connector for MLLMs" [96] ) should be a possible solution for performance improvement.</p>
<p>Fryum Example: Break Edge User Is there any anomaly in the image?</p>
<p>GPT-4o</p>
<p>The image shows a wheel-shaped snack on a green surface.There doesn't appear to be any obvious anomaly.</p>
<p>LLaVA-OV The image shows a circular object with a pattern that resembles a wheel or a gear, placed on a green textured surface.There doesn't appear to be any obvious anomaly in the image; it seems to be a simple photograph of an object against a background.If there is something specific you are referring to as an anomaly, please provide more details or clarify what aspect you are questioning.</p>
<p>Ours</p>
<p>Yes, there is an anomaly in the image.The pasta wheel has a noticeable crack on its lower right side, disrupting its otherwise smooth and continuous surface.</p>
<p>Table 12.Additional results on VisA-D&amp;R (Fryum).</p>
<p>Cashew Example: Brown Spot</p>
<p>User Does the image contain any anomaly?</p>
<p>GPT-4o</p>
<p>The image shows a cashew nut on a textured black surface.There are no visible anomalies; the cashew appears normal.</p>
<p>LLaVA-OV Based on the image provided, there are no visible anomalies.The object appears to be a single, intact cashew nut with a smooth surface and a natural brown coloration that is typical for cashews.There are no signs of damage, such as cracks or holes, and the background does not show any irregularities that would suggest an anomaly in the context of the image.</p>
<p>Ours</p>
<p>Yes, there is a noticeable anomaly on the cashew nut: a small brown spot located near the center of the nut.</p>
<p>Table 13.Additional results on VisA-D&amp;R (Cashew).</p>
<p>In-the-Wild Example: Graffitied Road Sign User Describe the anomaly in the image.</p>
<p>Anomaly-OV The anomaly in the image is the graffiti on the highway sign, which includes black spray paint marks and scribbles over the word 'Seattle' and other parts of the sign.</p>
<p>User What is the reason for the anomaly?</p>
<p>Anomaly-OV The graffiti is likely the result of vandalism, where individuals have used spray paint to deface public property for personal expression or as a form of protest.</p>
<p>User How does the anomaly affect the functionality of the object?</p>
<p>Anomaly-OV The graffiti affects the sign's functionality by potentially obstructing the visibility of important information, such as the word 'Seattle,' which could confuse drivers and lead to navigational errors.</p>
<p>User</p>
<p>How to avoid the anomaly in the future?</p>
<p>Anomaly-OV To avoid such anomalies, regular maintenance and cleaning of signs should be implemented.Additionally, community awareness programs can help reduce vandalism by promoting respect for public property.</p>
<p>Table 14.In-the-wild results for an unseen object (Road Sign).</p>
<p>Failure Example 1: Wrong Classification User Is there any anomaly visible in the image?</p>
<p>Anomaly-OV Yes, there is a noticeable anomaly on the soap bar: a hole with jagged edges located towards the left side.</p>
<p>Failure Example 2: General Description</p>
<p>User Are there any anomalies present in the image with the set of candles?</p>
<p>Anomaly-OV Yes, there is an anomaly present in the top right candle.</p>
<p>It has a noticeable crack on its surface, which is not typical for a smooth, intact candle.</p>
<p>Failure Example 3: Wrong Reasoning</p>
<p>User Are there any anomalies in the image?</p>
<p>Anomaly-OV Yes, the anomaly in the image is the absence of a crystal oscillator on the HC-SR04 ultrasonic sensor module.</p>
<p>Table 15.Failure results of Anomaly-OV on VisA-D&amp;R.</p>
<p>Figure 1 .
1
Figure1.Visualization of the image-level AUROC comparison between our Anomaly-OV and current state-of-the-art ZSAD methods (WinCLIP[38], AnoVL[19], AnomalyCLIP[110], AdaCLIP[6]).Notably, our zero-shot performance on VisA even surpasses most recent advances in the few-shot setting[28,51,112].</p>
<p>Figure 2 .
2
Figure 2. Industrial image anomaly reasoning results from GPT-4o [72] and our Anomaly-OV.The responses for fine-grained anomaly reasoning are highlighted, with the ground truth given for reference.</p>
<p>Figure 3 .
3
Figure 3. Overview of the Anomaly-OV architecture.It consists of two training stages: (1) professional training for the anomaly expert, and (2) visual instruction tuning for anomaly detection and reasoning.Text and visual tokens are distinguished by different colors.</p>
<p>Figure 4 .
4
Figure 4. Simulation of visual anomaly inspection by humans.</p>
<p>Figure 5 .
5
Figure 5. Composition of the instruction data in Anomaly-Instruct-125k.There are four main types of image samples: in-the-wild, industrial, medical, and 3D (in the format of multi-view images), covering most image anomaly detection tasks and enabling the possibility of a unified assistant for visual inspection.The reasoning words are highlighted in blue.For more information about dataset establishment, statistics, and the data collection pipeline, please refer to Section A1 in the supplementary.</p>
<p>Figure 6 .
6
Figure 6.Prompt examples in VisA-D&amp;R for detection and reasoning.The complex reasoning instructions are highlighted.</p>
<p>Figure 7 .
7
Figure 7. Visualization of the significance map on VisA samples.</p>
<p>A1. 1 .
1
How to highlight the anomaly?Prompt Example: Bubble Inside the Capsule User Do you observe any anomalies in the image?</p>
<p>system_prompt = """ You are an intelligent visual anomaly assistant, and you are seeing a single image.What you see are provided with a comprehensive image description and a deep analysis of the anomalies contained, describing the same image you are looking at.Answer all questions as you are seeing the image.</p>
<p>Table 1 .
1
Ablation study for the anomaly expert of Anomaly-OV.w/o.Look-back refers to the removal of v o 0 in LTFM.
MethodMVTec VisA HeadCT BrainMRIFull Model94.091.197.693.9w/o. Look-back92.890.596.693.5w/o. e + &amp; e ‚àí92.190.194.792.9w/o. Q-Former91.789.992.895.1w/o. WebAD88.588.991.293.4</p>
<p>Table 2
2, compared with existing methods, theanomaly expert of Anomaly-OV achieves significant image-level AUROC improvements on most of the ZSAD bench-
marks, which demonstrates that the text encoder widely applied in existing models is not necessary.The success of our model mainly originates from the extra data of WebAD (Table1), which enables the model to learn more generic</p>
<p>Table 2 .
2
Quantitative comparison of Image-level AUROC on different ZSAD methods (some of the results are borrowed from</p>
<p>Table 4 .
4
Quantitative comparison of text-based anomaly detection and reasoning for MLLMs.Notably, the Accuracy and F1-score for the anomaly expert of Anomaly-OV can be calculated as {0.78, 0.77} with threshold 0.5.<em> indicates the model is fine-tuned on our dataset.
ModelAnomaly DetectionLow-level ReasoningComplex ReasoningAccuracy Precision Recall F1-score ROUGE-L SBERT GPT-Score SBERT GPT-ScoreGPT-4V [71]0.680.900.490.550.160.653.310.775.64GPT-4o [72]0.700.830.710.680.240.714.840.816.89Qwen2-VL-2B [87]0.650.870.550.590.220.551.940.744.26Qwen2-VL-7B [87]0.760.910.690.750.250.613.090.684.62InternVL-2-8B [13]0.740.780.810.760.230.733.690.805.08InternVL-2-26B [13]0.730.860.660.680.210.744.130.805.49IXC-2.5-7B [101]0.720.880.630.670.210.582.450.775.14LLaVA-OV-0.5B [44]0.540.700.190.280.200.632.540.814.34LLaVA-OV-7B [44]0.710.950.560.630.240.663.570.795.44LLaVA-OV-0.5B</em>0.710.770.840.760.310.703.690.825.31Anomaly-OV-0.5B0.790.860.830.820.330.723.870.835.67Anomaly-OV-7B0.790.830.860.830.340.734.260.846.34</p>
<p>Table 6 .
6
Extension to 3D and medical AD &amp; reasoning.</p>
<p>Table 8 .
8
Comprehensive Image Description<strong>:\nThe image depicts a storefront window with shattered glass.It is part of a larger facade that includes additional glass panels on either side.The main window is severely damaged with multiple cracks spreading outward from a central impact point‚Ä¶ Per-class image-level AUROC of the anomaly expert of Anomaly-OV on VisA and MVTec AD.
"} ‚Ä¶Example2 = </strong>Conversation2 = """Figure 10. Prompt template for generating multi-round conversation in Anomaly-Instruct-125k (modified from the template of LLaVA [57]).in future research. One can change the base model to otherunseen anomaly features. Therefore, a more diverse indus-open-sourced MLLMs to resolve the wrong classificationtrial anomaly detection dataset is urgently required. Similarissue. Also, we found that the diversity of the anomalytype is very limited in existing industrial anomaly datasets(mainly 'crack' or 'broken'), causing the assistant to fail toprovide fine-grained anomaly reasoning or description for
to other traditional MLLMs, Anomaly-OV only utilizes the output visual tokens from the last layer of the visual encoder as the input for LLM.However, anomaly detection is highly dependent on low-level visual clues.Hence, forwarding PCB Example: Bent Transparent LEDUserIs there an anomaly in the image?</p>
<p>Table 10 .
10
Additional results on VisA-D&amp;R (Candle).</p>
<p>multi-level features from different layers to the LLM (asCapsule Example: Dark SpotUserIs there an anomaly present in the image?</p>
<p>Car, Airplane, Apple, Violin, Shirt, Banana, Door, Knife, Petri Dish, ‚Ä¶ "A scratched car", "A crashed airplane", "A molded apple", "A violin with crack", "A discolored shirt, ‚Ä¶Normal Image CollectionCar, Airplane, Apple, Violin, Shirt, Banana, Door, Knife, Petri Dish, ‚Ä¶ "A brand-new car", "A standard airplane", "A fresh apple", "A classical violin", "A clean shirt, ‚Ä¶ Anomalous Image CollectionGPT-4oNormal AnomalyImage CleaningCombine Prompt "A scratched car"GPT-4oA noticeable scratch runs horizontally across the car door.The scratch is white, suggesting the paint has been removed to show the underlying layer.It is not uniform, varying slightly ‚Ä¶ "human": Does the car in the image have any significant damage?"gpt": Yes, there is a noticeable horizontal scratch on the car door, which is white and varies in width and depth, indicating paint removal ‚Ä¶GPT-4oInstruction Generation to determine the normality and abnormality by itself.This step will remove the images with incorrect labels and the artificial images, such as cartons or art.Some samples in the collected WebAD dataset are shown in Figure9.In total, WebAD contains around 72k images from 380 classes and more than 5 anomaly types for each class.A1.3. Instruction Data GenerationFor existing datasets, we manually combine the anomaly type and the class name to create the short anomaly prompt (hint).Then, the image with or without the bounding boxes and the corresponding short prompt are utilized to prompt GPT-4o for the generation of detailed descriptions of the image and the anomalies.These descriptions contain all the information required for instruction-following data.The in-context learning strategy is implemented to generate the multi-round conversation data (see Figure10).Questions designed to elicit a one-word answer are utilized to balance the distribution of the normal and anomaly samples.A2. Training DetailsIn the professional training stage, we leverage AdamW[65]to be the optimizer and CosineAnnealingWarmRestarts[64]as the learning rate scheduler.The initial learning rate is set to be 1e ‚àí 4, and the restart iteration is half of the single epoch.The anomaly expert is trained on 8 H100 GPUs for 2 epochs (2 hours), and the total batch size is 128.In the instruction tuning stage, we follow the default training setting of LLaVA-OneVision<a href="reduce the batch size to 128">44</a>, and the total training time for 0.5B and 7B models are 7 hours and 50 hours on 8 H100, respectively.When sampling the instruction data from the original recipe of LLaVA-OneVision, we put more emphasis on low-level image understanding and 3D multi-view Q&amp;A, considering that anomaly detection originates from the low-level feature differences and the 3D anomaly detection requires multi-image understanding.Besides, for more knowledge in the medical domain, the model is also fed with the data from LLaVA-Med[45].A3. Experimental ResultsA3.1. Anomaly DetectionSimilar to previous ZSAD works, the detailed image-level AUROC results for the anomaly expert of Anomaly-OV on VisA[115]and MVTec AD[2]are provided in Table8.A3.2. Anomaly ReasoningTable9to 13 presents more comparison results of GPT-4o[72], LLaVA-OneVision[44], and Anomaly-OV on AD &amp; reasoning.Anomaly-OV shows better performance in the detection and description of the visual anomalies in the images.Table14demonstrates the low-level and complex reasoning capability of Anomaly-OV for an in-the-wild image, indicating a comprehensive understanding of the anomaly.A4. Limitation and Future WorkLimitation.As shown in Table15, sometimes, Anomaly-OV fails to provide an accurate classification of the target object, describes the anomaly by a general word (wax missing is described by "crack"), or presents wrong reasoning with hallucination.Also, there is still a large space for improvement in the detection performance of Anomaly-OV.Besides, the images contained in VisA-D&amp;R are from the industrial domain, so more benchmarks in other domains, such as 3D and medical anomaly detection, are required to evaluate a unified AD &amp; reasoning model.Future Work.The detection performance of Anomaly-OV is highly determined by the anomaly expert (see Table4), so a more advanced design of the expert model is recommended
Bmad: Benchmarks for medical anomaly detection. Jinan Bao, Hanshi Sun, Hanqiu Deng, Yinsheng He, Zhaoxiang Zhang, Xingyu Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024. 1, 5, 6, 8, 14</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019. 1, 5, 6, 1415</p>
<p>Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. Paul Bergmann, Michael Fauser, David Sattlegger, Carsten Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>The mvtec 3d-ad dataset for unsupervised 3d anomaly detection and localization. Paul Bergmann, Xin Jin, David Sattlegger, Carsten Steger, Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and ApplicationsSCITEPRESS -Science and Technology Publications2022</p>
<p>Anomaly detection under distribution shift. Tri Cao, Jiawen Zhu, Guansong Pang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection. Yunkang Cao, Jiangning Zhang, Luca Frittoli, Yuqi Cheng, Weiming Shen, Giacomo Boracchi, European Conference on Computer Vision. Springer2025. 1, 2, 3, 6, 7</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Sharegpt4v: Improving large multi-modal models with better captions. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, Dahua Lin, 2023514</p>
<p>A unified anomaly synthesis strategy with gradient ascent for industrial anomaly detection and localization. Qiyu Chen, Huiyuan Luo, Chengkan Lv, Zhengtao Zhang, arXiv:2407.093592024arXiv preprint</p>
<p>Clip2scene: Towards label-efficient 3d scene understanding by clip. Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, Wenping Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>April-gan: A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on few-shot ad. Xuhai Chen, Yue Han, Jiangning Zhang, 202327</p>
<p>Deep one-class classification via interpolated gaussian descriptor. Yuanhong Chen, Yu Tian, Guansong Pang, Gustavo Carneiro, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022</p>
<p>. Zhe Chen, Weiyun Wang, Shenglong Hao Tian, Zhangwei Ye, Erfei Gao, Wenwen Cui, Kongzhi Tong, Jiapeng Hu, Zheng Luo, Ji Ma, Jiaqi Ma, Xiaoyi Wang, Hang Dong, Hewei Yan, Conghui Guo, Botian He, Zhenjiang Shi, Chao Jin, Bin Xu, Xingjian Wang, Wei Wei, Wenjian Li, Bo Zhang, Pinlong Zhang, Licheng Cai, Xiangchao Wen, Min Yan, Lewei Dou, Xizhou Lu, Tong Zhu, Dahua Lu, Yu Lin, Jifeng Qiao, Wenhai Dai, Wang, 2024How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites</p>
<p>Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning. Zebang Cheng, Zhi-Qi Cheng, Jun-Yan He, Jingdong Sun, Kai Wang, Yuxiang Lin, Zheng Lian, Xiaojiang Peng, Alexander Hauptmann, Conference on Neural Information Processing Systems. 20241</p>
<p>InstructBLIP: Towards general-purpose vision-language models with instruction tuning. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, Steven Hoi, Thirtyseventh Conference on Neural Information Processing Systems. 20231</p>
<p>Automatic classification of defective photovoltaic module cells in electroluminescence images. Sergiu Deitsch, Stephan Vincent Christlein, Claudia Berger, Andreas Buerhop-Lutz, Florian Maier, Christian Gallwitz, Riess, Solar Energy. 18562019</p>
<p>Simclip: Refining image-text alignment with simple prompts for zero-/fewshot anomaly detection. Chenghao Deng, Haote Xu, Xiaolu Chen, Haodi Xu, Xiaotong Tu, Xinghao Ding, Yue Huang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Anomaly detection via reverse distillation from one-class embedding. Hanqiu Deng, Xingyu Li, IEEE/CVF conference on computer vision and pattern recognition. 2022</p>
<p>Anovl: Adapting vision-language models for unified zeroshot anomaly localization. Hanqiu Deng, Zhaoxiang Zhang, Jinan Bao, Xingyu Li, arXiv:2308.15939202317arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2019</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, International Conference on Learning Representations. 2021</p>
<p>Fastrecon: Few-shot industrial anomaly detection via fast feature reconstruction. Zheng Fang, Xiaoyang Wang, Haocheng Li, Jiejie Liu, Qiugui Hu, Jimin Xiao, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Chatpose: Chatting about 3d human pose. Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Michael J Black, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Deep learning for medical anomaly detection-a survey. Tharindu Fernando, Harshala Gammulle, Simon Denman, Sridha Sridharan, Clinton Fookes, ACM Computing Surveys (CSUR). 5472021</p>
<p>Transfusion-a transparency-based diffusion model for anomaly detection. Matic Fuƒçka, Vitjan Zavrtanik, Danijel Skoƒçaj, European conference on computer vision. Springer2025</p>
<p>Google-images-search. Google, 1.4.7, 2024614</p>
<p>Filo: Zero-shot anomaly detection by fine-grained description and high-quality localization. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Hao Li, Ming Tang, Jinqiao Wang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence1932-1940, 2024. 1, 2, 3, 5</p>
<p>Yuxiang Guo, Faizan Siddiqui, Yang Zhao, Rama Chellappa, Shao-Yuan Lo, Stimuvar, arXiv:2409.00304Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models. 20241arXiv preprint</p>
<p>Br35h: Brain tumor detection 2020. Ahmed Hamada, 2020</p>
<p>Madgan: Unsupervised medical anomaly detection gan using multiple adjacent brain mri slice reconstruction. Changhee Han, Leonardo Rundo, Kohei Murao, Tomoyuki Noguchi, Yuki Shimahara, Zolt√°n √Åd√°m Milacski, Saori Koshino, Evis Sala, Hideki Nakayama, Shin'ichi Satoh, BMC bioinformatics. 2212021</p>
<p>Learning unified reference representation for unsupervised multi-class anomaly detection. Liren He, Zhengkai Jiang, Jinlong Peng, Liang Liu, Qiangang Du, Xiaobin Hu, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang, arXiv:2403.115612024arXiv preprint</p>
<p>Long-tailed anomaly detection with learnable class names. Chih-Hui Ho, Kuan-Chuan Peng, Nuno Vasconcelos, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Divide-and-assemble: Learning block-wise memory for unsupervised anomaly detection. Jinlei Hou, Yingying Zhang, Qiaoyong Zhong, Di Xie, Shiliang Pu, Hong Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Registration based few-shot anomaly detection. Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, Yan-Feng Wang, European Conference on Computer Vision. Springer20221</p>
<p>Adapting visual-language models for generalizable anomaly detection in medical images. Chaoqin Huang, Aofan Jiang, Jinghao Feng, Ya Zhang, Xinchao Wang, Yanfeng Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20241</p>
<p>Towards open-world objectbased anomaly detection via self-supervised outlier synthesis. Brian Ks Isaac-Medina, Yona Falinie, Neelanjan Gaus, Toby P Bhowmik, Breckon, European Conference on Computer Vision (ECCV). 2024</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, Onkar Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition19606-19616, 2023. 1, 2, 7</p>
<p>Deep learning-based defect detection of metal parts: evaluating current methods in complex conditions. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, Milos Skotak, 2021 13th International congress on ultra modern telecommunications and control systems and workshops (ICUMT). IEEE2021</p>
<p>Brain tumor detection using mri images. Pranita Balaji, Kanade Gumaste, Brain. 322015</p>
<p>Head ct -hemorrhage. Felipe Campos, Kitamura , 2018</p>
<p>Text-guided variational image generation for industrial anomaly detection and segmentation. Mingyu Lee, Jongwon Choi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Zero-shot anomaly detection via batch normalization. Aodong Li, Chen Qiu, Marius Kloft, Padhraic Smyth, Maja Rudolph, Stephan Mandt, Advances in Neural Information Processing Systems. 202436</p>
<p>Llava-onevision: Easy visual task transfer. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li, 2024. 1, 3, 7, 815</p>
<p>LLaVA-med: Training a large languageand-vision assistant for biomedicine in one day. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao, Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023115</p>
<p>Cutpaste: Self-supervised learning for anomaly detection and localization. Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister, IEEE/CVF conference on computer vision and pattern recognition. 2021</p>
<p>Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, Chunyuan Li, arXiv:2407.07895Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. 202413arXiv preprint</p>
<p>Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International conference on machine learning. PMLR20221</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. 202324</p>
<p>Towards scalable 3d anomaly detection and localization: A benchmark via 3d anomaly synthesis and a self-supervised learning network. Wenqiao Li, Xiaohao Xu, Yao Gu, Bozhong Zheng, Shenghua Gao, Yingna Wu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20246</p>
<p>Promptad: Learning prompts with only normal samples for few-shot anomaly detection. Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, Lizhuang Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20241</p>
<p>Promptad: Zero-shot anomaly detection using text prompts. Yiting Li, Adam Goodge, Fayao Liu, Chuan-Sheng Foo, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Open-vocabulary semantic segmentation with mask-adapted clip. Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Clip is also an efficient segmenter: A text-driven approach for weakly supervised semantic segmentation. Yuqi Lin, Minghao Chen, Wenxiao Wang, Boxi Wu, Ke Li, Binbin Lin, Haifeng Liu, Xiaofei He, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Thirty-seventh Conference on Neural Information Processing Systems. 2023. 1, 2, 3, 5, 616</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, Yong Jae Lee, Llava-next: Improved reasoning, ocr, and world knowledge. 2024</p>
<p>Real3d-AD: A dataset of point cloud anomaly detection. Jiaqi Liu, Guoyang Xie, Xinpeng Li, Jinbao Wang, Yong Liu, Chengjie Wang, Feng Zheng, Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Clip-driven universal model for organ segmentation and tumor detection. Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Yixuan Bennett A Landman, Alan Yuan, Yucheng Yuille, Zongwei Tang, Zhou, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019</p>
<p>Adversarially robust one-class novelty detection. Shao-Yuan, Poojan Lo, Oza, Patel, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2022</p>
<p>SGDR: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 201715</p>
<p>Decoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, International Conference on Learning Representations. 201915</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Video anomaly detection and explanation via large language models. Hui Lv, Qianru Sun, arXiv:2401.0570220241arXiv preprint</p>
<p>Vt-adl: A vision transformer network for image anomaly detection and localization. Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, Gian Luca Foresti, 2021 IEEE 30th International Symposium on Industrial Electronics (ISIE). IEEE2021</p>
<p>RGI: robust GANinversion for mask-free image inpainting and unsupervised pixel-wise anomaly detection. Shancong Mou, Xiaoyi Gu, Meng Cao, Haoping Bai, Ping Huang, Jiulong Shan, Jianjun Shi, The Eleventh International Conference on Learning Representations. 20231</p>
<p>Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving. Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, Li Zhang, European Conference on Computer Vision. Springer2025</p>
<p>Gpt-4v(ision) system card. 2023OpenAI</p>
<p>Gpt-4o system card. 2024. 2, 5, 6, 7, 8, 1415OpenAI</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. 14PMLR, 2021. 2, 3, 7</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, 2023</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, 2019</p>
<p>Mean-shifted contrastive loss for anomaly detection. Tal Reiss, Yedid Hoshen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2023</p>
<p>Towards total recall in industrial anomaly detection. Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch√∂lkopf, Thomas Brox, Peter Gehler, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition20221</p>
<p>Promptguided zero-shot anomaly action recognition using pretrained deep skeleton features. Fumiaki Sato, Ryo Hachiuma, Taiki Sekii, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2023</p>
<p>Maeday: Mae for few-and zero-shot anomaly-detection. Computer Vision and Image Understanding, 241:103958. Eli Schwartz, Assaf Arbelle, Leonid Karlinsky, Sivan Harary, Florian Scheidegger, Sivan Doveh, Raja Giryes, 2024</p>
<p>Multimodal long-horizon reasoning for robotics. Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, J Nikhil, Joshi, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>A public fabric database for defect detection methods and results. Javier Silvestre-Blanes, Teresa Albero-Albero, Ignacio Miralles, Rub√©n P√©rez-Llorens, Jorge Moreno, Autex Research Journal. 1942019</p>
<p>Generalad: Anomaly detection across domains by attending to distorted features. Mohammadreza Luc Pj Str√§ter, Efstratios Salehi, Gavves, G M Cees, Yuki M Snoek, Asano, arXiv:2407.124272024arXiv preprint</p>
<p>Face-mllm: A large face perception model. Haomiao Sun, Mingjie He, Tianheng Lian, Hu Han, Shiguang Shan, 2024</p>
<p>An incremental unified framework for small defect inspection. Jiaqi Tang, Hao Lu, Xiaogang Xu, Ruizheng Wu, Sixing Hu, Tong Zhang, Tsz Wa Cheng, Ming Ge, Ying-Cong Chen, Fugee Tsung, European Conference on Computer Vision. Springer2025</p>
<p>Revisiting reverse distillation for anomaly detection. Tran Dinh Tien, Anh Tuan Nguyen, Nguyen Hoang Tran, Ta Duc Huy, Soan Duong, Chanh D Tr Nguyen, Steven Qh Truong, IEEE/CVF conference on computer vision and pattern recognition. 2023</p>
<p>Clipn for zero-shot ood detection: Teaching clip to say no. Hualiang Wang, Yi Li, Huifeng Yao, Xiaomeng Li, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, Junyang Lin, 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Conference on Neural Information Processing Systems. 2022</p>
<p>Anomaly detection for medical images based on a one-class classification. Qi Wei, Yinhao Ren, Rui Hou, Bibo Shi, Joseph Y Lo, Lawrence Carin, Computer-Aided Diagnosis. SPIE2018. 2018</p>
<p>Diffusion models for medical anomaly detection. Julia Wolleb, Florentin Bieder, Robin Sandk√ºhler, Philippe C Cattin, International Conference on Medical image computing and computer-assisted intervention. Springer2022</p>
<p>Funqa: Towards surprising video comprehension. Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, Ziwei Liu, European Conference on Computer Vision. Springer2025</p>
<p>Pushing the limits of fewshot anomaly detection in industry vision: Graphcore. Guoyang Xie, Jinbao Wang, Jiaqi Liu, Yaochu Jin, Feng Zheng, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Emovit: Revolutionizing emotion insights with visual instruction tuning. Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, Wen-Huang Cheng, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Follow the rules: reasoning for video anomaly detection with large language models. Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, Shao-Yuan Lo, arXiv:2407.1029920241arXiv preprint</p>
<p>Glad: Towards better reconstruction with global and local adaptive diffusion models for unsupervised anomaly detection. Hang Yao, Ming Liu, Haolin Wang, Zhicun Yin, Zifei Yan, Xiaopeng Hong, Wangmeng Zuo, arXiv:2406.074872024arXiv preprint</p>
<p>Dense connector for mllms. Huanjin Yao, Wenhao Wu, Taojiannan Yang, Yuxin Song, Mengxi Zhang, Haocheng Feng, Yifan Sun, Zhiheng Li, Wanli Ouyang, Jingdong Wang, 202417</p>
<p>Hierarchical gaussian mixture normalizing flow modeling for unified anomaly detection. Xincheng Yao, Ruoqi Li, Zefeng Qian, Lu Wang, Chongyang Zhang, arXiv:2403.133492024arXiv preprint</p>
<p>A unified model for multi-class anomaly detection. Zhiyuan You, Lei Cui, Yujun Shen, Kai Yang, Xin Lu, Yu Zheng, Xinyi Le, Advances in Neural Information Processing Systems. 2022</p>
<p>Draema discriminatively trained reconstruction embedding for surface anomaly detection. Vitjan Zavrtanik, Matej Kristan, Danijel Skoƒçaj, IEEE/CVF international conference on computer vision. 2021</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>. Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang, 2024Internlm-xcomposer-2.5: A versatile large vision language model supporting long-contextual input and output</p>
<p>Destseg: Segmentation guided denoising studentteacher for anomaly detection. Xuan Zhang, Shiyu Li, Xi Li, Ping Huang, Jiulong Shan, Ting Chen, IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023</p>
<p>Mediclip: Adapting clip for fewshot medical image anomaly detection. Ximiao Zhang, Min Xu, Dehui Qiu, Ruixin Yan, Ning Lang, Xiuzhuang Zhou, International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer2024</p>
<p>Realnet: A feature selection network with realistic synthetic anomaly for anomaly detection. Ximiao Zhang, Min Xu, Xiuzhuang Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, Transactions on Machine Learning Research. 22024</p>
<p>Anomaly detection for medical images using self-supervised and translation-consistent features. He Zhao, Yuexiang Li, Nanjun He, Kai Ma, Leyuan Fang, Huiqi Li, Yefeng Zheng, IEEE Transactions on Medical Imaging. 40122021</p>
<p>Navgpt: Explicit reasoning in vision-and-language navigation with large language models. Gengze Zhou, Yicong Hong, Qi Wu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2024</p>
<p>Learning to prompt for vision-language models. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu, International Journal of Computer Vision. 13092022</p>
<p>Vicor: Bridging visual understanding and commonsense reasoning with large language models. Kaiwen Zhou, Kwonjoon Lee, Teruhisa Misu, Xin Eric, Wang , Findings of the. Association for Computational Linguistics1202</p>
<p>AnomalyCLIP: Object-agnostic prompt learning for zero-shot anomaly detection. Qihang Zhou, Guansong Pang, Yu Tian, Shibo He, Jiming Chen, The Twelfth International Conference on Learning Representations. 2024. 1, 2, 3, 6, 7</p>
<p>MiniGPT-4: Enhancing vision-language understanding with advanced large language models. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny, The Twelfth International Conference on Learning Representations. 20241</p>
<p>Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. Jiawen Zhu, Guansong Pang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition20241</p>
<p>Do llms understand visual anomalies? uncovering llm's capabilities in zero-shot anomaly detection. Jiaqi Zhu, Shaofeng Cai, Fang Deng, Beng , Chin Ooi, Junran Wu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Fine-grained abnormality prompt learning for zeroshot anomaly detection. Jiawen Zhu, Yew-Soon Ong, Chunhua Shen, Guansong Pang, 2024</p>
<p>Spot-the-difference self-supervised pretraining for anomaly detection and segmentation. Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, Onkar Dabeer, European Conference on Computer Vision. Springer2022615</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1066 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1066</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1066</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-2bb5873a1a96205fb86cee12bf137f48ef13f675</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2bb5873a1a96205fb86cee12bf137f48ef13f675" target="_blank">Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a novel intrinsic reward, called causal curiosity, and shows that it allows reinforcement learning agents to learn optimal sequences of actions, and to discover causal factors in the dynamics.</p>
                <p><strong>Paper Abstract:</strong> Humans show an innate ability to learn the regularities of the world through interaction. By performing experiments in our environment, we are able to discern the causal factors of variation and infer how they affect the dynamics of our world. Analogously, here we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce a novel intrinsic reward, called causal curiosity, and show that it allows our agents to learn optimal sequences of actions, and to discover causal factors in the dynamics. The learned behavior allows the agent to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., to differentiate between heavy and light blocks, our agents learn to lift them), and are learnt in a self-supervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1066.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1066.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CausalCuriousAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causally-curious Reinforcement Learning Agent (uses Causal Curiosity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated robotic RL agent that uses an MDL-derived intrinsic reward called 'causal curiosity' to discover self-supervised experimental behaviors that isolate single causal factors, learn binary quantized causal representations, and improve transfer and zero-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Causally-curious agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent that (1) plans short experimental action sequences with a CEM-based Model Predictive Control planner optimizing an intrinsic MDL-based reward (causal curiosity) and (2) uses a PPO-optimized actor-critic policy for downstream tasks conditioned on learned causal representations. The intrinsic reward is computed via a fixed low-capacity bimodal clustering model and a silhouette-like MDL objective over Soft-DTW distances of trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (simulated 3-finger robot in PyBullet CausalWorld; also tested on simulated articulated agents in MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (PyBullet) object-manipulation; MuJoCo Control Suite locomotion (Ant, HalfCheetah, Hopper, Walker)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>CausalWorld: single-object manipulation tasks in a realistic physics simulator with observable state (positions, orientations); controlled causal factors include mass, size, and shape (cubes vs spheres). MuJoCo: articulated locomotion agents where body mass is varied. Complexity arises from multiple simultaneous causal factors affecting trajectories, rich physics interactions (friction, gravity, contact dynamics), and partial observability (agent only observes trajectories/observations, not causal factors).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized primarily by the number of causal factors that vary simultaneously (1, 2, or 3), number of distinct training environments, and the number of effective causal parents influencing an observation (minimized via MDL objective). Specific measurable quantities used in experiments: number of training environments (Mass: 5; SizeMass: 30; ShapeSizeMass: 60), trajectory horizon T=6 control steps, MDL objective computed with Soft-DTW distances and a bimodal clustering model (silhouette-like score).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varied: Mass = low complexity (1 varying factor), SizeMass = medium complexity (2 varying factors), ShapeSizeMass = high complexity (3 varying factors)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by number and parameter ranges of environment instances: Mass experiments use masses in [0.1, 0.5] kg over 5 envs; SizeMass uses masses ∈ [0.1,0.5] kg and sizes ∈ [0.05,0.1] m over 30 envs; ShapeSizeMass uses masses ∈ [0.1,0.5] kg, sizes ∈ [0.05,0.1] m, shapes ∈ {cube,sphere} over 60 envs. MuJoCo experiments vary agent body masses from 0.5× to 1.5× default. Transfer experiments used 10 training environments and tested on out-of-distribution masses (0.7, 0.75 kg).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Varied across experiments: Mass = low variation, SizeMass = medium variation, ShapeSizeMass = high variation; MuJoCo mass variation = medium</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>External task reward per trajectory (for downstream tasks), zero-shot generalization performance on unseen environments, convergence speed (training steps to reach performance), and sample efficiency (number of environment interactions). Additionally, intrinsic objective is MDL-based reward (negative information overflow) computed via clustering/Soft-DTW.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative and relative: causal-curiosity pre-training yields ~2.5× better sample efficiency than conventional supervised planners (paper claim). Example training detail: curiosity pre-training for a lifting skill used 600,000 timesteps. Results reported averaged over 10 seeds; no single absolute reward numbers given in paper, but plots indicate faster convergence and higher zero-shot rewards vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: (1) The method aims to reduce the 'effective complexity' of observations by finding actions that gate out most causal parents (minimize number of causal factors influencing the observation) using an MDL objective; (2) empirically, as the number of varying causal factors increases, the performance gap (zero-shot generalization) between the causally-curious agent and a Generalist baseline increases, indicating improved robustness to higher variation; (3) trade-off: combining curiosity with external reward (additive objective) leads to competing goals and suboptimal performance; (4) scalability caveat: the one-factor-at-a-time (OFAT)-style recursive scheme scales exponentially with the number of causal factors.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitative: in high-complexity, high-variation setup (ShapeSizeMass: 3 varying factors, 60 training envs) the agent discovered hierarchical binary latent factors and produced disentangled binary representations; causally-curious agents showed better zero-shot generalization and transfer than baselines (no absolute numeric reward reported).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Qualitative: in Mass (1 varying factor, 5 envs) the agent reliably learned semantically meaningful lift/push experiments that perfectly separate low/high mass clusters; these experiments were sample-efficient relative to baseline training.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Multi-environment training with a recursive OFAT-like tree procedure: use CEM-based MPC to discover short experimental action sequences across multiple training environments, cluster outcomes to learn a binary latent for one causal factor, then recursively partition environment set and repeat. Downstream/transfer policies trained with PPO using causal representations as contextual input.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Zero-shot generalization tested on unseen combinations and unseen parameter values (e.g., unseen masses 0.7 and 0.75 kg and unseen size/mass combinations). Causally-curious agents significantly outperformed both a Generalist (trained across training envs without causal reps) and a Specialist (trained only on test environments) averaged over 10 seeds; gap increases with more varying causal factors. Qualitatively reported as 'high zero-shot generalizability' and faster adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Reported approximate improvement: unsupervised experimental behaviors (pre-training with causal curiosity) are ≈2.5× more sample efficient than conventional supervised planners; example: curiosity pre-training used 600,000 timesteps for lifting skill. CEM experiment planner sampling regime: horizon 6, 30 plans sampled per CEM iteration, top 10% used, 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) An MDL-derived intrinsic reward (causal curiosity) yields self-supervised experimental behaviors that isolate single causal factors and produce interpretable binary latent representations. 2) These learned experiments are semantically meaningful (e.g., lift to test mass, roll to test shape) and are sample-efficient: ≈2.5× less data than conventional planners for acquiring similar behaviors. 3) Causal representations improve zero-shot generalization and transfer to more complex/unseen tasks; the performance gap versus non-causal baselines increases with the number of varying causal factors. 4) Combining curiosity and external reward naively (additive objective) leads to suboptimal trade-offs due to competing goals. 5) The recursive OFAT-style discovery scales poorly (exponential) as number of causal factors grows.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_notes_on_limitations</strong></td>
                            <td>No absolute numeric task-reward values reported in main text; many claims are relative/qualitative. Scalability to many causal factors is explicitly identified as a limitation (OFAT-style exponential scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1066.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1066.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VanillaCEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vanilla Cross-Entropy Method (CEM) Model Predictive Control baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard CEM-based MPC planner trained to maximize external task reward (no causal curiosity intrinsic reward), used as a baseline to compare convergence, sample efficiency, and worst-case performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Vanilla CEM planner</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model Predictive Control planner optimized with the Cross-Entropy Method (CEM) that samples candidate action sequences (30 plans per iteration in experiments), applies them to the true environment, and updates the sampling distribution based on top-performing plans to maximize the external task reward.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated planner controlling robotic agents in CausalWorld</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld object-manipulation (Lifting and Travel downstream tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same simulated single-object manipulation environments used for the causal-curiosity experiments; downstream tasks include Lifting (grasp and lift block to target height) and Travel (impart velocity to a block along a direction). Environment complexity and variation mirror those used in the causal curiosity experiments (varying mass/size/shape depending on setup).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same measures as main experiments (number of varying causal factors and number of training environments). Complexity of the task measured by convergence time and success on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varied depending on experiment (used for direct comparison in Mass/SizeMass/ShapeSizeMass settings); generally compared on same problem instances as the causal-curiosity agent.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Operates on the same training/test environment distributions (e.g., Mass: 5 envs with masses 0.1-0.5 kg; Transfer setups with 10 training envs and OOD test masses).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Matched to the corresponding causal-curiosity experimental condition (low/medium/high as per Mass/SizeMass/ShapeSizeMass).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>External task reward per trajectory, convergence time (training steps), and worst-case performance across seeds/environments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Relative qualitative: Vanilla CEM converges slower and is less sample-efficient than the causal-curiosity pre-trained agent; the paper reports that causal-curiosity pre-training achieved similar downstream performance with ≈2.5× less data than conventional planners. No absolute numeric reward values reported for Vanilla CEM in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Used as baseline to show that naive policy optimization across varied environments (domain generalist) struggles when the number of varying causal factors increases — i.e., higher variation and causal complexity degrade generalist performance more than the causally-curious agent.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitative: performed worse than causally-curious agent on high complexity/high variation experiments (ShapeSizeMass), converged slower and showed worse worst-case performance across random seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Qualitative: in simple Mass setting, Vanilla CEM can learn the task but requires more data/time than the curiosity-pretrained agent.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Single-objective external reward optimization via CEM-MPC on the true environment; no intrinsic curiosity during planner training (identical planner architecture and hyperparameters to the CEM used inside causal-curiosity procedure but optimized for external reward).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalist CEM baseline trained on the same training environments does not generalize as well zero-shot to unseen test environments compared to the causally-curious agent; Specialist baseline (trained only on test envs) performs poorly, indicating test tasks are non-trivial.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Qualitatively lower than causally-curious pre-training; paper reports ≈2.5× worse sample efficiency relative to curiosity-pretraining (no exact CEM-only timesteps reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Vanilla CEM baseline is less sample-efficient and generalizes worse to unseen environments with differing causal factors than the causally-curious agent; combining curiosity and external reward (additive objective) with CEM yields suboptimal performance due to conflicting objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causalworld: A robotic manipulation benchmark for causal structure and transfer learning <em>(Rating: 2)</em></li>
                <li>Generalized hidden parameter mdps: Transferable model-based rl in a handful of trials <em>(Rating: 2)</em></li>
                <li>Varibad: A very good method for bayes-adaptive deep rl via meta-learning <em>(Rating: 2)</em></li>
                <li>Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1066",
    "paper_id": "paper-2bb5873a1a96205fb86cee12bf137f48ef13f675",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CausalCuriousAgent",
            "name_full": "Causally-curious Reinforcement Learning Agent (uses Causal Curiosity)",
            "brief_description": "A simulated robotic RL agent that uses an MDL-derived intrinsic reward called 'causal curiosity' to discover self-supervised experimental behaviors that isolate single causal factors, learn binary quantized causal representations, and improve transfer and zero-shot generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Causally-curious agent",
            "agent_description": "Reinforcement learning agent that (1) plans short experimental action sequences with a CEM-based Model Predictive Control planner optimizing an intrinsic MDL-based reward (causal curiosity) and (2) uses a PPO-optimized actor-critic policy for downstream tasks conditioned on learned causal representations. The intrinsic reward is computed via a fixed low-capacity bimodal clustering model and a silhouette-like MDL objective over Soft-DTW distances of trajectories.",
            "agent_type": "simulated robotic agent (simulated 3-finger robot in PyBullet CausalWorld; also tested on simulated articulated agents in MuJoCo)",
            "environment_name": "CausalWorld (PyBullet) object-manipulation; MuJoCo Control Suite locomotion (Ant, HalfCheetah, Hopper, Walker)",
            "environment_description": "CausalWorld: single-object manipulation tasks in a realistic physics simulator with observable state (positions, orientations); controlled causal factors include mass, size, and shape (cubes vs spheres). MuJoCo: articulated locomotion agents where body mass is varied. Complexity arises from multiple simultaneous causal factors affecting trajectories, rich physics interactions (friction, gravity, contact dynamics), and partial observability (agent only observes trajectories/observations, not causal factors).",
            "complexity_measure": "Characterized primarily by the number of causal factors that vary simultaneously (1, 2, or 3), number of distinct training environments, and the number of effective causal parents influencing an observation (minimized via MDL objective). Specific measurable quantities used in experiments: number of training environments (Mass: 5; SizeMass: 30; ShapeSizeMass: 60), trajectory horizon T=6 control steps, MDL objective computed with Soft-DTW distances and a bimodal clustering model (silhouette-like score).",
            "complexity_level": "Varied: Mass = low complexity (1 varying factor), SizeMass = medium complexity (2 varying factors), ShapeSizeMass = high complexity (3 varying factors)",
            "variation_measure": "Measured by number and parameter ranges of environment instances: Mass experiments use masses in [0.1, 0.5] kg over 5 envs; SizeMass uses masses ∈ [0.1,0.5] kg and sizes ∈ [0.05,0.1] m over 30 envs; ShapeSizeMass uses masses ∈ [0.1,0.5] kg, sizes ∈ [0.05,0.1] m, shapes ∈ {cube,sphere} over 60 envs. MuJoCo experiments vary agent body masses from 0.5× to 1.5× default. Transfer experiments used 10 training environments and tested on out-of-distribution masses (0.7, 0.75 kg).",
            "variation_level": "Varied across experiments: Mass = low variation, SizeMass = medium variation, ShapeSizeMass = high variation; MuJoCo mass variation = medium",
            "performance_metric": "External task reward per trajectory (for downstream tasks), zero-shot generalization performance on unseen environments, convergence speed (training steps to reach performance), and sample efficiency (number of environment interactions). Additionally, intrinsic objective is MDL-based reward (negative information overflow) computed via clustering/Soft-DTW.",
            "performance_value": "Qualitative and relative: causal-curiosity pre-training yields ~2.5× better sample efficiency than conventional supervised planners (paper claim). Example training detail: curiosity pre-training for a lifting skill used 600,000 timesteps. Results reported averaged over 10 seeds; no single absolute reward numbers given in paper, but plots indicate faster convergence and higher zero-shot rewards vs baselines.",
            "complexity_variation_relationship": "Explicitly discussed: (1) The method aims to reduce the 'effective complexity' of observations by finding actions that gate out most causal parents (minimize number of causal factors influencing the observation) using an MDL objective; (2) empirically, as the number of varying causal factors increases, the performance gap (zero-shot generalization) between the causally-curious agent and a Generalist baseline increases, indicating improved robustness to higher variation; (3) trade-off: combining curiosity with external reward (additive objective) leads to competing goals and suboptimal performance; (4) scalability caveat: the one-factor-at-a-time (OFAT)-style recursive scheme scales exponentially with the number of causal factors.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitative: in high-complexity, high-variation setup (ShapeSizeMass: 3 varying factors, 60 training envs) the agent discovered hierarchical binary latent factors and produced disentangled binary representations; causally-curious agents showed better zero-shot generalization and transfer than baselines (no absolute numeric reward reported).",
            "low_complexity_low_variation_performance": "Qualitative: in Mass (1 varying factor, 5 envs) the agent reliably learned semantically meaningful lift/push experiments that perfectly separate low/high mass clusters; these experiments were sample-efficient relative to baseline training.",
            "training_strategy": "Multi-environment training with a recursive OFAT-like tree procedure: use CEM-based MPC to discover short experimental action sequences across multiple training environments, cluster outcomes to learn a binary latent for one causal factor, then recursively partition environment set and repeat. Downstream/transfer policies trained with PPO using causal representations as contextual input.",
            "generalization_tested": true,
            "generalization_results": "Zero-shot generalization tested on unseen combinations and unseen parameter values (e.g., unseen masses 0.7 and 0.75 kg and unseen size/mass combinations). Causally-curious agents significantly outperformed both a Generalist (trained across training envs without causal reps) and a Specialist (trained only on test environments) averaged over 10 seeds; gap increases with more varying causal factors. Qualitatively reported as 'high zero-shot generalizability' and faster adaptation.",
            "sample_efficiency": "Reported approximate improvement: unsupervised experimental behaviors (pre-training with causal curiosity) are ≈2.5× more sample efficient than conventional supervised planners; example: curiosity pre-training used 600,000 timesteps for lifting skill. CEM experiment planner sampling regime: horizon 6, 30 plans sampled per CEM iteration, top 10% used, 10 iterations.",
            "key_findings": "1) An MDL-derived intrinsic reward (causal curiosity) yields self-supervised experimental behaviors that isolate single causal factors and produce interpretable binary latent representations. 2) These learned experiments are semantically meaningful (e.g., lift to test mass, roll to test shape) and are sample-efficient: ≈2.5× less data than conventional planners for acquiring similar behaviors. 3) Causal representations improve zero-shot generalization and transfer to more complex/unseen tasks; the performance gap versus non-causal baselines increases with the number of varying causal factors. 4) Combining curiosity and external reward naively (additive objective) leads to suboptimal trade-offs due to competing goals. 5) The recursive OFAT-style discovery scales poorly (exponential) as number of causal factors grows.",
            "brief_notes_on_limitations": "No absolute numeric task-reward values reported in main text; many claims are relative/qualitative. Scalability to many causal factors is explicitly identified as a limitation (OFAT-style exponential scaling).",
            "uuid": "e1066.0",
            "source_info": {
                "paper_title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "VanillaCEM",
            "name_full": "Vanilla Cross-Entropy Method (CEM) Model Predictive Control baseline",
            "brief_description": "A standard CEM-based MPC planner trained to maximize external task reward (no causal curiosity intrinsic reward), used as a baseline to compare convergence, sample efficiency, and worst-case performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Vanilla CEM planner",
            "agent_description": "Model Predictive Control planner optimized with the Cross-Entropy Method (CEM) that samples candidate action sequences (30 plans per iteration in experiments), applies them to the true environment, and updates the sampling distribution based on top-performing plans to maximize the external task reward.",
            "agent_type": "simulated planner controlling robotic agents in CausalWorld",
            "environment_name": "CausalWorld object-manipulation (Lifting and Travel downstream tasks)",
            "environment_description": "Same simulated single-object manipulation environments used for the causal-curiosity experiments; downstream tasks include Lifting (grasp and lift block to target height) and Travel (impart velocity to a block along a direction). Environment complexity and variation mirror those used in the causal curiosity experiments (varying mass/size/shape depending on setup).",
            "complexity_measure": "Same measures as main experiments (number of varying causal factors and number of training environments). Complexity of the task measured by convergence time and success on downstream tasks.",
            "complexity_level": "Varied depending on experiment (used for direct comparison in Mass/SizeMass/ShapeSizeMass settings); generally compared on same problem instances as the causal-curiosity agent.",
            "variation_measure": "Operates on the same training/test environment distributions (e.g., Mass: 5 envs with masses 0.1-0.5 kg; Transfer setups with 10 training envs and OOD test masses).",
            "variation_level": "Matched to the corresponding causal-curiosity experimental condition (low/medium/high as per Mass/SizeMass/ShapeSizeMass).",
            "performance_metric": "External task reward per trajectory, convergence time (training steps), and worst-case performance across seeds/environments.",
            "performance_value": "Relative qualitative: Vanilla CEM converges slower and is less sample-efficient than the causal-curiosity pre-trained agent; the paper reports that causal-curiosity pre-training achieved similar downstream performance with ≈2.5× less data than conventional planners. No absolute numeric reward values reported for Vanilla CEM in main text.",
            "complexity_variation_relationship": "Used as baseline to show that naive policy optimization across varied environments (domain generalist) struggles when the number of varying causal factors increases — i.e., higher variation and causal complexity degrade generalist performance more than the causally-curious agent.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitative: performed worse than causally-curious agent on high complexity/high variation experiments (ShapeSizeMass), converged slower and showed worse worst-case performance across random seeds.",
            "low_complexity_low_variation_performance": "Qualitative: in simple Mass setting, Vanilla CEM can learn the task but requires more data/time than the curiosity-pretrained agent.",
            "training_strategy": "Single-objective external reward optimization via CEM-MPC on the true environment; no intrinsic curiosity during planner training (identical planner architecture and hyperparameters to the CEM used inside causal-curiosity procedure but optimized for external reward).",
            "generalization_tested": true,
            "generalization_results": "Generalist CEM baseline trained on the same training environments does not generalize as well zero-shot to unseen test environments compared to the causally-curious agent; Specialist baseline (trained only on test envs) performs poorly, indicating test tasks are non-trivial.",
            "sample_efficiency": "Qualitatively lower than causally-curious pre-training; paper reports ≈2.5× worse sample efficiency relative to curiosity-pretraining (no exact CEM-only timesteps reported).",
            "key_findings": "Vanilla CEM baseline is less sample-efficient and generalizes worse to unseen environments with differing causal factors than the causally-curious agent; combining curiosity and external reward (additive objective) with CEM yields suboptimal performance due to conflicting objectives.",
            "uuid": "e1066.1",
            "source_info": {
                "paper_title": "Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causalworld: A robotic manipulation benchmark for causal structure and transfer learning",
            "rating": 2
        },
        {
            "paper_title": "Generalized hidden parameter mdps: Transferable model-based rl in a handful of trials",
            "rating": 2
        },
        {
            "paper_title": "Varibad: A very good method for bayes-adaptive deep rl via meta-learning",
            "rating": 2
        },
        {
            "paper_title": "Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations",
            "rating": 2
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 1
        }
    ],
    "cost": 0.014662749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal Curiosity: RL Agents Discovering Self-supervised Experiments for Causal Representation Learning</h1>
<p>Sumedh A Sontakke ${ }^{1}$ Arash Mehrjou ${ }^{2}$ Laurent Itti ${ }^{1}$ Bernhard Schölkopf ${ }^{2}$</p>
<h4>Abstract</h4>
<p>Animals exhibit an innate ability to learn regularities of the world through interaction. By performing experiments in their environment, they are able to discern the causal factors of variation and infer how they affect the world's dynamics. Inspired by this, we attempt to equip reinforcement learning agents with the ability to perform experiments that facilitate a categorization of the rolled-out trajectories, and to subsequently infer the causal factors of the environment in a hierarchical manner. We introduce causal curiosity, a novel intrinsic reward, and show that it allows our agents to learn optimal sequences of actions and discover causal factors in the dynamics of the environment. The learned behavior allows the agents to infer a binary quantized representation for the ground-truth causal factors in every environment. Additionally, we find that these experimental behaviors are semantically meaningful (e.g., our agents learn to lift blocks to categorize them by weight), and are learnt in a selfsupervised manner with approximately 2.5 times less data than conventional supervised planners. We show that these behaviors can be re-purposed and fine-tuned (e.g., from lifting to pushing or other downstream tasks). Finally, we show that the knowledge of causal factor representations aids zero-shot learning for more complex tasks. Visit here for website.</p>
<h2>1. Introduction</h2>
<p>Discovering causation in environments an agent might encounter remains an open and challenging problem for reinforcement learning (Bengio et al., 2013; Schölkopf, 2015). In physical systems, causal factors such as gravity or friction affect the outcome of behaviors an agent might per-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>form. Thus, there has been recent interest in attempting to train agents to be robust or invariant against varying values of such causal factors, allowing them to learn modular behaviors that are useful across tasks. Most model-based approaches take the form of Bayes Adaptive Markov Decision Processes (BAMDPs) (Zintgraf et al., 2019) or Hidden Parameter MDPs (Hi-Param MDPs) (Doshi-Velez \&amp; Konidaris, 2016; Yao et al., 2018; Killian et al., 2017; Perez et al., 2020) which condition the transition and/or reward function of each environment on hidden parameters.</p>
<p>Formally, let $\mathbf{s} \in \mathcal{S}, \mathbf{a} \in \mathcal{A}, \mathbf{r} \in \mathcal{R}, \mathbf{h} \in \mathcal{H}$ where $\mathcal{S}, \mathcal{A}, \mathcal{R}$, and $\mathcal{H}$ are the set of states, actions, rewards and admissible causal factors, respectively. In the physical world, examples of the parameter $\mathbf{h}<em j="j">{j} \in \mathcal{H}$ might include gravity, coefficients of friction, masses and sizes of objects. Hi-Param MDP or BAMDP approaches treat each $\mathbf{h}</em>} \in \mathcal{H}$ as a latent variable for which an embedding is learnt during training (often using variational methods (Kingma et al., 2014; Ilse et al., 2019)). Let $\mathbf{a<em 0:="0:" T="T">{0: T}$ be a sequence of actions taken by an agent to maximize an external reward resulting in a state trajectory $\mathbf{s}</em>}$. The above approaches define a probability distribution over the entire observable sequence (i.e., rewards, states, actions) as $p\left(\mathbf{r<em 0:="0:" T="T">{0: T}, \mathbf{s}</em>\right)$ which factorizes as}, \mathbf{a}_{0: T-1</p>
<p>$$
\prod_{t=1}^{T-1} p\left(\mathbf{r}<em t="t">{t+1} \mid \mathbf{s}</em>}, \mathbf{a<em t_1="t+1">{t}, \mathbf{s}</em>}, \mathbf{z}\right) p\left(\mathbf{s<em t="t">{t+1} \mid \mathbf{s}</em>}, \mathbf{a<em t="t">{t}, \mathbf{z}\right) p\left(\mathbf{a}</em>\right)
$$} \mid \mathbf{s}_{t}, \mathbf{z</p>
<p>conditioned on the latent variable $\mathbf{z}$, a representation for the unobserved causal factors. At test time, in a new environment, the agent infers $\mathbf{z}$ by observing the trajectories produced by its initial actions issued by the latent conditioned policy obtained during training.</p>
<p>In practice, discovering causal factors in a physical environment is prone to various challenges that are due to the disjointed nature of the influence of these factors on the produced trajectories. More specifically, at each time step, the transition function is affected by a subset of global causal factors. This subset is implicitly defined on the basis of the current state and the action taken. For example, if a body in an environment loses contact with the ground, the coefficient of friction between the body and the ground no longer affects the outcome of any action that is taken. Likewise, the outcome of an upward force applied by the agent to a</p>
<p>body on the ground is unaffected by the friction coefficient.
Without knowledge of how independent causal mechanisms affect the outcome of a particular action in a given state in an environment, it becomes impossible for the agent to conclude where an encountered variation came from. Unsurprisingly, Hi-Param and BAMDP approaches fail to learn a disentangled embedding of the causal factors, making their behaviors uninterpretable. For example, if, in an environment, a body remains stationary under a particular force, the Hi-Param or BAMDP agent may apply a higher force to achieve its goal of perhaps moving the body, but will be unable to conclude whether the "un-movability" was caused by a high friction coefficient, or high mass. Additionally, these approaches require substantial reward engineering, making it difficult to apply them outside the simulated environments they are tested in.</p>
<p>Our goal is, instead of focusing on maximizing reward for a particular task, to allow agents to discover causal processes through exploratory interaction. During training, our agents discover self-supervised experimental behaviors which they apply to a set of training environments. These behaviors allow them to learn about the various causal mechanisms that govern the transitions in each environment. During inference in a novel environment, they perform these discovered behaviors sequentially and use the outcome of each behavior to infer the embedding for a single causal factor (Figure 1), allowing us to recover a disentangled embedding describing the causal factors of an environment.</p>
<p>The main challenge while learning a disentangled representation for the causal factors of the world is that several causal factors may affect the outcome of behaviors in each environment. For example, when pushing a body on the ground, the outcome, i.e., whether the body moves, or how far the body is pushed, depends on several factors, e.g., mass, shape and size, frictional coefficients, etc. However, if, instead of pushing on the ground, the agent executes a perfect grasp-and-lift behavior, only mass will affect whether the body is lifted off the ground or not.</p>
<p>Thus, it is clear that not all experimental behaviors are created equal and that the outcomes of some behaviors are caused by fewer causal factors than others. Our agents learn these behaviors without supervision using causal curiosity, an intrinsic reward. The outcome of a single such experimental behavior is then used to infer a binary quantized embedding describing the single isolated causal factor. While causal factors of variation in a physical world are easily identifiable to humans, a concrete definition is required to formalize our proposed method.
Definition 1 (Causal factors). Consider the POMDP ( $\mathcal{O}$, $\mathcal{S}, \mathcal{A}, \phi, \theta, r$ ) with observation space $\mathcal{O}$, state space $\mathcal{S}$, action space $\mathcal{A}$, the transition function $\phi$, emission function $\theta$, and the reward function $r$. Let $\mathbf{o}<em _="+">{0: T} \in \mathcal{O}^{T}$ denote a trajectory of observations of length $T$. Let $d(\cdot, \cdot): \mathcal{O}^{T} \times$ $\mathcal{O}^{T} \rightarrow \mathbb{R}</em>}$be a distance function defined on the space of trajectories of length $T$. The set $H=\left{\mathbf{h<em 1="1">{0}, \mathbf{h}</em>}, \ldots, \mathbf{h<em j="j">{K-1}\right}$ is called a set of $\epsilon$-causal factors if for every $\mathbf{h}</em>} \in H$, there exists a unique sequence of actions $\mathbf{a<em 1:="1:" m="m">{0: T}$ that clusters the observation trajectories into $m$ disjoint sets $C</em>$, a minimum separation distance of $\epsilon$ is ensured:}$ such that $\forall C_{a}, C_{b</p>
<p>$$
\min \left{d\left(\mathbf{o}<em 0:="0:" T="T">{0: T}, \mathbf{o}</em>}^{\prime}\right): \mathbf{o<em a="a">{0: T} \in C</em>}, \mathbf{o<em b="b">{0: T}^{\prime} \in C</em>\right}&gt;\epsilon
$$</p>
<p>and that $\mathbf{h}_{j}$ is the cause of the obtained trajectory of states i.e. $\forall v \neq v^{\prime}$,</p>
<p>$$
p\left(\mathbf{o}<em j="j">{0: T} \mid d o\left(\mathbf{h}</em>}=v\right), \mathbf{a<em 0:="0:" T="T">{0: T}\right) \neq p\left(\mathbf{o}</em>} \mid d o\left(\mathbf{h<em 0:="0:" T="T">{j}=v^{\prime}\right), \mathbf{a}</em>\right)
$$</p>
<p>where $d o\left(\mathbf{h}<em j="j">{j}\right)$ corresponds to an intervention on the value of the causal factor $\mathbf{h}</em>$.</p>
<p>According to Def. 1, a causal factor $\mathbf{h}<em j="j">{j}$ is a variable in the environment the value of which, when intervened on (i.e., varied) using $d o\left(\mathbf{h}</em>$. These clusters represent the quantized values of the causal factor. For example, mass, which is a causal factor of a body, under an action sequence of a grasping and lifting motion, may result in 2 clusters, liftable (low mass) and not-liftable (high mass).}\right)$ over a set of values, results in trajectories of observations that are divisible into disjoint clusters $C_{1: m}$ under a particular sequence of actions $\mathbf{a}_{0: T</p>
<p>However, such an action sequence is not known in advance. Therefore, discovering a causal factor in the environment boils down to finding a sequence of actions that makes the effect of that factor prominent by producing clustered trajectories for different values of that environmental factor. For simplicity, here we assume binary clusters. For a gentle introduction to the intuition about this definition, we refer the reader to Appendix D. For an introduction to causality and $d o(\cdot)$ notation, see (Pearl, 2009; Spirtes, 2010; Schölkopf, 2019; Elwert, 2013).</p>
<p>Our contributions of our work are as follows:</p>
<ul>
<li>Causal POMDPs: We extend Partially Observable Markov Decision Processes (POMDPs) by explicitly modelling the effect of causal factors on observations.</li>
<li>Unsupervised Behavior: We equip agents with the ability to perform experiments and behave in a semantically meaningful manner in a set of environments in an unsupervised manner. These behaviors can expose or obfuscate specific independent causal mechanisms that occur in the world surrounding the agent, allowing the agent to "experiment" and learn.</li>
<li>Disentangled Representation Learning: We introduce an minimalistic intrinsic reward, causal curiosity, which allows our agents to discover these behaviors</li>
</ul>
<p>without human-engineered complex rewards. The outcomes of the experiments are used to learn a disentangled quantized binary representation for the causal factors of the environment, analogous to the human ability to conclude whether objects are light/heavy, big/small etc.</p>
<ul>
<li>Sample Efficiency: Through extensive experiments, we conclude that knowledge of the causal factors aids sample efficiency in two ways - first, that the knowledge of the causal factors aids transfer learning across multiple environments; second, the learned experimental behaviors can be re-purposed for downstream tasks.</li>
</ul>
<h2>2. Method</h2>
<p>Consider a set of $N$ environments $\mathcal{E}$ with $\mathbf{e}^{i} \in \mathcal{E}$ where $\mathbf{e}^{i}$ denotes the $i^{t h}$ environment. Each causal factor $\mathbf{h}<em j="j">{j} \in H$ is itself a random variable which assumes a particular value for every instantiation of an environment. Thus, every environment $\mathbf{e}^{i}$ is represented with the values assumed by its causal factors $\left{\mathbf{h}</em>}^{i}, j=0,1, \ldots, K-1\right}$. For each environment $\mathbf{e}^{i},\left(\mathbf{z<em 1="1">{0}^{i}, \mathbf{z}</em>}^{i} \ldots \mathbf{z<em j="j">{K-1}^{i}\right)$ represents the disentangled embedding vector corresponding to the physical causal factors where $\mathbf{z}</em>$.}^{i}$ encodes $\mathbf{h}_{j}^{i</p>
<h3>2.1. POMDP Setup</h3>
<h3>2.1.1. Classical POMDPs</h3>
<p>Classical POMDPs $(\mathcal{O}, \mathcal{S}, \mathcal{A}, \phi, \theta, \mathrm{r})$ consist of an observation space $\mathcal{O}$, state space $\mathcal{S}$, action space $\mathcal{A}$, the transition function $\phi$, emission function $\theta$, and the reward function $r$. An agent in an unobserved state $\mathbf{s}<em t="t">{t}$ takes an action $\mathbf{a}</em>}$ and consequently causes a transition in the environment through $\phi\left(\mathbf{s<em t="t">{t+1} \mid \mathbf{s}</em>}, \mathbf{a<em t_1="t+1">{t}\right)$. The agent receives an observation $\mathbf{o}</em>}=\theta\left(\mathbf{s<em t_1="t+1">{t+1}\right)$ and a reward $\mathbf{r}</em>}=r\left(\mathbf{s<em t="t">{t}, \mathbf{a}</em>\right)$.</p>
<h3>2.1.2. CAUSAL POMDPs</h3>
<p>Our work divides the unobserved state $\mathbf{s} \in \mathcal{S}$ at each timestep into two portions - the controllable state $\mathbf{s}^{c}$ and the uncontrollable state $\mathbf{s}^{u}$. The uncontrollable portion of the state $\mathbf{s}^{u}$ consists of the causal factors of the environment. We assume that these remain constant during the interaction of the agent with a single instance of the environment. For example, the value of the gravitational acceleration does not change for a single environment. For the following discussion, we refer to the uncontrollable state as causal factors as in Def 1 i.e., $\mathbf{s}^{u} \equiv \mathcal{H}$.
The controllable state $\mathbf{s}^{c}$ consists of state variables such as positions and orientations of objects, location of endeffectors of the agent etc. Thus, by executing particular action sequences the agent can manipulate this portion of the state, which is hence controllable by the agent.</p>
<h3>2.1.3. Transition Probability</h3>
<p>A trajectory of the controllable state is dependent on both the action sequence that the agent executes and a subset of the causal factors. At each time step, only a subset of the causal factors of an environment affect the transition in the environment. This subset is implicitly selected by the employed policy for every state of the trajectory (depicted as a Gated Causal Graph (Figure 2)). For example, the outcome of an upward force applied by the agent to a body on the ground is unaffected by the friction coefficient between the body and the ground.</p>
<p>Thus the transition function of the controllable state is:</p>
<p>$$
\phi\left(\mathbf{s}<em t="t">{t+1}^{c} \mid \mathbf{s}</em>}^{c}, f_{s e l}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>\right)
$$}\right), \mathbf{a}_{t</p>
<p>where $f_{s e l}$ is the implicit Causal Selector Function which selects the subset of causal factors affecting the transition defined as:</p>
<p>$$
f_{s e l}: \mathcal{H} \times \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{S}(\mathcal{H})
$$</p>
<p>where $\mathbb{S}(\mathcal{H})$ is power-set of $\mathcal{H}$ and $f_{s e l}\left(\mathcal{H}, \mathbf{s}<em t="t">{t}^{c}, \mathbf{a}</em>}\right) \subset \mathcal{H}$ is the set of effective causal factors for the transition $\mathbf{s<em t_1="t+1">{t} \rightarrow \mathbf{s}</em>}$ i.e., $\forall v \neq v^{\prime}$ and $\forall \mathbf{h<em e="e" l="l" s="s">{j} \in f</em>}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>\right)$ :</p>
<p>$$
\phi\left(\mathbf{s}<em j="j">{t+1}^{c} \mid d o\left(\mathbf{h}</em>}=v\right), \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>}\right) \neq \phi\left(\mathbf{s<em j="j">{t+1}^{c} \mid d o\left(\mathbf{h}</em>}=v^{\prime}\right), \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>\right)
$$</p>
<p>where $d o\left(\mathbf{h}<em j="j">{j}\right)$ corresponds to an external intervention on the factor $\mathbf{h}</em>$ in an environment.</p>
<p>Intuitively, this means that if an agent takes an action $\mathbf{a}<em t="t">{t}$ in the controllable state $\mathbf{s}</em>}^{c}$, the transition to $\mathbf{s<em e="e" l="l" s="s">{t+1}^{c}$ is caused by a subset of the causal factors $f</em>}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>}\right)$. For example, if a body on the ground (i.e., state $\mathbf{s<em t="t">{t}^{c}$ ) is thrown upwards (i.e., action $\mathbf{a}</em>}$ ), the outcome $\mathbf{s<em e="e" l="l" s="s">{t+1}$ is caused by the causal factor gravity (i.e., $f</em>}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>$, would be different.}\right)={$ gravity $}$ ), a singleton subset of the global set of causal factors. The $d o()$ notation expresses this causation. If an external intervention on a causal factor is performed, e.g., if somehow the value of gravity was changed from $v$ to $v^{\prime}$, the outcome of throwing the body up from the ground, $\mathbf{s}_{t+1</p>
<h3>2.1.4. Emission Probability</h3>
<p>The agent neither has access to the controllable state, nor to the causal factors of each environment. It receives an observation described by the function:</p>
<p>$$
\mathbf{o}<em t="t">{t+1}=\theta\left(\mathbf{s}</em>}^{c}, f_{s e l}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{c}, \mathbf{a}</em>\right)
$$}\right), \mathbf{a}_{t</p>
<p>where $f_{s e l}$ is the implicit Causal Selector Function.</p>
<h3>2.2. Training the Experiment Planner</h3>
<p>The agent has access to a set of training environments with multiple causal factors varying simultaneously. Our goal</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of Inference. The exploration loop produces a series of $K$ experiments allowing the agent to infer the representations for $K$ causal factors. After exploration, the agent utilizes the acquired knowledge for downstream tasks. The details for the inference procedure are provided in Supplementary Material Algorithm 2.
is to allow the agent to discover action sequences $\mathbf{a}<em 0:="0:" T="T">{0: T}$ such that the resultant observation trajectory $\mathbf{o}</em>}^{i}$ is caused by a single causal factor i.e., $\forall t&lt;T, f_{\text {sel }}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{i}, \mathbf{a}</em>}\right)=$ constant and $\left|f_{\text {sel }}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{i}, \mathbf{a}</em>}\right)\right|=1$. Consequently, $\mathbf{o<em j="j">{0: T}^{i}$ can be used to learn a representation $\mathbf{z}</em>}^{i}$ for the causal factor $f_{\text {sel }}\left(\mathcal{H}, \mathbf{s<em t="t">{t}^{i}, \mathbf{a}</em>$.
}\right)$ for each environment $\mathbf{e}^{i<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Gated Causal Graph. A subset of the unobserved parent causal variables influence the observed variable $\mathbf{O}$. The action sequence $\mathbf{a}_{0: T}$ serves a gating mechanism, allowing or blocking particular edges of the causal graph using the implicit Causal Selector Function (Equation 4).</p>
<p>We motivate this from the perspective of algorithmic information theory (Janzing \&amp; Schölkopf, 2010). Consider the Gated Directed Acyclic Graph of the observed variable $\mathbf{O}$ and its causal parents (Figure 2). Each causal factor has its own causal mechanism, jointly bringing about $\mathbf{O}$. A central assumption of our approach is that causal factors are independent, i.e., the Independent Mechanisms Assumption (Schölkopf et al., 2012; Parascandolo et al., 2018; Schölkopf, 2019). The information in $\mathbf{O}$ is then the sum of information "injected" into it from the multiple causes, since, loosely speaking, for information to cancel, the mechanisms would need to be algorithmically dependent (Janzing \&amp; Schölkopf, 2010). Intuitively, the information content in $\mathbf{O}$ will be greater for a larger number of causal parents in the graph. Interestingly, a similar argument has been made to justify the thermodynamic arrow of time (Chaves et al., 2014; Janzing
et al., 2016): while a microscopic time evolution is invertible, the assumption of algorithmic independence for initial conditions and forward mechanisms generates an asymmetry. To invert time, the backward mechanism would need to depend on the initial state.</p>
<p>Thus we attempt to find an action sequence $\mathbf{a}_{0: T}$ for which the number of causal parents of the resultant observation $\mathbf{O}$ is low, i.e., the complexity of $\mathbf{O}$ is low. One could conceive of this by assuming that the generative model for $\mathbf{O}, \mathbf{M}$ has low Kolmogorov Complexity. Here, a low capacity bi-modal model is assumed. We utilize Minimum Description Length $L(\cdot)$ (MDL) as a tractable substitute of the Kolmogorov Complexity (Rissanen, 1978; Grunwald, 2004)).</p>
<p>Causal curiosity solves the following optimization problem.</p>
<p>$$
\mathbf{a}<em 0:="0:" T="T">{0: T}^{*}=\underset{\mathbf{a}</em>))
$$}}{\arg \min }(L(\mathbf{M})+L(\mathbf{O} \mid \mathbf{M</p>
<p>where each observed trajectory $\mathbf{O}=\mathbf{O}\left(\mathbf{a}_{0: T}\right)$ is a function of the action sequence. As mentioned earlier, the model is fixed in this formulation; hence, the first term $L(\mathbf{M})$ is constant and not a function of the actions. The MDL of the trajectories given binary categorization model, $-L(\mathbf{O} \mid \mathbf{M})$, is the inherent reward function that is fed back to the RL agent. We regard this reward function as causal curiosity. See Supplementary Material A for implementation details.</p>
<h3>2.3. Causal Inference Module</h3>
<p>By maximizing the causal curiosity reward it is possible to achieve behaviors which result in trajectories of states only caused by a single hidden parameter. Subsequently, we utilize the outcome of performing these experimental behaviors in each environment to infer a representation for the causal factor isolated by the experiment in question.</p>
<p>We achieve this through clustering. An action sequence $\mathbf{a}<em 0:="0:" j-1="j-1">{0: T} \sim \operatorname{CEM}\left(\cdot \mid \mathbf{z}</em>\right)$ is sampled from the Model Predictive Control Planner (Camacho \&amp; Alba, 2013) and applied to each of the training environments. The learnt clustering}^{i</p>
<div class="codehilite"><pre><span></span><code>Algorithm <span class="mi">1</span> Recursive Training Scheme
    Initialize <span class="err">\</span><span class="p">(</span><span class="ss">j</span><span class="o">=</span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
    Initialize training environment set Envs
    function <span class="err">\</span><span class="p">(</span><span class="err">\</span>operatorname<span class="p">{</span>Train<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>j<span class="p">,</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="o">.</span><span class="err">\</span><span class="p">),</span> Envs <span class="err">\</span><span class="p">()</span><span class="err">\</span><span class="p">)</span>
        <span class="k">if</span> <span class="err">\</span><span class="p">(</span><span class="ss">j</span><span class="o">==</span>K<span class="err">\</span><span class="p">)</span> <span class="k">then</span>
            Return
        end <span class="k">if</span>
        for iteration m to M do
            Sample experimental behavior <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>a<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> T<span class="p">}</span> <span class="err">\</span>sim <span class="err">\</span>operatorname<span class="p">{</span>CEM<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>cdot <span class="err">\</span>mid <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
            for <span class="err">\</span><span class="p">(</span>i<span class="err">^</span><span class="p">{</span>t h<span class="p">}</span><span class="err">\</span><span class="p">)</span> env <span class="k">in</span> Envs do
                Apply <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>a<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> T<span class="p">}</span><span class="err">\</span><span class="p">)</span> to env
                Collect <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>O<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="o">=</span><span class="err">\</span>mathbf<span class="p">{</span>o<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> T<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span><span class="p">)</span>
                Reset env
            end for
            Calculate <span class="err">\</span><span class="p">(</span><span class="o">-</span>L<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>O<span class="p">}</span> <span class="err">\</span>mid <span class="err">\</span>mathbf<span class="p">{</span>M<span class="p">})</span><span class="err">\</span><span class="p">)</span> given that <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>M<span class="p">}</span><span class="err">\</span><span class="p">)</span> is bimodal clus-
            tering model
            Update CEM<span class="p">(</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>cdot<span class="p">)</span><span class="err">\</span><span class="p">)</span> distribution <span class="k">with</span> highest reward tra-
            jectories
            end for
            Use learnt <span class="err">\</span><span class="p">(</span>q_<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span>j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span> <span class="err">\</span>mid <span class="err">\</span>mathbf<span class="p">{</span>O<span class="p">},</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span> for cluster assignment of
            each env <span class="k">in</span> Envs i<span class="o">.</span>e<span class="o">.</span> <span class="err">\</span><span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span>j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="o">=</span>q_<span class="p">{</span>M<span class="p">}</span><span class="err">\</span>left<span class="p">(</span><span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span> <span class="err">\</span>mid <span class="err">\</span>mathbf<span class="p">{</span>O<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">},</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
            Update <span class="err">\</span><span class="p">(</span><span class="ss">j</span><span class="o">=</span>j<span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
            <span class="err">\</span><span class="p">(</span><span class="err">\</span>operatorname<span class="p">{</span>Train<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>j<span class="p">,</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="o">.</span><span class="err">\</span><span class="p">),</span> Envs <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="o">.=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>mathbf<span class="p">{</span>e<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}:</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span>j-1<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="o">=</span><span class="mi">0</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
            <span class="err">\</span><span class="p">(</span><span class="err">\</span>operatorname<span class="p">{</span>Train<span class="p">}</span><span class="err">\</span>left<span class="p">(</span>j<span class="p">,</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span><span class="mi">0</span><span class="p">:</span> j<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="err">\</span>right<span class="o">.</span><span class="err">\</span><span class="p">),</span> Envs <span class="err">\</span><span class="p">(</span><span class="err">\</span>left<span class="o">.=</span><span class="err">\</span>left<span class="err">\</span><span class="p">{</span><span class="err">\</span>mathbf<span class="p">{</span>e<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}:</span> <span class="err">\</span>mathbf<span class="p">{</span>z<span class="p">}</span>_<span class="p">{</span>j-1<span class="p">}</span><span class="err">^</span><span class="p">{</span>i<span class="p">}</span><span class="o">=</span><span class="mi">1</span><span class="err">\</span>right<span class="err">\</span><span class="p">}</span><span class="err">\</span>right<span class="p">)</span><span class="err">\</span><span class="p">)</span>
    end function
</code></pre></div>

<p>model $\mathbf{M}$ is then used to infer a representation for each environment using the collected outcome $\mathbf{O}^{i}$ obtained by applying $\mathbf{a}_{0: T}$ to each environment.</p>
<p>$$
\mathbf{z}<em M="M">{j}^{i}=q</em>\right)
$$}\left(\mathbf{z} \mid \mathbf{O}^{i}, \mathbf{z}_{0: j-1}^{i</p>
<p>The learnt representation $\mathbf{z}$ is the cluster membership obtained from the learnt clustering model $\mathbf{M}$. It is binary in nature. This corresponds to the quantization of the continuous spectrum of values a causal factor takes in the training set into high and low values.</p>
<h3>2.4. Interventions on beliefs</h3>
<p>Having learnt about the effects of a single causal factor of the environment we wish to learn such experimental behaviors for each of the remaining hidden parameters that may vary in the training environments. To achieve this, in an ideal setting, the agent would require access to the generative mechanism of the environments it encounters. Ideally, it would hold the values of the causal factor already learnt about a constant i.e. $d o\left(\mathbf{h}<em j_prime="j^{\prime">{j}=\right.$ constant $)$, and intervene over (vary the value of) another causal factor over a set of values $V$ i.e. $d o\left(\mathbf{h}</em>=v\right)$ such that $v \in V$. For example, if a human scientist were to study the effects of a causal factor, say mass of a body, they would hold the values of all other causal factors constant (e.g., interact with cubes of the same
size and external texture), and vary only mass to see how it affects the outcome of specific behaviors they apply to each body.}</p>
<p>However, in the real world the agent does not have access to the generative mechanism of the environments it encounters, but merely has the ability to act in them. Thus, it can intervene on the representations of a causal factor of the environment i.e. $d o\left(\mathbf{z}_{i}=\right.$ constant $)$. For, example having learnt about gravity, the agent picks all environments it believes have the same gravity, and uses them to learn about a separate causal factor say, friction.</p>
<p>Thus, to learn about the $j^{\text {th }}$ causal factor, the agent proceeds in a tree-like manner, dividing each of the $2^{j-1}$ clusters of training environments into two sub-clusters corresponding to the binary quantized values of the $j^{\text {th }}$ causal factor. Each level of this tree corresponds to a single causal factor.</p>
<p>$$
\text { Envs }=\left{\mathbf{e}^{i}: \mathbf{z}_{j-1}^{i}=k\right}, k \in{0,1}
$$</p>
<p>This process continues iteratively (Algorithm 1 and Figure 3), where for each cluster of environments, a new experiment learns to split the cluster into 2 sub-clusters depending on the value of a hidden parameter. At level $n$, the agent produces $2^{n}$ experiments, having already intervened on the binary quantized representations of $n$ causal factors.</p>
<h2>3. Related Work</h2>
<p>Curiosity for robotics is not a new area of research. Pioneered by Schmidhuber in the 1990s, (Schmidhuber, 1991b;a; 2006; 2010), (Ngo et al., 2012), (Pathak et al., 2017) curiosity is described as the motivation behind the behavior of an agent in an environment for which the outcome is unpredictable, i.e., an intrinsic reward that motivates the agent to explore the unseen portions of the state space (and subsequent transitions). (Doshi-Velez \&amp; Konidaris, 2016) define a class Markov Decision Processes where transition probabilities $p\left(s_{t+1} \mid s_{t}, a_{t} ; \theta\right)$ depend on a hidden parameter $\theta$, whose value is not observed, but its effects are felt. (Killian et al., 2017) and (Yao et al., 2018) utilize these Hidden Parameter MDPs (Markov Decision Processes) to enable efficient policy transfer, assuming that transition probabilities across states are a function of hidden parameters. (Perez et al., 2020) relax this assumption, allowing both transition probabilities and reward functions to be functions of hidden parameters. (Zintgraf et al., 2019) approach the problem from a Bayes-optimal policy standpoint, defining transition probabilities and reward functions to be dependent on a hidden parameter characteristic of the MDP in consideration. We utilize this setup to define causal factors.
Substantial attempts have been made at unsupervised disentanglement, most notably, the $\beta$-VAE (Higgins et al.) (Burgess et al., 2018), where a combination of factored priors and the information bottleneck force disentangled</p>
<p>representations. (Kim \&amp; Mnih, 2018) enforce explicit factorization of the prior without compromising on the mutual information between the data and latent variables, a shortcoming of the $\beta$-VAE. (Chen et al., 2018) factor the KL divergence into a more explicit form, highlighting an improved objective function and a classifier-agnostic disentanglement metric. (Locatello et al., 2018) show theoretically that unsupervised disentanglement (in the absence of inductive biases) is impossible and highly unstable, susceptible to random seed values. They follow this up with (Locatello et al., 2020) where they show, both theoretically and experimentally, that pair-wise images provide sufficient inductive bias to disentangle causal factors of variation. However, these works have been applied to supervised learning problems whereas we attempt to disentangle the effects of hidden variables in dynamical environments, a relatively untouched question.</p>
<h2>4. Experiments</h2>
<p>Our work has 2 main thrusts - the discovered experimental behaviors and the representations obtained from the outcome of the behaviors in environments. We visualize these learnt behaviors and verify that they are indeed semantically meaningful and interpretable. We quantify the utility of the learned behaviors by using the behaviors as pre-training for a downstream task. In our experimental setup, we verify that these behaviors are indeed invariant to all other causal factors except one.
We visualize the representations obtained using these behaviors and verify that they are indeed the binary quantized representations for each of the ground truth causal factors that we manipulated in our experiments. Finally, we verify that the knowledge of the representation does indeed aid transfer learning and zero-shot generalizability in downstream tasks.</p>
<p>Causal World. We use the Causal World Simulation (Ahmed et al., 2020) based on the Pybullet Physics engine to test our approach. The simulator consists of a 3-fingered robot, with 3 joints on each finger. We constrain each environment to consist of a single object that the agent can interact with. The causal factors that we manipulate for each of the objects are size, shape and mass of the blocks. The simulator allows us to capture and track the positions and velocities of each of the movable objects in an environment.</p>
<p>Mujoco Control Suite. We optimize causal curiosity on 4 articulated agents that try to learn locomotion - Ant, HalfCheetah, Hopper, and Walker. For each agent type, we train with agent body masses from $0.5 \times$ to $1.5 \times$ the default.</p>
<h3>4.1. Visualizing Discovered Behaviors</h3>
<p>We would like to analyze whether the discovered experimental behaviors are human interpretable, i.e., are the experimental behaviors discovered in each of the setups semantically meaningful? We find that our agents learn to perform several useful behaviors without any supervision. For instance, to differentiate between objects with varying mass, we find that they acquire a perfect grasp-and-lift behavior with an upward force. In other random seed experiments, the agents learn to lift the blocks by using the wall of the environment for support. To differentiate between cubes and spheres, the agent discovers a pushing behavior which gently rolls the spheres along a horizontal direction. Qualitatively, we find that these behaviors are stable and predictable. See videos of discovered behaviors here (website under construction).</p>
<p>Concurrent with the objective they are trained on, we find that the acquired behaviors impose structure on the outcome when applied to each of the training environments. The outcome of each experimental behavior on the set of training environments results in dividing it into 2 subsets corresponding to the binary quantized values of a single factor, e.g., large or small, while being invariant to the values of other causal factors of the environments. We also perform ablation studies where instead of providing the full state vector, we provide only one coordinate (e.g., only x, y or z coordinate of the block). We find that causal curiosity results in behaviors that differentiate the environments based on outcomes along the direction provided. For example, when only the x coordinate was provided, the agent learned to evaluate mass by applying a pushing behavior along the x direction. Similarly, a lifting behavior was obtained when only the z coordinate was supplied to the curiosity module (Figure 4).</p>
<p>Causal curiosity also yields semantically meaningful behaviors that test out agent mass in Mujoco: the Half-Cheetah learns a front-flip, the Hopper learns to hop to gauge its own mass, in the absence of external rewards (Fig 7).</p>
<h3>4.2. Utility of learned behaviors for downstream tasks</h3>
<p>While the behaviors acquired are semantically meaningful, we would like to quantify their utility as pre-training for downstream tasks. We analyze the performance on Lifting where the agent must grasp and lift a block to a predetermined height and Travel, where the agent must impart a velocity to the block along a predetermined direction. We re-train the learnt planner using an external reward for these tasks (Curious). We implement a baseline vanilla Cross Entropy Method optimized Model Predictive Control Planner (De Boer et al., 2005) (Vanilla CEM) trained using the identical reward function and compare the rewards per trajectory during training. We also run a baseline (Additive</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Discovered hierarchical latent space. The agent learns experiments that differentiate the full set of blocks in ShapeSizeMass into hierarchical binary clusters. At each level, the environments are divided into 2 clusters on the basis of the value of a single causal factor. We also show the principal components of the trajectories in the top left. For brevity, the full of extent of the tree is not depicted here. For each level of hierarchy $k$, there are $2^{k}$ number of clusters.
reward) which explores whether the agent recieves both the causal curiosity reward and the external reward. We find high zero-shot generalizability and quicker convergence as compared to the vanilla CEM (Figure 5). We also find that additive rewards, achieves suboptimal performance due to competing objectives. For details, we refer the reader to the Supplementary Material C.</p>
<h3>4.3. Visualization of hierarchical binary latent space</h3>
<p>Our agents discover a disentangled latent space such that they are able to isolate the sources of causation of the variability they encounters in their environments. For every environment, they learn a disentangled embedding vector which describes each of the causal factors.</p>
<p>To show this, we use 3 separate experimental setups Mass, SizeMass and ShapeSizeMass where each of the causal factors are allowed to vary over a range of discrete values. For details of the setup, we refer the reader to Supplementary Material A.2.</p>
<p>During training, the agent discovers a hierarchical binary latent space (Figure 3), where each level of hierarchy corresponds to a single causal factor. The binary values at each level of hierarchy correspond to the high/low values of the causal factor in question. To our knowledge, we obtain the first interpretable latent space describing the various causal processes in the environment of an agent. This implies that
it learns to quantify each physical attribute of the blocks it encounters in a completely unsupervised manner.</p>
<h3>4.4. Knowledge of causal factors aids transfer</h3>
<p>Next, we test whether knowledge of the causal factors does indeed aid transfer and zero-shot generalizability. To this end, we supply the representations obtained by the agent during the experimental behavior phase as input to a policy network in addition to the state of the simulator, and train it for a place-and-orient downstream task (Figure 1). We define 2 experimental setups - TransferMass and TransferSizeMass where mass and size of the object in each environment is varied. We also test our agent in a separate task, StackingTower, where the agent is provided 2 blocks which it must use to build a stable tower configuration. These blocks vary in mass and the agent must use causal representations to learn to build towers with a heavy base for stability. In each of the setups, the agent learns about the varying causal mechanisms by optimizing causal curiosity. Subsequently, using the causal representation along with the state for each environment, it is trained to maximize external reward. For details of the setup, please see Supplementary Material B.</p>
<p>After training, the agents are exposed to a set of unseen test environments, where we analyze their zero-shot generalizability. These test environments consist of unseen masses</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Examples of discovered behaviors. The agent discovers experimental behaviors that allow it to characterize each environmental object in a binary manner, e.g., heavy/light, big small, rollable/not rollable, etc. These behaviors are acquired without any external supervision by maximizing the causal curiosity reward. A, B, C correspond to self-discovered toss, lift-and-spin and roll behaviors respectively. D shows an ablation study, where the agent is only provided the z coordinate of the block in every environment. Each line corresponds to one environment and the z coordinate of the block is plotted with time when the discovered behavior is applied. It learns a lifting behavior, where cluster 1 represents the heavy blocks (z coordinate does not change much) and cluster 2 represents the light blocks (z increases as block is lifted and then falls when dropped and subsequently increases again when it bounces).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Utility of discovered behaviors. We find that the behaviors discovered by the agents while optimizing causal curiosity show high zero-shot generalizability and converge to the same performance as conventional planners for downstream tasks. We also analyze the worst case performance and find that the pre-training ensures better performance than random initialization. The table compares the time-steps of training required on an average to acquire a skill with the time steps required to learn a similar behavior using external reward. We find that the unsupervised experimental behaviors are approximately 2.5 times more sample efficient. We also find that maxizing both curiosity and external reward in our experimental setups results in sub-optimal results.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Knowledge of causal factors aids transfer. We find that knowledge of the causal representation allows agents to generalize to unseen environments with high zero-shot performance. We find that as the number of varying causal factors increase, the difference in zero-shot performance of the Causally-curious agent and the Generalist increases, showing that the CC agents are indeed robust to multiple varying causal factors. Panes $\mathbf{A}$ and $\mathbf{B}$ are represent reward curves for TransferMass and TransferSizeMass. Pane $\mathbf{C}$ shows the training rewards curve for the new StackingTower experiment and Pane D shows the reward curves during adaptation to an unseen value of causal factors.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Mujoco Experiments. Pane A shows discovered experimental behaviors on Half-Cheetah and Hopper, while Pane B shows discovered experimental behaviors on Ant and Walker. The Half-Cheetah learns to front-flip, the Hopper learns to hop, and the Ant learns to rear up on its hind legs to gauge its mass.
and sizes and their unseen combinations. This corresponds to "Strong Generalization" as defined by (Perez et al., 2020). We report results averaged over 10 random seeds.</p>
<p>For each setup, we train a PPO-optimized Actor-Critic Policy (referred to as Causally-curious agent) with access to the causal representations and an observation vector from the environment i.e., $\mathbf{a}<em t="t">{t} \sim \pi\left(\cdot \mid \mathbf{o}</em>\right)$. Similar to (Perez et al., 2020), we implement 2 baselines - the Generalist and the Specialist. The Specialist consists of an agent with identical architecture as Causally-curious agent, but without access to causal representations. It is initialized randomly and is trained only on the test environments, serving as a benchmark for complexity of the test tasks. It performs poorly, indicating that the test tasks are complex. The architecture of the Generalist is identical to the Specialist. Like the Specialist, the Generalist also does not have access to the causal representations, but is trained on the same set of training environments that the Causally-curious agent is trained on. The poor performance of the generalist indicates that the tasks distribution of training and test tasks differs significantly and that memorization of behaviors does not yield good transfer. We find that causally-curious agents significantly outperform the both baselines indicating that indeed, knowledge of the causal representation does aid zero-shot generalizability.}, \mathbf{z}_{0: K</p>
<h2>5. Conclusion</h2>
<p>Our work introduces a causal viewpoint of POMDPs, where unobserved static state variables (i.e., causal factors) affect the transition of dynamic state variables. Causal curiosity rewards experimental behaviors an agent can conduct in an environment that underscore the effects of a subset of such global causal factors while obfuscating the effects of others. Motivated by the popular One-Factor-at-a-Time (OFAT) (Fisher, 1936; Hicks, 1964; Czitrom, 1999), our agents study the effects causal factors have on the dynamics of an environment through active experimentation and subsequently obtain a disentangled causal representation for
causal factors of the environment. We discuss the implication of OFAT in Supplementary Material E. Finally, we show that knowledge of causal representations does indeed improve sample efficiency in transfer learning.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by C-BRIC (one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA), the Army Research Office (W911NF2020053), and the Intel and CISCO Corporations. The authors affirm that the views expressed herein are solely their own, and do not represent the views of the United States government or any agency thereof. SAS was partly funded by the Annenberg Fellowship. Many thanks also to Alexander Neitz for sourcing of the CEM planning code. SAS would like to thank Stefan Bauer, Theofanis Karaletsos, Manuel Wüthrich, Francesco Locatello, Ossama Ahmed, Frederik Träuble, and everyone at MPI for useful discussions.</p>
<h2>References</h2>
<p>Ahmed, O., Träuble, F., Goyal, A., Neitz, A., Wüthrich, M., Bengio, Y., Schölkopf, B., and Bauer, S. Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296, 2020.</p>
<p>Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8): $1798-1828,2013$.</p>
<p>Burgess, C. P., Higgins, I., Pal, A., Matthey, L., Watters, N., Desjardins, G., and Lerchner, A. Understanding disentangling in $\beta$-vae. arxiv 2018. arXiv preprint arXiv:1804.03599, 2018.</p>
<p>Camacho, E. F. and Alba, C. B. Model predictive control. Springer Science \&amp; Business Media, 2013.</p>
<p>Chaves, R., Luft, L., Maciel, T., Gross, D., Janzing, D., and Schölkopf, B. Inferring latent structures via information inequalities. In Zhang, N. L. and Tian, J. (eds.), Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence, pp. 112-121, Corvallis, OR, 2014. AUAI Press.</p>
<p>Chen, T. Q., Li, X., Grosse, R. B., and Duvenaud, D. K. Isolating sources of disentanglement in variational autoencoders. In Advances in Neural Information Processing Systems, pp. 2610-2620, 2018.</p>
<p>Cuturi, M. and Blondel, M. Soft-dtw: a differentiable loss function for time-series. arXiv preprint arXiv:1703.01541, 2017.</p>
<p>Czitrom, V. One-factor-at-a-time versus designed experiments. The American Statistician, 53(2):126-131, 1999.</p>
<p>De Boer, P.-T., Kroese, D. P., Mannor, S., and Rubinstein, R. Y. A tutorial on the cross-entropy method. Annals of operations research, 134(1):19-67, 2005.</p>
<p>Doshi-Velez, F. and Konidaris, G. Hidden parameter markov decision processes: A semiparametric regression approach for discovering latent task parametrizations. In IJCAI: proceedings of the conference, volume 2016, pp. 1432. NIH Public Access, 2016.</p>
<p>Elwert, F. Graphical causal models. In Handbook of causal analysis for social research, pp. 245-273. Springer, 2013.</p>
<p>Fisher, R. A. Design of experiments. Br Med J, 1(3923): $554-554,1936$.</p>
<p>Grunwald, P. A tutorial introduction to the minimum description length principle. arXiv preprint math/0406077, 2004.</p>
<p>Hicks, C. R. Fundamental concepts in the design of experiments. 1964.</p>
<p>Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. betavae: Learning basic visual concepts with a constrained variational framework.</p>
<p>Hill, A., Raffin, A., Ernestus, M., Gleave, A., Kanervisto, A., Traore, R., Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., and Wu, Y. Stable baselines. https://github.com/ hill-a/stable-baselines, 2018.</p>
<p>Ilse, M., Tomczak, J. M., Louizos, C., and Welling, M. Diva: Domain invariant variational autoencoders. arXiv preprint arXiv:1905.10427, 2019.</p>
<p>Janzing, D. and Schölkopf, B. Causal inference using the algorithmic Markov condition. IEEE Transactions on Information Theory, 56(10):5168-5194, 2010.</p>
<p>Janzing, D., Chaves, R., and Schölkopf, B. Algorithmic independence of initial condition and dynamical law in thermodynamics and causal inference. New Journal of Physics, 18(9):093052, 2016.</p>
<p>Killian, T. W., Daulton, S., Konidaris, G., and Doshi-Velez, F. Robust and efficient transfer learning with hidden parameter markov decision processes. In Advances in neural information processing systems, pp. 6250-6261, 2017.</p>
<p>Kim, H. and Mnih, A. Disentangling by factorising. arXiv preprint arXiv:1802.05983, 2018.</p>
<p>Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. Semi-supervised learning with deep generative models. In Advances in neural information processing systems, pp. 3581-3589, 2014.</p>
<p>Locatello, F., Bauer, S., Lucic, M., Rätsch, G., Gelly, S., Schölkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. arXiv preprint arXiv:1811.12359, 2018.</p>
<p>Locatello, F., Poole, B., Rätsch, G., Schölkopf, B., Bachem, O., and Tschannen, M. Weakly-supervised disentanglement without compromises. arXiv preprint arXiv:2002.02886, 2020.</p>
<p>Ngo, H., Luciw, M., Forster, A., and Schmidhuber, J. Learning skills from play: artificial curiosity on a katana robot arm. In The 2012 international joint conference on neural networks (IJCNN), pp. 1-8. IEEE, 2012.</p>
<p>Parascandolo, G., Kilbertus, N., Rojas-Carulla, M., and Schölkopf, B. Learning independent causal mechanisms. In International Conference on Machine Learning, pp. 4036-4044. PMLR, 2018.</p>
<p>Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 16-17, 2017.</p>
<p>Pearl, J. Causality. Cambridge university press, 2009.
Perez, C. F., Such, F. P., and Karaletsos, T. Generalized hidden parameter mdps: Transferable model-based rl in a handful of trials. AAAI Conference On Artifical Intelligence, 2020.</p>
<p>Rissanen, J. Modeling by shortest data description. Automatica, 14(5):465-471, 1978.</p>
<p>Rousseeuw, P. J. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of computational and applied mathematics, 20:53-65, 1987.</p>
<p>Schmidhuber, J. Curious model-building control systems. In Proc. international joint conference on neural networks, pp. 1458-1463, 1991a.</p>
<p>Schmidhuber, J. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pp. 222227, 1991b.</p>
<p>Schmidhuber, J. Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. Connection Science, 18(2):173-187, 2006.</p>
<p>Schmidhuber, J. Formal theory of creativity, fun, and intrinsic motivation (1990-2010). IEEE Transactions on Autonomous Mental Development, 2(3):230-247, 2010.</p>
<p>Schölkopf, B. Artificial intelligence: Learning to see and act (News \&amp; Views). Nature, 518(7540):486-487, 2015.</p>
<p>Schölkopf, B. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.</p>
<p>Schölkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and Mooij, J. M. On causal and anticausal learning. In Proceedings of the 29th International Conference on Machine Learning (ICML), pp. 1255-1262, 2012.</p>
<p>Spirtes, P. Introduction to causal inference. Journal of Machine Learning Research, 11(5), 2010.</p>
<p>Yao, J., Killian, T., Konidaris, G., and Doshi-Velez, F. Direct policy transfer via hidden parameter markov decision processes. In LLARLA Workshop, FAIM, volume 2018, 2018.</p>
<p>Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and Whiteson, S. Varibad: A very good method for bayes-adaptive deep rl via meta-learning. arXiv preprint arXiv:1910.08348, 2019.</p>
<h2>A. Implementation Details for Experiment Discovery</h2>
<h2>A.1. Planner</h2>
<p>The Experiment Planner consisted of a uniform distribution planner for a horizon of 6 control signals. The planner was trained using the Cross Entropy Method Model Predictive Control (Camacho \&amp; Alba, 2013; De Boer et al., 2005) on the true environment. We sampled 30 plans per iteration from the distribution initialized to uniform $\mathcal{U}($ controlLow, controlHigh). Each of the sampled plans are applied to each of the training environments and the top $10 \%$ of the plans are used to update the distribution. The CEM training required 10 iterations.</p>
<h2>A.2. Training Environments</h2>
<p>The training environments vary in each experiment. In Section 4.3, we utilize 3 setups, Mass, SizeMass and ShapeSizeMass. For Mass, we allow the agent to access 5 environments with masses varying from 0.1 kg to 0.5 kg . In SizeMass, the agent has access to 30 environments with masses varying uniformly from 0.1 to 0.5 kg and sizes from 0.05 to 0.1 meters. Finally, in ShapeSizeMass, the agent has access to 60 environments, with masses varying uniformly from 0.1 to 0.5 kg , sizes from 0.05 to 0.1 meters and shapes either being cubes or spheres. During experiment discovery, in each environment, the agent has access to the position of the block in the environment along with its quaternion orientation.</p>
<p>The total number of causal causal factors of each environment are rather large in number due to the fact that the simulator is a complex realistic physics engine. Examples of the causal factors in the environment include gravity, friction coefficients between all on interacting surfaces, shapes, sizes and masses of blocks, control signal frequencies of the environment. However, we only vary 1 during Mass, 2 during SizeMass and 3 during ShapeSizeMass.</p>
<h2>A.3. Curiosity Reward Calculation</h2>
<p>We predetermine the minimum description length of the clustering model $L(\mathbf{M})$ by assuming that the observations $\mathbf{o}<em 0:="0:" T="T">{0: T}$, obtained by applying experimental behavior $\mathbf{a}</em>)$ is as small as possible. The planner, eq. (7) solves the following opti-
mization problem:}$ are produced by a bi-modal generator distribution, where each mode corresponds to either a low or high (quantized) value of a causal factor. This also ensures that $L(\mathbf{M</p>
<p>$$
\begin{aligned}
\underset{\mathbf{a}<em 0:="0:" T="T">{0: T} \in \mathcal{A}^{T}}{\arg \max } &amp; {\left[\min \left{d\left(\mathbf{o}</em>}, \mathbf{o}^{\prime}{ <em 0:="0:" T="T">{0: T}\right): \mathbf{o}</em>} \in C_{1}, \mathbf{o}^{\prime}{ <em 2="2">{0: T} \in C</em> \
&amp; \max \left{d\left(\mathbf{o}}\right}-\right.<em 0:="0:" T="T">{0: T}, \mathbf{o}^{\prime \prime}{ }</em>}\right): \mathbf{o}^{\prime \prime}{ <em 0:="0:" T="T">{0: T}, \mathbf{o}</em>\right}- \
&amp; \left.\max \left{d\left(\mathbf{o}} \in C_{1<em 0:="0:" T="T">{0: T}^{\prime}, \mathbf{o}^{\prime \prime \prime}{ }</em>}\right): \mathbf{o<em 0:="0:" T="T">{0: T}^{\prime}, \mathbf{o}^{\prime \prime \prime}{ }</em>\right}\right]
\end{aligned}
$$} \in C_{2</p>
<p>the distance function $d(\cdot, \cdot)$ in the space of trajectories is set to be Soft Dynamic Time Warping (Cuturi \&amp; Blondel, 2017). The trajectory length $T$ is 6 control steps long. The objective is a modified version of the Silhouette Score (Rousseeuw, 1987).</p>
<p>Intuitively, Objective (10) expresses the ability of a low complexity model, assumed to be bi-modal, to encode the state $\mathbf{O}=\mathbf{o}<em 1="1">{0: T}$. If multiple causal factors control $\mathbf{O}$, then the Minimum Description Length of $L(\mathbf{O})$ will be high. Subsequently, since $\mathbf{M}$ is a simple model, the deviation of $\mathbf{O}$ from $\mathbf{M}$ will be high i.e. $L(\mathbf{O} \mid \mathbf{M})$ will be high resulting in a low value of the optimization objective. $C</em>}$ and $C_{2}$ correspond to clusters of outcomes which quantize the values of a causal factor isolated by $\mathbf{a<em 0:="0:" T="T">{0: T} . \mathbf{o}</em>}, \mathbf{o}^{\prime \prime}{ <em 1="1">{0: T} \in C</em>}$ correspond to trajectories of states i.e. observations obtained by applying $\mathbf{a<em 0:="0:" T="T">{0: T}$ to environments with say, low values of a causal factor while $\mathbf{o}^{\prime}{ }</em>}, \mathbf{o}^{\prime \prime \prime}{ <em 2="2">{0: T} \in C</em>$ well.}$ correspond to trajectories of observations i.e. state obtained by applying $\mathbf{a}_{0: T}$ to environments with say, high values of the same causal factor. Objective (10) attempts to ensure that these clusters are far apart from each other and are tight i.e. a simple model $\mathbf{M}$ encodes $\mathbf{O</p>
<h2>B. Implementation Details for Transfer</h2>
<p>In Section 4.4, we show the utility of learning causal representations in 2 separate experimental setups. During TransferMass, the agent has access to 10 environments during training, with masses ranging from 0.1 to 0.5 kg . At test time, the agent is required to perform the place-andorient task masses 2 masses - 0.7 kg and 0.75 kg . During TransferSizeMass, the agent has access to 10 environments during training, with sizes from either 0.01 or 0.05 m and masses ranging from 0.1 to 0.5 kg . At test time the agent is asked to perform the task on 2 environments with masses 0.7 kg and 0.75 kg with sizes $=0.05 \mathrm{~m}$.
We find that testing with large and light blocks increase the chances of accidental goal completions. Thus, during test-time, we use environments with high masses for out-of-distribution testing. The causal representation is concatenated to the state of the environment as a contextual input and supplied to a PPO-Optimized Actor-Critic Policy i.e., it receives 57 dimensional input for TransferMass, and a 58 dimensional for TransferSizeMass). The policy network consists of 2 hidden layers with 256 and 128 units respectively. The experiments are parallelized</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Directed Graphical Model. A Directed Acyclic Graph (DAG) visually represents the causal dependencies of observed and unobserved variables. In (A), an observed variable $\mathbf{X}$ is caused by unobserved causal variables, $P A_{i}(\mathbf{X})$. In (B), describes the scenario modeled in the paper, where a subset of the unobserved parent causal variables influence the observed variable $\mathbf{O}$. The action sequence $\mathbf{a}_{0: T}$ serves a gating mechanism, allowing or blocking particular edges of the causal graph.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Overview of training. The experiment planner generates a trajectory of actions which is applied to each of the environments with varying causal factors namely mass, shape and size of blocks. For each environment, an observation trajectory or state $\mathbf{O}^{i} \in O$ is obtained. A simple model with fixed low expressive power is used to approximate the generative model for $\mathbf{O}$. The "information overflow" $L(\mathbf{O} \mid \mathbf{M})$ is returned as negative reward forcing $O$ to be caused by few causal factors.
on 10 CPUs and implemented using stable baselines (Hill et al., 2018). The PPO configuration was {"gamma":0.9995, "n_steps": 5000, "ent_coef": 0, "learning_rate": 0.00025, "vf_coef": 0.5, "max_grad_norm": 10, "nminibatches": 1000, "noptepochs": 4}</p>
<p>The agent receives a dense reward at each time step during the maximizing external reward phase (Figure 1), the negative of the distance of the block from the goal position scaled by factor of 1000. The control signal was repeated 10 times to the actuators of the motors on each finger.</p>
<h2>C. Implementation Details for Pre-trained Behaviors</h2>
<p>In section 4.2, we study how the acquired experimental behaviors obtained through Causal Curiosity can be used as pre-training for a variety of downstream tasks. The Vanilla CEM depicts the cost of training an experiment planner from scratch to maximize an external dense reward where the agent minimizes the distance between the position of a block in an environment from the goal in the Lifting setup and imparts a velocity to the block along a particular direction
in the Travel setup.</p>
<p>$$
R\left(\mathbf{a}<em t="t">{0: T}\right)=-\sum</em>} \operatorname{dist}\left(\mathbf{g o a l<em t="t">{t}-\mathbf{b l o c k}</em>\right)
$$</p>
<p>The second baseline (Additive Reward) studies the setup when the agent receives both the curiosity signal and the external reward and attempts to maximize both. The agent receives access all the training environments with varying causal factors and must simultaneously maximize both curiosity and the task reward. The equation below shows the reward maximized for the Lifting task.</p>
<p>$$
\begin{gathered}
R\left(\mathbf{a}<em e="e" n="n" s="s" v="v">{0: T}\right)=\sum</em>} \sum_{t}^{T}-\operatorname{dist}\left(\text { goal <em t="t">{t}-\text { block }</em>\right)+ \
{\left[\min \left{d\left(\mathbf{o}<em 0:="0:" T="T">{0: T}, \mathbf{o}^{\prime}{ }</em>}\right): \mathbf{o<em 1="1">{0: T} \in C</em>}, \mathbf{o<em 2="2">{0: T}^{\prime} \in C</em> \
\left.\max \left{d\left(\mathbf{o}}\right}-\right.<em 0:="0:" T="T">{0: T}, \mathbf{o}^{\prime \prime}{ }</em>}\right): \mathbf{o}^{\prime \prime}{ <em 0:="0:" T="T">{0: T}, \mathbf{o}</em>\right}-\right. \
\left.\max \left{d\left(\mathbf{o}} \in C_{1<em 0:="0:" T="T">{0: T}^{\prime}, \mathbf{o}^{\prime \prime \prime}{ }</em>}\right): \mathbf{o<em 0:="0:" T="T">{0: T}^{\prime}, \mathbf{o}^{\prime \prime \prime}{ }</em>\right}\right]
\end{gathered}
$$} \in C_{2</p>
<p>The curious agent first acquired the experimental behavior by interacting with multiple environments with varying causal factors. The lifting skill was obtained during Mass, when the agent attempted to differentiate between multiple blocks of varying mass. The curious agent trained for 600,000 time steps on the curiosity reward. The acquired</p>
<p>behavior was then applied to the downstream lifting task and fine tuned to external rewards. The Vanilla CEM baseline had an identical structure to that of the Curious agent, and received only external reward as in Equation (11). The additive agent simultaneously optimized both external reward and the curiosity reward as in Equation (12).</p>
<p>We find that maximizing the curiosity reward in addition to simultaneously maximizing external rewards results in suboptimal performance due to our formulation of the curiosity reward. To maximize curiosity, the agent must discover behaviors that divide environments into 2 clusters. Thus in the context of the experimental setups, this corresponds to acquiring a lifting/pushing behavior that allows the agent to lift/impart horizontal velocity to blocks in half of the environments, while not being able to do so in the remaining environments. However, the explicit external reward incentivizes the agent to lift/impart horizontal velocity blocks in all environments. Thus these competing objectives result in sub-par performance.</p>
<h2>D. Intuition for Definition of Causal Factors</h2>
<p>We begin with a simple example of a person walking on earth. This person experiences various physical processes while interacting in their world, for example gravity, friction, wind etc. These physical processes affect the outcome of interactions of the person with their environment. For example, while jumping on earth, the human experiences gravity which affects the outcome of their jump, the fact that they falls back to the ground. Additionally, these physical processes (or causal mechanisms) are parameterized by causal factors, for example, acceleration constant due to gravity $g=9.8 \mathrm{~m} / \mathrm{s}^{2}$ on earth, or coefficients of friction between their feet and the ground which assume particular numerical values.</p>
<p>These causal factors may vary across multiple environments. For example, the person may walk on sand or on ice, surfaces with varying frictional values. Thus the outcome of running on such surfaces will vary, running on sand will require significant effort, while running on ice may result in the person slipping. Thus the coefficient of friction between the person's feet and the surface they walk on affects the outcome of a particular behavior in said environment. In our definition, $\mathbf{h}_{j}$ are causal factors such as friction or gravity etc. $H$ is the global set containing all such causal factors.</p>
<p>Now we ask the question (which we subsequently answer), given multiple environments, how would a human characterize each of them depending on the value of a causal factor? Through experimental behaviors. The human in the above example would attempt to run in each of the environments she encountered, be it on sand, on ice, in mud etc. If they slipped in an environment, she would characterize</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mh">2</span><span class="w"> </span><span class="n">Inference</span><span class="w"> </span><span class="n">Loop</span>
<span class="w">    </span><span class="nl">Input:</span><span class="w"> </span><span class="n">Unseen</span><span class="w"> </span><span class="n">Test</span><span class="w"> </span><span class="n">Environment</span><span class="w"> </span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">trained</span><span class="w"> </span><span class="n">Planner</span><span class="w"> </span><span class="k">and</span>
<span class="w">    </span><span class="n">Causal</span><span class="w"> </span><span class="n">Inference</span><span class="w"> </span><span class="n">Module</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">causalRep</span><span class="w"> </span><span class="n">\(=[\]</span>
<span class="w">    </span><span class="n">Initialize</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">set</span><span class="w"> </span><span class="n">Envs</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">\(\mathrm{k}\)</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">range</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="w"> </span><span class="k">do</span>
<span class="w">        </span><span class="n">Reset</span><span class="w"> </span><span class="n">env</span>
<span class="w">        </span><span class="n">Sample</span><span class="w"> </span><span class="n">experimental</span><span class="w"> </span><span class="n">behavior</span><span class="w"> </span><span class="n">\(\mathbf{a}_{0:</span><span class="w"> </span><span class="n">T</span><span class="p">}</span><span class="w"> </span><span class="n">\sim\)</span>
<span class="w">        </span><span class="n">CEM</span><span class="p">(</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">causalRep</span><span class="p">)</span>
<span class="w">        </span><span class="n">Apply</span><span class="w"> </span><span class="n">\(\mathbf{a}_{0:</span><span class="w"> </span><span class="n">T</span><span class="p">}</span><span class="n">\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">env</span>
<span class="w">        </span><span class="n">Collect</span><span class="w"> </span><span class="n">\(\mathbf{O}=\mathbf{o}_{0:</span><span class="w"> </span><span class="n">T</span><span class="p">}</span><span class="n">\)</span>
<span class="w">        </span><span class="n">Use</span><span class="w"> </span><span class="n">learnt</span><span class="w"> </span><span class="n">\(q_{M}(\mathbf{z}</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">\mathbf{O}\),</span><span class="w"> </span><span class="n">causalRep</span><span class="w"> </span><span class="n">\()\)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">cluster</span><span class="w"> </span><span class="k">assign</span><span class="o">-</span>
<span class="w">        </span><span class="n">ment</span><span class="w"> </span><span class="n">i</span><span class="p">.</span><span class="n">e</span><span class="p">.</span><span class="w"> </span><span class="n">\(\mathbf{z}_{k}=q_{M}(\mathbf{z}</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">\mathbf{O}\),</span><span class="w"> </span><span class="n">causalRep</span><span class="w"> </span><span class="n">\()</span>
<span class="w">        </span><span class="n">Append</span><span class="w"> </span><span class="n">\(z_{k}\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">causalRep</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="n">Learn</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">policy</span><span class="w"> </span><span class="n">conditioned</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">causal</span><span class="w"> </span><span class="n">factors</span><span class="w"> </span><span class="n">\(\mathbf{a}_{t}</span><span class="w"> </span><span class="n">\sim\)</span>
<span class="w">    </span><span class="n">\(\pi\left(\cdot</span><span class="w"> </span><span class="n">\mid</span><span class="w"> </span><span class="n">\mathbf{o}_{t},</span><span class="w"> </span><span class="n">\mathbf{z}_{0:</span><span class="w"> </span><span class="n">K</span><span class="p">}</span><span class="n">\right)\)</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">maximize</span><span class="w"> </span><span class="n">external</span><span class="w"> </span><span class="n">reward</span><span class="p">.</span>
</code></pre></div>

<p>it as slippery. If they didn't, they would characterize it as non-slippery. We attempt to equip our agent with similar logic. The "sequence of actions" $\left(\mathbf{a}<em 0:="0:" T="T">{0: T}\right)$ described in our paper corresponds to the human running. The sequence of observations $\left(\mathbf{o}</em>}\right)$ corresponds to the outcome of running "experiment". $\mathbf{o<em a="a">{0: T}$ might belong to either of the clusters of outcomes $C</em>$ corresponding to slipping or not slipping.}$ or $C_{b</p>
<h2>E. Scalability Limitation</h2>
<p>We utilize the extremely popular One-Factor-at-a-time (OFAT) general paradigm of scientific investigation, as an inspiration for our method. In the case of many hundreds of causal factors, the complexity of this method will scale exponentially. However, we believe that this would indeed be the case given a human experimenter attempting to discover the causation in any system she is studying. Learning about causation is a computationally expensive affair. We point the reader towards a wealth of material on the design of scientific experiments and more specifically the lack of scalability of OFAT (Fisher, 1936; Hicks, 1964; Czitrom, 1999). Nevertheless, OFAT remains the de facto standard for scientific investigation.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ University of Southern California ${ }^{2}$ Max Planck Institute for Intelligent Systems. Correspondence to: Sumedh A Sontakke $&lt;$ ssontakk@usc.edu $&gt;$.</p>
<p>Proceedings of the $38^{\text {th }}$ International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
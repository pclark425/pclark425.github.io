<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1416 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1416</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1416</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e" target="_blank">Mastering Diverse Domains through World Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> DreamerV3 is presented, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration, and is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula.</p>
                <p><strong>Paper Abstract:</strong> Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1416.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1416.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV3-worldmodel</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamerV3 world model (discrete-latent RSSM implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The world model used in DreamerV3 is a latent recurrent state-space model (RSSM) with discrete categorical latents, a block-GRU sequence model, and predictors for dynamics, rewards, and episode continuation; it is trained with reconstruction, dynamics KL, and representation KL losses and used to generate imagined trajectories for actor-critic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV3 world model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Recurrent State-Space Model (RSSM) implemented with: an encoder that maps observations x_t to stochastic discrete latents z_t (a vector of softmax distributions with straight-through gradients and a 1% uniform unimix); a block-diagonal GRU sequence model f_phi(h_{t-1}, z_{t-1}, a_{t-1}) producing recurrent state h_t; a factorized dynamics predictor p_phi(hat z_t | h_t); reward and continue predictors p_phi(r_t | h_t, z_t), p_phi(c_t | h_t, z_t); and a decoder p_phi(x_t | h_t, z_t) for reconstruction. The model uses symlog transforms on vector inputs/targets, symexp twohot losses for reward and critic targets, free-bits clipping on KL terms (1 nat), and an imagination horizon (T≈16) to roll out imagined trajectories for actor-critic updates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete-latent recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Wide-ranging: Atari, ProcGen, DMLab, Minecraft (diamond task), Visual Control, Proprioceptive Control, BSuite, Atari100k and other continuous/discrete control and visual/non-visual tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Composite prediction losses: reconstruction loss (decoder) using symlog-squared loss, reward prediction trained via symexp twohot categorical cross-entropy, continue predictor logistic loss, dynamics KL and representation KL (both clipped with free bits); qualitative multi-step video prediction visualizations (e.g., 45-frame rollouts) used as fidelity illustrations.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No single numeric fidelity score (e.g., MSE) reported; qualitative long-horizon multi-step video predictions are shown (45 frames conditioned on 5 context images) and ablations indicate the unsupervised reconstruction objective of the world model is the primary contributor to downstream task performance. KL clips (free bits) are set to 1 nat to stabilize representation/dynamics losses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Predominantly a black-box neural latent model; some interpretability via visualization of reconstructions and multi-step rollouts (Figure 4) and inspection of latent reconstructions, but latent codes are discrete vectors without an explicit semantic mapping; no claim of disentangled or human-interpretable latent factors.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructed frames and multi-step imagined rollouts; ablation studies that remove reconstruction or reward/value gradients to inspect their contribution to representations; no explicit attention maps or symbolic extraction methods reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Model sizes range from 12M to 400M parameters (default/benchmark size 200M). Training reported on single NVIDIA A100 GPUs; example compute budgets: Minecraft 200M model trained ~8.9 GPU-days (100M env steps), Atari 200M model ~7.7 GPU-days (200M env steps), other benchmarks with listed GPU-days in Table 2. Uses AGC gradient clipping and LaProp optimizer. Memory and FLOP counts not provided explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Paper reports Dreamer (with this world model) outperforms tuned expert baselines and a high-quality PPO implementation across diverse domains while using fixed hyperparameters; compared to MuZero, Dreamer outperforms in Atari while using a fraction of the computational resources; in DMLab Dreamer reaches higher scores at 100M steps compared to IMPALA/R2D2+ at 1B steps (reported as a ~1000% data-efficiency gain).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Enables state-of-the-art or best-reported performance in the paper across many benchmarks (Atari, ProcGen, DMLab, Proprio Control, Visual Control, BSuite, Atari100k). Notably, first reported algorithm in this work to collect diamonds in Minecraft from scratch (no human data) within 100M environment steps; ablations show major contribution of the world model losses to final task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>World model utility is high: actor and critic learn from imagined trajectories produced by the world model, enabling policy improvement without expensive environment interaction. Ablations show the unsupervised reconstruction loss is the dominant signal for downstream performance, indicating that learning predictive, compact representations (rather than reward-only signals) is key to transferring fidelity into policy utility. Larger and more expressive world models both increase task performance and reduce required environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs addressed in design: discrete latents and twohot/symexp decouple gradient magnitudes from target scale improving stability (efficiency vs fidelity); symlog compresses targets to handle large dynamic ranges (stability vs absolute fidelity); free-bits KL clipping prevents trivial but overly-predictable dynamics (fidelity vs informativeness); increasing model size improves performance but increases computational cost (compute vs data-efficiency trade-off). The paper emphasizes stability and generality across domains over optimizing raw reconstruction fidelity in any single domain.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices include: RSSM architecture with discrete categorical latents (softmax vectors) and 1% unimix to prevent determinism; straight-through gradients for discrete sampling; block-diagonal GRU for scalable recurrence; symlog transform for inputs/targets; symexp twohot output representation and categorical cross-entropy for rewards/critic; free bits (1 nat) to clip KL losses; percentile-based return normalization for actor; AGC and LaProp optimizer; storing/updating latent states in replay buffer; imagination horizon ~15; fixed hyperparameters across domains; model sizes scaled from 12M to 400M.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against model-free baselines (PPO) and specialized model-based/planning methods (MuZero) and transformer-based world-model approaches in experiments: Dreamer's RSSM world model + imagined trajectories yields better overall sample efficiency and higher final scores across many domains with fixed hyperparameters; specifically, Dreamer outperforms PPO and several specialized expert algorithms and is reported to outperform MuZero on Atari while using less compute. The paper also compares favorably to transformer-based agents on the low-data Atari100k benchmark in the reported setup.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Empirical recommendations: use symlog/symexp twohot losses, discrete latents with unimix, KL free-bits (1 nat) and KL balancing, percentile return normalization, AGC and LaProp, and larger model sizes when compute permits (performance increases monotonically with model size and larger models require fewer environment interactions). Default practical settings given (e.g., model size 200M, replay ratios per benchmark) are recommended as robust configurations across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Diverse Domains through World Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1416.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1416.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent latent dynamics model that separates a deterministic recurrent state h_t from stochastic latent z_t and predicts future latents and observations; used in Dreamer line for planning in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning latent dynamics for planning from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sequence model with deterministic recurrent state h_t (GRU) and stochastic latent z_t inferred from encoder q_phi(z_t | h_t, x_t); dynamics predictor p_phi(z_t | h_t) predicts latents autoregressively; decoder reconstructs observations from (h_t, z_t) and predictors output rewards/continues. Implemented with discrete latents in DreamerV3.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (stochastic recurrent state-space)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Planning from pixels / control tasks (as used here: Atari, DMLab, Minecraft, continuous/discrete control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Dynamics KL between inferred q_phi(z_t|h_t,x_t) and predictor p_phi(z_t|h_t), reconstruction loss on observations, reward prediction loss; KLs are clipped with free-bits to prevent collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No absolute numeric fidelity reported in this paper for RSSM alone; qualitative multi-step rollouts demonstrate plausible long-horizon predictions (Figure 4) and RSSM KL terms are actively regulated (free bits=1 nat).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent/deterministic state decomposition gives some structural interpretability (separates memory from instantaneous stochasticity) but latent variables remain neural and not explicitly interpretable; visual reconstructions provide intuitive check of learned dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of reconstructions and imagined rollouts; ablations affecting RSSM losses to test their effect on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Costs scale with chosen model dimension; in DreamerV3, RSSM-based models vary from 12M to 400M parameters; block-GRU used to keep recurrence efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>RSSM-based Dreamer yields strong sample-efficiency compared to model-free baselines and competitive methods in reported experiments; no direct FLOP or latency comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>When used in DreamerV3, RSSM is central to achieving SOTA performance across many domains and to enabling imagination-based actor-critic updates that produce high-performing policies (e.g., Minecraft diamond achievement).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM's separation of deterministic recurrent state and stochastic latent helps capture environment dynamics needed for imagining trajectories that are useful for policy learning; the paper's ablations show that RSSM's reconstruction objective is critical for final performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM requires balancing reconstruction and KL regularization; too-strong regularization harms task-relevant detail, too-weak harms predictability. DreamerV3 uses free-bits and a small rep loss weight (0.1) to navigate this tradeoff.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete latents, unimix to avoid determinism, free-bits KL clipping, block-diagonal GRU recurrence to increase capacity without quadratic cost, factorized dynamics predictor for fast sampling during imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>RSSM is a middle ground between fully deterministic predictive RNNs and fully explicit simulators; within the paper, RSSM-based Dreamer outperforms model-free baselines and competes favorably with other learned-model planners.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests balancing KL with reconstruction via free-bits (1 nat) and small representation loss weight (0.1), using discrete latents with unimix, and scaling model size to task/compute budget for best trade-off between fidelity and efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Diverse Domains through World Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1416.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1416.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero (learned model with planning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning algorithm that learns dynamics, policy and value predictors and uses Monte-Carlo tree search over the learned model for planning; referenced as a high-performing learned-model planner on Atari and board games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero (learned dynamics + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A learned model that predicts rewards, values and policies for latent states and is used inside Monte-Carlo tree search (MCTS) for planning; paper references MuZero as a prior learned-model planning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>learned model used for planning (model-based planning with search)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games and Atari (as discussed in the paper's comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in detail in this paper; MuZero typically optimizes losses on predicted reward, predicted value, and policy distillation from MCTS, but DreamerV3 paper does not provide MuZero internal fidelity numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No numeric fidelity values given here; DreamerV3 reports outperforming MuZero on Atari while using only a fraction of the computational resources (qualitative claim).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural predictors coupled with MCTS; no interpretability methods discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None mentioned in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Described qualitatively as computationally expensive/complex; DreamerV3 claims to use only a fraction of MuZero's compute in their comparisons (no exact FLOP/GPU numbers provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV3 claims superior compute efficiency: it outperforms MuZero on Atari in the authors' experiments while using substantially less compute (qualitative statement in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>MuZero is referenced as a strong prior on Atari and board games; DreamerV3 reports higher empirical scores in their Atari experiments relative to MuZero's published results under their compute/budget comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's learned model supports strong planning performance via MCTS, but the paper argues DreamerV3's simpler imagination-based policy training can match or exceed MuZero in many settings with less compute and simpler components (no tree search at action selection time).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MuZero trades online planning compute (MCTS) for sample efficiency and asymptotic performance, whereas DreamerV3 trades off heavier learned world-model training and imagined rollouts for simpler action selection (no per-step planning) and claims lower overall compute in practice according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not detailed in this paper beyond mentioning that MuZero uses a value-prediction model and planning; the paper contrasts DreamerV3's single-configuration, imagination-based actor-critic with MuZero's planning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Paper positions MuZero as a strong learned-model planner baseline that DreamerV3 outperforms on Atari while using less compute; no head-to-head ablation of MuZero design choices is performed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed for MuZero in this paper; the paper's recommendation is that simpler imagined-rollout approaches with robust losses and transforms (the DreamerV3 recipe) can be more generally applicable with less compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Diverse Domains through World Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1416.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1416.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WorldModels_Ha</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World Models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An influential early latent world-model approach that learns a VAE-style observation model and an RNN dynamics model and trains a controller inside the learned latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models (VAE + RNN latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Original 'World Models' approach: vision VAE to compress frames into latent vectors, an RNN to predict latent dynamics, and a policy/controller trained in the learned latent space. Mentioned as prior work motivating latent imagination approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (VAE + RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Control from pixels and simple simulated environments (cited historically as inspiration for Dreamer line)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; historically uses reconstruction losses (VAE) and next-latent prediction accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper; cited as foundational prior demonstrating the utility of learning latent dynamics for control.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Original approach showed some interpretability via visualizing decoded rollouts, but the DreamerV3 paper treats it as background and does not provide interpretability analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Historical: visualization of decoded latent rollouts; DreamerV3 uses similar visualization for sanity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not discussed here; World Models originally designed for small-scale experiments; DreamerV3 scales these ideas to larger, more diverse domains.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV3 extends and stabilizes latent-world-model ideas to be broadly applicable across >150 tasks with fixed hyperparameters; DreamerV3's engineering (symlog, twohot, KL balance, unimix, etc.) is presented as improving robustness and efficiency over earlier latent-model recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>World Models historically demonstrated that controllers trained in latent spaces can solve some control tasks; DreamerV3 shows this idea scaled to many more domains and harder tasks (e.g., Minecraft diamonds).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Paper positions World Models as conceptual predecessors; DreamerV3 builds on the central utility (latent imagination for policy learning) and adds robustness and scaling techniques to improve utility across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Original World Models traded representation capacity and reconstruction fidelity for controller tractability; DreamerV3 emphasizes mechanisms (KL balance, free bits, symlog) to navigate these tradeoffs at large scale.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>DreamerV3 adopts the high-level idea (learn latent representations + latent dynamics) but changes low-level choices: discrete latents, twohot losses, free-bits KL clipping, block-GRU, symlog transforms, and other robustness techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>DreamerV3 references World Models as inspiration but claims improved robustness, scalability, and performance across many domains and budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided for World Models here; DreamerV3 provides a practical recipe that the authors argue is a more robust configuration for diverse domains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Diverse Domains through World Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1416.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1416.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer-worldmodels</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer-based world models (e.g., 'Transformers are sample efficient world models')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer-based predictive models used as world models in recent literature; referenced by DreamerV3 primarily in the context of low-data Atari (Atari100k) comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers are sample efficient world models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based world models (TWM / IRIS / transformer agents)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Predictive world models built with transformer architectures to model sequences and predict future observations/rewards; cited as competitive baselines in low-data regimes such as Atari100k.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent or sequence-based world model using transformer architectures</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Primarily Atari100k (low-data Atari) and other sample-efficient benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Not specified in this paper; generally measured by next-step/rollout prediction losses and downstream policy performance in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>DreamerV3 reports outperforming several transformer-based agents (IRIS, TWM) on Atari100k in their experimental setup; no numeric fidelity metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Transformer models are treated as black-box sequence models in the paper; no interpretability methods are discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None discussed in the DreamerV3 paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Transformer-based models can be computationally heavy; DreamerV3 emphasizes that it attains strong low-data performance without their architectural complexity in the reported comparisons (no FLOP/GPU counts provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>DreamerV3 states it outperforms transformer-based agents on Atari100k in their configuration, implying favorable sample-efficiency and compute trade-offs in those experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Referenced as state-of-the-art contenders on sample-efficient Atari; DreamerV3 claims to beat the remaining transformer-based methods in the Atari100k experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Transformer world models can be sample-efficient in low-data regimes per referenced works, but DreamerV3 argues that its RSSM-based approach plus robust loss/transforms provides better overall out-of-the-box performance across many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Transformers may offer modeling power for long-range dependencies at increased architectural complexity and compute; DreamerV3 emphasizes simpler recurrent latent models with robustness tricks as a practical alternative across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Not specified in detail in this paper; DreamerV3 contrasts their RSSM-based design and robustness techniques against transformer-based alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>In the Atari100k benchmark DreamerV3 outperforms the transformer-based baselines reported (IRIS, TWM) under the experimental conditions of this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided for transformer models in this paper; DreamerV3 provides its own preferred configuration and hyperparameters for robust, cross-domain performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Diverse Domains through World Models', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning latent dynamics for planning from pixels <em>(Rating: 2)</em></li>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Transformers are sample efficient world models <em>(Rating: 2)</em></li>
                <li>Modelbased reinforcement learning for atari <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1416",
    "paper_id": "paper-f2d952a183dfb0a1e031b8a3f535d9f8423d7a6e",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "DreamerV3-worldmodel",
            "name_full": "DreamerV3 world model (discrete-latent RSSM implementation)",
            "brief_description": "The world model used in DreamerV3 is a latent recurrent state-space model (RSSM) with discrete categorical latents, a block-GRU sequence model, and predictors for dynamics, rewards, and episode continuation; it is trained with reconstruction, dynamics KL, and representation KL losses and used to generate imagined trajectories for actor-critic learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DreamerV3 world model (RSSM)",
            "model_description": "A Recurrent State-Space Model (RSSM) implemented with: an encoder that maps observations x_t to stochastic discrete latents z_t (a vector of softmax distributions with straight-through gradients and a 1% uniform unimix); a block-diagonal GRU sequence model f_phi(h_{t-1}, z_{t-1}, a_{t-1}) producing recurrent state h_t; a factorized dynamics predictor p_phi(hat z_t | h_t); reward and continue predictors p_phi(r_t | h_t, z_t), p_phi(c_t | h_t, z_t); and a decoder p_phi(x_t | h_t, z_t) for reconstruction. The model uses symlog transforms on vector inputs/targets, symexp twohot losses for reward and critic targets, free-bits clipping on KL terms (1 nat), and an imagination horizon (T≈16) to roll out imagined trajectories for actor-critic updates.",
            "model_type": "latent world model (discrete-latent recurrent state-space model)",
            "task_domain": "Wide-ranging: Atari, ProcGen, DMLab, Minecraft (diamond task), Visual Control, Proprioceptive Control, BSuite, Atari100k and other continuous/discrete control and visual/non-visual tasks",
            "fidelity_metric": "Composite prediction losses: reconstruction loss (decoder) using symlog-squared loss, reward prediction trained via symexp twohot categorical cross-entropy, continue predictor logistic loss, dynamics KL and representation KL (both clipped with free bits); qualitative multi-step video prediction visualizations (e.g., 45-frame rollouts) used as fidelity illustrations.",
            "fidelity_performance": "No single numeric fidelity score (e.g., MSE) reported; qualitative long-horizon multi-step video predictions are shown (45 frames conditioned on 5 context images) and ablations indicate the unsupervised reconstruction objective of the world model is the primary contributor to downstream task performance. KL clips (free bits) are set to 1 nat to stabilize representation/dynamics losses.",
            "interpretability_assessment": "Predominantly a black-box neural latent model; some interpretability via visualization of reconstructions and multi-step rollouts (Figure 4) and inspection of latent reconstructions, but latent codes are discrete vectors without an explicit semantic mapping; no claim of disentangled or human-interpretable latent factors.",
            "interpretability_method": "Visualization of reconstructed frames and multi-step imagined rollouts; ablation studies that remove reconstruction or reward/value gradients to inspect their contribution to representations; no explicit attention maps or symbolic extraction methods reported.",
            "computational_cost": "Model sizes range from 12M to 400M parameters (default/benchmark size 200M). Training reported on single NVIDIA A100 GPUs; example compute budgets: Minecraft 200M model trained ~8.9 GPU-days (100M env steps), Atari 200M model ~7.7 GPU-days (200M env steps), other benchmarks with listed GPU-days in Table 2. Uses AGC gradient clipping and LaProp optimizer. Memory and FLOP counts not provided explicitly.",
            "efficiency_comparison": "Paper reports Dreamer (with this world model) outperforms tuned expert baselines and a high-quality PPO implementation across diverse domains while using fixed hyperparameters; compared to MuZero, Dreamer outperforms in Atari while using a fraction of the computational resources; in DMLab Dreamer reaches higher scores at 100M steps compared to IMPALA/R2D2+ at 1B steps (reported as a ~1000% data-efficiency gain).",
            "task_performance": "Enables state-of-the-art or best-reported performance in the paper across many benchmarks (Atari, ProcGen, DMLab, Proprio Control, Visual Control, BSuite, Atari100k). Notably, first reported algorithm in this work to collect diamonds in Minecraft from scratch (no human data) within 100M environment steps; ablations show major contribution of the world model losses to final task performance.",
            "task_utility_analysis": "World model utility is high: actor and critic learn from imagined trajectories produced by the world model, enabling policy improvement without expensive environment interaction. Ablations show the unsupervised reconstruction loss is the dominant signal for downstream performance, indicating that learning predictive, compact representations (rather than reward-only signals) is key to transferring fidelity into policy utility. Larger and more expressive world models both increase task performance and reduce required environment interactions.",
            "tradeoffs_observed": "Trade-offs addressed in design: discrete latents and twohot/symexp decouple gradient magnitudes from target scale improving stability (efficiency vs fidelity); symlog compresses targets to handle large dynamic ranges (stability vs absolute fidelity); free-bits KL clipping prevents trivial but overly-predictable dynamics (fidelity vs informativeness); increasing model size improves performance but increases computational cost (compute vs data-efficiency trade-off). The paper emphasizes stability and generality across domains over optimizing raw reconstruction fidelity in any single domain.",
            "design_choices": "Key design choices include: RSSM architecture with discrete categorical latents (softmax vectors) and 1% unimix to prevent determinism; straight-through gradients for discrete sampling; block-diagonal GRU for scalable recurrence; symlog transform for inputs/targets; symexp twohot output representation and categorical cross-entropy for rewards/critic; free bits (1 nat) to clip KL losses; percentile-based return normalization for actor; AGC and LaProp optimizer; storing/updating latent states in replay buffer; imagination horizon ~15; fixed hyperparameters across domains; model sizes scaled from 12M to 400M.",
            "comparison_to_alternatives": "Compared against model-free baselines (PPO) and specialized model-based/planning methods (MuZero) and transformer-based world-model approaches in experiments: Dreamer's RSSM world model + imagined trajectories yields better overall sample efficiency and higher final scores across many domains with fixed hyperparameters; specifically, Dreamer outperforms PPO and several specialized expert algorithms and is reported to outperform MuZero on Atari while using less compute. The paper also compares favorably to transformer-based agents on the low-data Atari100k benchmark in the reported setup.",
            "optimal_configuration": "Empirical recommendations: use symlog/symexp twohot losses, discrete latents with unimix, KL free-bits (1 nat) and KL balancing, percentile return normalization, AGC and LaProp, and larger model sizes when compute permits (performance increases monotonically with model size and larger models require fewer environment interactions). Default practical settings given (e.g., model size 200M, replay ratios per benchmark) are recommended as robust configurations across domains.",
            "uuid": "e1416.0",
            "source_info": {
                "paper_title": "Mastering Diverse Domains through World Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State-Space Model (RSSM)",
            "brief_description": "A recurrent latent dynamics model that separates a deterministic recurrent state h_t from stochastic latent z_t and predicts future latents and observations; used in Dreamer line for planning in latent space.",
            "citation_title": "Learning latent dynamics for planning from pixels",
            "mention_or_use": "use",
            "model_name": "Recurrent State-Space Model (RSSM)",
            "model_description": "Sequence model with deterministic recurrent state h_t (GRU) and stochastic latent z_t inferred from encoder q_phi(z_t | h_t, x_t); dynamics predictor p_phi(z_t | h_t) predicts latents autoregressively; decoder reconstructs observations from (h_t, z_t) and predictors output rewards/continues. Implemented with discrete latents in DreamerV3.",
            "model_type": "latent world model (stochastic recurrent state-space)",
            "task_domain": "Planning from pixels / control tasks (as used here: Atari, DMLab, Minecraft, continuous/discrete control)",
            "fidelity_metric": "Dynamics KL between inferred q_phi(z_t|h_t,x_t) and predictor p_phi(z_t|h_t), reconstruction loss on observations, reward prediction loss; KLs are clipped with free-bits to prevent collapse.",
            "fidelity_performance": "No absolute numeric fidelity reported in this paper for RSSM alone; qualitative multi-step rollouts demonstrate plausible long-horizon predictions (Figure 4) and RSSM KL terms are actively regulated (free bits=1 nat).",
            "interpretability_assessment": "Latent/deterministic state decomposition gives some structural interpretability (separates memory from instantaneous stochasticity) but latent variables remain neural and not explicitly interpretable; visual reconstructions provide intuitive check of learned dynamics.",
            "interpretability_method": "Visual inspection of reconstructions and imagined rollouts; ablations affecting RSSM losses to test their effect on downstream tasks.",
            "computational_cost": "Costs scale with chosen model dimension; in DreamerV3, RSSM-based models vary from 12M to 400M parameters; block-GRU used to keep recurrence efficient.",
            "efficiency_comparison": "RSSM-based Dreamer yields strong sample-efficiency compared to model-free baselines and competitive methods in reported experiments; no direct FLOP or latency comparison provided.",
            "task_performance": "When used in DreamerV3, RSSM is central to achieving SOTA performance across many domains and to enabling imagination-based actor-critic updates that produce high-performing policies (e.g., Minecraft diamond achievement).",
            "task_utility_analysis": "RSSM's separation of deterministic recurrent state and stochastic latent helps capture environment dynamics needed for imagining trajectories that are useful for policy learning; the paper's ablations show that RSSM's reconstruction objective is critical for final performance.",
            "tradeoffs_observed": "RSSM requires balancing reconstruction and KL regularization; too-strong regularization harms task-relevant detail, too-weak harms predictability. DreamerV3 uses free-bits and a small rep loss weight (0.1) to navigate this tradeoff.",
            "design_choices": "Discrete latents, unimix to avoid determinism, free-bits KL clipping, block-diagonal GRU recurrence to increase capacity without quadratic cost, factorized dynamics predictor for fast sampling during imagination.",
            "comparison_to_alternatives": "RSSM is a middle ground between fully deterministic predictive RNNs and fully explicit simulators; within the paper, RSSM-based Dreamer outperforms model-free baselines and competes favorably with other learned-model planners.",
            "optimal_configuration": "Paper suggests balancing KL with reconstruction via free-bits (1 nat) and small representation loss weight (0.1), using discrete latents with unimix, and scaling model size to task/compute budget for best trade-off between fidelity and efficiency.",
            "uuid": "e1416.1",
            "source_info": {
                "paper_title": "Mastering Diverse Domains through World Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "MuZero",
            "name_full": "MuZero (learned model with planning)",
            "brief_description": "A planning algorithm that learns dynamics, policy and value predictors and uses Monte-Carlo tree search over the learned model for planning; referenced as a high-performing learned-model planner on Atari and board games.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "mention",
            "model_name": "MuZero (learned dynamics + MCTS)",
            "model_description": "A learned model that predicts rewards, values and policies for latent states and is used inside Monte-Carlo tree search (MCTS) for planning; paper references MuZero as a prior learned-model planning approach.",
            "model_type": "learned model used for planning (model-based planning with search)",
            "task_domain": "Board games and Atari (as discussed in the paper's comparisons)",
            "fidelity_metric": "Not specified in detail in this paper; MuZero typically optimizes losses on predicted reward, predicted value, and policy distillation from MCTS, but DreamerV3 paper does not provide MuZero internal fidelity numbers.",
            "fidelity_performance": "No numeric fidelity values given here; DreamerV3 reports outperforming MuZero on Atari while using only a fraction of the computational resources (qualitative claim).",
            "interpretability_assessment": "Black-box neural predictors coupled with MCTS; no interpretability methods discussed in this paper.",
            "interpretability_method": "None mentioned in this paper.",
            "computational_cost": "Described qualitatively as computationally expensive/complex; DreamerV3 claims to use only a fraction of MuZero's compute in their comparisons (no exact FLOP/GPU numbers provided).",
            "efficiency_comparison": "DreamerV3 claims superior compute efficiency: it outperforms MuZero on Atari in the authors' experiments while using substantially less compute (qualitative statement in paper).",
            "task_performance": "MuZero is referenced as a strong prior on Atari and board games; DreamerV3 reports higher empirical scores in their Atari experiments relative to MuZero's published results under their compute/budget comparisons.",
            "task_utility_analysis": "MuZero's learned model supports strong planning performance via MCTS, but the paper argues DreamerV3's simpler imagination-based policy training can match or exceed MuZero in many settings with less compute and simpler components (no tree search at action selection time).",
            "tradeoffs_observed": "MuZero trades online planning compute (MCTS) for sample efficiency and asymptotic performance, whereas DreamerV3 trades off heavier learned world-model training and imagined rollouts for simpler action selection (no per-step planning) and claims lower overall compute in practice according to the authors.",
            "design_choices": "Not detailed in this paper beyond mentioning that MuZero uses a value-prediction model and planning; the paper contrasts DreamerV3's single-configuration, imagination-based actor-critic with MuZero's planning complexity.",
            "comparison_to_alternatives": "Paper positions MuZero as a strong learned-model planner baseline that DreamerV3 outperforms on Atari while using less compute; no head-to-head ablation of MuZero design choices is performed in this paper.",
            "optimal_configuration": "Not discussed for MuZero in this paper; the paper's recommendation is that simpler imagined-rollout approaches with robust losses and transforms (the DreamerV3 recipe) can be more generally applicable with less compute.",
            "uuid": "e1416.2",
            "source_info": {
                "paper_title": "Mastering Diverse Domains through World Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "WorldModels_Ha",
            "name_full": "World Models (Ha & Schmidhuber)",
            "brief_description": "An influential early latent world-model approach that learns a VAE-style observation model and an RNN dynamics model and trains a controller inside the learned latent space.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models (VAE + RNN latent model)",
            "model_description": "Original 'World Models' approach: vision VAE to compress frames into latent vectors, an RNN to predict latent dynamics, and a policy/controller trained in the learned latent space. Mentioned as prior work motivating latent imagination approaches.",
            "model_type": "latent world model (VAE + RNN)",
            "task_domain": "Control from pixels and simple simulated environments (cited historically as inspiration for Dreamer line)",
            "fidelity_metric": "Not specified in this paper; historically uses reconstruction losses (VAE) and next-latent prediction accuracy.",
            "fidelity_performance": "Not quantified in this paper; cited as foundational prior demonstrating the utility of learning latent dynamics for control.",
            "interpretability_assessment": "Original approach showed some interpretability via visualizing decoded rollouts, but the DreamerV3 paper treats it as background and does not provide interpretability analysis.",
            "interpretability_method": "Historical: visualization of decoded latent rollouts; DreamerV3 uses similar visualization for sanity checks.",
            "computational_cost": "Not discussed here; World Models originally designed for small-scale experiments; DreamerV3 scales these ideas to larger, more diverse domains.",
            "efficiency_comparison": "DreamerV3 extends and stabilizes latent-world-model ideas to be broadly applicable across &gt;150 tasks with fixed hyperparameters; DreamerV3's engineering (symlog, twohot, KL balance, unimix, etc.) is presented as improving robustness and efficiency over earlier latent-model recipes.",
            "task_performance": "World Models historically demonstrated that controllers trained in latent spaces can solve some control tasks; DreamerV3 shows this idea scaled to many more domains and harder tasks (e.g., Minecraft diamonds).",
            "task_utility_analysis": "Paper positions World Models as conceptual predecessors; DreamerV3 builds on the central utility (latent imagination for policy learning) and adds robustness and scaling techniques to improve utility across tasks.",
            "tradeoffs_observed": "Original World Models traded representation capacity and reconstruction fidelity for controller tractability; DreamerV3 emphasizes mechanisms (KL balance, free bits, symlog) to navigate these tradeoffs at large scale.",
            "design_choices": "DreamerV3 adopts the high-level idea (learn latent representations + latent dynamics) but changes low-level choices: discrete latents, twohot losses, free-bits KL clipping, block-GRU, symlog transforms, and other robustness techniques.",
            "comparison_to_alternatives": "DreamerV3 references World Models as inspiration but claims improved robustness, scalability, and performance across many domains and budgets.",
            "optimal_configuration": "Not provided for World Models here; DreamerV3 provides a practical recipe that the authors argue is a more robust configuration for diverse domains.",
            "uuid": "e1416.3",
            "source_info": {
                "paper_title": "Mastering Diverse Domains through World Models",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Transformer-worldmodels",
            "name_full": "Transformer-based world models (e.g., 'Transformers are sample efficient world models')",
            "brief_description": "Transformer-based predictive models used as world models in recent literature; referenced by DreamerV3 primarily in the context of low-data Atari (Atari100k) comparisons.",
            "citation_title": "Transformers are sample efficient world models",
            "mention_or_use": "mention",
            "model_name": "Transformer-based world models (TWM / IRIS / transformer agents)",
            "model_description": "Predictive world models built with transformer architectures to model sequences and predict future observations/rewards; cited as competitive baselines in low-data regimes such as Atari100k.",
            "model_type": "latent or sequence-based world model using transformer architectures",
            "task_domain": "Primarily Atari100k (low-data Atari) and other sample-efficient benchmarks",
            "fidelity_metric": "Not specified in this paper; generally measured by next-step/rollout prediction losses and downstream policy performance in referenced works.",
            "fidelity_performance": "DreamerV3 reports outperforming several transformer-based agents (IRIS, TWM) on Atari100k in their experimental setup; no numeric fidelity metrics provided in this paper.",
            "interpretability_assessment": "Transformer models are treated as black-box sequence models in the paper; no interpretability methods are discussed here.",
            "interpretability_method": "None discussed in the DreamerV3 paper.",
            "computational_cost": "Transformer-based models can be computationally heavy; DreamerV3 emphasizes that it attains strong low-data performance without their architectural complexity in the reported comparisons (no FLOP/GPU counts provided).",
            "efficiency_comparison": "DreamerV3 states it outperforms transformer-based agents on Atari100k in their configuration, implying favorable sample-efficiency and compute trade-offs in those experiments.",
            "task_performance": "Referenced as state-of-the-art contenders on sample-efficient Atari; DreamerV3 claims to beat the remaining transformer-based methods in the Atari100k experiments.",
            "task_utility_analysis": "Transformer world models can be sample-efficient in low-data regimes per referenced works, but DreamerV3 argues that its RSSM-based approach plus robust loss/transforms provides better overall out-of-the-box performance across many domains.",
            "tradeoffs_observed": "Transformers may offer modeling power for long-range dependencies at increased architectural complexity and compute; DreamerV3 emphasizes simpler recurrent latent models with robustness tricks as a practical alternative across domains.",
            "design_choices": "Not specified in detail in this paper; DreamerV3 contrasts their RSSM-based design and robustness techniques against transformer-based alternatives.",
            "comparison_to_alternatives": "In the Atari100k benchmark DreamerV3 outperforms the transformer-based baselines reported (IRIS, TWM) under the experimental conditions of this paper.",
            "optimal_configuration": "Not provided for transformer models in this paper; DreamerV3 provides its own preferred configuration and hyperparameters for robust, cross-domain performance.",
            "uuid": "e1416.4",
            "source_info": {
                "paper_title": "Mastering Diverse Domains through World Models",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning latent dynamics for planning from pixels",
            "rating": 2
        },
        {
            "paper_title": "World models",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2
        },
        {
            "paper_title": "Transformers are sample efficient world models",
            "rating": 2
        },
        {
            "paper_title": "Modelbased reinforcement learning for atari",
            "rating": 1
        }
    ],
    "cost": 0.01995625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Mastering Diverse Domains through World Models</h1>
<p>Danijar Hafner, ${ }^{12}$ Jurgis Pasukonis, ${ }^{1}$ Jimmy Ba, ${ }^{2}$ Timothy Lillicrap ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Benchmark summary. a, Using fixed hyperparameters across all domains, Dreamer outperforms tuned expert algorithms across a wide range of benchmarks and data budgets. Dreamer also substantially outperforms a high-quality implementation of the widely applicable PPO algorithm. b, Applied out of the box, Dreamer learns to obtain diamonds in the popular video game Minecraft from scratch given sparse rewards, a long-standing challenge in artificial intelligence for which previous approaches required human data or domain-specific heuristics.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diverse visual domains used in the experiments. Dreamer succeeds across these domains, ranging from robot locomotion and manipulation tasks over Atari games, procedurally generated ProcGen levels, and DMLab tasks, that require spatial and temporal reasoning, to the complex and infinite world of Minecraft. We also evaluate Dreamer on non-visual domains.</p>
<h1>Introduction</h1>
<p>Reinforcement learning has enabled computers to solve tasks through interaction, such as surpassing humans in the games of Go and Dota ${ }^{1,2}$. It is also a key component for improving large language models beyond what is demonstrated in their pretraining data ${ }^{3,4}$. While $\mathrm{PPO}^{5}$ has become a standard algorithm in the field of reinforcement learning, more specialized algorithms are often employed to achieve higher performance. These specialized algorithms target the unique challenges posed by different application domains, such as continuous control ${ }^{6}$, discrete actions ${ }^{7,8}$, sparse rewards ${ }^{9}$, image inputs ${ }^{10}$, spatial environments ${ }^{11}$, and board games ${ }^{12}$. However, applying reinforcement learning algorithms to sufficiently new tasks-such as moving from video games to robotics tasksrequires substantial effort, expertise, and computational resources for tweaking the hyperparameters of the algorithm ${ }^{13}$. This brittleness poses a bottleneck in applying reinforcement learning to new problems and also limits the applicability of reinforcement learning to computationally expensive models or tasks where tuning is prohibitive. Creating a general algorithm that learns to master new domains without having to be reconfigured has been a central challenge in artificial intelligence and would open up reinforcement learning to a wide range of practical applications.
We present Dreamer, a general algorithm that outperforms specialized expert algorithms across a wide range of domains while using fixed hyperparameters, making reinforcement learning readily applicable to new problems. The algorithm is based on the idea of learning a world model that equips the agent with rich perception and the ability to imagine the future ${ }^{14,15,16}$. The world model predicts the outcomes of potential actions, a critic neural network judges the value of each outcome, and an actor neural network chooses actions to reach the best outcomes. Although intuitively appealing, robustly learning and leveraging world models to achieve strong task performance has been an open problem ${ }^{17}$. Dreamer overcomes this challenge through a range of robustness techniques based on normalization, balancing, and transformations. We observe robust learning not only across over 150 tasks from the domains summarized in Figure 2, but also across model sizes and training budgets, offering a predictable way to increase performance. Notably, larger model sizes not only achieve higher scores but also require less interaction to solve a task.
To push the boundaries of reinforcement learning, we consider the popular video game Minecraft that has become a focal point of research in recent years ${ }^{18,19,20}$, with international competitions held for developing algorithms that autonomously learn to collect diamonds in Minecraft ${ }^{\star}$. Solving this</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Training process of Dreamer. The world model encodes sensory inputs into discrete representations $z_{t}$ that are predicted by a sequence model with recurrent state $h_{t}$ given actions $a_{t}$. The inputs are reconstructed to shape the representations. The actor and critic predict actions $a_{t}$ and values $v_{t}$ and learn from trajectories of abstract representations predicted by the world model.
problem without human data has been widely recognized as a substantial challenge for artificial intelligence because of the sparse rewards, exploration difficulty, long time horizons, and the procedural diversity of this open world game ${ }^{18}$. Due to these obstacles, previous approaches resorted to using human expert data and domain-specific curricula ${ }^{19,20}$. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch.</p>
<h1>Learning algorithm</h1>
<p>We present the third generation of the Dreamer algorithm ${ }^{21,22}$. The algorithm consists of three neural networks: the world model predicts the outcomes of potential actions, the critic judges the value of each outcome, and the actor chooses actions to reach the most valuable outcomes. The components are trained concurrently from replayed experience while the agent interacts with the environment. To succeed across domains, all three components need to accommodate different signal magnitudes and robustly balance terms in their objectives. This is challenging as we are not only targeting similar tasks within the same domain but aim to learn across diverse domains with fixed hyperparameters. This section introduces the world model, critic, and actor along with their robust loss functions, as well as tools for robustly predicting quantities of unknown orders of magnitude.</p>
<h2>World model learning</h2>
<p>The world model learns compact representations of sensory inputs through autoencoding ${ }^{23}$ and enables planning by predicting future representations and rewards for potential actions. We implement the world model as a Recurrent State-Space Model (RSSM) ${ }^{24}$, shown in Figure 3. First, an encoder maps sensory inputs $x_{t}$ to stochastic representations $z_{t}$. Then, a sequence model with recurrent state $h_{t}$ predicts the sequence of these representations given past actions $a_{t-1}$. The concatenation of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Multi-step video predictions of a DMLab maze (top) and a quadrupedal robot (bottom). Given 5 context images and the full action sequence, the model predicts 45 frames into the future without access to intermediate images. The world model learns an understanding of the underlying structure of each environment.
$h_{t}$ and $z_{t}$ forms the model state from which we predict rewards $r_{t}$ and episode continuation flags $c_{t} \in{0,1}$ and reconstruct the inputs to ensure informative representations:</p>
<p>$$
\text { RSSM }\left{\begin{array}{ll}
\text { Sequence model: } &amp; h_{t}=f_{\phi}\left(h_{t-1}, z_{t-1}, a_{t-1}\right) \
\text { Encoder: } &amp; z_{t} \sim q_{\phi}\left(z_{t} \mid h_{t}, x_{t}\right) \
\text { Dynamics predictor: } &amp; \hat{z}<em _phi="\phi">{t} \sim p</em>}\left(\hat{z<em t="t">{t} \mid h</em>\right) \
\text { Reward predictor: } &amp; \hat{r}<em _phi="\phi">{t} \sim p</em>}\left(\hat{r<em t="t">{t} \mid h</em>\right) \
\text { Continue predictor: } &amp; \hat{c}}, z_{t<em _phi="\phi">{t} \sim p</em>}\left(\hat{c<em t="t">{t} \mid h</em>\right) \
\text { Decoder: } &amp; \hat{x}}, z_{t<em _phi="\phi">{t} \sim p</em>}\left(\hat{x<em t="t">{t} \mid h</em>\right)
\end{array}\right.
$$}, z_{t</p>
<p>Figure 4 visualizes long-term video predictions of the world world. The encoder and decoder use convolutional neural networks (CNN) for image inputs and multi-layer perceptrons (MLPs) for vector inputs. The dynamics, reward, and continue predictors are also MLPs. The representations are sampled from a vector of softmax distributions and we take straight-through gradients through the sampling step ${ }^{25,22}$. Given a sequence batch of inputs $x_{1: T}$, actions $a_{1: T}$, rewards $r_{1: T}$, and continuation flags $c_{1: T}$, the world model parameters $\phi$ are optimized end-to-end to minimize the prediction loss $\mathcal{L}<em _dyn="{dyn" _text="\text">{\text {pred }}$, the dynamics loss $\mathcal{L}</em>}}$, and the representation loss $\mathcal{L<em _pred="{pred" _text="\text">{\text {rep }}$ with corresponding loss weights $\beta</em>=0.1$ :}}=1, \beta_{\text {dyn }}=1$, and $\beta_{\text {rep }</p>
<p>$$
\mathcal{L}(\phi) \doteq \mathrm{E}<em _phi="\phi">{q</em>}}\left[\sum_{t=1}^{T}\left(\beta_{\text {pred }} \mathcal{L<em _dyn="{dyn" _text="\text">{\text {pred }}(\phi)+\beta</em>}} \mathcal{L<em _rep="{rep" _text="\text">{\text {dyn }}(\phi)+\beta</em>(\phi)\right)\right]
$$}} \mathcal{L}_{\text {rep }</p>
<p>The prediction loss trains the decoder and reward predictor via the symlog squared loss described later, and the continue predictor via logistic regression. The dynamics loss trains the sequence model to predict the next representation by minimizing the KL divergence between the predictor $p_{\phi}\left(z_{t} \mid h_{t}\right)$ and the next stochastic representation $q_{\phi}\left(z_{t} \mid h_{t}, x_{t}\right)$. The representation loss, in turn, trains the representations to become more predictable allowing us to use a factorized dynamics predictor for fast sampling during imagination training. The two losses differ in the stop-gradient operator $\operatorname{sg}(\cdot)$ and their loss scale. To avoid a degenerate solution where the dynamics are trivial to predict but fail</p>
<p>to contain enough information about the inputs, we employ free bits ${ }^{26}$ by clipping the dynamics and representation losses below the value of 1 nat $\approx 1.44$ bits. This disables them while they are already minimized well to focus learning on the prediction loss:</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _phi="\phi">{\text {pred }}(\phi) &amp; \doteq-\ln p</em>\right) \
\mathcal{L}}\left(x_{t} \mid z_{t}, h_{t}\right)-\ln p_{\phi}\left(r_{t} \mid z_{t}, h_{t}\right)-\ln p_{\phi}\left(c_{t} \mid z_{t}, h_{t<em _phi="\phi">{\text {dyn }}(\phi) &amp; \doteq \max \left(1, \mathrm{KL}\left[\operatorname{sg}\left(q</em>\right)\right]\right) \
\mathcal{L}}\left(z_{t} \mid h_{t}, x_{t}\right)\right) | \quad p_{\phi}\left(z_{t} \mid h_{t<em _phi="\phi">{\text {rep }}(\phi) &amp; \doteq \max \left(1, \mathrm{KL}\left[\quad q</em>\right)\right)\right]\right)
\end{aligned}
$$}\left(z_{t} \mid h_{t}, x_{t}\right) | \operatorname{sg}\left(p_{\phi}\left(z_{t} \mid h_{t</p>
<p>Previous world models require scaling the representation loss differently based on the visual complexity of the environment ${ }^{21}$. Complex 3D environments contain details unnecessary for control and thus prompt a stronger regularizer to simplify the representations and make them more predictable. In games with static backgrounds and where individual pixels may matter for the task, a weak regularizer is required to extract fine details. We find that combining free bits with a small representation loss resolves this dilemma, allowing for fixed hyperparameters across domains. Moreover, we transform vector observations using the symlog function described later, to prevent large inputs and large reconstruction gradients, further stabilizing the trade-off with the representation loss.</p>
<p>We occasionally observed spikes the in KL losses in earlier experiments, consistent with reports for deep variational autoencoders ${ }^{27}$. To prevent this, we parameterize the categorical distributions of the encoder and dynamics predictor as mixtures of $1 \%$ uniform and $99 \%$ neural network output, making it impossible for them to become deterministic and thus ensuring well-behaved KL losses. Further model details and hyperparameters are included in the supplementary material.</p>
<h1>Critic learning</h1>
<p>The actor and critic neural networks learn behaviors purely from abstract trajectories of representations predicted by the world model ${ }^{14}$. For environment interaction, we select actions by sampling from the actor network without lookahead planning. The actor and critic operate on model states $s_{t} \doteq\left{h_{t}, z_{t}\right}$ and thus benefit from the Markovian representations learned by the recurrent world model. The actor aims to maximize the return $R_{t} \doteq \sum_{\tau=0}^{\infty} \gamma^{\tau} r_{t+\tau}$ with a discount factor $\gamma=0.997$ for each model state. To consider rewards beyond the prediction horizon $T=16$, the critic learns to approximate the distribution of returns ${ }^{28}$ for each state under the current actor behavior:</p>
<p>$$
\text { Actor: } \quad a_{t} \sim \pi_{\theta}\left(a_{t} \mid s_{t}\right) \quad \text { Critic: } \quad v_{\psi}\left(R_{t} \mid s_{t}\right)
$$</p>
<p>Starting from representations of replayed inputs, the world model and actor generate a trajectory of imagined model states $s_{1: T}$, actions $a_{1: T}$, rewards $r_{1: T}$, and continuation flags $c_{1: T}$. Because the critic predicts a distribution, we read out its predicted values $v_{t} \doteq \mathrm{E}\left[v_{\psi}(\cdot \mid s_{t})\right]$ as the expectation of the distribution. To estimate returns that consider rewards beyond the prediction horizon, we compute bootstrapped $\lambda$-returns ${ }^{29}$ that integrate the predicted rewards and the values. The critic learns to predict the distribution of the return estimates $R_{t}^{\lambda}$ using the maximum likelihood loss:</p>
<p>$$
\mathcal{L}(\psi) \doteq-\sum_{t=1}^{T} \ln p_{\psi}\left(R_{t}^{\lambda} \mid s_{t}\right) \quad R_{t}^{\lambda} \doteq r_{t}+\gamma c_{t}\left((1-\lambda) v_{t}+\lambda R_{t+1}^{\lambda}\right) \quad R_{T}^{\lambda} \doteq v_{T}
$$</p>
<p>While a simple choice would be to parameterize the critic as a Normal distribution, the return distribution can have multiple modes and vary by orders of magnitude across environments. To stabilize and accelerate learning under these conditions, we parameterize the critic as categorical distribution with exponentially spaced bins, decoupling the scale of gradients from the prediction</p>
<p>targets as described later. To improve value prediction in environments where rewards are challenging to predict, we apply the critic loss both to imagined trajectories with loss scale $\beta_{\text {val }}=1$ and to trajectories sampled from the replay buffer with loss scale $\beta_{\text {repval }}=0.3$. The critic replay loss uses the imagination returns $R_{t}^{\lambda}$ at the start states of the imagination rollouts as on-policy value annotations for the replay trajectory to then compute $\lambda$-returns over the replay rewards.
Because the critic regresses targets that depend on its own predictions, we stabilize learning by regularizing the critic towards predicting the outputs of an exponentially moving average of its own parameters. This is similar to target networks used previously in reinforcement learning ${ }^{7}$ but allows us to compute returns using the current critic network. We further noticed that the randomly initialized reward predictor and critic networks at the start of training can result in large predicted rewards that can delay the onset of learning. We thus initialize the output weight matrix of the reward predictor and critic to zeros, which alleviates the problem and accelerates early learning.</p>
<h1>Actor learning</h1>
<p>The actor learns to choose actions that maximize return while exploring through an entropy regularizer ${ }^{30}$. However, the correct scale for this regularizer depends both on the scale and frequency of rewards in the environment. Ideally, we would like the agent to explore more if rewards are sparse and exploit more if rewards are dense or nearby. At the same time, the exploration amount should not be influenced by arbitrary scaling of rewards in the environment. This requires normalizing the return scale while preserving information about reward frequency.
To use a fixed entropy scale of $\eta=3 \times 10^{-4}$ across domains, we normalize returns to be approximately contained in the interval $[0,1]$. In practice, substracting an offset from the returns does not change the actor gradient and thus dividing by the range $S$ is sufficient. Moreover, to avoid amplifying noise from function approximation under sparse rewards, we only scale down large return magnitudes but leave small returns below the threshold of $L=1$ untouched. We use the Reinforce estimator ${ }^{31}$ for both discrete and continuous actions, resulting in the surrogate loss function:</p>
<p>$$
\mathcal{L}(\theta) \doteq-\sum_{t=1}^{T} \operatorname{sg}\left(\left(R_{t}^{\lambda}-v_{\psi}\left(s_{t}\right)\right) / \max (1, S)\right) \log \pi_{\theta}\left(a_{t} \mid s_{t}\right)+\eta \mathrm{H}\left[\pi_{\theta}\left(a_{t} \mid s_{t}\right)\right]
$$</p>
<p>The return distribution can be multi-modal and include outliers, especially for randomized environments where some episodes have higher achievable returns than others. Normalizing by the smallest and largest observed returns would then scale returns down too much and may cause suboptimal convergence. To be robust to these outliers, we compute the range from the $5^{\text {th }}$ to the $95^{\text {th }}$ return percentile over the return batch and smooth out the estimate using an exponential moving average:</p>
<p>$$
S \doteq \operatorname{EMA}\left(\operatorname{Per}\left(R_{t}^{\lambda}, 95\right)-\operatorname{Per}\left(R_{t}^{\lambda}, 5\right), 0.99\right)
$$</p>
<p>Previous work typically normalizes advantages ${ }^{5}$ rather than returns, which puts a fixed amount of emphasis on maximizing returns over entropy regardless of whether rewards are within reach. Scaling up advantages when rewards are sparse can amplify noise that outweighs the entropy regularizer and stagnates exploration. Normalizing rewards or returns by standard deviation can fail under sparse rewards where their standard deviation is near zero, drastically amplifying rewards regardless of their size. Constrained optimization targets a fixed entropy on average across states ${ }^{32,33}$ regardless of achievable returns, which is robust but explores slowly under sparse rewards and converges lower under dense rewards. We did not find stable hyperparameters across domains for these approaches. Return normalization with a denominator limit overcomes these challenges, exploring rapidly under sparse rewards and converging to high performance across diverse domains.</p>
<h1>Robust predictions</h1>
<p>Reconstructing inputs and predicting rewards and returns can be challenging because the scale of these quantities can vary across domains. Predicting large targets using a squared loss can lead to divergence whereas absolute and Huber losses ${ }^{7}$ stagnate learning. On the other hand, normalizing targets based on running statistics ${ }^{5}$ introduces non-stationarity into the optimization. We suggest the symlog squared error as a simple solution to this dilemma. For this, a neural network $f(x, \theta)$ with inputs $x$ and parameters $\theta$ learns to predict a transformed version of its targets $y$. To read out predictions $\hat{y}$ of the network, we apply the inverse transformation:</p>
<p>$$
\mathcal{L}(\theta) \doteq \frac{1}{2}(f(x, \theta)-\operatorname{symlog}(y))^{2} \quad \hat{y} \doteq \operatorname{symexp}(f(x, \theta))
$$</p>
<p>Using the logarithm as transformation would not allow us to predict targets that take on negative values. Therefore, we choose a function from the bi-symmetric logarithmic family ${ }^{34}$ that we name symlog as the transformation with the symexp function as its inverse:</p>
<p>$$
\operatorname{symlog}(x) \doteq \operatorname{sign}(x) \ln (|x|+1) \quad \operatorname{symexp}(x) \doteq \operatorname{sign}(x)(\exp (|x|)-1)
$$</p>
<p>The symlog function compresses the magnitudes of both large positive and negative values. Unlike the logarithm, it is symmetric around the origin while preserving the input sign. This allows the optimization process to quickly move the network predictions to large values when needed. The symlog function approximates the identity around the origin so that it does not affect learning of targets that are already small enough.
For potentially stochastic targets, such as rewards or returns, we introduce the symexp twohot loss. Here, the network outputs the logits for a softmax distribution over exponentially spaced bins $b_{i} \in B$. Predictions are read out as the weighted average of the bin positions weighted by their predicted probabilities. Importantly, the network can output any continuous value in the interval because the weighted average can fall between the buckets:</p>
<p>$$
\hat{y} \doteq \operatorname{softmax}(f(x))^{T} B \quad B \doteq \operatorname{symexp}\left(\left[\begin{array}{lll}
-20 &amp; \ldots &amp; +20
\end{array}\right]\right)
$$</p>
<p>The network is trained on twohot encoded targets ${ }^{8,28}$, a generalization of onehot encoding to continuous values. The twohot encoding of a scalar is a vector with $|B|$ entries that are all 0 except at the indices $k$ and $k+1$ of the two bins closest to the encoded scalar. The two entries sum up to 1 , with linearly higher weight given to the bin that is closer to the encoded continuous number. The network is then trained to minimize the categorical cross entropy loss for classification with soft targets. Note that the loss only depends on the probabilities assigned to the bins but not on the continuous values associated with the bin locations, decoupling the size of the gradients from the size of the targets:</p>
<p>$$
\mathcal{L}(\theta) \doteq-\operatorname{twohot}(y)^{T} \log \operatorname{softmax}(f(x, \theta))
$$</p>
<p>Applying these principles, Dreamer transforms vector observations using the symlog functions, both for the encoder inputs and the decoder targets and employs the synexp twohot loss for the reward predictor and critic. We find that these techniques enable robust and fast learning across many diverse domains. For critic learning, an alternative asymmetric transformation has previously been proposed ${ }^{35}$, which we found less effective on average across domains. Unlike alternatives, symlog transformations avoid truncating large targets ${ }^{7}$, introducing non-stationary from normalization ${ }^{5}$, or adjusting network weights when new extreme values are detected ${ }^{36}$.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Fraction of trained agents that discover each of the three latest items in the Minecraft Diamond task. Although previous algorithms progress up to the iron pickaxe, Dreamer is the only compared algorithm that manages to discover a diamond, and does so reliably.</p>
<h1>Results</h1>
<p>We evaluate the generality of Dreamer across 8 domains-with over 150 tasks-under fixed hyperparameters. We designed the experiments to compare Dreamer to the best methods in the literature, which are often specifically designed and tuned for the benchmark at hand. We further compare to a high-quality implementation of $\mathrm{PPO}^{5}$, a standard reinforcement learning algorithm that is known for its robustness. We run PPO with fixed hyperparameters chosen to maximize performance across domains and that reproduce strong published results of PPO on ProcGen ${ }^{37}$. To push the boundaries of reinforcement learning, we apply Dreamer to the challenging video game Minecraft, comparing it to strong previous algorithms. Finally, we analyze the importance of individual components of Dreamer and its robustness to different model sizes and computational budgets. All Dreamer agents are trained on a single Nvidia A100 GPU each, making it reproducible for many research labs. A public implementation of Dreamer that reproduces all results is available on the project website.</p>
<p>Benchmarks We perform an extensive empirical study across 8 domains that include continuous and discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward scales, 2D and 3D worlds, and procedural generation. Figure 1 summarizes the benchmark results, showing that Dreamer outperforms a wide range of previous expert algorithms across diverse domains. Crucially, Dreamer substantially outperforms PPO across all domains.</p>
<ul>
<li>Atari This established benchmark contains 57 Atari 2600 games with a budget of 200M frames, posing a diverse range of challenges ${ }^{38}$. We use the sticky action simulator setting ${ }^{39}$. Dreamer outperforms the powerful MuZero algorithm ${ }^{8}$ while using only a fraction of the computational resources. Dreamer also outperforms the widely-used expert algorithms Rainbow ${ }^{40}$ and IQN ${ }^{41}$.</li>
<li>ProcGen This benchmark of 16 games features randomized levels and visual distractions to test the robustness and generalization of agents ${ }^{42}$. Within the budget of 50M frames, Dreamer matches the tuned expert algorithm $\mathrm{PPG}^{37}$ and outperforms Rainbow ${ }^{42,40}$. Our PPO agent with fixed hyperparameters matches the published score of the highly tuned official PPO implementation ${ }^{37}$.</li>
<li>DMLab This suite of 30 tasks features 3D environments that test spatial and temporal reasoning ${ }^{43}$. In 100M frames, Dreamer exceeds the performance of the scalable IMPALA and R2D2+ agents ${ }^{35}$ at 1B environment steps, amounting to a data-efficiency gain of over $1000 \%$. We note that these baselines were not designed for data-efficiency but serve as a valuable comparison point for the performance previously achievable at scale.</li>
</ul>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Ablations and robust scaling of Dreamer. a, All individual robustness techniques contribute to the performance of Dreamer on average, although each individual technique may only affect some tasks. Training curves of individual tasks are included in the supplementary material. b, The performance of Dreamer predominantly rests on the unsupervised reconstruction loss of its world model, unlike most prior algorithms that rely predominantly on reward and value prediction gradients ${ }^{7,5,8}$. c, The performance of Dreamer increases monotonically with larger model sizes, ranging from 12 M to 400 M parameters. Notably, larger models not only increase task performance but also require less environment interaction. d, Higher replay ratios predictably increase the performance of Dreamer. Together with model size, this allows practitioners to improve task performance and data-efficiency by employing more computational resources.</p>
<ul>
<li>Atari100k This data-efficiency benchmark comntains 26 Atari games and a budget of only 400 K frames, amounting to 2 hours of game time ${ }^{17}$. EfficientZero ${ }^{44}$ holds the state-of-the-art by combining online tree search, prioritized replay, and hyperparameter scheduling, but also resets levels early to increase data diversity, making a comparison difficult. Without this complexity, Dreamer outperforms the best remaining methods, including the transformer-based IRIS and TWM agents, the model-free SPR, and SimPLe ${ }^{45}$.</li>
<li>Proprio Control This benchmark contains 18 control tasks with continuous actions, proprioceptive vector inputs, and a budget of 500 K environment steps ${ }^{46}$. The tasks range from classical control over locomotion to robot manipulation tasks, featuring dense and sparse rewards. Dreamer sets a new state-of-the-art on this benchmark, outperforming D4PG, DMPO, and MPO ${ }^{33}$.</li>
<li>
<p>Visual Control This benchmark consists of 20 continuous control tasks where the agent receives only high-dimensional images as input and has a budget of 1 M environment steps ${ }^{46}$. Dreamer establishes a new state-of-the-art on this benchmark, outperforming DrQ-v2 and CURL ${ }^{47}$, which are specialized to visual environments and leverage data augmentation.</p>
</li>
<li>
<p>BSuite This benchmark includes 23 environments with a total of 468 configurations that are specifically designed to test credit assignment, robustness to reward scale and stochasticity, memory, generalization, and exploration ${ }^{48}$. Dreamer establishes a new state-of-the-art on this benchmark, outperforming Boot DQN and other methods ${ }^{49}$. Dreamer improves over previous algorithms especially in the scale robustness category.</p>
</li>
</ul>
<p>Minecraft Collecting diamonds in the popular game Minecraft has been a long-standing challenge in artificial intelligence ${ }^{18,19,20}$. Every episode in this game is set in a unique randomly generated and infinite 3D world. Episodes last until the player dies or up to 36000 steps equaling 30 minutes, during which the player needs to discover a sequence of 12 items from sparse rewards by foraging for resources and crafting tools. It takes about 20 minutes for experienced human players to obtain diamonds ${ }^{20}$. We follow the block breaking setting of prior work ${ }^{19}$ because the provided action space would make it challenging for stochastic policies to keep a key pressed for a prolonged time.
Because of the training time in this complex domain, extensive tuning would be difficult for Minecraft. Instead, we apply Dreamer out of the box with its default hyperparameters. As shown in Figures 1 and 5, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without using human data as was required by $\mathrm{VPT}^{20}$ or adaptive curricula ${ }^{19}$. All the Dreamer agents we trained on Minecraft discover diamonds in 100M environment steps. While several strong baselines progress to advanced items such as the iron pickaxe, none of them discovers a diamond.
Ablations In Figure 6, we ablate the robustness techniques and learning signals on a diverse set of 14 tasks to understand their importance. The training curves of individual tasks are included in the supplementary material. We observe that all robustness techniques contribute to performance, most notably the KL objective of the world model, followed by return normalization and symexp twohot regression for reward and value prediction. In general, we find that each individual technique is critical on a subset of tasks but may not affect performance on other tasks.
To investigate the effect of the world model, we ablate the learning signals of Dreamer by stopping either the task-specific reward and value prediction gradients or the task-agnostic reconstruction gradients from shaping its representations. Unlike previous reinforcement learning algorithms that often rely only on task-specific learning signals ${ }^{7,8}$, Dreamer rests predominantly on the unsupervised objective of its world model. This finding could allow for future algorithm variants that leverage pretraining on unsupervised data.
Scaling properties To investigate whether Dreamer can scale robustly, we train 6 model sizes ranging from 12 M to 400 M parameters, as well as different replay ratios on Crafter ${ }^{50}$ and a DMLab task ${ }^{43}$. The replay ratio affects the number of gradient updates performed by the agent. Figure 6 shows robust learning with fixed hyperparameters across the compared model sizes and replay ratios. Moreover, increasing the model size directly translates to both higher task performance and a lower data requirement. Increasing the number of gradient steps further reduces the interactions needed to learn successful behaviors. The results show that Dreamer learns robustly across model sizes and replay ratios and that its performance and provides a predictable way for increasing performance given computational resources.</p>
<h1>Previous work</h1>
<p>Developing general-purpose algorithms has long been a goal of reinforcement learning research. $\mathrm{PPO}^{3}$ is one of the most widely used algorithms and is relatively robust but requires large amounts of experience and often yields lower performance than specialized alternatives. SAC $^{32}$ is a popular choice for continuous control and leverages experience replay for data-efficiency, but in practice requires tuning, especially for its entropy scale, and struggles under high-dimensional inputs ${ }^{51}$. MuZero ${ }^{8}$ plans using a value prediction model and has been applied to board games and Atari, but the authors did not release an implementation and the algorithm contains several complex components, making it challenging to reproduce. Gato ${ }^{52}$ fits one large model to expert demonstrations of multiple tasks, but is only applicable when expert data is available. In comparison, Dreamer masters a diverse range of environments with fixed hyperparameters, does not require expert data, and its implementation is open source.
Minecraft has been a focus of recent research. With MALMO ${ }^{53}$, Microsoft released a free version of the successful game for research purposes. MineRL ${ }^{18}$ offers several competition environments, which we rely on as the basis for our experiments. The MineRL competition supports agents in exploring and learning meaningful skills through a diverse human dataset ${ }^{18}$. Voyager obtains items at a similar depth in the technology tree as Dreamer using API calls to a language model but operates on top of the MineFlayer bot scripting layer that was specifically engineered to the game and exposes high-level actions ${ }^{54}$. VPT $^{20}$ trained an agent to play Minecraft through behavioral cloning based on expert data of keyboard and mouse actions collected by contractors and finetuning using reinforcement learning to obtain diamonds using 720 GPUs for 9 days. In comparison, Dreamer uses the MineRL competition action space to autonomously learn to collect diamonds from sparse rewards using 1 GPU for 9 days, without human data.</p>
<h2>Conclusion</h2>
<p>We present the third generation of the Dreamer algorithm, a general reinforcement learning algorithm that masters a wide range of domains with fixed hyperparameters. Dreamer excels not only across over 150 tasks but also learns robustly across varying data and compute budgets, moving reinforcement learning toward a wide range of practical applications. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch, achieving a significant milestone in the field of artificial intelligence. As a high-performing algorithm that is based on a learned world model, Dreamer paves the way for future research directions, including teaching agents world knowledge from internet videos and learning a single world model across domains to allow artificial agents to build up increasingly general knowledge and competency.
Acknowledgements We thank Mohammad Norouzi, Jessy Lin, Abbas Abdolmaleki, John Schulman, Adam Kosiorek, and Oleh Rybkin for insightful discussions. We thank Bobak Shahriari, Denis Yarats, Karl Cobbe, and Hubert Soyer for sharing training curves of baseline algorithms. We thank Daniel Furrer, Andrew Chen, and Dakshesh Garambha for help with Google Cloud infrastructure.</p>
<h1>References</h1>
<ol>
<li>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587): 484, 2016.</li>
<li>OpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018.</li>
<li>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730-27744, 2022.</li>
<li>Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314-21328, 2022.</li>
<li>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</li>
<li>Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</li>
<li>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</li>
<li>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. arXiv preprint arXiv:1911.08265, 2019.</li>
<li>Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.</li>
<li>Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre Côté, and R Devon Hjelm. Unsupervised state representation learning in atari. Advances in neural information processing systems, 32, 2019.</li>
<li>Danny Driess, Ingmar Schubert, Pete Florence, Yunzhu Li, and Marc Toussaint. Reinforcement learning with neural radiance fields. arXiv preprint arXiv:2206.01634, 2022.</li>
<li>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354, 2017.</li>
<li>Marcin Andrychowicz, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in on-policy reinforcement learning? a large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020.</li>
<li>
<p>Richard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM SIGART Bulletin, 2(4):160-163, 1991.</p>
</li>
<li>
<p>Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 2786-2793. IEEE, 2017.</p>
</li>
<li>David Ha and Jürgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.</li>
<li>Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Modelbased reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</li>
<li>William H Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, et al. The minerl competition on sample efficient reinforcement learning using human priors. arXiv e-prints, pages arXiv-1904, 2019.</li>
<li>Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, et al. Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint arXiv:2106.14876, 2021.</li>
<li>Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv:2206.11795, 2022.</li>
<li>Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</li>
<li>Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.</li>
<li>Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</li>
<li>Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551, 2018.</li>
<li>Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.</li>
<li>Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. Advances in neural information processing systems, 29, 2016.</li>
<li>Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. arXiv preprint arXiv:2011.10650, 2020.</li>
<li>Marc G Bellemare, Will Dabney, and Rémi Munos. A distributional perspective on reinforcement learning. In International Conference on Machine Learning, pages 449-458. PMLR, 2017.</li>
<li>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</li>
<li>
<p>Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241-268, 1991.</p>
</li>
<li>
<p>Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256, 1992.</p>
</li>
<li>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Offpolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290, 2018.</li>
<li>Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.</li>
<li>J Beau W Webber. A bi-symmetric log transformation for wide-range data. Measurement Science and Technology, 24(2):027001, 2012.</li>
<li>Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations, 2018.</li>
<li>Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van Hasselt. Multi-task deep reinforcement learning with popart. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3796-3803, 2019.</li>
<li>Karl W Cobbe, Jacob Hilton, Oleg Klimov, and John Schulman. Phasic policy gradient. In International Conference on Machine Learning, pages 2020-2027. PMLR, 2021.</li>
<li>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</li>
<li>Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.</li>
<li>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.</li>
<li>Will Dabney, Georg Ostrovski, David Silver, and Rémi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pages 1096-1105. PMLR, 2018.</li>
<li>Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pages 2048-2056. PMLR, 2020.</li>
<li>Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016.</li>
<li>Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, and Yang Gao. Mastering atari games with limited data. Advances in Neural Information Processing Systems, 34:25476-25488, 2021.</li>
<li>
<p>Vincent Micheli, Eloi Alonso, and François Fleuret. Transformers are sample efficient world models. arXiv preprint arXiv:2209.00588, 2022.</p>
</li>
<li>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
</li>
<li>Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.</li>
<li>Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.</li>
<li>Olivia Dizon-Paradis, Stephen Wormald, Daniel Capecci, Avanti Bhandarkar, and Damon Woodard. Investigating the practicality of existing reinforcement learning algorithms: A performance comparison. Authorea Preprints, 2023.</li>
<li>Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021.</li>
<li>Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.</li>
<li>Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.</li>
<li>Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, pages 4246-4247. Citeseer, 2016.</li>
<li>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023.</li>
<li>Shengyi Huang, Rousslan Fernand Julien Dossa, Antonin Raffin, Anssi Kanervisto, and Weixun Wang. The 37 implementation details of proximal policy optimization. The ICLR Blog Track 2023, 2022.</li>
<li>Matt Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, et al. Acme: A research framework for distributed reinforcement learning. arXiv preprint arXiv:2006.00979, 2020.</li>
<li>Simon Schmitt, Matteo Hessel, and Karen Simonyan. Off-policy actor-critic with shared experience replay. In International Conference on Machine Learning, pages 8545-8554. PMLR, 2020.</li>
<li>Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.</li>
<li>Andy Brock, Soham De, Samuel L Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In International Conference on Machine Learning, pages 1059-1071. PMLR, 2021.</li>
<li>
<p>Liu Ziyin, Zhikang T Wang, and Masahito Ueda. Laprop: Separating momentum and adaptivity in adam. arXiv preprint arXiv:2002.04839, 2020.</p>
</li>
<li>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
</li>
<li>Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learning. arXiv preprint arXiv:1704.04651, 2017.</li>
<li>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</li>
<li>Matthijs Van Keirsbilck, Alexander Keller, and Xiaodong Yang. Rethinking full connectivity in recurrent neural networks. arXiv preprint arXiv:1905.12340, 2019.</li>
<li>Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.</li>
<li>Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. arXiv preprint arXiv:1802.01561, 2018.</li>
</ol>
<h1>Methods</h1>
<h2>Baselines</h2>
<p>We employ the Proximal Policy Optimization (PPO) algorithm ${ }^{5}$, which has become a standard choice in the field, to compare Dreamer under fixed hyperparameters across all benchmarks. There are a large number of PPO implementations available publicly and they are known to substantially vary in task performance ${ }^{55}$. To ensure a comparison that is representative of the highest performance PPO can achieve under fixed hyperparameters across domains, we choose the high-quality PPO implementation available in the Acme framework ${ }^{56}$ and select its hyperparameters in Table 1 following recommendations ${ }^{55,13}$ and additionally tune its epoch batch size to be large enough for complex environments ${ }^{42}$, its learning rate, and its entropy scale. We match the discount factor to Dreamer because it works well across domains and is a common choice in the literature ${ }^{35,8}$. We choose the IMPALA network architecture that we have found performed better than alternatives ${ }^{42}$ and set the minibatch size to the largest possible for one A100 GPU. We verify the performance of our PPO implementation and hyperparameters on the ProcGen benchmark, where a highly tuned PPO implementation has been reported by the PPO authors ${ }^{37}$. We find that our implementation matches or slightly outperforms this performance reference.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Observation normalization</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Reward normalization</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Reward clipping (stddev.)</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">Epoch batch</td>
<td style="text-align: center;">$64 \times 256$</td>
</tr>
<tr>
<td style="text-align: left;">Number of epochs</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">Minibatch size</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">Minibatch length</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">Policy trust region</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">Value trust region</td>
<td style="text-align: center;">No</td>
</tr>
<tr>
<td style="text-align: left;">Advantage normalization</td>
<td style="text-align: center;">Yes</td>
</tr>
<tr>
<td style="text-align: left;">Entropy penalty scale</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: left;">Discount factor</td>
<td style="text-align: center;">0.997</td>
</tr>
<tr>
<td style="text-align: left;">GAE lambda</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: center;">$3 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">Gradient clipping</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: left;">Adam epsilon</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: PPO hyperparameters used across all benchmarks.</p>
<p>For Minecraft, we additionally tune and run the IMPALA and Rainbow algorithms because not successful end-to-end learning from scratch has been reported in the literature ${ }^{18}$. We use the Acme implementations ${ }^{56}$ of these algorithms, use the same IMPALA network we used for PPO, and tuned the learning rate and entropy regularizers. For all other benchmarks, we compare to tuned expert algorithms reported in the literature as referenced in the results section.</p>
<h1>Implementation</h1>
<p>Experience replay We implement Dreamer using a uniform replay buffer with an online queue ${ }^{57}$. Specifically, each minibatch is formed first from non-overlapping online trajectories and then filled up with uniformly sampled trajectories from the replay buffer. We store latent states into the replay buffer during data collection to initialize the world model on replayed trajectories, and write the fresh latent states of the training rollout back into the buffer. While prioritized replay ${ }^{58}$ is used by some of the expert algorithms we compare to and we found it to also improve the performance of Dreamer, we opt for uniform replay in our experiments for ease of implementation.
We parameterize the amount of training via the replay ratio. This is the fraction of time steps trained on per time step collected from the environment, without action repeat. Dividing the replay ratio by the time steps in a minibatch and by action repeat yields the ratio of gradient steps to env steps. For example, a replay ratio of 32 on Atari with action repeat of 4 and batch shape $16 \times 64$ corresponds to 1 gradient step every 128 env steps, or 1.5 M gradient steps over 200 M env steps.
Optimizer We employ Adaptive Gradient Clipping (AGC) ${ }^{59}$, which clips per-tensor gradients if they exceed $30 \%$ of the L2 norm of the weight matrix they correspond to, with its default $\epsilon=10^{-3}$. AGC decouples the clipping threshold from the loss scales, allowing to change loss functions or loss scales without adjusting the clipping threshold. We apply the clipped gradients using the LaProp optimizer ${ }^{60}$ with $\epsilon=10^{-20}$ and its default parameters $\beta_{1}=0.9$ and $\beta_{2}=0.99$. LaProp normalizes gradients by RMSProp and then smoothes them by momentum, instead of computing both momentum and normalizer on raw gradients as Adam does ${ }^{61}$. This simple change allows for a smaller epsilon and avoids occasional instabilities that we observed under Adam.
Distributions The encoder, dynamics predictor, and actor distributions are mixtures of $99 \%$ the predicted softmax output and $1 \%$ of a uniform distribution ${ }^{62}$ to prevent zero probabilities and infinite log probabilities. The rewards and critic neural networks output a softmax distribution over exponentially spaced bins $b \in B$ and are trained towards twohot encoded targets:</p>
<p>$$
\operatorname{twohot}(x)<em k_1="k+1">{i} \doteq \begin{cases}\left|b</em>
$$}-x\right| /\left|b_{k+1}-b_{k}\right| &amp; \text { if } i=k \ \left|b_{k}-x\right| /\left|b_{k+1}-b_{k}\right| &amp; \text { if } i=k+1 \ 0 &amp; \text { else } \quad k \doteq \sum_{j=1}^{|B|} \delta\left(b_{j}&lt;x\right)\end{cases</p>
<p>The output weights of twohot distributions are initialized to zero to ensure that the agent does not hallucinate rewards and values at initialization. For computing the expected prediction of the softmax distribution under bins that span many orders of magnitude, the summation order matters and positive and negative bins should be summed up separately, from small to large bins, and then added. Refer to the source code for an implementation.
Networks Images are encoded using stride 2 convolutions to resolution $6 \times 6$ or $4 \times 4$ and then flattened and are decoded using transposed stride 2 convolutions, with sigmoid activation on the output. Vector inputs are symlog transformed and then encoded and decoded using 3-layer MLPs. The actor and critic neural networks are also 3-layer MLPs and the reward and continue predictors are 1-layer MLPs. The sequence model is a GRU ${ }^{63}$ with block-diagonal recurrent weights ${ }^{64}$ of 8 blocks to allow for a large number of memory units without quadratic increase in parameters and FLOPs. The input to the GRU at each time step is a linear embedding of the sampled latent $z_{t}$, of the action $a_{t}$, and of the recurrent state to allow mixing between blocks.</p>
<h1>Benchmarks</h1>
<p>Protocols Summarized in Table 2, we follow the standard evaluation protocols for the benchmarks where established. Atari ${ }^{38}$ uses 57 tasks with sticky actions ${ }^{65}$. The random and human reference scores used to normalize scores vary across the literature and we chose the most common reference values, replicated in Table 6. DMLab ${ }^{43}$ uses 30 tasks ${ }^{66}$ and we use the fixed action space ${ }^{36,35}$. We evaluate at 100 M steps because running for 10 B as in some prior work was infeasible. Because existing published baselines perform poorly at 100 M steps, we compare to their performance at 1 B steps instead, giving them a $10 \times$ data advantage. ProcGen uses the hard difficulty setting and the unlimited level set ${ }^{42}$. Prior work compares at different step budgets ${ }^{42,37}$ and we compare at 50M steps due to computational cost, as there is no action repeat. For Minecraft Diamond purely from sparse rewards, we establish the evaluation protocol to report the episode return measured at 100M env steps, corresponding to about 100 days of in-game time. Atari100k ${ }^{17}$ includes 26 tasks with a budget of 400 K env steps, 100 K after action repeat. Prior work has used various environment settings, summarized in Table 10, and we chose the environments as originally introduced. Visual Control ${ }^{46,21}$ spans 20 tasks with an action repeat of 2 . Proprioceptive Control follows the same protocol but we exclude the two quadruped tasks because of baseline availability in prior work ${ }^{47}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Benchmark</th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Env <br> Steps</th>
<th style="text-align: center;">Action <br> Repeat</th>
<th style="text-align: center;">Env <br> Instances</th>
<th style="text-align: center;">Replay <br> Ratio</th>
<th style="text-align: center;">GPU <br> Days</th>
<th style="text-align: center;">Model <br> Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Minecraft</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">100 M</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">DMLab</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">100 M</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">ProcGen</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">50 M</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">Atari</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">200 M</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">Atari100K</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">400 K</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">BSuite</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">200 M</td>
</tr>
<tr>
<td style="text-align: left;">Proprio Control</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">500 K</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">12 M</td>
</tr>
<tr>
<td style="text-align: left;">Visual Control</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">1 M</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">12 M</td>
</tr>
</tbody>
</table>
<p>Table 2: Benchmark overview. All agents were trained on a single Nvidia A100 GPU each.
Environment instances In earlier experiments, we observed that the performance of both Dreamer and PPO is robust to the number of environment instances. Based on the CPU resources available on our training machines, we use 16 environment instance by default. For BSuite, the benchmark requires using a single environment instance. We also use a single environment instance for Atari100K because the benchmark has a budget of 400 K env steps whereas the maximum episode length in Atari is in principle 432 K env steps. For Minecraft, we use 64 environments using remote CPU workers to speed up experiments because the environment is slower to step.
Seeds and error bars We run 5 seeds for each Dreamer and PPO per benchmark, with the exception of 1 seed for ProcGen due to computational constraints, 10 seeds for BSuite as required by the benchmark, and 10 seeds for Minecraft to reliably report the fraction of runs that achieve diamonds. All curves show the mean over seeds with one standard deviation shaded.
Computational choices All Dreamer and PPO agents in this paper were trained on a single Nvidia A100 GPU each. Dreamer uses the 200M model size by default. On the two control suitse, Dreamer the same performance using the substantially faster 12 M model, making it more accessible to researchers. The replay ratio control the trade-off between computational cost and data efficiency as analyzed in Figure 6 and is chosen to fit the step budget of each benchmark.</p>
<h1>Model sizes</h1>
<p>To accommodate different computational budgets and analyze robustness to different model sizes, we define a range of models ranging from 12 M to 400 M parameters shown in Table 3. The sizes are parameterized by the model dimension, which approximately increases in multiples of 1.5 , alternating between powers of two and power of two scaled by 1.5. This yields tensor shapes that are multiples of 8 as required for hardware efficiency. Sizes of different network components derive from the model dimension. The MLPs have the model dimension as the number of hidden units. The sequence model has 8 times the number of recurrent units, split into 8 blocks of the same size as the MLPs. The convolutional encoder and decoder layers closest to the data use $16 \times$ fewer channels than the model dimension. Each latent also uses $16 \times$ fewer codes than the model dimension. The number of hidden layers and number of latents is fixed across model sizes. All hyperparamters, including the learning rate and batch size, are fixed across model sizes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameters</th>
<th style="text-align: right;">$\mathbf{1 2 M}$</th>
<th style="text-align: right;">$\mathbf{2 5 M}$</th>
<th style="text-align: right;">$\mathbf{5 0 M}$</th>
<th style="text-align: right;">$\mathbf{1 0 0 M}$</th>
<th style="text-align: right;">$\mathbf{2 0 0 M}$</th>
<th style="text-align: right;">$\mathbf{4 0 0 M}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Hidden size $(d)$</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">384</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">768</td>
<td style="text-align: right;">1024</td>
<td style="text-align: right;">1536</td>
</tr>
<tr>
<td style="text-align: left;">Recurrent units $(8 d)$</td>
<td style="text-align: right;">1024</td>
<td style="text-align: right;">3072</td>
<td style="text-align: right;">4096</td>
<td style="text-align: right;">6144</td>
<td style="text-align: right;">8192</td>
<td style="text-align: right;">12288</td>
</tr>
<tr>
<td style="text-align: left;">Base CNN channels $(d / 16)$</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">96</td>
</tr>
<tr>
<td style="text-align: left;">Codes per latent $(d / 16)$</td>
<td style="text-align: right;">16</td>
<td style="text-align: right;">24</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">48</td>
<td style="text-align: right;">64</td>
<td style="text-align: right;">96</td>
</tr>
</tbody>
</table>
<p>Table 3: Dreamer model sizes. The number of MLP hidden units defines the model dimension, from which recurrent units, convolutional channels, and number of codes per latent are derived. The number of layers and latents is constant across model sizes.</p>
<h2>Previous Dreamer generations</h2>
<p>We present the third generation of the Dreamer line of work. Where the distinction is useful, we refer to this algorithm as DreamerV3. The DreamerV1 algorithm ${ }^{21}$ was limited to continuous control, the DreamerV2 algorithm ${ }^{22}$ surpassed human performance on Atari, and the DreamerV3 algorithm enables out-of-the-box learning across diverse benchmarks.
We summarize the changes that DreamerV3 introduces as follows:</p>
<ul>
<li>Robustness techniques: Observation symlog, KL balance and free bits, $1 \%$ unimix for all categoricals, percentile return normalization, symexp twohot loss for the reward head and critic.</li>
<li>Network architecture: Block GRU, RMSNorm normalization, SiLu activation.</li>
<li>Optimizer: Adaptive gradient clipping (AGC), LaProp (RMSProp followed by momentum).</li>
<li>Replay buffer: Larger capacity, online queue, storing and updating latent states.</li>
</ul>
<h1>Hyperparameters</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">Symbol</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">General</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Replay capacity</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$5 \times 10^{6}$</td>
</tr>
<tr>
<td style="text-align: center;">Batch size</td>
<td style="text-align: center;">B</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Batch length</td>
<td style="text-align: center;">T</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Activation</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">RMSNorm + SiLU</td>
</tr>
<tr>
<td style="text-align: center;">Learning rate</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$4 \times 10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Gradient clipping</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$\operatorname{AGC}(0.3)$</td>
</tr>
<tr>
<td style="text-align: center;">Optimizer</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$\operatorname{LaProp}\left(\epsilon=10^{-20}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">World Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Reconstruction loss scale</td>
<td style="text-align: center;">$\beta_{\text {pred }}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Dynamics loss scale</td>
<td style="text-align: center;">$\beta_{\text {dyn }}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Representation loss scale</td>
<td style="text-align: center;">$\beta_{\text {rep }}$</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">Latent unimix</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Free nats</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Actor Critic</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Imagination horizon</td>
<td style="text-align: center;">H</td>
<td style="text-align: center;">15</td>
</tr>
<tr>
<td style="text-align: center;">Discount horizon</td>
<td style="text-align: center;">$1 /(1-\gamma)$</td>
<td style="text-align: center;">333</td>
</tr>
<tr>
<td style="text-align: center;">Return lambda</td>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: center;">Critic loss scale</td>
<td style="text-align: center;">$\beta_{\text {val }}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Critic replay loss scale</td>
<td style="text-align: center;">$\beta_{\text {repval }}$</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">Critic EMA regularizer</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Critic EMA decay</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">0.98</td>
</tr>
<tr>
<td style="text-align: center;">Actor loss scale</td>
<td style="text-align: center;">$\beta_{\text {pol }}$</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Actor entropy regularizer</td>
<td style="text-align: center;">$\eta$</td>
<td style="text-align: center;">$3 \times 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">Actor unimix</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">$1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Actor RetNorm scale</td>
<td style="text-align: center;">$S$</td>
<td style="text-align: center;">$\operatorname{Per}(R, 95)-\operatorname{Per}(R, 5)$</td>
</tr>
<tr>
<td style="text-align: center;">Actor RetNorm limit</td>
<td style="text-align: center;">L</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">Actor RetNorm decay</td>
<td style="text-align: center;">—</td>
<td style="text-align: center;">0.99</td>
</tr>
</tbody>
</table>
<p>Table 4: Dreamer hyperparameters. The same values are used across all benchmarks, including proprioceptive and visual inputs, continuous and discrete actions, and 2D and 3D domains. We do not use any hyperparameter annealing, prioritized replay, weight decay, or dropout.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*The MineRL Diamond Competitions were held in 2019, 2020, and 2021 and provided a dataset of human expert trajectories: https://miner1.io/diamond. Competitions in the following years focused on a wide range of tasks.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1826 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1826</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1826</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-6f681faaa985ed38bc9b30777d57d9e1e3765861</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6f681faaa985ed38bc9b30777d57d9e1e3765861" target="_blank">History Aware Multimodal Transformer for Vision-and-Language Navigation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A History Aware Multimodal Transformer (HAMT) is introduced to incorporate a long-horizon history into multimodal decision making for vision-and-language navigation and achieves new state of the art on a broad range of VLN tasks.</p>
                <p><strong>Paper Abstract:</strong> Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1826.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1826.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal pretraining method that pretrains a transformer on instruction–single-step observation pairs (text + single panoramic observation) for downstream vision-and-language navigation (VLN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards learning a generic agent for vision-and-language navigation via pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A transformer-based multimodal model pretrained on pairs of navigation instructions and single-step visual observations (no trajectory history) to learn cross-modal alignment and single-step action prediction; later fine-tuned for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Instruction + single-step visual observations (vision-language pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretraining uses pairs of natural language navigation instructions and single-step panoramic observations (instruction–observation pairs). The HAMT paper describes PREVALENT as pretraining on instruction and single-step observations without trajectory history; the original PREVALENT paper contains full dataset/scale details (not fully enumerated in HAMT).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R and related VLN benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D indoor navigation tasks where an agent receives a natural language instruction and must navigate a discretized connectivity graph of panoramas (Matterport3D-based R2R family), selecting navigable viewpoints at each step to reach a target location.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Implicitly: next-step action selection derived from instruction semantics (single-step next-view selection; pretraining pairs map instruction to correct next observation).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation actions over candidate navigable panoramic viewpoints (select one of K candidate views each step, plus stop action), with angular headings/elevations for chosen view.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pretraining on instruction–single-step observation pairs teaches the model to map instruction semantics to the correct next-view selection (single-step action prediction). At finetuning, the same next-view prediction head or policy is used to select navigable viewpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic views (36 views per panorama in HAMT evaluation); relative angle encodings; pretrained visual features (PREVALENT uses visual features paired with text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported PREVALENT baseline numbers in HAMT's Table 5: Validation Seen SR 69%, SPL 65; Validation Unseen SR 58%, SPL 53; Test Unseen SR 54%, SPL 51 (units: %).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Direct alignment between instruction tokens and single-step observations; learning cross-modal grounding between language and visual observations during pretraining improves single-step action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Pretraining without trajectory history (no long-horizon context) limits learning of history-aware navigation and can encourage overfitting to seen environment structures; limited visual optimization if visual encoder not end-to-end tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining on instruction–single-step observation pairs provides positive transfer to VLN by improving single-step alignment between language and visual observations, but absence of trajectory history in pretraining can limit generalization for long-horizon/history-dependent navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History Aware Multimodal Transformer for Vision-and-Language Navigation', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1826.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1826.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRESS (BERT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRESS (uses pretrained BERT instruction encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLN model that replaces LSTM-based instruction encoders with a pretrained BERT language encoder to improve language representations for navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust navigation with language pretraining and stochastic sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PRESS (BERT as instruction encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A vision-and-language navigation architecture that uses a pretrained BERT language encoder to produce richer contextual instruction embeddings, which are fused with visual features for action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale language corpora (BERT language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained BERT encoder (as in the BERT paper) — pretrained on large text corpora (e.g., BookCorpus and Wikipedia per the BERT reference). HAMT cites PRESS as using pretrained BERT to improve language representation; PRESS-specific pretraining dataset details are in PRESS paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R and similar VLN benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Discrete viewpoint navigation in 3D photo-realistic indoor environments guided by natural language instructions; agent selects navigable viewpoints at each step to follow instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions describing sequences of navigation actions and landmarks (no explicit low-level motor commands in text pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigable viewpoint selection and stop action (viewpoint-level navigation over graph).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pretrained BERT provides contextualized language embeddings; downstream action head is trained (or fine-tuned) to map those embeddings (in combination with visual inputs) to discrete next-view actions — effectively the mapping is learned during VLN finetuning rather than hand-coded.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic observations (visual features) and relative angle encodings; PRESS couples BERT text embeddings with visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>PRESS numbers reported in HAMT's Table 5 (baseline): Validation Seen SR 58%, SPL 55; Validation Unseen SR 49%, SPL 45; Test Unseen SR 49%, SPL 45 (units: %).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>High-quality, contextualized language representations from BERT reduce language-understanding errors and improve instruction grounding when combined with visual inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>BERT pretraining alone does not provide visual grounding or history modeling; improvements are limited if cross-modal alignment and visual features are weak or if trajectory history is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initializing instruction encoders with pretrained language models (BERT) improves VLN performance by providing stronger language representations, but must be combined with appropriate visual grounding and history modeling for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History Aware Multimodal Transformer for Vision-and-Language Navigation', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1826.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1826.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-and-language model that measures compatibility between an instruction and images in a path using image–text pretraining, but (as described) does not support sequential action prediction with history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving vision-and-language navigation with image-text pairs from the web.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>VLN-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A transformer-based VLN model pretrained on image–text pairs to learn image–text alignment for navigation tasks; designed to evaluate instruction–image compatibility along paths rather than to perform sequential action prediction with long history.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text pairs (web-derived image–text data)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>VLN-BERT pretrains on image–text pairs from the web to learn cross-modal representations; HAMT cites VLN-BERT as measuring compatibilities between instructions and images in a path but not supporting action prediction — detailed datasets and scale are in the VLN-BERT paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Vision-and-Language Navigation (R2R family and related tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Agents must follow textual navigation instructions in 3D indoor environments (discrete panoramic viewpoint graph); VLN-BERT targets alignment tasks and path compatibility evaluation rather than full history-based action policies.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions describing navigation; image–text pretraining does not include explicit action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete viewpoint selection and stop actions in the navigation graph (when used as part of a downstream navigation system).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>VLN-BERT learns image–text compatibility scores; mapping to actions for sequential navigation is not its primary function (HAMT notes it 'does not support action prediction').</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Image-level visual features (RGB panoramas or view images) paired with instruction text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale image–text pretraining improves cross-modal alignment and can assist downstream grounding tasks where instruction–image compatibility is key.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>If pretraining omits sequential/action prediction and trajectory history, the model may not transfer well to long-horizon, history-dependent embodied navigation policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Image–text pretraining helps cross-modal alignment, but methods that omit action prediction or history modeling are limited for sequential VLN where history and action sequencing matter.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History Aware Multimodal Transformer for Vision-and-Language Navigation', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1826.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1826.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multilingual BERT (init)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pretrained Multilingual BERT (used to initialize language encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained multilingual transformer language model used to initialize HAMT's language encoder for the multilingual RxR VLN dataset, providing multilingual language representations for navigation instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised cross-lingual representation learning at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Multilingual BERT (initialization for HAMT on RxR)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained multilingual transformer language model (used as an initialization for HAMT's unimodal language encoder) to handle multilingual instructions in RxR; provides contextual language embeddings across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Multilingual text corpora (cross-lingual language pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>HAMT states it uses pretrained multilingual BERT [59] to initialize the unimodal language encoder for RxR (three languages: English, Hindi, Telugu). The referenced multilingual model is pretrained on large multilingual corpora (see referenced paper for exact corpora and scale).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RxR (Room-to-Room multilingual VLN)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Multilingual VLN on Matterport3D where agents follow instructions in English, Hindi or Telugu to navigate indoor environments; requires cross-lingual understanding and grounding to visual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions in multiple languages describing navigation steps and landmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete navigation over candidate panoramic viewpoints and stop action.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pretrained multilingual embeddings provide language representations that are fused with visual features during downstream training; the mapping from language to actions is learned during HAMT finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic views (visual features) and relative angle encodings; multilingual instruction text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>HAMT (multilingual) results on RxR reported in HAMT: Val Seen SR 59.4%, SPL 58.9%, nDTW 65.3%, SDTW 50.9%; Val Unseen SR 56.5%, SPL 56.0% (units: %). HAMT used multilingual BERT to initialize language encoder for these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Language-model initialization provides robust multilingual contextual embeddings which ease downstream learning and generalization across languages.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Language pretraining does not supply visual grounding or history modeling; success depends on complementary multimodal pretraining and finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Initializing VLN agents with multilingual pretrained language models enables handling of multilingual instructions and contributes to strong performance on multilingual embodied navigation benchmarks when combined with appropriate visual grounding and finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History Aware Multimodal Transformer for Vision-and-Language Navigation', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1826.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1826.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (features)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Contrastive Language–Image Pretraining) features</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large-scale image–text contrastive model pretrained on web image–text pairs; leaderboard methods using CLIP features (CLIP-ViL, CLEAR-CLIP) show strong VLN performance when their visual backbones use CLIP-derived embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP (used as pretrained visual-text embedding provider for some RxR leaderboard methods)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A contrastive image–text pretraining model that produces joint visual-text embeddings; several RxR leaderboard entries use CLIP-derived visual features (fixed, not end-to-end optimized in HAMT evaluations) to improve VLN performance.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Large-scale image–text pairs from the web (contrastive image–text pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>CLIP was pretrained on hundreds of millions of image–text pairs (see CLIP paper); HAMT notes that RxR leaderboard methods CLIP-ViL and CLEAR-CLIP use CLIP features and reports their RxR test numbers (CLIP-ViL and CLEAR-CLIP scores in HAMT Table 12). HAMT used the same visual features for fair comparison (without end-to-end optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>RxR (multilingual VLN) and other VLN benchmarks where CLIP features have been adopted by competing entries</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Discretized viewpoint navigation in Matterport3D environments, following multilingual natural language instructions; agents rely on CLIP-derived visual features for perception.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions (text) used as supervision; CLIP pretraining itself does not include action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete viewpoint selection among candidate navigable views and stop action for VLN.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>CLIP provides image–text-aligned embeddings that are used as visual inputs; downstream VLN models learn to map fused CLIP visual/text features to navigation actions during VLN finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic images represented via CLIP visual embeddings; relative angle encodings and other navigation-specific features added by downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>RxR test-split numbers reported in HAMT (Table 12): CLIP-ViL SR 38.34%, SPL 35.17%, nDTW 51.10%; CLEAR-CLIP SR 40.29%, SPL 36.57%, nDTW 53.69% (units: %).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP's image–text alignment provides strong visual features already grounded in language semantics, improving down-stream multimodal grounding and navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>CLIP pretraining is not navigation-specific and lacks trajectory/history reasoning; using fixed CLIP features without end-to-end adaptation can limit performance in tasks requiring fine-grained object grounding or domain-specific visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using image–text contrastively pretrained features (CLIP) as visual backbones provides sizable gains on VLN benchmarks compared to older visual features, but optimal performance requires task-specific finetuning and incorporation of history-aware reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'History Aware Multimodal Transformer for Vision-and-Language Navigation', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards learning a generic agent for vision-and-language navigation via pre-training. <em>(Rating: 2)</em></li>
                <li>Robust navigation with language pretraining and stochastic sampling. <em>(Rating: 2)</em></li>
                <li>Improving vision-and-language navigation with image-text pairs from the web. <em>(Rating: 2)</em></li>
                <li>Bert: Pre-training of deep bidirectional transformers for language understanding. <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision. <em>(Rating: 2)</em></li>
                <li>Unsupervised cross-lingual representation learning at scale. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1826",
    "paper_id": "paper-6f681faaa985ed38bc9b30777d57d9e1e3765861",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "PREVALENT",
            "name_full": "PREVALENT",
            "brief_description": "A multimodal pretraining method that pretrains a transformer on instruction–single-step observation pairs (text + single panoramic observation) for downstream vision-and-language navigation (VLN).",
            "citation_title": "Towards learning a generic agent for vision-and-language navigation via pre-training.",
            "mention_or_use": "mention",
            "model_agent_name": "PREVALENT",
            "model_agent_description": "A transformer-based multimodal model pretrained on pairs of navigation instructions and single-step visual observations (no trajectory history) to learn cross-modal alignment and single-step action prediction; later fine-tuned for VLN.",
            "pretraining_data_type": "Instruction + single-step visual observations (vision-language pairs)",
            "pretraining_data_details": "Pretraining uses pairs of natural language navigation instructions and single-step panoramic observations (instruction–observation pairs). The HAMT paper describes PREVALENT as pretraining on instruction and single-step observations without trajectory history; the original PREVALENT paper contains full dataset/scale details (not fully enumerated in HAMT).",
            "embodied_task_name": "Vision-and-Language Navigation (R2R and related VLN benchmarks)",
            "embodied_task_description": "3D indoor navigation tasks where an agent receives a natural language instruction and must navigate a discretized connectivity graph of panoramas (Matterport3D-based R2R family), selecting navigable viewpoints at each step to reach a target location.",
            "action_space_text": "Implicitly: next-step action selection derived from instruction semantics (single-step next-view selection; pretraining pairs map instruction to correct next observation).",
            "action_space_embodied": "Discrete navigation actions over candidate navigable panoramic viewpoints (select one of K candidate views each step, plus stop action), with angular headings/elevations for chosen view.",
            "action_mapping_method": "Pretraining on instruction–single-step observation pairs teaches the model to map instruction semantics to the correct next-view selection (single-step action prediction). At finetuning, the same next-view prediction head or policy is used to select navigable viewpoints.",
            "perception_requirements": "RGB panoramic views (36 views per panorama in HAMT evaluation); relative angle encodings; pretrained visual features (PREVALENT uses visual features paired with text).",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported PREVALENT baseline numbers in HAMT's Table 5: Validation Seen SR 69%, SPL 65; Validation Unseen SR 58%, SPL 53; Test Unseen SR 54%, SPL 51 (units: %).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Direct alignment between instruction tokens and single-step observations; learning cross-modal grounding between language and visual observations during pretraining improves single-step action selection.",
            "transfer_failure_factors": "Pretraining without trajectory history (no long-horizon context) limits learning of history-aware navigation and can encourage overfitting to seen environment structures; limited visual optimization if visual encoder not end-to-end tuned.",
            "key_findings": "Pretraining on instruction–single-step observation pairs provides positive transfer to VLN by improving single-step alignment between language and visual observations, but absence of trajectory history in pretraining can limit generalization for long-horizon/history-dependent navigation.",
            "uuid": "e1826.0",
            "source_info": {
                "paper_title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "PRESS (BERT-based)",
            "name_full": "PRESS (uses pretrained BERT instruction encoder)",
            "brief_description": "A VLN model that replaces LSTM-based instruction encoders with a pretrained BERT language encoder to improve language representations for navigation.",
            "citation_title": "Robust navigation with language pretraining and stochastic sampling.",
            "mention_or_use": "mention",
            "model_agent_name": "PRESS (BERT as instruction encoder)",
            "model_agent_description": "A vision-and-language navigation architecture that uses a pretrained BERT language encoder to produce richer contextual instruction embeddings, which are fused with visual features for action prediction.",
            "pretraining_data_type": "Large-scale language corpora (BERT language pretraining)",
            "pretraining_data_details": "Pretrained BERT encoder (as in the BERT paper) — pretrained on large text corpora (e.g., BookCorpus and Wikipedia per the BERT reference). HAMT cites PRESS as using pretrained BERT to improve language representation; PRESS-specific pretraining dataset details are in PRESS paper.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R and similar VLN benchmarks)",
            "embodied_task_description": "Discrete viewpoint navigation in 3D photo-realistic indoor environments guided by natural language instructions; agent selects navigable viewpoints at each step to follow instructions.",
            "action_space_text": "Natural language instructions describing sequences of navigation actions and landmarks (no explicit low-level motor commands in text pretraining).",
            "action_space_embodied": "Discrete navigable viewpoint selection and stop action (viewpoint-level navigation over graph).",
            "action_mapping_method": "Pretrained BERT provides contextualized language embeddings; downstream action head is trained (or fine-tuned) to map those embeddings (in combination with visual inputs) to discrete next-view actions — effectively the mapping is learned during VLN finetuning rather than hand-coded.",
            "perception_requirements": "RGB panoramic observations (visual features) and relative angle encodings; PRESS couples BERT text embeddings with visual features.",
            "transfer_successful": true,
            "performance_with_pretraining": "PRESS numbers reported in HAMT's Table 5 (baseline): Validation Seen SR 58%, SPL 55; Validation Unseen SR 49%, SPL 45; Test Unseen SR 49%, SPL 45 (units: %).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "High-quality, contextualized language representations from BERT reduce language-understanding errors and improve instruction grounding when combined with visual inputs.",
            "transfer_failure_factors": "BERT pretraining alone does not provide visual grounding or history modeling; improvements are limited if cross-modal alignment and visual features are weak or if trajectory history is needed.",
            "key_findings": "Initializing instruction encoders with pretrained language models (BERT) improves VLN performance by providing stronger language representations, but must be combined with appropriate visual grounding and history modeling for best results.",
            "uuid": "e1826.1",
            "source_info": {
                "paper_title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "VLN-BERT",
            "name_full": "VLN-BERT",
            "brief_description": "A vision-and-language model that measures compatibility between an instruction and images in a path using image–text pretraining, but (as described) does not support sequential action prediction with history.",
            "citation_title": "Improving vision-and-language navigation with image-text pairs from the web.",
            "mention_or_use": "mention",
            "model_agent_name": "VLN-BERT",
            "model_agent_description": "A transformer-based VLN model pretrained on image–text pairs to learn image–text alignment for navigation tasks; designed to evaluate instruction–image compatibility along paths rather than to perform sequential action prediction with long history.",
            "pretraining_data_type": "Image–text pairs (web-derived image–text data)",
            "pretraining_data_details": "VLN-BERT pretrains on image–text pairs from the web to learn cross-modal representations; HAMT cites VLN-BERT as measuring compatibilities between instructions and images in a path but not supporting action prediction — detailed datasets and scale are in the VLN-BERT paper.",
            "embodied_task_name": "Vision-and-Language Navigation (R2R family and related tasks)",
            "embodied_task_description": "Agents must follow textual navigation instructions in 3D indoor environments (discrete panoramic viewpoint graph); VLN-BERT targets alignment tasks and path compatibility evaluation rather than full history-based action policies.",
            "action_space_text": "Natural language instructions describing navigation; image–text pretraining does not include explicit action sequences.",
            "action_space_embodied": "Discrete viewpoint selection and stop actions in the navigation graph (when used as part of a downstream navigation system).",
            "action_mapping_method": "VLN-BERT learns image–text compatibility scores; mapping to actions for sequential navigation is not its primary function (HAMT notes it 'does not support action prediction').",
            "perception_requirements": "Image-level visual features (RGB panoramas or view images) paired with instruction text.",
            "transfer_successful": null,
            "performance_with_pretraining": null,
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale image–text pretraining improves cross-modal alignment and can assist downstream grounding tasks where instruction–image compatibility is key.",
            "transfer_failure_factors": "If pretraining omits sequential/action prediction and trajectory history, the model may not transfer well to long-horizon, history-dependent embodied navigation policies.",
            "key_findings": "Image–text pretraining helps cross-modal alignment, but methods that omit action prediction or history modeling are limited for sequential VLN where history and action sequencing matter.",
            "uuid": "e1826.2",
            "source_info": {
                "paper_title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Multilingual BERT (init)",
            "name_full": "Pretrained Multilingual BERT (used to initialize language encoder)",
            "brief_description": "A pretrained multilingual transformer language model used to initialize HAMT's language encoder for the multilingual RxR VLN dataset, providing multilingual language representations for navigation instructions.",
            "citation_title": "Unsupervised cross-lingual representation learning at scale.",
            "mention_or_use": "use",
            "model_agent_name": "Multilingual BERT (initialization for HAMT on RxR)",
            "model_agent_description": "A pretrained multilingual transformer language model (used as an initialization for HAMT's unimodal language encoder) to handle multilingual instructions in RxR; provides contextual language embeddings across languages.",
            "pretraining_data_type": "Multilingual text corpora (cross-lingual language pretraining)",
            "pretraining_data_details": "HAMT states it uses pretrained multilingual BERT [59] to initialize the unimodal language encoder for RxR (three languages: English, Hindi, Telugu). The referenced multilingual model is pretrained on large multilingual corpora (see referenced paper for exact corpora and scale).",
            "embodied_task_name": "RxR (Room-to-Room multilingual VLN)",
            "embodied_task_description": "Multilingual VLN on Matterport3D where agents follow instructions in English, Hindi or Telugu to navigate indoor environments; requires cross-lingual understanding and grounding to visual observations.",
            "action_space_text": "Natural language instructions in multiple languages describing navigation steps and landmarks.",
            "action_space_embodied": "Discrete navigation over candidate panoramic viewpoints and stop action.",
            "action_mapping_method": "Pretrained multilingual embeddings provide language representations that are fused with visual features during downstream training; the mapping from language to actions is learned during HAMT finetuning.",
            "perception_requirements": "RGB panoramic views (visual features) and relative angle encodings; multilingual instruction text.",
            "transfer_successful": true,
            "performance_with_pretraining": "HAMT (multilingual) results on RxR reported in HAMT: Val Seen SR 59.4%, SPL 58.9%, nDTW 65.3%, SDTW 50.9%; Val Unseen SR 56.5%, SPL 56.0% (units: %). HAMT used multilingual BERT to initialize language encoder for these experiments.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Language-model initialization provides robust multilingual contextual embeddings which ease downstream learning and generalization across languages.",
            "transfer_failure_factors": "Language pretraining does not supply visual grounding or history modeling; success depends on complementary multimodal pretraining and finetuning.",
            "key_findings": "Initializing VLN agents with multilingual pretrained language models enables handling of multilingual instructions and contributes to strong performance on multilingual embodied navigation benchmarks when combined with appropriate visual grounding and finetuning.",
            "uuid": "e1826.3",
            "source_info": {
                "paper_title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "CLIP (features)",
            "name_full": "CLIP (Contrastive Language–Image Pretraining) features",
            "brief_description": "A large-scale image–text contrastive model pretrained on web image–text pairs; leaderboard methods using CLIP features (CLIP-ViL, CLEAR-CLIP) show strong VLN performance when their visual backbones use CLIP-derived embeddings.",
            "citation_title": "Learning transferable visual models from natural language supervision.",
            "mention_or_use": "mention",
            "model_agent_name": "CLIP (used as pretrained visual-text embedding provider for some RxR leaderboard methods)",
            "model_agent_description": "A contrastive image–text pretraining model that produces joint visual-text embeddings; several RxR leaderboard entries use CLIP-derived visual features (fixed, not end-to-end optimized in HAMT evaluations) to improve VLN performance.",
            "pretraining_data_type": "Large-scale image–text pairs from the web (contrastive image–text pretraining)",
            "pretraining_data_details": "CLIP was pretrained on hundreds of millions of image–text pairs (see CLIP paper); HAMT notes that RxR leaderboard methods CLIP-ViL and CLEAR-CLIP use CLIP features and reports their RxR test numbers (CLIP-ViL and CLEAR-CLIP scores in HAMT Table 12). HAMT used the same visual features for fair comparison (without end-to-end optimization).",
            "embodied_task_name": "RxR (multilingual VLN) and other VLN benchmarks where CLIP features have been adopted by competing entries",
            "embodied_task_description": "Discretized viewpoint navigation in Matterport3D environments, following multilingual natural language instructions; agents rely on CLIP-derived visual features for perception.",
            "action_space_text": "Natural language instructions (text) used as supervision; CLIP pretraining itself does not include action sequences.",
            "action_space_embodied": "Discrete viewpoint selection among candidate navigable views and stop action for VLN.",
            "action_mapping_method": "CLIP provides image–text-aligned embeddings that are used as visual inputs; downstream VLN models learn to map fused CLIP visual/text features to navigation actions during VLN finetuning.",
            "perception_requirements": "RGB panoramic images represented via CLIP visual embeddings; relative angle encodings and other navigation-specific features added by downstream models.",
            "transfer_successful": true,
            "performance_with_pretraining": "RxR test-split numbers reported in HAMT (Table 12): CLIP-ViL SR 38.34%, SPL 35.17%, nDTW 51.10%; CLEAR-CLIP SR 40.29%, SPL 36.57%, nDTW 53.69% (units: %).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "CLIP's image–text alignment provides strong visual features already grounded in language semantics, improving down-stream multimodal grounding and navigation.",
            "transfer_failure_factors": "CLIP pretraining is not navigation-specific and lacks trajectory/history reasoning; using fixed CLIP features without end-to-end adaptation can limit performance in tasks requiring fine-grained object grounding or domain-specific visual features.",
            "key_findings": "Using image–text contrastively pretrained features (CLIP) as visual backbones provides sizable gains on VLN benchmarks compared to older visual features, but optimal performance requires task-specific finetuning and incorporation of history-aware reasoning.",
            "uuid": "e1826.4",
            "source_info": {
                "paper_title": "History Aware Multimodal Transformer for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards learning a generic agent for vision-and-language navigation via pre-training.",
            "rating": 2
        },
        {
            "paper_title": "Robust navigation with language pretraining and stochastic sampling.",
            "rating": 2
        },
        {
            "paper_title": "Improving vision-and-language navigation with image-text pairs from the web.",
            "rating": 2
        },
        {
            "paper_title": "Bert: Pre-training of deep bidirectional transformers for language understanding.",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision.",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised cross-lingual representation learning at scale.",
            "rating": 1
        }
    ],
    "cost": 0.0204385,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>History Aware Multimodal Transformer for Vision-and-Language Navigation</h1>
<p>Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, Ivan Laptev<br>Inria, École normale supérieure, CNRS, PSL Research University<br>{shizhe.chen, pierre-louis.guhur, cordelia.schmid, ivan.laptev}@inria.fr<br>https://cshizhe.github.io/projects/vln_hamt.html</p>
<h4>Abstract</h4>
<p>Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making. HAMT efficiently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which first encodes individual images with ViT, then models spatial relation between images in a panoramic observation and finally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We first train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with fine-grained instructions (R2R, RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories.</p>
<h2>1 Introduction</h2>
<p>Vision-and-language navigation (VLN) has recently received growing attention [1-5]. VLN requires an agent to understand natural language instructions, perceive the visual world, and perform navigation actions to arrive at a target location. A number of datasets have been proposed to support various VLN tasks such as indoor and outdoor navigation with fine-grained instructions [2, 6, 7], language-driven remote object finding [8] and navigation in dialogs [9].
VLN agents are faced with several challenges. First, as opposed to static vision-text grounding [10], the agent continuously receives new visual observations and should align them with instructions. Most of existing works adopt recurrent neural networks (RNNs) [6, 11-16] to encode historical observations and actions within a fixed-size state vector to predict the next action. Such condensed states might be sub-optimal for capturing essential information in extended trajectories [17]. For instance, "bring the spoon to me" requires the agent to remember its start location after navigating to the "spoon", while early memories are prone to fade in the recurrent state. Few endeavors [18, 19] construct external map-like memories for received observations. Nevertheless, these approaches still rely on RNNs to track the navigation state. As the history plays an important role in environment understanding and instruction grounding, we propose to explicitly encode the history as a sequence of previous actions and observations instead of using recurrent states.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The architecture of History Aware Multimodal Tranformer (HAMT). HAMT jointly encodes textual instruction, full history of previous observations and actions, and current observation to predict the next action.</p>
<p>Another VLN challenge concerns the generalizations of agents to new environments that have not been observed during training [4]. One direction is to learn more generic text-image representations. The PRESS model [20] improves language representation with a pretrained BERT encoder [21], and PREVALENT [22] uses pairs of instruction and single-step observations to pretrain a multimodal transformer. Though achieved promising results, these works do not optimize visual representation for the target navigation task. Moreover, lack of history in training [22] makes it hard to learn cross-modal alignment and increases the risk of overfitting to training environments. Another direction towards better generalization is to overcome exposure bias [23] due to discrepancy between training and inference. Different methods have been adopted for VLN including DAgger [6, 24] and scheduled sampling [20, 25]. Reinforcement Learning (RL) [12, 26] is one of the most effective approach among them, but it is considered unstable to directly train large-scale transformers via RL [27].</p>
<p>To address the above challenges, we propose the History Aware Multimodal Transformer (HAMT), a fully transformer-based architecture for multimodal decision making in VLN tasks. As illustrated in Figure 1, HAMT consists of unimodal transformers for text, history and observation encoding, and a cross-modal transformer to capture long-range dependencies of the history sequence, current observation and instruction. Since our history contains a sequence of all previous observations, its encoding is computationally expensive. To resolve complexity issues, we propose a hierarchical vision transformer as shown in Figure 2, which progressively learns representations for a single view, spatial relationships among views within a panorama and, finally, the temporal dynamics across panoramas of the history. In order to learn better visual representations, we propose auxiliary proxy tasks for end-to-end training. Such tasks include single-step action prediction based on imitation learning, self-supervised spatial relationship reasoning, masked language and image predictions and instruction-trajectory matching. We empirically show that our training facilitates the subsequent fine-tuning of our model with RL [28]. We carry out extensive experiments on various VLN tasks, including VLN with <em>fine-grained instructions</em> (R2R [6] and RxR [7]), <em>high-level instructions</em> (REVERIE [8] and our proposed R2R-Last), <em>dialogs</em> [9] as well as <em>long-horizon VLN</em> (R4R [3] and our proposed R2R-Back which requires the agent to return back after arriving at the target location). HAMT outperforms state of the art on both seen and unseen environments in all the tasks.</p>
<p>We summarize our contributions as follows: (1) We introduce HAMT to efficiently model long-horizon history of observed panoramas and actions via hierarchical vision transformer; (2) We train HAMT with auxiliary proxy tasks in an end-to-end fashion and use RL to improve the navigation policy; (3) We validate our method and outperform state of the art in a diverse range of VLN tasks, while demonstrating larger gains for long-horizon navigation.</p>
<h2>2 Related work</h2>
<p><strong>Vision-and-language navigation.</strong> Training instruction-following navigation agents has attracted increasing research attention [1, 2, 6–8, 29]. Anderson <em>et al</em>. [6] propose a sequence-to-sequence</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Hierarchical history encoding. It first encodes individual view images with ViT, then models the spatial relation between images in each panorama, and finally captures the temporal relation between panoramas in the history.
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Flattened history encoding. It encodes spatial and temporal relations at the same time.
<img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Temporal-only history encoding. It only considers temporal relation of oriented views.</p>
<p>Figure 2: A comparison of history encoding methods. Circle nodes in different colors denote view images of panorama at different steps. Darker circle nodes are the oriented view of the agent.</p>
<p>LSTM baseline for the VLN task. Fried et al. [11] extend it with panoramic action space and synthesized instructions. To improve cross-modal alignment, the self-monitoring agent [13] proposes co-grounding and progress estimation, and RelGraph [15] uses graphs to model relationships across scene, objects and directions. Reinforcement learning (RL) is typically used to improve navigation policy. The EnvDrop model [12] mixes imitation learning and A3C [28]. The RCM [14] utilizes intrinsic reward of cross-modal matching in REINFORCE algorithm. Wang et al. [30] propose to learn rewards via soft expert distillation. Due to the success of transformer [31], recent works explore transformer architectures in VLN. PRESS [20] replaces LSTM instruction encoder with pretrained BERT [21]. SIA [16] uses transformer for single-step multimodal fusion and LSTM for sequential action prediction. PTA [32] is a transformer VLN model using CNNs to extract visual features [33]. Here we propose the first full transformer architecture for VLN and train it end-to-end.</p>
<p>Memory-based policy for navigation. LSTMs [34] have been the dominant approach to encode memories for navigation [6, 11, 12, 14]. Condensing all history into one feature vector, however, is prone to the loss of information. Alternative approaches include topological map memory structures [35, 36]. Deng et al. [18] use graphs to capture environment layout and enable long-term planing. A similar graph is adopted in [19] with frontier-exploration based decision making. But these works still utilize LSTMs for state tracking. To exploit long-term spatio-temporal dependencies, Fang et al. [17] store histories in a sequence encoded with transformer. Recurrent VLN-BERT [5] injects a recurrent unit to encode histories in transformer for VLN. The most similar work to ours is Episodic Transformer (E.T.) [37]. Differently from [37], we propose a hierarchical encoding of the panoramic observation history and optimize the whole model in end-to-end training.</p>
<p>Multimodal pretraining with transformers. Recent works show significant progress in vision and language tasks using multimodal pretraining. In particular, transformer architectures such as one-stream [38, 39] and dual-stream [40, 41] achieve state of the art for a number of downstream tasks including visual question answering, image-text retrieval and image captioning. While most previous methods rely on CNN to extract image representations, ViLT [42] adopts Vision Transformer (ViT) [43] and trains it with associated texts in an end-to-end manner thanks to the efficiency of ViT. A few endeavors [22, 44] explore multimodal pretraining for VLN. PREVALENT [22] pretrains a transformer using instructions and single-step observations without referring to trajectory history. VLN-BERT [44] measures the compatibility between an instruction and images in a path but does not support action prediction. Our work presents the first end-to-end trainable VLN transformer that jointly encodes text, history and observation, and is able to sequentially predict actions.</p>
<h2>3 Method</h2>
<p>Problem definition The VLN problem [6] is formulated as a partially observable Markov decision process, where future observations are independent of the past conditioning on current state $s_{t}$. Given</p>
<p>an instruction $\mathcal{W}$ containing a sequence of $L$ words $\left(w_{1}, w_{2}, \cdots, w_{L}\right)$, an agent should follow the instruction to move in a connectivity graph to reach the goal location. At each step $t$, the agent receives an observation $\mathcal{O}<em t="t">{t}$, a panorama of its surrounding environment. The $\mathcal{O}</em>}$ consists of $K$ single view images split from the panorama $\mathcal{O<em 1="1">{t} \triangleq\left(\left[v</em>}^{o} ; a_{1}^{o}\right], \cdots,\left[v_{K}^{o} ; a_{K}^{o}\right]\right)$, where $v_{i}^{o}$ is the visual feature of the $i$-th view and $a_{i}^{o}$ denotes the relative angle to face the view (subscript $t$ is omitted for simplicity). There are $n$ navigable viewpoints among all the $K$ views ${ }^{1}$, denoted as $\mathcal{O<em 1="1">{t}^{o} \triangleq\left(\left[v</em>}^{i} ; a_{1}^{i}\right], \cdots,\left[v_{n}^{o} ; a_{n}^{o}\right]\right)$. We follow the setup in [11] and use $\mathcal{O<em t="t">{t}^{i}$ as the decision space, so the agent only needs to select a candidate in $\mathcal{O}</em>}^{o}$ at each step. All observations $\mathcal{O<em i="i">{i}$ and performed actions $a</em>}^{h}$ before step $t$ form the history $\mathcal{H<em 1="1">{t} \triangleq\left(\left[\mathcal{O}</em>} ; a_{1}^{h}\right], \cdots,\left[\mathcal{O<em t-1="t-1">{t-1} ; a</em>}^{h}\right]\right)$, where $a_{i}^{h}$ denotes the turned angles at step $i$. The goal is to learn a policy $\pi$ parametrized by $\Theta$ to predict the next action based on the instruction, history and the current observation, which is $\pi\left(a_{t} \mid \mathcal{W}, \mathcal{H<em t="t">{t}, \mathcal{O}</em>}, \mathcal{O<em t="t">{t}^{o} ; \Theta\right)$.
Unlike dominant recurrent approaches to condense $\mathcal{H}</em>$ into a fixed-size vector, in this section, we present the History Aware Multimodal Transformer (HAMT) that jointly encodes text, long-horizon history, and observation for sequential action prediction. The model architecture is described in Section 3.1. We propose end-to-end training for HAMT in Section 3.2 to learn unimodal and multimodal representations, and then use RL to fine-tune the navigation policy in Section 3.3.</p>
<h1>3.1 HAMT: History Aware Multimodal Transformer</h1>
<p>Figure 1 illustrates the model architecture of HAMT. The inputs text $\mathcal{W}$, history $\mathcal{H}<em t="t">{t}$ and observation $\mathcal{O}</em>$ are first encoded via the corresponding unimodal transformers respectively, and then fed into the cross-modal transformer encoder to capture multimodal relationships.
Text Encoding. For each token $i$ in the instruction $\mathcal{W}$, we embed it as the summation of its word embedding $w_{i}$, position embedding $E_{i}^{T}$ and type embedding of text $E_{0}^{T}$. Then we employ a transformer with $N_{L}$ layers to obtain contextual representation $x_{i}$ following the standard BERT [21].
Observation Encoding. For each view $\left[v_{i}^{o} ; a_{i}^{o}\right]$ in the panoramic observation $\mathcal{O}<em i="i">{t}$, we first represent the relative angle $a</em>$ is as follows:}^{o}$ as $E_{a_{i}^{o}}^{A}=\left(\sin \theta_{i}, \cos \theta_{i}, \sin \phi_{i}, \cos \phi_{i}\right)$ where $\theta_{i}$ and $\phi_{i}$ are the relative heading and elevation angle to the agent's orientation. Then the observation embedding $o_{i</p>
<p>$$
o_{i}=\operatorname{LN}\left(W_{o}^{o} v_{i}^{o}\right)+\operatorname{LN}\left(W_{a}^{o} E_{a_{i}^{o}}^{A}\right)+E_{o_{i}}^{N}+E_{1}^{T}
$$</p>
<p>where $W_{o}^{o}, W_{a}^{o}$ are learnable weights. The $E_{o_{i}}^{N}$ denotes the navigable embedding to differentiate types of views, with $E_{0}^{N}$ for non-navigable view, $E_{1}^{N}$ for navigable view and $E_{2}^{N}$ for stop view (we append a stop token in observation to support stop action). The $E_{1}^{T}$ is the type embedding of observation. We omit bias terms for simplicity. The LN denotes layer normalization [45]. Because $a_{i}^{o}$ has much lower feature dimensions than $v_{i}^{o}$, we apply LN to balance the encoded $a_{i}^{o}$ and $v_{i}^{o}$.
Hierarchical History Encoding. As $\mathcal{H}<em i="i">{t}$ consists of all the past panoramic observations $\mathcal{O}</em>}$ and performed actions $a_{i}^{h}$ before step $t$, it is important to encode $\mathcal{H<em i="i">{t}$ efficiently as context. Figures 2b-2c depict the flattened and temporal-only history encoding approaches used in VLN-BERT [44] and E.T. [37] respectively. The flattened approach treats each view image in $\mathcal{O}</em>}$ as a token, so the history sequence contains $t K$ tokens. Though it enables to learn relationships among all image views, the computation cost quadratically increases with the sequence length, making it inefficient for long-horizon tasks. In the temporal-only approach, only the oriented view of the agent in each $\mathcal{O<em i="i">{i}$ is taken as inputs instead of the whole panorama, so only $t$ temporal tokens are encoded. However, this approach can lose critical information in past observations. For example, in the instruction "with the windows on your left, walk through the large room past the sitting areas", the object "window" does not appear in the oriented view of the agent. Therefore, the encoded history is insufficient to tell whether the agent passed the window or not, making the model confused to take the next action.
In order to balance computational efficiency and information integrity, we propose a hierarchical history encoding approach as illustrated in Figure 2a. It hierarchically encodes view images within each panorama and then temporal relationships across panoramas, similar to the factorized spatialtemporal video transformer [46]. For each $\mathcal{O}</em>$ layers to learn spatial relationships within the panorama. We apply average pooling to obtain panorama embedding, and add it with the}$, its constituent view images are first embeded via ViT and Eq (1), and then encoded via a panoramic transformer with $N_{h</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Comparison of HAMT and previous VLN transformers.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Inputs</th>
<th></th>
<th></th>
<th></th>
<th>Proxy Tasks</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Text</td>
<td>History</td>
<td>Observation</td>
<td>MLM</td>
<td>MRM</td>
<td>ITM</td>
<td>SAP/SAR</td>
<td>SPREL</td>
</tr>
<tr>
<td>PREVALENT [22]</td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td></td>
<td>$\checkmark$</td>
<td></td>
</tr>
<tr>
<td>VLN-BERT [44]</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td></td>
<td></td>
</tr>
<tr>
<td>HAMT (Ours)</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
</tr>
</tbody>
</table>
<p>oriented view image feature in residual connection. The parameters in ViT and panoramic transformer are shared for different steps. In this way, each historical observation $\mathcal{O}<em i="i">{i}$ is represented as $v</em>$ is computed as:}^{h}$, and the final temporal token $h_{i</p>
<p>$$
h_{i}=\operatorname{LN}\left(W_{v}^{h} v_{i}^{h}\right)+\operatorname{LN}\left(W_{a}^{h} E_{a_{i}^{h}}^{A}\right)+E_{i}^{S}+E_{2}^{T}
$$</p>
<p>where $E_{i}^{S}$ denotes the $i$-th step embedding, $E_{2}^{T}$ is the type embedding of history. The computational cost is $O\left(t K^{2}+t^{2}\right)$, which significantly reduces from $O\left(t^{2} K^{2}\right)$ in the flattened approach. To be noted, we add a special token [cls] to the start of the history sequence to obtain a global representation. The embedding of [cls] is a parameter to learn, which is initialized from a zero vector.</p>
<p>Cross-modal Encoding. We concatenate history and observation as the vision modality, and use cross-modal transformer with $N_{x}$ layers to fuse features from text, history and observation as shown in the right of Figure 1. The reason of using such dual-stream architecture rather than onestream is that the length of different modalities can be highly imbalanced, and the dual-stream architecture can balance the importance of intra- and inter-modal relationships by model design [47]. In each cross-modal layer, a vision-text cross-attention is firstly performed for vision modality to attend relevant text information and vice versa for text modality. Then each modality uses selfattention to learn intra-modal relationship such as interaction between observation and history, followed by a fully-connected neural network. Finally, the HAMT model outputs embeddings $X^{\prime}=\left(x_{\mathrm{cls}}^{\prime}, x_{1}^{\prime}, \cdots, x_{L}^{\prime}\right), H_{t}^{\prime}=\left(h_{\mathrm{cls}}^{\prime}, h_{1}^{\prime}, \cdots, h_{t-1}^{\prime}\right), O_{t}^{\prime}=\left(o_{1}^{\prime}, \cdots, o_{K}^{\prime}, o_{\text {stop }}^{\prime}\right)$ for tokens in text, history and observation respectively.</p>
<h1>3.2 End-to-end training with proxy tasks</h1>
<p>As it is difficult to train large-scale transformers with RL due to sparse supervision [27], we propose to first end-to-end train HAMT via several proxy tasks to learn unimodal and multimodal representation.</p>
<p>Table 1 compares our HAMT with previous VLN transformers PREVALENT [22] and VLNBERT [44] in inputs and proxy tasks. As neither PREVALENT nor VLN-BERT jointly encodes text, history and observation, a limited choice of proxy tasks can be applied in training. Our model instead can take advantage of various proxy tasks to learn cross-modal alignment, spatial and temporal reasoning, and history-aware action prediction. Given the input pair $\left(\mathcal{W}, \mathcal{H}<em t="t">{T}\right)$ where $T$ is the length of full trajectory, we can apply common proxy tasks as in vision-and-language pretraining [40, 44], including Masked Language Modeling (MLM), Masked Region Modeling (MRM) and Instruction Trajectory Matching (ITM). Details of the three proxy tasks are presented in the supplementary material. In the following, we introduce new proxy tasks given the triplet input $\left(\mathcal{W}, \mathcal{H}</em>\right)$ specifically for VLN tasks.}, \mathcal{O}_{t</p>
<p>Single-step Action Prediction/Regression (SAP/SAR). The task deploys imitation learning to predict the next action based on instruction, history from expert demonstration and the current observation. We formulate it as a classification and a regression task respectively. In the SAP classification task, we predict action probability for each navigable view in $\mathcal{O}<em t="t">{t}^{c}$ which is $p</em>$ is output embedding of special text token [cls]. The objective is to minimize negative log probability of the target view action $o_{}\left(o_{i}^{\prime}\right)=\frac{\exp \left(f_{\text {SAP }}\left(o_{i}^{\prime} \odot x_{\mathrm{cls}}^{\prime}\right)\right)}{2_{s j} \exp \left(f_{\text {SAP }}\left(o_{j}^{\prime} \odot x_{\mathrm{cls}}^{\prime}\right)\right)}$, where $f_{\text {SAP }}$ is a two-layer fully-connected network, $\odot$ is element-wise multiplication and $x_{\mathrm{cls}}^{\prime<em>}^{\prime}: L_{\mathrm{SAP}}=-\log p_{t}\left(o_{</em>}^{\prime}\right)$. In SAR regression task, we directly predict the action heading and elevation angles based on the text token [cls] which is $\hat{\theta}<em t="t">{t}, \hat{\phi}</em>}=f_{\mathrm{SAR}}\left(x_{\mathrm{cls}}^{\prime}\right)$. The loss function is $L_{\mathrm{SAR}}=\left(\hat{\theta<em t="t">{t}-\theta</em>}\right)^{2}+\left(\hat{\phi<em t="t">{t}-\phi</em>$. The two proxy tasks enable the model to learn how to make action decision conditioning on instruction and contextual history.}\right)^{2</p>
<p>Spatial Relationship Prediction (SPREL). Expressions of egocentric and allocentric spatial relations are frequent in navigational instructions, such as "walk into the room on your left" and "enter the</p>
<p>bedroom next to the stairs". In order to learn spatial relation aware representations, we propose the SPREL self-supervised task to predict relative spatial position of two views in a panorama based on only visual feature, angle feature or both. Assume $\left[v_{i}^{o} ; a_{i}^{o}\right]$ and $\left[v_{j}^{o} ; a_{j}^{o}\right]$ are two views in $\mathcal{O}<em>{t}$, we randomly zero out $v</em>{<em>}^{o}$ or $a_{</em>}^{o}$ with probability of 0.3 . Their encoded representations are $o_{i}^{\prime}$ and $o_{j}^{\prime}$, and their relative heading and elevation angles are $\theta_{i j}, \phi_{i j}$. We then predict $\hat{\theta}<em i="i" j="j">{i j}, \hat{\phi}</em>}=f_{\text {SPREL }}\left(\left[o_{i}^{\prime} ; o_{j}^{\prime}\right]\right)$ where $[;]$ denotes vector concatenation and optimize $L_{\text {SPREL }}=\left(\hat{\theta<em i="i" j="j">{i j}-\theta</em>}\right)^{2}+\left(\hat{\phi<em i="i" j="j">{i j}-\phi</em>$. The task helps for spatial relationship reasoning in the observation.}\right)^{2</p>
<p>Training Strategy. Instead of directly training the whole HAMT model at once, we propose to progressively train HAMT in two stages. In the first stage, we freeze ViT pretrained on ImageNet [48] and train the rest of the modules which are randomly initialized. This aims to avoid catastrophic forgetting of the pretrained weights in ViT. Then we unfreeze ViT and train the whole model end-toend. The learning rate for ViT is set to be higher than for others modules to avoid vanishing gradients and to speedup convergence. We empirically show that the proposed two-stage training outperforms one-stage training in the supplementary material.</p>
<h1>3.3 Fine-tuning for sequential action prediction</h1>
<p>Structure Variants. We present two variants of HAMT for action prediction in the following. 1) MLP action head: we directly reuse the action prediction network $f_{\text {SAP }}$ in the SAP task to predict navigable views. We use it as default for VLN tasks. 2) MLP action head based on encoder-decoder structure: the original HAMT model applies cross-modal attention for both vision-to-text and text-tovision, which is computationally expensive when instructions are long. Therefore, we remove the cross-modal attention from text to vision. In this way, we separate the cross-modal transformer into an encoder which only takes instruction as input, and a decoder that inputs history and observation as query and attends over encoded text tokens. Please see supplementary material for details.
RL+IL Objective. We combine Reinforcement Learning (RL) and Imitation Learning (IL) to finetune HAMT for sequential action prediction. The IL relies on the SAP loss defined in Section 3.2 and follows the expert action at each step while RL samples actions according to the policy $\pi$. Specifically, we use the Asynchronous Advantage Actor-Critic (A3C) RL algorithm [28]. At each step $t$, the agent samples an action based on policy $\pi: \hat{a}<em t="t">{t}^{h} \sim \pi\left(a</em>} \mid \mathcal{W}, \mathcal{H<em t="t">{t}, \mathcal{O}</em>}, \mathcal{O<em t="t">{t}^{*}\right)$ and receives an immediate reward $r</em>\right)$. As the reward signal favors shortest distance, we empirically find it benefits to combine A3C RL with IL weighted by $\lambda$, which is:}$. For non-stop actions, we set $r_{t}$ as the reduced distance of taking the action to the target and the increased alignment score [3] compared to expert demonstration as defined in [5]; for the stop action, $r_{t}=2$ if the agent successfully arrives at the target otherwise -2 . A critic network is trained to estimate the value of each state $s_{t}$, which is $R_{t}=\sum_{k=0}^{T-t} \gamma^{k} r_{t+k}$ where $\gamma$ is discount factor. We implement it as $V_{t}=f_{\text {critic }}\left(x_{\text {cls }}^{\prime} \odot h_{\text {cls }}^{\prime</p>
<p>$$
\Theta \leftarrow \Theta+\underbrace{\mu \frac{1}{T} \sum_{t=1}^{T} \nabla_{\Theta} \log \pi\left(\hat{a}<em t="t">{t}^{h} ; \Theta\right)\left(R</em>}-V_{t}\right)<em>{\text {Reinforcement Learning (RL) }}+\underbrace{\lambda \mu \frac{1}{T</em>{<em>}} \sum_{t=1}^{T^{</em>}} \nabla_{\Theta} \log \pi\left(a_{t}^{*} ; \Theta\right)}_{\text {Imitation Learning (IL) }}
$$</p>
<p>where $\mu$ is the learning rate, $a_{t}^{<em>}$ is the expert action at step $t$ of the expert trajectory of length $T^{</em>}$.</p>
<h2>4 Experiments</h2>
<h3>4.1 Experimental setup</h3>
<p>Datasets. We evaluate our method on four VLN tasks (seven datasets): VLN with fine-grained instructions (R2R [6], RxR [7]); VLN with high-level instructions (REVERIE [8], R2R-Last); vision-and-dialogue navigation (CVDN [9]); and long-horizon VLN (R4R [3], R2R-Back).</p>
<ul>
<li>
<p>R2R [1] builds upon Matterport3D [49] and includes 90 photo-realistic houses with 10,567 panoramas. It contains 7,189 shortest-path trajectories, each associated with 3 instructions. The dataset is split into train, val seen, val unseen and test unseen sets with $61,56,11$ and 18 houses respectively. Houses in val seen split are the same as training, while houses in val unseen and test splits are different from training.</p>
</li>
<li>
<p>RxR [7] is a large multilingual VLN dataset based on Matterport 3D. The instructions are in three different languages (English, Hindi and Telugu). The dataset emphasizes the role of language in VLN by addressing biases in paths and describing more visible entities than R2R.</p>
</li>
<li>R4R [3] extends R2R dataset by concatenating two adjacent tail-to-head trajectories in R2R. Therefore, it has longer instructions and trajectories. The trajectories are also less biased as they are not necessarily the shortest-path from start to end location.</li>
<li>R2R-Back is a new VLN setup proposed in this work. The agent is required to return to its start location after arriving at the destination. The agent needs to remember its navigation histories to solve the task. We add a return command at the end of each instruction in R2R and a reverse path from the end to start locations as expert demonstration.</li>
<li>CVDN [9] defines a navigation from dialog history task, which requires an agent to arrive at goal regions based on multi-turn question-answering dialogs. Such types of instructions are often ambiguous and under-specified. The lengths of instructions and paths are also long.</li>
<li>REVERIE [8] replaces step-by-step instructions in R2R with high-level instructions, which mainly describe the target location and object. The agent, hence, is required to navigate to the goal without detailed guidance and depends on its past experiences.</li>
<li>R2R-Last is our proposed VLN setup similar to REVERIE. It only uses the last sentence from the original R2R instructions describing the final destination.
Evaluation metrics. We adopt standard metrics [1], including (1) Trajectory Length (TL): the agent's navigated path in meters; (2) Navigation Error (NE): the average distance in meters between the agent's final position and the target; (3) Success Rate (SR): the ratio of trajectories reaching the destination with a maximum error of 3 meters to the target; and (4) Success Rate normalized by the ratio between the length of the shortest path and the predicted path (SPL). SPL is more relevant than SR as it balances the navigation accuracy and efficiency. For long-horizon VLN task (R4R and R2R-Back), we further employ three metrics to measure the path fidelity between the predicted path and target path, including (5) Coverage weighted by Length Score (CLS) [3]; (6) the normalized Dynamic Time Warping (nDTW) [50]; and (7) the Success weighted by nDTW (SDTW).
Implementation details. For the HAMT model, we set $N_{L}=9$ for language transformer, $N_{h}=2$ for panoramic transformer in hierarchical history encoding, and $N_{x}=4$ for cross-modal transformer. There are $K=36$ view images in each panoramic observation. We use ViT-B/16 [43] for image encoding if not otherwise specified. In training with proxy tasks, we randomly select proxy tasks for each mini-batch with predefined ratio. We train HAMT for 200k iterations with fixed ViT using learning rate of $5 \mathrm{e}-5$ and batch size of 64 on 4 NVIDIA Tesla P100 GPUs ( $\sim 1$ day). The whole HAMT model is trained end-to-end for 20k iterations on 20 NVIDIA V100 GPUs with learning rate of $5 \mathrm{e}-5$ for ViT and $1 \mathrm{e}-5$ for the others ( $\sim 20$ hours). We use R2R training set and augmented pairs from [22] for training unless otherwise noted. In fine-tuning with RL+IL, we set $\lambda=0.2$ in Eq (3) and $\gamma=0.9$. The model is fine-tuned for 100k iterations with learning rate of $1 \mathrm{e}-5$ and batch size of 8 on a single GPU. Unimodal encoders are fixed by default. The best model is selected according to performance on val unseen split. We use the same augmented data as [5] for R2R for fair comparison, while no augmented data is used for other datasets. Greedy search is applied in inference following the single-run setting. Please see supplementary material for more details.</li>
</ul>
<h1>4.2 Ablation studies</h1>
<p>In this section, we evaluate each component in the HAMT model, including: hierarchical history encoding, end-to-end training with proxy tasks, and fine-tuning objectives.</p>
<p>How important is the history encoding for VLN? For fair comparison with the state-of-the-art recurrent architecture RecBERT [5], we use the same Resnet152 visual features and train all the models from scratch with RL+IL objectives to avoid the influence of different weight initialization. The models are optimized for 300k iterations end-to-end except for the visual feature. Table 2 compares different history encoding approaches on R2R dataset. Our recurrent model slightly differs from RecBERT (no init. OSCAR) [5] in trans-</p>
<p>Table 3: Ablations for end-to-end HAMT training on R2R dataset using proposed proxy tasks.</p>
<p>(a) Comparison of visual features and end-to-end training. The “PT” stands for proxy tasks in training; “e2e” for optimizing the visual representation.</p>
<table>
<thead>
<tr>
<th>feature</th>
<th>PT</th>
<th>e2e</th>
<th>Val Seen</th>
<th></th>
<th>Val Unseen</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>SR↑</td>
<td>SPL↑</td>
<td>SR↑</td>
<td>SPL↑</td>
</tr>
<tr>
<td>Resnet</td>
<td>$\times$</td>
<td>$\times$</td>
<td>65.5_{±1.2}</td>
<td>61.3_{±1.4}</td>
<td>54.4_{±0.4}</td>
<td>48.7_{±0.4}</td>
</tr>
<tr>
<td>152</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>69.3_{±1.0}</td>
<td>64.8_{±1.2}</td>
<td>63.5_{±0.5}</td>
<td>57.5_{±0.5}</td>
</tr>
<tr>
<td>ViT</td>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>75.7_{±1.0}</td>
<td>72.5_{±1.0}</td>
<td>64.4_{±0.3}</td>
<td>58.8_{±0.0}</td>
</tr>
<tr>
<td></td>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>75.0_{±0.9}</td>
<td>71.7_{±0.7}</td>
<td>65.7_{±0.7}</td>
<td>60.9_{±0.7}</td>
</tr>
</tbody>
</table>
<p>(b) Comparison of different proxy tasks. The “SAP(R)” denotes the single step action prediction and regression task, and “SPREL” is the spatial relationship prediction task.</p>
<table>
<thead>
<tr>
<th>SAP</th>
<th>SP</th>
<th>Val Seen</th>
<th></th>
<th>Val Unseen</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>(R)</td>
<td>REL</td>
<td>SR↑</td>
<td>SPL↑</td>
<td>SR↑</td>
<td>SPL↑</td>
</tr>
<tr>
<td>$\times$</td>
<td>$\times$</td>
<td>71.2_{±2.3}</td>
<td>67.2_{±2.0}</td>
<td>62.8_{±1.3}</td>
<td>57.7_{±1.0}</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\times$</td>
<td>74.7_{±0.6}</td>
<td>71.1_{±0.9}</td>
<td>63.6_{±0.1}</td>
<td>58.1_{±0.4}</td>
</tr>
<tr>
<td>$\checkmark$</td>
<td>$\checkmark$</td>
<td>75.7_{±1.0}</td>
<td>72.5_{±1.0}</td>
<td>64.4_{±0.3}</td>
<td>58.8_{±0.0}</td>
</tr>
</tbody>
</table>
<p>former architecture as shown in Figure 1. It achieves slightly better performance on val unseen split. The temporal-only model uses transformer to encode agent’s oriented visual observations in history sequence, and outperforms the recurrent method by relative gains of 1.9% on SR and 2.1% on SPL for val unseen split. Adding panoramic observations in a hierarchical way results in 4.2% (SR) and 3.6% (SLP) relative improvements on the val unseen split compared to the recurrent method. Even larger improvements are achieved on val seen split as the hierarchical model has a larger capacity to fit the seen environments. This evaluation demonstrates the advantage of our hierarchical history representation compared to the recurrent and temporal-only history representation.</p>
<p>How much does training with proxy tasks help? We next evaluate the advantage of training HAMT end-to-end with proxy tasks. In Table 3a, the first row uses RL+IL objectives to train HAMT from scratch, while the second row uses proxy tasks for training prior to RL+IL fine-tuning. We can see that it significantly boosts the performance to first train with proxy tasks. It improves on val unseen split with 16.7% and 18.0% relative gains on SR and SPL respectively, indicating that training with auxiliary proxy tasks enables better generalization. In the third row, we replace the visual feature from Resnet152 to ViT. The ViT feature improves the performance on both val seen and val unseen splits, showing that more powerful visual representations matter. Finally, training ViT end-to-end obtains 2.1% gains on SPL on val unseen split. This is the first time to show that optimizing visual representations end-to-end is beneficial for VLN tasks. In Table 3b, we evaluate the benefit of the two new proxy tasks for frozen ViT features using the other proxy tasks by default. The SAP(R) uses imitation learning to predict actions, which directly influences the navigation policy and improves the performance by a large margin. The SPREL is a self-supervised proxy task that forces the model to learn spatial relationships in panorama and helps generalization in unseen environments. More experiments to ablate contributions from history encoding and proxy tasks, contributions of proxy tasks in end-to-end training etc. are presented in supplementary material.</p>
<p>What is the impact of the fine-tuning objectives? Table 4 presents results using different objectives in fine-tuning. The first row directly applies HAMT trained by proxy tasks, which achieves lower performance than that after IL finetuning, because we mainly use augmented data in proxy task training to increase visual diversity, but such noisy data deteriorates action prediction performance. Previous work [12] has shown that RL alone performs poorly. However, training with proxy tasks stabilizes the followup RL fine-tuning. HAMT optimized by RL achieves much better performance than that when fine-tuning with IL on the SR metric. It indicates that RL is able to learn better exploration strategy on unseen environments. However, as the reward for RL focuses more on shortest paths rather than path fidelity with instructions, the improvement on SPL metric is relatively small compared to SR metric. Moreover, the fluctuation of the pure RL objective is larger than IL. Therefore, mixing the RL and IL achieves the best performance.</p>
<h3>4.3 Comparison to state of the art</h3>
<p>VLN with fine-grained instructions: R2R and RxR. Table 5 compares HAMT with previous VLN methods on the R2R benchmark. Our model outperforms state-of-the-art results of RecBERT [5]</p>
<p>Table 5: Comparison with state-of-the-art methods on R2R dataset.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th>Test Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
</tr>
<tr>
<td>Seq2Seq [6]</td>
<td>11.33</td>
<td>6.01</td>
<td>39</td>
<td>-</td>
<td>8.39</td>
<td>7.81</td>
<td>22</td>
<td>-</td>
<td>8.13</td>
<td>7.85</td>
<td>20</td>
<td>18</td>
</tr>
<tr>
<td>SF [11]</td>
<td>-</td>
<td>3.36</td>
<td>66</td>
<td>-</td>
<td>-</td>
<td>6.62</td>
<td>35</td>
<td>-</td>
<td>14.82</td>
<td>6.62</td>
<td>35</td>
<td>28</td>
</tr>
<tr>
<td>PRESS [20]</td>
<td>10.57</td>
<td>4.39</td>
<td>58</td>
<td>55</td>
<td>10.36</td>
<td>5.28</td>
<td>49</td>
<td>45</td>
<td>10.77</td>
<td>5.49</td>
<td>49</td>
<td>45</td>
</tr>
<tr>
<td>EnvDrop [12]</td>
<td>11.00</td>
<td>3.99</td>
<td>62</td>
<td>59</td>
<td>10.70</td>
<td>5.22</td>
<td>52</td>
<td>48</td>
<td>11.66</td>
<td>5.23</td>
<td>51</td>
<td>47</td>
</tr>
<tr>
<td>AuxRN [51]</td>
<td>-</td>
<td>3.33</td>
<td>70</td>
<td>67</td>
<td>-</td>
<td>5.28</td>
<td>55</td>
<td>50</td>
<td>-</td>
<td>5.15</td>
<td>55</td>
<td>51</td>
</tr>
<tr>
<td>PREVALENT [22]</td>
<td>10.32</td>
<td>3.67</td>
<td>69</td>
<td>65</td>
<td>10.19</td>
<td>4.71</td>
<td>58</td>
<td>53</td>
<td>10.51</td>
<td>5.30</td>
<td>54</td>
<td>51</td>
</tr>
<tr>
<td>RelGraph [15]</td>
<td>10.13</td>
<td>3.47</td>
<td>67</td>
<td>65</td>
<td>9.99</td>
<td>4.73</td>
<td>57</td>
<td>53</td>
<td>10.29</td>
<td>4.75</td>
<td>55</td>
<td>52</td>
</tr>
<tr>
<td>RecBERT [5]</td>
<td>11.13</td>
<td>2.90</td>
<td>72</td>
<td>68</td>
<td>12.01</td>
<td>3.93</td>
<td>63</td>
<td>57</td>
<td>12.35</td>
<td>4.09</td>
<td>63</td>
<td>57</td>
</tr>
<tr>
<td>HAMT (Ours)</td>
<td>11.15</td>
<td>2.51</td>
<td>76</td>
<td>72</td>
<td>11.46</td>
<td>3.62</td>
<td>66</td>
<td>61</td>
<td>12.27</td>
<td>3.93</td>
<td>65</td>
<td>60</td>
</tr>
</tbody>
</table>
<p>by relative $5.9 \%$ and $7.0 \%$ improvements in SPL on val seen and unseen splits respectively. We achieve state-of-the-art performance under the single-run setting on the unseen testing split of the leaderboard ${ }^{2}$. It demonstrates the effectiveness and generalization of our model. We further provide computation time in inference for HAMT and RecBERT in the supplementary material to show the efficiency of our HAMT model. We also achieve large improvements on RxR dataset. The full results are presented in supplementary material.</p>
<p>Long-horizon VLN: R4R and R2R-Back. Table 6 shows navigation results on R4R dataset. As R4R contains longer instructions and trajectories compared to R2R, we use the encoder-decoder variant of HAMT for better efficiency. Our method outperforms previous approaches in all metrics and shows particularly large improvements for the path fidelity related metrics. Compared to RecBERT, HAMT achives $8.2 \%$ and $9.5 \%$ relative improvement in CLS and nDTW respectively. The large improvements on these path fidelity related metrics indicate that HAMT is better to follow the designated path of the fine-grained instruction. Figure 3 evaluates the performance of HAMT and RecBERT with respect to instruction length measured by words. Though the nDTW decreases for longer instructions, the relative improvement of HAMT increases with the instruction length.</p>
<p>Table 6: Comparison on R4R val unseen split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">NE $\downarrow$</th>
<th style="text-align: center;">SR $\uparrow$</th>
<th style="text-align: center;">CLS $\uparrow$</th>
<th style="text-align: center;">nDTW $\uparrow$</th>
<th style="text-align: center;">SDTW $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SF [11]</td>
<td style="text-align: center;">8.47</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">RCM [14]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: left;">PTA [32]</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">EGP [18]</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: left;">RelGraph [15]</td>
<td style="text-align: center;">7.43</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">34</td>
</tr>
<tr>
<td style="text-align: left;">RecBERT $^{\dagger}$ [5]</td>
<td style="text-align: center;">6.67</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: left;">HAMT (Ours)</td>
<td style="text-align: center;">$\mathbf{6 . 0 9}$</td>
<td style="text-align: center;">$\mathbf{4 4 . 6}$</td>
<td style="text-align: center;">$\mathbf{5 7 . 7}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 3}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 8}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 3: nDTW with respect to instruction length on R4R val unseen split.</p>
<p>The navigation performance on R2R-Back dataset is presented in Table 7. We compare with two state-of-the-art recurrent models EnvDrop [12] and RecBERT [5] based on LSTM and transformer respectively (both models are trained on R2R-Back for fair comparison). The improvements are more significant on this task as it requires the agent to remember the way it came to the target to successfully return back. The recurrent state is insufficient to capture such history and leads to inferior performance compared to the HAMT model.</p>
<p>Table 7: Comparison of methods on the R2R-Back dataset.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Val Seen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Val Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>nDTW $\uparrow$</td>
<td>SDTW $\uparrow$</td>
<td>TL</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>nDTW $\uparrow$</td>
<td>SDTW $\uparrow$</td>
</tr>
<tr>
<td>EnvDrop ${ }^{\dagger}$ [12]</td>
<td>23.83</td>
<td>44.1</td>
<td>42.0</td>
<td>61.3</td>
<td>39.4</td>
<td>24.57</td>
<td>32.4</td>
<td>30.2</td>
<td>51.1</td>
<td>28.0</td>
</tr>
<tr>
<td>RecBERT ${ }^{\dagger}$ [5]</td>
<td>22.33</td>
<td>51.4</td>
<td>48.4</td>
<td>67.3</td>
<td>45.7</td>
<td>23.35</td>
<td>41.1</td>
<td>37.7</td>
<td>58.2</td>
<td>35.6</td>
</tr>
<tr>
<td>HAMT (Ours)</td>
<td>22.76</td>
<td>$\mathbf{6 4 . 8}$</td>
<td>$\mathbf{6 1 . 8}$</td>
<td>$\mathbf{7 3 . 7}$</td>
<td>$\mathbf{5 8 . 9}$</td>
<td>23.78</td>
<td>$\mathbf{5 7 . 2}$</td>
<td>$\mathbf{5 3 . 1}$</td>
<td>$\mathbf{6 5 . 1}$</td>
<td>$\mathbf{4 9 . 5}$</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 8: Navigation performance on CVDN dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;">Val Unseen</th>
<th style="text-align: center;">Test Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PREVALENT [22]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">3.15</td>
<td style="text-align: center;">2.44</td>
</tr>
<tr>
<td style="text-align: left;">VISITRON [52]</td>
<td style="text-align: center;">5.11</td>
<td style="text-align: center;">3.25</td>
<td style="text-align: center;">3.11</td>
</tr>
<tr>
<td style="text-align: left;">MT-RCM+EnvAg [53]</td>
<td style="text-align: center;">5.07</td>
<td style="text-align: center;">4.65</td>
<td style="text-align: center;">3.91</td>
</tr>
<tr>
<td style="text-align: left;">HAMT (Ours)</td>
<td style="text-align: center;">$\mathbf{6 . 9 1}$</td>
<td style="text-align: center;">$\mathbf{5 . 1 3}$</td>
<td style="text-align: center;">$\mathbf{5 . 5 8}$</td>
</tr>
</tbody>
</table>
<p>Vision-and-Dialog Navigation: CVDN. The CVDN dataset contains dialogs as instructions and use Goal Progress (GP) in meters as the primary evaluation metric. GP measures the difference between completed distance and left distance to the goal, so the higher the better. There are two types of demonstrations in the dataset. One is shortest-path trajectory and the other is player's navigation trajectory. We mix the two types of demonstrations as supervision in training which has shown to be the most effective in previous works [22, 52, 53]. As navigation paths in CVDN dataset are much longer than R2R dataset, we adopt the encoder-decoder variant of HAMT. As shown in Table 8, HAMT outperforms existing recurrent approaches on both seen and unseen environments, and achieves the top position in the leaderboard ${ }^{1}$. It demonstrates that our HAMT model is generalizable to different types of instructions in new VLN tasks.</p>
<p>VLN with high-level instructions: R2R-Last and REVERIE. Table 9 shows results on the R2R-Last dataset that specifies the goal location and contains no step-by-step instructions. The HAMT model with the hierarchical history encoding is able to better accumulate the knowledge of the environment and achieves $9.8 \%$ and $10.5 \%$ relative gains on SPL metric on seen and unseen splits respectively compared to RecBERT [5]. The REVERIE dataset also contains high-level instructions but requires object grounding at the target location besides navigation. We provide results on REVERIE dataset in supplementary material. Our HAMT achieves SPL 30.20 and 26.67 on val unseen and test splits respectively, outperforming the state of the art navigation performance [5] by $5.3 \%$ and $2.7 \%$.</p>
<h2>5 Conclusion</h2>
<p>This paper presents the first end-to-end transformer for vision-and-language navigation, denoted as History Aware Multimodal Transformer (HAMT). Our method efficiently encodes long-horizon history and combines it with instructions and observations to derive multimodal action prediction. The HAMT is first trained with proxy tasks in an end-to-end manner, and is then fine-tuned with RL to improve the navigation policy. We achieve state-of-the-art navigation performance on a diverse range of challenging VLN tasks, demonstrating improved accuracy and generalization of our approach compared to the dominant recurrent methods. Future work could extend our history-aware transformer to VLN with continuous actions [54] and could benefit from pretraining on larger navigation datasets. This paper has minimal ethical, privacy and safety concerns.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>This work was granted access to the HPC resources of IDRIS under the allocation 101002 made by GENCI. It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute) and by Louis Vuitton ENS Chair on Artificial Intelligence.</p>
<h2>References</h2>
<p>[1] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 1, 2, 6, 7
[2] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12538-12547, 2019. 1,2
[3] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1862-1872, 2019. $2,6,7,16$
[4] Yubo Zhang, Hao Tan, and Mohit Bansal. Diagnosing the environment bias in vision-andlanguage navigation. International Joint Conferences on Artificial Intelligence, 2020. 2
[5] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo, and Stephen Gould. Vln bert: A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1643-1653, 2021. 1, 3, 4, 6, 7, 8, 9, 10, 17, $18,20,21,22$
[6] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683, 2018. 1, 2, 3, 6, 9, 16,18
[7] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-acrossroom: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4392-4412, 2020. 1, 2, 6, 7, 16, 17
[8] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9982-9991, 2020. 1, 2, 6, 7, 16, 18
[9] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. In Conference on Robot Learning, pages 394-406. PMLR, 2020. 1, 2, 6, 7, 16
[10] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1307-1315, 2018. 1
[11] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speakerfollower models for vision-and-language navigation. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, pages 3318-3329, 2018. 1, 3, 4, 9
[12] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2610-2621, 2019. 2, 3, 4, 8, 9, 10, 18
[13] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Self-monitoring navigation agent via auxiliary progress estimation. In Proceedings of the International Conference on Learning Representations, 2019. 3, 18
[14] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6629-6638, 2019. 3, 9, 18
[15] Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity relationship graph for agent navigation. Advances in Neural Information Processing Systems, 33:7685-7696, 2020. 3, 9</p>
<p>[16] Xiangru Lin, Guanbin Li, and Yizhou Yu. Scene-intuitive agent for remote embodied visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7036-7045, 2021. 1, 3, 18
[17] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 538-547, 2019. 1, 3
[18] Zhiwei Deng, Karthik Narasimhan, and Olga Russakovsky. Evolving graphical planner: Contextual global planning for vision-and-language navigation. In Advances in Neural Information Processing Systems, volume 33, 2020. 1, 3, 9
[19] Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, and Jianbing Shen. Structured scene memory for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8455-8464, 2021. 1, 3
[20] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah A Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1494-1499, 2019. 2, 3, 9
[21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019. 2, 3, 4
[22] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning a generic agent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13137-13146, 2020. $2,3,5,7,9,10,18$
[23] Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks. International Conference on Learning Representations, 2016. 2
[24] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635. JMLR Workshop and Conference Proceedings, 2011. 2
[25] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, volume 28, 2015. 2
[26] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. 2
[27] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning. In International Conference on Machine Learning, pages 7487-7498. PMLR, 2020. 2, 5
[28] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928-1937. PMLR, 2016. 2, 3, 6
[29] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10740-10749, 2020. 2</p>
<p>[30] Hu Wang, Qi Wu, and Chunhua Shen. Soft expert reward learning for vision-and-language navigation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16, pages 126-141. Springer, 2020. 3
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 3
[32] Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, and Rita Cucchiara. Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation. arXiv preprint arXiv:1911.12377, 2019. 3, 9
[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages $770-778,2016.3$
[34] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. 3
[35] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616-2625, 2017. 3
[36] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. International Conference on Learning Representations, 2018. 3
[37] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-andlanguage navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15942-15952, 2021. 3, 4
[38] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, pages 104-120. Springer, 2020. 3
[39] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In European Conference on Computer Vision, pages 121-137. Springer, 2020. 3
[40] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems, volume 32, 2019. 3, 5
[41] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5103-5114, 2019. 3
[42] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 5583-5594, 2021. 3
[43] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \times 16$ words: Transformers for image recognition at scale. International Conference on Learning Representations, 2020. 3, 7, 15
[44] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. In European Conference on Computer Vision, pages 259-274. Springer, 2020. 3, 4, 5
[45] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 4</p>
<p>[46] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lučić, and Cordelia Schmid. Vivit: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6836-6846, 2021. 4
[47] Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene: Revealing the secrets of pre-trained vision-and-language models. In European Conference on Computer Vision, pages 565-580. Springer, 2020. 5
[48] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. IEEE, 2009. 6
[49] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In 2017 International Conference on 3D Vision (3DV), pages 667-676. IEEE, 2017. 6
[50] Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruction conditioned navigation using dynamic time warping. In Visually Grounded Interaction and Language (ViGIL), NeurIPS 2019 Workshop, 2019. 7
[51] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012-10022, 2020. 9
[52] Ayush Shrivastava, Karthik Gopalakrishnan, Yang Liu, Robinson Piramuthu, Gokhan Tür, Devi Parikh, and Dilek Hakkani-Tür. Visitron: Visual semantics-aligned interactively trained object-navigator. arXiv preprint arXiv:2105.11589, 2021. 10
[53] Xin Eric Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa Kozareva, and Sujith Ravi. Environment-agnostic multitask learning for natural language grounded navigation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XXIV 16, pages 413-430. Springer, 2020. 10
[54] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the navgraph: Vision-and-language navigation in continuous environments. In European Conference on Computer Vision, pages 104-120. Springer, 2020. 10
[55] Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 297-304. JMLR Workshop and Conference Proceedings, 2010. 15
[56] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, 2019. 16
[57] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 702-703, 2020. 16
[58] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European conference on computer vision, pages 646-661. Springer, 2016. 16
[59] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440-8451, 2020. 17
[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning, pages 8748-8763, 2021. 17</p>
<h1>Supplementary Material for HAMT</h1>
<p>Section A provides additional details for the model. The experimental setup is described in Section B, including datasets, metrics and implementation details. Section C presents computation time on R2R dataset and full experimental results on RxR and REVERIE datasets. Section D includes more ablations. Finally, Section E illustrates qualitative results.</p>
<h2>A Model details</h2>
<h2>A. 1 Proxy tasks in training</h2>
<p>We employ five proxy tasks to train HAMT and introduced SAP/SAR and SPREL in Section 3.2. In the following, we present the other three proxy tasks, which are all based on the input pair $\left(\mathcal{W}, \mathcal{H}<em T="T">{T}\right)$, where $\mathcal{W}$ is the textual instruction and $\mathcal{H}</em>$ is the full trajectory with length $T$.
Masked Language Modeling (MLM). The task predicts masked words based on contextual words and the full trajectory. We randomly mask out tokens in $\mathcal{W}$ with the probability of $15 \%$ with a special token [mask] as in BERT, and predict the word distribution $p\left(w_{i} \mid \mathcal{W}<em T="T">{\backslash i}, \mathcal{H}</em>}\right)=f_{\text {MLM }}\left(x_{i}^{\prime}\right)$ where $\mathcal{W<em i="i">{\backslash i}$ is the masked instruction, $x</em>}^{\prime}$ is the output embedding of the masked word $w_{i}$ and $f_{\text {MLM }}$ is a two-layer fully-connected network. The objective is to minimize the negative log-likelihood of original words: $L_{\text {MLM }}=-\log p\left(w_{i} \mid \mathcal{W<em T="T">{\backslash i}, \mathcal{H}</em>\right)$. The task is beneficial to learn grounded language representations and cross-modal alignment.
Masked Region Modeling (MRM). The task aims to predict semantic labels of masked observations in the trajectory given an instruction and neighboring observations. We zero out observations in $\mathcal{H}<em i="i">{T} 15 \%$ of the time. The target of a masked $\mathcal{O}</em>}$ is the class probability predicted by an image classification model pretrained on ImageNet. We use ViT-B/16 [43] in this work. Suppose $P_{i} \in \mathbb{R}^{1000}$ is the target class probability for a masked $\mathcal{O<em i="i">{i}$, we predict $\hat{P}</em>}=f_{\text {MRM }}\left(o_{i}^{\prime}\right)$ where $o_{i}^{\prime}$ is the output embedding of masked $\mathcal{O<em _MRM="{MRM" _text="\text">{i}$, and minimize the KL divergence between the two probability distributions: $L</em>}}=-\sum_{j=1}^{1000} P_{i, j} \log \hat{P<em i="i">{i, j}$. In order to solve the task, $o</em>$ should capture temporal continuity in the history sequence and align with relevant instructions.
Instruction Trajectory Matching (ITM). The task predicts whether a pair of instruction and trajectory is aligned. We predict the alignment score as $s\left(\mathcal{W}, \mathcal{H}}^{\prime<em _ITM="{ITM" _text="\text">{T}\right)=f</em>}}\left(x_{\mathrm{cls}}^{\prime} \odot h_{\mathrm{cls}}^{\prime}\right)$, where $\odot$ is elementwise multiplication and $x_{\mathrm{cls}}^{\prime}, h_{\mathrm{cls}}^{\prime}$ are output embeddings for the text [cls] token and the history [cls] token respectively. We sample 4 negative trajectories for each positive instruction-trajectory pair during training, in which two negative trajectories are randomly selected from other positive pairs in the mini-batch, two are obtained by temporally shuffling the positive trajectory. The objective is the Noisy Contrastive Estimation loss [55]: $L_{\text {ITM }}=-\log \frac{\exp \left(s\left(\mathcal{W}, \mathcal{H<em T="T">{T}\right)\right)}{\exp \left(s\left(\mathcal{W}, \mathcal{H}</em>$. The model is supposed to learn cross-modal alignment and be sensitive to temporal orders of history to solve the task.}\right)\right)+\sum_{t=1}^{T} \exp \left(s\left(\mathcal{W}, \mathcal{H}_{T, t}^{\text {text }}\right)\right)</p>
<h2>A. 2 Structure variants in fine-tuning</h2>
<p>We present the encoder-decoder variant of HAMT in fine-tuning on the right of Figure 4. Compared to the original cross-modal transformer on the left, the variant removes text-tovision cross-modal attention. The encoder encodes the texts to obtain textual embeddings. Then the decoder reuses the same text embeddings in vision-to-text attention layer at each navigation step. In this way, the variant is more efficient when instructions are long e.g. in R4R and RxR datasets.</p>
<h2>B Experimental setup</h2>
<h2>B. 1 Dataset details</h2>
<p>Table 10 summarizes details of the dataset split. The proposed R2R-Back and R2R-Last setups consider exactly the same splits as the R2R dataset. We present details to construct R2R-Back and R2R-Last in the following.</p>
<p>Table 10: Dataset statistics. #traj, #instr denote the number of trajectories and instructions respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Train</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Unseen</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#traj</td>
<td style="text-align: center;">#instr</td>
<td style="text-align: center;">#traj</td>
<td style="text-align: center;">#instr</td>
<td style="text-align: center;">#traj</td>
<td style="text-align: center;">#instr</td>
<td style="text-align: center;">#traj</td>
<td style="text-align: center;">#instr</td>
</tr>
<tr>
<td style="text-align: center;">R2R [6]</td>
<td style="text-align: center;">4,675</td>
<td style="text-align: center;">14,039</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">1,021</td>
<td style="text-align: center;">783</td>
<td style="text-align: center;">2,349</td>
<td style="text-align: center;">1,391</td>
<td style="text-align: center;">4,173</td>
</tr>
<tr>
<td style="text-align: center;">RxR [7]</td>
<td style="text-align: center;">11,077</td>
<td style="text-align: center;">79,467</td>
<td style="text-align: center;">1,244</td>
<td style="text-align: center;">8,813</td>
<td style="text-align: center;">1,517</td>
<td style="text-align: center;">13,652</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11,888</td>
</tr>
<tr>
<td style="text-align: center;">R4R [3]</td>
<td style="text-align: center;">25,921</td>
<td style="text-align: center;">233,532</td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">1,035</td>
<td style="text-align: center;">5,026</td>
<td style="text-align: center;">45,234</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">R2R-Back</td>
<td style="text-align: center;">4,675</td>
<td style="text-align: center;">14,039</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">1,021</td>
<td style="text-align: center;">783</td>
<td style="text-align: center;">2,349</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CVDN [9]</td>
<td style="text-align: center;">4,742</td>
<td style="text-align: center;">4,742</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">382</td>
<td style="text-align: center;">907</td>
<td style="text-align: center;">907</td>
<td style="text-align: center;">1,384</td>
<td style="text-align: center;">1,384</td>
</tr>
<tr>
<td style="text-align: center;">R2R-Last</td>
<td style="text-align: center;">4,675</td>
<td style="text-align: center;">14,039</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">1,021</td>
<td style="text-align: center;">783</td>
<td style="text-align: center;">2,349</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">REVERIE [8]</td>
<td style="text-align: center;">4,150</td>
<td style="text-align: center;">10,466</td>
<td style="text-align: center;">515</td>
<td style="text-align: center;">1,423</td>
<td style="text-align: center;">1,328</td>
<td style="text-align: center;">3,521</td>
<td style="text-align: center;">2,304</td>
<td style="text-align: center;">6,292</td>
</tr>
</tbody>
</table>
<p>R2R-Back. We append a returning command at the end of annotated instructions in R2R to create new instructions for R2R-Back. The returning command is randomly sampled from the following sentences: "walk back to the start", "return by the way you came", "double back to where you start", "backtrack to the start", "back the way you came", "return to the starting point". The original target location is viewed as a middle stop point. The groundtruth trajectory in R2R-Back is the concatenation of the original and its inverse trajectory.
R2R-Last. We use spacy toolkit ${ }^{4}$ to split sentences for instructions in R2R. We only select the last sentence in each instruction as the new high-level instruction. It mainly describes where the goal location is e.g. "stop in front of the vent", requiring the agent to explore houses without step-by-step textual guidance. The groundtruth trajectory is the same as R2R.</p>
<h1>B. 2 Evaluation Metrics</h1>
<p>In R2R, RxR, R4R and R2R-Last datasets, a predicted trajectory is considered to be successful if the agent arrives 3 meters near to the final destination. However, such definition would make a motionless agent achieve $100 \%$ success rate (SR) on R2R-Back dataset as the final destination is the same as the starting location. Therefore, in R2R-Back evaluation, we define the success as that an agent firstly arrives 3 meters near to the original destination and then returns 3 meters near to its starting location. The groundtruth length in the SPL metric is also modified as the total traversed distance in groundtruth trajectory rather than the shortest distance between start and target location. As the REVERIE task aims for remote object grounding, the success on REVERIE is defined as arriving at a viewpoint where the target object is visible.</p>
<h2>B. 3 Implementation Details</h2>
<p>Training with proxy tasks. We sample proxy tasks for each mini-batch to train the HAMT model. The sampling ratio is MLM:MRM:ITM:SAP:SAR:SPREL=5:2:2:1:1:1. The optimizer is AdamW [56]. In the end-to-end training stage, we use image augmentation and regularization techniques to avoid overfitting of the ViT model, including RandAugment [57] and stochastic depth [58].
Fine-tuning for sequential action prediction. Due to different goals in various VLN tasks, we design different rewards in reinforcement learning for each downstream VLN dataset. In R2R, RxR and R4R datasets, the reward is introduced in Section 3.3 to take both goal distance and path fidelity into account. In R2R-Last, REVERIE and CVDN datasets where the instruction may not describe detailed navigation path, we only use the reduced distance to the goal viewpoints as rewards. We normalize the reduced distance in the same way as in the R2R dataset. In R2R-Back dataset, we use a different fine-tune strategy to avoid trivial motionless solutions. We require the agent to predict stop actions twice for the original destination (midpoint) and its starting point (final destination) respectively. Before arriving at the midpoint, the RL reward is computed based on distances to the midpoint. If the agent predicts a wrong location to stop for the midpoint, the episode is stopped; otherwise the agent continues its task while receiving rewards based on the distance to the final destination for fine-tuning. We run each experiment twice for ablation study and use the best result on the validation unseen split for the state-of-the-art comparison.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 13: Navigation performances on RxR val seen and val unseen splits.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">nDTW $\uparrow$</td>
<td style="text-align: center;">SDTW $\uparrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">nDTW $\uparrow$</td>
<td style="text-align: center;">SDTW $\uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">Multilingual Baseline [7]</td>
<td style="text-align: center;">25.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">18.2</td>
</tr>
<tr>
<td style="text-align: left;">Monolingual Baseline [7]</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: left;">Multilingual HAMT</td>
<td style="text-align: center;">$\mathbf{5 9 . 4}$</td>
<td style="text-align: center;">$\mathbf{5 8 . 9}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 3}$</td>
<td style="text-align: center;">$\mathbf{5 0 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 5}$</td>
<td style="text-align: center;">$\mathbf{5 6 . 0}$</td>
<td style="text-align: center;">$\mathbf{6 3 . 1}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 3}$</td>
</tr>
</tbody>
</table>
<h1>C Experimental results</h1>
<h2>C. 1 Computation Efficiency</h2>
<p>To assess the influence of history encoding on the inference time, we compare HAMT with RecBERT [5]. The HAMT and RecBERT use the same number of layers in the language transformer and cross-modal transformer. The main difference of two models is in the history encoding and the attended length of history for action prediction. We run each model on the R2R val unseen split (2349 instructions) and report inference times averaged over two runs using a single Tesla P100 GPU. For our method we compare variants with and without Text-to-Vision Attention (see Section A.2), denoted here as HAMT and HAMT noT2V respectively. We can see that HAMT and its noT2V variant are only 1.5 x and 1.1x slower compared to RecBERT, suggesting that attending to the whole history does not increase the inference time significantly. Moreover, while HAMT noT2V is only $10 \%$ slower compared to [5], it still outperforms [5] in SR and SPL on val unseen split.</p>
<h2>C. 2 RxR dataset</h2>
<p>As shown in Table 10, RxR dataset contains much more instructions than R2R dataset. Therefore, we directly use RxR in training proxy tasks rather than R2R with augmented data. As there are three different languages in RxR, we take advantage of pretrained multilingual BERT [59] to initialize the unimodal language encoder, so we are able to deal with multilingual</p>
<p>Table 11: Computation time in inference on R2R val unseen split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Inference <br> Time (s)</th>
<th style="text-align: center;">SR</th>
<th style="text-align: center;">SPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RecBERT [5]</td>
<td style="text-align: center;">69</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: left;">HAMT</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">61</td>
</tr>
<tr>
<td style="text-align: left;">HAMT noT2V</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">60</td>
</tr>
</tbody>
</table>
<p>Table 12: Navigation performance on RxR test split.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">PL</th>
<th style="text-align: center;">SR $\uparrow$</th>
<th style="text-align: center;">SPL $\uparrow$</th>
<th style="text-align: center;">nDTW $\uparrow$</th>
<th style="text-align: center;">SDTW $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Multilingual Baseline [7]</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">20.98</td>
<td style="text-align: center;">18.55</td>
<td style="text-align: center;">41.05</td>
<td style="text-align: center;">20.59</td>
</tr>
<tr>
<td style="text-align: left;">Monolingual Baseline [7]</td>
<td style="text-align: center;">17.05</td>
<td style="text-align: center;">25.40</td>
<td style="text-align: center;">22.59</td>
<td style="text-align: center;">41.05</td>
<td style="text-align: center;">20.59</td>
</tr>
<tr>
<td style="text-align: left;">CLIP-ViL</td>
<td style="text-align: center;">15.43</td>
<td style="text-align: center;">38.34</td>
<td style="text-align: center;">35.17</td>
<td style="text-align: center;">51.10</td>
<td style="text-align: center;">32.42</td>
</tr>
<tr>
<td style="text-align: left;">CLEAR-CLIP</td>
<td style="text-align: center;">16.46</td>
<td style="text-align: center;">40.29</td>
<td style="text-align: center;">36.57</td>
<td style="text-align: center;">53.69</td>
<td style="text-align: center;">34.86</td>
</tr>
<tr>
<td style="text-align: left;">Multilingual HAMT</td>
<td style="text-align: center;">19.77</td>
<td style="text-align: center;">$\mathbf{5 3 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 6 2}$</td>
<td style="text-align: center;">$\mathbf{5 9 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{4 5 . 1 9}$</td>
</tr>
<tr>
<td style="text-align: left;">Human</td>
<td style="text-align: center;">20.78</td>
<td style="text-align: center;">93.92</td>
<td style="text-align: center;">74.13</td>
<td style="text-align: center;">79.48</td>
<td style="text-align: center;">76.90</td>
</tr>
</tbody>
</table>
<p>model. We employ the encoder-decoder variant of HAMT for computational efficiency. For fair comparison with other approaches in RxR testing leaderboard ${ }^{5}$ which adopt pretrained CLIP [60] features, we use the same visual features without end-to-end optimization. Table 12 presents navigation performances on RxR test split. Our multilingual HAMT model achieves $12.83 \%$ and $6.25 \%$ gains on SR and nDTW respectively than the second place. Nevertheless, there is still a large gap compared to the human performance. We further present results on val seen and val unseen splits in Table 13.</p>
<h2>C. 3 REVERIE dataset</h2>
<p>The remote object localization task in REVERIE dataset requires both navigation and object grounding. To support the two subtasks in HAMT, we concatenate object features with original view image features for each viewpoint, and add an object grounding head to predict the target object given output embeddings of all object tokens. We fine-tune HAMT that is end-to-end pretrained on R2R dataset, and use the optimized ViT to extract object features given groundtruth object bounding boxes in REVERIE dataset. As shown in Table 14, HAMT achieves better navigation performance (SR and SPL), but the object grounding performance (RGS and RGSPL) on test split is worse than state of the art. Since HAMT can more effectively encode observed visual scenes and actions in the history sequence, it is able to better understand house environments and navigate to target viewpoints more</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 14: Navigation and object grounding performances on REVERIE val unseen and test splits.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Test Unseen</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Navigation</td>
<td></td>
<td></td>
<td>Grounding</td>
<td></td>
<td></td>
<td>Navigation</td>
<td></td>
<td></td>
<td>Grounding</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>TL</td>
<td>SR $\uparrow$</td>
<td>OSR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>RGS $\uparrow$</td>
<td>RGSPL $\uparrow$</td>
<td>TL</td>
<td>SR $\uparrow$</td>
<td>OSR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>RGS $\uparrow$</td>
<td>RGSPL $\uparrow$</td>
</tr>
<tr>
<td>Seq2Seq [6]</td>
<td>11.07</td>
<td>4.20</td>
<td>8.07</td>
<td>2.84</td>
<td>2.16</td>
<td>1.63</td>
<td>10.89</td>
<td>3.99</td>
<td>6.88</td>
<td>3.09</td>
<td>2.00</td>
<td>1.58</td>
</tr>
<tr>
<td>RCM [14]</td>
<td>11.98</td>
<td>9.29</td>
<td>14.23</td>
<td>6.97</td>
<td>4.89</td>
<td>3.89</td>
<td>10.60</td>
<td>7.84</td>
<td>11.68</td>
<td>6.67</td>
<td>3.67</td>
<td>3.14</td>
</tr>
<tr>
<td>SMNA [13]</td>
<td>9.07</td>
<td>8.15</td>
<td>11.28</td>
<td>6.44</td>
<td>4.54</td>
<td>3.61</td>
<td>9.23</td>
<td>5.80</td>
<td>8.39</td>
<td>4.53</td>
<td>3.10</td>
<td>2.39</td>
</tr>
<tr>
<td>FAST-MATTN [8]</td>
<td>45.28</td>
<td>14.40</td>
<td>28.20</td>
<td>7.19</td>
<td>7.84</td>
<td>4.67</td>
<td>39.05</td>
<td>19.88</td>
<td>30.63</td>
<td>11.6</td>
<td>11.28</td>
<td>6.08</td>
</tr>
<tr>
<td>SIA [16]</td>
<td>41.53</td>
<td>31.53</td>
<td>44.67</td>
<td>16.28</td>
<td>22.41</td>
<td>11.56</td>
<td>48.61</td>
<td>30.80</td>
<td>44.56</td>
<td>14.85</td>
<td>19.02</td>
<td>9.20</td>
</tr>
<tr>
<td>RecBERT [5]</td>
<td>16.78</td>
<td>30.67</td>
<td>35.02</td>
<td>24.90</td>
<td>18.77</td>
<td>15.27</td>
<td>15.86</td>
<td>29.61</td>
<td>32.91</td>
<td>23.99</td>
<td>16.50</td>
<td>13.51</td>
</tr>
<tr>
<td>HAMT</td>
<td>14.08</td>
<td>32.95</td>
<td>36.84</td>
<td>30.20</td>
<td>18.92</td>
<td>17.28</td>
<td>13.62</td>
<td>30.40</td>
<td>33.41</td>
<td>26.67</td>
<td>14.88</td>
<td>13.08</td>
</tr>
</tbody>
</table>
<p>efficiently as shown in the much higher SPL score. However, as we use ViT optimized on R2R dataset to extract object features, the object representation might not be as generalizable as object features used in previous works which are pretrained on large-scale object detection datasets.</p>
<h1>D Additional ablations</h1>
<h2>D. 1 History in training with proxy tasks</h2>
<p>We show that the history input plays a critical role for training with proxy tasks. We compare HAMT with history input and PREVALENT [22] without history. For fair comparison, we re-implement PREVALENT which only takes instruction $\mathcal{W}$ and single-step observation $\mathcal{O}_{t}$ as input and the other architectures are set the same as HAMT. We train PREVALENT with all proxy tasks except the ITM task because there is no trajectory input in PREVALENT for instruction-trajectory matching. ViT features pretrained on ImageNet are used in this experiment.
In Figure 5, we present the single-step action prediction (SAP) accuracy of HAMT and PREVALENT during the training. The SAP accuracies on val seen split are similar for the two models, however, PREVALENT performs much worse on the val unseen split than HAMT. Due to the capacity of large-scale transformer, PREVALENT is likely to memorize the map structure of seen houses, and thus achieves comparable performance to HAMT. However, such knowledge cannot be transferred to unseen houses because the structure and visual observations are distinct for seen and unseen houses. Feeding history as inputs avoids the model simply cramming the structure of seen houses, and enables it to align the history with an instruction to predict actions for better generalization. After fine-tuning the two models on R2R dataset, we obtain SPL 57.5 on val unseen split for HAMT, while 52.7 for PREVALENT without history input. As the same proxy tasks are used in training, the large gains of our HAMT model contribute to the history encoding. Therefore, the proposed history encoding can largely improve the navigation performance on top of training proxy tasks.</p>
<h2>D. 2 Visual features in training with proxy tasks</h2>
<p>Table 15 provides an additional experiment in the third row compared to Table 3a. It demonstrates that ViT features outperform ResNet152 features with and without training proxy tasks. Comparing the last two rows in Table 15, end-to-end feature optimization improves SPL by $2.1 \%$ on val unseen split but decreases SPL by $0.8 \%$ on val seen split. Note that we follow previous VLN works [5, 12] to select the best model based on val unseen and use the same model for val seen split. We observe that the performance on val seen split can be improved with longer training time.</p>
<p>Table 15: Comparison of features (same notations as Table 3a).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Features</th>
<th style="text-align: center;">PT</th>
<th style="text-align: center;">e2e</th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;">Val Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">SPL</td>
</tr>
<tr>
<td style="text-align: center;">Resnet</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr>
<td style="text-align: center;">152</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">69.3</td>
<td style="text-align: center;">64.8</td>
</tr>
<tr>
<td style="text-align: center;">ViT</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">66.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">72.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">71.7</td>
</tr>
</tbody>
</table>
<p>After optimizing visual representations, HAMT converges faster on val unseen split and achieves the best performance at earlier iterations. Therefore, the performance on val seen split is slightly worse than no end-to-end optimization. If training longer, the performance with optimized ViT features on val seen split can be higher.</p>
<h1>D. 3 Different proxy tasks in end-to-end training</h1>
<p>Table 16: Comparison of different proxy tasks in end-to-end optimization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Unseen</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SAP(R)</td>
<td style="text-align: center;">SPREL</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">SPL</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">SPL</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">65.9</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">57.7</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">59.4</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{7 5 . 0}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 9}$</td>
</tr>
</tbody>
</table>
<p>In Table 3b of the main paper, we fix ViT features to ablate contributions of different proxy tasks in training. We further present the ablation results in a fully end-to-end training setup in Table 16, where different proxy tasks are used to train HAMT including the ViT features. The results show the same trend as Table 3b, where our proposed two new proxy tasks (SAP/R and SPREL) are beneficial. Moreover, we can see that the end-to-end ViT features are superior to fixed ViT features in Table 3b on val unseen split for all the three proxy task combinations.</p>
<h2>D. 4 Two-stage end-to-end (e2e) training strategy</h2>
<p>We compare our two-stage e2e training strategy with a single-stage e2e training of HAMT. However, single-stage e2e training achieves inferior performance to the two-stage training or even no e2e training. When trained for 25 k iterations and evaluated on the val unseen split, the single-stage e2e training of HAMT results in SPL 53.5 while no e2e training achieves SPL 56.5. We hypothesize that the single-stage e2e training is less effective for VLN given (a) the limited training data available for the VLN task and (b) the higher complexity of VLN compared to common vision and language tasks.</p>
<h2>D. 5 History encoding in long-horizon VLN task</h2>
<p>We compare different history encoding approaches on the R2R-Back dataset to show that the history information is more beneficial for the long-horizon VLN task. Table 17 presents navigation results. All the models are initialized from weights after training with proxy tasks. In order to successfully return back, the agent should remember the way it comes to the targets. The recurrent state is insufficient to capture all the information and achieves the worst navigation performance. Encoding agent's oriented view at each step in temporal-only model improves over the recurrent approach. However, as the oriented view of the agent in backward trajectory is different from the view in forward trajectory, temporal-only model does not take advantage of the full memory in previous exploration and performs inferior to our hierarchical history encoding model. It demonstrates the effectiveness of our proposed method in long-horizon VLN task that requires long-term dependency. We also show that using the end-to-end trained ViT features further benefits the navigation performance.</p>
<p>Table 17: Navigation results for R2R-Back dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">History <br> Encoding</th>
<th style="text-align: center;">e2e</th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">nDTW $\uparrow$</td>
<td style="text-align: center;">SDTW $\uparrow$</td>
<td style="text-align: center;">TL</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">nDTW $\uparrow$</td>
<td style="text-align: center;">SDTW $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">Recurrent</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">22.33</td>
<td style="text-align: center;">51.4</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">23.35</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">35.6</td>
</tr>
<tr>
<td style="text-align: center;">Temporal-only</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">22.70</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">46.7</td>
<td style="text-align: center;">22.93</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">40.2</td>
</tr>
<tr>
<td style="text-align: center;">Hierarchical</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">23.52</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">24.58</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">48.4</td>
</tr>
<tr>
<td style="text-align: center;">Hierarchical</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">22.76</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">23.78</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">49.5</td>
</tr>
</tbody>
</table>
<h2>D. 6 Structure variants in fine-tuning</h2>
<p>Our model reuses the $f_{\mathrm{SAP}}\left(o_{i}^{\prime} \odot x_{\mathrm{cls}}^{\prime}\right)$ in training proxy tasks to sequentially predict action in fine-tuning. In Table 18, we compare using different input tokens for the action prediction in $f_{\mathrm{SAP}}$, including different combinations of the observation token $o_{i}^{\prime}$, global history token $h_{\mathrm{cls}}^{\prime}$ and special text token $x_{\mathrm{cls}}^{\prime}$. We can see that the performance varies little on the val unseen split, which indicates that the crossmodal transformer in our model is able to effectively fuse different modalities so that the performance is influenced little by tokens used in prediction.</p>
<p>Table 18: Comparison of using different tokens in $f_{\mathrm{SAP}}$ in fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Action</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Val Unseen</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prediction Token</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">obs</td>
<td style="text-align: center;">txt</td>
<td style="text-align: center;">hist</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">65.7</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">75.9</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">65.5</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is "Walk to the right of the stairs. Continue past and to the right of the stairs that go down. Turn right and stop in the doorway of the double glass doors." (id: 697_0). The RecBERT misunderstands the instruction and goes down the stairs instead of turning right. Our HAMT is better to understand the instruction and spatial relation related to the stairs to turn to the right of the stairs.</p>
<h1>E Qualitative results</h1>
<p>Figures 6-9 illustrate trajectories obtained by our HAMT model and compare them to results of the state-of-the-art RecBERT [5] model. We can see that HAMT enables to better interpret instructions (Figure 6), recognize the scene (Figure 7), follow the correct direction (Figure 8), and align the current observation with the instruction (Figure 9). We also provide some failure cases in Figures 10-11, where the HAMT model still needs improvements on scene and object recognition.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is "Walk into the kitchen area. Walk by the sink and oven. Walk straight into the hallway. Turn right into the little room. Turn left and walk into the bedroom. Stop by the corner of the bed." (id: 155_0). The RecBERT fails to recognize the kitchen area and navigates back and forth in wrong locations. Our HAMT correctly recognizes the kitchen and follows the instruction.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Examples in R2R val unseen split. Navigation steps inside red box are incorrect. The instruction is "Walk straight until you get to a room that has a black table on the left with flowers on it. Wait there." (id: 4182_2). The RecBERT takes the wrong direction at the first step, while our HAMT follows the instruction and successfully stops.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://ai.google.com/research/rxr/competition?active_tab=leaderboard (25/10/2021).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
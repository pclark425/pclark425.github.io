<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5614 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5614</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5614</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267095393</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.12794v3.pdf" target="_blank">Benchmarking LLMs via Uncertainty Quantification</a></p>
                <p><strong>Paper Abstract:</strong> The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5614",
    "paper_id": "paper-267095393",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00584475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking LLMs via Uncertainty Quantification
31 Oct 2024</p>
<p>Fanghua Ye fanghua.ye.19@ucl.ac.uk 
Tencent AI Lab</p>
<p>University College London</p>
<p>Mingming Yang 
Tencent AI Lab</p>
<p>Jianhui Pang 
Tencent AI Lab</p>
<p>University of Macau</p>
<p>Longyue Wang 
Tencent AI Lab</p>
<p>Derek F Wong derekfw@um.edu.mo 
University of Macau</p>
<p>Emine Yilmaz emine.yilmaz@ucl.ac.uk 
University College London</p>
<p>Shuming Shi shumingshi@tencent.com 
Tencent AI Lab</p>
<p>Zhaopeng Tu zptu@tencent.com 
Tencent AI Lab</p>
<p>Benchmarking LLMs via Uncertainty Quantification
31 Oct 20248AE55EBEDC3EC2EA61E261F113AEDA01arXiv:2401.12794v3[cs.CL]
The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods.However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -uncertainty, which is vital for thoroughly assessing LLMs.To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification.Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks.Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs.These results underscore the significance of incorporating uncertainty into the evaluation of LLMs.Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have gained significant traction within both academia and industry, with numerous organizations and companies open-sourcing their versions of LLMs [9,74,36,68].LLMs are highly versatile, demonstrating capabilities in various tasks such as question answering, document summarization, dialogue systems, and machine translation [70,52].Given the growing interest and advancements in LLMs, it is crucial to establish appropriate methods for evaluating their performance [42,71,9].However, conducting a comprehensive evaluation of LLMs remains a challenging endeavor [28,75].</p>
<p>To address this challenge, several open leaderboards such as the popular HuggingFace open LLM leaderboard, 2 OpenCompass [14], Chatbot Arena [75], and FlagEval [6] have emerged, providing a comparative analysis of LLM performance.Despite their usefulness, these leaderboards possess a significant limitation: They do not take into account the uncertainty of LLMs.For example, the HuggingFace open LLM leaderboard only utilizes accuracy as the evaluation metric.However, as demonstrated in Figure 1, two LLMs may achieve identical accuracy scores but exhibit different levels of uncertainty regarding the question.This is analogous to students taking exams of multiple-choice questions, where two students may select the same answer but actually possess distinct degrees of uncertainty or comprehension about the question.Consequently, it is necessary to incorporate uncertainty into the evaluation process to achieve a more comprehensive assessment of LLMs.</p>
<p>In this paper, we propose the utilization of conformal prediction [64,5] as the method to quantify uncertainty in LLMs.Compared to alternative methods such as Bayesian variational inference [30], conformal prediction offers multiple advantages including ease of implementation, high efficiency, distribution-free and model-agnostic, and a statistically rigorous estimation of uncertainty rather than a heuristic approximation [5].Hence, conformal prediction can serve as a practical and principled means for assessing the uncertainty of LLMs.</p>
<p>Figure 1: An illustration of two LLMs accurately predicting the true answer (with option A possessing the highest probability), but showing different levels of uncertainty.Note that when both LLMs predict a wrong answer, they may also display different levels of uncertainty.Specifically, we benchmark nine open-source LLMs (LLM series) across five typical Natural Language Processing (NLP) tasks, namely question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization.Given that most existing open leaderboards and benchmarking datasets [45] focus on multiplechoice tasks, we also adopt the multiple-choice question setting for all tasks.Although some of these tasks (e.g., document summarization) are inherently generative, it is challenging to develop a deterministic and reproducible method for quantifying the uncertainty within the generated text due to randomness in the generation process.Instead, we convert all tasks into multiple-choice questions, with the objective of each task being to select the correct option from the provided choices.Our empirical results reveal the following observations: I) LLMs demonstrating higher accuracy may exhibit lower certainty; II) LLMs with larger scales may display greater uncertainty than their smaller counterparts; and III) LLMs after instruction-finetuning tend to possess higher uncertainty.</p>
<ol>
<li>
<p>Identify a heuristic notion of uncertainty based on the model f ; 2. Define a conformal score function s(X, Y ) ∈ R with larger scores encoding worse agreement between X and Y ;</p>
</li>
<li>
<p>Compute conformal scores on the calibration set s 1 = s(X</p>
</li>
</ol>
<p>c , Y</p>
<p>c ), . . ., s n = (X
(n) c , Y(n)
c ) and calculate a threshold q as the ⌈(n+1)(1−α)⌉ n quantile of the calibration scores, q = quant {s 1 , . . ., s n }, ⌈(n + 1)
(1 − α)⌉ n ,(2)
where ⌈•⌉ is the ceiling function; 4. Construct the prediction set for each test instance X t as
C(X t ) = {Y ′ ∈ Y : s(X t , Y ′ ) ≤ q}.(3)
For classification tasks, it is a common choice to adopt the softmax score (i.e.estimated probability of each class by the model) as the heuristic notion of uncertainty.However, this score usually does not reflect the true class distribution due to over-confident or under-confident model predictions.In this work, we consider two conformal score functions to convert the softmax score to a statistically rigorous notion of uncertainty (which is calibrated in the sense that the prediction sets satisfy the coverage guarantee requirement).</p>
<p>Least Ambiguous set-valued Classifiers (LAC) LAC [58] defines the conformal score function as
s(X, Y ) = 1 − f (X) Y ,(4)
where f (X) Y is the softmax score corresponding to the true label.It has been proven that LAC can lead to prediction sets with the smallest average size [58].However, it may undercover hard instances and overcover easy ones.</p>
<p>Adaptive Prediction Sets (APS) APS [57] defines the conformal score function as
s(X, Y ) = {Y ′ ∈Y:f (x) Y ′ ≥f (x) Y } f (X) Y ′ ,(5)
where f (X) Y ′ represents the softmax score corresponding to the label Y ′ ∈ Y. Equation ( 5) is equivalent to summing the ranked scores of each label, from the higher to the lower, until reaching the true label.Compared to LAC, APS leverages the softmax scores of all labels, not just the true label.It addresses the limitation of LAC but suffers from, on average, larger prediction sets.</p>
<p>The overall process of employing conformal prediction for uncertainty quantification in LLMs is illustrated in Figure 2. In the following sections, we first elucidate on the evaluation tasks and their associated datasets, then provide details about the evaluation prompts used to extract softmax scores (i.e.predicted probabilities) from LLMs, and finally, introduce the adopted evaluation metrics.</p>
<p>Evaluation tasks and datasets</p>
<p>LLMs have demonstrated remarkable capabilities across various aspects [28,50].It is essential to develop multiple tasks to evaluate their performance comprehensively.For this purpose, we consider five typical NLP tasks, including question answering, reading comprehension, commonsense inference, dialogue response selection, and document summarization.For each task, we prepare a dataset with 10,000 instances.In addition, we formulate each task as a Multiple-Choice Question Answering (MCQA) task and the objective is to select the only correct answer out of six possible options (i.e.A, B, C, D, E, and F).It is worth emphasizing that the prevailing benchmarking open leaderboards and datasets also focus on MCQA tasks [28,45].</p>
<p>Question Answering (QA) QA is applied to evaluate an LLM's proficiency in utilizing its extensive world knowledge to provide answers to a diverse range of questions.For this task, we adopt MMLU [29] as the evaluation dataset.MMLU encompasses a total of 57 subjects, spanning various disciplines such as elementary mathematics, US history, computer science, and law.These subjects are further classified into four broad categories, namely humanities, social sciences, STEM, and others (business, health, misc.).For each category, we sample 2500 instances, leading to 10,000 instances in total.</p>
<p>Reading Comprehension (RC) RC is used for testing an LLM's ability to understand and analyze a given context, comprehend the meaning of words and sentences, and answer questions based on the information presented in the context.It also tests the ability of LLMs to make inferences and draw conclusions from the given context.We take CosmosQA [31] as the evaluation dataset.</p>
<p>CosmosQA focuses on reading between the lines [51] over a diverse collection of people's everyday narratives that require reasoning beyond the exact text spans in the context.Due to the unavailability of ground truth labels for the test set in CosmosQA, we sample 10,000 instances from the training and development sets.</p>
<p>Commonsense Inference (CI) CI is leveraged to evaluate the ability of LLMs to understand and reason about the relationships between concepts and events based on commonsense and background knowledge.This type of testing helps to assess the generalization and reasoning abilities of LLMs beyond simple pattern recognition tasks.We employ HellaSwag [72] as the evaluation dataset.HellaSwag focuses on commonsense natural language inference whose target is to select the most likely followup to a given event description.Same as CosmosQA, we sample 10,000 instances from the training and development sets of HellaSwag for the purpose of evaluation.</p>
<p>Dialogue Response Selection (DRS) DRS is adopted for assessing the ability of LLMs to comprehend the meaning of a given dialogue and select an appropriate response from a set of possible responses.This includes the ability to comprehend the meaning of the user's input, select appropriate responses that are relevant to the conversational context, and maintain coherence and consistency in the dialogue.We utilize the dialogue data from the HaluEval [40] benchmark as the evaluation dataset (denoted as HaluDial).This dataset consists of exactly 10,000 instances and is built upon OpenDialKG [49], a knowledge-grounded dialogue dataset.</p>
<p>Document Summarization (DS) DS is taken to evaluate the proficiency of LLMs in comprehending the substance and context of a given document, and in producing a succinct and cohesive summary that effectively conveys the crucial information and main ideas of the document.This requires the LLM to have a good understanding of the language, the topic, and the structure of the document.Similar to DRS, we adopt the summarization data from the HaluEval [40] benchmark as the evaluation dataset (denoted as HaluSum).This dataset also comprises precisely 10,000 instances and is derived from CNN/Daily Mail [59], a summarization dataset pertaining to news articles.</p>
<p>Out of the aforementioned five datasets, MMLU, CosmosQA, and HellaSwag originally consisted of questions with four options each, while HaluDial and HaluSum had only two options per question.To standardize the number of options, two additional choices were added to each question in HaluDial and HaluSum by randomly selecting from the choices of other questions in HaluDial and HaluSum, respectively.Furthermore, all datasets were modified to include two more options, "I don't know" and "None of the above", resulting in a total of six possible options for each question.</p>
<p>Evaluation prompts and metrics</p>
<p>As delineated in § 3, one crucial step of leveraging conformal prediction for uncertainty quantification is to acquire the softmax score corresponding to each option.Next, we explicate the method used to elicit these scores from LLMs, followed by a description of the evaluation metrics utilized.</p>
<p>Prompting Strategies Following previous works [29,24,73,12,75], we rely on prompt engineering rather than supervised finetuning as the testing approach to evaluating the performance of LLMs on each task.However, our preliminary results show that LLMs are sensitive to prompts.In this regard, we consider three prompting strategies, including Base Prompt, Shared Instruction Prompt, and Task-specific Instruction Prompt, to reduce the influence of LLMs' sensitivity to different prompts, thereby ensuring a fairer comparison.</p>
<p>• Base Prompt: This strategy directly combines the question and all of its options as the input and prompts the LLM to output the correct option with a prefix "Answer:".</p>
<p>• Shared Instruction Prompt: This strategy adds a general description of the task before the question, informing the LLM that the task is to solve a multiple-choice question and there is only one correct answer out of six options.</p>
<p>• Task-specific Instruction Prompt: Instead of using a shared instruction, this strategy provides a task-specific instruction that briefly describes the task and the expected type of option.</p>
<p>These prompting strategies facilitate a systematic and standardized evaluation of the performance of LLMs.The prompt templates linked with each strategy are elaborated in Appendix D. The softmax score for each prompt is derived by subjecting the logits corresponding to each option letter (i.e.A, B, C, D, E, and F) to the softmax function.The said logits are generated by the language modeling head in contemporary causal LLMs.It is worth noting that only the logits associated with the last token of the prompt input are utilized.</p>
<p>Evaluation Metrics We evaluate LLMs from two perspectives, namely prediction accuracy and prediction uncertainty.For prediction accuracy, we adopt the commonly used metric -Accuracy (Acc).To evaluate prediction uncertainty, we use Set Size (SS), which is a primary metric for conformal prediction [5].Let Y p be the prediction for the test instance (X t , Y t ) ∈ D test .These two metrics can be calculated as follows:
Acc = 1 |D test | (Xt,Yt)∈Dtest 1(Y p = Y t ), SS = 1 |D test | (Xt,Yt)∈Dtest |C(X t )|,(6)
where 1(•) is the indicator function.</p>
<p>In addition to Acc and SS, we report the Coverage Rate (CR) to verify if the coverage guarantee requirement shown in Eq. (1) has been satisfied.The CR metric is calculated as
CR = 1 |D test | (Xt,Yt)∈Dtest 1(Y t ∈ C(X t )). (7)
6 Evaluation results</p>
<p>Setup</p>
<p>In our experiments, we set the error rate α to 0.1, implying that the prediction set should include the true label with a probability of at least 0.9.In order to better excite the ability of LLMs, we incorporate examples or demonstrations in the prompt, adhering to the in-context learning paradigm [19].Specifically, we provide five demonstrations for QA, RC, and CI tasks.For the DRS task, we utilize three demonstrations, while we use only one demonstration for the DS task due to constraints on input length and inference cost.The maximum input length for all tasks is set to 2048 tokens.In addition, for each task, we allocate 50% of the data as the calibration set and the remaining 50% as the test set.We report results on the test set.These results represent the average value obtained from the two conformal score functions, namely, LAC and APS, as well as the three prompting strategies.It is noteworthy that while both LAC and APS fulfill the coverage guarantee requirement, they may produce prediction sets of varying sizes.By taking the average value of these two score functions, we aim to mitigate the influence of different score functions on the evaluation of uncertainty, thereby ensuring a more rigorous and reliable assessment.</p>
<p>Evaluated models</p>
<p>We select a diverse set of nine representative models (or model series) from the vast array of opensource LLMs available.These models encompass various architectures and training methodologies, thereby allowing for a comprehensive benchmarking analysis.Specifically, the chosen models include the Llama-2 series [63], Mistral-7B [34], Falcon series4 [3], MPT-7B [62], Gemma-7B [60], Qwen series [7], Yi series [1], DeepSeek series [15], and InternLM-7B [61].For all models, we utilize their checkpoints from the HuggingFace platform: https://huggingface.co/models.</p>
<p>Main findings</p>
<p>In our primary experiments, we focus on LLMs with sizes ranging from 6B to 14B parameters.The outcomes of CR, Acc and SS are presented in Table 1.As previously mentioned, the reported results are the mean value derived from the two conformal score functions, LAC and APS.For a detailed analysis of the results pertaining to each function, please refer to Appendix C.1.In principle, an LLM having higher accuracy is expected to demonstrate lower uncertainty.However, as shown in Table 1, the results regarding the SS metric reveal that in practice, higher accuracy does not necessarily correlate with lower uncertainty.Concretely, for each task, we observe that the ranking of LLMs based on accuracy differs from that based on uncertainty, suggesting that some LLMs possessing higher accuracy actually display higher uncertainty.Notably, two LLMs with a significant difference in accuracy may even display inverse uncertainty.For example, on the DRS task, InternLM-7B demonstrates higher performance than MPT-7B in accuracy by 19.34 absolute points, yet it shows higher uncertainty.This pattern is also observed on the QA task with Qwen-7B and Llama-2-7B (9.61), the CI task with Qwen-14B and Yi-6B (14.50), and the DS task with InternLM-7B and Falcon-7B (9.69).In each case, the LLM with much higher accuracy exhibits greater uncertainty compared to its counterpart.These observations underscore the importance of considering uncertainty in addition to accuracy when evaluating LLMs.</p>
<p>Effects of model scale</p>
<p>LLMs with larger sizes are usually pretrained on more data and tend to exhibit superior capabilities across various tasks.In this study, we aim to investigate how an LLM's performance changes when scaling its model size.We present the results of the Qwen model series in Figure 3.It is evident that in the majority of cases, the coverage guarantee requirement has been satisfied.In terms of Acc, with the exception of Qwen-72B and Qwen-14B on the CI task, an increase in model size consistently leads to improved performance.Concerning uncertainty (i.e.SS), there is a general trend of decreasing uncertainty when scaling the model size from 1.8B to 14B.However, Qwen-7B displays higher uncertainty compared to Qwen-1.8B on the QA task.When further scaling the model size from 14B to 72B, the enhancements in uncertainty become less pronounced, and more variations are observed.Notably, on both the RC and DRS tasks, Qwen-72B demonstrates higher uncertainty than Qwen-14B, although Qwen-72B achieves higher accuracy.</p>
<p>We provide the results of the Llama-2 series, Yi series, DeepSeek series, and Falcon series in Appendix C.2, which reveal similar findings.</p>
<p>Effects of instruction finetuning</p>
<p>In this part, we further delve into the comparative analysis of the performance between the base pretrained model and the instruction-finetuned variant of LLMs.The average results across five tasks of the Llama-2 model series are illustrated in Figure 4.For the instruction-finetuned version, two methods are adopted to prepare the prompt input.The first method aligns with the format of the instruction data 6 (denoted as Chat-V1).This method aims to evaluate the model's proficiency in adhering to instructions to accomplish tasks.The second method employs the same prompt format as the base version (denoted as Chat-V2).This method aims to assess the extent of the base model's capabilities retained after instruction-finetuning.</p>
<p>Figure 4 shows that Chat-V1 consistently results in lower accuracy and higher uncertainty across all model sizes.Conversely, Chat-V2 enhances accuracy for Llama-2-7B and Llama-2-13B.However, for Llama-2-70B, Chat-V2 also leads to a decline in accuracy.Regarding uncertainty, Chat-V2 consistently results in increased uncertainty compared to the base model, although the extent of degradation is less severe than Chat-V1.These findings suggest that instruction-finetuning tends to impair model performance, particularly in terms of uncertainty.</p>
<p>We provide the results of the Yi series, DeepSeek series, and Falcon series in Appendix C.3.Given the predicted probabilities, another widely used measure of uncertainty is entropy [56].There have also been some entropy-based uncertainty quantification methods for language models [20].</p>
<p>Here, we compare conformal prediction to entropy.Since conformal prediction uses the prediction set size (SS) to measure uncertainty, a direct comparison with entropy is not straightforward.To address this issue, we convert entropy to perplexity [33], which is defined as P P L = 2 H , where H denotes the entropy.Perplexity takes values in the range of [1, |Y|], allowing it to be interpreted as prediction set size.For instance, when the predicted probability for each class (option) is 1 |Y| , P P L = |Y|.The results regarding InternLM-7B are presented in Table 2, from which, we observe that the coverage rate of perplexity varies significantly across different tasks.On the QA task, the coverage rate is only 83.44%.In contrast, the coverage rate of conformal prediction consistently exceeds 90%.This is because when measuring uncertainty, entropy doesn't take accuracy into account.Entropy remains the same when predicted probabilities are permuted, even though prediction accuracy may differ.</p>
<p>To further demonstrate the superiority of conformal prediction, we conduct additional experiments comparing it with entropy and maximal predicted probability in terms of the Expected Calibration Error (ECE) metric [27].The results corresponding to InternLM-7B are presented in Table 3.The observation that conformal prediction yields the lowest average ECE score suggests that it offers more reliable uncertainty quantification.</p>
<p>Overall, these results demonstrate the advantages of adopting conformal prediction for uncertainty quantification.We provide more analyses in Appendix C.7.In this part, we extend our benchmarking from open-source LLMs to closedsource LLMs.While obtaining the exact output logits of closed-source LLMs is challenging, we can sample multiple answers and then estimate the probability of each choice.We perform an experiment on the MMLU (the QA task) dataset with GPT-3.5 and GPT-4 as the closed-source LLMs.To save cost, we only consider the base prompting strategy.Specifically, we first sample 50 answers for each question and calculate the frequency of each option.Then, we apply the softmax function with temperature scaling to prevent zero probabilities.To demonstrate the quality of this approximation, we also report the results of the open-source model Qwen-72B when getting its predictions via sampling and via logits, respectively.The results are shown in Table 4.</p>
<p>Expanding benchmarking to closed-source LLMs</p>
<p>It is observed that GPT-4 demonstrates the highest accuracy and the lowest uncertainty.In addition, the average prediction set size (SS) of Qwen-72B (sampling) is relatively close to that of Qwen-72B (logits).For each question, we further calculate the Jensen-Shannon divergence (JSD) between the predictions of Qwen-72B (sampling) and Qwen-72B (logits).The average JSD is 0.05, indicating that the two predictions (estimated probability distributions) are highly similar.Therefore, we conclude that the approximation is of high quality.</p>
<p>Expanding benchmarking to free-form text generation</p>
<p>Here, we further extend our benchmarking from multiple-choice question answering to free-form text generation.However, applying conformal prediction to text generation is a complex task due to the extensive range of potential responses.It is not feasible to compute the probability for each possible response and then use conformal prediction to select a subset.Nevertheless, many potential responses have a low probability of being generated, which allows us to reduce the selection space by sampling multiple generations.</p>
<p>Specifically, we adopt the TriviaQA dataset [35] (sampling 10,000 dev instances) for free-form text generation.We first generate 20 answers for each question.Then, we employ the perplexity [33] of each generation as the conformal score function and utilize exact match to verify the accuracy of the generated answer.The results are displayed in Table 5.Note that the value of SS falls into [1,20].</p>
<p>From Table 5, we observe that the prediction set size (SS) varies among LLMs, which could provide some insights into the uncertainty of these models.However, we must note that in this sampling setting, the coverage rate cannot be guaranteed any more because there might not be a correct answer within the 20 sampled responses.In other words, even if the prediction set size is 20, indicating high model uncertainty, the coverage rate for that instance could still be zero if there are no correct answers present.Nonetheless, it is observed that when the LLM is stronger, the coverage guarantee requirement is more likely to be satisfied.</p>
<p>In-depth analysis of the set size metric in relation to prediction accuracy</p>
<p>While our main focus is on the high probability of the prediction set covering the ground truth, it is also insightful to explore the relationship between stratified set size and prediction accuracy.In our experiments, we employ InternLM-7B on the QA task, grouping instances by their predicted set size and reporting the accuracy within each group in Table 6.The results reveal that instances with smaller set sizes are generally associated with higher prediction accuracy, indicating that set size serves as a useful indicator of prediction uncertainty.However, it is important to note that even when the set size reaches its maximum value, there are still instances where the prediction is accurate.Consequently, a comprehensive analysis of both prediction accuracy and prediction uncertainty is essential for a thorough assessment of the performance of LLMs.</p>
<p>Conclusion</p>
<p>In this work, we have provided an extensive examination of the performance of LLMs by focusing on prediction uncertainty.To achieve this, we have employed conformal prediction for uncertainty quantification.Our comprehensive investigation, which involves nine open-source LLMs (or LLM series) and spans five typical NLP tasks, demonstrates that relying solely on accuracy for benchmarking LLMs is insufficient.Instead, it is imperative to take uncertainty into account when assessing their overall performance.Last but not least, we have verified the superiority of conformal prediction compared to several other uncertainty quantification methods.We have also extended our analyses to closed-source LLMs and free-form text generation.We present a summary of the pseudo code for applying conformal prediction to quantify the uncertainty of LLMs in Algorithm 1.The procedure is outlined as follows:</p>
<p>A Pseudo code</p>
<p>1.For each instance, input it into the LLM to obtain the logits output for all possible options.</p>
<ol>
<li>
<p>Apply the softmax function to transform these logits into probability values.</p>
</li>
<li>
<p>Divide the dataset into a calibration set and a test set.</p>
</li>
<li>
<p>Employ the user-specified error rate α and the calibration set to determine the conformal threshold.</p>
</li>
<li>
<p>Generate prediction sets for instances in the test set based on the conformal threshold.</p>
</li>
</ol>
<p>6.In the event that a prediction set is empty, select the option with the highest probability as the final prediction.</p>
<ol>
<li>Calculate the evaluation metrics, namely Acc, SS, and CR.</li>
</ol>
<p>In our experiments, we use a server with eight A100 40GB cards to load each LLM checkpoint and perform inference with a batch size of 1.</p>
<p>B Dataset statistics</p>
<p>Figure 5 presents the distribution of correct answer choices for each task.It is noteworthy that while we have incorporated options E ("I don't know") and F ("None of the above") for every question, the correct answer consistently falls within the set {A, B, C, D}.As depicted in Figure 5, the distribution of correct answers is nearly uniform across options A, B, C, and D for all tasks except the QA task.However, even on the QA task, the distribution does not exhibit a significant skew.These statistics indicate that the created datasets are suitable for rigorously evaluating the performance of LLMs.The detailed results are reported in Table 7, Table 8, and Table 9.</p>
<p>It is evident that for both conformal score functions, the ranking of LLMs based on accuracy can be different from that based on uncertainty.These results reaffirm the importance of considering uncertainty in order to evaluate the performance of LLMs in a more holistic manner.Another notable observation is the difference in the uncertainty estimations produced by LAC and APS.In general, APS tends to produce larger prediction sets.More importantly, APS can lead to a significantly different ranking of LLMs based on uncertainty compared to LAC.For example, on the CI task, Qwen-14B secures the lowest uncertainty when LAC is utilized as the conformal score function.However, when APS is employed as the conformal score function, Qwen-14B is ranked sixth.This observation suggests that it is essential to average the results of the two conformal score functions  to provide a more accurate quantification of uncertainty.Last but not least, both conformal score functions can achieve high coverage rates, verifying again the rationality of relying on prediction set size to estimate uncertainty.It is also noted that APS tends to achieve higher coverage rates than LAC due to its larger prediction sets in most cases.</p>
<p>C.  DS task, Llama-2-70B exhibits inferior performance compared to Llama-2-13B across both accuracy and uncertainty.On the CI task, although Yi-34B demonstrates higher accuracy than Yi-6B, it shows higher uncertainty.</p>
<p>C.3 Effects of instruction finetuning (cont.)</p>
<p>Figures 10-12 depict the average results across five tasks of the Yi series, DeepSeek series, and Falcon series, as well as their instruction-finetuned counterparts.Recall that for the instruction-finetuned version, two approaches are adopted to prepare the prompt input.The first approach adheres to the format of the instruction data, which is denoted as Chat-V1.The second approach employs the same prompt format as the base version and is denoted as Chat-V2.For the Yi series, it is observed that both Chat-V1 and Chat-V2 consistently yield inferior performance than the base model in terms of both Acc and SS.In contrast, for the DeepSeek series, both Chat-V1 and Chat-V2 exhibit enhanced performance in terms of Acc.Nevertheless, Chat-V1 results in greater uncertainty compared to the base model for both DeepSeek-7B and DeepSeek-70B.Chat-V2 also demonstrates higher uncertainty relative to the base model for DeepSeek-7B.For the Falcon series, it is observed that both Chat-V1 and Chat-V2 consistently lead to higher uncertainty than the base model, even though Chat-V2 achieves better performance in terms of Acc.average uncertainty among the four LLMs.These observations show that MoE is indeed an effective method to enhance the accuracy and reduce the uncertainty of LLMs.</p>
<p>C.4 Effects of mixture of experts</p>
<p>C.5 Effects of error rate</p>
<p>Conformal prediction ensures that the prediction set encompasses the true label with a probability no less than 1 − α, where α denotes a user-specified error rate.In consideration of this, it is imperative to investigate the impact of varying the value of the error rate α on the prediction set and, subsequently, the estimation of uncertainty.For this purpose, we vary the value of α within the range of 0.05 to 0.3 and report the results of the Llama-2 series in Figure 13.It is observed that as the value of α increases, the coverage rate decreases monotonically.This outcome is anticipated, as a higher error rate implies a reduced probability of the true label being included in the prediction set.Nevertheless, the coverage rate consistently remains greater than 1 − α.This observation reaffirms the statistical guarantee provided by conformal prediction in generating the prediction set.It is also observed that the average set size (SS) decreases monotonically with an increase in the value of α.This observation is logical, as a larger error rate suggests that the prediction set can miss the true label with a higher probability and consequently, have a smaller size.Another noteworthy observation is that, regardless of the value of α, Llama-2-70B consistently exhibits lower uncertainty than Llama-2-13B, and Llama-2-13B consistently displays lower uncertainty than Llama-2-7B.This finding is of paramount importance, as it demonstrates that although different values of the error rate α can lead to varying sizes of the prediction set, the relative rankings of different LLMs based on uncertainty remain unchanged.</p>
<p>C.6 Effects of amount of calibration data</p>
<p>As described in § 3, conformal prediction requires a calibration set to calculate the threshold q.In our prior analyses, we have allocated 50% of the data as the calibration set and the remaining 50% as the test set.Here, we explore the impact of varying the proportion of calibration data, ranging from 10% to 50%, on uncertainty quantification.Note that the same 50% of data is consistently used as the test We compare the performance of conformal prediction (CP) and perplexity (PPL).</p>
<p>set.The average results across five tasks of the Llama-2 series are shown in Figure 14.It is observed that there are no significant variations in coverage rate (CR) and uncertainty (SS) for all versions of the Llama-2 series when varying the amount of calibration data.This observation confirms the efficacy of applying conformal prediction for uncertainty quantification in our analysis.</p>
<p>C.7 Effects of softmax temperature</p>
<p>Recall that we utilize the softmax function to convert option logits generated by LLMs into probability values.These probability values are then employed by the LAC and APS score functions to estimate uncertainty.In practice, we can incorporate a temperature parameter τ into the softmax function to adjust the probability distributions [11], as shown in the following equation:
sof tmax(z i , τ ) = e z i τ m j=1 e z j τ ,(8)
where z = (z 1 , . . ., z m ) ∈ R m .A higher temperature results in a more uniform probability distribution (i.e. with higher entropy and "more random"), while a lower temperature leads to a sharper probability distribution, with one value dominating.In this part, we investigate how the temperature τ affects uncertainty estimation when using conformal prediction and perplexity, respectively.We modify the value of τ from 0.2 to 10.0 and report the results of InternLM-7B in Figure 15.Note that the temperature does not affect accuracy, so we omit results regarding accuracy and only provide results of CR and SS.</p>
<p>It is observed that when using conformal prediction, we can obtain relatively stable performance in terms of both coverage rate and uncertainty (measured by SS).However, perplexity is highly sensitive to the temperature τ .When τ takes small values, perplexity results in high certainty but low coverage rate.Note that when we vary the value of τ , we do not change the LLM's accuracy.Thus, a low temperature causes the LLM to be overconfident.In this scenario, it is actually not ideal to estimate low uncertainty.Conformal prediction penalizes this phenomenon by producing large prediction sets (implying high uncertainty), which is more desirable.It is also observed that when τ takes large values and the probability distribution is close to a uniform distribution (indicating high entropy and that the LLM becomes underconfident), perplexity produces large prediction sets, which are uninformative.In contrast, conformal prediction is able to produce compact prediction sets consistently.This advantage is attributed to the use of a calibration set in conformal prediction.In summary, these observations suggest that conformal prediction is a more reliable method than entropy or perplexity for evaluating uncertainty.</p>
<p>C.8 Unify all tasks as one</p>
<p>In our previous analyses, we treat each task as an independent one.Therefore, we need to calculate a separate conformal threshold for each task and generate the prediction set based on the task-specific threshold.However, considering that LLMs are capable of solving multiple tasks, it is possible to consider all five tasks in this study as a single unified task (assuming that the datasets corresponding Table 11: Results of unifying all tasks as a single joint one for which a common conformal threshold is computed for all tasks and the prediction sets are generated based on this shared threshold (reported in the "Joint" column).For the sake of comparison, we also include the average results of the five tasks when treated individually (reported in the "Average" column).Note that both settings achieve the same performance in terms of accuracy.to the five tasks are drawn from a joint distribution).By doing so, we only need to calculate one conformal threshold and generate the prediction set for all tasks based on this shared threshold.In particular, we combine the calibration sets of all tasks into one and similarly merge the test sets of all tasks into one.The results of various LLMs using this unified approach are presented in Table 11, where we also include the average results of the five tasks when treated individually for comparison purposes.It can be observed that when treating all tasks as a single (joint) one, all LLMs are still able to meet the coverage guarantee requirement.However, they exhibit higher uncertainty in terms of the average set size (SS).This finding suggests that while LLMs are indeed capable of addressing multiple tasks, it remains crucial to analyze each task independently since LLMs can demonstrate varying degrees of uncertainty across different tasks.</p>
<p>LLMs</p>
<p>C.9 Rate of predicted options being E or F When preparing datasets, in order to enhance the complexity of tasks and effectively quantify uncertainty, two additional answer choices, E ("I don't know") and F ("None of the above"), are incorporated into the datasets for each task.It is important to note that neither of these options represents the correct answer for any of the questions.With this in mind, our study aims to investigate whether an LLM might predict options E or F as the answer, and if so, the number of test instances for which such predictions would be made.Table 12 presents the results of various LLMs, demonstrating that, across all LLMs, only a tiny proportion of test instances are predicted to have a true answer of either E or F. Furthermore, we observe that for a particular LLM, there can be no questions whose predicted answer is E or F on some tasks (e.g., Mistral-7B on the RC task).</p>
<p>We also report the average prediction set size (SS) when options E and F are excluded from the answer choices to evaluate their impact on uncertainty quantification.The results, presented in  0.11 0.03 0.00 0.47 0.72 0.27 0.05 0.07 0.00 0.17 0.00 0.06 DeepSeek-67B 3.46 2.95 0.75 0.81 0.00 1.60 0.04 0.00 0.00 0.00 0.02 0.01 Yi-6B 5.88 1.01 0.00 5.71 0.00 2.52 0.05 0.15 0.00 0.03 0.00 0.05 Gemma-7B 0.61 0.03 0.00 0.03 0.04 0.14 0.00 0.00 0.00 0.00 0.00 0.00 Mistral-7B 0.18 0.00 0.00 0.01 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 Llama-2-13B 0.99 0.05 0.00 0.00 0.00 0.21 0.16 0.00 0.00 0.00 0.00 0.03 Qwen-7B</p>
<p>1.25 0.51 0.00 0.32 0.00 0.42 1.01 0.74 0.00 0.04 0.00 0.36 InternLM-7B</p>
<p>1.05 0.00 0.03 0.07 2.71 0.77 0.00 0.00 0.00 0.00 1.61 0.32 Llama-2-7B 0.39 0.00 0.00 0.00 1.85 0.45 0.01 0.00 0.00 0.00 0.12 0.03 DeepSeek-7B</p>
<p>1.05 1.67 0.00 0.00 0.39 0.62 0.58 0.00 0.00 0.00 0.00 0.12 Qwen-1.8B</p>
<p>1.65 0.51 0.68 0.00 1.01 0.77 0.00 0.00 0.00 0.00 0.52 0.10 Falcon-40B 0.15 0.01 0.00 0.00 3.72 0.78 0.00 0.05 0.05 0.00 0.95 0.21 MPT-7B 0.05 0.00 0.00 0.00 0.04 0.02 0.00 0.00 0.00 0.00 0.00 0.00 Falcon-7B 0.16 0.05 0.00 0.01 0.03 0.05 0.00 0.01 0.00 0.00 0.10 0.02 Consequently, the inclusion of these additional options does not significantly impact the accuracy of the evaluation process.This indicates that our approach to incorporating options E and F effectively increases the difficulty of the tasks without compromising the reliability of the assessment.With more options in the answer choices, we can also quantify uncertainty more accurately.</p>
<p>C.10 Comparison of prompting strategies</p>
<p>In § 5, we have introduced three prompting strategies and our previous analyses are based on the average results obtained from these prompting strategies, aiming to reduce the influence of LLMs' sensitivities to prompts.In this subsection, we delve deeper into the comparative performance of these prompting strategies.Specifically, we conduct experiments on the DS task and report the results of Yi-34B, Qwen-72B, Llama-2-70B, and DeepSeek-67B in Table 14.It can be observed that while the Inspired by in-context learning [66], our previous analyses have incorporated demonstrations for each task.Here, we aim to compare the performance of LLMs when demonstrations are included or excluded from the prompt input.The results of Yi-34B and Llama-2-70B are reported in Table 15.We observe that the presence of demonstrations leads to improved performance for both models, as evidenced by increased accuracy.Having demonstrations also leads to lower uncertainty for Llama-2-70B but higher uncertainty for Yi-34B.Overall, these results substantiate the effectiveness of incorporating demonstrations into the prompt to stimulate the capabilities of LLMs.</p>
<p>C.12 Case study Table 16 presents an example of the prediction sets produced by the Yi-34B model on the QA task.It is noteworthy that the correct answer is always encompassed in the prediction sets, irrespective of the employed prompting strategies (including base prompt, shared instruction prompt, and task-specific instruction prompt) and conformal score functions (including LAC and APS).In this specific example, we further observe that the LAC score function consistently produces prediction sets with smaller sizes compared to APS, with the prediction sets only containing the correct answer.However, the prediction sets generated by APS can exhibit variations dependent on the prompting strategies.In this particular example, we observe that different prompting strategies result in different prediction sets when APS is utilized as the conformal score function.This case study reveals that varying conformal score functions and prompting strategies can yield different measurements of uncertainty, even though the true answer is encompassed in the prediction sets.In practice, the mean results</p>
<p>D Prompt templates</p>
<p>We provide the prompt templates employed by the three prompting strategies in Table 17, Table 18, and Table 19, respectively.For the QA task, there is no background information for each question.</p>
<p>For the RC and CI tasks, each question has an associated contextual description, as indicated by the keyword "Context" in the prompt templates.For the DRS task and the DS task, we use "Dialogue" and "Document" rather than "Context" as the keywords to incorporate the dialogue history and document content into the prompt.When evaluating instruction-finetuned LLMs, we treat the entire prompt input as the message from users and then employ the "apply_chat_template" function to transform the prompt input into a chat format.</p>
<p>E Limitations</p>
<p>Despite the multiple advantages of conformal prediction over other uncertainty quantification methods, it demonstrates three key limitations when employed to assess the uncertainty of LLMs.Firstly, the application of conformal prediction necessitates access to model output logits, which precludes the possibility of benchmarking LLMs such as ChatGPT that are only accessible via their APIs.Secondly, the adoption of conformal prediction poses challenges in evaluating the generative capabilities of LLMs.In our analyses, all tasks are transformed into multiple-choice questions, thereby primarily assessing the language understanding abilities of LLMs rather than their generative potential.Thirdly, the prediction sets generated by conformal prediction could be significantly influenced by the conformal score function utilized.Thus, for a specific LLM, varying conformal score functions may yield disparate estimations of uncertainty.It is worth mentioning that while we have extended our benchmarking to closed-source LLMs and free-form text generation, the extension can only provide an approximation.</p>
<p>Nevertheless, the limitations of other uncertainty quantification methods prevent us from applying them in the context of LLMs.Per our knowledge, conformal prediction is currently the most feasible technique for robust uncertainty quantification of LLMs.Furthermore, some recent studies have tried to incorporate conformal prediction into the language generation process [55,53,16].We posit that conformal prediction will eventually evolve into an appropriate method for quantifying the uncertainty of language generation in the future.</p>
<p>Last but not least, it is important to note that the scope of this study is limited to evaluating the capabilities of LLMs in the context of language processing exclusively.The current trend in the field is towards the development of multi-modal foundation models [44,41,47], which have the capacity to process multiple modalities rather than just language.Therefore, it would be a significant extension of this research to investigate the uncertainty associated with these foundation models when they are applied to non-language modalities, which constitutes an important avenue for future research.</p>
<p>F Societal Impacts</p>
<p>While benchmarking LLMs via uncertainty quantification can lead to more accurate assessments of their performance, it is crucial to consider and address any potential negative societal impacts.We list several possible concerns below:</p>
<p>• Misuse of Technology: Enhanced performance and reliability of LLMs could lead to their misuse in generating misleading or harmful content, such as deepfakes, disinformation, or automated trolling.Improved uncertainty quantification might make these models more convincing and harder to detect.• Bias and Fairness: Even with improved uncertainty quantification, LLMs can still perpetuate and amplify existing biases present in the training data.This can lead to unfair treatment of certain groups or individuals, reinforcing stereotypes and discrimination.• Job Displacement: As LLMs become more capable, there is a potential for job displacement in fields that rely heavily on language processing, such as customer service, translation, and content creation.This could lead to economic and social challenges for affected workers.</p>
<p>Table 17: Prompt template for the base prompting strategy.Note that for the QA task, there is no background information pertaining to questions.For the RC and CI tasks, we adopt the keyword "Context" to include the contextual information associated with each question.For the DRS and DS tasks, we employ the keywords "Dialogue" and "Document" to incorporate the pertaining information, respectively.Table 18: Prompt template for the shared instruction prompting strategy.In this strategy, we add a shared general task description at the beginning of the prompt.Note that for the QA task, there is no background information pertaining to questions.For the RC and CI tasks, we adopt the keyword "Context" to include the contextual information associated with each question.For the DRS and DS tasks, we employ the keywords "Dialogue" and "Document" to incorporate the pertaining information, respectively.</p>
<p>Below are some examples of multiple-choice questions with six potential answers.For each question, only one option is correct.</p>
<p>{Demonstrations in the same format as the question shown below except that the true answers are provided for questions in the demonstrations} Now make your best effort and select the correct answer for the following question.You only need to output the option.Table 19: Prompt template for the task-specific instruction prompting strategy.In this strategy, we add a task-specific description at the beginning of the prompt.Note that for the QA task, there is no background information pertaining to questions.For the RC and CI tasks, we adopt the keyword "Context" to include the contextual information associated with each question.For the DRS and DS tasks, we employ the keywords "Dialogue" and "Document" to incorporate the pertaining information, respectively.</p>
<p>(for the QA task) Below are some examples of multiple-choice questions about question answering.Each question should be answered based on your world knowledge and problem solving ability./(for the RC task) Below are some examples of multiple-choice questions about reading comprehension.Each question should be answered based on the given context and commonsense reasoning when necessary./(for the CI task) Below are some examples of multiple-choice questions about commonsense natural language inference.For each question, there is a given context and the answer is the option that most likely follows the context./(for the DRS task) Below are some examples of multiple-choice questions about dialogue response selection.For each question, the answer is the option that represents the most suitable response for the given dialogue history, without hallucination and non-factual information./(for the DS task) Below are some examples of multiple-choice questions about document summarization.For each question, the answer is the option that accurately summarizes the given document without hallucination and non-factual information.</p>
<p>{Demonstrations in the same format as the question shown below except that the true answers are provided for questions in the demonstrations} Now make your best effort and select the correct answer for the following question.You only need to output the option.</p>
<p>Question:Figure 2 :
2
Figure 2: The overall process of applying conformal prediction for uncertainty quantification in LLMs.(a) Five distinct tasks are considered, and a dataset comprising 10,000 instances is prepared for each task.(b) Each data instance is transformed into a multiple-choice question, and nine LLMs (LLM series) are prompted to generate predicted probabilities for the given options.(c) Each dataset is divided into a calibration set and a test set, followed by the application of conformal prediction to generate prediction sets for test set instances.For illustrative purposes, demonstrations in the prompt input are excluded, and solely the process of constructing prediction sets utilizing the LAC conformal score function is demonstrated.In addition, only four options of the question are presented.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Performance comparison of different versions of the Qwen series (1.8B to 72B).</p>
<p>Figure 5 :
5
Figure 5: The distributions of correct answer choices on each task.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Performance comparison of different versions of the Llama-2 series (7B to 70B).</p>
<p>Figure 8 :Figure 9 :Figure 10 :
8910
Figure 8: Performance comparison of different versions of the DeepSeek series (7B to 67B).</p>
<p>Figure 11 :Figure 12 :
1112
Figure 11: Mean performance outcomes of the DeepSeek series' base pretrained model and the instruction-finetuned chat model across five tasks.Chat-V1 converts inputs into a chat format.Chat-V2 shares the same format as Base.</p>
<p>5 SSFigure 13 : 5 SSFigure 14 :
513514
Figure13: Average results across five tasks of the Llama-2 series when varying the error rate α.Note that the ideal coverage rate should be no less than 1 − α.</p>
<p>SSFigure 15 :
15
Figure15: Average results across five tasks of InternLM-7B when varying the softmax temperature τ .We compare the performance of conformal prediction (CP) and perplexity (PPL).</p>
<p>{Demonstrations in the same format as the question shown below except that the true answers are provided for questions in the demonstrations} Context/Dialogue/Document: {The context or dialogue history or document corresponding to the following question} Question: {Question} Choices: A. {Content of option A} B. {Content of option B} C. {Content of option C} D. {Content of option D} E. I don't know F. None of the above Answer:</p>
<p>Context/Dialogue/Document: {The context or dialogue history or document corresponding to the following question} Question: {Question} Choices: A. {Content of option A} B. {Content of option B} C. {Content of option C} D. {Content of option D} E. I don't know F. None of the above Answer:</p>
<p>Table 1 :
1
The evaluation results of LLMs with sizes ranging from 6B to 14B.These results represent the mean values of LAC and APS.The "Avg." column denotes the average performance across the five tasks.The small number in parentheses next to each score indicates the ranking of the LLM for that specific task.The relative ranking of LLMs is also visually demonstrated by the depth of color, with darker colors signifying higher rankings.
From Table 1, it is evidentthat in the majority of cases,the coverage rate is at least90%, indicating that the cov-erage guarantee requirementhas been met. Although thereare cases where the cover-age rate falls below 90%, 5 the values are still in close proximity to the 90% thresh-old. The lowest coverage rate is attained by Qwen-LLMs Coverage Rate -CR (%) QA Qwen-14B 92.58 Yi-6B 91.30 Gemma-7B 93.57RC 95.20 94.06 94.16CI 95.29 92.35 92.13DRS 91.92 91.36 91.59DS 89.71 91.38 90.70Avg. 92.94 92.09 92.437B on the DS task, with aMistral-7B93.0092.9189.9491.0292.0591.78value of 89.56%. Moreover, all models achieve an aver-age coverage rate exceedingLlama-2-13B Qwen-7B InternLM-7B Llama-2-7B92.59 92.79 90.68 91.3793.15 94.02 93.28 90.6990.50 91.53 90.10 90.9790.92 92.43 90.40 89.6090.82 89.56 90.34 90.0491.60 92.07 90.96 90.5390% across the five tasks. These findings suggest that the generated prediction sets are meaningful, as they can cover the true label with a high probability. Therefore,DeepSeek-7B MPT-7B Falcon-7B Prediction Accuracy -Acc (%) ↑ 91.18 89.95 89.79 90.54 90.04 89.95 Qwen-14B 64.25(1) 91.52(1) Yi-6B 57.57(4) 85.99(2) Gemma-7B 62.24(2) 85.29(3)90.16 90.12 89.82 91.00(1) 76.50(2) 73.58(3)90.89 90.80 90.46 73.90(1) 58.72(4) 66.79(2)90.21 89.71 90.71 49.33(4) 66.06(1) 40.80(7)90.48 90.19 90.19 74.00(1) 68.97(2) 65.74(3)the size of the prediction set can serve as a reliable indica-tor of uncertainty.Mistral-7B Llama-2-13B Qwen-7B InternLM-7B60.44(3) 52.52(6) 55.21(5) 48.37(7)81.94(5) 77.23(6) 83.89(4) 73.86(7)62.93(5) 59.66(6) 63.70(4) 46.21(7)53.21(5) 52.65(6) 64.04(3) 43.72(7)62.16(2) 60.05(3) 32.53(9) 34.38(8)64.14(4) 60.42(5) 59.87(6) 49.31(7)Llama-2-7B45.60(9)65.79(8)43.05(8)32.61(9)45.60(5)46.53(8)DeepSeek-7B45.65(8)65.39(9)42.66(9)33.50(8)42.15(6)45.87(9)MPT-7BPrediction Uncertainty -SS ↓Qwen-14B2.80(2)1.74(1)2.02(2)1.94(1)2.37(3)2.17(1)Yi-6B3.20(5)1.92(4)1.88(1)2.85(6)1.96(1)2.36(2)Gemma-7B2.72(1)1.88(3)2.04(3)2.14(2)3.11(7)2.38(3)Mistral-7B2.80(2)1.75(2)2.48(5)2.71(5)2.40(4)2.43(4)Llama-2-13B3.06(4)2.24(7)2.72(6)2.55(4)2.24(2)2.56(5)Qwen-7B3.26(7)2.15(5)2.28(4)2.51(3)2.92(5)2.63(6)InternLM-7B3.49(9)2.19(6)3.28(9)3.63(10)4.47(11)3.41(9)Llama-2-7B3.20(5)2.39(8)3.27(8)3.26(7)3.30(8)3.09(7)DeepSeek-7B3.34(8)2.77(9)3.06(7)3.40(8)3.08(6)3.13(8)MPT-7B3.53(10)3.46(10)3.60(10)3.59(9)3.66(9)3.57(10)Falcon-7B3.90(11)3.60(11)3.66(11)3.64(11)3.92(10)3.75(11)
(10)9(10)31.69(10)25.50(10) 24.38(11) 24.86(10) 27.18(10) Falcon-7B 23.75(11) 24.98(11) 24.91(11) 25.86(10) 24.69(11) 24.84(11)</p>
<p>Table 2 :
2
Comparison between conformal prediction (CP) and perplexity (PPL) using InternLM-7B.
TasksCR (%)SSCPPPLCP PPLQA90.68 83.44 3.49 2.89RC93.28 95.48 2.19 2.39CI90.10 96.25 3.28 3.97DRS90.40 86.80 3.63 3.42DS90.34 87.13 4.47 4.33Avg.90.96 89.82 3.41 3.40</p>
<p>Table 3 :
3
Comparison among conformal prediction (CP), entropy (Entropy), and maximal predicted probability (P max ) using InternLM-7B in terms of ECE (%).
TasksCPEntropy P maxQA15.8315.8315.83RC1.331.321.41CI3.163.453.75DRS12.1112.4012.45DS9.309.309.62Avg.8.358.468.61</p>
<p>Table 4 :
4
The evaluation results of closed-source LLMs.
LLMsCR (%) Acc (%)SSGPT-490.4181.751.65GPT-3.589.9862.993.05Qwen-72B (sampling)90.5470.292.43Qwen-72B (logits)93.3473.552.33</p>
<p>Table 5 :
5
The evaluation results of free-form text generation on TriviaQA.
LLMsCR (%) Acc (%)SSQwen-72B88.9276.452.63Llama-2-13B83.8971.832.40Qwen-14B82.7966.573.83Llama-2-7B78.8364.913.06Qwen-7B77.8959.445.02DeepSeek-7B78.4157.526.12Falcon-7B76.5155.746.27</p>
<p>Table 6 :
6
The relationship between stratified prediction set size (SS) and prediction accuracy (Acc).
SS LAC APSAvg.180.39 92.69 86.54259.77 82.21 70.99340.55 63.70 52.12440.07 41.71 40.89531.50 34.92 33.21613.43 None 13.42</p>
<p>Conformal Prediction for Uncertainty Quantification of LLMs Require: An LLM M, a task-specific dataset D, a user-specified error rate α, a test data ratio β, a prompting strategy P, and a conformal score function s (either LAC or APS); Output: Evaluation results in terms of evaluation metrics Acc, SS, and CR; D cal , Dtest ← CalibrationTestSplit(D, β); 10: O cal , Otest ← CalibrationTestSplit(O, β); 11: Acc ← CalculateAccuracy(Dtest, Otest); 12: ▷ Apply conformal prediction 13: S ← ComputeConformalScores(D cal , O cal , s); 14: p ← CalculateConformalThreshold(S, α); 15: B ← Initialized as an empty list; 16: for each instance X in Dtest do
1: ▷ Get model predictions2: O ← Initialized as an empty list;3: for each instance X in D do4:X ′ ← FormatPromptInput(X, P);5:L(X) ← GetLogitsOfOptions(M, X ′ );6:P (X) ← Softmax(L(X));7:Append P (X) to O;8: end for9: 17:Get P (X) from Otest;18:C(X) ← CreatePredictionSet(P (X), p, s);19:if |C(X)| == 0 then20:C(X) ← {Option with the largest probability};21:end if22:Append C(X) to B;23: end for24: SS ← CalculateAverageSetSize(B);25: CR ← CalculateCoverageRate(B, Dtest);26: return Acc, SS, CR.
Algorithm 1</p>
<p>Table 7 :
7
The CR (%) results of LLMs with sizes ranging from 6B to 14B.The "Avg." column denotes the average performance across tasks.These results correspond to using LAC and APS as the conformal score function separately.
LLMsLACAPSQARCCIDRSDSAvg.QARCCIDRSDSAvg.Qwen-14B90.43 91.57 91.40 90.31 89.63 90.67 94.74 98.83 99.19 93.52 89.79 95.21Yi-6B89.96 90.04 89.83 90.96 90.40 90.24 92.64 98.09 94.86 91.77 92.37 93.95Gemma-7B90.38 90.48 90.57 90.30 90.81 90.51 96.76 97.85 93.70 92.87 90.60 94.35Mistral-7B89.64 89.48 89.99 91.01 90.86 90.20 96.37 96.33 89.88 91.04 93.24 93.37Llama-2-13B 90.06 89.91 90.32 90.60 90.46 90.27 95.12 96.39 90.68 91.25 91.18 92.92Qwen-7B90.54 89.80 89.88 90.50 90.00 90.14 95.04 98.25 93.18 94.35 89.12 93.99InternLM-7B 89.38 90.33 89.96 90.61 90.62 90.18 91.98 96.23 90.24 90.18 90.06 91.74Llama-2-7B90.42 89.61 91.00 89.79 89.85 90.13 92.33 91.76 90.95 89.40 90.24 90.94DeepSeek-7B 89.94 88.48 89.79 91.09 90.16 89.89 92.42 91.43 90.53 90.70 90.26 91.07MPT-7B90.24 90.77 90.19 90.89 89.48 90.31 89.35 90.32 90.06 90.72 89.94 90.08Falcon-7B90.06 89.94 89.77 90.72 90.60 90.22 90.01 89.96 89.86 90.20 90.82 90.17</p>
<p>Table 8 :
8
Table 1 has presented the average results derived from the two conformal score functions, namely, LAC and APS.In this part, we analyze the results associated with each conformal score function.
LLMsAcc (%) ↑SS ↓QARCCIDRSDSAvg.QARCCIDRSDSAvg.C Further experimental resultsC.1 Detailed results of LAC and APS
The Acc and SS results of LLMs with sizes ranging from 6B to 14B.These results are obtained when LAC is adopted as the conformal score function.The "Avg." column denotes the average performance across tasks.The small number in parentheses indicates the rank of the model on each task.</p>
<p>Table 9 :
9
The Acc and SS results of LLMs with sizes ranging from 6B to 14B.These results are obtained when APS is adopted as the conformal score function.The "Avg." column denotes the average performance across tasks.The small number in parentheses indicates the rank of the model on each task.
LLMsAcc (%) ↑SS ↓QARCCIDRSDSAvg.QARCCIDRSDSAvg.Avg. CRAvg.DS86 88 90 92 94 96QADSQADRSRCDRSRCCICI</p>
<p>Table 10 :
10
Average results across five tasks of Mixtral-8x7B, Mistral-7B, Llama-2-13B, and Qwen-14B.
LLMsCR (%) Acc (%) ↑ SS ↓Mixtral-8x7B92.5973.742.03Mistral-7B91.7864.142.43Llama-2-13B91.6060.422.56Qwen-14B92.9474.002.17
observed, Mixtral-8x7B consistently outperforms Mistral-7B across both accuracy and uncertainty.Remarkably, it achieves accuracy levels comparable to Qwen-14B, while demonstrating the lowest</p>
<p>Table 13 ,
13
indicate that the SS values remain relatively consistent with those observed when options E and F are included.Furthermore, while smaller average SS values are usually obtained when LLMs demonstrate strong performance, larger SS values are observed even with fewer answer choices (i.e.without options E and F) when LLMs exhibit weaker performance (e.g., 3.57 vs. 3.74 for MPT-7B and 3.48 vs. 3.55 for Falcon-40B).It is also noted that Llama-2-70B, DeepSeek-67B and Yi-6B achieve the same average SS value of 1.89 when options E and F are excluded, making them not differentiable in terms of prediction uncertainty.</p>
<p>Table 12 :
12
The ratio of test instances for which the predicted answer is option E ("I don't know") or option F ("None of the above").Note that neither of them corresponds to the ground truth answer.
LLMsE Rate (%)F Rate (%)QA RCCIDRS DS Avg. QA RCCIDRS DS Avg.Yi-34B1.79 0.41 0.00 1.99 0.00 0.84 0.37 0.08 0.07 0.07 0.00 0.12Qwen-72B0.31 0.47 0.39 1.53 0.00 0.54 0.32 0.62 1.19 3.22 0.02 1.07Qwen-14B0.21 0.83 0.08 0.19 0.00 0.26 0.43 0.70 0.17 0.19 0.27 0.35Llama-2-70B</p>
<p>Table 13 :
13
The average prediction set size (SS) when the option E ("I don't know") and option F ("None of the above") are included in or excluded from the answer choices.Note that neither of them corresponds to the ground truth answer.
LLMsWith Options E and FWithout Options E and FQA RCCIDRS DS Avg. QA RCCIDRS DS Avg.</p>
<p>Table 14 :
14
Comparison of different prompting strategies on the DS task.The reported values of the SS metric are obtained using LAC as the conformal score function.LLM varies with different prompting strategies, the discrepancies are generally marginal.Of greater significance is the observation that different LLMs demonstrate preferences for different prompting strategies.For example, the base prompting strategy yields the highest accuracy for Yi-34B.Conversely, Qwen-72B performs optimally with the task-specific instruction prompting strategy, while DeepSeek-67B exhibits superior accuracy with the shared instruction prompting strategy.Similar patterns can be observed from the results of the SS metric.These observations highlight the importance of considering multiple prompting strategies when benchmarking LLMs.By averaging the results from various strategies, we can mitigate the impact of prompt sensitivities and ensure a more equitable comparison of LLM performance.
LLMsPrompting StrategyAcc (%)SSBase Prompt73.191.40Yi-34BShared Instruction Prompt69.871.46Task-specific Instruction Prompt71.351.42Base Prompt58.301.70Qwen-72BShared Instruction Prompt61.081.67Task-specific Instruction Prompt62.511.61Base Prompt56.662.08Llama-2-70BShared Instruction Prompt56.182.21Task-specific Instruction Prompt59.382.18Base Prompt55.602.10DeepSeek-67BShared Instruction Prompt56.662.03Task-specific Instruction Prompt56.342.18performance of each C.11 Ablation study</p>
<p>Table 15 :
15
Average results across five tasks of Yi-34B and Llama-2-70B.<em> indicates the results obtained without using demonstrations in the prompt input.
LLMsCR (%) Acc (%) ↑ SS ↓Yi-34B94.2281.461.93Yi-34B  </em>93.3180.101.88Llama-2-70B93.1172.242.16Llama-2-70B  *91.5563.552.72</p>
<p>Table 16 :
16
An example of the prediction sets produced by the Yi-34B model on the QA task.We include results derived from both the LAC and APS score functions and the three prompting strategies.All generated prediction sets encompass the true answer.Which of the following is thought to be implicated in the development of peripheral muscle fatigue during multiple sprint activities?
Choices:A. An accumulation of inorganic phosphate.B. Development of hyperosmolality in the muscles.C. An excess of antioxidants.D. A lack of potassium.E. I don't knowF. None of the aboveCorrect Answer: APredicted Answer based on LAC:Base Prompt: {A}Shared Instruction Prompt: {A}Task-specific Instruction Prompt: {A}Predicted Answer based on APS:Base Prompt: {A, B, D}Shared Instruction Prompt: {A, F}Task-specific Instruction Prompt: {A, E}derived from these conformal score functions and prompting strategies can be used to achieve a moreprecise uncertainty quantification.
Question:</p>
<p>It is also required that the test data points are drawn from the same distribution as the calibration data.
We omit Falcon-180B due to insufficient GPU resources.
While the theoretical guarantee of conformal prediction is rigorous, there can be minor fluctuations in practice due to finite-sample variability[5].
We achieve this by applying the "apply_chat_template" function of the corresponding tokenizer to each prompt input.
https://huggingface.co/mistralai/Mixtral-8x7B-v0.1
https://mistral.ai/news/mixtral-of-experts/
Acknowledgments and Disclosure of FundingThis work was supported in part by the Tencent AI Lab Rhino-Bird (Grant No. EF2023-00151-FST), the Science and Technology Development Fund, Macau SAR (Grant Nos.FDCT/060/2022/AFJ, FDCT/0070/2022/AMJ), and the Multi-year Research Grant from the University of Macau (Grant No. MYRG-GRG2023-00006-FST-UMDF).We thank all reviewers for their precious comments.
. A I Yi, 2023</p>
<p>A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Information fusion. Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, Rajendra Acharya, 202176</p>
<p>Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models: Towards open frontier models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, 2023Julien Launay, Quentin Malartic</p>
<p>Uncertainty sets for image classifiers using conformal prediction. Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, Michael I Jordan, arXiv:2009.141932020arXiv preprint</p>
<p>A gentle introduction to conformal prediction and distribution-free uncertainty quantification. N Anastasios, Stephen Angelopoulos, Bates, arXiv:2107.075112021arXiv preprint</p>
<p>Flageval: An open-source evaluation toolkit and an open platform for evaluation of large models. BAAI. 2023</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv:2309.16609Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. 2023arXiv preprint</p>
<p>Conformal prediction for reliable machine learning: theory, adaptations and applications. Shen-Shyang Vineeth Balasubramanian, Vladimir Ho, Vovk, 2014Newnes</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, arXiv:2307.031092023arXiv preprint</p>
<p>Quantifying uncertainty in answers from any language model via intrinsic and extrinsic confidence assessment. Jiuhai Chen, Jonas Mueller, arXiv:2308.161752023arXiv preprint</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Felm: Benchmarking factuality evaluation of large language models. Shiqi Chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, Junxian He, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Opencompass: A universal evaluation platform for foundation models. 2023</p>
<p>Deepseek, Deepseek llm: Let there be answers. 2023</p>
<p>Nicolas Deutschmann, Marvin Alberts, María Rodríguez, Martínez , arXiv:2309.03797Conformal autoregressive generation: Beam search with coverage guarantees. 2023arXiv preprint</p>
<p>Conformal prediction for text infilling and part-of-speech prediction. Neil Dey, Jing Ding, Jack Ferrell, Carolina Kapper, Maxwell Lovig, Emiliano Planchon, Jonathan P Williams, The New England Journal of Statistics in Data Science. 112022</p>
<p>Bold: Dataset and metrics for measuring biases in open-ended language generation. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, Rahul Gupta, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Zhifang Sui, arXiv:2301.00234A survey for in-context learning. 2022arXiv preprint</p>
<p>LM-polygraph: Uncertainty estimation for language models. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Artem Shelmanov, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2023 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsSingaporeAssociation for Computational LinguisticsDecember 2023Yansong Feng and Els Lefever</p>
<p>Efficient conformal prediction via cascaded inference with expanded admission. Adam Fisch, Tal Schuster, Tommi Jaakkola, Regina Barzilay, arXiv:2007.031142020arXiv preprint</p>
<p>Unsupervised quality estimation for neural machine translation. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, Lucia Specia, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Conformal prediction: a unified review of theory and new challenges. Matteo Fontana, Gianluca Zeni, Simone Vantini, Bernoulli. 2912023</p>
<p>A framework for few-shot language model evaluation. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, Niklas Mcdonell, Chris Muennighoff, Jason Ociepa, Laria Phang, Hailey Reynolds, Aviya Schoelkopf, Lintang Skowron, Eric Sutawika, Anish Tang, Ben Thite, Kevin Wang, Andy Wang, Zou, 122023</p>
<p>A survey of uncertainty in deep neural networks. Jakob Gawlikowski, Cedrique Rovile, Njieutcheu Tassi, Mohsin Ali, Jongseok Lee, Matthias Humt, Jianxiang Feng, Anna Kruspe, Rudolph Triebel, Peter Jung, Ribana Roscher, Artificial Intelligence Review. 5612023Suppl</p>
<p>Transformer-based conformal predictors for paraphrase detection. Patrizio Giovannotti, Alex Gammerman, Conformal and Probabilistic Prediction and Applications. PMLR2021</p>
<p>On calibration of modern neural networks. Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q Weinberger, International conference on machine learning. PMLR2017</p>
<p>Evaluating large language models: A comprehensive survey. Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, arXiv:2310.197362023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Mengting Hu, Zhen Zhang, Shiwan Zhao, Minlie Huang, Bingzhe Wu, arXiv:2306.04459Uncertainty in natural language processing: Sources, quantification, and applications. 2023arXiv preprint</p>
<p>Cosmos QA: Machine reading comprehension with contextual commonsense reasoning. Lifu Huang, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Look before you leap: An exploratory study of uncertainty measurement for large language models. Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen, Lei Ma, arXiv:2307.102362023arXiv preprint</p>
<p>Perplexity-a measure of the difficulty of speech recognition tasks. Fred Jelinek, Robert L Mercer, Lalit R Bahl, James K Baker, The Journal of the Acoustical Society of America. 62S11977</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, Robert Mchardy, arXiv:2307.10169Challenges and applications of large language models. 2023arXiv preprint</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. Lorenz Kuhn, Yarin Gal, Sebastian Farquhar, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Conformal prediction with large language models for multi-choice question answering. Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, Andrew Beam, arXiv:2305.184042023arXiv preprint</p>
<p>Uncertainty quantification using bayesian neural networks in classification: Application to biomedical image segmentation. Yongchan Kwon, Joong-Ho Won, Beom Joon Kim, Myunghee Cho, Paik , Computational Statistics &amp; Data Analysis. 1421068162020</p>
<p>HaluEval: A large-scale hallucination evaluation benchmark for large language models. Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang Lyu, Min Zhang, arXiv:2311.07536A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering. 2023arXiv preprint</p>
<p>Holistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.091102022arXiv preprint</p>
<p>Generating with confidence: Uncertainty quantification for black-box large language models. Zhen Lin, Shubhendu Trivedi, Jimeng Sun, arXiv:2305.191872023arXiv preprint</p>
<p>Retrieval-augmented multi-modal chain-of-thoughts reasoning for large language models. Bingshuai Liu, Chenyang Lyu, Zijun Min, Zhanyu Wang, Jinsong Su, Longyue Wang, arXiv:2312.017142023arXiv preprint</p>
<p>Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin, arXiv:2402.18041Datasets for large language models: A comprehensive survey. 2024arXiv preprint</p>
<p>Federated conformal predictors for distributed uncertainty quantification. Charles Lu, Yaodong Yu, Sai Praneeth Karimireddy, Michael Jordan, Ramesh Raskar, International Conference on Machine Learning. PMLR2023</p>
<p>Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, Zhaopeng Tu, arXiv:2306.09093Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. 2023arXiv preprint</p>
<p>Revisiting the calibration of modern neural networks. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, Mario Lucic, Advances in Neural Information Processing Systems. 202134</p>
<p>OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs. Seungwhan Moon, Pararth Shah, Anuj Kumar, Rajen Subba, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, Ajmal Mian, arXiv:2307.06435A comprehensive overview of large language models. 2023arXiv preprint</p>
<p>A unified theory of inference for text understanding. Peter Norvig, 1987University of California, BerkeleyPhD thesis</p>
<p>Salute the classic: Revisiting challenges of machine translation in the age of large language models. Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F Wong, Shuming Shi, Zhaopeng Tu, arXiv:2401.083502024arXiv preprint</p>
<p>Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S Jaakkola, Regina Barzilay, arXiv:2306.10193Conformal language modeling. 2023arXiv preprint</p>
<p>Uncertainty quantification and deep ensembles. Rahul Rahaman, Advances in Neural Information Processing Systems. 20063-20075, 202134</p>
<p>Conformal nucleus sampling. Shauli Ravfogel, Yoav Goldberg, Jacob Goldberger, Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>On measures of entropy and information. Alfréd Rényi, Proceedings of the fourth Berkeley symposium on mathematical statistics and probability. the fourth Berkeley symposium on mathematical statistics and probabilityUniversity of California Press19611</p>
<p>Classification with valid and adaptive coverage. Yaniv Romano, Matteo Sesia, Emmanuel Candes, Advances in Neural Information Processing Systems. 202033</p>
<p>Least ambiguous set-valued classifiers with bounded error levels. Mauricio Sadinle, Jing Lei, Larry Wasserman, Journal of the American Statistical Association. 1145252019</p>
<p>Get to the point: Summarization with pointer-generator networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171</p>
<p>Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, arXiv:2403.08295Open models based on gemini research and technology. 2024arXiv preprint</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. Internlm Team, 2023</p>
<p>Introducing mpt-7b: A new standard for open-source, commercially usable llms. Nlp Mosaicml, Team, 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Algorithmic learning in a random world. Vladimir Vovk, Alexander Gammerman, Glenn Shafer, 2005Springer29</p>
<p>Empirical evaluation of uncertainty quantification in retrieval-augmented language models for science. Sridevi Wagle, Sai Munikoti, Anurag Acharya, Sara Smith, Sameera Horawalavithana, arXiv:2311.093582023arXiv preprint</p>
<p>An explanation of in-context learning as implicit bayesian inference. Sang Michael Xie, Aditi Raghunathan, Percy Liang, Tengyu Ma, International Conference on Learning Representations. 2021</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi, arXiv:2306.130632023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, arXiv:2304.137122023arXiv preprint</p>
<p>Improving the reliability of large language models by leveraging uncertainty-aware in-context learning. Yuchen Yang, Houqiang Li, Yanfeng Wang, Yu Wang, arXiv:2310.047822023arXiv preprint</p>
<p>Enhancing conversational search: Large language model-aided informative query rewriting. Fanghua Ye, Meng Fang, Shenghui Li, Emine Yilmaz, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Flask: Fine-grained language model evaluation based on alignment skill sets. Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, Minjoon Seo, arXiv:2307.109282023arXiv preprint</p>
<p>HellaSwag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJuly 2019</p>
<p>Safetybench: Evaluating the safety of large language models with multiple choice questions. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang, arXiv:2309.070452023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Advances in Neural Information Processing Systems. 362024</p>
<p>. Qwen-14b , </p>
<p>. Qwen-14b , </p>            </div>
        </div>

    </div>
</body>
</html>
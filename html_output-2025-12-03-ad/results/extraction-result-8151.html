<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8151 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8151</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8151</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-278911801</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.20993v1.pdf" target="_blank">Who Reasons in the Large Language Models?</a></p>
                <p><strong>Paper Abstract:</strong> Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities--such as mathematical reasoning--remains largely empirical and opaque. A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting. In this work, we hypothesize that the reasoning capabilities in well-trained LLMs are primarily attributed to the output projection module (oproj) in the Transformer's multi-head self-attention (MHSA) mechanism. To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs. Using SfN, we provide both circumstantial and empirical evidence suggesting that oproj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue. These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8151.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8151.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o_proj</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Output projection (o_proj) of the MHSA module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The linear output-projection matrix that maps multi-head self-attention outputs back to token-space; hypothesized and empirically implicated by this paper as the primary locus of emergent reasoning (including arithmetic) after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-* and DeepSeek-R1-Distill-Qwen-* families (1.5B, 7B, 14B, 32B, 70B examined)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer decoder-only LLMs (Qwen2.5 base models and DeepSeek-R1 distilled reasoning variants) across multiple sizes (1.5B, 7B, 14B, 32B, 70B); same architecture between base and reasoning model pairs, differing only in weights after SFT/RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-step mathematical reasoning benchmarks and word problems (AIME 2024 problems, Math-500 benchmark, other multi-step math/GPQA/Diamond reasoning benchmarks used in paper evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Hypothesized functional bottleneck: o_proj stores/implements the reasoning transformation (changes in o_proj weights encode reasoning algorithms or mappings needed for multi-step arithmetic). Paper reports o_proj shows largest absolute L2 weight shifts and a distinctive bimodal relative-weight-change distribution compared to other linear modules.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Delta Stethoscope (per-module L2 and relative weight-change analysis), Merge Stethoscope (module replacement: swap o_proj from reasoning model into base), Freeze Stethoscope (fine-tune only o_proj (+ layernorm, embed_tokens, lm_head) while freezing other modules), Destruction Stethoscope (zero / reinit / remove o_proj during inference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative + quantitative: (1) Delta: o_proj exhibits the largest or second-largest L2 weight shift across sizes (most pronounced at 14B and 32B). (2) Merge: replacing only o_proj in a 1.5B base with o_proj from the reasoning model produced level-IV outputs (correct reasoning) on several AIME 2024 questions and longer detailed responses (qualitative). (3) Freeze experiments (Table 2): tuning Emb+Head+o_proj (F2) improved AIME metric from 0.167 -> 0.367 (Qwen2.5-32B base A -> F2) and Math metric from 0.836 -> 0.890 and GPQA-like metric from 0.485 -> 0.520; for Qwen2.5-14B AIME improved 0.133 -> 0.266 and Math 0.810 -> 0.848 (see Table 2 rows labeled F2).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Scale-dependent failures: (a) At larger model scales merging o_proj alone sometimes failed to confer reasoning gains due to mismatched normalization parameters between base and reasoning models; (b) tuning all parameters (especially MLP) leads to overfitting / memorization of training traces; (c) o_proj replacement is not a universal fix—7B/ larger models show weaker or no Merge benefit unless layernorm (and sometimes other small components) are also aligned/tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Converging empirical signals: (1) Delta Stethoscope: largest L2 distance and a unique bimodal distribution of relative changes for o_proj across many model sizes; (2) Merge Stethoscope: atomic replacement of o_proj from reasoning model into base produces correct multi-step arithmetic outputs (level IV) in 1.5B experiments while replacing other modules did not; (3) Freeze Stethoscope: fine-tuning only o_proj (+ LN, embed, lm_head) yields large reasoning gains comparable to full fine-tuning but with faster training and lower memory; (4) Loss curves: tuning only o_proj yields smoother training loss vs clear overfitting when MLP is unfrozen.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Merge Stethoscope success is scale- and normalization-dependent: merging o_proj into larger bases (e.g., 7B+) did not reliably produce reasoning gains unless layernorm parameters were also reconciled or unless a specific fine-tuning strategy was applied. The paper notes a possible coincidence risk: large o_proj weight shifts might correlate with reasoning but not be strictly causal in all settings; a theoretical mechanistic account is not provided and generalization across architectures is not established.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Reasons in the Large Language Models?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8151.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8151.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Delta Stethoscope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Delta Stethoscope (weight-difference probing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic that compares per-module weight differences between a base model A and its reasoning-fine-tuned counterpart B to localize parameter changes associated with acquired capabilities (e.g., arithmetic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-* base and DeepSeek-R1-Distill-Qwen-* reasoning variants (1.5B, 7B, 14B, 32B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same-architecture model pairs where B is produced by supervised fine-tuning / distillation on reasoning traces from A.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Benchmark-level multi-step math reasoning problems (AIME 2024 etc.) used to induce/measure reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a mechanism per se but a probing representation: per-module L2 norm of parameter differences and relative changes highlight which modules underwent the biggest weight shifts during reasoning fine-tuning; o_proj stands out (largest absolute shifts and bimodal relative-change distribution).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Compute ||w_X(B) - w_X(A)||_2 per linear module across layers; compute relative shifts (w_X(B)-w_X(A))/w_X(A) and visualize layer-wise distributions (clipped) to identify distinctive patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Delta results: across sizes (1.5B, 7B, 14B, 32B, 70B) o_proj shows the largest absolute L2 change (or second-largest for 1.5B) and a distinctive bimodal relative-change distribution while other linear modules show unimodal distributions centered near zero (figures reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Delta is correlational: large weight shift in o_proj is evidence but not by itself proof of causality; possible coincidental co-adaptation of other frozen modules. Delta does not indicate whether o_proj alone is sufficient, only that it changed markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Consistent per-module weight-shift patterns across multiple sizes and base/reasoning model pairs show o_proj is uniquely differentially updated during reasoning fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>A large weight shift could be necessary but not sufficient; the paper follows up with Merge/Freeze/Destruction to test causality because Delta alone cannot exclude coincidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Reasons in the Large Language Models?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8151.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8151.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Merge Stethoscope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Merge Stethoscope (module replacement / surgical merging)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that constructs hybrid models by replacing specific modules in a base model with the corresponding modules from a reasoning-fine-tuned model to test whether those modules carry the reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Merged variants of Qwen2.5-Math-1.5B (A) with DeepSeek-R1-Distill-Qwen-1.5B (B) modules; qualitative attempts at larger sizes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rudely merged models M where a subset of modules (e.g., o_proj, {q,k,v}_proj, mlp) are copied from B into A without further tuning; evaluated on same generation interface (generate) for reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>AIME 2024 contest problems (multi-step arithmetic reasoning) and example word problems (e.g., walking-speed/time problems).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>If replacement of a single module (o_proj) yields correct reasoning, that module likely encodes the computation/representation enabling arithmetic reasoning; other modules (q/k/v, mlp) do not produce the same effect when swapped individually.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Atomic module replacement (only o_proj, or only q/k/v_proj, or only mlp) and run inference on math problems; judge output level (I-IV) where Level IV = correct reasoning and answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/controlled results: For 1.5B experiments, M1 (replace o_proj only) produced Level IV outputs (correct reasoning and answer) on several AIME 2024 questions and increased output length; M2 (replace {q,k,v}_proj) produced Level III outputs (context-aware but failing hard reasoning); M3 (replace mlp) produced Level I outputs (nonsense). For larger models merging o_proj often did not yield notable improvements unless normalization parameters were also reconciled.</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Merging can break generation (expected) and is sensitive to normalization mismatch; for larger models layernorm incompatibility with remaining base parameters can prevent merged model from reasoning even when o_proj is replaced. Also merged models are not tuned and can produce incoherent chat responses in chat interface, requiring use of the generate interface.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct causal test: replacing only o_proj produced correct arithmetic reasoning for several difficult problems in 1.5B experiments while other single-module replacements did not; this supports o_proj being sufficient (in some configurations) to confer reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Scale-dependence: at 7B+ sizes merging o_proj alone often fails to produce reasoning gains; merging requires also adjusting layernorm (and even then may fail), indicating o_proj's sufficiency is conditional on compatibility with the rest of the parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Reasons in the Large Language Models?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8151.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8151.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Freeze Stethoscope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Freeze Stethoscope (selective fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intervention that fine-tunes only a small subset of parameters (notably o_proj and layernorm, plus embed_tokens and lm_head) from a base model while freezing other modules, to test whether those few components suffice to acquire reasoning (arithmetic) capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-32B-Instruct and Qwen2.5-14B-Instruct (base A), fine-tuned variants F1..F4 in Table 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base autoregressive transformer instruction-tuned models (14B and 32B); fine-tuning configurations vary by which modules are unfrozen (Emb+Head; Emb+Head+o_proj; Emb+Head+{q,k,v,o}_proj; All). Training used dataset s1K (~1,000 high-quality reasoning traces) and hyperparameters noted in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>AIME 2024, Math-500, GPQA/Diamond style reasoning benchmarks (multi-step arithmetic and logic).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Shows that updating a small parameter subset (o_proj + layernorm + embed_tokens + lm_head) can reconfigure the model to perform arithmetic-reasoning tasks, suggesting that learned arithmetic behavior can be induced via modification of these output-projection and normalization pathways rather than wholesale weight rewiring across the network.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Selective freezing/unfreezing during supervised fine-tuning: compare variants F1 (Emb+Head), F2 (Emb+Head+o_proj), F3 (Emb+Head+{q,k,v,o}_proj), F4 (All) and measure reasoning metrics and training dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2 (selected numbers): Qwen2.5-32B: A (base) AIME=0.167, Math=0.836, GPQA-like=0.485; F2 (Emb+Head+o_proj) AIME=0.367, Math=0.890, GPQA-like=0.520; F4 (All) AIME=0.367, Math=0.906, GPQA-like=0.591. Qwen2.5-14B: A AIME=0.133, Math=0.810, GPQA-like=0.449; F2 AIME=0.266, Math=0.848, GPQA-like=0.485; F4 AIME=0.266, Math=0.872, GPQA-like=0.530. F2 achieves large fractions of full-finetune gains while training ~3x faster and using less GPU memory (paper claim).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Tuning only layernorm (without o_proj) harms reasoning; unfreezing all parameters results in overfitting (MLP capacity memorizes training set) as evidenced by training loss curves; adding {q,k,v}_proj yields little additional gain and may even harm.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Comparative fine-tuning experiments show that F2 (o_proj + LN tuning) recovers most of the reasoning gains of full fine-tuning (F4) on benchmarks while maintaining smoother training loss (less overfitting), supporting that o_proj+LN updates are a compact, effective route to induce arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Reported results depend on tuning recipe and dataset (s1K) and limited compute; authors note they did not fully reproduce original s1 pipeline and results may not generalize across architectures or training datasets. Also embedding and lm_head are tuned in these runs, so pure o_proj-only sufficiency is not fully isolated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Reasons in the Large Language Models?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8151.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8151.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Destruction Stethoscope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Destruction Stethoscope (module ablation / corruption)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of destructive interventions (zeroing, re-initialization, or removal of modules) applied to specific layers to identify modules required for conversational fluency versus reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-32B (64-layer) experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large base model (Qwen2.5-32B) with selective destruction applied to layers 5-30 to avoid trivial total collapse; three destructors: Zero (set parameters to zero), ReInit (Gaussian random re-init), Remove (delete layer).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Indirect: the diagnostics primarily judge conversational output levels (I-IV) rather than arithmetic-only metrics, but results inform which modules are important for conversation vs reasoning (and thus for arithmetic emergence).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Functional separation: destruction results indicate MLP components and q_proj/k_proj are important for conversational fluency (and thus for supporting reasoning contexts), while o_proj appears less important for conversation despite being central to reasoning in other tests.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Ablation-style interventions: Zero, ReInit, Remove applied to specific modules across layers 5-30, then observe output level (I-IV).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative outcomes summarized in Table 3: Zeroing o_proj -> Level III (conversation maintained), ReInit o_proj -> Level III, Remove o_proj -> Level III (i.e., destructive operations on o_proj did not break conversational-level outputs in the tested range). By contrast, up_proj/gate_proj/down_proj (MLP) reinit/zero/remove -> Level I (nonsense), q_proj/k_proj destructive operations produced Level I/II outputs, v_proj less impactful (Level II/III depending on destructor).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Destructive reinit often causes amplified noise across subsequent blocks and can produce level I outputs; destruction experiments are sensitive to which layers are targeted (early/late-layer destructions collapse outputs across the board). Results are qualitative and not presented with statistical aggregations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Module-specific destruction shows that destroying MLPs (up/down/gate) and q/k projections degrades conversational output strongly while destroying o_proj does not, supporting the conjecture that conversation and reasoning may be localized to different module sets (o_proj crucial for reasoning but less for conversation).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Destruction findings are weaker/qualitative (single-conversation examples, not broad statistics). Destroying many layers or selecting different layer ranges can trivially collapse outputs, limiting interpretability; results do not prove that o_proj is unnecessary for all conversational content or that destroyed models would preserve arithmetic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Reasons in the Large Language Models?', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Qwen technical report <em>(Rating: 1)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>Transformer feed-forward layers are key-value memories <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8151",
    "paper_id": "paper-278911801",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "o_proj",
            "name_full": "Output projection (o_proj) of the MHSA module",
            "brief_description": "The linear output-projection matrix that maps multi-head self-attention outputs back to token-space; hypothesized and empirically implicated by this paper as the primary locus of emergent reasoning (including arithmetic) after fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-* and DeepSeek-R1-Distill-Qwen-* families (1.5B, 7B, 14B, 32B, 70B examined)",
            "model_description": "Transformer decoder-only LLMs (Qwen2.5 base models and DeepSeek-R1 distilled reasoning variants) across multiple sizes (1.5B, 7B, 14B, 32B, 70B); same architecture between base and reasoning model pairs, differing only in weights after SFT/RLHF.",
            "arithmetic_task_type": "Multi-step mathematical reasoning benchmarks and word problems (AIME 2024 problems, Math-500 benchmark, other multi-step math/GPQA/Diamond reasoning benchmarks used in paper evaluations).",
            "mechanism_or_representation": "Hypothesized functional bottleneck: o_proj stores/implements the reasoning transformation (changes in o_proj weights encode reasoning algorithms or mappings needed for multi-step arithmetic). Paper reports o_proj shows largest absolute L2 weight shifts and a distinctive bimodal relative-weight-change distribution compared to other linear modules.",
            "probing_or_intervention_method": "Delta Stethoscope (per-module L2 and relative weight-change analysis), Merge Stethoscope (module replacement: swap o_proj from reasoning model into base), Freeze Stethoscope (fine-tune only o_proj (+ layernorm, embed_tokens, lm_head) while freezing other modules), Destruction Stethoscope (zero / reinit / remove o_proj during inference).",
            "performance_metrics": "Qualitative + quantitative: (1) Delta: o_proj exhibits the largest or second-largest L2 weight shift across sizes (most pronounced at 14B and 32B). (2) Merge: replacing only o_proj in a 1.5B base with o_proj from the reasoning model produced level-IV outputs (correct reasoning) on several AIME 2024 questions and longer detailed responses (qualitative). (3) Freeze experiments (Table 2): tuning Emb+Head+o_proj (F2) improved AIME metric from 0.167 -&gt; 0.367 (Qwen2.5-32B base A -&gt; F2) and Math metric from 0.836 -&gt; 0.890 and GPQA-like metric from 0.485 -&gt; 0.520; for Qwen2.5-14B AIME improved 0.133 -&gt; 0.266 and Math 0.810 -&gt; 0.848 (see Table 2 rows labeled F2).",
            "error_types_or_failure_modes": "Scale-dependent failures: (a) At larger model scales merging o_proj alone sometimes failed to confer reasoning gains due to mismatched normalization parameters between base and reasoning models; (b) tuning all parameters (especially MLP) leads to overfitting / memorization of training traces; (c) o_proj replacement is not a universal fix—7B/ larger models show weaker or no Merge benefit unless layernorm (and sometimes other small components) are also aligned/tuned.",
            "evidence_for_mechanism": "Converging empirical signals: (1) Delta Stethoscope: largest L2 distance and a unique bimodal distribution of relative changes for o_proj across many model sizes; (2) Merge Stethoscope: atomic replacement of o_proj from reasoning model into base produces correct multi-step arithmetic outputs (level IV) in 1.5B experiments while replacing other modules did not; (3) Freeze Stethoscope: fine-tuning only o_proj (+ LN, embed, lm_head) yields large reasoning gains comparable to full fine-tuning but with faster training and lower memory; (4) Loss curves: tuning only o_proj yields smoother training loss vs clear overfitting when MLP is unfrozen.",
            "counterexamples_or_challenges": "Merge Stethoscope success is scale- and normalization-dependent: merging o_proj into larger bases (e.g., 7B+) did not reliably produce reasoning gains unless layernorm parameters were also reconciled or unless a specific fine-tuning strategy was applied. The paper notes a possible coincidence risk: large o_proj weight shifts might correlate with reasoning but not be strictly causal in all settings; a theoretical mechanistic account is not provided and generalization across architectures is not established.",
            "uuid": "e8151.0",
            "source_info": {
                "paper_title": "Who Reasons in the Large Language Models?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Delta Stethoscope",
            "name_full": "Delta Stethoscope (weight-difference probing)",
            "brief_description": "A diagnostic that compares per-module weight differences between a base model A and its reasoning-fine-tuned counterpart B to localize parameter changes associated with acquired capabilities (e.g., arithmetic reasoning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-* base and DeepSeek-R1-Distill-Qwen-* reasoning variants (1.5B, 7B, 14B, 32B, 70B)",
            "model_description": "Same-architecture model pairs where B is produced by supervised fine-tuning / distillation on reasoning traces from A.",
            "arithmetic_task_type": "Benchmark-level multi-step math reasoning problems (AIME 2024 etc.) used to induce/measure reasoning capability.",
            "mechanism_or_representation": "Not a mechanism per se but a probing representation: per-module L2 norm of parameter differences and relative changes highlight which modules underwent the biggest weight shifts during reasoning fine-tuning; o_proj stands out (largest absolute shifts and bimodal relative-change distribution).",
            "probing_or_intervention_method": "Compute ||w_X(B) - w_X(A)||_2 per linear module across layers; compute relative shifts (w_X(B)-w_X(A))/w_X(A) and visualize layer-wise distributions (clipped) to identify distinctive patterns.",
            "performance_metrics": "Delta results: across sizes (1.5B, 7B, 14B, 32B, 70B) o_proj shows the largest absolute L2 change (or second-largest for 1.5B) and a distinctive bimodal relative-change distribution while other linear modules show unimodal distributions centered near zero (figures reported in paper).",
            "error_types_or_failure_modes": "Delta is correlational: large weight shift in o_proj is evidence but not by itself proof of causality; possible coincidental co-adaptation of other frozen modules. Delta does not indicate whether o_proj alone is sufficient, only that it changed markedly.",
            "evidence_for_mechanism": "Consistent per-module weight-shift patterns across multiple sizes and base/reasoning model pairs show o_proj is uniquely differentially updated during reasoning fine-tuning.",
            "counterexamples_or_challenges": "A large weight shift could be necessary but not sufficient; the paper follows up with Merge/Freeze/Destruction to test causality because Delta alone cannot exclude coincidence.",
            "uuid": "e8151.1",
            "source_info": {
                "paper_title": "Who Reasons in the Large Language Models?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Merge Stethoscope",
            "name_full": "Merge Stethoscope (module replacement / surgical merging)",
            "brief_description": "An intervention that constructs hybrid models by replacing specific modules in a base model with the corresponding modules from a reasoning-fine-tuned model to test whether those modules carry the reasoning capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Merged variants of Qwen2.5-Math-1.5B (A) with DeepSeek-R1-Distill-Qwen-1.5B (B) modules; qualitative attempts at larger sizes reported.",
            "model_description": "Rudely merged models M where a subset of modules (e.g., o_proj, {q,k,v}_proj, mlp) are copied from B into A without further tuning; evaluated on same generation interface (generate) for reasoning outputs.",
            "arithmetic_task_type": "AIME 2024 contest problems (multi-step arithmetic reasoning) and example word problems (e.g., walking-speed/time problems).",
            "mechanism_or_representation": "If replacement of a single module (o_proj) yields correct reasoning, that module likely encodes the computation/representation enabling arithmetic reasoning; other modules (q/k/v, mlp) do not produce the same effect when swapped individually.",
            "probing_or_intervention_method": "Atomic module replacement (only o_proj, or only q/k/v_proj, or only mlp) and run inference on math problems; judge output level (I-IV) where Level IV = correct reasoning and answer.",
            "performance_metrics": "Qualitative/controlled results: For 1.5B experiments, M1 (replace o_proj only) produced Level IV outputs (correct reasoning and answer) on several AIME 2024 questions and increased output length; M2 (replace {q,k,v}_proj) produced Level III outputs (context-aware but failing hard reasoning); M3 (replace mlp) produced Level I outputs (nonsense). For larger models merging o_proj often did not yield notable improvements unless normalization parameters were also reconciled.",
            "error_types_or_failure_modes": "Merging can break generation (expected) and is sensitive to normalization mismatch; for larger models layernorm incompatibility with remaining base parameters can prevent merged model from reasoning even when o_proj is replaced. Also merged models are not tuned and can produce incoherent chat responses in chat interface, requiring use of the generate interface.",
            "evidence_for_mechanism": "Direct causal test: replacing only o_proj produced correct arithmetic reasoning for several difficult problems in 1.5B experiments while other single-module replacements did not; this supports o_proj being sufficient (in some configurations) to confer reasoning.",
            "counterexamples_or_challenges": "Scale-dependence: at 7B+ sizes merging o_proj alone often fails to produce reasoning gains; merging requires also adjusting layernorm (and even then may fail), indicating o_proj's sufficiency is conditional on compatibility with the rest of the parameterization.",
            "uuid": "e8151.2",
            "source_info": {
                "paper_title": "Who Reasons in the Large Language Models?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Freeze Stethoscope",
            "name_full": "Freeze Stethoscope (selective fine-tuning)",
            "brief_description": "An intervention that fine-tunes only a small subset of parameters (notably o_proj and layernorm, plus embed_tokens and lm_head) from a base model while freezing other modules, to test whether those few components suffice to acquire reasoning (arithmetic) capability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-32B-Instruct and Qwen2.5-14B-Instruct (base A), fine-tuned variants F1..F4 in Table 2",
            "model_description": "Base autoregressive transformer instruction-tuned models (14B and 32B); fine-tuning configurations vary by which modules are unfrozen (Emb+Head; Emb+Head+o_proj; Emb+Head+{q,k,v,o}_proj; All). Training used dataset s1K (~1,000 high-quality reasoning traces) and hyperparameters noted in paper.",
            "arithmetic_task_type": "AIME 2024, Math-500, GPQA/Diamond style reasoning benchmarks (multi-step arithmetic and logic).",
            "mechanism_or_representation": "Shows that updating a small parameter subset (o_proj + layernorm + embed_tokens + lm_head) can reconfigure the model to perform arithmetic-reasoning tasks, suggesting that learned arithmetic behavior can be induced via modification of these output-projection and normalization pathways rather than wholesale weight rewiring across the network.",
            "probing_or_intervention_method": "Selective freezing/unfreezing during supervised fine-tuning: compare variants F1 (Emb+Head), F2 (Emb+Head+o_proj), F3 (Emb+Head+{q,k,v,o}_proj), F4 (All) and measure reasoning metrics and training dynamics.",
            "performance_metrics": "Table 2 (selected numbers): Qwen2.5-32B: A (base) AIME=0.167, Math=0.836, GPQA-like=0.485; F2 (Emb+Head+o_proj) AIME=0.367, Math=0.890, GPQA-like=0.520; F4 (All) AIME=0.367, Math=0.906, GPQA-like=0.591. Qwen2.5-14B: A AIME=0.133, Math=0.810, GPQA-like=0.449; F2 AIME=0.266, Math=0.848, GPQA-like=0.485; F4 AIME=0.266, Math=0.872, GPQA-like=0.530. F2 achieves large fractions of full-finetune gains while training ~3x faster and using less GPU memory (paper claim).",
            "error_types_or_failure_modes": "Tuning only layernorm (without o_proj) harms reasoning; unfreezing all parameters results in overfitting (MLP capacity memorizes training set) as evidenced by training loss curves; adding {q,k,v}_proj yields little additional gain and may even harm.",
            "evidence_for_mechanism": "Comparative fine-tuning experiments show that F2 (o_proj + LN tuning) recovers most of the reasoning gains of full fine-tuning (F4) on benchmarks while maintaining smoother training loss (less overfitting), supporting that o_proj+LN updates are a compact, effective route to induce arithmetic reasoning.",
            "counterexamples_or_challenges": "Reported results depend on tuning recipe and dataset (s1K) and limited compute; authors note they did not fully reproduce original s1 pipeline and results may not generalize across architectures or training datasets. Also embedding and lm_head are tuned in these runs, so pure o_proj-only sufficiency is not fully isolated.",
            "uuid": "e8151.3",
            "source_info": {
                "paper_title": "Who Reasons in the Large Language Models?",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Destruction Stethoscope",
            "name_full": "Destruction Stethoscope (module ablation / corruption)",
            "brief_description": "A set of destructive interventions (zeroing, re-initialization, or removal of modules) applied to specific layers to identify modules required for conversational fluency versus reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-32B (64-layer) experiments",
            "model_description": "Large base model (Qwen2.5-32B) with selective destruction applied to layers 5-30 to avoid trivial total collapse; three destructors: Zero (set parameters to zero), ReInit (Gaussian random re-init), Remove (delete layer).",
            "arithmetic_task_type": "Indirect: the diagnostics primarily judge conversational output levels (I-IV) rather than arithmetic-only metrics, but results inform which modules are important for conversation vs reasoning (and thus for arithmetic emergence).",
            "mechanism_or_representation": "Functional separation: destruction results indicate MLP components and q_proj/k_proj are important for conversational fluency (and thus for supporting reasoning contexts), while o_proj appears less important for conversation despite being central to reasoning in other tests.",
            "probing_or_intervention_method": "Ablation-style interventions: Zero, ReInit, Remove applied to specific modules across layers 5-30, then observe output level (I-IV).",
            "performance_metrics": "Qualitative outcomes summarized in Table 3: Zeroing o_proj -&gt; Level III (conversation maintained), ReInit o_proj -&gt; Level III, Remove o_proj -&gt; Level III (i.e., destructive operations on o_proj did not break conversational-level outputs in the tested range). By contrast, up_proj/gate_proj/down_proj (MLP) reinit/zero/remove -&gt; Level I (nonsense), q_proj/k_proj destructive operations produced Level I/II outputs, v_proj less impactful (Level II/III depending on destructor).",
            "error_types_or_failure_modes": "Destructive reinit often causes amplified noise across subsequent blocks and can produce level I outputs; destruction experiments are sensitive to which layers are targeted (early/late-layer destructions collapse outputs across the board). Results are qualitative and not presented with statistical aggregations.",
            "evidence_for_mechanism": "Module-specific destruction shows that destroying MLPs (up/down/gate) and q/k projections degrades conversational output strongly while destroying o_proj does not, supporting the conjecture that conversation and reasoning may be localized to different module sets (o_proj crucial for reasoning but less for conversation).",
            "counterexamples_or_challenges": "Destruction findings are weaker/qualitative (single-conversation examples, not broad statistics). Destroying many layers or selecting different layer ranges can trivially collapse outputs, limiting interpretability; results do not prove that o_proj is unnecessary for all conversational content or that destroyed models would preserve arithmetic performance.",
            "uuid": "e8151.4",
            "source_info": {
                "paper_title": "Who Reasons in the Large Language Models?",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        },
        {
            "paper_title": "Qwen technical report",
            "rating": 1,
            "sanitized_title": "qwen_technical_report"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Pushing the limits of mathematical reasoning in open language models",
            "rating": 2,
            "sanitized_title": "pushing_the_limits_of_mathematical_reasoning_in_open_language_models"
        },
        {
            "paper_title": "Transformer feed-forward layers are key-value memories",
            "rating": 1,
            "sanitized_title": "transformer_feedforward_layers_are_keyvalue_memories"
        }
    ],
    "cost": 0.016455499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Who Reasons in the Large Language Models?
27 May 2025</p>
<p>Jie Shao shaoj@lamda.nju.edu.cn 
Jianxin Wu </p>
<p>National Key Laboratory for Novel Software Technology
Nanjing University
China</p>
<p>School of Artificial Intelligence
Nanjing University
China</p>
<p>Who Reasons in the Large Language Models?
27 May 2025DD17581335D89ADA1D6B9524498C9D42arXiv:2505.20993v1[cs.CL]
Despite the impressive performance of large language models (LLMs), the process of endowing them with new capabilities-such as mathematical reasoningremains largely empirical and opaque.A critical open question is whether reasoning abilities stem from the entire model, specific modules, or are merely artifacts of overfitting.In this work, we hypothesize that the reasoning capabilities in welltrained LLMs are primarily attributed to the output projection module (o_proj) in the Transformer's multi-head self-attention (MHSA) module.To support this hypothesis, we introduce Stethoscope for Networks (SfN), a suite of diagnostic tools designed to probe and analyze the internal behaviors of LLMs.Using SfN, we provide both circumstantial and empirical evidence suggesting that o_proj plays a central role in enabling reasoning, whereas other modules contribute more to fluent dialogue.These findings offer a new perspective on LLM interpretability and open avenues for more targeted training strategies, potentially enabling more efficient and specialized LLMs.</p>
<p>Introduction</p>
<p>Although large language models (LLMs) [29,6,41,5] have exhibited great success and potential in various aspects, developing new capabilities for LLMs [53,17,37,14] is still a trial and error experimentation process in most cases.For example, one of the most exciting milestones is LLMs that can reason [18,13,39], e.g., solving complicated mathematical problems using a reasoning sequence that is agreeable by human experts.This success, however, is still in the black-box style.Currently, there are two primary approaches to inspiring reasoning capabilities in LLMs.For the most advanced models [13,51], reinforcement learning method (for example, PPO [36], DPO [30], or GRPO [37]) is commonly adopted to enhance the model's ability to solve complex mathematical or programming problems in a step-by-step manner [48].A more efficient alternative involves supervised fine-tuning (SFT): by providing the backbone LLM with well-prepared, diverse, and step-by-step reasoning traces-often generated through handcrafted examples or existing reasoning models [54,25,13,51]-the model surprisingly acquires reasoning abilities after training.However, despite the practical success of this method, the underlying mechanism remains largely unexplained.It is still unclear why or how this ability emerges.Several potential explanations may account for this phenomenon: Case 1 Is it the LLM in its entirety (i.e., the union of all its weights) that leads to this capability, such that this miracle is not explainable?</p>
<p>Case 2 Or, is there certain module(s) in it that should be praised for this success, such that we can advance our understanding of LLMs?</p>
<p>Figure 1: Stethoscope for Networks.SfN is a framework designed to identify which components of an LLM give rise to specific abilities.By comparing weight changes and observing behaviors under controlled module merging, tuning, or destruction, SfN provides interpretable insights into the origin of capabilities like reasoning.</p>
<p>Case 3 Or in the worst scenario, is reasoning an illusion (e.g., by overfitting to certain types of data), such that we have overestimated the potentials of LLMs?</p>
<p>A definitive answer to any of the above questions will be extremely valuable to guiding the future direction of LLM research.Even a hypothesis or conjecture supported by circumstantial evidences will be highly enlightening, too, let alone when convincing empirical evidences are available.</p>
<p>To this end, our hypothesis is that Case 2 holds in LLMs that reason well.To be more precise, we hypothesize that it is the output projection's parameters (o_proj) in the Transformer [43]'s multi-head self-attention (MHSA) module that is in charge of reasoning in an LLM.</p>
<p>To support our hypothesis, we propose a few techniques for diagnosing LLM's behaviors, in particular, the potential functionalities and impacts of various modules in it.We call these techniques Stethoscope for Networks, or SfN (summarized and illustrated in Figure 1).Starting from reasoning-enhanced models, we argue that the weight differences between a base LLM and its fine-tuned counterpart (e.g., for reasoning tasks) provide firsthand and crucial evidence for understanding internal changes.We refer to this approach as the Delta Stethoscope.</p>
<p>In addition, we introduce two novel and previously unexplored methods within the SfN framework: the Merge Stethoscope and the Destruction Stethoscope.The Merge Stethoscope replaces specific modules in a base model with those from a reasoning model.Surprisingly, the resulting variant can maintain fluent dialogue and demonstrate improved reasoning ability in some cases.This phenomenon offers strong clues about the origin and localization of reasoning capability in LLMs.The Destruction Stethoscope, in contrast, systematically disables individual modules and observes the resulting behavior to infer the functional roles of each component.We also propose the Freeze Stethoscope, which selectively freezes parts of the model during fine-tuning.By controlling which modules are updated, we provide convincing empirical support for earlier insights and clues into the localization of reasoning within LLMs.</p>
<p>With different gadgets we propose in SfN, we provide not only sanity check level tests for our hypothesis, but also more convincing circumstantial supports and even direct empirical evidences.In short, the contributions in this paper are two-fold:</p>
<p>• With various diagnosis evidence (SfN), we are confident in hypothesizing that the output projection o_proj is mainly responsible for the reasoning in LLMs.The impact of this finding include not only potential ways to improve LLM that reasons (e.g., training much faster), but may generalize to produce better LLMs for other tasks (e.g., for a vertical LLM designed specifically for a domain).Our further conjecture is that other modules combined together lead to lucid conversations, but o_proj is less important in conversational ability.</p>
<p>• The proposed Stethoscope for Networks (SfN) gadgets are a set of tools that are useful in understanding modern LLMs and even other networks, which have the potential to enhance our understanding of LLM or deep neural network and may lead to alternative routes for further deep learning research.</p>
<p>Key Hypothesis: Output Projection is the Key for Reasoning</p>
<p>To present our findings, we start by introducing necessary background information and notations, while discussions on related work are deferred to Section 5.</p>
<p>Modern LLMs [41,5,29] mostly consist of many Transformer blocks.A Transformer [43] block is composed of a multi-head self-attention (MHSA) module and a multi-layer perceptron (MLP) module.Components in MHSA include various projections, such as those for computing Q, K and V, denoted as q_proj, k_proj, and v_proj, respectively.The output projection (o_proj) produces MHSA's output.Components in the MLP are mainly linear projections: up, down, and gate [16,41,5] projections, denoted as up_proj, down_proj, and gate_proj, respectively.The computation process is defined as:
x attn = w o Softmax (w q x)(w k x) ⊤ √ d (w v x) x mlp = w down σ(w gate x) ⊙ (w up x)(1)
For simplicity, we omit residual connections and present the computation at the token level, without using matrix or vectorized notation.Other essential components not explicitly included in equation 1 include rotary positional embeddings (RoPE) [38], input embeddings (embed_tokens), layer normalization [4] (layernorm), and the language modeling head (lm_head).</p>
<p>Let A be an LLM with weak or no reasoning ability.By carefully procuring a dataset of reasoning examples [13,25,51], one can cleanse and improve the quality of the dataset into the training data D, and then finetune the existing model A by using techniques such as SFT.The resulting LLM, model B, exhibits strong reasoning capabilities.For example, in commonly adopted practices, the base LLM A is typically a widely used open-source model such as Qwen2.5-Math-1.5B,7B or Qwen2.5-14B,32B [52].The reasoning model B denotes a publicly available reasoning-enhanced variant, such as DeepSeek-R1-Distill-Qwen-1.5B, 7B, 14B, 32B [13], which comes with a clearly specified base model and well-documented training procedure.Models that are either not open-sourced [13,39], or open-sourced without sufficient training details [40] or access to the base model [51], are not discussed in this paper.</p>
<p>The Delta Stethoscope</p>
<p>In the above scenario, it is obvious that A and B share exactly the same network architecture and structure, with their sole difference being the weights (parameters) inside various components.Suppose w(A) (w(B)) denotes the set of weights for all modules in A (B).Then, it is natural to conclude that to understand the difference between A and B (i.e., reasoning or not), we should focus on the difference between w(A) and w(B).Hence, we propose our first Stethoscope for Network.</p>
<p>Assumption 1 (The Delta Stethoscope) Suppose A and B are two LLMs with weak and strong reasoning ability, respectively, and B is obtained by finetuning from A. Then w(B) − w(A) contains essential information if we want to pinpoint the source of the reasoning ability in B.</p>
<p>For each component X (e.g.X = q_proj), we compute the ℓ 2 norm of the weight difference, ∥w X (B) − w X (A)∥ ℓ2 , and visualize the results across all the blocks in Figure 2.For simplicity and due to space constraints, we present three representative comparisons: A is Qwen2.5-Math-1.5B[53] or Qwen2.5-14B,32B [52] and B is DeepSeek-R1-Distill-Qwen-1.5B, 14B, 32B [13].Additional results for other model sizes (7B and 8B) are provided in the appendix and exhibit similar patterns.</p>
<p>For the 1.5B models, the signal is less clear, but o_proj still exhibits a distinct pattern compared to q,k,v_proj-showing the largest change within the attention module and the second-largest across the entire model.As model size increases to 14B and 32B, this trend becomes more pronounced.In both cases, the most notable observation is that when X = o_proj, the ℓ 2 norm is at least two times larger than any other component, indicating the substantial changes in this module during reasoning enhancement.</p>
<p>In Figure 3, we further analyze the distribution of relative weight changes w X (B)−w X (A)
w X (A)
for each linear module.To improve clarity and visual appeal, we plot the distribution every 5 layers and clip values in the range [−1.0, 1.0] to mitigate the influence of outliers.The vertical axis represents the   frequency.A striking and consistent finding is that all linear modules-except o_proj-exhibit a unimodal distribution centered around zero, whereas o_proj uniquely displays a clear bimodal pattern, highlighting its distinct role.</p>
<p>Both observations hold consistently across model sizes and base models: o_proj exhibits the largest or second-largest weight shift, and the overall weight difference patterns remain strikingly similar.Therefore, it is reasonable to guess that the output projection o_proj plays a pivotal role in curating B's reasoning ability.We are, however, not aware of o_proj's specific role: is it solely responsible for reasoning?Or, is it collaborating with another module(s)?Or, in the worst scenario, is this difference in ∥w X (B) − w X (A)∥ ℓ2 and w X (B)−w X (A)
w X (A)
coincidental?From level I to level IV, the model exhibits stronger language organization and logical reasoning skills.Each example includes a question (e.g., a math problem from AIME or a typical user-issued request) and the corresponding response generated by the LLM.</p>
<p>The Merge Stethoscope</p>
<p>We design another gadget, the Merge Stethoscope, to answer this question.Suppose an LLM M is formed by merging models A and B, that is, M has the same structure as A and B, while a subset of its modules' parameters come from A and the rest from B. In a conversational or reasoning task, what will the output of M look like?We can imagine 4 levels of different output, as Level I A sequence of random or nonsense tokens.</p>
<p>Level II A sequence that looks like normal sentences, but does not fit into the context of the task.Level III A sequence that is meaningful sentences that match the task's context well but will fail to reason in difficult problems.Level IV A sequence that reasons-and reasons correctly in most cases.</p>
<p>Figure 4 shows examples of level I to IV outputs.It is worth highlighting that M is rudely merged from A and B without any further tuning.Hence, the intuitive conjecture will be that M will produce level I output (i.e., ushering meaningless tokens).However, if model M , when merged in a specific configuration, is capable of producing level IV outputs for questions that model A fails to solve, then the specially merged components are likely critical for reasoning.</p>
<p>Assumption 2 (The Merge Stethoscope) Suppose M is created by merging the output projection (o_proj) weights of B (which has strong reasoning ability) and all other components of A (which is weak in reasoning), and further suppose that M has stronger reasoning ability compared to A. Then, we assume o_proj is crucial in achieving reasoning in LLMs.</p>
<p>We attempt a minimal or atomic merge by replacing only the o_proj modules in model A = Qwen2.5-Math-1.5B[53] with that of model B = DeepSeek-R1-Distill-Qwen-1.5B[13], keeping all other components unchanged.Although we initially expected the resulting model to produce level I or level II outputs, the results turn out to be surprising.On the AIME 2024 benchmark [19], the merged model M 1 achieves level IV performance on several questions that model A cannot solve.As shown in Table 1, the merged model not only yields correct reasoning and answers, but also tends to generate longer and more detailed responses compared to A. In contrast, replacing other modules such as {q,k,v}_proj and mlp leads to performance degradation.For example, model M 2 , which replaces {q,k,v}_proj, produces level III outputs, while M 3 , which replaces mlp, deteriorates to level I.Only replacing o_proj results in a correct reasoning process and a correct answer, as illustrated in Figure 5.This striking difference motivates our further investigation in Section 3.These results clearly show that the merged model M has a stronger reasoning capacity than A, despite that M is sutured from two completely different models and has never being finetuned.Now we feel confident in our assumption that o_proj is the key component responsible for reasoning in LLMs.</p>
<p>Model</p>
<p>The Freeze Stethoscope</p>
<p>As models A and B scale up (e.g., to 7B parameters), merging components such as q,k,v_proj or mlp still results in significant performance degradation.However, unfortunately, merging o_proj no longer brings notable improvements in solving complex mathematical problems-although it does not harm accuracy, and still increases the generated output length.</p>
<p>Our analysis of ||w X (B) − w X (A)|| ℓ2 suggests that this is due to a substantial mismatch in normalization parameters (that is, layernorm modules) between A and B at larger scales, compared to smaller models (e.g.1.5B).Even when we merge both o_proj and layernorm parameters from B, the resulting model M still fails to reason effectively, probably because the remaining parameters of A are incompatible with the normalization parameters of B. To investigate this hypothesis in larger LLMs, we introduce the Freeze Stethoscope.</p>
<p>Assumption 3 (The Freeze Stethoscope) Suppose that an LLM F is obtained by supervised finetuning using the dataset D. F is initialized from A, and both o_proj and normalization components are tuned while other components are frozen.If F exhibits strong reasoning ability, then we assume that o_proj is crucial in achieving reasoning in LLMs even in large-scale models.</p>
<p>It is worth noting that embed_tokens and lm_head are also tuned. 2 Normalization module parameters are unfrozen by default.We adopt the pipeline of s1 [25] as our baseline, which uses the base model A = Qwen2.5-32B-Instructand the dataset D = s1K containing 1,000 high-quality reasoning traces.The results are shown in Table 2, where our model F 4 corresponds to model B in Assumption 3. We do not strictly follow the training or testing setup of s1, primarily due to limited computational resources and the lack of an exact testing recipe to reproduce the reported results.However, our objective is not to optimize accuracy via testing tricks or prompt tuning, but to highlight the effectiveness of o_proj tuning compared to full-parameter tuning.For fair comparison, we adopt the "Budget Forcing Wait 2x" setting from s1 and retain all configurations without hyperparameter tuning.</p>
<p>Using this simplest possible experimental setup, Table 2 clearly shows that simply tuning o_proj and layernorm (model F 2 )) leads to strong reasoning ability, while at the same time only tuning layernorm (model F 1 ) harms the reasoning of the LLM.Further unfreezing the parameters of {q,k,v}_proj (model F 3 ) yields little additional gain or even negative impact.</p>
<p>The training loss curves are shown in Figure 6.When all parameters including MLP are unfrozen, the model exhibits clear signs of overfitting, likely using the large MLP capacity to memorize the training set.In contrast, tuning only o_proj yields a smoother and more stable curve.Combined  with its competitive performance, this suggests that the model learns to reason rather than simply memorize.Hence, we are now prepared and feel supported to propose our key hypothesis:</p>
<p>Hypothesis 1 (Outstanding Output Projection) In an LLM that reasons well, we hypothesize that the output projection (o_proj) component is the single or at least the most important module that dominates its reasoning ability.</p>
<p>With carefully chosen tuning strategy and hyperparameters, there is reason to believe that tuning only o_proj (+LN) can reach the level of model B in terms of reasoning performance.And, beyond exhibiting reasoning abilities, Table 2 also shows that tuning only o_proj (+LN) has other significant advantages: e.g., significantly faster finetuning (3 times faster) and smaller GPU memory consumption.These advantages will become more established when larger LLMs are tuned.</p>
<p>Conjecture: Conversation Hinges on Other Modules but Not Output</p>
<p>We are mainly concerned with two abilities of LLMs: conversation and reasoning, which map to level III and IV in our categorization of LLM's outputs, respectively.Our Hypothesis 1 is on reasoning, but are there one module or several modules accounting for lucid conversations?In this section, we further propose a new stethoscope to diagnose this question and raise our conjectures accordingly.</p>
<p>The Destruction Stethoscope</p>
<p>Our previous stethoscopes follow a "constructive proof" style, while now we resort to the "proof by contradiction" style.If one module in an LLM is "destructed", and the LLM can still produce level III conversation outputs, then we have good reasons to guess that this module is not important in conversational ability; while it is important if the LLM ceases to dialogue regularly.</p>
<p>Assumption 4 (The Destruction Stethoscope) Suppose a module X is destructed (i.e., its normal functionality is disabled by some destruction method) in an LLM A. We denote the resulting LLM as We propose 3 destructors to destroy a module:</p>
<p>Zero Set all parameters within X to 0.</p>
<p>ReInit Re-initialize all parameters inside X using Gaussian random numbers (mean=0, std=0.02).</p>
<p>Remove Remove the entire layer.</p>
<p>The Zero destructor is often equivalent to setting the output activation of X to zeros (e.g., in a linear module like o_proj).We want to emphasize that ReInit incurs more serious damages to an LLM than Zero does.Zero may change activations to zero, but ReInit exerts random effects (i.e., noise) to LLM activations.What is more important, these random effects will act as input to the next Transformer block and the noise is quickly amplified.Hence, level I or II output is expected when X is destroyed (especially when reinitialized) in a large number of Transformer blocks.</p>
<p>Conjectures Concerning the Conversation Capability</p>
<p>For model Qwen2.5-32B with 64 layers, we observe that destroying modules in early or late layerswhere input and output representations are more sensitive-consistently yields level I outputs.To avoid this, we restrict destruction to blocks 5-30.This range is empirically chosen, as affecting more layers often causes all outputs to degrade to level I, making distinctions between modules impossible.</p>
<p>The experimental results are presented in Table 3. Specifically, we destroy selected modules and analyze the corresponding output.The Remove destructor removes the transformer layers as a whole.Note that the results are not statistics computed in many different experiments-it only reflects the conversation illustrated in Figure 4, but we observed similar patterns for other conversations.</p>
<p>Table 3 reveals distinct roles of modules in conversation.Notably, o_proj-crucial for reasoningappears unimportant for conversation.In contrast, all MLP components (up_proj, down_proj, gate_proj) are essential.Within MHSA, q_proj and k_proj are important, while v_proj plays a minor role.Based on these (admittedly weaker) observations, we propose the following conjecture.</p>
<p>Conjecture 1 (Division of Labor) Based on current observations, an LLM can be roughly divided as two sets of modules: output projection (o_proj) and all others, where o_proj is mainly responsible for reasoning and other modules for conversation.</p>
<p>Then, output projection plays a unique role if this conjecture holds.Hence, we further propose another conjecture for it.</p>
<p>Conjecture 2 (Output Projection Plugin) With conversational capabilities provided by other (frozen) modules, output projections may act as a plugin.For example, one set of o_proj for reasoning, and another set of o_proj for migrating an LLM to a vertical domain.</p>
<p>Potential Implications and Applications</p>
<p>This paper mainly diagnoses LLMs from a theoretical, highly abstract perspective.However, our hypothesis and conjectures can also have highly practical implications and applications as long as they are correct or at least partially hold.</p>
<p>• Fast and better reasoning LLMs.By finetuning only o_proj, we can potentially find a better reasoning LLM with much faster training and much smaller GPU memory footprint.</p>
<p>• Integrating non-reasoning and reasoning LLMs.There is a recent trend to integrate chatting and reasoning LLMs into one model [51].When we finetune a base LLM into a reasoning one using the previous procedure, they only differ in o_proj, layernorm, embed_tokens and lmhead, which occupy only 10% of model size.Hence, the two LLMs are easily loaded as one LLM with two sets of these module for different purposes.</p>
<p>• Vertical LLMs.Similarly, when equipped with different output projection plugins, one may adeptly obtain vertical LLMs for different domains.</p>
<p>• Understanding deep neural networks.The proposed Stethoscopes for Networks might be useful gadgets to understand other deep models, and new stethoscopes can be further developed.They will be potentially useful in diagnosing existing networks and even in providing alternative directions to future deep learning research.</p>
<p>Related Work</p>
<p>Large Language Models.Modern LLMs such as GPT [29,6], LLaMA [41,42], Qwen [5,52], and other representative models [7,20] adopt an auto-regressive architecture and have demonstrated impressive capabilities across a wide range of natural language processing tasks, including question answering [32,22], summarization [26,27], and translation [50].These models are typically trained on large-scale corpora using next-token prediction objectives, and their performance has been shown to scale with model size [21].Further improvements in alignment and usability have been achieved through instruction tuning [28,9,46] and reinforcement learning from human feedback (RLHF) [8,30], enabling more controllable and helpful dialogue generation.</p>
<p>Reasoning Models.While LLMs exhibit emergent reasoning abilities [47], recent efforts have further enhanced these capabilities through fine-tuning and architectural modifications [35,55].Chain-of-thought prompting [48] encourages intermediate reasoning steps, improving performance in arithmetic tasks, while self-consistency decoding [45] improves robustness by sampling multiple reasoning paths.Inspired by OpenAI's o1 [18], most advanced models now employ reinforcement learning [36,30] to generate long reasoning traces with sparse rewards.This leads to significant improvements, particularly in complex math, code, and other professional domains [13,51].Despite these advances, the origin and location of reasoning ability in LLMs remain underexplored.</p>
<p>Interpretability of LLMs.Understanding the inner workings of LLMs has attracted growing interest.Prior efforts include attention visualization [44], probing [15], and model editing [24,34], with the aim of interpreting internal representations.Other studies decompose the behavior of the model into attribute functions to specific modules [11].The "Physics of Language Models" series [1,2,3] investigates LLMs through controlled setups to reveal empirical and universal laws that dictate LLM behavior.However, these studies often exclude the most advanced models or focus on narrow, synthetic settings, offering limited insight into real-world models.Their findings provide little practical guidance for understanding reasoning in state-of-the-art models.</p>
<p>Conclusions</p>
<p>This work investigates a fundamental question in understanding large language models (LLMs): Is there a component or several components that are responsible for achieving the reasoning ability in LLMs?If the answer is affirmative, which components are responsible for the improvement?</p>
<p>We hypothesize that the output projection (o_proj) module plays a central role in enabling reasoning capabilities.To support this, we propose Stethoscope for Networks (SfN), a diagnostic framework that encompasses several probing techniques.Through the proposed Delta, Merge, Freeze, and Destruction stethoscopes, we observe consistent patterns indicating that o_proj is critical for reasoning, while other modules primarily support conversational fluency.These findings open new directions for efficient and modular LLM training.</p>
<p>Our findings are primarily based on a limited set of model families and reasoning benchmarks, and may not generalize to all architectures or tasks.Some diagnostic results rely on qualitative assessments rather than statistical validation.Furthermore, while the role of o_proj is empirically highlighted, a theoretical understanding of its function in reasoning remains to be established.</p>
<p>Acknowledgments and Disclosure of Funding</p>
<p>This work was partly supported by the National Natural Science Foundation of China under Grant 62276123 JW proposed the assumptions (Stethoscopes for Networks), hypothesis and conjectures.JS started this line of research in our group, proposed the Zero destructor, and effectively supported our main findings with experimental results.JW and JS wrote the paper.</p>
<p>We thank Ke Zhu for discussions.</p>
<p>A Experimental Details</p>
<p>We primarily utilize open-sourced models to conduct experiments in this work.Given that DeepSeek-R1 is one of the most widely adopted reasoning models, and its authors have released a series of distilled models based on R1 [13], including both the specified base and finetuned reasoning models, we adopt their configurations in our study.Specifically, we use the DeepSeek-R1-Distill-Qwen [13] models with sizes of 1.5B, 7B, 14B, 32B and 70B as our reasoning models, and select Qwen2.5-Math-1.5B,7B [53], LLaMA3.1-8B[12], Qwen2.5-14B,32B [52] or Llama-3.3-70B-Instruct[12] as base models.All models are loaded and run using the Transformers library [49].</p>
<p>Our evaluation framework is based on the lm-evaluation-harness package [10].To accelerate inference, we use vLLM [23] as the backend, which may slightly affect performance due to backend-specific optimizations.In the Merge Stethoscope experiments, we observe that the "chat" interface often generates irrelevant or nonsensical responses, while the "generate" interface produces coherent and contextually appropriate outputs.We suspect this discrepancy arises from misinterpreted system prompts.Therefore, we rely on the "generate" interface and implement a custom evaluation toolkit.</p>
<p>For the Freeze Stethoscope experiments, we build on the codebase of s1 [25].We use a learning rate of 1e-5, weight decay of 1e-4, a batch size of 16, and train for 5 epochs.Due to hardware limitations (i.e., lack of access to 16 H100 GPUs), we leverage DeepSpeed [33] with ZeRO Stage 3 [31] to enable efficient training.The base model used here is Qwen2.5-32B-Instruct[52].Evaluation is again conducted with lm-evaluation-harness, following the modified pipeline by the authors of s1, which disables generation of the end-of-thinking token and optionally appends the string "Wait" to the reasoning trace to encourage model reflection.We adopt the Budget Forcing "Wait" ×2 as our default testing configuration.</p>
<p>All visualization and inference experiments on 1.5B-14B models are conducted on a single NVIDIA A100 GPU.For training and evaluating 32B-70B models, we use a cluster of 8 NVIDIA A100 GPUs.</p>
<p>Training typically takes around 6 hours, while testing on a single dataset usually requires about 2 hours.</p>
<p>B More Experimental Results</p>
<p>In the main paper, we present visualization results for the 1.5B, 14B, and 32B models.Here, we supplement those results by providing additional visualizations for the 7B, 8B, and 70B models.Following the Delta Stethoscope pipeline, we visualize both the absolute weight shift |w X (B) − w X (A)| ℓ2 and the relative weight shift w X (B)−w X (A) w X (A)</p>
<p>. The absolute weight shifts are shown in Figure 7, and the relative weight shifts are presented in Figure 8.The trends observed in the main paper remain consistent across these additional models.Notably, o_proj consistently exhibits the</p>
<p>C Statistical Significance and Broader Impacts</p>
<p>We report appropriate information regarding the statistical significance of our experiments.While we do not primarily focus on classical significance tests such as p-values, we provide multiple forms of empirical evidence-such as consistent module-specific weight shifts, response-level comparisons under controlled manipulations, and loss curves under different tuning strategies-that collectively establish the robustness of our findings.These analyses serve as a practical alternative to traditional error bars or confidence intervals and help substantiate our key claims.</p>
<p>This research has both promising benefits and important risks to consider.On the positive side, the proposed Stethoscope for Networks (SfN) framework provides a novel set of tools for interpreting LLMs, especially by localizing specific capabilities-such as reasoning-to individual components like the output projection (o_proj).These tools may significantly improve our understanding of LLMs, enabling more transparent, modular, and efficient model development.For instance, if reasoning abilities can be enhanced by tuning a small subset of parameters, it could greatly reduce computational costs and increase accessibility for developing domain-specific or lightweight models.</p>
<p>However, this line of work also carries potential risks.Precisely identifying and isolating reasoningrelated components might lower the barrier for targeted manipulation, such as unauthorized transfer or removal of reasoning abilities across models.This could facilitate misuse scenarios, including capability extraction, tampering, or model theft.Furthermore, while the diagnostic methods proposed aim to support interpretability, there is a risk that they may be overinterpreted, leading to an inflated sense of model transparency that does not generalize across architectures or tasks.</p>
<p>Qwen2. 5 -
5
14B vs. DeepSeek-R1-Distill-Qwen-14B Qwen2.5-32B vs. DeepSeek-R1-Distill-Qwen-32B Qwen2.5-Math-1.5Bvs. DeepSeek-R1-Distill-Qwen-1</p>
<p>Figure 2 :
2
Figure 2: Per-module L2 distance of linear weights between models A and B. Notably, the o_proj module shows the second-largest change in 1.5B models, and the largest in 14B and 32B models, highlighting its potential importance for reasoning.Similar trends are observed in 7B and 8B models (see appendix).</p>
<p>Qwen2. 5 -Figure 3 :
53
Figure 3: Layer-wise distribution of relative weight changes between models A and B. While most modules display a unimodal distribution, the o_proj module uniquely exhibits a bimodal distribution, highlighting its distinctive behavior.Consistent patterns are observed across models of other sizes, with detailed results provided in the appendix.</p>
<p>Can you write a short paragraph about the importance of reading books?A1: [[[[[[ … / A2: the importance of course, the importance of course, of course, the importance of course, of course … Level II: Irrelevant Grammatically correct, but contextually off-topic.Q: Can you write a short paragraph about the importance of reading books?A: Can you provide me with some data?Level III: Context-Aware Understands the question, but may apply flawed reasoning.Q: Every morning, Aya … This morning, if she walks at s+1/2 kilometers per hour, how many minutes will the walk take?A: First, let's set up the equations … Simplify and solve: Fully understands the context and provides a valid solution.Q: Every morning, Aya … This morning, if she walks at s+1/2 kilometers per hour, how many minutes will the walk take?A: First, the problem says that … Subtract: Then: Today: speed is 3 km/h, walk = 180 min, total = 180 + 24 = 204 minutes.</p>
<p>5 3
5
.6 → s(s + 2) = 11.25 → s = 2.Can you write a short paragraph about the importance of reading books?A: Absolutely! Reading is a crucial skill that can help students improve … and understand the implications of these skills.</p>
<p>Figure 4 :
4
Figure 4: Four levels of responses generated by the LLM.From level I to level IV, the model exhibits stronger language organization and logical reasoning skills.Each example includes a question (e.g., a math problem from AIME or a typical user-issued request) and the corresponding response generated by the LLM.</p>
<p>M 2 : 3 Figure 5 :
235
Figure 5: Examples of outputs generated by merged models.Only M 1 produces both a valid reasoning process and the correct answer.</p>
<p>4 Figure 6 :
46
Figure 6: Training loss curves for fine-tuning Qwen2.5-14B,32B-Instruct on reasoning tasks.Different models unfreeze different sets of parameters, as detailed in Table2.</p>
<p>Qwen2.5Math-7B vs. DeepSeek-R1-Distill-Qwen-7B Llama-3.1-8B vs. DeepSeek-R1-Distill-Llama-8B Llama3.3-70B-Instruct vs. DeepSeek-R1-Distill-Llama-70B</p>
<p>Figure 7 :Figure 8 :
78
Figure 7: Per-module L2 distance of linear weights between models A and B. Notably, the o_proj module shows the largest in 7B, 8B and 70B models, highlighting its potential importance for reasoning.</p>
<p>Table 1 :
1
AIME 2024 accuracy of the base model, the reasoning model, and their merged variants.Each merged model is constructed by replacing specific modules in model A with the corresponding module from model B. Every morning, Aya does a 9 kilometer walk … if she walks at s+1/2 kilometers per hour, how many minutes will the walk take?: To solve this problem, we need to determine … So, the walk will take 204 minutes, including the 24 minutes at the coffee shop.The final answer is 204.
ReplacedAIMEAverageModule2024TokensA (Q-1.5B)-0.0672421M 1M 1o_proj0.2005418M 2{q,k,v}_proj 0.0002058M 3mlp0.00015532B (D-1.5B)-0.23311892
Q:</p>
<p>:</p>
<p>To solve this problem … output 12.0000000000000.The output indicates that the time taken for the walk is 12 minutes.So, the final answer is 12.</p>
<p>Table 2 .
2ModelFintuned Modules#Param (B) Steps/s AIME 2024 Math 500 GPQA DiamondA (Q-32B) ---0.1670.8360.485F 1Emb + Head1.50.0550.2000.7560.444F 2Emb + Head + o_proj3.20.0520.3670.8900.520F 3Emb + Head + {q,k,v,o}_proj5.60.0440.3000.8860.525F 4 (B)All32.80.0150.3670.9060.591A (Q-14B) ---0.1330.8100.449F 1Emb + Head1.50.1060.1330.7220.414F 2Emb + Head + o_proj2.80.0990.2660.8480.485F 3Emb + Head + {q,k,v,o}_proj3.70.0810.2330.8540.490F 4 (B)All14.70.0530.2660.8720.530</p>
<p>Table 2 :
2
Reasoning</p>
<p>performance of different fine-tuning strategies on Qwen2.5-{14B,32B}-Instruct.Emb denotes embed_tokens, Head denotes lm_head, and Attn denotes the entire MHSA.#Param refers to the number of trainable parameters, Steps/s indicates training speed, and the last three columns report commonly used metrics for evaluating reasoning models.</p>
<p>Table 3 :
3
Output levels of different modules under the three destruction methods: Zero, ReInit, and Remove.All experiments are based on Qwen2.5-32B with destruction applied to specific layers.D.Then, the fact that D continues (or ceases to) produce level III output (meaningful sentences in the conversation's context) indicates whether X is important for conversational abilities or not.
Destruction MethodModuleOutput LevelDestruction MethodModuleOutput Levelq_projIq_projIk_projIk_projIv_projIIIv_projIIZeroo_projIIIReInito_projIIIup_projIup_projIgate_projIgate_projIdown_projIdown_projIRemove-I
Without tuning these components, finetuning failed to converge.</p>
<p>Physics of language models: Part 3.1, knowledge storage and extraction. Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2309.143162023arXiv preprint</p>
<p>Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2309.14402Physics of language models: Part 3.2, knowledge manipulation. 2023arXiv preprint</p>
<p>Zeyuan Allen, -Zhu , Yuanzhi Li, arXiv:2404.05405Physics of language models: Part 3.3, knowledge capacity scaling laws. 2024arXiv preprint</p>
<p>. Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E Hinton, arXiv:1607.064502016Layer normalization. arXiv preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.166092023Qwen technical report. arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Nina Mielke, Alec Radford, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Jan Paul F Christiano, Leike, Miljan Tom B Brown, Shane Martic, Dario Legg, Amodei, Advances in neural information processing systems. 201730</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Xin Wang, Xingyu Yuan, Adams Yu, Sharan Narang, arXiv:2210.114162022arXiv preprint</p>
<p>The language model evaluation harness. Leo Gao, Jonathan Tow, Stella Baber Abbasi, Sid Biderman, Anthony Black, Charles Dipofi, Laurence Foster, Jeffrey Golding, Alain Hsu, Haonan Le Noac'h, Kyle Li, ; Mcdonell, Andy Zou, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite. Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf; Ben Wang, Kevin Wang,072024</p>
<p>Transformer feed-forward layers are key-value memories. Mor Geva, Tal Schuster, Jonathan Berant, arXiv:2012.149132021arXiv preprint</p>
<p>The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv:2407.217832024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Deepseek-coder: When the large language model meets programming-the rise of code intelligence. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, Li, arXiv:2401.141962024arXiv preprint</p>
<p>A structural probe for finding syntax in word representations. John Hewitt, Christopher D Manning, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies20191</p>
<p>Transformer quality in linear time. Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc Le, International conference on machine learning. PMLR2022</p>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, arXiv:2409.121865-coder technical report. 2024arXiv preprint</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>. Maxwell Jia, 2024. 2024</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, arXiv:2001.08361Scaling laws for neural language models. 2020arXiv preprint</p>
<p>Natural questions: a benchmark for question answering research. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Transactions of the Association for Computational Linguistics. 72019</p>
<p>Efficient memory management for large language model serving with pagedattention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, Ion Stoica, Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. the ACM SIGOPS 29th Symposium on Operating Systems Principles2023</p>
<p>Locating and editing factual associations in gpt. Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov, Advances in neural information processing systems. 202235</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>
<p>Abstractive text summarization using sequence-to-sequence rnns and beyond. Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, arXiv:1602.060232016arXiv preprint</p>
<p>Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. Shashi Narayan, Shay B Cohen, Mirella Lapata, arXiv:1808.087452018arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>Zero: Memory optimizations toward training trillion parameter models. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He, SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE2020</p>
<p>Squad: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, arXiv:1606.052502016arXiv preprint</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining. the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining2020</p>
<p>The mechanistic basis of data dependence and abrupt learning in an in-context classification task. Gautam Reddy, arXiv:2312.030022023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Ananya Dwivedi-Yu, Roberta Raileanu, Saghar Hosseini, Murray Chadwick, Gaurav Mishra, Siddharth Karamcheti, Neil Houlsby, Aravind Elangovan, Mike Lewis, arXiv:2302.047612023arXiv preprint</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu, Neurocomputing. 5681270632024</p>
<p>Kimi k1. 5: Scaling reinforcement learning with llms. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, arXiv:2501.125992025arXiv preprint</p>
<p>QwQ-32B: Embracing the Power of Reinforcement Learning. Qwen Team, March 2025</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Analyzing the structure of attention in a transformer language model. Jesse Vig, Yonatan Belinkov, arXiv:1906.042842019arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, arXiv:2212.105602022arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032022arXiv preprint</p>
<p>Transformers: State-of-theart natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Xu, Sylvain Le Scao, Mariama Gugger, Quentin Drame, Alexander M Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAssociation for Computational LinguisticsOctober 2020</p>
<p>Google's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, Mohammad Quoc V Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.081442016arXiv preprint</p>
<p>. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, 2025Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren,Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.1511520245 technical report. arXiv preprint</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, arXiv:2409.12122Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. 2024arXiv preprint</p>
<p>Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, Junxian He, 2025</p>
<p>Denny Zhou, Dale Schuurmans, Xuezhi Wang, Ed Chi, Quoc V Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
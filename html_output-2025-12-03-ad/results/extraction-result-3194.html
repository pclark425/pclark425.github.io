<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3194 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3194</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3194</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-8ee45aeb7c97e3346cc62f216f673b91277ac718</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8ee45aeb7c97e3346cc62f216f673b91277ac718" target="_blank">LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Computer Vision</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents and proposes a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment.</p>
                <p><strong>Paper Abstract:</strong> This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. 1</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3194.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3194.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-Planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A few-shot high-level planner that uses GPT-3 via in-context prompting to generate high-level plans for embodied agents, and that grounds and updates plans online by injecting perceived objects and completed subgoals into prompts (grounded re-planning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-Planner (GPT-3 text-davinci-003 as high-level planner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical planner: GPT-3 is used as the high-level planner via carefully designed prompts and dynamically retrieved in-context examples; a separate low-level planner (from HLSM) maps subgoals to primitive actions. Supports dynamic grounded re-planning by re-prompting GPT-3 with the list of observed objects and completed subgoals when stuck or after a timeout.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented in-context exemplars (kNN retrieval) and episodic observed-object list (episode memory injected into prompt); completed-subgoal history</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Two complementary memory mechanisms are used: (1) a kNN retriever (frozen BERT embeddings, Euclidean distance) selects K nearest training examples as in-context exemplars that are inserted into the prompt to provide task-specific demonstrations; (2) an episodic environment memory O collects objects detected by the agent's object detector over time (only labels above 80% confidence), plus a list G of completed subgoals; both O and G are injected into the prompt during grounded re-planning and used with logit biases favoring observed objects, and re-planning is triggered on subgoal failure or timeout.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon embodied instruction following in AI2-THOR-based ALFRED: given a natural language instruction (goal-level and optionally step-by-step), produce and execute a sequence of navigation and interaction actions to achieve goal-state changes in partially-observable household environments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / embodied instruction following (vision-and-language navigation with interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Dynamic LLM-Planner + HLSM (uses kNN retrieval + grounded re-planning): Goal-only (Test Unseen) SR=13.41% , GC=22.89%; Test Seen SR=15.33% , GC=24.57%; Valid Unseen HLP ACC=33.81–55.85. Step-by-step (Test Unseen) SR=16.42% , GC=23.37%; Test Seen SR=18.20% , GC=26.77%; Valid Unseen HLP ACC=46.59–68.31 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Static LLM-Planner (no grounded re-planning) + HLSM: Goal-only (Test Unseen) SR=11.58% , GC=18.47%; Step-by-step (Test Unseen) SR=15.83% , GC=20.99% (Table 1). Ablation removing kNN retriever: LOOCV HLP accuracy drops from 40.59 to 17.48; removing logit biases reduces to 38.10; removing both to 13.43 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented in-context exemplars and episodic observed-object memory (injected into prompts) materially improve high-level planning quality and end-to-end task success in few-shot regime: kNN retrieval is critical (large drop in HLP ACC when removed), and grounded re-planning improves success rates over static planning (e.g., +~1.83% SR on unseen test for goal-only). LLM-Planner achieves competitive few-shot performance (<0.5% paired training data) compared to full-data baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends on the quality of the object detector and low-level controller (object-detection failures can bottleneck end-to-end SR); memory is limited to what is injected into the prompt (prompt window constraints); additional API cost from LLM calls (though fewer than some baselines); re-planning still leaves substantial room for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3194.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3194.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SayCan (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do as i can and not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking-based LLM planner that combines LLM relevance scores with learned affordance/value functions to rank admissible skills (action-object pairs) and select executable skills for robotic tasks; in this paper it was adapted to ALFRED with oracle environment knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can and not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan (GPT-3 ranker + value function; ALFRED adaptation used oracle object information)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>SayCan ranks a pre-compiled list of admissible skills (each described in natural language) using an LLM probability and multiplies by an affordance/value score that estimates executability; selection is based on the combined score. In the ALFRED adaptation the authors provided oracle knowledge of objects/affordances to compile the skill list and used an oracle value function.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>oracle admissible-skills list / environment-affordance value function (environment memory used to compile and score skills)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Relies on an explicit list of admissible skills compiled from known environment objects (in the paper's ALFRED adaptation this was provided as oracle metadata) and a value/affordance function that gives an executability score; at each decision step SayCan queries the LLM for each skill's relevance and combines with affordance scores to pick a skill.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFRED (adaptation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same ALFRED embodied instruction following tasks; SayCan was adapted to work by treating skills as (high-level action, object) pairs and using the same low-level controller for execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / embodied instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Adapted SayCan (with oracle environment info) on few-shot step-by-step setting: Valid Unseen SR=9.88% , GC=22.54% , HLP ACC=37.57; Valid Seen SR=12.30% , GC=24.52% , HLP ACC=35.15 (Table 1). Note: test split evaluation not possible due to ALFRED test metadata restrictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even with an oracle admissible-skill list and oracle affordance scores, SayCan underperforms the generative LLM-Planner in the few-shot ALFRED adaptation; SayCan requires many more LLM calls (avg. 22 per task) compared to LLM-Planner (avg. 7 per task), making it less efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires knowledge of admissible actions/skills a priori (hard in partially-observable environments), scales poorly as environment complexity increases (combinatorial growth in skills), needs a trained value function (they used oracle in adaptation), higher LLM call count increases cost and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can and not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>True few-shot learning with language models <em>(Rating: 2)</em></li>
                <li>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents <em>(Rating: 2)</em></li>
                <li>Thinking about gpt-3 in-context learning for biomedical ie? think again <em>(Rating: 1)</em></li>
                <li>ProgPrompt: Generating situated robot task plans using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3194",
    "paper_id": "paper-8ee45aeb7c97e3346cc62f216f673b91277ac718",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "LLM-Planner",
            "name_full": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
            "brief_description": "A few-shot high-level planner that uses GPT-3 via in-context prompting to generate high-level plans for embodied agents, and that grounds and updates plans online by injecting perceived objects and completed subgoals into prompts (grounded re-planning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM-Planner (GPT-3 text-davinci-003 as high-level planner)",
            "agent_description": "Hierarchical planner: GPT-3 is used as the high-level planner via carefully designed prompts and dynamically retrieved in-context examples; a separate low-level planner (from HLSM) maps subgoals to primitive actions. Supports dynamic grounded re-planning by re-prompting GPT-3 with the list of observed objects and completed subgoals when stuck or after a timeout.",
            "memory_used": true,
            "memory_type": "retrieval-augmented in-context exemplars (kNN retrieval) and episodic observed-object list (episode memory injected into prompt); completed-subgoal history",
            "memory_mechanism_description": "Two complementary memory mechanisms are used: (1) a kNN retriever (frozen BERT embeddings, Euclidean distance) selects K nearest training examples as in-context exemplars that are inserted into the prompt to provide task-specific demonstrations; (2) an episodic environment memory O collects objects detected by the agent's object detector over time (only labels above 80% confidence), plus a list G of completed subgoals; both O and G are injected into the prompt during grounded re-planning and used with logit biases favoring observed objects, and re-planning is triggered on subgoal failure or timeout.",
            "task_name": "ALFRED",
            "task_description": "Long-horizon embodied instruction following in AI2-THOR-based ALFRED: given a natural language instruction (goal-level and optionally step-by-step), produce and execute a sequence of navigation and interaction actions to achieve goal-state changes in partially-observable household environments.",
            "task_type": "planning / embodied instruction following (vision-and-language navigation with interactions)",
            "performance_with_memory": "Dynamic LLM-Planner + HLSM (uses kNN retrieval + grounded re-planning): Goal-only (Test Unseen) SR=13.41% , GC=22.89%; Test Seen SR=15.33% , GC=24.57%; Valid Unseen HLP ACC=33.81–55.85. Step-by-step (Test Unseen) SR=16.42% , GC=23.37%; Test Seen SR=18.20% , GC=26.77%; Valid Unseen HLP ACC=46.59–68.31 (Table 1).",
            "performance_without_memory": "Static LLM-Planner (no grounded re-planning) + HLSM: Goal-only (Test Unseen) SR=11.58% , GC=18.47%; Step-by-step (Test Unseen) SR=15.83% , GC=20.99% (Table 1). Ablation removing kNN retriever: LOOCV HLP accuracy drops from 40.59 to 17.48; removing logit biases reduces to 38.10; removing both to 13.43 (Table 2).",
            "has_performance_comparison": true,
            "key_findings": "Retrieval-augmented in-context exemplars and episodic observed-object memory (injected into prompts) materially improve high-level planning quality and end-to-end task success in few-shot regime: kNN retrieval is critical (large drop in HLP ACC when removed), and grounded re-planning improves success rates over static planning (e.g., +~1.83% SR on unseen test for goal-only). LLM-Planner achieves competitive few-shot performance (&lt;0.5% paired training data) compared to full-data baselines.",
            "limitations_or_challenges": "Performance depends on the quality of the object detector and low-level controller (object-detection failures can bottleneck end-to-end SR); memory is limited to what is injected into the prompt (prompt window constraints); additional API cost from LLM calls (though fewer than some baselines); re-planning still leaves substantial room for improvement.",
            "uuid": "e3194.0",
            "source_info": {
                "paper_title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "SayCan (adapted)",
            "name_full": "Do as i can and not as i say: Grounding language in robotic affordances",
            "brief_description": "A ranking-based LLM planner that combines LLM relevance scores with learned affordance/value functions to rank admissible skills (action-object pairs) and select executable skills for robotic tasks; in this paper it was adapted to ALFRED with oracle environment knowledge.",
            "citation_title": "Do as i can and not as i say: Grounding language in robotic affordances",
            "mention_or_use": "use",
            "agent_name": "SayCan (GPT-3 ranker + value function; ALFRED adaptation used oracle object information)",
            "agent_description": "SayCan ranks a pre-compiled list of admissible skills (each described in natural language) using an LLM probability and multiplies by an affordance/value score that estimates executability; selection is based on the combined score. In the ALFRED adaptation the authors provided oracle knowledge of objects/affordances to compile the skill list and used an oracle value function.",
            "memory_used": true,
            "memory_type": "oracle admissible-skills list / environment-affordance value function (environment memory used to compile and score skills)",
            "memory_mechanism_description": "Relies on an explicit list of admissible skills compiled from known environment objects (in the paper's ALFRED adaptation this was provided as oracle metadata) and a value/affordance function that gives an executability score; at each decision step SayCan queries the LLM for each skill's relevance and combines with affordance scores to pick a skill.",
            "task_name": "ALFRED (adaptation)",
            "task_description": "Same ALFRED embodied instruction following tasks; SayCan was adapted to work by treating skills as (high-level action, object) pairs and using the same low-level controller for execution.",
            "task_type": "planning / embodied instruction following",
            "performance_with_memory": "Adapted SayCan (with oracle environment info) on few-shot step-by-step setting: Valid Unseen SR=9.88% , GC=22.54% , HLP ACC=37.57; Valid Seen SR=12.30% , GC=24.52% , HLP ACC=35.15 (Table 1). Note: test split evaluation not possible due to ALFRED test metadata restrictions.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "key_findings": "Even with an oracle admissible-skill list and oracle affordance scores, SayCan underperforms the generative LLM-Planner in the few-shot ALFRED adaptation; SayCan requires many more LLM calls (avg. 22 per task) compared to LLM-Planner (avg. 7 per task), making it less efficient.",
            "limitations_or_challenges": "Requires knowledge of admissible actions/skills a priori (hard in partially-observable environments), scales poorly as environment complexity increases (combinatorial growth in skills), needs a trained value function (they used oracle in adaptation), higher LLM call count increases cost and latency.",
            "uuid": "e3194.1",
            "source_info": {
                "paper_title": "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can and not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "True few-shot learning with language models",
            "rating": 2
        },
        {
            "paper_title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
            "rating": 2
        },
        {
            "paper_title": "Thinking about gpt-3 in-context learning for biomedical ie? think again",
            "rating": 1
        },
        {
            "paper_title": "ProgPrompt: Generating situated robot task plans using large language models",
            "rating": 1
        }
    ],
    "cost": 0.015133499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models</h1>
<p>Chan Hee Song
The Ohio State University
song.1855@osu.edu
Jiaman Wu
The Ohio State University
wu.5686@osu.edu
Clayton Washington
The Ohio State University
washington.534@osu.edu
Brian M. Sadler
DEVCOM ARL
brian.m.sadler6.civ@army.mil
Wei-Lun Chao
The Ohio State University
chao.209@osu.edu
Yu Su
The Ohio State University
su.806@osu.edu</p>
<h6>Abstract</h6>
<p>This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any task successfully under the same few-shot setting. Our work opens the door for developing versatile and sample-efficient embodied agents that can quickly learn many tasks. ${ }^{1}$</p>
<h2>1. Introduction</h2>
<p>Building versatile embodied agents such as robots that can follow natural language commands to do different tasks as well as learn to do new tasks quickly has long been desired. However, contemporary language-driven agents still require a large number of labeled examples (pairs of language instructions and gold trajectories) to learn each task, which is highly costly and hinders the development of truly versatile agents [34, 29, 25, 8, 37, 17, 40, 27, 11, 2, 16]. Re-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of LLM-Planner for high-level planning. After receiving the natural language instruction ( $t=0$ ), LLM-Planner first generates a high-level plan by prompting a large language model (e.g., GPT-3). When the embodied agent gets stuck during the execution of the current plan ( $t=5$ and 20), LLM-Planner re-plans based on observations from the environment to generate a more grounded plan, which may help the agent get unstuck. The commonsense knowledge in the LLM (e.g., food is often stored in a fridge) allows it to produce plausible high-level plans and re-plan based on new environmental perception.
cently, an array of seminal work has shown the remarkable potential of large language models (LLMs) such as GPT3 [4] as a few-shot planner for embodied AI agents [1, 13, 21, 35]. Agents equipped with LLM-based planners have started to show the ability to learn a new task with a few training examples.</p>
<p>While showing great promises as proof of concepts, existing work still presents significant limitations that may prevent larger-scale applications beyond their limited eval-</p>
<p>uation setting. As an example, SayCan [1], one of the pioneering work on using LLMs for embodied instruction following, is evaluated on two environments with only 15 object types. The agent is assumed to be able to enumerate all admissible skills (i.e., [action, object] pairs) up front so it can use an LLM to rank the skills. This assumption could break easily in partially-observable environments when deploying an agent to new environments. The cost could also quickly pile up in more complex environments with more objects because the agent needs to call the LLM to evaluate every admissible skill at every step; efficiency deteriorates at the same time. Finally, most existing work [1, 35, 13, 24] uses LLMs to generate a single static plan from the language instruction and then executes on the entire plan. However, the optimal plan for the same language instruction is dependent on the environment; different environments may need different plans. There lacks a way to dynamically adjust the plan from LLMs based on environmental perception.</p>
<p>Building on existing work, we propose LLM-Planner, an LLM-based planner for embodied instruction following. An important design goal is to be able to handle a wide range of tasks in diverse, partially-observable environments, and can dynamically adjust the plan based on perceptions from the environment. Therefore, different from SayCan, we use LLMs to directly generate plans instead of ranking admissible skills, obviating the need to have sufficient knowledge about the environment a priori while also significantly reducing the number of calls to LLMs. Another unique strength of LLM-Planner is its ability to dynamically re-plan based on what the agent observes in the current environment, which produces more grounded plans.</p>
<p>More specifically, we adopt hierarchical planning models (e.g., [38, 33]), which consist of a high-level planner and a low-level planner. We use LLMs to generate high-level plans (HLPs), i.e., a sequence of subgoals (e.g., [Navigation potato, Pickup potato, Navigation microwave, ...]) that the agent needs to achieve, in the specified order, to accomplish the final goal specified by the language instruction. The lowlevel planner then maps each subgoal into a sequence of primitive actions for achieving that subgoal in the current environment and state. An important observation is that, given a high-level plan, low-level planning becomes conditionally independent of the natural language instruction. It becomes the classic object localization and navigation problem [6] (for navigation subgoals) or simply executing the specified interaction action with the right objects (for interaction subgoals). The low-level planner can be trained with data synthesized from the simulator (see, e.g., [26, 3]).</p>
<p>Furthermore, we follow the in-context learning paradigm [4, 20] and only use a small number of paired examples. In addition, no parameter update is needed, which saves development time. For the example in Fig-
ure 1, at the beginning of an episode $(t=0)$, given a natural language instruction, we directly prompt the LLM to generate the HLP by giving it several exemplar pairs of (instruction, HLP) in its context. We also leverage established techniques such as dynamic in-context example retrieval [28, 31, 9, 19] and logit biases [10] to further improve the in-context learning performance.</p>
<p>While the HLPs generated by LLMs are already plausible at first glance, they still lack a fundamental aspect of embodied agents - physical grounding; i.e., the generated HLP needs to be grounded to the environment the agent is in. Previous approaches [1, 35, 13] train a separate model that translates the LLM plans to the grounded admissible actions. However, this is possible under the assumption that the LLM plan can be matched to a reasonable admissible action. If the LLM plans are not contained in the list of admissible action, which is the case in the diverse environments, this creates an undetermined behavior for those agents. To overcome this problem, we propose a novel grounded replanning algorithm to empower LLM-Planner with physical grounding. Specifically, as an agent is executing the initial HLP, whenever it has taken too many steps to reach the current subgoal or has made too many failed attempts, we dynamically prompt the LLM again to generate a new continuation of the partial HLP that has been completed at that point. For grounding, we add the list of objects perceived in the environment so far into the prompt as a simple but effective description of the current environment. Figure 1 demonstrates how our grounded re-planning algorithm can help the agent overcome a plan that is unattainable. For the example at $t=5$, the agent is taking too long to find a potato. It re-prompts the LLM with the object fridge observed in the environment, and LLM-Planner generates a new HLP from scratch (because no subgoal has been completed so far) that directs the agent to look for a potato in the fridge. By introducing a way to incorporate feedback from the environment, we aim to create a closed-loop between the environment and the LLMs where LLMs can dynamically adapt the generated high-level plans to the environment.</p>
<p>While most existing work [1, 14, 13, 35, 24] is evaluated under a limited setting (e.g., limited/known environments, short-horizon tasks, or simple environments with a small number of objects), we evaluate LLM-Planner on ALFRED [34], a large-scale dataset with diverse partiallyobservable environments and a wide variety of tasks and objects. We test our LLM-Planner by integrating it with the perception module and low-level planner from a strong baseline model, HLSM [3]. Using less than $0.5 \%$ of paired training data, LLM-Planner achieves competitive performance compared with HLSM and outperforms multiple other baselines, which are trained with the full training set. Under the same few-shot setting, existing methods can barely complete any task successfully. Our work opens a</p>
<p>new door for developing versatile and extremely sampleefficient embodied agents by harnessing the power of large language models and grounding.</p>
<h2>2. Related Work</h2>
<h3>2.1. Vision-and-language Navigation</h3>
<p>In navigation-only VLN datasets such as R2R [2], models that generate the action sequence end-to-end with a Transformer model can already achieve a good performance [37, 27]. Recent work [17, 23, 25, 11] employs BERT and its variants [7, 22] to get better language understanding. These models jointly learn the linguistic and visual representations with cross-attention for grounding.</p>
<p>However, in more complex VLN, or embodied instruction following in datasets such as ALFRED [34], hierarchical planning models [3, 26, 18] that separate the high-level and low-level planning have proven to be most effective. These models use pretrained language models (e.g. BERT) to generate high-level plans and construct a semantic map to guide the agent to find the target objects specified in the high-level plan.</p>
<p>Recent work has shown that hierarchical planning models are advantageous in the low-data regime. (SL) ${ }^{3}$ [33] uses $10 \%$ of ALFRED's training data to learn how to generate natural language subtasks and then match primitive actions to each subtask. We take this modular approach one step further and propose to use large language models (LLMs) under the few-shot setting. More discussion of (SL) ${ }^{3}$ is in the supplementary materials.</p>
<h3>2.2. Prompting for VLN</h3>
<p>The use of LLMs for decision making has become an increasingly popular topic for research. Two major branches of LLM usages among existing works are 1) using the LLM as an auxiliary helper or 2) using the LLM as a planner. We categorize each work into these categories and outline the difference between those works and ours.
LLM as an Auxiliary Helper This branch of work uses LLM as an auxiliary helper to generate relevant information to help the main model. LM-Nav [32] prompts LLMs with raw navigation instructions and 3 in-context examples to generate a list of landmarks for a vision-language model to infer a joint probability distribution over landmarks and images. However, we show that LLM can be used for more than an auxiliary information generator and can be used to perform planning while being grounded to the environment. LLM as a Planner This branch of LLM usage focuses on the LLM's ability to generate a plan that is executable in the environment directly or indirectly by using a low-level planner. Several studies have explored the usage of LLM as a planner for embodied agents [1, 24, 13, 41, 12, 35]. Majority of the works assume the availability of admissible ac-
tions in the environment and formulate the approach based on that assumption. Some are due to the underlying evaluation setup [13, 35, 24], while others try to train a model to predict a list of admissible actions in the environment [1]. However, such an assumption leads to various implications on practicality: 1) This admissible action list may be hard or infeasible to obtain, especially in partially-observable environments, and 2) the length of the list grows combinatorially w.r.t. environment complexity (e.g., # of objects). In contrast, LLM-Planner is a generative model. It generates the high-level plan without assuming the knowledge of specifics of the current environment, and dynamically refines the plan based on new observations. To validate our claim, we implement one of the major works, SayCan [1] to our evaluation dataset (ALFRED) and compare the difference in section 5.</p>
<p>Other work [41] that does not evaluate under that assumption uses LLM as a static generator for high-level plans. However, we take one step further and propose a LLM-Planner without the aforementioned assumptions. LLM-Planner is able to ground the LLM to the current environment by using a pre-trained vision model. Next, it can directly predict HLP without relying on a list of admissible actions in the current environment. Additionally, LLMPlanner can perform the aforementioned capabilities while re-planning during the task execution to dynamically adapt the high-level plans to the current environment. At last, LLM-Planner is evaluated on a diverse set of tasks in the ALFRED environment, testing the real-life applicability of our approach. With careful prompt design and other techniques for better in-context learning, we show that LLMPlanner can generate complete and high-quality high-level plans that are grounded in the current environment with a fraction of labeled data.</p>
<h2>3. Preliminaries</h2>
<p>Vision-and-Language Navigation. Embodied instruction following is often also referred as vision-and-language navigation (VLN), though it additionally involves interaction actions and usually features a much longer time horizon than typical VLN tasks (e.g., Room2Room [2]). To be consistent with the literature, we will use these two terms interchangeably. We will primarily focus on the standard ALFRED [34] dataset, which is built on top of the AI2Thor [15] simulator, but our method can easily generalize to other datasets and environment. We choose ALFRED mainly considering its diversity in task types ( 7 different task types) and long-horizon tasks (on average 50 actions per task).</p>
<p>The VLN task is defined as following: Given a language instruction $I$, an agent needs to predict and carry out a sequence of primitive actions in the environment $E$ to accom-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of LLM-Planner with prompt design and grounded re-planning.</p>
<p>Plink the task. In datasets like ALFRED [34], the instruction <em>I</em> consists of a high-level goal <em>I<sub>H</sub></em> and (optionally) a list of step-by-step instructions <em>I<sub>L</sub></em>. A VLN task can thus be represented by a tuple (<em>I</em>, <em>E</em>, <em>G</em>), where <em>G</em> is the goal test. We consider hierarchical planning models [38] for VLN, which is explored to various extent in several recent studies [26, 3, 33, 36], but none of them considers the few-shot setting or LLMs for planning. In this formulation, planning is modeled in a hierarchical fashion. The high-level planner maps the instruction <em>I</em> into a high-level plan (HLP) <em>L<sub>h</sub></em> = [<em>g<sub>0</sub></em>, <em>g<sub>1</sub></em>, ..., <em>g<sub>T</sub></em>, where each subgoal <em>g<sub>i</sub></em> is specified as (high-level action, object). We define a high-level action to be a collection of primitive actions that can complete a single goal-condition in ALFRED [34]. We take the interaction actions directly from ALFRED and we only add the Navigation action. Therefore, the high-level action space consists of 1 navigation action (Navigation) and 7 interaction actions (PickupObject, PutObject, OpenObject, CloseObject, ToggleOnObject, ToggleOffObject, SliceObject). Similar actions are commonly used in other related work such as SayCan [1] and LM zero-shot planner [13].</p>
<p>The low-level planner maps each subgoal into a sequence of primitive actions <em>L<sub>l</sub></em> = [<em>a<sub>0</sub></em>, <em>a<sub>1</sub></em>, ..., <em>a<sub>T<sub>l</sub></sub></em>]. State-of-the-art VLN methods [26, 3] use a map-based low-level planner and a simple path-finding algorithm to find the target object in the current subgoal from the map. It is important to note that, once the high-level plan <em>L<sub>h</sub></em> is specified, the low-level planning becomes independent of the instruction <em>I</em>. More formally, <em>P(L<sub>l</sub> | I, L<sub>h</sub>, E) = P(L<sub>l</sub> | L<sub>h</sub>, E)</em>. All the components involved in the low-level planner are either deterministic or trained using synthetic data from the simulator. No paired data involving language instructions is needed.</p>
<h3>In-Context Learning/Prompting</h3>
<p>Recently, in-context learning (also known as prompting) [4] has drawn great attention with the rise of LLMs. By designing different prompts, LLMs can be adapted to different downstream tasks with a few examples as demonstration without updating any of the parameters. In this work, we explore in-context learning with LLMs for embodied agent planning.</p>
<h3>True Few-Shot Setting</h3>
<p>While only using a small number of training examples, many few-shot studies use a large validation set for prompt design and model selection [4]. Recent studies [28] have shown that such large validation sets are responsible for overestimation of the efficacy of language models because they create a strong bias for model selection and violate the intended few-shot setting. To avoid such bias, we adhere to the true few-shot setting [28] in which prompt design and model selection is conducted via cross-validation on the same small training set instead of using a separate validation set.</p>
<h1>4. LLM-Planner</h1>
<p>In this section, we describe our method, LLM-Planner, which leverages LLMs such as GPT-3 (TEXT-DAVINCI-003) to do few-shot grounded high-level planning for embodied agents.</p>
<h2>4.1. Overview</h2>
<p>LLMs such as GPT-3 are pre-trained to generate natural language. To adapt them as high-level planners, the first step is to design an appropriate prompt to guide them to generate high-level plans. We discuss our prompt design in Section 4.2. The choice of in-context examples is critical for the performance of LLMs, and recent works [28, 9] have shown that dynamically retrieving similar examples for each test example is beneficial. We adopt a k-nearest-neighbor (kNN) retriever to select the in-context examples (Section 4.3). We also use logit biases [10] to further constrain the output space of the LLM to the allowed set of actions and objects. With all the above designs, we have</p>
<p>obtained the static version of LLM-Planner, which can already generate reasonable HLPs. In Section 4.4, we propose a novel grounded re-planning algorithm to enhance LLMs with the ability to ground to the current environment, which further improves the HLP quality. Finally, we discuss how to integrate LLM-Planner into existing embodied agents to empower them with few-shot planning capabilities in Section 4.5. An overview of LLM-Planner is shown in Figure 2.</p>
<h3>4.2. Prompt Design</h3>
<p>While GPT-3 is shown to be a powerful few-shot learner in a variety of tasks, its power can only be unleashed with carefully designed prompts that are tailored for the desired behavior. The final HLP quality can be sensitive to minor design choices in the prompt (e.g., how the HLP is presented, or sometimes even the choice of punctuation). Therefore, we identify core components of the prompt and systemically compare different design choices under the true few-shot setting based on leave-one-out crossvalidation (LOOCV). The evaluations for some of the key design choices are discussed in Section 5.5 and 5.6.</p>
<p>Our final optimal prompt is shown in Figure 2. The prompt begins with an intuitive explanation of the task and the list of allowable high-level actions. It is then followed by the in-context examples selected by the kNN retriever (Section 4.3). When we provide only the high-level goal instruction to GPT-3, we use the format "Task description: [high-level goal instruction]." When we include the step-by-step instructions, we include another line "Step-bystep instructions: [step-by-step instructions]" following the goal instruction. For dynamic grounded re-planning (Section 4.4), we add the subgoals that have been completed and the list of objects observed so far in the environment after the task description. Finally, we append the test example in the same format that ends with "Next plan:".</p>
<h3>4.3. In-context Example Retrieval</h3>
<p>The in-context examples are an important source of taskspecific information for the LLM. Different examples could provide different information for the current task. Intuitively, if the current task is to "cook a potato," an in-context example that demonstrates the HLP for "cooking an egg" is likely more informative than one that demonstrates how to "clean a plate." Specifically, we use a frozen BERT-base model [7] to evaluate the pairwise similarity between each training example and the current test example. The similarity of two examples is defined based on the Euclidean distance between the BERT embedding of their corresponding instruction. For each test example, we then retrieve the $K$ most similar examples from the small set of paired training examples we have, where $K$ is a hyperparameter that we tune under the true few-shot setting (Section 5.6).</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">Dynamic</span><span class="w"> </span><span class="n">Grounded</span><span class="w"> </span><span class="n">Re</span><span class="o">-</span><span class="n">planning</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">LLM</span><span class="o">-</span>
<span class="n">Planner</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">I</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Instruction</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">O</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Set</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">observed</span><span class="w"> </span><span class="k">object</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">G</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">completed</span><span class="w"> </span><span class="n">subgoals</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">far</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">LLM</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Planner</span><span class="err">}</span><span class="p">(</span><span class="n">I</span><span class="p">,</span><span class="w"> </span><span class="n">O</span><span class="p">,</span><span class="w"> </span><span class="n">G</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Full</span><span class="w"> </span><span class="n">HLP</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nc">Time</span><span class="w"> </span><span class="n">step</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">Subgoal</span><span class="w"> </span><span class="k">index</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">First</span><span class="w"> </span><span class="n">subgoal</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="n">a_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Low</span><span class="err">}</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">Level</span><span class="err">}</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Planner</span><span class="err">}</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">First</span><span class="w"> </span><span class="k">action</span>
<span class="w">    </span><span class="k">while</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">k</span><span class="err">}</span><span class="o">&lt;</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="nf">len</span><span class="err">}</span><span class="p">(</span><span class="err">\</span><span class="n">mathrm</span><span class="err">{</span><span class="n">S</span><span class="err">}</span><span class="p">)</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">execute</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">a_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">O_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Object</span><span class="o">-</span><span class="n">Detector</span><span class="p">(</span><span class="k">current</span><span class="w"> </span><span class="n">camera</span><span class="w"> </span><span class="k">input</span><span class="p">)</span>
<span class="w">        </span><span class="n">O</span><span class="p">.</span><span class="k">insert</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nf">left</span><span class="p">(</span><span class="n">O_</span><span class="err">{</span><span class="n">t</span><span class="err">}\</span><span class="nf">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="n">subgoal</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="n">fails</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="k">after</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">n</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nc">time</span><span class="w"> </span><span class="n">steps</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">S</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">LLM</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Planner</span><span class="err">}</span><span class="p">(</span><span class="n">I</span><span class="p">,</span><span class="w"> </span><span class="n">O</span><span class="p">,</span><span class="w"> </span><span class="n">G</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">New</span><span class="w"> </span><span class="n">HLP</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="mi">0</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="k">current</span><span class="w"> </span><span class="n">subgoal</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">completed</span><span class="w"> </span><span class="k">then</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">k</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="n">s</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">S</span><span class="o">[</span><span class="n">k</span><span class="o">]</span><span class="w"> </span><span class="err">\</span><span class="n">quad</span><span class="w"> </span><span class="err">\</span><span class="n">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Get</span><span class="w"> </span><span class="k">next</span><span class="w"> </span><span class="n">subgoal</span>
<span class="w">        </span><span class="k">end</span><span class="w"> </span><span class="k">if</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="n">a_</span><span class="err">{</span><span class="n">t</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Low</span><span class="err">}</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="k">Level</span><span class="err">}</span><span class="o">-</span><span class="err">\</span><span class="n">operatorname</span><span class="err">{</span><span class="n">Planner</span><span class="err">}</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">end</span><span class="w"> </span><span class="k">while</span>
</code></pre></div>

<h3>4.4. Grounded Re-planning</h3>
<p>Using LLM-Planner as a static high-level planner that only predicts an HLP at the beginning of a task already shows good data efficiency and accuracy. As discussed earlier, however, such static planning lacks grounding to the physical environment and can lead to incorrect objects and unattainable plans (Figure 1). When such issues happen, the agent cannot complete the current subgoal specified in the HLP, which will lead to one of two possible situations: 1) it fails to execute an action (e.g., bumping into a wall or failing to interact with an object), or 2) it takes a long time and still has not completed the current subgoal (e.g., wandering endlessly). Intuitively, knowing the objects in the current environment can be very helpful for addressing both of these issues. For example, knowing that there is a fridge, the LLM may produce an HLP that directs the agent to go to the fridge and try to find a potato in that, because it may have learned the commonsense knowledge that food is likely stored in a fridge during language model pre-training.</p>
<p>To this end, we present a simple but effective way to enhance LLMs with physical grounding by injecting a list of observed objects, which may be detected using the object detector of the embodied agent, from the environment into the prompt (Figure 2). We also add logit biases to these observed objects so LLM-Planner can prioritize producing a plan with those objects if they are relevant for the task.</p>
<p>Based on that, we propose a grounded re-planning algorithm (Algorithm 1) to dynamically update the HLP during</p>
<p>the course of completing a task. This is in contrast with most existing work that adopts a similar hierarchical planning model (e.g., [26]), which only predicts a fixed HLP up front and sticks to that no matter what happens during the execution. In our algorithm, re-planning will be triggered under either of two conditions: 1) the agent fails to execute an action, or 2) after a fixed number of time steps. A new continuation of the already-completed partial HLP will be generated by LLM-Planner based on the observed objects, and the agent will carry on with the new plan, which may help it get unstuck.</p>
<h3>4.5. Integration with Existing VLN models</h3>
<p>We now discuss how to integrate LLM-Planner with the existing models to empower them with the few-shot planning capability. LLM-Planner provides a fairly generic and flexible interface for integration. As shown in Algorithm 1, it only needs the embodied agent to provide an object list and has a low-level planner that can turn the predicted HLP into low-level actions. It has no assumption about the inner working of the agent. For evaluating the end-to-end task completion performance of LLM-Planner, we integrate it with a strong baseline method, HLSM [3], which satisfies such an interface.</p>
<h2>5. Experiments</h2>
<h3>5.1. Dataset</h3>
<p>We evaluate the efficacy of LLM-Planner in generating high-level plans using the ALFRED [34] benchmark, a vision-and-language navigation dataset that requires embodied agents to follow instructions and use visual input to complete tasks in a simulated, spatially continuous household environment. The dataset consists of 7 task types spanning across 207 unique environments, 115 different object types, and 4,703 tasks. The task ranges in difficulty from moving a single object to a new location to placing a heated slice of an object into a receptacle. Each task is accompanied by human-written annotations of a high-level goal and a series of more granular step-by-step instructions, created by human annotators as they watched expert demonstrations of the tasks. Due to the noise in the natural language instructions and the complexity of planning required to complete such long-horizon tasks, ALFRED is a challenging test of an embodied agent's ability to produce robust and accurate plans.</p>
<h3>5.2. Metrics</h3>
<p>We report two main metrics used by ALFRED and one metric created by us to calculate the high-level planning accuracy. Success rate (SR) is the percentage of tasks fully completed by the agent. A task is only considered complete when all the subgoals are completed. Goal-condition success rate (GC) is the percentage of completed goalconditions. Goal-conditions are defined as state changes necessary to complete the task. For example, in the task "Slice a heated bread," bread being sliced and bread being heated are both goal-conditions.</p>
<p>To directly evaluate high-level planning, we introduce a new metric named high-level planning accuracy (HLP ACC), i.e., the accuracy of the predicted HLP compared to the ground-truth HLP. For the static planning setting, we compare the generated HLP with the ground-truth HLP and deems a plan as incorrect if it does not perfectly match the ground truth, and correct otherwise. For the dynamic planning setting, we report a range because we cannot fully separate LLM-Planner's performance with the low-level controller choice because we do not have access to an oracle low-level controller. The lower bound is the HLP accuracy of the full generated plan regardless of whether it was executed successfully by the low-level controller (i.e. same as evaluating static HLP). The upper bound is the HLP accuracy of the predicted HLP that was successfully executed in the environment by the low-level controller when a task has ended (i.e. a task success or a catastrophic failure).</p>
<h3>5.3. Implementation Details</h3>
<p>We choose 100 examples for our LLM-Planner among 21,023 ALFRED training examples. We apply random stratified sampling to ensure we have a fair representation of all 7 task types in the 100 -example set. For the kNN retriever, we use the pretrained BERT-base-uncased model from the Huggingface Transformers Library [39]. For the LLM, we use the public GPT-3 [4] API with 9 in-context examples chosen from the 100 training examples by the kNN retriever. We set the temperature to 0 and apply a logit bias of 0.1 to all allowable output tokens. The object list for grounded re-planning is retrieved from the object detector. Specifically, we use the pretrained object detector from HLSM's perception model. We only include objects with a label confidence more than $80 \%$ to reduce noise. It is worth noting that we can potentially use any object detector to obtain the object list, and we only use HLSM's perception model to save computation cost and time. To avoid violating our few-shot assumption, we use the pretrained navigation, perception, and depth model from HLSM which are trained using only synthesized trajectories from the simulator, without any paired training data involving natural language instructions or human annotations.</p>
<p>We compare with two main baseline models, HLSM [3] and FILM [26] They are also hierarchical planning models and achieve strong performance on the ALFRED leaderboard. We directly replace the trained high-level planner for both models with our LLM-Planner and did not modify any other parts. In addition, we re-train these models to compare with LLM-Planner under the same few-shot shot</p>
<p>setting. We also compare with several other published baselines models that are trained with the full data. Additionally, we also implement SayCan to ALFRED and compare under the same few-shot setting as LLM-Planner. Further implementation details can be found in the supplementary.</p>
<p>SayCan [1] is a ranking based high-level planner that requires a list of admissible actions and ranks them using the LLM. To make it possible for SayCan to work in the complex, partially-observable environments in ALFRED, we give it an unfair competitive advantage-it knows all the objects and affordances in the current environment a priori to compile the list of skills. We also equip SayCan with the same kNN retriever from LLM-Planner, which was not needed in their original paper because of the less diverse tasks. More details on the implementation is provided in the supplementary materials.</p>
<p>Other Baselines. For other baselines included in Table 1, we retrieve the results directly from the published version of the corresponding paper. If the ALFRED leaderboard entry is better than the numbers in the original paper, we report the higher.</p>
<h3>5.4. Main Results</h3>
<p>The main results are shown in Table 1. We first compare the performance of HLSM when using our LLM-Planner as the high-level planner compared with its native version, which is trained using the full training set of ALFRED. We find that LLM-Planner's few-shot performance is competitive to the original HLSM, and outperforms several recent baselines such as E.T., HiTUT, and M-TRACK, despite using less than $0.5$ of paired training data. On the other hand, when trained using the same 100 examples (i.e., retraining HLSM's high-level planner), HLSM (and FILM as well) can barely complete any task successfully. Furthermore, the results show that SayCan still largely underperforms LLM-Planner despite the access to the full environment information. Another significant difference is cost and efficiency. Because of SayCan's ranking nature, it needs to call the LLM many more times than a generative model like LLM-Planner: LLM-Planner calls GPT-3 avg. 7 times per task and SayCan calls it 22 times, even with oracle knowledge of the current environment to shrink the skill list. Lastly, we see a considerable improvement from grounded re-planning over static planning, especially in the goal instruction only setting, where it improves 1.83% SR in the unseen test split. This confirms the effectiveness of the grounded re-planning. But we also note that there is still a large room for further improvement.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: LOOCV HLP accuracy for varying number of in-context examples and training examples.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Case studies for LLM-Planner.</p>
<h3>5.5. Ablation Studies</h3>
<p>We conduct an ablation study on different components of LLM-Planner to validate their effectiveness. We follow the LOOCV process and use only the high-level planning accuracy to determine our choices. Results from this study are in Table 2. We first ablate the kNN retriever module, by replacing it with a retriever that randomly selects in-context examples from the 100 example set. Results in Table 2 show that this leads to a significant drop in performance, confirming the necessity of dynamic retrieval.</p>
<p>Furthermore, we find that enabling logit biases to favor objects that appear in the environment lead to a decent boost in the high-level planning accuracy. Having LLM-Planner favor objects that appear in the environment makes it more robust in the cases where the instruction is ambiguous or objects are referred with different names. For example, for an instruction "Turn on the lamp," different types of lamps, e.g., table lamps or floor lamps, could be. By enabling logit biases to favor objects that appear in the environment (e.g., TableLamp), we can correctly guide LLM-Planner to output (TurnOnObject, TableLamp). Another example is when the instruction refers to RecycleBin but the object name used in the environment is GarbageCan. In this case, using logit biases can correctly guide LLM-Planner to output the relevant and correct objects.</p>
<h3>5.6. Fine-grained Analyses</h3>
<p>Effect of Number of Examples. For the main experiments, we chose 100 as the number of training examples without</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Test Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Unseen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Valid Seen</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">HLP ACC</td>
<td style="text-align: center;">SR</td>
<td style="text-align: center;">GC</td>
<td style="text-align: center;">HLP ACC</td>
</tr>
<tr>
<td style="text-align: center;">Full-data setting: 21,023 (instruction, trajectory) pairs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Goal instruction only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HiTUT [40]</td>
<td style="text-align: center;">11.12</td>
<td style="text-align: center;">17.89</td>
<td style="text-align: center;">13.63</td>
<td style="text-align: center;">21.11</td>
<td style="text-align: center;">10.23</td>
<td style="text-align: center;">20.71</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">25.27</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">HLSM [3]</td>
<td style="text-align: center;">20.27</td>
<td style="text-align: center;">27.24</td>
<td style="text-align: center;">25.11</td>
<td style="text-align: center;">35.79</td>
<td style="text-align: center;">18.28</td>
<td style="text-align: center;">31.24</td>
<td style="text-align: center;">$31.24-\mathbf{7 0 . 1 7}$</td>
<td style="text-align: center;">29.63</td>
<td style="text-align: center;">38.74</td>
<td style="text-align: center;">$38.74-\mathbf{7 7 . 6 4}$</td>
</tr>
<tr>
<td style="text-align: center;">Step-by-step instructions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">E.T. [27]</td>
<td style="text-align: center;">8.57</td>
<td style="text-align: center;">18.56</td>
<td style="text-align: center;">38.42</td>
<td style="text-align: center;">45.44</td>
<td style="text-align: center;">7.32</td>
<td style="text-align: center;">20.87</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">46.59</td>
<td style="text-align: center;">52.92</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">HiTUT [40]</td>
<td style="text-align: center;">13.87</td>
<td style="text-align: center;">20.31</td>
<td style="text-align: center;">21.27</td>
<td style="text-align: center;">29.97</td>
<td style="text-align: center;">12.44</td>
<td style="text-align: center;">23.71</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">25.24</td>
<td style="text-align: center;">34.85</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">M-TRACK [36]</td>
<td style="text-align: center;">16.29</td>
<td style="text-align: center;">22.60</td>
<td style="text-align: center;">24.79</td>
<td style="text-align: center;">33.35</td>
<td style="text-align: center;">17.29</td>
<td style="text-align: center;">28.98</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">26.70</td>
<td style="text-align: center;">33.21</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">FILM [26]</td>
<td style="text-align: center;">27.80</td>
<td style="text-align: center;">38.52</td>
<td style="text-align: center;">28.83</td>
<td style="text-align: center;">39.55</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">54.93</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">60.86</td>
</tr>
<tr>
<td style="text-align: center;">LEBP [18]</td>
<td style="text-align: center;">28.30</td>
<td style="text-align: center;">36.79</td>
<td style="text-align: center;">28.97</td>
<td style="text-align: center;">36.33</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">$-$</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot setting: 100 (instruction, high-level plan) pairs</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Goal instruction only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LLM-Planner (Static) + HLSM</td>
<td style="text-align: center;">11.58</td>
<td style="text-align: center;">18.47</td>
<td style="text-align: center;">13.05</td>
<td style="text-align: center;">20.58</td>
<td style="text-align: center;">11.10</td>
<td style="text-align: center;">22.44</td>
<td style="text-align: center;">28.67</td>
<td style="text-align: center;">11.82</td>
<td style="text-align: center;">23.54</td>
<td style="text-align: center;">27.45</td>
</tr>
<tr>
<td style="text-align: center;">LLM-Planner + HLSM</td>
<td style="text-align: center;">13.41</td>
<td style="text-align: center;">22.89</td>
<td style="text-align: center;">15.33</td>
<td style="text-align: center;">24.57</td>
<td style="text-align: center;">12.92</td>
<td style="text-align: center;">25.35</td>
<td style="text-align: center;">$33.81-55.85$</td>
<td style="text-align: center;">13.53</td>
<td style="text-align: center;">28.28</td>
<td style="text-align: center;">$35.08-54.33$</td>
</tr>
<tr>
<td style="text-align: center;">Step-by-step instructions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">HLSM [3]</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">3.72</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.86</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">2.82</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">FILM [26]</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">6.71</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">4.23</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">9.65</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">13.19</td>
<td style="text-align: center;">0.00</td>
</tr>
<tr>
<td style="text-align: center;">SayCan [1]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.88</td>
<td style="text-align: center;">22.54</td>
<td style="text-align: center;">37.57</td>
<td style="text-align: center;">12.30</td>
<td style="text-align: center;">24.52</td>
<td style="text-align: center;">35.15</td>
</tr>
<tr>
<td style="text-align: center;">LLM-Planner (Static) + HLSM</td>
<td style="text-align: center;">15.83</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">17.87</td>
<td style="text-align: center;">23.10</td>
<td style="text-align: center;">14.26</td>
<td style="text-align: center;">26.12</td>
<td style="text-align: center;">43.24</td>
<td style="text-align: center;">15.84</td>
<td style="text-align: center;">25.43</td>
<td style="text-align: center;">39.87</td>
</tr>
<tr>
<td style="text-align: center;">LLM-Planner + HLSM</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">23.37</td>
<td style="text-align: center;">18.20</td>
<td style="text-align: center;">26.77</td>
<td style="text-align: center;">15.36</td>
<td style="text-align: center;">29.88</td>
<td style="text-align: center;">$46.59-\mathbf{6 8 . 3 1}$</td>
<td style="text-align: center;">16.45</td>
<td style="text-align: center;">30.11</td>
<td style="text-align: center;">$50.33-\mathbf{7 1 . 8 4}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Main results on the ALFRED dataset. "(Static)" means the static planning setting, otherwise it is the default dynamic setting with grounded re-planning. Some methods support using only the goal instruction or additionally using the step-by-step instructions. We compare under both configurations. We could not evaluate SayCan on the test split because ALFRED prohibits using the test metadata, which is needed by SayCan for compiling the admissible actions.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">LOOCV HLP accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Best Model</td>
<td style="text-align: center;">40.59</td>
</tr>
<tr>
<td style="text-align: left;">- kNN Retriever</td>
<td style="text-align: center;">17.48</td>
</tr>
<tr>
<td style="text-align: left;">- Logit Biases</td>
<td style="text-align: center;">38.10</td>
</tr>
<tr>
<td style="text-align: left;">- Both</td>
<td style="text-align: center;">13.43</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation of LLM-Planner's components.
any cross-validation because it is our target number for the few-shot setting. We then use LOOCV to select the best number of in-context examples using the 100 sampled training examples. However, we are still curious about the effect of different choices, so we conduct this analysis after the main experiments to show the sensitivity to these hyperparameters. It is worth noting that the design choices for the main experiments are not informed by this analysis, to respect the true few-shot setting.</p>
<p>As shown in Figure 3, HLP accuracy generally improves with more training examples, though we start to get a diminishing return around 250 training examples. A decent improvement can be expected for the main experiments in Table 1 if we choose to use more training examples (e.g., 250). Furthermore, we find that 9 is generally a good number for
in-context examples. Although adding more in-context examples could still improve the performance slightly, it may not be meaningful enough to justify the additional cost. Not too surprisingly, more in-context examples is more beneficial when there is less training examples, because there are less useful examples to retrieve from.
Case Studies. In Figure 4, we show two examples where LLM-Planner helps with object localization and disambiguation through grounded re-planning. For the first case, even using only the high-level goal instruction, LLMPlanner correctly predicts that the cup is likely located in the cabinet after failing to find a cup but observing a cabinet in the environment. This shows LLM-Planner can achieve a similar effect to what the semantic map tries to achieve in FILM [26], i.e., predicting plausible location for target objects. For the second case, we show that LLM-Planner can correctly ground the word "lamp" to the desklamp in the environment.</p>
<h2>6. Conclusion</h2>
<p>We demonstrate a novel high-level planner based on large language models for embodied agents that can be used</p>
<p>in diverse, partially-observable, and complex environments. It can also dynamically re-plan based on environmental perception to produce more grounded plans. Our work can dramatically reduce the amount of human annotations needed for learning the instruction following task. Furthermore, it opens a new door for developing versatile and extremely sample-efficient embodied agents by harnessing the power of large language models and enhancing them with physical grounding. Promising future directions include exploring other LLMs such as Codex [5], better prompt design, and more advanced methods for grounding and dynamic replanning.</p>
<h2>Acknowledgement</h2>
<p>The authors would like to thank the colleagues from the OSU NLP group for their thoughtful comments. This research was supported by ARL W911NF2220144.</p>
<h2>References</h2>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022. 1, 2, 3, 4, 7, 8, 12
[2] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, pages 3674-3683, 2018. 1, 3
[3] Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706-717. PMLR, 2022. 2, 3, $4,6,8,12,14$
[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Infor-
mation Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. 1, 2, 4, 6
[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. 9
[6] G.N. Desouza and A.C. Kak. Vision for mobile robot navigation: a survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(2):237-267, 2002. 2
[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 41714186. Association for Computational Linguistics, 2019. 3, 5, 12
[8] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Neural Information Processing Systems (NeurIPS), 2018. 1
[9] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pretrained language models better few-shot learners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816-3830, Online, Aug. 2021. Association for Computational Linguistics. 2, 4
[10] Bernal Jiménez Gutiérrez, Nikolas McNeal, Clay Washington, You Chen, Lang Li, Huan Sun, and Yu Su. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:2203.08410, 2022. 2, 4
[11] Yicong Hong, Qi Wu, Yuankai Qi, Cristian RodriguezOpazo, and Stephen Gould. A recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1643-1653, June 2021. 1, 3
[12] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation. arXiv preprint arXiv:2210.05714, 2022. 3</p>
<p>[13] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022. 1, 2, 3, 4
[14] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In arXiv preprint arXiv:2207.05608, 2022. 2
[15] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017. 3
[16] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Conference on Empirical Methods for Natural Language Processing (EMNLP), 2020. 1
[17] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah A. Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1494-1499, Hong Kong, China, Nov. 2019. Association for Computational Linguistics. 1, 3
[18] Hao Liu, Yang Liu, Hong He, and Hang Yang. Lebp - language expectation \&amp; binding policy: A two-stream framework for embodied vision-and-language interaction task learning agents. ArXiv, abs/2203.04637, 2022. 3, 8
[19] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good incontext examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online, May 2022. Association for Computational Linguistics. 2
[20] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., sep 2022. 2
[21] Xiaotian Liu, Hector Palacios, and Christian Muise. A planning based neural-symbolic approach for embodied instruction following. Interactions, 9(8):17, 2022. 1
[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. 3
[23] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. 3
[24] Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. Neuro-
symbolic procedural planning with commonsense prompting, 2022. 2, 3
[25] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VI, pages 259-274, 2020. 1, 3
[26] So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, and Ruslan Salakhutdinov. FILM: Following instructions in language with modular methods. In International Conference on Learning Representations, 2022. 2, 3, 4, 6, 8, 12
[27] Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic Transformer for Vision-and-Language Navigation. In ICCV, 2021. 1, 3, 8
[28] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True fewshot learning with language models. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. 2, 4
[29] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8494-8502, 2018. 1
[30] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. 13
[31] Timo Schick and Hinrich Schütze. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2339-2352, 2021. 2
[32] Dhruv Shah, Błażej Osiński, Sergey Levine, et al. Robotic navigation with large pre-trained models of language, vision, and action. In 6th Annual Conference on Robot Learning, 2022. 3
[33] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with latent language. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1713-1726, Dublin, Ireland, May 2022. Association for Computational Linguistics. 2, 3, 4, 13
[34] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2, 3, 4, 6
[35] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. ProgPrompt: Generating situated robot task plans using large language models. 2022. 1, 2, 3
[36] Chan Hee Song, Jihyung Kil, Tai-Yu Pan, Brian M. Sadler, Wei-Lun Chao, and Yu Su. One step at a time: Long-horizon</p>
<p>vision-and-language navigation with milestones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15482-15491, June 2022. 4, 8
[37] Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, and Gaurav Sukhatme. Embodied bert: A transformer model for embodied, language-guided visual task completion, 2021. 1, 3
[38] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999. 2, 4
[39] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online, Oct. 2020. Association for Computational Linguistics. 6
[40] Yichi Zhang and Joyce Chai. Hierarchical task learning from language instructions with unified transformers and self-monitoring. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4202-4213, Online, Aug. 2021. Association for Computational Linguistics. 1,8
[41] Kai Zheng, KAI-QING Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Li, Xuehai He, and Xin Eric Wang. Jarvis: A neurosymbolic commonsense reasoning framework for conversational embodied agents. ArXiv, abs/2208.13266, 2022. 3</p>
<h2>Appendices</h2>
<p>In this supplementary material, we present additional details and clarifications that are omitted in the main text due to space constraints.</p>
<ul>
<li>Appendix A: Additional Model Implementation Details</li>
<li>Appendix B: Comparison with (SL) ${ }^{3}$ on ALFRED.</li>
<li>Appendix C: Prompt design choices and prompt selection under true few-shot setting (cf. section 4.2 in the main paper).</li>
<li>Appendix D: Additional fine-grained analyses (cf. section 5 in the main paper).</li>
</ul>
<h2>A. Additional Model Implementation Details</h2>
<h2>A.1. HLSM</h2>
<p>HLSM [3] consists of three components: a semantic voxel map, a high-level planner, and a low-level planner. First, a 3D semantic voxel map is constructed by applying semantic segmentation and depth estimation to the visual inputs, which stores the agent's and the objects' real-time locations. Next, the high-level planner takes the language instructions, the semantic map encoding, and the previous subgoal history to predict the next subgoal. Lastly, the lowlevel planner is a mixture of deterministic algorithms and learned components (e.g., learning a yaw and pitch angle to face the object). HLSM first processes the sensory image input to create/update a map, which is used as an input to the high-level planner along with the language instructions to predict the next subgoal. Finally, the low-level planner maps the subgoal into a sequence of primitive actions.</p>
<p>To adapt HLSM to the few-shot setting, we need to re-train the components of the model that need paired trajectory-instruction data for training. For HLSM, paired data was only used for training the high-level controller. Therefore, we re-train the high-level controller with the same 100 training examples we use for LLM-Planner. Specifically, we use the same set of hyperparameters as HLSM. While the original HLSM focuses on the goal instruction only setting, we found that the step-by-step instructions are essential for the few-shot setting, so we concatenate goal instruction with step-by-step instructions for re-training HLSM's high-level planner. We leave the other components intact, which are downloaded from the official codebase. ${ }^{3}$</p>
<h2>A.2. FILM</h2>
<p>FILM [26] consists of four components: a semantic map, a semantic search policy, a template-based high-level plan-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ner, and a low-level planner. At the beginning of each task, five separate BERT-based classifiers [7] are used to predict five parameters (task type, target objects, receptacles, parent objects, and whether slicing is needed), each of which takes the goal and optionally the step-by-step instructions as input to predict the respective parameter. FILM then generates the high-level plan by choosing a pre-defined template based on the predicted task type and filling the other parameters into the template. In addition, the semantic map is updated at each time step with the sensory image inputs. At every 25 steps, the semantic search policy predicts the coordinates of the target object on the semantic map, which are then used by a deterministic low-level planner to decide on a low-level plan to navigate from the current location to the target object's location.</p>
<p>Only the BERT-based classifiers need the languagerelated data for training. Therefore, to adapt FILM to the few-shot setting, the five BERT-based classifiers are retrained with the same 100 training examples used by the LLM-Planner. Similar to HLSM, we concatenate the goal and the step-by-step instructions as input to the BERT-based classifiers. We use default hyperparameters for BERT models that are found in the paper. We use the predictions from these models to generate the high-level plans with the same pre-defined templates in FILM. We leave other components intact, which are downloaded from the official codebase. ${ }^{4}$</p>
<h2>A.3. SayCan</h2>
<p>SayCan [1] consists of 3 components: an LLM ranker, set of skills, and a value function. We use the LLM ranker adapted from SayCan's codebase ${ }^{5}$ with the same settings (e.g. temperature and log probability) and use GPT-3 (text-davinci-003) as the choice of LLM. First, SayCan generates a list of skills and their affordance score in the current environment using a pre-trained value function. Then, it prompts the LLM with natural language description of each skill and generates a probability that represents how relevant it is to the task success. Finally, SayCan combines the skill's LLM probability and the affordance score to choose which skill to execute.</p>
<p>To adapt SayCan to ALFRED, we need to define a skill in the ALFRED environment. From SayCan, a skill is defined as "atomic" behaviors that are capable of low-level visuomotor control. Each skill can perform a short task, such as picking up a particular object. This is identical to our definition of high-level plan in $\S 3$, therefore we treat each skill as analogous to the (high-level action, object) pair. This formulation allows us to use the same low-level controller we used for LLM-Planner. Furthermore, the value function is an another important concept for the SayCan.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Options</th>
<th style="text-align: center;">Task <br> Introduction</th>
<th style="text-align: center;">Goal <br> Instruction</th>
<th style="text-align: center;">Step-by-step <br> Instructions</th>
<th style="text-align: center;">Plan List</th>
<th style="text-align: center;">Object List</th>
<th style="text-align: center;">Retrieval <br> Message</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Default</td>
<td style="text-align: center;">Create a high-level plan for completing a household task using the allowed actions and visible objects.</td>
<td style="text-align: center;">Task description: [goal instruction]</td>
<td style="text-align: center;">Step-by-step instructions: [instructions]</td>
<td style="text-align: center;">(Completed, Next) plan: [subgoals]</td>
<td style="text-align: center;">Visible objects are [objects]</td>
<td style="text-align: center;">Next plan:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Allowed actions are [action list]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Punctuation</td>
<td style="text-align: center;">("PickupObject") <br> (PickupObject) <br> PickupObject</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">("PickupObject", "Apple") <br> (PickupObject, Apple) <br> PickupObject, Apple</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Naturalization</td>
<td style="text-align: center;">PickupObject <br> Pickup <br> Pick up</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PickupObject <br> Pickup <br> Pick up</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Delimiter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pick up, go to <br> Pick up. Go to. <br> Pick up $\backslash \mathrm{n}$ Go to</td>
<td style="text-align: center;">Pickup, Navigate <br> Pickup. Navigate <br> Pickup $\backslash \mathrm{n}$ Navigate</td>
<td style="text-align: center;">Apple, orange <br> Apple. orange <br> Apple $\backslash \mathrm{n}$ Orange</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: For each element in our prompt design, we list the default phrasing. For the representation of actions, objects, and lists, we additionally experiment with different choices of punctuation, naturalization, and the delimiter between elements in a list. We select the optimal prompt design using LOOCV on the 100 training examples. The chosen options are highlighted in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Type</th>
<th style="text-align: center;">HLP Accuracy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Valid Unseen</td>
<td style="text-align: center;">Valid Seen</td>
</tr>
<tr>
<td style="text-align: left;">Pick \&amp; Place</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: left;">Stack \&amp; Place</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: left;">Place Two</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">45</td>
</tr>
<tr>
<td style="text-align: left;">Examine</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: left;">Heat \&amp; Place</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">Cool \&amp; Place</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: left;">Clean \&amp; Place</td>
<td style="text-align: center;">48.8</td>
<td style="text-align: center;">32</td>
</tr>
</tbody>
</table>
<p>Table 4: Static LLM-Planner's high-level planning accuracy breakdown by task type.</p>
<p>The value function predicts how likely an individual skill is to be executable in the current environment. However, due to the resource constraint we were not able to generate the data and train a policy for the value function. On the other hand, we decided to give SayCan an unfair advantage: we use the ground truth object information to construct an oracle value function. Additionally, instead of iterating through a list of all possible (high-level action, object), we shrink the size of the skill to contain only the object type available in the current environment. As we described in $\S 5.3$, this gives SayCan an unfair competitive advantage by giving it the oracle knowledge of all objects and affordances
in the current environment a priori to compiling the list of skills. Even though SayCan can shrink the skill space with the extra knowledge, SayCan's ranking nature calls LLM significantly more times than a generative model like LLMPlanner. In fact, LLM-Planner calls GPT-3 avg. 7 times per task and SayCan calls it 22 times even with the oracle knowledge of the current environment to shrink the skill list.</p>
<h2>B. Comparison with (SL) ${ }^{3}$ on ALFRED</h2>
<p>(SL) ${ }^{3}$ [33] is a recent hierarchical planning model that is also evaluated on the ALFRED benchmark. It randomly samples $10 \%$ of ALFRED's training data for training. The high-level planner is based on a pre-trained T5-small [30] model, which is fine-tuned to generate high-level plans from the goal instruction. The low-level planner is another finetuned T5-small model, which is tasked of generating a lowlevel plan for each subgoal in the high-level plan. Both goal and step-by-step instructions are needed for training, but only goal instructions are needed at inference time.</p>
<p>We could not compare (SL) ${ }^{3}$ under the same few-shot setting as LLM-Planner because its code was not publicly available at the time of submission. However, we would like to highlight that our method achieves comparable performance on the validation set despite using only less than $1 / 20$ of training data than $(\mathrm{SL})^{3}(0.5 \%$ vs. $10 \%$ of ALFRED's training data).</p>
<table>
<thead>
<tr>
<th>Training Size</th>
<th>$\mathbf{5 0}$</th>
<th>$\mathbf{1 0 0}$</th>
<th>$\mathbf{5 0 0}$</th>
<th>$\mathbf{1 k}$</th>
<th>$\mathbf{1 0 k}$</th>
<th>Full (21k)</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM-Planner</td>
<td>10.06</td>
<td>15.36</td>
<td>16.59</td>
<td>16.46</td>
<td>16.83</td>
<td>17.80</td>
</tr>
<tr>
<td>HLSM</td>
<td>0.00</td>
<td>0.00</td>
<td>0.37</td>
<td>1.59</td>
<td>9.51</td>
<td>18.28</td>
</tr>
</tbody>
</table>
<p>Table 5: Scaling experiment of LLM-Planner and HLSM on valid unseen. Metric used is the task success rate.</p>
<h2>C. Prompt Design Choices</h2>
<p>In-context learning with GPT-3 could be sensitive to the prompt design. In Table 3, we show different prompt design choices we have experimented for LLM-Planner. We structure our prompt into six consecutive parts: task introduction, goal instruction, step-by-step instruction, plan list, object list, and retrieval message. For each part, we have a default phrase and a list of additional options to try on top of the default phrasing signified as []. All the options listed only modify the phrase that goes in []. First, we try adding punctuation marks around actions and object. Next, we naturalize each action name as a plain English text. Lastly, we experiment with finding the optimal delimiter between action list and step-by-step instruction list. We compared comma, period, and newline inserted between the sentences. The best prompt was chosen from the LOOCV accuracy for high-level plans and is bolded.</p>
<h2>D. Additional Fine-Grained Analyses</h2>
<h2>D.1. HLP Accuracy by Task Type</h2>
<p>We show LLM-Planner's high-level planning (HLP) accuracy breakdown by task type in Table 4. Because it is difficulty to determine a single value for the HLP accuracy for dynamic LLM-Planner, here we focus on the static version, but the HLP accuracy of the dynamic version generally correlates well with that of the static version. From the results, we observe that the results do not depend much on the difficulty of the task. For example, the task "Stack \&amp; Place" is often considered as the most difficult task based on the success rate of state-of-the-art models, but LLM-Planner's HLP accuracy is similar to those of easier tasks such as "Place two". We find that LLM-Planner is not overly sensitive to the complexity of tasks. This suggests that it could generalize well to different types of tasks with only a few in-context examples.</p>
<h2>D.2. End-to-End Performance by Task Type</h2>
<p>We show the end-to-end performance breakdown by task type of dynamic LLM-Planner + HLSM in Figure 5. As a reference, we also compare with HLSM and FILM trained with the full training set of ALFRED. Keep in mind that this is not apples-to-apples comparison because LLM-Planner is under the few-shot setting. Despite that, we can see that LLM-Planner + HLSM achieves comparable performance</p>
<p>SR By Task Type
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Success rate by task type on ALFRED valid unseen split.
with HLSM, and the distribution of the two are similar. This is likely due to the shared low-level planner and object detector, which introduce a similar error profile. This again shows that our few-shot high-level planner is as good as HLSM's high-level planner that is trained with the full training set. On the other hand, it also shows that there is still a large room to improve by using better low-level planners and object detectors. For example, even though our HLP accuracy for "Heat \&amp; Place" is $36 \%$ as shown in Table 4, we could only get $1.8 \%$ success rate due to the object detector from HLSM often failing to detect the "microwave". If we use FILM's low-level planner and object detector, we may be able to achieve much better performance on this task.</p>
<h2>D.3. Scaling Comparison with HLSM</h2>
<p>We show LLM-Planner's scaling experiments in comparison with the HLSM [3] in Table 5. We can see that LLMPlanner significantly outperforms HLSM on almost all data size except for the full data setting. This result shows that LLM-Planner is more data-efficient across the board compared to the existing methods. Even with the full data setting, LLM-Planner only falls behind 0.48 SR compared to the HLSM. Our work can dramatically reduce the amount of human annotations needed for learning the task while maintaining a similar performance.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/valtsblukis/h1sm&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ https://github.com/soyeonm/FILM
${ }^{5}$ https://github.com/google-research/
google-research/tree/master/saycan&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
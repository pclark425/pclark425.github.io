<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1252 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1252</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1252</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-220495729</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2007.05655v1.pdf" target="_blank">Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation</a></p>
                <p><strong>Paper Abstract:</strong> The ability to perform effective planning is crucial for building an instruction-following agent. When navigating through a new environment, an agent is challenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more flexible decision making, and performs efficient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task through pure imitation learning, outperforming previous navigation architectures by up to 5%.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1252.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1252.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R2R-EGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Room-to-Room (R2R) with Evolving Graphical Planner (EGP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vision-and-Language Navigation on the Room-to-Room dataset using the Evolving Graphical Planner, which builds an online topological graph of visited locations and possible actions and performs global planning via GNN-based proxy graphs and multi-channel message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Room-to-Room (R2R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic indoor navigation benchmark (Matterport3D-based) where agents follow natural-language route instructions in previously unseen real indoor environments; domain: household/indoor navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Topological, incremental graph: nodes represent visited locations (internal) and candidate actions (leaf nodes); connectivity stored in a tensor M_t with function types (forward/backward between visited nodes and visited-to-leaf connections). Graph is sparse and grown online; per-node max outgoing navigable actions = 16 (per dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Not specified as number of nodes; dataset statistics: 7,189 paths (21,567 instructions). Per-state maximum navigable options = 16; graph grows over exploration (top-K expansion used to control growth).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Evolving Graphical Planner (EGP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An end-to-end imitation-learned navigation agent that (1) dynamically constructs a topological graph (visited/internal nodes + leaf action nodes), (2) pools a fixed-size proxy graph via learned attention (A_t) and performs multi-step GNN message passing (planning) on the proxy graph, (3) supports global 'graph jump' actions (select a distant node and execute shortest-path route), and (4) uses graph-augmented supervision for student-forcing training.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Path Length (PL), Navigation Error (NE, meters), Success Rate (SR %), SPL (Success weighted by Path Length), Oracle Success Rate (OSR).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Val-Unseen (EGP default): PL = 13.68 m, NE = 5.34 m, SR = 52%, SPL = 41%, OSR = 65% (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>52% (Val-Unseen, default EGP settings); 53% on test split with synthetic data augmentation (EGP*).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Global planning / memory-based policy: graph-structured policy with GNN-based planning and the ability to select long-distance actions (one-step jumps) outperforms purely local/reactive policies.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Concretely reported relationships: (1) Top-K expansion (controls how many candidate leaf/action nodes are added) positively correlates with success but increases action choices: top-K=3 → SR 47% (PL 13.07, NE 5.95m); top-K=5 → SR 49% (PL 13.17, NE 5.75m); top-K=10 → SR 50% (PL 13.50, NE 5.71m); default top-K=16 → SR 52% (PL 13.68, NE 5.34m). (2) Message passing and planner channels: mp=0,channel=1 → SR 42% (PL 18.83, NE 6.06m); mp=3,channel=1 → SR 50% (PL 14.65, NE 5.73m); mp=3,channel=3 (default) → SR 52% (PL 13.68, NE 5.34m). (3) Using graph-augmented supervision (vs shortest-path student forcing) improves instruction grounding / fidelity: EGP with shortest-path supervision gave SR 46% (PL 14.68, NE 5.65m) vs graph-augmented results above. Overall: richer/global topology (larger action set) and deeper message passing lead to higher SR at modest PL cost.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>The paper varies the effective action-space topology via top-K expansion: larger top-K (more candidate outgoing edges/nodes per visited node) increases Success Rate (from 47% at top-K=3 to 52% at top-K=16) while slightly increasing or keeping PL similar; removing message passing (mp=0) substantially reduces SR and increases PL, indicating planning over graph structure is critical. No canonical graph-theoretic metrics (diameter, clustering coefficient, dead-end proportions) were compared.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies that incorporate a growing topological memory and global action-selection (one-step jumps via shortest paths on the internal graph) perform better: they enable long-range decisions and straightforward error-correction (single-step backtracking). Multi-channel GNN planning and multi-step message passing improve SR; graph-augmented supervision reduces instruction–trajectory mismatch and yields better grounding than naive student-forcing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1252.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1252.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R4R-EGP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Room-for-Room (R4R) with Evolving Graphical Planner (EGP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of EGP on the Room-for-Room benchmark, which emphasizes instruction fidelity (paths should follow instructions), showing improved fidelity metrics (CLS, nDTW, SDTW) under pure imitation learning enabled by the graphical representation and planning core.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Room-for-Room (R4R)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>An extension of R2R emphasizing instruction fidelity by producing longer/twisted routes and multiple instruction variants; domain: indoor/household instruction-following navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Same evolving topological representation as R2R: nodes for visited locations and leaf candidate actions, connectivity encoded in M_t with function types; proxy graphs created by normalized pooling (attention) for fixed-size planning.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Dataset-level: R4R contains 278,766 instructions connecting twisted routes between R2R shortest-path endpoints. Per-state maximum navigable options = 16; exact graph node counts during episodes not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Evolving Graphical Planner (EGP)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>As above: online topological graph builder + proxy-graph GNN planner + graph-augmented supervision; trained with pure imitation learning on R4R to prioritize instruction fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Coverage weighted by Length Score (CLS), normalized Dynamic Time Warping (nDTW), Success Rate weighted DTW (SDTW), Path Length (PL), Navigation Error (NE).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>Reported (Val-Unseen) for EGP (IL only): PL = 18.3 m, NE = 8.0 m, CLS = 30.2, nDTW = 44.4, SDTW = 37.4.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Memory-based global planning policy that emphasizes fidelity to instruction (graph-based planning with supervision matching instruction-grounded nodes).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper does not report explicit graph-theoretic topology metrics for R4R, but claims the same design choices (global graph, proxy planning, multi-channel message passing, graph-augmented supervision) enable better fidelity metrics compared to prior methods that mix imitation and RL. No direct experiments varying topology parameters (like top-K) reported specifically for R4R in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td>No explicit comparison across different graph topologies for R4R reported; improvements attributed to the same architectural components validated on R2R.</td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Graph-based memory + global planning allow pure imitation learning to achieve higher instruction-fidelity metrics (CLS, nDTW, SDTW) than prior mixed IL+RL baselines; policy benefits from the proxy-graph planning and graph-augmented supervision for fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>Semi-parametric topological memory for navigation <em>(Rating: 2)</em></li>
                <li>Neural map: Structured memory for deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Sparse graphical memory for robust planning <em>(Rating: 2)</em></li>
                <li>Hallucinative topological memory for zero-shot visual planning <em>(Rating: 1)</em></li>
                <li>The regretful agent: Heuristic-aided navigation through progress estimation <em>(Rating: 1)</em></li>
                <li>Self-monitoring navigation agent via auxiliary progress estimation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1252",
    "paper_id": "paper-220495729",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "R2R-EGP",
            "name_full": "Room-to-Room (R2R) with Evolving Graphical Planner (EGP)",
            "brief_description": "Vision-and-Language Navigation on the Room-to-Room dataset using the Evolving Graphical Planner, which builds an online topological graph of visited locations and possible actions and performs global planning via GNN-based proxy graphs and multi-channel message passing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Room-to-Room (R2R)",
            "environment_description": "Photorealistic indoor navigation benchmark (Matterport3D-based) where agents follow natural-language route instructions in previously unseen real indoor environments; domain: household/indoor navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Topological, incremental graph: nodes represent visited locations (internal) and candidate actions (leaf nodes); connectivity stored in a tensor M_t with function types (forward/backward between visited nodes and visited-to-leaf connections). Graph is sparse and grown online; per-node max outgoing navigable actions = 16 (per dataset).",
            "environment_size": "Not specified as number of nodes; dataset statistics: 7,189 paths (21,567 instructions). Per-state maximum navigable options = 16; graph grows over exploration (top-K expansion used to control growth).",
            "agent_name": "Evolving Graphical Planner (EGP)",
            "agent_description": "An end-to-end imitation-learned navigation agent that (1) dynamically constructs a topological graph (visited/internal nodes + leaf action nodes), (2) pools a fixed-size proxy graph via learned attention (A_t) and performs multi-step GNN message passing (planning) on the proxy graph, (3) supports global 'graph jump' actions (select a distant node and execute shortest-path route), and (4) uses graph-augmented supervision for student-forcing training.",
            "exploration_efficiency_metric": "Path Length (PL), Navigation Error (NE, meters), Success Rate (SR %), SPL (Success weighted by Path Length), Oracle Success Rate (OSR).",
            "exploration_efficiency_value": "Val-Unseen (EGP default): PL = 13.68 m, NE = 5.34 m, SR = 52%, SPL = 41%, OSR = 65% (reported).",
            "success_rate": "52% (Val-Unseen, default EGP settings); 53% on test split with synthetic data augmentation (EGP*).",
            "optimal_policy_type": "Global planning / memory-based policy: graph-structured policy with GNN-based planning and the ability to select long-distance actions (one-step jumps) outperforms purely local/reactive policies.",
            "topology_performance_relationship": "Concretely reported relationships: (1) Top-K expansion (controls how many candidate leaf/action nodes are added) positively correlates with success but increases action choices: top-K=3 → SR 47% (PL 13.07, NE 5.95m); top-K=5 → SR 49% (PL 13.17, NE 5.75m); top-K=10 → SR 50% (PL 13.50, NE 5.71m); default top-K=16 → SR 52% (PL 13.68, NE 5.34m). (2) Message passing and planner channels: mp=0,channel=1 → SR 42% (PL 18.83, NE 6.06m); mp=3,channel=1 → SR 50% (PL 14.65, NE 5.73m); mp=3,channel=3 (default) → SR 52% (PL 13.68, NE 5.34m). (3) Using graph-augmented supervision (vs shortest-path student forcing) improves instruction grounding / fidelity: EGP with shortest-path supervision gave SR 46% (PL 14.68, NE 5.65m) vs graph-augmented results above. Overall: richer/global topology (larger action set) and deeper message passing lead to higher SR at modest PL cost.",
            "comparison_across_topologies": true,
            "topology_comparison_results": "The paper varies the effective action-space topology via top-K expansion: larger top-K (more candidate outgoing edges/nodes per visited node) increases Success Rate (from 47% at top-K=3 to 52% at top-K=16) while slightly increasing or keeping PL similar; removing message passing (mp=0) substantially reduces SR and increases PL, indicating planning over graph structure is critical. No canonical graph-theoretic metrics (diameter, clustering coefficient, dead-end proportions) were compared.",
            "policy_structure_findings": "Policies that incorporate a growing topological memory and global action-selection (one-step jumps via shortest paths on the internal graph) perform better: they enable long-range decisions and straightforward error-correction (single-step backtracking). Multi-channel GNN planning and multi-step message passing improve SR; graph-augmented supervision reduces instruction–trajectory mismatch and yields better grounding than naive student-forcing.",
            "uuid": "e1252.0",
            "source_info": {
                "paper_title": "Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "R4R-EGP",
            "name_full": "Room-for-Room (R4R) with Evolving Graphical Planner (EGP)",
            "brief_description": "Evaluation of EGP on the Room-for-Room benchmark, which emphasizes instruction fidelity (paths should follow instructions), showing improved fidelity metrics (CLS, nDTW, SDTW) under pure imitation learning enabled by the graphical representation and planning core.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Room-for-Room (R4R)",
            "environment_description": "An extension of R2R emphasizing instruction fidelity by producing longer/twisted routes and multiple instruction variants; domain: indoor/household instruction-following navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": null,
            "graph_connectivity": "Same evolving topological representation as R2R: nodes for visited locations and leaf candidate actions, connectivity encoded in M_t with function types; proxy graphs created by normalized pooling (attention) for fixed-size planning.",
            "environment_size": "Dataset-level: R4R contains 278,766 instructions connecting twisted routes between R2R shortest-path endpoints. Per-state maximum navigable options = 16; exact graph node counts during episodes not specified.",
            "agent_name": "Evolving Graphical Planner (EGP)",
            "agent_description": "As above: online topological graph builder + proxy-graph GNN planner + graph-augmented supervision; trained with pure imitation learning on R4R to prioritize instruction fidelity metrics.",
            "exploration_efficiency_metric": "Coverage weighted by Length Score (CLS), normalized Dynamic Time Warping (nDTW), Success Rate weighted DTW (SDTW), Path Length (PL), Navigation Error (NE).",
            "exploration_efficiency_value": "Reported (Val-Unseen) for EGP (IL only): PL = 18.3 m, NE = 8.0 m, CLS = 30.2, nDTW = 44.4, SDTW = 37.4.",
            "success_rate": null,
            "optimal_policy_type": "Memory-based global planning policy that emphasizes fidelity to instruction (graph-based planning with supervision matching instruction-grounded nodes).",
            "topology_performance_relationship": "Paper does not report explicit graph-theoretic topology metrics for R4R, but claims the same design choices (global graph, proxy planning, multi-channel message passing, graph-augmented supervision) enable better fidelity metrics compared to prior methods that mix imitation and RL. No direct experiments varying topology parameters (like top-K) reported specifically for R4R in the paper.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "No explicit comparison across different graph topologies for R4R reported; improvements attributed to the same architectural components validated on R2R.",
            "policy_structure_findings": "Graph-based memory + global planning allow pure imitation learning to achieve higher instruction-fidelity metrics (CLS, nDTW, SDTW) than prior mixed IL+RL baselines; policy benefits from the proxy-graph planning and graph-augmented supervision for fidelity.",
            "uuid": "e1252.1",
            "source_info": {
                "paper_title": "Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.",
            "rating": 2,
            "sanitized_title": "visionandlanguage_navigation_interpreting_visuallygrounded_navigation_instructions_in_real_environments"
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2,
            "sanitized_title": "speakerfollower_models_for_visionandlanguage_navigation"
        },
        {
            "paper_title": "Semi-parametric topological memory for navigation",
            "rating": 2,
            "sanitized_title": "semiparametric_topological_memory_for_navigation"
        },
        {
            "paper_title": "Neural map: Structured memory for deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "neural_map_structured_memory_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Sparse graphical memory for robust planning",
            "rating": 2,
            "sanitized_title": "sparse_graphical_memory_for_robust_planning"
        },
        {
            "paper_title": "Hallucinative topological memory for zero-shot visual planning",
            "rating": 1,
            "sanitized_title": "hallucinative_topological_memory_for_zeroshot_visual_planning"
        },
        {
            "paper_title": "The regretful agent: Heuristic-aided navigation through progress estimation",
            "rating": 1,
            "sanitized_title": "the_regretful_agent_heuristicaided_navigation_through_progress_estimation"
        },
        {
            "paper_title": "Self-monitoring navigation agent via auxiliary progress estimation",
            "rating": 1,
            "sanitized_title": "selfmonitoring_navigation_agent_via_auxiliary_progress_estimation"
        }
    ],
    "cost": 0.0141445,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation</p>
<p>Zhiwei Deng zhiweid@cs.princeton.edu 
Department of Computer Science
Princeton University</p>
<p>Karthik Narasimhan karthikn@cs.princeton.edu 
Department of Computer Science
Princeton University</p>
<p>Olga Russakovsky 
Department of Computer Science
Princeton University</p>
<p>Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation</p>
<p>The ability to perform effective planning is crucial for building an instructionfollowing agent. When navigating through a new environment, an agent is challenged with (1) connecting the natural language instructions with its progressively growing knowledge of the world; and (2) performing long-range planning and decision making in the form of effective exploration and error correction. Current methods are still limited on both fronts despite extensive efforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a model that performs global planning for navigation based on raw sensory input. The model dynamically constructs a graphical representation, generalizes the action space to allow for more flexible decision making, and performs efficient planning on a proxy graph representation. We evaluate our model on a challenging Vision-and-Language Navigation (VLN) task with photorealistic images, and achieve superior performance compared to previous navigation architectures. For instance, we achieve a 53% success rate on the test split of the Room-to-Room navigation task [1] through pure imitation learning, outperforming previous navigation architectures by up to 5%.Preprint. Under review.</p>
<p>Introduction</p>
<p>Recent work has made remarkable progress towards building autonomous agents that navigate by following instructions [2][3][4][5][6][7][8][9][10] and constructing memory structures for maps [11][12][13]. An important problem setting within this space is the paradigm of online navigation, where an agent needs to perform navigation based on goal descriptions in an unseen environment using a limited number of steps [14,1].</p>
<p>In order to successfully navigate through an unseen environment, an agent needs to overcome two key challenges. First, the instructions given to the agent are natural language descriptions of the goal and the landmarks along the way; these descriptions need to be grounded onto the evolving visual world that the agent is observing. Second, the agent needs to perform non-trivial planning over a large action space, including: 1) deciding which step to take next to resolve ambiguities in the instructions through novel observations and 2) gaining a better understanding of the environment layout in order to progress towards the goal or recover from its prior mistakes. Notably this planning requires not only selecting from an increasingly large set of possible actions but also performing complex long-term reasoning.</p>
<p>Existing work tackles only one or two components of the above and may require additional preprocessing steps. Some are constrained to use local control policies [14,2] or use rule-based algorithms such as beam or A * search [2,15,14] to perform localized path corrections. Others focus on processing long-range observations instead of actions [16] or employ offline pre-training schemes to learn topological structures [12,13,17]. This is challenging since accurate construction of graphs is non-trivial and requires special adaptation to work during real-time navigation [13,17].  Figure 1: Under the guidance of natural language instruction, the autonomous agent needs to navigate through the environment from the start state to the target location (red flag). Our proposed Evolving Graphical Planner (EGP) constructs a dynamic representation and makes decisions in a global action space (right). With the EGP, the agent, currently in the orange node, maintains and reasons over the evolving graph to select the next node to visit (green) from possible choices (blue).</p>
<p>In this paper, we propose the Evolving Graphical Planner (EGP) (Figure 1), which 1) dynamically constructs a graphical map of the environment in an online fashion during exploration and 2) incorporates a global planning module for selecting actions. EGP can operate directly on raw sensory inputs in partially observable settings by building a structured representation of the geometric layout of the environment using discrete symbols to represent visited states and unexplored actions. This expressive representation allows our agent to choose from a greater number of global actions conditioned on the text instruction and perform course corrections if needed.</p>
<p>Incorporating a global navigation module is challenging since we do not always have access to ground truth supervisions from the environment. Further, the ever expanding size of the global graphs requires a scalable action selection module. To solve the first challenge, we introduce a novel method for training our planning modules using imitation learning -this allows the agent to efficiently learn how to select global actions and backtrack when necessary. For the second, we introduce proxy graphs, which are local approximations of the entire map and allow for more scalable planning. Our entire model is end-to-end differentiable under pure imitation learning framework.</p>
<p>We test EGP on two benchmarks for 3D navigation with instructions -Room-to-Room [1] and Room-for-Room [18]. Our model outperforms several state-of-the-art backbone architectures on both datasets -e.g., on Room-to-Room, we achieve a 5% improvement over the Regretful agent [19] on success rate. We also perform a series of ablation studies on our model to justify the design choices.</p>
<p>Related work</p>
<p>Embodied Navigation Agent Many recent papers develop neural architectures for navigation tasks [19, 2, 14, 20-22, 16, 23]. Vision-and-Language Navigation (VLN) [1,5] is one representative task that focuses on language-driven navigation across photo-realistic 3D environments. Anderson et al. [1] propose the Room-to-Room benchmark and an attention-based sequence-to-sequence method. Fried et al. [2] extend the model by a pragmatic agent with the ability to synthesize data through a speaker model. With an emphasize on grounding, the self-monitoring agent [14] adopts a co-grounding module and a progress estimation auxiliary task for more progress-sensitive alignment. Similarly, an intrinsic reward [24] is introduced to improve cross-modal alignment for navigation agent. Ma et al. extends the existing works towards a graph search like algorithm by adding one-step regretful action. Anderson et al. [25] proposes an agent formulated under Bayesian filtering with a global mapper. Hand-crafted decoding algorithms are also used as a post-processing technique but may lead to long trajectories and difficulty on joint optimization [2,15,26].</p>
<p>Navigation memory structures A recent emerging trend of navigation focuses on extending the agents with different types of memory structures. Simple structures: Parisotto et al. [11] propose a tensor-based memory structure with operations that agent can use to access and perform navigation. Fang et al. [16] adopts transformer to extract historical memory information stored in a sequence. Topological structures: The landmark-based topological representation has shown to be effective in pre-exploration based navigation tasks without the need of externally provided camera poses or ego-motion information [12]. Laskin et al. proposes methods to sparsify the graphical memory through consistency checkings and graph cleanups [13]. Liu et al. uses a contrastive energy model for building more accurate edges in the memory graph [17].</p>
<p>Graphical representation Graph-based methods have been shown to be an effective intermediate representation for information exchange [27][28][29][30][31]. In image and video understanding, graphical representation is used for visual question answering [32,33], video captioning [34] or action recognition [35,36]. In robotics, Huang et al. [37] propose Neural Task Graph (NTG) as an intermediate modularized representation leading to better generalization. Graph Neural Networks are demonstrated to be effective in learning structured policies [24] and automatic robot design [38].</p>
<p>Supervision strategies for imitation learning The proper training of sequential models is challenging due to the drifting issues [39]. DAgger [39] proposes to aggregate datasets with expert supervision provided for the sequences samples from student models. Scheduled sampling [40] tackles this problem through mixing the samples from both ground truth and student models. Professor forcing [41] shows a more effective approach through adversarial domain adaptation. OCD [42] adopts the online computed characters as ground truth for speech recognition. In this paper, we instead propose a graph-augmented strategy to provide expert supervisions, which alleviates the mismatch issue between instructions and new-computed expert trajectories [14].</p>
<p>Model</p>
<p>Problem definition and setup We follow the standard instruction-following navigation problem setting [1]. Given the demonstration dataset
D = {(x i , τ * i , ENV i )} |D| i=1
, where x i is the language instruction with length |x t |, τ * is the expert navigation trajectories (a * 1 , a * 2 , ..., a * |τ * | ) and ENV i is the environment paired with the data, the agent is trained to imitate the expert behaviours to correctly follow the instruction and navigate to the goal location. At each navigation step t, the agent is located at the state s t with observations o t from the environment and performs an action a t ∈ A st , where A st is the decision space at state s t . In our task, decision space is the set of navigable locations [2].</p>
<p>To set up an agent, we build upon a basic navigation architecture in [14] and utilize the language encoder and the attention-based decoder. The agent encodes the language instruction x into a hidden encoding c = (c 1 , c 1 , ..., c |x| ) through an LSTM [43]. Conditioned on c, the agent uses an attentionbased decoder to model the distribution over the trajectory p(τ |x, ENV). At every step, the decoder takes in the encoding c, the observation o t and a maintained hidden memory h t−1 to produce the per-step action probability distribution p(a t |h t−1 , o t , c). Note that such an navigation agent suffers from the constrained local action set and lacks the ability to perform long-range planning over the navigation space and to effectively correct errors along the navigation.</p>
<p>Our approach In this section, we introduce our Evolving Graphical Planner (EGP), an end-to-end global planner that navigates a agent through a new environment via re-defining the decision space and performing long-term plannings over the graphs. With the graphical representation, the agent accumulates the knowledge about the unseen environment and has access to the entire action space. Each long-distance navigation is then reduced to one-step decision making, leading to easier and more efficient exploration and error correction. We also show that the graphical representation elicits a new supervision strategy for effective training of imitation agent, which alleviates the mismatch issue in standard navigation agent training [14]. The global planning is performed on a efficient proxy representation, making the model scalable to navigation with longer horizon.</p>
<p>The proposed model consists of two core components: (1) an evolving graphical representation that generalizes the local action space; (2) a planning core that performs multi-channel information propagation on a proxy representation. Starting from the initial information, the EGP agent gradually expands an internal representation of the explored space (Section 3.1) and perform planning efficiently over the graphs (Section 3.2).</p>
<p>Evolving Graphical Planner: Graphical representation</p>
<p>We start with introducing the notations of the representation. Our graphical representation progressively expands during navigation (light-yellow = visited nodes; orange = current node; blue = potential action nodes; green = selected node). Based on the graphical representation, the agent performs planning over the actions, and selects the next action through student sampling. The top-K nodes on the current state will be kept in the graph. (2): The model performs multi-channel planning on a proxy graph pooled from the entire graph representation. The refined node states are unpooled back to the entire graph and used to compute the probability distribution over actions.
Let G t = {V t , E t , M t } denote a graph at time t, where V t = {v 1 , v 2 , ..., v |Vt| }, v i ∈ R d is the set of node embeddings, E t = {e ij }, e ij ∈ R d
is the set of edge embeddings between node i and node j, and M t ∈ R |Vt|×|Vt|×|F | is the graph connectivity tensor with function types. In our graph, nodes are separated into two types: the leaf nodes represent the possible actions (e.g. navigable locations) and internal nodes represent the visited locations, as shown in figure 2.</p>
<p>Graph construction The agent builds up the graphical representation progressively during navigation. Initialized as empty set, the graph G t expands the node set, edge set and connectivity function tensor through the observations and local connection information given by the environment. There are three function types considered and stored in the tensor M t : the forward and backward directions between two visited nodes, and the connection from the visited node to the leaf node. With the new graph G t+1 , the agent continues navigation and the loop repeats. To reduce the memory cost, the model has the option to selectively add nodes to the graph. We use top-K leaf nodes, ranked by confidence scores in policy distribution, to expand the graph, as shown in fig. 2.</p>
<p>Navigation with graph jump With graph G t representing the entire action space, the agent can easily choose to navigate to execute actions that have not been explored in previous visited locations.</p>
<p>For the proposed action a t from the planner, the agent computes the shortest-path route based on the graph G t and plans the navigation route τ . This allows the agent to "jump" around the full graph-defined action space and execute the unexplored actions through a single-step decision. The long-range decision space also makes error-correction easier for the agent: a single step decision is all the agent needs to backtrack to the correct location. With the navigation steps on the internal route τ generated through the graph, the agent keeps updating the hidden states h based on the observation o t from the environments.</p>
<p>Supervising imitation learning The proper training of the imitation learning agent has been a challenging problem [42,39,41]. Student forcing with new computed route as supervision is a widely used solution in navigation [14,19,2]. However, the mismatch between the new route and language instructions could potentially lead to noisy and incorrect signals for learning language groundings and navigation. We provide a new graph-augmented solution for computing the supervision for each student sampled trajectory. Assume a metric M(·, ·) between two navigation trajectories (e.g. [44]). With graph memorizing the entire possible action space, the subset of nodes in ground truth route τ * is guaranteed to exist in G t . We choose the node v on the τ * that maximizes the metric M(τ t ∪ (v), τ * ) as the ground truth action for step t. This basically provides a "correction" signal that indicates the best action to take for correcting the mistake made by the agent.</p>
<p>Evolving Graphical Planner: Planning Core</p>
<p>With the graphical representation G t , a straightforward method is to directly perform planning on the full graph using the embeddings of nodes and edges. However, a progressively growing graph can lead to high costs and limit the scalability of the model. Pre-exploration based methods often tackle this issue through performing offline sparsification with multiple rounds of cleanups on the pre-collected graphs [13] containing full knowledge over the map, which are unsuitable under the online navigation settings. In this section, we show that, interestingly, the effective planning can be achieved not through the full graph, and present the second component of our model, where it performs a goal-driven information extraction dynamically on the entire graph G t to build a condensed proxy representationḠ t used for planning. Our model utilizes Graph Neural Networks (GNN) as a basic operator, which we explain at the end of the section.</p>
<p>Proxy graph Denote the proxy graph asḠ
t = (V t ,Ē t ,M t ), whereV t = {v 1 ,v 2 , ...,v |Vt| },v i ∈ R d , E t = {ē ij }
andM t are the pooled node embedding set, edge embedding set and connectivity matrix with function types respectively. The proxy graphḠ t contains a fixed number of nodes invariant to the growing graph size in G t . We hypothesize that given the instruction information and the current states of the agent, there are only a subset of nodes providing useful information for planning. To construct the proxy representation, the model uses a normalized pooling similar to [45]. Differently, our graphical representation consists of a rich set of information including edge states, function types in connectivity matrices besides the node states. We describe the process of generating the proxy representation and corresponding planning as follows.</p>
<p>Given the entire graphical representation G t = (V t , E t , E t ), the planner contains two functions, GNN L and GNN P . The function GNN L takes in the agent state information and constructs the proxy representation through a lightweight neural network. Assume the pooling function uses d L as the graph dimension in the propagation model, and performs K L step message passing. We derive the pooling matrix as follows, using a small d L :
A t = softmax GNN L G t , [h t ; c]; K L , d L ∈ R |Vt|×|V t |
The normalized pooling matrix is the attention weights on the entire graph and extracts relevant information conditioned on the agent states h t and instructions c for further planning. To help the description, we denote the concatenated matrix of all node vectors as V t ∈ R |Vt|×d , the concatenation of all edge vectors as tensor E t ∈ R |Vt|×|Vt|×d . The concatenation order is aligned with the connectivity function matrix. The following operations are used for deriving the proxy
representation.V t = A T t V t ∈ R |V t |×d M t,i = A T t M t,i A t ∈ R |V t |×|V t | , i ∈ {1, 2, ..., |F|} E t,i = A T t E t,i A t ∈ R |V t |×|V t | , i ∈ {1, 2, ..., d}
whereM t,i is a non-negative matrix indicating the weights of connectivity among nodes for function type i,Ē t,i is the matrix at i th dimension for the edge state tensor. The pooled tensorsV t andĒ t are corresponded to the node setV t and edge setĒ t of the proxy graphḠ t</p>
<p>Planning The planning of the navigation agent is achieved through propagating information among nodes in the proxy graph, conditioned on the agent state information h t and instruction encodings c. Denote the graph dimension used in the propagation model as d P , the number of steps for message passing operations as K P , the refined node embedding of the proxy graph is derived through GNN P , (V t , [h t ; c]; K P , d P ), with d P controlling the capacity of the function. The refined node embedding contains the information involving the neighboring nodes (visited locations and unexplored actions), the state of agent, the instruction, and the connectivity types between nodes. With the globally refinement step, node embedding vector is unpooled back to the original graph through V t =V t A T t . With the update node representation containing both the current state of the agent and the full action-observation space in the history, the distribution over actions is generated based on the node vectors:
a i t = f out (h t , v i t ); p(a i t |h t , c) = exp(â i t )/ j exp(â j t )(1)
where the f out function is a dot-product with linear mapping (W hv h t ) T v i t parameterized by W hv . Multi-channel planning The model is further strengthened with the ability to perform multi-channel planning over the graph G t . Instead of only using one proxy graph representationḠ t to extract information and perform propagation, we find it useful to learn a set of proxy graphs {Ḡ t } and perform planning independently on each of them. The final information are aggregated through summation over the embedding across the channels. The final policy over actions is generated through the same operation described in eqn. 1</p>
<p>Training objective We train our full agent through the standard maximum likelihood objective using cross-entropy loss. Given the demonstration D, the loss function to optimize is formulated as:
L = E (τ * ,x,ENV)∼D t −y i t log(p(a i t |h t , c = LSTM(x)))(2)
where τ * is the ground truth trajectory, y i t is the ground truth label on action i at step t, generated through the graph-augmented supervision using the information of τ * , as described in Sec. 3.1.</p>
<p>Message Passing Operations</p>
<p>In this subsection we explain the Graph Neural Network (GNN) operator used in the EGP. As the operator is used in both pooling and planning, we describe it as a general function which takes in a graph G and a general context information r (e.g. the agent hidden state and language encodings), with hyper-parameters K and d G . Formally, given the input graph G = (V 0 , E 0 , M ) and the context vector r, the function GNN(G, r; K, d G ) generates the refined node vectors V K+1 after K steps of message passings, where d G is the vector dimension in the propagation model. The subscription on nodes and edges denotes the index of message passing iterations, with a slight abuse of notation. The GNN(·, ·; K, d G ) function contains two components: an input network and a propagation network.</p>
<p>Input network Along with the initial vectors for nodes and edges, the input network considers the context vector r as a shared additional information across nodes and edges. The input model maps the context vector with node and edge vectors respectively into two fixed-size embedding as follows:
v i 1 = f in (v i 0 , r); e ij 1 = f in (e ij 0 , r); v i 1 , e ij 1 ∈ R d G(3)
where f in is a neural network, and the generated embedding vectors are used for message communications among graph nodes in the propagation model.</p>
<p>Propagation network The propagation model, taking in the mapped embedding vectors {v i 1 }, consecutively generates a sequence of node embedding vectors (v i 2 , v i 3 , ..., v i K+1 ), k ∈ {1, ..., K +1} for each node. At step k, the propagation operation updates every node through computing and aggregating information from the neighborhood nodes. The process is executed in the order:
e ij k = n M i,j,n · f n v i k , v j k , e ij 1 ∈ R d G ; (4) v i k+1 = g j∈Ni e ij k , v i k ∈ R d G(5)
where f n (·, ·, ·) : R 3d → R d is the message function. Function g(·) : R 2d → R d is the aggregator function that collects messages back to node vectors. N i represents the neighbours of node i in the graph. The refined node vector set V K+1 = {v i K+1 }, containing global information from the whole graph is mapped through a matrix W out to recover the input node dimension d in and is used as the output of the GNN(·, ·) function for either pooling or planning component, as described in Sec. 3.2.</p>
<p>Experiments</p>
<p>Experimental setup</p>
<p>Datasets We evaluate our methods on the standard benchmark datasets for Vision-and-Language Navigation (VLN). The VLN task is built upon photo-realistic simulated environments [46] with human-generated instructions describing the landmarks and directions for navigation routes. Starting at a random sampled location in the environment, the agent needs to follow the instruction to navigate through the environment. There are two datasets commonly used for VLN: (1) Room-to-Room (R2R) benchmark [1] with 7,189 paths, each associated with 3 sentences, resulting in 21,567 total human instructions. The paths are produced through computing shortest paths from start to end points; (2) Room-for-Room (R4R) [18], which extends the R2R dataset by re-emphasizing on the necessity of following instructions compared to the goal-driven definition in R2R. The R4R dataset contains 278,766 instructions associated with twisted routes connecting two shortest-path trajectories in R2R. The dataset details are summarized in table 1.  Implementation details We follow [14] and adopt the co-grounding agent (w/o auxiliary loss) as our base agent. As the standard protocol for R2R, visual features for each location are precomputed ResNet features from the panoramic images. In the Evolving Graphical Planner, we use 256 dimensions as the graph embedding size for both the full graph and the proxy graph. The propagation model uses three iterations of message passing operations. For every expansion step, the default setting adds all the possible navigable locations into the graph (top-K is set to 16, the maximum number of navigable location in both datasets). For student-forced training, we use graphaugmented ground truth supervision throughout the experiments except for the ablation study on supervision methods. The model is trained jointly, using Adam [47] with 1e-4 as the default learning rate.</p>
<p>Room-to-Room benchmark</p>
<p>Evaluation metrics We follow prior works on the R2R dataset and report: navigation error (NE) in meters, lower is better; Success Rate (SR), i.e., the percentage of navigation end-locations which are within 3m of the true global location; Success Rate divided by path Length in meters (SPL); and Oracle Success Rate (OSR), i.e., the path at any point passes within 3m of the goal state.</p>
<p>Comparison with prior art</p>
<p>Architectures for comparison We compare our model with the following state-of-the-art navigation architectures: (1) the Seq2Seq agent [1] that translates instructions to actions; (2) Speaker-Follower (SF) [2] agent that augments the dataset with a speaker model; (3) the Reinforced Cross-Modal (RCM) agent [48] using modal-alignment score as intrinsic reward for reinforcement learning; (4) the Self-Monitoring (Monitor) agent [14] that uses a co-grounding module and a progress estimation component to increase the progress alignment between texts and trajectories; (5) the Regretful agent [19] that uses the Regretful module and Progress Marker to perform one-step rollback; (6) the Ghost [25] with Bayesian filters.</p>
<p>Results</p>
<p>We report results in table 2. We train our models both by using only the standard demonstration and by augmenting the dataset with the synthetic data containing 178,330 instruction-route pairs generated by the Speaker model [2]. On the Val Unseen split, we observe that just through using the EGP module (without synthetic data augmentation), the performance of agent can be increased over the baseline agent by 0.86 meters on NE (from 6.20 to 5.34), by 9% on SR (from 43% to 52%) on SR, by 5% on SPL (from 36% to 41%), and by 13% on OSR (from 52% to 65%). Our path length remains short, at 13.7 meters compared to 12.8m for baseline, 14.8m for RCM [48] and 15.2m SF [2] (not shown in the table). Most notably, our EGP agent with synthetic data augmentation outperforms prior art on all metrics, across both the validation-unseen and the test set. Concretely, on the test set we achieve a 0.35 meters reduction in NE (from 5.69 to 5.34), a 5% improvement on SR (from 48% to 53%), a 2% improvement on SPL (from 40% to 42%), and a 2% improvement on OSR (from 59% to 61%) over the best performing prior model on each metric respectively.</p>
<p>Discussion of other works Note that there are other works contributing to this benchmark through non-architecture approaches: using more data (6,582K) for BERT-type pre-training [4]; exploiting web data [49]; adding extra tasks [50]; adding dropout regularization [51]; different settings of evaluation (fusing information from three instructions) [52,53]; post-processing decoding method [15]. We contribute to the backbone navigation architectures and these works can potentially be useful as complementary approaches.  </p>
<p>Ablation studies</p>
<p>We now justify the design choices of our model by analyzing the individual components. In addition to the metrics above we also include Path Length (PL) for completeness. Results are summarized in table 3, with the last row depicting our model with the default settings.</p>
<p>Does global planning matter We verify the importance of global planning and navigation through controlling the top-K expansion rate for the graphical representation. In R2R dataset, there are maximally 16 navigable locations for each state. With smaller expansion rate, the EGP planner will have less expressive power on exploiting global information from the environments. As seen in the top group of rows of table 3, with smaller top-K, the path becomes shorter (fewer options to explore) but the accuracy of the model consistently drops, indicating the importance of the global planning.</p>
<p>Planner implementation Next we analyze the effects of message passing steps and the number of channels used on our planner module. The results are summarized in the second group of rows in table 3. Through the information propagation operations, our model achieves a 8% increase on SR (from 42% with mp=0,channel=1 to 50% with mp=3,channel=1).With more independent planning channels, we can obtain a further 2% improvement on SR (from 50% with mp=3,channel=1 to 52% for our default setting with mp=3,channel=3, last row). To verify whether the increase is due to more parameters in the model, we also add a comparison through using a single channel planner with three times larger graph dimensions (768), which shows no similar effect to the multi-channel models.</p>
<p>Compare across supervision methods Finally, we compare our methods with the standard supervision method navigation agent trained by student forcing. The standard supervision used for student forcing requires recomputing the new route to goal for each location, leading to potential noisy data and larger generalization error, shown in the second-from-last row of table 3.</p>
<p>Room-for-Room benchmark</p>
<p>Evaluation metrics Room-for-Room dataset emphasizes on the ability of correctly following instructions instead of solely on reaching the goal locations. We follow the metrics in [18,44] and mainly compare our results on Coverage weighted by Length Score (CLS) that measures the fidelity of the agent's path to the reference, weighted by the length score, and the Success rate weighted normalized Dynamic Time Warping (SDTW) and normalized Dynamic Time Warping (nDTW) that measures the spatiotemporal similarity of the paths by the agent and the expert refence.</p>
<p>Architectures for comparison We compare our model with the Speaker-Follower model [2], Reinforced Cross-Modal agent [48] trained under goal-directed and fidelity-oriended rewards, reported in [18], and the Perceive Transform Act (PTA) agent [54] using more complex multi-modal attentions.</p>
<p>Results analysis</p>
<p>We summarize the results in table 4. Note that all previous state-of-the-art methods require the mixed training between imitation learning and reinforcement learning objectives. The student forcing method leads to goal-oriented supervisions and harms the ability of following  instructions for agents [18]. Our model is the first that successfully train the navigation agent through pure imitation learning on the R4R benchmark, due to the benefit of the graphical representation, the powerful planning module and the new supervision method. We obtain a consistent margin across all metrics. Specifically, our model outperforms other architectures by 7.0, 5.0 and 4.9 on the fidelity-oriented measurements CLS, nDTW and SDTW respectively. Also, although using a global search mechanism, our model maintains a relatively short path length, which is difficult in other rule-based global search algorithms [2,15].   [44] as [18] did not report DTW-based results)</p>
<p>Conclusion</p>
<p>In this work, we proposed a solution to the long-standing problem of contextual global planning for vision-and-language navigation. Our system based on the new Evolving Graphical Planner (EGP) module outperforms prior backbone navigation architectures on multiple metrics across two benchmarks. Specifically, we show that building a policy over the global action space is critical to decision making, the graphical representation can further elicit a new supervising strategy for student forcing in navigation, and, interestingly, the actual planning can be achieved through a proxy graph rather than the actual topological representation which leads to high costs in both time and memory.</p>
<p>Broader Impact</p>
<p>This work has several downstream applications in areas like autonomous navigation and robotic control, especially through the use of natural language instruction. Potential downstream uses of such agents range from healthcare delivery to elderly home assistance to disaster relief efforts. We believe that imbuing these agents with a global awareness of the environment and long-term planning will enable them to handle more challenging tasks and recover gracefully from mistakes. While the graph-based approach we propose is scalable and easy to manipulate in real time, future research can address computation challenges and increase the planning time-scale to enable better decision making.</p>
<p>Acknowledgement</p>
<p>This work is partially supported by King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR) under Award No. OSRCRG2017-3405 and by Princeton University's Center for Statistics and Machine Learning (CSML) DataX fund. We would also like to thank Felix Yu and Zeyu Wang for offering insightful discussions and comments on the paper.</p>
<p>S
Task: Turn around and continue down the stairs on the right. At the bottom of the stairs stop.</p>
<p>Figure 2 :
2Overall scheme of the Evolving Graphical Planner model. (1):</p>
<p>When receiving an observation o t and the set of information {o at } st over possible actions {a t } st at the new state s t , the agent maps the current location information o t and action information o at through two separate neural networks, resulting to the node embedding v st and {v a k } t . The incoming and outgoing edges of new nodes are determined through the local map information and the moving direction of the agent.</p>
<p>Table 1 :
1Dataset statistics.</p>
<p>Val Unseen Test
UnseenModelsTypeNE ↓ SR % ↑ SPL % ↑ OSR % ↑ NE ↓ SR % ↑ SPL % ↑ OSR % ↑Seq2Seq [1] 
IL 
6.01 
39 
-
53 
7.81 
22 
-
28 
Ghost [25] 
IL 
7.20 
35 
31 
44 
7.83 
33 
30 
42 
SF  *  [2] 
IL 
6.62 
36 
-
45 
6.62 
35 
28 
44 
RCM  *  [48] 
IL+RL 
5.88 
43 
-
52 
6.12 
43 
38 
50 
Monitor [14] 
IL 
5.98 
44 
30 
58 
-
-
-
-
Monitor  *  [14] 
IL 
5.52 
45 
32 
56 
5.67 
48 
35 
59 
Regretful [19] 
IL 
5.36 
48 
37 
61 
-
-
-
-
Regretful  *  [19] 
IL 
5.32 
50 
41 
59 
5.69 
48 
40 
56 </p>
<p>Baseline agent 
IL 
6.20 
43 
36 
52 
-
-
-
-
EGP (ours) 
IL 
5.34 
52 
41 
65 
-
-
-
-
EGP  *  (ours) 
IL 
4.83 
56 
44 
64 
5.34 
53 
42 
61 </p>
<p>Table 2 :
2We compare our architecture with previous state-of-the-art architectures on the Val Unseen 
and Test splits of R2R [1]. ( * : models using additional synthetic data. %: numbers in percentage). </p>
<p>Model PL↓ NE↓ SR % ↑ SPL % ↑ OSR % ↑Ablation type </p>
<p>EGP -topK = 3 
13.07 5.95 
47 
38 
56 
Global vs local planning EGP -topK = 5 
13.17 5.75 
49 
40 
59 
EGP -topK = 10 
13.50 5.71 
50 
40 
61 </p>
<p>EGP -mp=0,channel=1 
18.83 6.06 
42 
32 
62 
Planner implementation 
EGP -mp=3,channel=1 
14.65 5.73 
50 
40 
62 
EGP -graph dim × 3 
14.16 5.68 
49 
38 
60 </p>
<p>Supervision 
EGP -with shortest path 14.68 5.65 
46 
36 
57 </p>
<p>-
EGP 
13.68 5.34 
52 
41 
65 </p>
<p>Table 3 :
3We show ablation studies of our method on the val-unseen set of Room-to-Room. The default setting for our model (bottom row) is top-K= 16, mp=3, channel=3, graph dimension=256; we use graph-augmented supervision rather than shortest path for training.</p>
<h2>ModelsType PL NE ↓ SR % ↑ CLS↑ nDTW↑ SDTW↑ RCM + fidelity-oriented[18] IL+RL 28.5Random</h2>
<p>23.6 
10.4 
13.8 
22.3 
18.5 
4.1 
Speaker-Follower[18] 
IL+RL 19.9 
8.47 
23.8 
29.6 
-
-
RCM + goal-oriented[18] 
IL+RL 32.5 
8.45 
28.6 
20.4 
26.9  <em><br />
11.4  </em><br />
8.08 
26.1 
34.6 
30.4  <em><br />
12.6  </em><br />
PTA low-level[54] 
IL+RL 10.2 
8.19 
27.0 
35.0 
20.0 
8.0 
PTA high-level[54] 
IL+RL 17.7 
8.25 
24.0 
37.0 
32.0 
10.0 
EGP (ours) 
IL 
18.3 
8.0 
30.2 
44.4 
37.4 
17.5 </p>
<p>Table 4 :
4Comparison across methods on R4R Val Unseen split. Path Length (PL) is reported as a reference. ( * Note that we refer to the numbers from</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683, 2018.</p>
<p>Speakerfollower models for vision-and-language navigation. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell, Advances in Neural Information Processing Systems. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker- follower models for vision-and-language navigation. In Advances in Neural Information Processing Systems, pages 3314-3325, 2018.</p>
<p>From language to goals: Inverse reinforcement learning for vision-based instruction following. Justin Fu, Anoop Korattikara, Sergey Levine, Sergio Guadarrama, arXiv:1902.07742arXiv preprintJustin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse reinforcement learning for vision-based instruction following. arXiv preprint arXiv:1902.07742, 2019.</p>
<p>Towards learning a generic agent for vision-and-language navigation via pre-training. Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, Jianfeng Gao, arXiv:2002.10638arXiv preprintWeituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learn- ing a generic agent for vision-and-language navigation via pre-training. arXiv preprint arXiv:2002.10638, 2020.</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHoward Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12538-12547, 2019.</p>
<p>Learning to navigate in cities without a map. Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, Advances in Neural Information Processing Systems. Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al. Learning to navigate in cities without a map. In Advances in Neural Information Processing Systems, pages 2419-2430, 2018.</p>
<p>Zero-shot task generalization with multi-task deep reinforcement learning. Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generaliza- tion with multi-task deep reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2661-2670. JMLR. org, 2017.</p>
<p>Jesse Thomason, Michael Murray, Maya Cakmak, Luke Zettlemoyer, arXiv:1907.04957Vision-and-dialog navigation. arXiv preprintJesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. arXiv preprint arXiv:1907.04957, 2019.</p>
<p>Rmm: A recursive mental model for dialog navigation. Yonatan Homero Roman Roman, Jesse Bisk, Asli Thomason, Jianfeng Celikyilmaz, Gao, arXiv:2005.00728arXiv preprintHomero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. Rmm: A recursive mental model for dialog navigation. arXiv preprint arXiv:2005.00728, 2020.</p>
<p>Learning to cooperate: Emergent communication in multi-agent navigation. Ivana Kajić, Eser Aygün, Doina Precup, arXiv:2004.01097arXiv preprintIvana Kajić, Eser Aygün, and Doina Precup. Learning to cooperate: Emergent communication in multi-agent navigation. arXiv preprint arXiv:2004.01097, 2020.</p>
<p>Neural map: Structured memory for deep reinforcement learning. Emilio Parisotto, Ruslan Salakhutdinov, arXiv:1702.08360arXiv preprintEmilio Parisotto and Ruslan Salakhutdinov. Neural map: Structured memory for deep reinforce- ment learning. arXiv preprint arXiv:1702.08360, 2017.</p>
<p>Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun, arXiv:1803.00653Semi-parametric topological memory for navigation. arXiv preprintNikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory for navigation. arXiv preprint arXiv:1803.00653, 2018.</p>
<p>Michael Laskin, Scott Emmons, Ajay Jain, Thanard Kurutach, arXiv:2003.06417Pieter Abbeel, and Deepak Pathak. Sparse graphical memory for robust planning. arXiv preprintMichael Laskin, Scott Emmons, Ajay Jain, Thanard Kurutach, Pieter Abbeel, and Deepak Pathak. Sparse graphical memory for robust planning. arXiv preprint arXiv:2003.06417, 2020.</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Alregib, Zsolt Kira, Richard Socher, Caiming Xiong, arXiv:1901.03035arXiv preprintChih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Self-monitoring navigation agent via auxiliary progress estimation. arXiv preprint arXiv:1901.03035, 2019.</p>
<p>Tactical rewind: Self-correction via backtracking in vision-and-language navigation. Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLiyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6741-6749, 2019.</p>
<p>Scene memory transformer for embodied agents in long-horizon tasks. Kuan Fang, Alexander Toshev, Li Fei-Fei, Silvio Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionKuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio Savarese. Scene memory transformer for embodied agents in long-horizon tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 538-547, 2019.</p>
<p>Hallucinative topological memory for zero-shot visual planning. Kara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, Aviv Tamar, arXiv:2002.12336arXiv preprintKara Liu, Thanard Kurutach, Christine Tung, Pieter Abbeel, and Aviv Tamar. Hallucinative topological memory for zero-shot visual planning. arXiv preprint arXiv:2002.12336, 2020.</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. Vihan Jain, Gabriel Magalhaes, Alex Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge, arXiv:1905.12255arXiv preprintVihan Jain, Gabriel Magalhaes, Alex Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. arXiv preprint arXiv:1905.12255, 2019.</p>
<p>The regretful agent: Heuristic-aided navigation through progress estimation. Chih-Yao Ma, Zuxuan Wu, Ghassan Alregib, Caiming Xiong, Zsolt Kira, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6732-6740, 2019.</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, J Joseph, Abhinav Lim, Li Gupta, Ali Fei-Fei, Farhadi, 2017 IEEE international conference on robotics and automation (ICRA). IEEEYuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In 2017 IEEE international conference on robotics and automation (ICRA), pages 3357-3364. IEEE, 2017.</p>
<p>Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSaurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, and Jitendra Malik. Cogni- tive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616-2625, 2017.</p>
<p>Modular multitask reinforcement learning with policy sketches. Jacob Andreas, Dan Klein, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In Proceedings of the 34th International Conference on Machine Learning- Volume 70, pages 166-175. JMLR. org, 2017.</p>
<p>Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. Fei Xia, B William, Chengshu Shen, Priya Li, Micael Edmond Kasimbeg, Alexander Tchapmi, Roberto Toshev, Silvio Martín-Martín, Savarese, IEEE Robotics and Automation Letters. 52Fei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Micael Edmond Tchapmi, Alexan- der Toshev, Roberto Martín-Martín, and Silvio Savarese. Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. IEEE Robotics and Automation Letters, 5(2):713-720, 2020.</p>
<p>Nervenet: Learning structured policy with graph neural networks. Tingwu Wang, Renjie Liao, Jimmy Ba, Sanja Fidler, Tingwu Wang, Renjie Liao, Jimmy Ba, and Sanja Fidler. Nervenet: Learning structured policy with graph neural networks. 2018.</p>
<p>Chasing ghosts: Instruction following as bayesian state tracking. Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, Stefan Lee, Advances in Neural Information Processing Systems. Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, and Stefan Lee. Chasing ghosts: Instruction following as bayesian state tracking. In Advances in Neural Information Processing Systems, pages 369-379, 2019.</p>
<p>Incremental sampling-based algorithms for optimal motion planning. Sertac Karaman, Emilio Frazzoli, Robotics Science and Systems VI. 1042Sertac Karaman and Emilio Frazzoli. Incremental sampling-based algorithms for optimal motion planning. Robotics Science and Systems VI, 104(2), 2010.</p>
<p>Convolutional neural networks on graphs with fast localized spectral filtering. Michaël Defferrard, Xavier Bresson, Pierre Vandergheynst, Advances in Neural Information Processing Systems. Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pages 3844-3852, 2016.</p>
<p>Gated graph sequence neural networks. Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel, arXiv:1511.05493arXiv preprintYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>Mikael Henaff, Joan Bruna, Yann Lecun, arXiv:1506.05163Deep convolutional networks on graph-structured data. arXiv preprintMikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data. arXiv preprint arXiv:1506.05163, 2015.</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. Dougal David K Duvenaud, Jorge Maclaurin, Rafael Iparraguirre, Timothy Bombarell, Alán Hirzel, Ryan P Aspuru-Guzik, Adams, Advances in neural information processing systems. David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pages 2224- 2232, 2015.</p>
<p>Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. Zhiwei Deng, Arash Vahdat, Hexiang Hu, Greg Mori, Computer Vision and Pattern Recognition (CVPR). Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori. Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition. In Computer Vision and Pattern Recognition (CVPR), 2016.</p>
<p>Graph-structured representations for visual question answering. Damien Teney, Lingqiao Liu, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDamien Teney, Lingqiao Liu, and Anton van Den Hengel. Graph-structured representations for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-9, 2017.</p>
<p>A simple neural network module for relational reasoning. Adam Santoro, David Raposo, G David, Mateusz Barrett, Razvan Malinowski, Peter Pascanu, Timothy Battaglia, Lillicrap, Advances in neural information processing systems. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In Advances in neural information processing systems, pages 4967-4976, 2017.</p>
<p>Spatio-temporal graph for video captioning with knowledge distillation. Haoye Boxiao Pan, De-An Cai, Kuan-Hui Huang, Adrien Lee, Ehsan Gaidon, Juan Carlos Adeli, Niebles, arXiv:2003.13942arXiv preprintBoxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan Adeli, and Juan Carlos Niebles. Spatio-temporal graph for video captioning with knowledge distillation. arXiv preprint arXiv:2003.13942, 2020.</p>
<p>Neural graph matching networks for fewshot 3d action recognition. Michelle Guo, Edward Chou, De-An Huang, Shuran Song, Serena Yeung, Li Fei-Fei, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Michelle Guo, Edward Chou, De-An Huang, Shuran Song, Serena Yeung, and Li Fei-Fei. Neural graph matching networks for fewshot 3d action recognition. In Proceedings of the European Conference on Computer Vision (ECCV), pages 653-669, 2018.</p>
<p>Unsupervised visuallinguistic reference resolution in instructional videos. De-An Huang, Joseph J Lim, Li Fei-Fei, Juan Carlos Niebles, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDe-An Huang, Joseph J Lim, Li Fei-Fei, and Juan Carlos Niebles. Unsupervised visual- linguistic reference resolution in instructional videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2183-2192, 2017.</p>
<p>Neural task graphs: Generalizing to unseen tasks from a single video demonstration. De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, Juan Carlos Niebles, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionDe-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li Fei-Fei, Silvio Savarese, and Juan Carlos Niebles. Neural task graphs: Generalizing to unseen tasks from a single video demonstration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8565-8574, 2019.</p>
<p>Neural graph evolution: Towards efficient automatic robot design. Tingwu Wang, Yuhao Zhou, Sanja Fidler, Jimmy Ba, arXiv:1906.05370arXiv preprintTingwu Wang, Yuhao Zhou, Sanja Fidler, and Jimmy Ba. Neural graph evolution: Towards efficient automatic robot design. arXiv preprint arXiv:1906.05370, 2019.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, Proceedings of the fourteenth international conference on artificial intelligence and statistics. the fourteenth international conference on artificial intelligence and statisticsStéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 627-635, 2011.</p>
<p>Scheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Advances in Neural Information Processing Systems. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171-1179, 2015.</p>
<p>Professor forcing: A new algorithm for training recurrent networks. Alex M Lamb, Anirudh Goyal Alias Parth, Ying Goyal, Saizheng Zhang, Zhang, C Aaron, Yoshua Courville, Bengio, Advances In Neural Information Processing Systems. Alex M Lamb, Anirudh Goyal Alias Parth Goyal, Ying Zhang, Saizheng Zhang, Aaron C Courville, and Yoshua Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pages 4601-4609, 2016.</p>
<p>Optimal completion distillation for sequence learning. Sara Sabour, William Chan, Mohammad Norouzi, arXiv:1810.01398arXiv preprintSara Sabour, William Chan, and Mohammad Norouzi. Optimal completion distillation for sequence learning. arXiv preprint arXiv:1810.01398, 2018.</p>
<p>Learning to forget: Continual prediction with lstm. Jürgen Felix A Gers, Fred Schmidhuber, Cummins, Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. Learning to forget: Continual prediction with lstm. 1999.</p>
<p>General evaluation for instruction conditioned navigation using dynamic time warping. Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, Jason Baldridge, arXiv:1907.05446arXiv preprintGabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General eval- uation for instruction conditioned navigation using dynamic time warping. arXiv preprint arXiv:1907.05446, 2019.</p>
<p>Hierarchical graph representation learning with differentiable pooling. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, Jure Leskovec, Advances in neural information processing systems. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. In Advances in neural information processing systems, pages 4800-4810, 2018.</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprintAngel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionXin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6629-6638, 2019.</p>
<p>Improving vision-and-language navigation with image-text pairs from the web. Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, Dhruv Batra, arXiv:2004.14973arXiv preprintArjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improving vision-and-language navigation with image-text pairs from the web. arXiv preprint arXiv:2004.14973, 2020.</p>
<p>Vision-language navigation with self-supervised auxiliary reasoning tasks. Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang, arXiv:1911.07883arXiv preprintFengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. arXiv preprint arXiv:1911.07883, 2019.</p>
<p>Learning to navigate unseen environments: Back translation with environmental dropout. Licheng Hao Tan, Mohit Yu, Bansal, arXiv:1904.04195arXiv preprintHao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. arXiv preprint arXiv:1904.04195, 2019.</p>
<p>Multi-view learning for vision-and-language navigation. Qiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui, Jianfeng Gao, Yejin Choi, Noah A Smith, arXiv:2003.00857arXiv preprintQiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui, Jianfeng Gao, Yejin Choi, and Noah A Smith. Multi-view learning for vision-and-language navigation. arXiv preprint arXiv:2003.00857, 2020.</p>
<p>Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, Yejin Choi, arXiv:1909.02244Robust navigation with language pretraining and stochastic sampling. arXiv preprintXiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. arXiv preprint arXiv:1909.02244, 2019.</p>
<p>Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation. Federico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, Rita Cucchiara, arXiv:1911.12377arXiv preprintFederico Landi, Lorenzo Baraldi, Marcella Cornia, Massimiliano Corsini, and Rita Cucchiara. Perceive, transform, and act: Multi-modal attention networks for vision-and-language naviga- tion. arXiv preprint arXiv:1911.12377, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
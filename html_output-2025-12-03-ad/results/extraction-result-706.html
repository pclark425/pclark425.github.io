<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-706 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-706</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-706</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-4674781</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1709.06560v1.pdf" target="_blank">Deep Reinforcement Learning that Matters</a></p>
                <p><strong>Paper Abstract:</strong> In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to maintaining this rapid progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results difficult to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines, and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field, by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e706.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e706.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hyperparameter mismatch (architecture/activations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperparameter mismatch: network architecture and activation function differences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Changes in network size and activation functions (e.g., (64,64) vs (400,300); tanh vs ReLU vs LeakyReLU) produce large, inconsistent effects on RL baseline performance across algorithms and environments, causing misalignment between paper descriptions and code used in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>policy-gradient experimental pipeline (MuJoCo / OpenAI Gym baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental RL system using policy-gradient algorithms (TRPO, PPO, DDPG, ACKTR) trained and evaluated on MuJoCo continuous control tasks via different codebases.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / reported hyperparameter table</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>baseline codebase implementations (OpenAI Baselines, rllab, original code)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers or method descriptions state network architectures/activations as a default or omit ranges, but actual codebases or experiments use different architectures or activations (or omit which variant was used). These differences change learning dynamics (e.g., KL divergence of updates, stability) and can require retuning other hyperparameters (learning rate, clipping) to obtain similar results.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture / hyperparameters</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility experiments comparing performance across architecture/activation permutations; cross-codebase experiment runs</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Measured final average returns ± standard error across 5 trials after 2M samples (Tables of final evaluation), KL-divergence comparisons of parameter updates (reported 33.52x greater KL for large network vs small), and visualization of learning curves across architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large and inconsistent: architecture/activation changes produced substantial changes in final returns and learning behaviour across algorithms and environments (tables/figures show multi-hundred to multi-thousand point differences in returns for some settings); large-networks increased KL of updates by ~33.5x, indicating different optimization dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common in surveyed literature; many works use differing architectures/activations without consistent reporting (paper's literature review shows widespread inconsistency).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insufficiently detailed reporting of hyperparameter choices and their ranges; papers often omit architecture/activation specifics or assume defaults; interactions between hyperparameters are not documented.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Report full hyperparameter settings, include code and default configs; run hyperparameter search to match baseline performance; report sensitivity analyses over architectures; prefer hyperparameter-agnostic algorithm designs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Qualitative: publishing code and full hyperparameters allows matching reported performance; experiments show retuning other hyperparameters (e.g., learning rate, clipping) can compensate for architecture changes, improving reproducibility (no single numeric effectiveness reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep reinforcement learning / machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e706.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e706.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward scaling mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward scaling and rescaling mismatch between description and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reward rescaling (multiplicative scaling or clipping) described in methods or prior work is sometimes applied inconsistently or not reported in code, causing large performance differences (including catastrophic failures) especially for off-policy methods like DDPG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DDPG-based experimental setup (MuJoCo environments)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Off-policy actor-critic system (DDPG) where reward scale influences gradient magnitudes and value estimates during training.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / prior-work descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / baseline implementations</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>hyperparameter mismatch (reward scaling) / incomplete specification</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers and prior implementations report using reward rescaling (e.g., scale σ) but either do not report the exact value used or apply different scaling in baseline implementations; this leads to inconsistent behavior across codebases and environments (some reward scales cause failure to learn).</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / hyperparameters (reward preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>controlled experiments varying reward scale (with and without layer normalization) and observing learning curves and final performance</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Learning curves for DDPG across reward scales (examples: tested scales include 1e-4 .. 100), observation of failures below σ = 1e-2 in an example, and comparison of final returns across rescaling settings (figures and supplemental material).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Large: reward rescaling altered DDPG performance dramatically; certain small scales caused failure to learn (example: failure below σ = 1e-2), and different scales yielded substantially different final returns across environments. Interaction with layer normalization further changed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Reported as used in several recent works; tuning reward scale is common for DDPG baselines (paper cites multiple works applying rescaling), but reporting is inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Papers/prior works do not consistently report reward preprocessing or default scaling; implicit assumptions about reward magnitudes and unstated preprocessing lead to mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly report reward scaling/preprocessing in publications and code; conduct and report sensitivity analyses over reward scale; consider principled adaptive rescaling methods (cite van Hasselt et al. 2016).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: reporting and retuning reward scale avoids some failures; adaptive rescaling methods are proposed as more principled solutions but require further research (no definitive quantitative improvement provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep reinforcement learning / continuous control</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e706.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e706.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-seed & averaging bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random seed selection and improper averaging (top-N selection) bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Selecting particular random seeds or averaging too few trials (or reporting top-N trials) can produce misleading performance claims; different seed partitions produce statistically different learning distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>experimental evaluation protocol (trial averaging over random seeds)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation procedure that aggregates results across multiple independent runs initialized with different RNG seeds and reports mean/other summary statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reported evaluation protocol in papers / methods section</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>experiment scripts / result aggregation code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / selection bias</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers often report averages over a small number of trials or select top-N runs without clearly specifying selection criteria; this misaligns natural-language reported metrics with what an experimenter would obtain running the provided code over a random seed set.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / experimental protocol</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>reproducibility experiment: ran 10 trials with same hyperparameters and split into two groups of 5, then compared averaged learning curves</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Statistical tests across runs: 2-sample t-test across training distributions produced t = -9.0916, p = 0.0016 for a HalfCheetah example; bootstrap/power analyses used to estimate required sample sizes and percent significant under lifts.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantial: averaging different splits of runs produced statistically different distributions; top-N reporting or small N (<5) can misrepresent algorithm performance and lead to false claims of improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not uncommon: literature review shows many works report small numbers of trials or top-N selection (examples include top-2/3 or N < 5 in several cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Practical constraints (compute/time) leading to few trials, cultural norms in reporting, and omission of exact trial-selection procedures in papers.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Run and report many independent trials over different seeds; report seed selection policy; use significance testing, bootstrap confidence intervals, and power analysis to justify sample sizes; avoid reporting only top-N runs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: statistical testing, bootstrap confidence intervals, and power analysis reveal when sample sizes are insufficient and reduce false positives; the paper demonstrates how these methods identify where more trials are needed (no single numeric threshold prescribed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep reinforcement learning / experimental statistics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e706.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e706.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codebase implementation differences</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codebase implementation differences between published descriptions and baseline code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different publicly available implementations of the same algorithm (e.g., TRPO, DDPG across OpenAI Baselines, rllab, original authors' code) contain implementation-level differences that materially change performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>baseline algorithm codebases (OpenAI Baselines, rllab, original authors)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multiple independent implementations of canonical RL algorithms used as baselines for comparison in new research.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>algorithm specification in papers vs available code repositories/implementations</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>library code / baseline codebases / experiment scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithm variant / implementation detail mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Public codebases vary in implementation details (optimizers, default hyperparameters, evaluation code, value-function approximator choices, layer norms, partial vs full-trajectory evaluation) that are not fully enumerated in method descriptions, producing divergent empirical results even when papers state identical hyperparameters.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training procedure / implementation internals / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Cross-codebase experimental comparison using identical hyperparameter settings across implementations and measuring resulting performance differences (figures and tables compare codebases).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Final average returns ± SE after fixed training (2M samples) across codebases, direct learning-curve comparisons across implementations (Figures 34,35), and tabulated comparisons against reported literature results (Tables 5 and 6).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Dramatic: measured large differences in final performance across codebases for the same nominal algorithm and hyperparameters; authors state 'implementation differences ... can have dramatic impacts on performance.'</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Frequent: multiple widely-used baseline implementations exist and are commonly used interchangeably in literature despite differences; paper documents several examples (TRPO, DDPG across rllab, OpenAI baselines, original code).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omitted implementation details in papers, differing defaults in repositories, differences in evaluation logic, and implicit pre/post-processing steps not described in text.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Publish and package code with papers, enumerate implementation details and defaults, validate that new baseline code reproduces original baseline results before comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Practical: matching codebases and publishing code reduces ambiguity and wasted effort; the paper shows reproducibility improves when implementations are available and matched, though exact quantitative gains depend on the case.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep reinforcement learning / software engineering for experiments</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e706.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e706.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation protocol mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation protocol mismatch: online vs policy-optimization views and partial vs full-trajectory evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers may describe an evaluation protocol (e.g., offline evaluation of a fixed policy across full trajectories) but code or baseline implementations sometimes perform online/partial-trajectory evaluation or aggregate across different policies, leading to non-equivalent reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>evaluation pipeline / metric computation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedures for computing policy performance (average cumulative reward over runs, maximum reward over timesteps, partial trajectory evaluations) in RL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>experimental protocol / evaluation description in methods or supplemental</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts / experiment code</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / ambiguous description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper highlights differences between an 'online view' (evaluations during learning possibly aggregating partial trajectories across changing policies) and a 'policy optimization view' (offline evaluation of a single target policy over full task trajectories). Some baseline code (e.g., DDPG baseline) evaluated partial trajectories across several policies by default, contrasting with offline full-trajectory evaluations described in methods, causing mismatch in reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / evaluation procedure</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual inspection of baseline implementations and modification of evaluation routines (authors modified DDPG evaluation to use 10 full trajectories per epoch) and comparison of resulting metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Compare learning curves and final reported returns under different evaluation modes (partial vs full trajectories), and note differences in reported averages and maxes; no single numeric metric universal, but differences visible in plotted curves and tabulated results.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Meaningful: evaluation-mode differences change reported algorithm performance and can bias comparisons if one method is evaluated more favorably (e.g., partial-traj evaluation may be optimistic or not reflect stable policy performance).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in at least some popular baseline implementations (authors found default DDPG evaluation used partial trajectories); literature lacks consistent convention.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Ambiguous or underspecified evaluation protocols in papers and differing default practices in code; authors sometimes assume a common understanding of 'evaluation' without detailing online vs offline distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly state evaluation view and procedures in papers; use consistent evaluation (e.g., offline evaluation over full trajectories), adapt baseline code to match described evaluation, and provide scripts that reproduce evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: making evaluation explicit and aligning code with described protocol removes a source of discrepancy; authors demonstrate changed metrics after modifying the DDPG evaluation procedure (no single numeric improvement reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>reinforcement learning / experimental methodology</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e706.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e706.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Omitted implementation details</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Omitted implementation and reporting details in natural-language descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Papers often omit low-level implementation choices (optimizers, layer normalization, exact value-function parametrization, batch sizes, evaluation frequency, random-seed policies) which leads to misalignment between the written description and the actual code used to produce results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>experimental reporting / publication artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The prose and supplementary material that describe experimental setup and algorithmic details in research papers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / supplemental / hyperparameter tables</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>baseline code / experiment configuration files</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing preprocessing step / ambiguous description</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The paper documents multiple instances where implementation-level choices are absent or underspecified in publications (e.g., reward clipping/rescaling, layer-norm usage, optimizer defaults, value-function approximator differences), making it impossible to faithfully reproduce results from the text alone.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>throughout pipeline: preprocessing, model, optimization, evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reproducibility attempts, literature survey, and comparison of reported hyperparameters across related works and codebases; authors cross-checked implementations and observed undocumented differences.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative assessment supported by quantitative discrepancies when reproducing (differences in final returns, inconsistent ability to match reported baselines), and summary tables showing inconsistent hyperparameter reporting across papers.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>High: omitting such details contributes to failures to reproduce reported results and to large variance across replications; the paper emphasizes wasted effort and misinterpretation arising from these omissions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread: the paper's literature review and tables show inconsistent and incomplete reporting is common across many works surveyed.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Cultural norms of concise method descriptions, page limits, assumptions about defaults, and not releasing full code/configuration alongside publications.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require publication of identifiable experiment code and full configuration; include exhaustive hyperparameter tables and evaluation scripts in supplement or repository; use standardized reporting checklists.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective in principle: authors recommend and demonstrate that publishing code and configurations would reduce wasted effort and improve reproducibility; the paper reports improved clarity where code/configs are available (no single numeric metric provided).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning / empirical RL</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Reinforcement Learning that Matters', 'publication_date_yy_mm': '2017-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Benchmarking deep reinforcement learning for continuous control <em>(Rating: 2)</em></li>
                <li>Reproducibility of benchmarked deep reinforcement learning tasks for continuous control <em>(Rating: 2)</em></li>
                <li>Continuous control with deep reinforcement learning <em>(Rating: 1)</em></li>
                <li>Trust region policy optimization <em>(Rating: 1)</em></li>
                <li>Proximal policy optimization algorithms <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-706",
    "paper_id": "paper-4674781",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Hyperparameter mismatch (architecture/activations)",
            "name_full": "Hyperparameter mismatch: network architecture and activation function differences",
            "brief_description": "Changes in network size and activation functions (e.g., (64,64) vs (400,300); tanh vs ReLU vs LeakyReLU) produce large, inconsistent effects on RL baseline performance across algorithms and environments, causing misalignment between paper descriptions and code used in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "policy-gradient experimental pipeline (MuJoCo / OpenAI Gym baselines)",
            "system_description": "Experimental RL system using policy-gradient algorithms (TRPO, PPO, DDPG, ACKTR) trained and evaluated on MuJoCo continuous control tasks via different codebases.",
            "nl_description_type": "research paper methods section / reported hyperparameter table",
            "code_implementation_type": "baseline codebase implementations (OpenAI Baselines, rllab, original code)",
            "gap_type": "hyperparameter mismatch",
            "gap_description": "Papers or method descriptions state network architectures/activations as a default or omit ranges, but actual codebases or experiments use different architectures or activations (or omit which variant was used). These differences change learning dynamics (e.g., KL divergence of updates, stability) and can require retuning other hyperparameters (learning rate, clipping) to obtain similar results.",
            "gap_location": "model architecture / hyperparameters",
            "detection_method": "reproducibility experiments comparing performance across architecture/activation permutations; cross-codebase experiment runs",
            "measurement_method": "Measured final average returns ± standard error across 5 trials after 2M samples (Tables of final evaluation), KL-divergence comparisons of parameter updates (reported 33.52x greater KL for large network vs small), and visualization of learning curves across architectures.",
            "impact_on_results": "Large and inconsistent: architecture/activation changes produced substantial changes in final returns and learning behaviour across algorithms and environments (tables/figures show multi-hundred to multi-thousand point differences in returns for some settings); large-networks increased KL of updates by ~33.5x, indicating different optimization dynamics.",
            "frequency_or_prevalence": "Common in surveyed literature; many works use differing architectures/activations without consistent reporting (paper's literature review shows widespread inconsistency).",
            "root_cause": "Insufficiently detailed reporting of hyperparameter choices and their ranges; papers often omit architecture/activation specifics or assume defaults; interactions between hyperparameters are not documented.",
            "mitigation_approach": "Report full hyperparameter settings, include code and default configs; run hyperparameter search to match baseline performance; report sensitivity analyses over architectures; prefer hyperparameter-agnostic algorithm designs.",
            "mitigation_effectiveness": "Qualitative: publishing code and full hyperparameters allows matching reported performance; experiments show retuning other hyperparameters (e.g., learning rate, clipping) can compensate for architecture changes, improving reproducibility (no single numeric effectiveness reported).",
            "domain_or_field": "deep reinforcement learning / machine learning",
            "reproducibility_impact": true,
            "uuid": "e706.0",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Reward scaling mismatch",
            "name_full": "Reward scaling and rescaling mismatch between description and implementation",
            "brief_description": "Reward rescaling (multiplicative scaling or clipping) described in methods or prior work is sometimes applied inconsistently or not reported in code, causing large performance differences (including catastrophic failures) especially for off-policy methods like DDPG.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DDPG-based experimental setup (MuJoCo environments)",
            "system_description": "Off-policy actor-critic system (DDPG) where reward scale influences gradient magnitudes and value estimates during training.",
            "nl_description_type": "research paper methods section / prior-work descriptions",
            "code_implementation_type": "experiment scripts / baseline implementations",
            "gap_type": "hyperparameter mismatch (reward scaling) / incomplete specification",
            "gap_description": "Papers and prior implementations report using reward rescaling (e.g., scale σ) but either do not report the exact value used or apply different scaling in baseline implementations; this leads to inconsistent behavior across codebases and environments (some reward scales cause failure to learn).",
            "gap_location": "training procedure / hyperparameters (reward preprocessing)",
            "detection_method": "controlled experiments varying reward scale (with and without layer normalization) and observing learning curves and final performance",
            "measurement_method": "Learning curves for DDPG across reward scales (examples: tested scales include 1e-4 .. 100), observation of failures below σ = 1e-2 in an example, and comparison of final returns across rescaling settings (figures and supplemental material).",
            "impact_on_results": "Large: reward rescaling altered DDPG performance dramatically; certain small scales caused failure to learn (example: failure below σ = 1e-2), and different scales yielded substantially different final returns across environments. Interaction with layer normalization further changed outcomes.",
            "frequency_or_prevalence": "Reported as used in several recent works; tuning reward scale is common for DDPG baselines (paper cites multiple works applying rescaling), but reporting is inconsistent.",
            "root_cause": "Papers/prior works do not consistently report reward preprocessing or default scaling; implicit assumptions about reward magnitudes and unstated preprocessing lead to mismatches.",
            "mitigation_approach": "Explicitly report reward scaling/preprocessing in publications and code; conduct and report sensitivity analyses over reward scale; consider principled adaptive rescaling methods (cite van Hasselt et al. 2016).",
            "mitigation_effectiveness": "Partially effective: reporting and retuning reward scale avoids some failures; adaptive rescaling methods are proposed as more principled solutions but require further research (no definitive quantitative improvement provided in this paper).",
            "domain_or_field": "deep reinforcement learning / continuous control",
            "reproducibility_impact": true,
            "uuid": "e706.1",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Random-seed & averaging bias",
            "name_full": "Random seed selection and improper averaging (top-N selection) bias",
            "brief_description": "Selecting particular random seeds or averaging too few trials (or reporting top-N trials) can produce misleading performance claims; different seed partitions produce statistically different learning distributions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "experimental evaluation protocol (trial averaging over random seeds)",
            "system_description": "Evaluation procedure that aggregates results across multiple independent runs initialized with different RNG seeds and reports mean/other summary statistics.",
            "nl_description_type": "reported evaluation protocol in papers / methods section",
            "code_implementation_type": "experiment scripts / result aggregation code",
            "gap_type": "incomplete specification / selection bias",
            "gap_description": "Papers often report averages over a small number of trials or select top-N runs without clearly specifying selection criteria; this misaligns natural-language reported metrics with what an experimenter would obtain running the provided code over a random seed set.",
            "gap_location": "evaluation metrics / experimental protocol",
            "detection_method": "reproducibility experiment: ran 10 trials with same hyperparameters and split into two groups of 5, then compared averaged learning curves",
            "measurement_method": "Statistical tests across runs: 2-sample t-test across training distributions produced t = -9.0916, p = 0.0016 for a HalfCheetah example; bootstrap/power analyses used to estimate required sample sizes and percent significant under lifts.",
            "impact_on_results": "Substantial: averaging different splits of runs produced statistically different distributions; top-N reporting or small N (&lt;5) can misrepresent algorithm performance and lead to false claims of improvement.",
            "frequency_or_prevalence": "Not uncommon: literature review shows many works report small numbers of trials or top-N selection (examples include top-2/3 or N &lt; 5 in several cited works).",
            "root_cause": "Practical constraints (compute/time) leading to few trials, cultural norms in reporting, and omission of exact trial-selection procedures in papers.",
            "mitigation_approach": "Run and report many independent trials over different seeds; report seed selection policy; use significance testing, bootstrap confidence intervals, and power analysis to justify sample sizes; avoid reporting only top-N runs.",
            "mitigation_effectiveness": "Effective: statistical testing, bootstrap confidence intervals, and power analysis reveal when sample sizes are insufficient and reduce false positives; the paper demonstrates how these methods identify where more trials are needed (no single numeric threshold prescribed).",
            "domain_or_field": "deep reinforcement learning / experimental statistics",
            "reproducibility_impact": true,
            "uuid": "e706.2",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Codebase implementation differences",
            "name_full": "Codebase implementation differences between published descriptions and baseline code",
            "brief_description": "Different publicly available implementations of the same algorithm (e.g., TRPO, DDPG across OpenAI Baselines, rllab, original authors' code) contain implementation-level differences that materially change performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "baseline algorithm codebases (OpenAI Baselines, rllab, original authors)",
            "system_description": "Multiple independent implementations of canonical RL algorithms used as baselines for comparison in new research.",
            "nl_description_type": "algorithm specification in papers vs available code repositories/implementations",
            "code_implementation_type": "library code / baseline codebases / experiment scripts",
            "gap_type": "different algorithm variant / implementation detail mismatch",
            "gap_description": "Public codebases vary in implementation details (optimizers, default hyperparameters, evaluation code, value-function approximator choices, layer norms, partial vs full-trajectory evaluation) that are not fully enumerated in method descriptions, producing divergent empirical results even when papers state identical hyperparameters.",
            "gap_location": "training procedure / implementation internals / evaluation",
            "detection_method": "Cross-codebase experimental comparison using identical hyperparameter settings across implementations and measuring resulting performance differences (figures and tables compare codebases).",
            "measurement_method": "Final average returns ± SE after fixed training (2M samples) across codebases, direct learning-curve comparisons across implementations (Figures 34,35), and tabulated comparisons against reported literature results (Tables 5 and 6).",
            "impact_on_results": "Dramatic: measured large differences in final performance across codebases for the same nominal algorithm and hyperparameters; authors state 'implementation differences ... can have dramatic impacts on performance.'",
            "frequency_or_prevalence": "Frequent: multiple widely-used baseline implementations exist and are commonly used interchangeably in literature despite differences; paper documents several examples (TRPO, DDPG across rllab, OpenAI baselines, original code).",
            "root_cause": "Omitted implementation details in papers, differing defaults in repositories, differences in evaluation logic, and implicit pre/post-processing steps not described in text.",
            "mitigation_approach": "Publish and package code with papers, enumerate implementation details and defaults, validate that new baseline code reproduces original baseline results before comparisons.",
            "mitigation_effectiveness": "Practical: matching codebases and publishing code reduces ambiguity and wasted effort; the paper shows reproducibility improves when implementations are available and matched, though exact quantitative gains depend on the case.",
            "domain_or_field": "deep reinforcement learning / software engineering for experiments",
            "reproducibility_impact": true,
            "uuid": "e706.3",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Evaluation protocol mismatch",
            "name_full": "Evaluation protocol mismatch: online vs policy-optimization views and partial vs full-trajectory evaluation",
            "brief_description": "Papers may describe an evaluation protocol (e.g., offline evaluation of a fixed policy across full trajectories) but code or baseline implementations sometimes perform online/partial-trajectory evaluation or aggregate across different policies, leading to non-equivalent reported metrics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "evaluation pipeline / metric computation",
            "system_description": "Procedures for computing policy performance (average cumulative reward over runs, maximum reward over timesteps, partial trajectory evaluations) in RL experiments.",
            "nl_description_type": "experimental protocol / evaluation description in methods or supplemental",
            "code_implementation_type": "evaluation scripts / experiment code",
            "gap_type": "incomplete specification / ambiguous description",
            "gap_description": "The paper highlights differences between an 'online view' (evaluations during learning possibly aggregating partial trajectories across changing policies) and a 'policy optimization view' (offline evaluation of a single target policy over full task trajectories). Some baseline code (e.g., DDPG baseline) evaluated partial trajectories across several policies by default, contrasting with offline full-trajectory evaluations described in methods, causing mismatch in reported metrics.",
            "gap_location": "evaluation metrics / evaluation procedure",
            "detection_method": "Manual inspection of baseline implementations and modification of evaluation routines (authors modified DDPG evaluation to use 10 full trajectories per epoch) and comparison of resulting metrics.",
            "measurement_method": "Compare learning curves and final reported returns under different evaluation modes (partial vs full trajectories), and note differences in reported averages and maxes; no single numeric metric universal, but differences visible in plotted curves and tabulated results.",
            "impact_on_results": "Meaningful: evaluation-mode differences change reported algorithm performance and can bias comparisons if one method is evaluated more favorably (e.g., partial-traj evaluation may be optimistic or not reflect stable policy performance).",
            "frequency_or_prevalence": "Observed in at least some popular baseline implementations (authors found default DDPG evaluation used partial trajectories); literature lacks consistent convention.",
            "root_cause": "Ambiguous or underspecified evaluation protocols in papers and differing default practices in code; authors sometimes assume a common understanding of 'evaluation' without detailing online vs offline distinctions.",
            "mitigation_approach": "Explicitly state evaluation view and procedures in papers; use consistent evaluation (e.g., offline evaluation over full trajectories), adapt baseline code to match described evaluation, and provide scripts that reproduce evaluation.",
            "mitigation_effectiveness": "Effective: making evaluation explicit and aligning code with described protocol removes a source of discrepancy; authors demonstrate changed metrics after modifying the DDPG evaluation procedure (no single numeric improvement reported).",
            "domain_or_field": "reinforcement learning / experimental methodology",
            "reproducibility_impact": true,
            "uuid": "e706.4",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        },
        {
            "name_short": "Omitted implementation details",
            "name_full": "Omitted implementation and reporting details in natural-language descriptions",
            "brief_description": "Papers often omit low-level implementation choices (optimizers, layer normalization, exact value-function parametrization, batch sizes, evaluation frequency, random-seed policies) which leads to misalignment between the written description and the actual code used to produce results.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "experimental reporting / publication artifacts",
            "system_description": "The prose and supplementary material that describe experimental setup and algorithmic details in research papers.",
            "nl_description_type": "research paper methods section / supplemental / hyperparameter tables",
            "code_implementation_type": "baseline code / experiment configuration files",
            "gap_type": "incomplete specification / missing preprocessing step / ambiguous description",
            "gap_description": "The paper documents multiple instances where implementation-level choices are absent or underspecified in publications (e.g., reward clipping/rescaling, layer-norm usage, optimizer defaults, value-function approximator differences), making it impossible to faithfully reproduce results from the text alone.",
            "gap_location": "throughout pipeline: preprocessing, model, optimization, evaluation",
            "detection_method": "Reproducibility attempts, literature survey, and comparison of reported hyperparameters across related works and codebases; authors cross-checked implementations and observed undocumented differences.",
            "measurement_method": "Qualitative assessment supported by quantitative discrepancies when reproducing (differences in final returns, inconsistent ability to match reported baselines), and summary tables showing inconsistent hyperparameter reporting across papers.",
            "impact_on_results": "High: omitting such details contributes to failures to reproduce reported results and to large variance across replications; the paper emphasizes wasted effort and misinterpretation arising from these omissions.",
            "frequency_or_prevalence": "Widespread: the paper's literature review and tables show inconsistent and incomplete reporting is common across many works surveyed.",
            "root_cause": "Cultural norms of concise method descriptions, page limits, assumptions about defaults, and not releasing full code/configuration alongside publications.",
            "mitigation_approach": "Require publication of identifiable experiment code and full configuration; include exhaustive hyperparameter tables and evaluation scripts in supplement or repository; use standardized reporting checklists.",
            "mitigation_effectiveness": "Effective in principle: authors recommend and demonstrate that publishing code and configurations would reduce wasted effort and improve reproducibility; the paper reports improved clarity where code/configs are available (no single numeric metric provided).",
            "domain_or_field": "machine learning / empirical RL",
            "reproducibility_impact": true,
            "uuid": "e706.5",
            "source_info": {
                "paper_title": "Deep Reinforcement Learning that Matters",
                "publication_date_yy_mm": "2017-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Benchmarking deep reinforcement learning for continuous control",
            "rating": 2
        },
        {
            "paper_title": "Reproducibility of benchmarked deep reinforcement learning tasks for continuous control",
            "rating": 2
        },
        {
            "paper_title": "Continuous control with deep reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Trust region policy optimization",
            "rating": 1
        },
        {
            "paper_title": "Proximal policy optimization algorithms",
            "rating": 1
        }
    ],
    "cost": 0.01577275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Deep Reinforcement Learning that Matters</p>
<p>Peter Henderson 
McGill University
MontrealCanada</p>
<p>Riashat Islam riashat.islam@mail.mcgill.caphbachma@microsoft.com 
McGill University
MontrealCanada</p>
<p>Microsoft Maluuba
MontrealCanada</p>
<p>Philip Bachman 
Microsoft Maluuba
MontrealCanada</p>
<p>Joelle Pineau jpineau@cs.mcgill.ca 
McGill University
MontrealCanada</p>
<p>Doina Precup dprecup@cs.mcgill.ca 
McGill University
MontrealCanada</p>
<p>David Meger dmeger@cim.mcgill.ca 
McGill University
MontrealCanada</p>
<p>Deep Reinforcement Learning that Matters</p>
<p>In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to maintaining this rapid progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results difficult to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines, and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field, by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.</p>
<p>Introduction</p>
<p>Reinforcement learning (RL) is the study of how an agent can interact with its environment to learn a policy which maximizes expected returns over its lifespan. Recently, RL has experienced dramatic growth in attention and interest due to promising results in areas like: controlling continuous systems as in robotics (Lillicrap et al. 2015a), playing Go (Silver et al. 2016), Atari (Mnih et al. 2013), and competitive video games (Vinyals et al. 2017;Silva and Chaimowicz 2017). Figure 1 illustrates growth of the field through the number of publications per year.</p>
<p>To maintain rapid progress in RL research, it is important that existing works can be easily reproduced and compared to accurately judge improvements offered by novel methods. However, reproducing deep RL results is seldom straightforward, and the literature reports a wide range of results for the same baseline algorithms (Islam et al. 2017). Reproducibility can be affected by extrinsic factors (e.g. hyperparameters or codebases) and intrinsic factors (e.g. effects of random seeds or environment properties). We investigate these sources of variance in reported results through * These two authors contributed equally Shown are the number of RL-related publications (y-axis) per year (x-axis). Results were scraped from Google Scholar searches. a representative set of experiments. For clarity, we focus our investigation on policy gradient (PG) methods in continuous control. Policy gradient methods with neural network function approximators have been particularly successful in continuous control (Schulman et al. 2015a;Lillicrap et al. 2015b), and are competitive with value-based methods in discrete settings. We note that the diversity of metrics and lack of significance testing in the RL literature creates the potential for misleading reporting of results. We demonstrate possible benefits of significance testing using techniques common in machine learning and statistics.</p>
<p>In (Duan et al. 2016), the authors implement and benchmark several RL algorithms to provide the community with baseline implementations. However, while the question of reproducibility and good experimental practice has been examined in related fields (Wagstaff 2012;Boulesteix, Lauer, and Eugster 2013;Stodden, Leisch, and Peng 2014;Bouckaert and Frank 2004;Bouckaert 2004;Vaughan and Wawerla 2012), to the best of our knowledge this is the first work to address this important question in the context of RL.</p>
<p>In each section of our experimental analysis, we pose questions regarding key factors affecting reproducibility. We find that there are numerous sources of non-determinism when reproducing and comparing RL algorithms. To this end, we show that detailed experimental procedure is extremely important. Based on our experimental methods, we conclude with possible recommendations, lines of investigation, and points of discussion for future works to ensure that deep reinforcement learning is reproducible and continues to matter.</p>
<p>Technical Background</p>
<p>This work focuses on several model-free policy gradient algorithms with published baseline codebases, which have been often used in the literature as baselines for comparison against novel methods. We experiment with the Trust Region Policy Optimization (TRPO) (Schulman et al. 2015a), Deep Deterministic Policy Gradients (DDPG) (Lillicrap et al. 2015b), Proximal Policy Optimization (PPO) (Schulman et al. 2017) and the Actor Critic using Kronecker-Factored Trust Region (ACKTR) (Wu et al. 2017). These methods have shown promising results in continuous control MuJoCo domain tasks (Todorov, Erez, and Tassa 2012) from OpenAI Gym (Brockman et al. 2016). These methods generally optimize ρ(θ, s 0 ) = E π θ [ ∞ t=0 γ t r(s t )], using the policy gradient theorem: δρ(θ,s0) δθ = s µ π θ (s|s 0 ) a δπ θ (a|s) δθ Q π θ (s, a), where µ π θ (s|s 0 ) = ∞ t=0 γ t P (s t = s|s 0 ) (Sutton et al. 2000). TRPO (Schulman et al. 2015a) and PPO (Schulman et al. 2017) use constrained updates and advantage estimation to perform this update, reformulating the optimization as: max θ E t π θ (at|st) π θ old (at|st) A t subject to E t [KL [π θ old (·|s t ), π θ (·|s t )]] ≤ δ where A t is the generalized advantage function (Schulman et al. 2015b). TRPO uses conjugate gradient descent with a KL constraint to optimize this, while PPO reformulates the constraint as a penalty (or clipping objective). DDPG and ACKTR use actor-critic methods which estimate Q(s, a) and optimize a policy that maximizes the Q-function based on Monte-Carlo rollouts. DDPG does this using deterministic policies, while ACKTR uses Kronecketer-factored Trust Regions to ensure stability with stochastic policies.</p>
<p>Experimental Analysis</p>
<p>We pose several questions about the factors affecting reproducibility of state-of-the-art PG methods. We use a set of experiments in an attempt to gain insight into the questions posed. In particular, we investigate the effects of: specific hyperparameters on algorithm performance if not properly tuned; random seeds and the number of averaged experiment trials; specific environment characteristics; differences in algorithm performance due to stochastic environments, codebase differences with most other factors held constant. For most of our experiments, except for those comparing codebases, we generally use the OpenAI Baselines 1 implementations of the following algorithms: ACKTR (Wu et al. 2017), PPO (Schulman et al. 2017), DDPG (Plappert et al. 2017), TRPO (Schulman et al. 2017) 2 . We use the Hopper-v1 and HalfCheetah-v1 MuJoCo (Todorov, Erez, and Tassa 2012) environments from OpenAI Gym (Brockman et al. 2016). These two environments provide contrasting dynamics (the former being more unstable).</p>
<p>To ensure fairness we run 5 experiment trials, each with a different preset random seed, for each evaluation (all exper-1 https://www.github.com/openai/baselines 2 Specific details can be found in the supplemental and code can be found at: https://github.com/Breakend/ DeepReinforcementLearningThatMatters iments use the same set of random seeds). In all cases, we highlight important results here, with a full detailed experimental setup and additional learning curves included in the supplemental material. Unless otherwise mentioned, we use default configurations as follows, while modifying hyperparameters under investigation.</p>
<p>In all cases we use multilayer perceptron function approximators. We denote the hidden layer sizes and activations as (N, M, activation). For default settings, we varying hyperparameters under investigation one at a time. For DDPG on both the actor and critic we use a network structure of: (64, 64, ReLU). For TRPO and PPO, we use: (64,64,TanH). For ACKTR, we use (64, 64, TanH) for the actor and (64, 64, Elu) for the critic.</p>
<p>Hyperparameters</p>
<p>What is the magnitude of the effect hyperparameter settings can have on baseline performance?</p>
<p>Tuned hyperparameters play a large role in eliciting the best results for many algorithms. However, the choice of optimal hyperparameter configuration is often not consistent in related literature, and the range of values considered is often not reported 3 . Furthermore, poor hyperparameter selection can be detrimental to a fair comparison against baseline algorithms. Here, we investigate several aspects of hyperparameter selection on performance.</p>
<p>Network Architecture</p>
<p>How does the choice of network architecture for the policy and value function approximation affect performance?</p>
<p>In (Islam et al. 2017), it is shown that policy network architecture can significantly impact results in both TRPO and DDPG. Furthermore, certain activation functions such as Rectified Linear Unit (ReLU) have been shown to cause worsened learning performance due to the "dying relu" problem (Xu et al. 2015). As such, we examine both network architecture and activation functions for both policy and value function approximators. In the literature, similar lines of investigation have shown the differences in performance when comparing linear approximators, RBFs, and neural networks (Rajeswaran et al. 2017). Tables 1 and 2 summarize the final evaluation performance of all architectural variations after training on 2M samples (with each sample being a step in the environment). All learning curves and details on setup can be found in the supplemental material. We vary hyperaparameters one at a time, while using a default setting for all others. We investigate three multilayer perceptron (MLP) architectures commonly seen in the literature: (64, 64), (100,50,25), and (400, 300). Furthermore, we vary the activation functions of both the value and policy networks across tanh, ReLU, and Leaky ReLU activations.</p>
<p>Results Figure 2 shows how drastically performance can be affected by simple changes to the policy or value network activations. We find that usually ReLU or Leaky ReLU activations perform the best across environments and algorithms. 3 A sampled literature review can be found in the supplemental.  The effects are not consistent across algorithms or environments. This inconsistency demonstrates how interconnected network architecture is to algorithm methodology. For example, using a large network with PPO may require tweaking other hyperparameters such as the trust region clipping or learning rate to compensate for the architectural change 4 . This intricate interplay of hyperparameters is one of the reasons reproducing current policy gradient methods is so difficult. It is exceedingly important to choose an appropriate architecture for proper baseline results. This also suggests a need for hyperparameter agnostic algorithms-that is algorithms that incorporate hyperparameter adaptation as part of the design-such that fair comparisons can be made without concern about improper settings for the task at hand.</p>
<p>Reward Scale</p>
<p>How can the reward scale affect results? Why is reward rescaling used?</p>
<p>Reward rescaling has been used in several recent works (Duan et al. 2016;Gu et al. 2016) to improve results for DDPG. This involves simply multiplying the rewards (r = rσ) generated from an environment before training function approximators on the rescaled rewards. Typically, these works report using a reward scale ofσ = 1 −1 . In Atari domains, this is akin to clipping the rewards to [0, 1]. By intuition, in gradient based methods (as used in most deep RL) a large and sparse output scale can result in problems regarding saturation and inefficiency in learning (LeCun et al. 2012;Glorot and Bengio 2010;Vincent, de Brébisson, and Bouthillier 2015). Therefore clipping or rescaling rewards compresses the space of estimated expected returns in action value function based methods such as DDPG. We run a set of experiments using reward rescaling in DDPG (with and without layer normalization) for insights into how this aspect affects performance.</p>
<p>Results Our analysis shows that reward rescaling can have a large effect (full experiment results can be found in the supplemental material), but results were inconsistent across environments and scaling values. Figure 3 shows one such example where reward rescaling affects results, causing a failure to learn in small settings belowσ = 1 −2 . In particular, layer normalization changes how the rescaling factor affects results, suggesting that these impacts are due to the use of deep networks and gradient-based methods. With the value function approximator tracking a moving target distribution, this can potentially affect learning in unstable environments where a deep Q-value function approximator is used. Furthermore, some environments may have untuned reward scales (e.g. the HumanoidStandup-v1 of OpenAI gym which can reach rewards in the scale of millions). Therefore, we suggest that this hyperparameter has the potential to have a large impact if address properly. Rather than rescaling rewards in some environments, a more principled approach should be taken to address this. An initial foray into this problem is made in (van Hasselt et al. 2016) where the authors adaptively rescale reward targets with normalized stochastic gradient, but further research is needed.</p>
<p>Random Seeds and Trials</p>
<p>Can random seeds drastically alter performance? Can one skew results by averaging an improper number of trials?</p>
<p>A major concern with deep RL is the variance in results due to environment stochasticity or stochasticity due to random number generators in the learning process (e.g. weight initialization). As such, even averaging several learning results    together across totally different random seeds can lead to the reporting of misleading results. We highlight this in the form of an experiment. Results We perform 10 experiment trials, for the same hyperparameter configuration, only varying the random seed across all 10 trials. We then split the trials into two sets of 5 and average these two groupings together. As shown in Figure 5, we find that the performance of algorithms can be drastically different. We demonstrate that the variance between runs is enough to create statistically different distributions just from varying random seeds. Unfortunately, in recent reported results, it is not uncommon for the top-N trials to be selected from among several trials (Wu et al. 2017;Mnih et al. 2016) or averaged over only small number of trials (N &lt; 5) (Gu et al. 2017;Wu et al. 2017). Our experiment with random seeds shows that this can be potentially misleading. Particularly for Half-Cheetah, it is possible to get learning curves that do not fall within the same distribution at all, just by averaging different runs with the same hyperparameters, but different random seeds. While there can be no specific number of trials specified as a recommendation, it is possible that power analysis methods can be used to give a general idea to this extent as we will discuss later and more investigation is needed to answer this open problem. HalfCheetah-v1, with same hyperparameter configurations, averaged over two splits of 5 different random seeds. In this case, the average 2-sample t-test across entire training distribution resulted in t = −9.0916, p = 0.0016.</p>
<p>Environments</p>
<p>How do the environment properties affect variability in reported RL algorithmic performance?</p>
<p>To assess how the choice of evaluation environment can affect the presented results, we use our aforementioned default set of hyperparameters across our chosen testbed of algorithms and investigate how well each algorithm performs across an extended suite of continuous control tasks. For these experiments, we use the following environments from OpenAI Gym: Hopper-v1, HalfCheetah-v1, Swimmer-v1 and Walker2d-v1. The choice of environment often plays an important role in demonstrating how well a new proposed algorithm performs against baselines. In continuous control tasks, often the environments have random stochasticity, shortened trajectories, or different dynamic properties. We demonstrate that as a result of these differences, algorithm performance can vary across environments and the best performing algorithm across all environments is not always clear. Thus it is increasingly important to present results a wide range of environments and not only pick those which show a novel work outperforming other methods.</p>
<p>Results As shown in Figure 4, in environments with stable dynamics (e.g. HalfCheetah-v1), DDPG outperforms all other algorithsm. However, as dynamics become more unstable (e.g. in Hopper-v1) performance gains rapidly diminish. As DDPG is an off-policy method, exploration noise can cause sudden failures in unstable environments. Therefore, learning a proper Q-value estimation of expected returns is difficult, particularly since many exploratory paths will result in failure. Since failures in such tasks are characterized by shortened trajectories, a local optimum in this case would be simply to survive until the maximum length of the trajectory (corresponding to one thousand timesteps and similar reward due to a survival bonus in the case of Hopper-v1). As can be seen in Figure 4, DDPG with Hopper does exactly this. This is a clear example of where, showing only the favourable and stable HalfCheetah in when reporting DDPG-based experiments would be unfair.</p>
<p>Furthermore, let us consider the Swimmer-v1 environment shown in Figure 4. In Swimmer-v1, TRPO significantly outperforms all other algorithms. Due to the dynamics of the water-like environment, a local optimum for the system is to curl up and flail without proper swimming. However, this corresponds to a trajectory return of ∼ 130. Due to this effect of reaching a local optimum, learning curves can indicate successful optimization of the policy over time, when in reality the returns achieved are not qualitatively representative of learning the desired behaviour 5 . Therefore, it is important to show not only rewards but demonstrations of the learned policy in action. Without understanding what the evaluation returns indicate, it is possible that misleading results can be reported which in reality only optimize local optima rather than reaching the desired behaviour.</p>
<p>Codebases</p>
<p>Are commonly used baseline implementations comparable?</p>
<p>In many cases, authors implement their own versions of baseline algorithms to compare against. We investigate the Ope-nAI baselines implementation of TRPO as used in (Schulman et al. 2017), the original TRPO code (Schulman et al. 2015a), and the rllab (Duan et al. 2016) Tensorflow implementation of TRPO. We also compare the rllab Theano (Duan et al. 2016), rllabplusplus (Gu et al. 2016), and OpenAI baselines (Plappert et al. 2017) implementations of DDPG. Our goal is to draw attention to the variance due to implementation details across algorithms. We run a subset of our architecture experiments as with the OpenAI baselines implementations using the same hyperparameters as in those experiments 6 .  Results We find that implementation differences which are often not reflected in publications can have dramatic impacts on performance. This can be seen for our final evaluation performance after training on 2M samples in Tables 1  and 2, as well as a sample comparison in Figure 6. This demonstrates the necessity that implementation details be enumerated, codebases packaged with publications, and that performance of baseline experiments in novel works matches the original baseline publication code.</p>
<p>Reporting Evaluation Metrics</p>
<p>In this section we analyse some of the evaluation metrics commonly used in the policy gradients literature. In practice, policy gradient algorithms are often evaluated by simply looking into plots of average cumulative reward, and more recently, on maximum reward achieved over a fixed number of timesteps. Due to the unstable nature of several algorithms, simply looking at maximum returns or even average returns without investigating the confidence bounds of the metrics can be misleading. Average return across N trials is a typical metric long seen in RL literature. Alone, this may not provide a clear picture of an algorithm's range of performance. However, when combined with confidence intervals, this may be adequate to make an informed decision given a large enough N . We investigate using the bootstrap and significance testing as in ML (Kohavi and others 1995;Bouckaert and Frank 2004;Nadeau and Bengio 2000) to evaluate algorithm performance.</p>
<p>Online View vs. Policy Optimization An important distinction when reporting results is the online learning view versus the policy optimization view of RL. In the online view, include different optimizers for the value function baseline and only a linear approximator being provided in the Tensorflow rllab implementation for the value function. Leaky ReLU activations are left out to narrow the experiment scope. an agent will optimize the returns across the entire learning process and there is not necessarily an end to the agent's trajectory. In this view, evaluations can use the average cumulative rewards across the entire learning process (balancing exploration and exploitation) as in (Hofer and Gimbert 2016) or can possibly use offline evaluation as in (Mandel et al. 2016). The alternate view corresponds to policy optimization, where evaluation is performed using a target policy in an offline manner. In the policy optimization view it is important to run evaluations across the entire length of the task trajectory with a single target policy to determine the average returns that the target can obtain. We focus on evaluation methods for the policy optimization view (with offline evaluation), but the same principles can be applied to the online view.</p>
<p>Confidence Bounds The sample bootstrap has been a popular method to gain insight into a population distribution from a smaller sample (Efron and Tibshirani 1994). Bootstrap methods are particularly popular for A/B testing, and we can borrow some ideas from this field. Generally a bootstrap estimator is obtained by resampling with replacement many times to generate a statistically relevant mean and confidence bound. Using this technique, we can gain insight into what is the 95% confidence interval of the results from our section on environments. Table 3 shows the bootstrap mean and 95% confidence bounds on our environments experiments. Confidence intervals can vary wildly between algorithms and environments. We find that TRPO and PPO are the most stable with small confidence bounds from the bootstrap. In cases where confidence bounds are exceedingly large, it may be necessary to run more trials (i.e. increase the sample size).</p>
<p>Power Analysis Another method to determine if the sample size must be increased is bootstrap power analysis (Tufféry 2011;Yuan and Hayashi 2003). If we use our sample and give it some uniform lift (for example, scaling uniformly by 1.25), we can run many bootstrap simulations and determine what percentage of the simulation results in statistically significant values with the lift. If there is a small percentage of significant values, a larger sample size is needed (more trials must be run). We do this across all environment experiment trial runs and find that indeed in more unstable results, the bootstrap power percentage learns towards insignificant results in the lift experiment, whereas in stable trials (e.g. TRPO on Hopper-v1) with a small sample size, the lift experiment shows that no more trials are needed to generate significant comparisons. These results are provided in the supplemental material.</p>
<p>Significance An important factor when deciding on an RL algorithm to use is the significance of the reported gains based on a given metric. Several works have investigated the use of significance metrics to assess the reliability of reported evaluation metrics in ML. However, few works in reinforcement learning assess the significance of reported metrics. Based on our experimental results which indicate that algorithm performance can vary wildly based simply on perturbations of random seeds, it is clear that some metric is necessary for assessing the significance of algorithm performance gains and the confidence of reported metrics. While more research and investigation is needed to determin the best metrics for assessing RL algorithms, we investigate an initial set of metrics based on results from ML.</p>
<p>In supervised learning, k-fold t-test, corrected resampled ttest and other significance metrics have been discussed when comparing machine learning results (Bouckaert and Frank 2004;Nadeau and Bengio 2000). However the assumptions pertaining to the underlying data with corrected metrics do not necessarily apply RL. Indeed, further work is needed to investigate proper corrected significance tests for RL. Nonetheless, we explore several significance measures which give insight into whether a novel algorithm is truly performing as the state-of-the-art. We consider the simple 2-sample ttest (sorting all final evaluation returns across N random trials with different random seeds); the Kolmogorov-Smirnov test (Wilcox 2005); and bootstrap percent differences with 95% confidence intervals. All calculated metrics can be found in the supplemental. Generally, we find that the significance values match up to what is to be expected. Take for example, comparing Walker2d-v1 performance of ACKTR vs. DDPG. ACKTR performs slightly better, but this performance is not significant due to the overlapping confidence intervals of the two: t = 1.03, p = 0.334, KS = 0.40, p = 0.697, bootstrapped percent difference 44.47% (-80.62%, 111.72%).</p>
<p>Discussion and Conclusion</p>
<p>Through experimental methods focusing on PG methods for continuous control, we investigate problems with reproducibility in RL. We find that both intrinsic (e.g. random seeds, environment properties) and extrinsic sources (e.g. hyperparameters, codebases) of non-determinism can contribute to difficulties in reproducing baseline algorithms. Moreover, we find that highly varied results due to intrinsic sources bolster the need for using proper significance analysis. We propose several such methods and show their values on a subset of our experiments.</p>
<p>What recommendations can we draw from our experiments?</p>
<p>Based on our experimental results and investigations, we can provide some general recommendations. Hyperparameters can have drastically different effects across algorithms and environments so it is important to find the working set at least matches the original reported performance of baseline algorithm through standard optimization and hyperparameter search. Similarly, a new baseline codebase should match the original codebase results if available. Overall, due to the high variance across trials and random seeds of reinforcement learning algorithms, many trials must be run with different random seeds when comparing performance. Unless random seed selection is explicitly part of the algorithm, averaging multiple runs over different random seeds gives insight into the population distribution of the algorithm performance on an environment. Similarly, due to these effects, it is important to perform proper significance testing to determine if the higher average returns are in fact representative of better performance. We highlight several forms of significance testing and find that they give generally expected results when taking confidence intervals into consideration. Furthermore, we demonstrate that bootstrapping and power analysis as possible ways to gain insight into the number of trial runs necessary to make an informed decision about the significance of algorithm performance gains. In general, however, the most important step to reproducibility is to report all hyperparameters, implementation details, experimental setup, and evaluation methods for both baseline comparison methods and novel work. Without the publication of implementations and related details, wasted effort on reproducing state-of-theart works will plague the community and slow down progress.</p>
<p>What are possible future lines of investigation?</p>
<p>Due to the drastic effects of hyperparameters (particularly reward scaling), an important line of future investigation is in building hyperparameter agnostic algorithms. Such an approach would ensure that there is no unfairness introduced from external sources when comparing algorithms agnostic to parameters such as reward scale, batch size, or network structure. Furthermore, while we investigate an initial set of significance metrics here, they may not be the best fit for comparing RL algorithms. Several works have begun investigating policy evaluation methods for the purposes of safe RL (Thomas and Brunskill 2016;Thomas, Theocharous, and Ghavamzadeh 2015), but further work is needed in significance testing and statistical analysis. Similar lines of investigation to (Nadeau and Bengio 2000;Bouckaert and Frank 2004) would be helpful to determine the best methods for evaluating performance gain significance.</p>
<p>How can we ensure that deep RL matters?</p>
<p>We discuss many different factors affecting reproducibility of RL algorithms. However, RL algorithms center around optimizing a pre-specified reward function. These reward functions are analogous to cost functions in machine learning, where RL algorithms would be the optimization method. Since certain algorithms are particularly susceptible to reward scale and environment dynamics, as we have demonstrated, perhaps more emphasis should be placed on the applicability of RL algorithms to real-world tasks as cost-optimization methods are. This is something that is addressed for machine learning in (Wagstaff 2012) and may warrant more discussion for RL. That is, since there is often no clear winner among all benchmark environments, perhaps recommended areas of application should be demonstrated along with benchmark environment results when presenting a new algorithm. Maybe new methods should be answering the question: in what setting would this work be useful? As a community, we must not only ensure controlled and reproducible results with fair comparisons, but we must also consider what are the best ways to demonstrate that RL continues to matter.</p>
<p>Supplemental Material</p>
<p>In this supplemental material, we include a detailed review of experiment configurations of related work with policy gradient methods in continuous control MuJoCo (Todorov, Erez, and Tassa 2012) environment tasks from OpenAI Gym (Brockman et al. 2016). We include a detailed list of the hyperparameters and reported metrics typically used in policy gradient literature in deep RL. We also include all our experimental results, with baseline algorithms DDPG (Lillicrap et al. 2015b), TRPO (Schulman et al. 2015a), PPO (Schulman et al. 2017) and ACKTR (Wu et al. 2017)) as discussed in the paper. Our experimental results include figures with different hyperparameters (network architectures, activation functions) to highlight the differences this can have across algorithms and environments. Finally, as discussed in the paper, we include detailed lists of significance metrics we suggested, and show why it maybe useful to use these metrics for evaluating deep RL algorithms.</p>
<p>Literature Reviews Hyperparameters</p>
<p>In this section, we include a list of hyperparameters that are reported in related literature, as shown in figure 4. Our analysis shows that often there is no consistency in the type of network architectures and activation functions that are used in related literature. As shown in the paper and from our experimental results in later sections, we find, however, that these hyperparameters can have a significant effect in the performance of algorithms across benchmark environments typically used. </p>
<p>Reported Results on Benchmarked Environments</p>
<p>We then demonstrate how experimental reported results, on two different environments (HalfCheetah-v1 and Hopper-v1) can vary across different related work that uses these algorithms for baseline comparison. We further show the results we get, using the same hyperparameter configuration, but using two different codebase implementations (note that these implementations are often used as baseline codebase to develop algorithms). We highlight that, depending on the codebase used, experimental results can vary significantly.   </p>
<p>Reported Evaluation Metrics in Related Work</p>
<p>In  </p>
<p>Experimental Setup</p>
<p>In this section, we show detailed analysis of our experimental results, using same hyperparameter configurations used in related work. Experimental results are included for the OpenAI Gym (Brockman et al. 2016) Hopper-v1 and HalfCheetah-v1 environments, using the policy gradient algorithms including DDPG, TRPO, PPO and ACKTR. Our experiments are done using the available codebase from OpenAI rllab (Duan et al. 2016) and OpenAI Baselines. Each of our experiments are performed over 5 experimental trials with different random seeds, and results averaged over all trials. Unless explicitly specified as otherwise (such as in hyperparameter modifications where we alter a hyperparameter under investigation), hyperparameters were as follows. </p>
<p>Modifications to Baseline Implementations</p>
<p>To ensure fairness of comparison, we make several modifications to the existing implementations. First, we change evaluation in DDPG (Plappert et al. 2017) such that during evaluation at the end of an epoch, 10 full trajectories are evaluated. In the current implementation, only a partial trajectory is evaluated immediately after training such that a full trajectory will be evaluated across several different policies, this corresponds more closely to the online view of evaluation, while we take a policy optimization view when evaluating algorithms.</p>
<p>Hyperparameters : Network Structures and Activation Functions</p>
<p>Below, we examine the significance of the network configurations used for the non-linear function approximators in policy gradient methods. Several related work have used different sets of network configurations (network sizes and activation functions). We use the reported network configurations from other works, and demonstrate the significance of careful fine tuning that is required. We demonstrate results using the network activation functions, ReLU, TanH and Leaky ReLU, where most papers use ReLU and TanH as activation functions without detailed reporting of the effect of these activation functions. We analyse the signifcance of using different activations in the policy and action value networks. Previously, we included a detailed table showing average reward with standard error obtained for each of the hyperparameter configurations. In the results below, we show detailed results of how each of these policy gradient algorithms are affected by the choice of the network configuration.    rs=1e-4 rs=1e-3 rs=1e-2 rs=1e-1 rs=1 rs=10 rs=100 Figure 21: DDPG reward rescaling on HalfCheetah-v1, with and without layer norm.</p>
<p>Proximal Policy Optimization (PPO)</p>
<p>Several related work (Gu et al. 2016;Duan et al. 2016) have often reported that for DDPG the reward scaling parameter often needs to be fine-tuned for stabilizing the performance of DDPG. It can make a significant impact in performance of DDPG based on the choice of environment. We examine several reward scaling parameters and demonstrate the effect this parameter can have on the stability and performance of DDPG, based on the HalfCheetah and Hopper environments. Our experiment results, as demonstrated in Figure 21 and 20, show that the reward scaling parameter indeed can have a significant impact on performance. Our results show that, very small or negligible reward scaling parameter can significantly detriment the performance of DDPG across all environments. Furthermore, a scaling parameter of 10 or 1 often performs good. Based on our analysis, we suggest that every time DDPG is reported as a baseline algorithm for comparison, the reward scaling parameter should be fine-tuned, specific to the algorithm.  We run batch size experiments using the original TRPO code (Schulman et al. 2015a) and the OpenAI baselines code (Schulman et al. 2017). These results can be found in Experiment results in Figure 22 and Figure 23, show that for both HalfCheetah-v1 and Hopper-v1 environments, a batch size of 1024 for TRPO performs best, while perform degrades consecutively as the batch size is increased.</p>
<p>Random Seeds</p>
<p>To determine much random seeds can affect results, we run 10 trials total on two environments using the default previously described settings usign the (Gu et al. 2016) implementation of DDPG and the (Duan et al. 2016) version of TRPO. We divide our trials random into 2 partitions and plot them in Figures 24 and Fig 25. As can be seen, statistically different distributions can be attained just from the random seeds with the same exact hyperparameters. As we will discuss later, bootstrapping off of the sample can give an idea for how drastic this effect will be, though too small a bootstrap will still not give concrete enough results.  </p>
<p>Choice of Benchmark Continuous Control Environment</p>
<p>We previously demonstrated that the performance of policy gradient algorithms can be highly biased based on the choice of the environment. In this section, we include further results examining the impact the choice of environment can have. We show that no single algorithm can perform consistenly better in all environments. This is often unlike the results we see with DQN networks in Atari domains, where results can often be demonstrated across a wide range of Atari games. Our results, for example, shows that while TRPO can perform significantly better than other algorithms on the Swimmer environment, it may perform quite poorly n the HalfCheetah environment, and marginally better on the Hopper environment compared to PPO. We demonstrate our results using the OpenAI MuJoCo Gym environments including Hopper, HalfCheetah, Swimmer and Walker environments. It is notable to see the varying performance these algorithms can have even in this small set of environment domains. The choice of reporting algorithm performance results can therefore often be biased based on the algorithm designer's experience with these environments.    Often in related literature, there is different baseline codebase people use for implementation of algorithms. One such example is for the TRPO algorithm. It is a commonly used policy gradient method for continuous control tasks, and there exists several implementations from OpenAI Baselines (Plappert et al. 2017), OpenAI rllab (Duan et al. 2016) and the original TRPO codebase (Schulman et al. 2015a). In this section, we perform an analysis of the impact the choice of algorithm codebase can have on the performance. Figures 27 and 28 summarizes our results with TRPO policy network and value networks, using the original TRPO codebase from (Schulman et al. 2015a). Figure 29 shows the results using the rllab implementation of TRPO using the same hyperparameters as our default experiments aforementioned. Note, we use a linear function approximator rather than a neural network due to the fact that the Tensorflow implementation of OpenAI rllab doesn't provide anything else. We note that this is commonly used in other works (Duan et al. 2016;Stadie, Abbeel, and Sutskever 2017), but may cause differences in performance. Furthermore, we leave out our value function network experiments due to this.   Figure 35 shows a comparison of the TRPO implementations using the default hyperparamters as specified earlier in the supplemental. Note, the exception is that we use a larger batch size for rllab and original TRPO code of 20k samples per batch, as optimized in a second set of experiments. Figure 30 and 31 show the same network experiments for DDPG with the rllab++ code (Gu et al. 2016). We can then compare the performance of the algorithm across 3 codebases (keeping all hyperparameters constant at the defaults), this can be seen in Figure 34.</p>
<p>Significance</p>
<p>Our full results from significance testing with difference metrics can be found in Table 9 and Table 10. Our bootstrap mean and confidence intervals can be found in Table 13. Bootstrap power analysis can be found in Table 14. To performance significance testing, we use our 5 sample trials to generate a bootstrap with 10k bootstraps. From this confidence intervals can be obtained. For the t-test and KS-test, the average returns from the 5 trials are sorted and compared using the normal 2-sample versions of these tests. Scipy ( https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ks_2samp. html, https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) and Facebook Boostrapped (https://github.com/facebookincubator/bootstrapped) are used for the KS test, t-test, and bootstrap analysis. For power analysis, we attempt to determine if a sample is enough to game the significance of a 25% lift. This is commonly used in A/B testing (Tufféry 2011).          </p>
<p>Figure 1 :
1Growth of published reinforcement learning papers.</p>
<p>Figure 3 :
3DDPG reward rescaling on HalfCheetah-v1, with and without layer norm.</p>
<p>Figure 5 :
5Two different TRPO experiment runs on</p>
<p>Figure 6 :
6TRPO codebase comparison using our default set of hyperparameters (as used in other experiments).</p>
<p>•-
DDPG Policy Network: (64, relu, 64, relu, tanh); Q Network (64, relu, 64, relu, linear) -Normalized observations with running mean filter -Actor LR: 1e − 4; Critic LR: 1e − 3 -Reward Scale: 1.0 -Noise type: O-U 0.2 -Soft target update τ = .01 γ = 0.995 batch size = 128 -Critic L2 reg 1e − 2 • PPO -Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, tanh, 64, tanh, linear) -Normalized observations with running mean filter -Timesteps per batch 2048 clip param = 0.2 entropy coeff = 0.0 -Optimizer epochs per iteration = 10 -Optimizer step size 3e − 4 -Optimizer batch size 64 -Discount γ = 0.995, GAE λ = 0.97 learning rate schedule is constant • TRPO -Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, tanh, 64, tanh, linear) -Normalized observations with running mean filter -Timesteps per batch 5000 max KL=0.01 -Conjugate gradient iterations = 20 -CG damping = 0.1 -VF Iterations = 5 -VF Batch Size = 64 -VF Step Size = 1e − 3 entropy coeff = 0.0 -Discount γ = 0.995, GAE λ = 0.97 • ACKTR -Policy Network: (64, tanh, 64, tanh, Linear) + Standard Deviation variable; Value Network (64, elu, 64, elu, linear) -Normalized observations with running mean filter -Timesteps per batch 2500 desired KL = .002 -Discount γ = 0.995, GAE λ = 0.97</p>
<p>Figure 7 :Figure 8 :Figure 9 :
789PPO Policy and Value Network activation Experiment results in Figure 7, 8, and 9 in this section show the effect of the policy network structures and activation functions in the Proximal Policy Optimization (PPO) algorithm. PPO Policy Network structure PPO Value Network structure</p>
<p>Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :
131415161718ACKTR Value Network ActivationWe then similarly, show the significance of these hyperparameters in the ACKTR algorithm. Our results show that the value network structure can have a significant effect on the performance of ACKTR algorithm.Trust Region Policy Optimization (TRPO) TRPO Policy Network structure TRPO TRPO Policy and Value Network activation TRPO Policy and Value Network activationInFigures 14, 15, 16, and 17 we show the effects of network structure on the OpenAI baselines implementation of TRPO. In this case, only the policy architecture seems to have a large effect on the performance of the algorithm's ability to learn. Policy or Actor Network Architecture experiments for DDPG on HalfCheetah and Hopper Environment We further analyze the actor and critic network configurations for use in DDPG. As in default configurations, we first use the ReLU activation function for policy networks, and examine the effect of different activations and network sizes for the critic networks. Similarly, keeping critic network configurations under default setting, we also examine the effect of actor network activation functions and network sizes.</p>
<p>Figure 19 :Figure 20 :
1920Significance of Value Function or Critic Network Activations for DDPG on HalfCheetah and Hopper EnvironmentReward Scaling Parameter in DDPG DDPG reward rescaling on Hopper-v1, with and without layer norm. v1(DDPG, Reward Scale, No Layer Norm)    </p>
<p>Figure 23 :
23TRPO (Schulman et al. 2017) baselines code batch size experiments.</p>
<p>Figure 25 :
25Two different DDPG experiment runs, with same hyperparameter configurations, averaged over two splits of 5 different random seeds.</p>
<p>Figure 26 :
26Comparing Policy Gradients across various environmentsCodebasesWe include a detailed analysis of performance comparison, with different network structures and activations, based on the choice of the algorithm implementation codebase.</p>
<p>Figure 27 :Figure 28 :
2728TRPO Policy and Value Network structure TRPO Policy and Value Network activations.</p>
<p>Figure 33 :
33DDPG rllab Policy and Value Network activations.</p>
<p>Figure 35 :
35TRPO codebase comparison using our default set of hyperparameters (as used in other experiments).</p>
<p>t
= 3.07, p = 0.015 KS = 0.80, p = 0.036 85.01 % (-31.02 %, 144.35 %) t = 1.02, p = 0.338 KS = 0.60, p = 0.209 28.07 % (-65.67 %, 71.71 %) t = −0.57, p = 0.582 KS = 0.40, p = 0.697 -4.75 % (-19.06 %, 10.02 %) -</p>
<p>Table 1 :
1Results for our policy architecture permutations across various implementations and algorithms. Final Average ±Standard Error across 5 trials of returns across the last 100 trajectories after 2M training samples. For ACKTR, we use elu 
activations instead of leaky relu. </p>
<p>Algorithm 
Environment 
400,300 
64,64 
100,50,25 
TanH 
ReLU 
LeakyReLU 
TRPO 
Hopper-v1 
3011 ± 171 2674 ± 227 2782 ± 120 2674 ± 227 
3104 ± 84 
-
(Schulman et al. 2015a) HalfCheetah-v1 
2355 ± 48 
1939 ± 140 1673 ± 148 1939 ± 140 
2281 ± 91 
-
TRPO 
Hopper-v1 
2909 ± 87 
2828 ± 70 
2812 ± 88 
2828 ± 70 
2829 ± 76 
3047 ± 68 
(Schulman et al. 2017) 
HalfCheetah-v1 
178 ± 242 
205 ± 256 
172 ± 257 
205 ± 256 
235 ± 260 
325 ± 208 
PPO 
Hopper-v1 
2704 ± 37 
2790 ± 62 
2969 ± 111 
2790 ± 62 
2687 ± 144 
2748 ± 77 
(Schulman et al. 2017) 
HalfCheetah-v1 1523 ± 297 2201 ± 323 1807 ± 309 2201 ± 323 
1288 ± 12 
1227 ± 462 
DDPG 
Hopper-v1 
1419 ± 312 1632 ± 458 1569 ± 453 
971 ± 137 
852 ± 143 
843 ± 160 
(Plappert et al. 2017) 
HalfCheetah-v1 5600 ± 601 4197 ± 606 4713 ± 374 3908 ± 293 4197 ± 606 
5324 ± 280 
DDPG 
Hopper-v1 
523 ± 248 
343 ± 34 
345 ± 44 
436 ± 48 
343 ± 34 
-
(Gu et al. 2016) 
HalfCheetah-v1 1373 ± 678 1717 ± 508 1868 ± 620 1128 ± 511 1717 ± 508 
-
DDPG 
Hopper-v1 
1208 ± 423 
394 ± 144 
380 ± 65 
354 ± 91 
394 ± 144 
-
(Duan et al. 2016) 
HalfCheetah-v1 
789 ± 91 
1095 ± 139 
988 ± 52 
1311 ± 271 1095 ± 139 
-
ACKTR 
Hopper-v1 
152 ± 47 
1930 ± 185 1589 ± 225 
691 ± 55 
500 ± 379 
1930 ± 185 
(Wu et al. 2017) 
HalfCheetah-v1 
518 ± 632 
3018 ± 386 2554 ± 219 2547 ± 172 3362 ± 682 
3018 ± 38 </p>
<p>Table 2 :
2Results for our value function (Q or V ) architecture permutations across various implementations and algorithms. Final 
Average ± Standard Error across 5 trials of returns across the last 100 trajectories after 2M training samples. For ACKTR, we 
use elu activations instead of leaky relu. </p>
<p>Figure 4: Performance of several policy gradient algorithms across benchmark MuJoCo environment suites </p>
<p>Environment 
DDPG 
ACKTR 
TRPO 
PPO 
HalfCheetah-v1 
5037 (3664, 6574) 
3888 (2288, 5131) 1254.5 (999, 1464) 3043 (1920, 4165) 
Hopper-v1 
1632.1 (607, 2370) 2546 (1875, 3217) 
2965 (2854, 3076) 
2715 (2589, 2847) 
Walker2d-v1 
1582 (901, 2174) 
2285 (1246, 3235) 
3072 (2957, 3183) 
2926 (2514, 3361) 
Swimmer-v1 
31 (21, 46) 
50 (42, 55) 
214 (141, 287) 
107 (101, 118) </p>
<p>Table 3 :
3Bootstrap mean and 95% confidence bounds for a subset of environment experiments. 10k bootstrap iterations and the pivotal method were used.</p>
<p>Table 4 :
4Evaluation Hyperparameters of baseline algorithms reported in related literatureRelated Work 
(Algorithm) </p>
<p>Policy 
Network </p>
<p>Policy 
Network 
Activation </p>
<p>Value 
Network </p>
<p>Value 
Network 
Activation </p>
<p>Reward 
Scaling </p>
<p>Batch 
Size </p>
<p>DDPG 
64x64 
ReLU 
64x64 
ReLU 
1.0 
128 
TRPO 
64x64 
TanH 
64x64 
TanH 
-
5k 
PPO 
64x64 
TanH 
64x64 
TanH 
-
2048 
ACKTR 
64x64 
TanH 
64x64 
ELU 
-
2500 
Q-Prop 
(DDPG) 
100x50x25 TanH 
100x100 
ReLU 
0.1 
64 </p>
<p>Q-Prop 
(TRPO) 
100x50x25 TanH 
100x100 
ReLU 
-
5k </p>
<p>IPG 
(TRPO) 
100x50x25 TanH 
100x100 
ReLU 
-
10k </p>
<p>Param Noise 
(DDPG) 
64x64 
ReLU 
64x64 
ReLU 
-
128 </p>
<p>Param Noise 
(TRPO) 
64x64 
TanH 
64x64 
TanH 
-
5k </p>
<p>Benchmarking 
(DDPG) 
400x300 
ReLU 
400x300 
ReLU 
0.1 
64 </p>
<p>Benchmarking 
(TRPO) 
100x50x25 TanH 
100x50x25 TanH 
-
25k </p>
<p>Table 5 :
5Comparison with Related Reported Results with Hopper EnvironmentEnvironment Metric 
rllab 
QProp IPG TRPO 
Our Results 
(rllab) </p>
<p>Our Results 
(Baselines) 
TRPO on 
Hopper 
Environment </p>
<p>Number of Iterations 500 
500 
500 500 
500 
500 
Average Return 
1183.3 -
-
-
2021.34 
2965.3 
Max Average Return -
2486 
3668.8 3229.1 
3034.4 </p>
<p>Table 6 :
6Comparison with Related Reported Results with HalfCheetah EnvironmentEnvironment Metric 
rllab 
QProp IPG 
TRPO 
Our Results 
(rllab) </p>
<p>Our Results 
(Baselines) 
TRPO on 
HalfCheetah 
Environment </p>
<p>Number of Iterations 500 
500 
500 
500 
500 
500 
Average Return 
1914.0 
-
-
3576.08 
1045.6 
Max Average Return -
4734 
2889 4855 
5197 
1045.6 </p>
<p>Work 
Number of Trials 
(Mnih et al. 2016) 
top-5 
(Schulman et al. 2017) 
3-9 
(Duan et al. 2016) 
5 (5) 
(Gu et al. 2017) 
3 
(Lillicrap et al. 2015b) 
5 
(Schulman et al. 2015a) 
5 
(Wu et al. 2017) 
top-2, top-3 </p>
<p>Table 7 :
7Number of trials reported during evaluation in various works.</p>
<p>table 8 we show the evaluation metrics, and reported results in further details across related work.</p>
<p>Table 8 :
8Reported Evaluation Metrics of baseline algorithms in related literatureRelated Work 
(Algorithm) 
Environments </p>
<p>Timesteps 
or Episodes 
or Iterations </p>
<p>Evaluation Metrics </p>
<p>Average 
Return </p>
<p>Max 
Return </p>
<p>Std 
Error </p>
<p>PPO 
HalfCheetah 
Hopper 
1M 
∼1800 
∼2200 
-
-</p>
<p>ACKTR 
HalfCheetah 
Hopper 
1M 
∼2400 
∼3500 
-
-</p>
<p>Q-Prop 
(DDPG) </p>
<p>HalfCheetah 
Hopper 
6k (eps) 
∼6000 
-</p>
<p>7490 
2604 </p>
<h2>-</h2>
<p>Q-Prop 
(TRPO) </p>
<p>HalfCheetah 
Hopper 
5k (timesteps) 
∼4000 
-</p>
<p>4734 
2486 </p>
<h2>-</h2>
<p>IPG 
(TRPO) </p>
<p>HalfCheetah 
Hopper 
10k (eps) 
∼3000 
-
2889 
-
-
Param Noise 
(DDPG) </p>
<p>HalfCheetah 
Hopper 
1M 
∼1800 
∼500 </p>
<h2>-</h2>
<h2>-</h2>
<p>Param Noise 
(TRPO) </p>
<p>HalfCheetah 
Hopper 
1M 
∼3900 
∼2400 </p>
<h2>-</h2>
<h2>-</h2>
<p>Benchmarking 
(DDPG) </p>
<p>HalfCheetah 
Hopper </p>
<p>500 iters 
(25k eps) 
∼2148 
∼267 </p>
<h2>-</h2>
<p>702 
43 
Benchmarking 
(TRPO) </p>
<p>HalfCheetah 
Hopper </p>
<p>500 iters 
(925k eps) 
∼1914 
∼1183 </p>
<h2>-</h2>
<p>150 
120 </p>
<p>Figure 31: DDPG rllab++ Policy and Value Network activations. Similarly, Figures 32 and 33 show the same network experiments for DDPG with the Theano implementation of rllab code (Duan et al. 2016).Figure 32: DDPG rllab Policy and Value Network structure HalfCheetah-v1 (DDPG, rllab, Policy Network Activation) Hopper-v1 (DDPG, rllab, Policy Network Activation)0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>−250 </p>
<p>0 </p>
<p>250 </p>
<p>500 </p>
<p>750 </p>
<p>1000 </p>
<p>1250 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (TRPO, rllab, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>1200 </p>
<p>1400 </p>
<p>Average Return </p>
<p>Hopper-v1 (TRPO, rllab, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>−250 </p>
<p>0 </p>
<p>250 </p>
<p>500 </p>
<p>750 </p>
<p>1000 </p>
<p>1250 </p>
<p>1500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (TRPO, rllab, Policy Network Activation) </p>
<p>tanh 
relu 
leaky relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>1200 </p>
<p>1400 </p>
<p>Average Return </p>
<p>Hopper-v1 (TRPO, rllab, Policy Network Activation) </p>
<p>tanh 
relu 
leaky relu </p>
<p>Figure 29: TRPO rllab Policy Structure and Activation </p>
<p>0.00 </p>
<p>0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>3000 </p>
<p>3500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab++, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>100 </p>
<p>200 </p>
<p>300 </p>
<p>400 </p>
<p>500 </p>
<p>600 </p>
<p>700 </p>
<p>800 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab++, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>1200 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab++, Value Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab++, Value Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>Figure 30: DDPG rllab++ Policy and Value Network structure </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>3000 </p>
<p>3500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab++, Policy Network Activation) </p>
<p>tanh 
relu 
leaky relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab++, Policy Network Activation) </p>
<p>tanh 
relu 
leaky </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab++, Value Network Activation) </p>
<p>tanh 
relu 
leaky relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab++, Value Network Activation) </p>
<p>tanh 
relu 
leaky relu </p>
<p>0.00 </p>
<p>0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>2500 </p>
<p>3000 </p>
<p>3500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>100 </p>
<p>200 </p>
<p>300 </p>
<p>400 </p>
<p>500 </p>
<p>600 </p>
<p>700 </p>
<p>800 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab, Policy Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>250 </p>
<p>500 </p>
<p>750 </p>
<p>1000 </p>
<p>1250 </p>
<p>1500 </p>
<p>1750 </p>
<p>2000 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab, Value Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab, Value Network Structure) </p>
<p>(64,64) </p>
<p>(100,50,25) </p>
<p>(400,300) </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>Average Return </p>
<p>tanh 
relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>Average Return </p>
<p>tanh 
relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, rllab, Value Network Activation) </p>
<p>tanh 
relu </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>200 </p>
<p>400 </p>
<p>600 </p>
<p>800 </p>
<p>1000 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, rllab, Value Network Activation) </p>
<p>tanh 
relu </p>
<p>Figure 34: DDPG codebase comparison using our default set of hyperparameters (as used in other experiments).0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>1000 </p>
<p>2000 </p>
<p>3000 </p>
<p>4000 </p>
<p>5000 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (DDPG, Codebase Comparison) </p>
<p>Duan 2016 
Gu 2016 
Plapper 2017 </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>0 </p>
<p>250 </p>
<p>500 </p>
<p>750 </p>
<p>1000 </p>
<p>1250 </p>
<p>1500 </p>
<p>1750 </p>
<p>Average Return </p>
<p>Hopper-v1 (DDPG, Codebase Comparison) </p>
<p>Duan 2016 
Gu 2016 
Plapper 2017 </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Timesteps </p>
<p>×10 6 </p>
<p>−500 </p>
<p>0 </p>
<p>500 </p>
<p>1000 </p>
<p>1500 </p>
<p>2000 </p>
<p>Average Return </p>
<p>HalfCheetah-v1 (TRPO, Codebase Comparison) </p>
<p>Schulman 2015 
Schulman 2017 
Duan 2016 </p>
<p>0.00 
0.25 
0.50 
0.75 
1.00 
1.25 
1.50 
1.75 
2.00 </p>
<p>Table 9 :
9HalfCheetah Significance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% confidence bounds.</p>
<p>Table 10 :
10Hopper Significance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% confidence bounds.</p>
<p>Table 11 :
11Walker2d Significance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% confidence bounds.</p>
<p>Table 12 :
12Swimmer Significance values and metrics for different algorithms. Rows in cells are: sorted 2-sample t-test, Kolmogorov-Smirnov test, bootstrap A/B comparison % difference with 95% confidence bounds.Environment 
DDPG 
ACKTR 
TRPO 
PPO 
HalfCheetah-v1 5037.26 (3664.11, 6574.01) 3888.85 (2288.13, 5131.96) 
1254.55 (999.52, 1464.86) 
3043.1 (1920.4, 4165.86) 
Hopper-v1 
1632.13 (607.98, 2370.21) 
2546.89 (1875.79, 3217.98) 2965.33 (2854.66, 3076.00) 2715.72 (2589.06, 2847.93) 
Walker2d-v1 
1582.04 (901.66, 2174.66) 
2285.49 (1246.00, 3235.96) 3072.97 (2957.94, 3183.10) 2926.92 (2514.83, 3361.43) 
Swimmer-v1 
31.92 (21.68, 46.23) 
50.22 (42.47, 55.37) 
214.69 (141.52, 287.92) 
107.88 (101.13, 118.56) </p>
<p>Table 13 :
13Envs bootstrap mean and 95% confidence boundsEnvironment 
DDPG 
ACKTR 
TRPO 
PPO </p>
<p>HalfCheetah-v1 </p>
<p>100.00 % 
0.00 % 
0.00 % </p>
<p>79.03 % 
11.53 % 
9.43 % </p>
<p>79.47 % 
20.53 % 
0.00 % </p>
<p>61.07 % 
10.50 % 
28.43 % </p>
<p>Hopper-v1 </p>
<p>60.90 % 
10.00 % 
29.10 % </p>
<p>79.60 % 
11.00 % 
9.40 % </p>
<p>0.00 % 
100.00 % 
0.00 % </p>
<p>0.00 % 
100.00 % 
0.00 % </p>
<p>Walker2d-v1 </p>
<p>89.50 % 
0.00 % 
10.50 % </p>
<p>60.33 % 
9.73 % 
29.93 % </p>
<p>0.00 % 
100.00 % 
0.00 % </p>
<p>59.80 % 
31.27 % 
8.93 % </p>
<p>Swimmer-v1 </p>
<p>89.97 % 
0.00 % 
10.03 % </p>
<p>59.90 % 
40.10 % 
0.00 % </p>
<p>89.47 % 
0.00 % 
10.53 % </p>
<p>40.27 % 
59.73 % 
0.00 % </p>
<p>Table 14 :
14Power Analysis for predicted significance of 25% lift. Rows in cells are: % insignificant simulations,% positive significant, % negative significant.
We find that the KL divergence of updates with the large network (400, 300) seen inFigure 2is on average 33.52 times higher than the KL divergence of updates with the (64, 64) network.</p>
<p>Evaluating the replicability of significance tests for comparing learning algorithms. R R Bouckaert, E Frank, PAKDD. SpringerBouckaert, R. R., and Frank, E. 2004. Evaluating the replicability of significance tests for comparing learning algorithms. In PAKDD, 3-12. Springer.</p>
<p>Estimating replicability of classifier learning experiments. R R Bouckaert, Proceedings of the 21st International Conference on Machine Learning (ICML). the 21st International Conference on Machine Learning (ICML)Bouckaert, R. R. 2004. Estimating replicability of classifier learning experiments. In Proceedings of the 21st International Conference on Machine Learning (ICML).</p>
<p>A plea for neutral comparison studies in computational sciences. A.-L Boulesteix, S Lauer, M J Eugster, PloS one. 8461562Boulesteix, A.-L.; Lauer, S.; and Eugster, M. J. 2013. A plea for neutral comparison studies in computational sciences. PloS one 8(4):e61562.</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.01540OpenAI gym. arXiv preprintBrockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI gym. arXiv preprint arXiv:1606.01540.</p>
<p>Benchmarking deep reinforcement learning for continuous control. Y Duan, X Chen, R Houthooft, J Schulman, P Abbeel, Proceedings of the 33rd International Conference on Machine Learning (ICML). the 33rd International Conference on Machine Learning (ICML)Duan, Y.; Chen, X.; Houthooft, R.; Schulman, J.; and Abbeel, P. 2016. Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning (ICML).</p>
<p>An introduction to the bootstrap. B Efron, R J Tibshirani, CRC pressEfron, B., and Tibshirani, R. J. 1994. An introduction to the boot- strap. CRC press.</p>
<p>Understanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsGlorot, X., and Bengio, Y. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 249-256.</p>
<p>. S Gu, T Lillicrap, Z Ghahramani, R E Turner, S Levine, Gu, S.; Lillicrap, T.; Ghahramani, Z.; Turner, R. E.; and Levine, S.</p>
<p>Sample-efficient policy gradient with an off-policy critic. Q-Prop, arXiv:1611.02247arXiv preprintQ-prop: Sample-efficient policy gradient with an off-policy critic. arXiv preprint arXiv:1611.02247.</p>
<p>Interpolated policy gradient: Merging onpolicy and off-policy gradient estimation for deep reinforcement learning. S Gu, T Lillicrap, Z Ghahramani, R E Turner, B Schölkopf, S Levine, arXiv:1706.00387arXiv preprintGu, S.; Lillicrap, T.; Ghahramani, Z.; Turner, R. E.; Schölkopf, B.; and Levine, S. 2017. Interpolated policy gradient: Merging on- policy and off-policy gradient estimation for deep reinforcement learning. arXiv preprint arXiv:1706.00387.</p>
<p>Online reinforcement learning for real-time exploration in continuous state and action markov decision processes. L Hofer, H Gimbert, arXiv:1612.03780arXiv preprintHofer, L., and Gimbert, H. 2016. Online reinforcement learning for real-time exploration in continuous state and action markov decision processes. arXiv preprint arXiv:1612.03780.</p>
<p>R Islam, P Henderson, M Gomrokchi, D Precup, arXiv:1708.04133Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprintIslam, R.; Henderson, P.; Gomrokchi, M.; and Precup, D. 2017. Reproducibility of benchmarked deep reinforcement learning tasks for continuous control. arXiv preprint arXiv:1708.04133.</p>
<p>A study of cross-validation and bootstrap for accuracy estimation and model selection. R Kohavi, IJCAI. 14Kohavi, R., et al. 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. In IJCAI, volume 14.</p>
<p>Efficient backprop. Y A Lecun, L Bottou, G B Orr, K.-R Müller, T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, T P Lillicrap, J J Hunt, A Pritzel, N Heess, T Erez, Y Tassa, D Silver, D Wierstra, arXiv:1509.02971arXiv:1509.02971Continuous control with deep reinforcement learning. SpringerarXiv preprintContinuous control with deep reinforcement learningLeCun, Y. A.; Bottou, L.; Orr, G. B.; and Müller, K.-R. 2012. Effi- cient backprop. In Neural Networks: Tricks of the Trade. Springer. Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D. 2015a. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D. 2015b. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.</p>
<p>Offline Evaluation of Online Reinforcement Learning Algorithms. T Mandel, Y.-E Liu, E Brunskill, Z Popovic, AAAI. Mandel, T.; Liu, Y.-E.; Brunskill, E.; and Popovic, Z. 2016. Offline Evaluation of Online Reinforcement Learning Algorithms. In AAAI.</p>
<p>V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.5602Playing atari with deep reinforcement learning. arXiv preprintMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M. 2013. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning. Mnih, V.; Badia, A. P.; Mirza, M.; Graves, A.; Lillicrap, T.; Harley, T.; Silver, D.; and Kavukcuoglu, K. 2016. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, 1928-1937.</p>
<p>Inference for the generalization error. C Nadeau, Y ; M Bengio, R Houthooft, P Dhariwal, S Sidor, R Y Chen, X Chen, T Asfour, P Abbeel, M Andrychowicz, arXiv:1706.01905Advances in neural information processing systems. Plappert. arXiv preprintParameter space noise for explorationNadeau, C., and Bengio, Y. 2000. Inference for the generalization error. In Advances in neural information processing systems. Plappert, M.; Houthooft, R.; Dhariwal, P.; Sidor, S.; Chen, R. Y.; Chen, X.; Asfour, T.; Abbeel, P.; and Andrychowicz, M. 2017. Parameter space noise for exploration. arXiv preprint arXiv:1706.01905.</p>
<p>Towards generalization and simplicity in continuous control. A Rajeswaran, K Lowrey, E Todorov, S Kakade, arXiv:1703.02660arXiv preprintRajeswaran, A.; Lowrey, K.; Todorov, E.; and Kakade, S. 2017. Towards generalization and simplicity in continuous control. arXiv preprint arXiv:1703.02660.</p>
<p>Trust region policy optimization. J Schulman, S Levine, P Abbeel, M Jordan, P Moritz, Proceedings of the 32nd International Conference on Machine Learning (ICML). the 32nd International Conference on Machine Learning (ICML)Schulman, J.; Levine, S.; Abbeel, P.; Jordan, M.; and Moritz, P. 2015a. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML).</p>
<p>High-dimensional continuous control using generalized advantage estimation. J Schulman, P Moritz, S Levine, M Jordan, P Abbeel, arXiv:1506.02438arXiv preprintSchulman, J.; Moritz, P.; Levine, S.; Jordan, M.; and Abbeel, P. 2015b. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438.</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.</p>
<p>Mastering the game of go with deep neural networks and tree search. V D N Silva, L Chaimowicz, D Silver, A Huang, C J Maddison, A Guez, L Sifre, G Van Den Driessche, J Schrittwieser, I Antonoglou, V Panneershelvam, M Lanctot, arXiv:1705.10443Moba: a new arena for game ai. 529arXiv preprintSilva, V. d. N., and Chaimowicz, L. 2017. Moba: a new arena for game ai. arXiv preprint arXiv:1705.10443. Silver, D.; Huang, A.; Maddison, C. J.; Guez, A.; Sifre, L.; Van Den Driessche, G.; Schrittwieser, J.; Antonoglou, I.; Panneershel- vam, V.; Lanctot, M.; et al. 2016. Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484-489.</p>
<p>Third-person imitation learning. B C Stadie, P Abbeel, I Sutskever, V Stodden, F Leisch, R D Peng, arXiv:1703.01703CRC PressarXiv preprintImplementing reproducible researchStadie, B. C.; Abbeel, P.; and Sutskever, I. 2017. Third-person imitation learning. arXiv preprint arXiv:1703.01703. Stodden, V.; Leisch, F.; and Peng, R. D. 2014. Implementing reproducible research. CRC Press.</p>
<p>Policy gradient methods for reinforcement learning with function approximation. R S Sutton, D A Mcallester, S P Singh, Y Mansour, Advances in neural information processing systems. Sutton, R. S.; McAllester, D. A.; Singh, S. P.; and Mansour, Y. 2000. Policy gradient methods for reinforcement learning with func- tion approximation. In Advances in neural information processing systems.</p>
<p>Data-efficient off-policy policy evaluation for reinforcement learning. P Thomas, E Brunskill, International Conference on Machine Learning. Thomas, P., and Brunskill, E. 2016. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, 2139-2148.</p>
<p>High-Confidence Off-Policy Evaluation. P S Thomas, G Theocharous, M Ghavamzadeh, AAAI. Thomas, P. S.; Theocharous, G.; and Ghavamzadeh, M. 2015. High- Confidence Off-Policy Evaluation. In AAAI.</p>
<p>Mujoco: A physics engine for model-based control. E Todorov, T Erez, Y Tassa, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012. Vilamoura, Algarve, PortugalTodorov, E.; Erez, T.; and Tassa, Y. 2012. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Confer- ence on Intelligent Robots and Systems, IROS 2012, Vilamoura, Algarve, Portugal, October 7-12, 2012, 5026-5033.</p>
<p>Learning values across many orders of magnitude. S Tufféry, H P Van Hasselt, A Guez, M Hessel, V Mnih, D Silver, Advances in Neural Information Processing Systems. Wiley Chichester2Data mining and statistics for decision makingTufféry, S. 2011. Data mining and statistics for decision making, volume 2. Wiley Chichester. van Hasselt, H. P.; Guez, A.; Hessel, M.; Mnih, V.; and Silver, D. 2016. Learning values across many orders of magnitude. In Advances in Neural Information Processing Systems, 4287-4295.</p>
<p>Publishing identifiable experiment code and configuration is important, good and easy. R T Vaughan, J Wawerla, CoRR abs/1204.2235Vaughan, R. T., and Wawerla, J. 2012. Publishing identifiable experiment code and configuration is important, good and easy. CoRR abs/1204.2235.</p>
<p>Efficient exact gradient update for training deep networks with very large sparse targets. P Vincent, A De Brébisson, X Bouthillier, Advances in Neural Information Processing Systems. Vincent, P.; de Brébisson, A.; and Bouthillier, X. 2015. Efficient exact gradient update for training deep networks with very large sparse targets. In Advances in Neural Information Processing Sys- tems, 1108-1116.</p>
<p>O Vinyals, T Ewalds, S Bartunov, P Georgiev, A S Vezhnevets, M Yeo, A Makhzani, H Küttler, J Agapiou, J Schrittwieser, arXiv:1708.04782Starcraft ii: A new challenge for reinforcement learning. arXiv preprintVinyals, O.; Ewalds, T.; Bartunov, S.; Georgiev, P.; Vezhnevets, A. S.; Yeo, M.; Makhzani, A.; Küttler, H.; Agapiou, J.; Schrittwieser, J.; et al. 2017. Starcraft ii: A new challenge for reinforcement learning. arXiv preprint arXiv:1708.04782.</p>
<p>K Wagstaff, arXiv:1206.4656Machine learning that matters. arXiv preprintWagstaff, K. 2012. Machine learning that matters. arXiv preprint arXiv:1206.4656.</p>
<p>Kolmogorov-smirnov test. Encyclopedia of biostatistics. R Wilcox, Wilcox, R. 2005. Kolmogorov-smirnov test. Encyclopedia of biostatistics.</p>
<p>Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. Y Wu, E Mansimov, S Liao, R Grosse, J Ba, arXiv:1708.05144arXiv preprintWu, Y.; Mansimov, E.; Liao, S.; Grosse, R.; and Ba, J. 2017. Scalable trust-region method for deep reinforcement learn- ing using kronecker-factored approximation. arXiv preprint arXiv:1708.05144.</p>
<p>B Xu, N Wang, T Chen, M Li, arXiv:1505.00853Empirical evaluation of rectified activations in convolutional network. arXiv preprintXu, B.; Wang, N.; Chen, T.; and Li, M. 2015. Empirical evaluation of rectified activations in convolutional network. arXiv preprint arXiv:1505.00853.</p>
<p>Bootstrap approach to inference and power analysis based on three test statistics for covariance structure models. K.-H Yuan, K Hayashi, British Journal of Mathematical and Statistical Psychology. 561Yuan, K.-H., and Hayashi, K. 2003. Bootstrap approach to inference and power analysis based on three test statistics for covariance structure models. British Journal of Mathematical and Statistical Psychology 56(1):93-110.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8452 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8452</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8452</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-32c9b3859086d15184989454eb878638659e64c6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32c9b3859086d15184989454eb878638659e64c6" target="_blank">MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work introduces MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions, and proposes a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function.</p>
                <p><strong>Paper Abstract:</strong> Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8452",
    "paper_id": "paper-32c9b3859086d15184989454eb878638659e64c6",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0064617500000000005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Minedojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge</h1>
<p>Linxi Fan ${ }^{1}$, Guanzhi Wang ${ }^{2 <em>}$, Yunfan Jiang ${ }^{3 </em>}$, Ajay Mandlekar ${ }^{1}$, Yuncong Yang ${ }^{4}$, Haoyi Zhu ${ }^{5}$, Andrew Tang ${ }^{4}$, De-An Huang ${ }^{1}$, Yuke Zhu ${ }^{161}$, Anima Anandkumar ${ }^{12 \dagger}$<br>${ }^{1}$ NVIDIA, ${ }^{2}$ Caltech, ${ }^{3}$ Stanford, ${ }^{4}$ Columbia, ${ }^{5}$ SJTU, ${ }^{6}$ UT Austin<br>*Equal contribution ${ }^{\dagger}$ Equal advising<br>https://minedojo.org</p>
<h4>Abstract</h4>
<p>Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MinEDOJO, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MinEDOJO's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of openended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.</p>
<h2>1 Introduction</h2>
<p>Developing autonomous embodied agents that can attain human-level performance across a wide spectrum of tasks has been a long-standing goal for AI research. There has been impressive progress towards this goal, most notably in games [80, 85, 126] and robotics [68, 99, 146, 134, 107]. These embodied agents are typically trained tabula rasa in isolated worlds with limited complexity and diversity. Although highly performant, they are specialist models that do not generalize beyond a narrow set of tasks. In contrast, humans inhabit an infinitely rich reality, continuously learn from and adapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge from their own experiences as well as others.</p>
<p>We argue that three main pillars are necessary for generalist embodied agents to emerge. First, the environment in which the agent acts needs to enable an unlimited variety of open-ended goals [116, 71, 120, 117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms thanks to the infinitely varied ecological settings that the Earth supports [117, 129]. This process has not stagnated for billions of years. In contrast, today's agent training algorithms cease to make new progress after convergence in narrow environments [80, 146]. Second, a large-scale database of prior knowledge is necessary to facilitate learning in open-ended settings. Just as humans frequently learn from the internet, agents should also be able to harvest practical knowledge encoded in large amounts of video demos [42, 77], multimedia tutorials [79], and forum discussions [127, 65, 54]. In a</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: MinEDojo is a novel framework for developing open-ended, generally capable agents that can learn and adapt continually to new goals. MinEDojo features a benchmarking suite with thousands of diverse open-ended tasks specified in natural language prompts, and also provides an internet-scale, multimodal knowledge base of YouTube videos, Wiki pages, and Reddit posts. The database captures the collective experience and wisdom of millions of Minecraft gamers for an AI agent to learn from. Best viewed zoomed in.
complex world, it would be extremely inefficient for an agent to learn everything from scratch through trial and error. Third, the agent's architecture needs to be flexible enough to pursue any task in openended environments, and scalable enough to convert large-scale knowledge sources into actionable insights [19, 96]. This motivates the design of an agent that has a unified observation/action space, conditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27, 91, 15] to internalize knowledge effectively.
In light of these three pillars, we introduce MinEDojo, a new framework to help the community develop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a player explores a procedurally generated 3D world with diverse types of terrains to roam, materials to mine, tools to craft, structures to build, and wonders to discover. Unlike most other games [80, 85, 126], Minecraft defines no specific reward to maximize and no fixed storyline to follow, making it well suited for developing open-ended environments for embodied AI research. We make the following three major contributions:</p>
<ol>
<li>Simulation platform with thousands of diverse open-ended tasks. MinEDojo provides convenient APIs on top of Minecraft that standardize task specification, world settings, and agent's observation/action spaces. We introduce a benchmark suite that consists of thousands of natural language-prompted tasks, making it two orders of magnitude larger than prior Minecraft benchmarks like the MineRL Challenge [48, 62]. The suite includes long-horizon, open-ended tasks that cannot be easily evaluated through automated procedures, such as "build an epic modern house with two floors and a swimming pool". Inspired by the Inception score [98] and FID score [55] that are commonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol using a large video-language model pre-trained on Minecraft YouTube videos. This complements human scoring [104] that is precise but more expensive. Our learned evaluation metric has good agreement with human judgment in a subset of the full task suite considered in the experiments.</li>
<li>Internet-scale multimodal Minecraft knowledge base. Minecraft has more than 100 million active players [131], who have collectively generated an enormous wealth of data. They record tutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums. MinEDojo features a massive collection of 730K+ YouTube videos with time-aligned transcripts, 6 K + free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that this enormous knowledge base can help the agent acquire diverse skills, develop complex strategies, discover interesting objectives, and learn actionable representations automatically.</li>
</ol>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Visualization of our agent's learned behaviors on four selected tasks. Leftmost texts are the task prompts used in training. Best viewed on a color display.
3. Novel algorithm for embodied agents with large-scale pre-training. We develop a new learning algorithm for embodied agents that makes use of the internet-scale domain knowledge we have collected from the web. Using the massive volume of YouTube videos from MineDoJo, we train a video-text contrastive model in the spirit of CLIP [92], which associates natural language subtitles with their time-aligned video segments. We demonstrate that this learned correlation score can be used effectively as an open-vocabulary, massively multi-task reward function for RL training. Our agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2). It achieves competitive performance to agents trained with meticulously engineered dense-shaping rewards, and in some cases outperforms them, with up to $73 \%$ improvement in success rates. For open-ended tasks that do not have a simple success criterion, our agents also perform well without any special modifications.</p>
<p>In summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent learning with recent advances on large pre-trained models [13]. We have open-sourced MineDoJo's simulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task curation tools at https://minedojo.org/. We hope that MinEDojo will serve as an effective starter framework for the community to develop new algorithms and advance towards generally capable embodied agents.</p>
<h1>2 MinEDojo Simulator \&amp; Benchmark Suite</h1>
<p>MineDoJo offers a set of simulator APIs help researchers develop generally capable, open-ended agents in Minecraft. It builds upon the open-source MineRL codebase [48] and makes the following upgrades: 1) We provide unified observation and action spaces across all tasks, facilitating the development of multi-task and continually learning agents that can constantly adapt to new scenarios and novel tasks. This deviates from the MineRL Challenge design that tailors observation and action spaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including the Overworld, the Nether, and the End, which substantially expands the possible task space, while MineRL only supports the Overworld natively; and 3) We provide convenient APIs to configure initial conditions and world settings to standardize our tasks.</p>
<p>With this MinEDojo simulator, we define thousands of benchmarking tasks, which are divided into two categories: 1) Programmatic tasks that can be automatically assessed based on the ground-truth simulator states; and 2) Creative tasks that do not have well-defined or easily-automated success criteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up the number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI's GPT-3 [15]</p>
<p>service to generate substantially more task definitions. Compared to Creative tasks, Programmatic tasks are simpler to get started, but tend to have restricted scope, limited language variations, and less open-endedness in general.</p>
<h1>2.1 Task Suite I: Programmatic Tasks</h1>
<p>We formalize each programmatic task as a 5-tuple: $T=\left(G, \mathcal{G}, \mathcal{I}, f_{\mathcal{S}}, f_{\mathcal{R}}\right) . G$ is an English description of the task goal, such as "find material and craft a gold pickaxe". $\mathcal{G}$ is a natural language guidance that provides helpful hints, recipes, or advice to the agent. We leverage OpenAI's GPT-3-davinci API to automatically generate detailed guidance for a subset of the tasks. For the example goal "bring a pig into Nether", GPT-3 returns: 1) Find a pig in the overworld; 2) Right-click on the pig with a lead; 3) Right-click on the Nether Portal with the lead and pig selected; 4) The pig will be pulled through the portal! $\mathcal{I}$ is the initial conditions of the agent and the world, such as the initial inventory, spawn terrain, and weather. $f_{\mathcal{S}}: s_{t} \rightarrow{0,1}$ is the success criterion, a deterministic function that maps the current world state $s_{t}$ to a Boolean success label. $f_{\mathcal{R}}: s_{t} \rightarrow \mathbb{R}$ is an optional dense reward function. We only provide $f_{\mathcal{R}}$ for a small subset of the tasks in MinEDojo due to the high costs of meticulously crafting dense rewards. For our current agent implementation (Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan [3] and Socratic Models [143], one potential idea is to feed each step in the guidance to our learned reward model sequentially so that it becomes a stagewise reward function for a complex multi-stage task.
MinEDOJO provides 4 categories of programmatic tasks with 1,581 template-generated natural language goals to evaluate the agent's different capabilities systematically and comprehensively:</p>
<ol>
<li>Survival: surviving for a designated number of days.</li>
<li>Harvest: finding, obtaining, cultivating, or manufacturing hundreds of materials and objects.</li>
<li>Tech Tree: crafting and using a hierarchy of tools.</li>
<li>Combat: fighting various monsters and creatures that require fast reflex and martial skills.</li>
</ol>
<p>Each task template has a number of variations based on the terrain, initial inventory, quantity, etc., which form a flexible spectrum of difficulty. In comparison, the NeurIPS MineRL Diamond challenge [48] is a subset of our programmatic task suite, defined by the task goal "obtain 1 diamond" in MinEDojo.</p>
<h3>2.2 Task Suite II: Creative Tasks</h3>
<p>We define each creative task as a 3-tuple, $T=(G, \mathcal{G}, \mathcal{I})$, which differs from programmatic tasks due to the lack of straightforward success criteria. Inspired by model-based metrics like the Inception score [98] and FID score [55] for image generation, we design a novel task evaluation metric based on a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we find that the learned metric exhibits a high level of agreement with human evaluations (see Table 2).
We brainstorm and author 216 Creative tasks, such as "build a haunted house with zombie inside" and "race by riding a pig". Nonetheless, such a manual approach is not scalable. Therefore, we develop two systematic approaches to extend the total number of task definitions to 1,560. This makes our Creative tasks 3 orders of magnitude larger than Minecraft BASALT challenge [104], which has 4 Creative tasks.</p>
<p>Approach 1. Task Mining from YouTube Tutorial Videos. We identify our YouTube dataset as a rich source of tasks, as many human players demonstrate and narrate creative missions in the tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage pipeline that makes it easy to find and annotate interesting tasks (see Sec. C. 2 for details). Through this pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran Minecraft gamers, such as "make an automated mining machine" and "grow cactus up to the sky".</p>
<p>Approach 2. Task Creation by GPT-3. We leverage GPT-3's few-shot capability to generate new task ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt template is: Here are some example creative tasks in Minecraft: {a few examples}.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: MineDoJo's internet-scale, multimodal knowledge base. Left, YouTube videos: Minecraft gamers showcase the impressive feats they are able to achieve. Clockwise order: an archery range, Hogwarts castle, Taj Mahal, a Nether homebase. Middle, Wiki: Wiki pages contain multimodal knowledge in structured layouts, such as comprehensive catalogs of creatures and recipes for crafting. More examples in Fig. A.4 and A.5. Right, Reddit: We create a word cloud from Reddit posts and comment threads. Gamers ask questions, share achievements, and discuss strategies extensively. Sample posts in Fig. A.7. Best viewed zoomed in.</p>
<p>Let's brainstorm more detailed while reasonable creative tasks in Minecraft. GPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proficient understanding of Minecraft terminology.</p>
<h3>2.3 Collection of Starter Tasks</h3>
<p>We curate a set of 64 core tasks for future researchers to get started more easily. If their agent works well on these tasks, they can more confidently scale to the full benchmark.</p>
<ul>
<li>32 programmatic tasks: 16 "standard" and 16 "difficult", spanning all 4 categories (survival, harvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difficulty level. "Standard" tasks require fewer steps and lower resource dependencies to complete.</li>
<li>32 creative tasks: 16 "standard" and 16 "difficult". Similarly, tasks labeled with "standard" are typically short-horizon tasks.</li>
</ul>
<p>We recommend that researchers run 100 evaluation episodes for each task and report the percentage success rate. The programmatic tasks have ground-truth success, while the creative tasks need our novel evaluation protocol (Sec. 5).</p>
<h2>3 Internet-scale Knowledge Base</h2>
<p>Two commonly used approaches [112, 126, 85, 36] to train embodied agents include training agents from scratch using RL with well-tuned reward functions for each task, or using a large amount of human-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is challenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large amounts of demonstration data would also be costly and infeasible [126].</p>
<p>Instead, we turn to the open web as an ever-growing, virtually unlimited source of learning material for embodied agents. The internet provides a vast amount of domain knowledge about Minecraft, which we harvest by extensive web scraping and filtering. We collect 33 years worth of YouTube videos, 6K+ Wiki pages, and millions of Reddit comment threads. Instead of hiring a handful of human demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the world. Furthermore, language is a key and pervasive component of our database that takes the form of YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates open-vocabulary understanding, provides grounding for image and video modalities, and unlocks the power of large language models [27, 109, 15] for embodied agents. To ensure socially responsible model development, we take special measures to filter out low-quality and toxic contents [13, 51] from our databases, detailed in the Appendix (Sec. D).</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Algorithm design. MineCLIP is a contrastive video-language model pre-trained on MineDojo's massive Youtube database. It computes the correlation between an open-vocabulary language goal string and a 16 -frame video snippet. The correlation score can be used as a learned dense reward function to train a strong multi-task RL agent.</p>
<p>YouTube Videos and Transcripts. Minecraft is among the most streamed games on YouTube [41]. Human players have demonstrated a stunning range of creative activities and sophisticated missions that take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which add up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77] is a large-scale human instructional video dataset that includes 15 years of experience in total - about half of our volume. The time-aligned transcripts enable the agent to ground free-form natural language in video pixels and learn the semantics of diverse activities without laborious human labeling. We operationalize this insight in our pre-trained video-language model (Sec. 4.1).</p>
<p>Minecraft Wiki. The Wiki pages cover almost every aspect of the game mechanics, and supply a rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step tutorials. We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and diagrams. The pages are highly unstructured and do not share any common schema, as the Wiki is meant for human consumption rather than AI training. To preserve the layout information, we additionally save the screenshots of entire pages and extract 2.2 M bounding boxes of the visual elements (visualization in Fig. A. 4 and A.5). We do not use Wiki data in our current experiments. Since the Wiki contains detailed recipes for all crafted objects, they could be provided as input or training data for hierarchical planning methods and policy sketches [8]. Another promising future direction is to apply document understanding models such as LayoutLM [138, 137] and DocFormer [9] to learn actionable knowledge from these unstructured Wiki data.</p>
<p>Reddit. We scrape 340K+ posts along with 6.6M comments under the "r/Minecraft" subreddit. These posts ask questions on how to solve certain tasks, showcase cool architectures and achievements in image/video snippets, and discuss general tips and tricks for players of all expertise levels. We do not use Reddit data for training in Sec. 5, but a potential idea is to finetune large language models $[27,91]$ on our Reddit corpus to generate instructions and execution plans that are better grounded in the Minecraft domain. Concurrent works [3, 56, 143] have explored similar ideas and showed excellent results on robot learning, which is encouraging for more future research in MineDojo.</p>
<h1>4 Agent Learning with Large-scale Pre-training</h1>
<p>One of the grand challenges of embodied AI is to build a single agent that can complete a wide range of open-world tasks. The MineDojo framework aims to facilitate new techniques towards this goal by providing an open-ended task suite (Sec. 2) and large-scale internet knowledge base (Sec. 3). Here we take an initial step towards this goal by developing a proof of concept that demonstrates how a single language-prompted agent can be trained in MineDojo to complete several complex Minecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the massive YouTube data offered by MineDojo. We note that this is only one of the numerous possible</p>
<p>Table 1: Our novel MINECLIP reward model is able to achieve competitive performance with manually written dense reward function for Programmatic tasks, and significantly outperforms the CLIP $_{\text {OpenAI }}$ method across all Creative tasks. Entries represent percentage success rates averaged over 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but estimated by MineCLIP for Creative tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Group</th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Ours (Attn)</th>
<th style="text-align: center;">Ours (Avg)</th>
<th style="text-align: center;">Manual Reward</th>
<th style="text-align: center;">Sparse-only</th>
<th style="text-align: center;">CLIP $_{\text {OpenAI }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Milk Cow</td>
<td style="text-align: center;">$64.5 \pm 37.1$</td>
<td style="text-align: center;">$6.5 \pm 3.5$</td>
<td style="text-align: center;">$62.8 \pm 40.1$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hunt Cow</td>
<td style="text-align: center;">$83.5 \pm 7.1$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$48.3 \pm 35.9$</td>
<td style="text-align: center;">$0.3 \pm 0.4$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shear Sheep</td>
<td style="text-align: center;">$12.1 \pm 9.1$</td>
<td style="text-align: center;">$0.6 \pm 0.2$</td>
<td style="text-align: center;">$52.3 \pm 33.2$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hunt Sheep</td>
<td style="text-align: center;">$8.1 \pm 4.1$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$41.9 \pm 33.0$</td>
<td style="text-align: center;">$0.3 \pm 0.4$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Spider</td>
<td style="text-align: center;">$80.5 \pm 13.0$</td>
<td style="text-align: center;">$60.1 \pm 42.5$</td>
<td style="text-align: center;">$87.5 \pm 4.6$</td>
<td style="text-align: center;">$47.8 \pm 33.8$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Zombie</td>
<td style="text-align: center;">$47.3 \pm 10.6$</td>
<td style="text-align: center;">$72.3 \pm 6.4$</td>
<td style="text-align: center;">$49.8 \pm 26.9$</td>
<td style="text-align: center;">$8.8 \pm 12.4$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Pigman</td>
<td style="text-align: center;">$1.6 \pm 2.3$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$13.6 \pm 9.8$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Enderman</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.3 \pm 0.2$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Find Nether Portal</td>
<td style="text-align: center;">$37.4 \pm 40.8$</td>
<td style="text-align: center;">$89.8 \pm 5.7$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$26.3 \pm 32.6$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Find Ocean</td>
<td style="text-align: center;">$33.4 \pm 45.6$</td>
<td style="text-align: center;">$54.3 \pm 40.7$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$9.9 \pm 14.1$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dig Hole</td>
<td style="text-align: center;">$91.6 \pm 5.9$</td>
<td style="text-align: center;">$88.1 \pm 13.3$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lay Carpet</td>
<td style="text-align: center;">$97.6 \pm 1.9$</td>
<td style="text-align: center;">$98.8 \pm 1.0$</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
</tbody>
</table>
<p>ways to use MinEDojo's internet database - the Wiki and Reddit corpus also hold great potential to drive new algorithm discoveries for the community in future works.</p>
<p>In this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked with completing a collection of MinEDojo tasks specified by language instructions (Sec. 2). Solving these tasks often requires the agent to interact with the Minecraft world in a prolonged fashion. Agents developed in popular RL benchmarks [119, 146] often rely on meticulously crafted dense and task-specific reward functions to guide random explorations. However, these rewards are hard or even infeasible to define for our diverse and open-ended tasks in MinEDojo. To address this challenge, our key insight is to learn a dense, language-conditioned reward function from in-the-wild YouTube videos and their transcripts. Therefore, we introduce MINECLIP, a contrastive video-language model that learns to correlate video snippets and natural language descriptions (Fig. 4). MINECLIP is multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.</p>
<p>During RL training, MINECLIP provides a high-quality reward signal without any domain adaptation techniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered frames. MINECLIP eliminates the need to manually engineer reward functions for each and every MinEDojo task. For Creative tasks that lack a simple success criterion (Sec. 2.2), MINECLIP also serves the dual purpose of an automatic evaluation metric that agrees well with human judgement on a subset of tasks we investigate (Sec. 4.2, Table 2). Because the learned reward model incurs a non-trivial computational overhead, we introduce several techniques to significantly improve RL training efficiency, making MINECLIP a practical module for open-ended agent learning in Minecraft (Sec. 4.2).</p>
<h1>4.1 Pre-Training MINECLIP on Large-scale Videos</h1>
<p>Formally, the learned reward function can be defined as $\Phi_{\mathcal{R}}:(G, V) \rightarrow \mathbb{R}$ that maps a language goal $G$ and a video snippet $V$ to a scalar reward. An ideal $\Phi_{\mathcal{R}}$ should return a high reward if the behavior depicted in the video faithfully follows the language description, and a low reward otherwise. This can be achieved by optimizing the InfoNCE objective [125, 52, 20], which learns to correlate positive video and text pairs $[118,6,78,4,136]$.</p>
<p>Similar to the image-text CLIP model [92], MINECLIP is composed of a separate text encoder $\phi_{G}$ that embeds a language goal and a video encoder $\phi_{V}$ that embeds a moving window of 16 consecutive frames with $160 \times 256$ resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip [75], where $\phi_{G}$ reuses OpenAI CLIP's pretrained text encoder, and $\phi_{V}$ is factorized into a frame-wise image encoder $\phi_{I}$ and a temporal aggregator $\phi_{a}$ that summarizes the sequence of 16 image features into a single video embedding. Unlike CLIP4Clip, we insert two extra layers of residual CLIP Adapter [38] after the aggregator $\phi_{a}$ to produce a better video feature, and finetune only the last two layers of the pretrained $\phi_{I}$ and $\phi_{G}$.</p>
<p>Table 2: MineCLIP agrees well with the ground-truth human judgment on the Creative tasks we consider. Numbers are F1 scores between MineCLIP’s binary classification of tasks success and human labels (scaled to the percentage for better readability).</p>
<table>
<thead>
<tr>
<th>Tasks</th>
<th>Find Nether Portal</th>
<th>Find Ocean</th>
<th>Dig Hole</th>
<th>Lay Carpet</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ours (Attn)</td>
<td>98.7</td>
<td>100.0</td>
<td>99.4</td>
<td>97.4</td>
</tr>
<tr>
<td>Ours (Avg)</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>98.4</td>
</tr>
<tr>
<td>CLIP${}_{\text{DperAI}}$</td>
<td>48.7</td>
<td>98.4</td>
<td>80.6</td>
<td>54.1</td>
</tr>
</tbody>
</table>
<p>From the MineDojo YouTube database, we follow the procedure in VideoCLIP [136] to sample 640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword filter. We train two MineCLIP variants with different types of aggregator $\phi_{a}$: (1) MineCLIP[avg] does simple average pooling, which is fast but agnostic to the temporal ordering; (2) MineCLIP[attn] encodes the sequence by two transformer layers, which is relatively slower but captures more temporal information, and thus produces a better reward signal in general. Details of data preprocessing, architecture, and hyperparameters are listed in the Appendix (Sec. E).</p>
<h3>4.2 RL with MineCLIP Reward</h3>
<p>We train a language-conditioned policy network that takes as input raw pixels and predicts discrete control. The policy is trained with PPO [102] on the MineCLIP rewards. In each episode, the agent is prompted with a language goal and takes a sequence of actions to fulfill this goal. When calculating the MineCLIP rewards, we concatenate the agent’s latest 16 egocentric RGB frames in a temporal window to form a video snippet. MineCLIP handles all task prompts zero-shot without any further finetuning. In our experiments (Sec. 5), we show that MineCLIP provides effective dense rewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator frames. Besides regular video data augmentation, we do not employ any special domain adaptation methods during pre-training. Our finding is consistent with CLIP’s strong zero-shot performances on robustness benchmarks in object recognition [92].</p>
<p>Compared to hard-coded reward functions in popular benchmarks [146, 119, 34], the MineCLIP model has 150M parameters and is thus much more expensive to query. We make several design choices to greatly accelerate RL training with MineCLIP in the loop:</p>
<ol>
<li>The language goal $G$ is fixed for a specific task, so the text features $\phi_{G}$ can be precomputed to avoid invoking the text encoder repeatedly.</li>
<li>Our agent’s RGB encoder reuses the pre-trained weights of $\phi_{I}$ from MineCLIP. We do not finetune $\phi_{I}$ during RL training, which saves computation and endows the agent with good visual representations from the beginning.</li>
<li>MineCLIP’s video encoder $\phi_{V}$ is factorized into an image encoder $\phi_{I}$ and a light-weight aggregator $\phi_{a}$. This design choice enables efficient image feature caching. Consider two overlapping video sequences of 8 frames, $\mathrm{V}[0: 8]$ and $\mathrm{V}[1: 9]$. We can cache the image features of the 7 overlapping frames $\mathrm{V}[1]$ to $\mathrm{V}[7]$ to maximize compute savings. If $\phi_{V}$ is a monolithic model like S3D [135] in VideoCLIP [136], then the video features from every sliding window must be recomputed, which would incur a much higher cost per time step.</li>
<li>We leverage Self-Imitation Learning [84] to store the trajectories with high MineCLIP reward values in a buffer, and alternate between PPO and self-imitation gradient steps. It further improves sample efficiency as shown in the Appendix (Fig. A.8).</li>
</ol>
<h2>5 Experiments</h2>
<p>We evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks from the MineDojo benchmarking suite. We select these 12 tasks due to the diversity of skills required to solve them (e.g., harvesting, combat, building, navigation) and domain-specific entities (e.g., animals, resources, monsters, terrains, and structures). We split the tasks into 3 groups and train one multi-task agent for each group: Animal-Zoo (4 Programmatic tasks on hunting or</p>
<p>Table 3: MINECLIP agents have stronger zero-shot visual generalization ability to unseen terrains, weathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3 seeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Ours (Attn), train</th>
<th style="text-align: center;">Ours (Attn), unseen test</th>
<th style="text-align: center;">CLIP $_{\text {OpenAI }}$, train</th>
<th style="text-align: center;">CLIP $_{\text {OpenAI }}$, unseen test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Milk Cow</td>
<td style="text-align: center;">$64.5 \pm 37.1$</td>
<td style="text-align: center;">$64.8 \pm 31.3(+0.8 \%)$</td>
<td style="text-align: center;">$90.0 \pm 0.4$</td>
<td style="text-align: center;">$29.2 \pm 3.7(-67.6 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hunt Cow</td>
<td style="text-align: center;">$83.5 \pm 7.1$</td>
<td style="text-align: center;">$55.9 \pm 7.2(-32.9 \%)$</td>
<td style="text-align: center;">$72.7 \pm 3.5$</td>
<td style="text-align: center;">$16.7 \pm 1.6(-77.0 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Spider</td>
<td style="text-align: center;">$80.5 \pm 13.0$</td>
<td style="text-align: center;">$62.1 \pm 29.7(-22.9 \%)$</td>
<td style="text-align: center;">$79.5 \pm 2.5$</td>
<td style="text-align: center;">$54.2 \pm 9.6(-31.8 \%)$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Zombie</td>
<td style="text-align: center;">$47.3 \pm 10.6$</td>
<td style="text-align: center;">$39.9 \pm 25.3(-15.4 \%)$</td>
<td style="text-align: center;">$50.2 \pm 7.5$</td>
<td style="text-align: center;">$30.8 \pm 14.4(-38.6 \%)$</td>
</tr>
</tbody>
</table>
<p>harvesting resource from animals), Mob-Combat (Programmatic, fight 4 types of hostile monsters), and Creative (4 tasks).</p>
<p>In the experiments, we empirically check the quality of MINECLIP against manually written reward functions, and quantify how different variants of our learned model affect the RL performance. Table 1 presents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks. Policy networks of all methods share the same architecture and are trained by PPO + Self-Imitation (Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:</p>
<ul>
<li>Ours (Attn): our agent trained with the MINECLIP[attn] reward model. For Programmatic tasks, we also add the final success condition as a binary reward. For Creative tasks, MINECLIP is the only source of reward.</li>
<li>Ours (Avg): the average-pooling variant of our method.</li>
<li>Manual Reward: hand-engineered dense reward using ground-truth simulator states.</li>
<li>Sparse-only: the final binary success as a single sparse reward. Note that neither sparse-only nor manual reward is available for Creative tasks.</li>
<li>CLIP $_{\text {OpenAI }}$ : pre-trained OpenAI CLIP model that has not been finetuned on any MinEDojo videos.</li>
</ul>
<p>MINECLIP is competitive with manual reward. For Programmatic tasks (first 8 rows), RL agents guided by MINECLIP achieve competitive performance as those trained by manual reward. In three of the tasks, they even outperform the hand-engineered reward functions, which rely on privileged simulator states unavailable to MINECLIP. For a more statistically sound analysis, we conduct the Paired Student's $t$-test to compare the mean success rate of each task (pairing column 3 "Ours (Attn)" and column 5 "Manual Reward" in Table 1). The test yields p-value $0.3991 \gg 0.05$, which indicates that the difference between our method and manual reward is not considered statistically significant, and therefore they are comparable with each other. Because all tasks require nontrivial exploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP model fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically different from their real-world counterparts, which causes CLIP to produce misleading signals worse than no shaping reward at all. It implies the importance of finetuning on MinEDojo's YouTube data.</p>
<p>MineCLIP provides automated evaluation. For Creative tasks (last 4 rows), there are no programmatic success criteria available. We convert MINECLIP into a binary success classifier by thresholding the reward value it outputs for an episode. To test the quality of MINECLIP as an automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and 100 failed trajectories for each task. We then run both MINECLIP variants and CLIP $<em _OpenAI="{OpenAI" _text="\text">{\text {OpenAI }}$ on the dataset and report the binary F1 score of their judgement against human ground-truth in Table 2. The results demonstrate that both MINECLIP[attn] and MINECLIP[avg] attain a very high degree of agreement with human evaluation results on this subset of the Creative task suite that we investigate. CLIP $</em>$-guided agents. It shows that MINECLIP is an effective approach to solving open-ended tasks when no straightforward reward signal is available. We provide further analysis beyond these 4 tasks in the Appendix (Sec. G.4).}}$ baseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely because real-world oceans and holes have similar texture. We use the attn variant as an automated success criterion to score the 4 Creative task results in Table 1. Our proposed method consistently learns better than CLIP $_{\text {OpenAI }</p>
<p>Table 4: We train a single multi-task agent for all 12 tasks. All numbers represent percentage success rates averaged over 3 seeds, each tested for 200 episodes.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Group</th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Single Agent on All Tasks</th>
<th style="text-align: center;">Original</th>
<th style="text-align: center;">Performance Change</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Milk Cow</td>
<td style="text-align: center;">$91.5 \pm 0.7$</td>
<td style="text-align: center;">$64.5 \pm 37.1$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hunt Cow</td>
<td style="text-align: center;">$46.8 \pm 3.7$</td>
<td style="text-align: center;">$83.5 \pm 7.1$</td>
<td style="text-align: center;">$\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Shear Sheep</td>
<td style="text-align: center;">$73.5 \pm 0.8$</td>
<td style="text-align: center;">$12.1 \pm 9.1$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Hunt Sheep</td>
<td style="text-align: center;">$27.0 \pm 1.0$</td>
<td style="text-align: center;">$8.1 \pm 4.1$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Spider</td>
<td style="text-align: center;">$72.1 \pm 1.3$</td>
<td style="text-align: center;">$80.5 \pm 13.0$</td>
<td style="text-align: center;">$\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Zombie</td>
<td style="text-align: center;">$27.1 \pm 2.7$</td>
<td style="text-align: center;">$47.3 \pm 10.6$</td>
<td style="text-align: center;">$\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Pigman</td>
<td style="text-align: center;">$6.5 \pm 1.2$</td>
<td style="text-align: center;">$1.6 \pm 2.3$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Combat Enderman</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
<td style="text-align: center;">$=$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Find Nether Portal</td>
<td style="text-align: center;">$99.1 \pm 0.4$</td>
<td style="text-align: center;">$37.4 \pm 40.8$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Find Ocean</td>
<td style="text-align: center;">$95.1 \pm 1.5$</td>
<td style="text-align: center;">$33.4 \pm 45.6$</td>
<td style="text-align: center;">$\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dig Hole</td>
<td style="text-align: center;">$85.8 \pm 1.2$</td>
<td style="text-align: center;">$91.6 \pm 5.9$</td>
<td style="text-align: center;">$\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lay Carpet</td>
<td style="text-align: center;">$96.5 \pm 0.8$</td>
<td style="text-align: center;">$97.6 \pm 1.9$</td>
<td style="text-align: center;">$=$</td>
</tr>
</tbody>
</table>
<p>Table 5: We test the open-vocabulary generalization ability to two unseen tasks. All numbers represent percentage success rates averaged over 3 seeds, each tested for 200 episodes.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Ours (zero-shot)</th>
<th style="text-align: center;">Ours (after RL finetune)</th>
<th style="text-align: center;">Baseline (RL from scratch)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Hunt Pig</td>
<td style="text-align: center;">$1.3 \pm 0.6$</td>
<td style="text-align: center;">$46.0 \pm 15.3$</td>
<td style="text-align: center;">$0.0 \pm 0.0$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Harvest Spider String</td>
<td style="text-align: center;">$1.6 \pm 1.3$</td>
<td style="text-align: center;">$36.5 \pm 16.9$</td>
<td style="text-align: center;">$12.5 \pm 12.7$</td>
</tr>
</tbody>
</table>
<p>MineCLIP shows good zero-shot generalization to significant visual distribution shift. We evaluate the learned policy without finetuning on a combination of unseen weather, lighting conditions, and terrains - 27 scenarios in total. For the baseline, we train agents with the original $\mathrm{CLIP}_{\text {OpenAI }}$ image encoder (not trained on our YouTube videos) by imitation learning. The robustness against visual shift can be quantitatively measured by the relative performance degradation on novel test scenarios for each task. Table 3 shows that while all methods incur performance drops, agents with MINECLIP encoder is more robust to visual changes than the baseline across all tasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual generalization capability, a finding consistent with literature [92, 82].</p>
<p>Learning a Single Agent for All 12 Tasks We have also trained a single agent for all 12 tasks. To reduce the computational burden without loss of generality, the agent is trained by self-imitating from successful trajectories generated from the self-imitation learning pipeline (Section F.3). We summarize the result in Table 4. Similar to our main experiments, all numbers represent percentage success rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original agents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks, and roughly the same success rates in 2 tasks. This result suggests that there are both positive and negative task transfers happening. To improve the multi-task performance, more advanced algorithms [141, 133] can be employed to mitigate the negative transfer effects.
We also perform Paired Student's $t$-test to statistically compare the performance of the 12-multitask agent and those separately trained on each task group. We obtain a p-value of $0.3720 \gg 0.05$, which suggests that the difference between the two training settings is not statistically significant.</p>
<p>Generalize to Novel Tasks To test the ability to generalize to new open-vocabulary commands, we evaluate the agent on two novel tasks: "harvest spider string" and "hunt pig". Table 5 shows that the agent struggles in the zero-shot setting because it has not interacted with pigs or spider strings during training, and thus does not know how to interact with them effectively. However, the performance improves substantially by finetuning with the MINECLIP reward. Here the baseline methods are trained from scratch using RL with the MINECLIP encoders and reward. Therefore, the only difference is whether the policy has been pre-trained on the 12 tasks or not. Given the</p>
<p>same environment sampling budget (only around 5\% of total samples), it significantly outperforms baselines. It suggests that the multitask agent has learned transferable knowledge on hunting and resource collection, which enables it to quickly adapt to novel tasks.</p>
<h1>6 Related work</h1>
<p>Open-ended Environments for Decision-making Agents. There are many environments developed with the goal of open-ended agent learning. Prior works include maze-style worlds [121, 129, 61], purely text-based game [69], grid worlds [21, 16], browser/GUI-based environments [108, 124], and indoor simulators for robotics [1, 107, 114, 34, 110, 99, 89]. Minecraft offers an exciting alternative for open-ended agent learning. It is a 3D visual world with procedurally generated landscapes and extremely flexible game mechanics that support an enormous variety of activities. Prior methods in open-ended agent learning [30, 57, 130, 63, 26] do not make use of external knowledge, but our approach leverages internet-scale database to learn open-vocabulary reward models, thanks to Minecraft's abundance of gameplay data online.</p>
<p>Minecraft for AI Research. The Malmo platform [60] is the first comprehensive release of a Gym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and human play trajectories for the annual Diamond Challenge at NeurIPS [47, 49, 62]. MinEDOJO's simulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking task suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44] and IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4 open-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast experimentation. Unlike prior works, MinEDOJO's core mission is to facilitate the development of generally capable embodied agents using internet-scale knowledge. We include a feature comparison table of different Minecraft platforms for AI research in Table A.1.</p>
<p>Internet-scale Multimodal Knowledge Bases. Big dataset such as Common Crawl [24], the Pile [37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pretrained language models [27, 91, 15] and multimodal models [118, 6, 78, 145, 7, 4, 136]. While generally useful for learning representations, these datasets are not specifically targeted at embodied agents. To provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms, and Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison, MinEDOJO's knowledge base is constructed without human curation efforts, much larger in volume, more diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.</p>
<p>Embodied Agents with Large-scale Pre-training. Inspired by the success in NLP, embodied agent research [29, 11, 94, 23] has seen a surge in adoption of the large-scale pre-training paradigm. The recent advances can be roughly divided into 4 categories. 1) Novel agent architecture: Decision Transformer [19, 58, 144] applies the powerful self-attention models to sequential decision making. GATO [95] and Unified-IO [74] learn a single model to solve various decision-making tasks with different control interfaces. VIMA [59] unifies a wide range of robot manipulation tasks with multimodal prompting. 2) Pre-training for better representations: R3M [82] trains a general-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages the pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation. 3) Pre-training for better policies: AlphaStar [126] achieves champion-level performance on StarCraft by imitating from numerous human demos. SayCan [3] leverages large language models (LMs) to ground value functions in the physical world. [72] and [96] directly reuse pre-trained LMs as policy backbone. VPT [10] is a concurrent work that learns an inverse dynamics model from human contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to our approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned reward model. 4) Data-driven reward functions: Concept2Robot [105] and DVD [18] learn a binary classifier to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans labels to train language-conditioned reward function for offline RL. AVID [113] and XIRL [142] extract reward signals via cycle consistency. MinEDOJO's task benchmark and internet knowledge base are generally useful for developing new algorithms in all the above categories. In Sec. 4, we also propose an open-vocabulary, multi-task reward model using MinEDOJO YouTube videos.</p>
<h1>7 Conclusion</h1>
<p>In this work, we introduce the MinEDojo framework for developing generally capable embodied agents. MinEDojo features a benchmarking suite of thousands of Programmatic and Creative tasks, and an internet-scale multimodal knowledge base of videos, wiki, and forum discussions. As an example of the novel research possibilities enabled by MinEDojo, we propose MinECLIP as an effective language-conditioned reward function trained with in-the-wild YouTube videos. MinECLIP achieves strong performance empirically and agrees well with human evaluation results, making it a good automatic metric for Creative tasks. We look forward to seeing how MinEDojo empowers the community to make progress on the important challenge of open-ended agent learning.</p>
<h2>8 Acknowledgement</h2>
<p>We are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie, Jean Kossaifi, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John Spitzer, Zhiyuan "Jerry" Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack Parker-Holder, and many other colleagues and friends for their helpful feedback and insightful discussions. We also thank the anonymous reviewers for offering us highly constructive advice and kind encouragement during the review and rebuttal period. NVIDIA provides the necessary computing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak fellowship in Computing and Mathematical Sciences at Caltech.</p>
<h2>References</h2>
<p>[1] Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin, Rachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, Timothy Lillicrap, Kory Mathewson, Soňa Mokrá, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Duncan Williams, Nathaniel Wong, Chen Yan, and Rui Zhu. Imitating interactive intelligence. arXiv preprint arXiv: Arxiv-2012.05672, 2020.
[2] Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large-scale video classification benchmark. arXiv preprint arXiv: Arxiv-1609.08675, 2016.
[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv: Arxiv-2204.01691, 2022.
[4] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. arXiv preprint arXiv: Arxiv-2104.11178, 2021.
[5] Rami Al-Rfou, Marc Pickett, Javier Snaider, Yun hsuan Sung, Brian Strope, and Ray Kurzweil. Conversational contextual cues: The case of personalization and history for response ranking. arXiv preprint arXiv: Arxiv-1606.00372, 2016.
[6] Jean-Baptiste Alayrac, Adrià Recasens, Rosalia Schneider, Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Selfsupervised multimodal versatile networks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips. cc/paper/2020/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html.</p>
<p>[7] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex M. Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 6644-6652. AAAI Press, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16822.
[8] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 166-175. PMLR, 2017. URL http://proceedings.mlr.press/v70/andreas17a.html.
[9] Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R. Manmatha. Docformer: End-to-end transformer for document understanding. arXiv preprint arXiv: Arxiv2106.11539, 2021.
[10] Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. arXiv preprint arXiv: Arxiv-2206.11795, 2022.
[11] Dhruv Batra, Angel X. Chang, Sonia Chernova, Andrew J. Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, Manolis Savva, and Hao Su. Rearrangement: A challenge for embodied ai. arXiv preprint arXiv: Arxiv-2011.01975, 2020.
[12] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. Objectnav revisited: On evaluation of embodied agents navigating to objects. arXiv preprint arXiv: Arxiv-2006.13171, 2020.
[13] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models. arXiv preprint arXiv: Arxiv-2108.07258, 2021.
[14] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv: Arxiv-1606.01540, 2016.
[15] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020.
[16] Tianshi Cao, Jingkang Wang, Yining Zhang, and Sivabalan Manivasagam. Babyai++: Towards grounded-language learning beyond memorization. arXiv preprint arXiv: Arxiv-2004.07200, 2020.</p>
<p>[17] João Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 4724-4733. IEEE Computer Society, 2017. doi: 10.1109/CVPR.2017.502. URL https://doi.org/10.1109/CVPR.2017.502.
[18] Annie S. Chen, Suraj Nair, and Chelsea Finn. Learning generalizable robotic reward functions from in-the-wild human videos. In Dylan A. Shell, Marc Toussaint, and M. Ani Hsieh, editors, Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021, 2021. doi: 10.15607/RSS.2021.XVII.012. URL https://doi.org/10.15607/RSS.2021.XVII.012.
[19] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 15084-15097, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 7f489f642a0ddb10272b5c31057f0663-Abstract.html.
[20] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607. PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html.
[21] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id=rJeXCo0cYX.
[22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. arXiv preprint arXiv: Arxiv-2204.02311, 2022.
[23] Jack Collins, Shelvin Chand, Anthony Vanderkop, and David Howard. A review of physics simulators for robotic applications. IEEE Access, 9:51416-51431, 2021.
[24] Common Crawl. Common crawl. https://commoncrawl.org/, 2012. Accessed: 2022-0606.
[25] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 885-897. PMLR, 2019. URL http://proceedings.mlr.press/v100/dasari20a.html.
[26] Michael Dennis, Natasha Jaques, Eugene Vinitsky, Alexandre M. Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:</p>
<p>Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 985e9a46e10005356bbaf194249f6856-Abstract.html.
[27] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv: Arxiv1810.04805, 2018.
[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: Arxiv-2010.11929, 2020.
[29] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied AI: from simulators to research tasks. IEEE Trans. Emerg. Top. Comput. Intell., 6(2):230-244, 2022. doi: 10.1109/TETCI.2022.3141105. URL https://doi.org/10.1109/TETCI. 2022. 3141105 .
[30] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O. Stanley, and Jeff Clune. Go-explore: a new approach for hard-exploration problems. arXiv preprint arXiv: Arxiv-1901.10995, 2019.
[31] Ashley D. Edwards, Himanshu Sahni, Yannick Schroecker, and Charles L. Isbell. Imitating latent policies from observation. arXiv preprint arXiv: Arxiv-1805.07914, 2018.
[32] William Falcon and The PyTorch Lightning team. PyTorch Lightning. Github, 3 2019. doi: 10.5281/zenodo.3828935. URL https://github.com/PyTorchLightning/ pytorch-lightning.
[33] Linxi Fan<em>, Shyamal Buch</em>, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and Li Fei-Fei. Rubiksnet: Learnable 3d-shift for efficient video action recognition. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.
[34] Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li Fei-Fei, Yuke Zhu, and Anima Anandkumar. Secant: Self-expert cloning for zero-shot generalization of visual policies. arXiv preprint arXiv: Arxiv-2106.09678, 2021.
[35] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv: Arxiv-2004.07219, 2020.
[36] Florian Fuchs, Yunlong Song, Elia Kaufmann, Davide Scaramuzza, and Peter Dürr. Superhuman performance in gran turismo sport using deep reinforcement learning. IEEE Robotics Autom. Lett., 6(3):4257-4264, 2021. doi: 10.1109/LRA.2021.3064284. URL https://doi. org/10.1109/LRA. 2021.3064284.
[37] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
[38] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv: Arxiv-2110.04544, 2021.
[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M. Wallach, Hal Daumé III, and Kate Crawford. Datasheets for datasets. Commun. ACM, 64(12): 86-92, 2021. doi: 10.1145/3458723. URL https://doi.org/10.1145/3458723.
[40] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv: Arxiv-2009.11462, 2020.
[41] Jordan Gerblick. Minecraft, the most-watched game on youtube, passes 1 trillion views, Dec 2021. URL https://www.gamesradar.com/ minecraft-the-most-watched-game-on-youtube-passes-1-trillion-views/.</p>
<p>[42] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The" something something" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017.
[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around the world in 3,000 hours of egocentric video. arXiv preprint arXiv: Arxiv-2110.07058, 2021.
[44] Jonathan Gray, Kavya Srinet, Yacine Jernite, Haonan Yu, Zhuoyuan Chen, Demi Guo, Siddharth Goyal, C. Lawrence Zitnick, and Arthur Szlam. Craftassist: A framework for dialogueenabled interactive agents. arXiv preprint arXiv: Arxiv-1907.08584, 2019.
[45] Djordje Grbic, Rasmus Berg Palm, Elias Najarro, Claire Glanois, and Sebastian Risi. EvoCraft: A New Challenge for Open-Endedness, pages 325-340. Springer International Publishing, 2021. doi: 10.1007/978-3-030-72699-7_21. URL http://link.springer.com/content/ pdf/10.1007/978-3-030-72699-7_21.
[46] Agrim Gupta, Linxi Fan, Surya Ganguli, and Li Fei-Fei. Metamorph: Learning universal controllers with transformers. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=0pmqtk_GvYL.
[47] William H. Guss, Cayden Codel, Katja Hofmann, Brandon Houghton, Noboru Kuno, Stephanie Milani, Sharada Mohanty, Diego Perez Liebana, Ruslan Salakhutdinov, Nicholay Topin, Manuela Veloso, and Phillip Wang. The minerl 2019 competition on sample efficient reinforcement learning using human priors. arXiv preprint arXiv: Arxiv-1904.10079, 2019.
[48] William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. arXiv preprint arXiv: Arxiv-1907.13440, 2019.
[49] William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton, Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke Nakata, Ruslan Salakhutdinov, John Schulman, Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, and Oriol Vinyals. The minerl 2020 competition on sample efficient reinforcement learning using human priors. arXiv preprint arXiv: Arxiv-2101.11071, 2021.
[50] Danijar Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv: Arxiv-2109.06780, 2021.
[51] Laura Hanu and Unitary team. Detoxify. Github. https://github.com/unitaryai/ detoxify, 2020.
[52] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 97269735. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.00975. URL https://doi.org/10.1109/CVPR42600.2020.00975.</p>
<p>[53] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. arXiv preprint arXiv: Arxiv-2111.06377, 2021.
[54] Matthew Henderson, Paweł Budzianowski, Iñigo Casanueva, Sam Coope, Daniela Gerz, Girish Kumar, Nikola Mrkšić, Georgios Spithourakis, Pei-Hao Su, Ivan Vulić, and Tsung-Hsien Wen. A repository of conversational datasets. arXiv preprint arXiv: Arxiv-1904.06472, 2019.
[55] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 6626-6637, 2017. URL https://proceedings.neurips. cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html.
[56] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv: Arxiv2207.05608, 2022.
[57] Joost Huizinga and Jeff Clune. Evolving multimodal robot behavior via many stepping stones with the combinatorial multiobjective evolutionary algorithm. Evolutionary computation, 30 (2):131-164, 2022.
[58] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1273-1286, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/ 099fe6b0b444c23836c4a5d07346082b-Abstract.html.
[59] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv: Arxiv-2210.03094, 2022.
[60] Matthew Johnson, Katja Hofmann, Tim Hutton, and David Bignell. The malmo platform for artificial intelligence experimentation. IJCAI, 2016. URL https://dl.acm.org/doi/10. $5555 / 3061053.3061259$.
[61] Arthur Juliani, Ahmed Khalifa, Vincent-Pierre Berges, Jonathan Harper, Ervin Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange. Obstacle tower: A generalization challenge in vision, control, and planning. In Sarit Kraus, editor, Proceedings of the TwentyEighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 2684-2691. ijcai.org, 2019. doi: 10.24963/ijcai.2019/373. URL https://doi.org/10.24963/ijcai.2019/373.
[62] Anssi Kanervisto, Stephanie Milani, Karolis Ramanauskas, Nicholay Topin, Zichuan Lin, Junyou Li, Jianing Shi, Deheng Ye, Qiang Fu, Wei Yang, Weijun Hong, Zhongyue Huang, Haicheng Chen, Guangjun Zeng, Yue Lin, Vincent Micheli, Eloi Alonso, François Fleuret, Alexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned. arXiv preprint arXiv: Arxiv2202.10583, 2022.
[63] Ingmar Kanitscheider, Joost Huizinga, David Farhi, William Hebgen Guss, Brandon Houghton, Raul Sampedro, Peter Zhokhov, Bowen Baker, Adrien Ecoffet, Jie Tang, Oleg Klimov, and Jeff Clune. Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint arXiv: Arxiv-2106.14876, 2021.
[64] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset. arXiv preprint arXiv: Arxiv1705.06950, 2017.</p>
<p>[65] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. Abstractive summarization of reddit posts with multi-level memory networks. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2519-2531. Association for Computational Linguistics, 2019. doi: 10.18653/v1/n19-1260. URL https://doi.org/10.18653/v1/n19-1260.
[66] Julia Kiseleva, Ziming Li, Mohammad Aliannejadi, Shrestha Mohanty, Maartje ter Hoeve, Mikhail Burtsev, Alexey Skrynnik, Artem Zholus, Aleksandr Panov, Kavya Srinet, Arthur Szlam, Yuxuan Sun, Katja Hofmann, Michel Galley, and Ahmed Awadallah. Neurips 2021 competition iglu: Interactive grounded language understanding in a collaborative environment. arXiv preprint arXiv: Arxiv-2110.06536, 2021.
[67] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv: Arxiv-2205.11916, 2022.
[68] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv: Arxiv-1712.05474, 2017.
[69] Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktäschel. The nethack learning environment. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 7671-7684. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 569ff987c643b4bedf504efda8f786c2-Paper.pdf.
[70] Label Studio. Label studio. https://labelstud.io/, 2020. Accessed: 2022-06-06.
[71] WB Langdon. Pfeiffer-a distributed open-ended evolutionary system. In AISB, volume 5, pages $7-13$. Citeseer, 2005.
[72] Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Jacob Andreas, Igor Mordatch, Antonio Torralba, and Yuke Zhu. Pre-trained language models for interactive decision-making. arXiv preprint arXiv: Arxiv-2202.01771, 2022.
[73] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https: //openreview.net/forum?id=Skq89Scxx.
[74] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. arXiv preprint arXiv: Arxiv-2206.08916, 2022.
[75] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv: Arxiv2104.08860, 2021.
[76] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ.
[77] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. arXiv preprint arXiv: Arxiv-1906.03327, 2019.</p>
<p>[78] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from uncurated instructional videos. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 9876-9886. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.00990. URL https://openaccess.thecvf.com/content_ CVPR_2020/html/Miech_End-to-End_Learning_of_Visual_Representations_ From_Uncurated_Instructional_Videos_CVPR_2020_paper.html.
[79] Minecraft Wiki. Minecraft wiki. hhttps://minecraft.fandom.com/wiki/Minecraft_ Wiki, 2016. Accessed: 2022-06-06.
[80] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv: Arxiv-1312.5602, 2013.
[81] Suraj Nair, Eric Mitchell, Kevin Chen, Brian Ichter, Silvio Savarese, and Chelsea Finn. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Research, pages 1303-1315. PMLR, 2021. URL https://proceedings.mlr.press/v164/ nair22a.html.
[82] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universal visual representation for robot manipulation. arXiv preprint arXiv: Arxiv-2203.12601, 2022.
[83] Evgenii Nikishin, Max Schwarzer, Pierluca D’Oro, Pierre-Luc Bacon, and Aaron Courville. The primacy bias in deep reinforcement learning. arXiv preprint arXiv: Arxiv-2205.07802, 2022.
[84] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In International Conference on Machine Learning, pages 3878-3887. PMLR, 2018.
[85] OpenAI, :, Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Józefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv: Arxiv-1912.06680, 2019.
[86] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv: Arxiv-1912.01703, 2019.
[87] Diego Perez-Liebana, Katja Hofmann, Sharada Prasanna Mohanty, Noburu Kuno, Andre Kramer, Sam Devlin, Raluca D. Gaina, and Daniel Ionita. The multi-agent reinforcement learning in malmÓ (marlÓ) competition. arXiv preprint arXiv: Arxiv-1901.08129, 2019.
[88] PRAW: The Python Reddit API Wrapper. Praw: The python reddit api wrapper. https: //github.com/praw-dev/praw, 2010. Accessed: 2022-06-06.
[89] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome: Simulating household activities via programs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 8494-8502. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00886. URL http://openaccess.thecvf.com/content_cvpr_ 2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html.
[90] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI, 2018.</p>
<p>[91] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[92] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748-8763. PMLR, 2021.
[93] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv: Arxiv-2204.06125, 2022.
[94] Harish Ravichandar, Athanasios S Polydoros, Sonia Chernova, and Aude Billard. Recent advances in robot learning from demonstration. Annual review of control, robotics, and autonomous systems, 3:297-330, 2020.
[95] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. arXiv preprint arXiv: Arxiv-2205.06175, 2022.
[96] Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can wikipedia help offline reinforcement learning? arXiv preprint arXiv: Arxiv-2201.12122, 2022.
[97] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv: Arxiv2205.11487, 2022.
[98] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages 2226-2234, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/ 8a3363abe792db2d8761d6403605aeb7-Abstract.html.
[99] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.
[100] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.
[101] John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1506.02438.
[102] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv: Arxiv-1707.06347, 2017.
[103] Selenium WebDriver. Selenium webdriver. https://www.selenium.dev/, 2011. Accessed: 2022-06-06.
[104] Rohin Shah, Cody Wild, Steven H. Wang, Neel Alex, Brandon Houghton, William Guss, Sharada Mohanty, Anssi Kanervisto, Stephanie Milani, Nicholay Topin, Pieter Abbeel, Stuart Russell, and Anca Dragan. The minerl basalt competition on learning from human feedback. arXiv preprint arXiv: Arxiv-2107.01969, 2021.</p>
<p>[105] Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning manipulation concepts from instructions and human demonstrations. The International Journal of Robotics Research, 40(12-14):1419-1434, 2021.
[106] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv: Arxiv-2002.05202, 2020.
[107] Bokui Shen, Fei Xia, Chengshu Li, Roberto Martín-Martín, Linxi Fan, Guanzhi Wang, Claudia Pérez-D'Arpino, Shyamal Buch, Sanjana Srivastava, Lyne P. Tchapmi, Micael E. Tchapmi, Kent Vainio, Josiah Wong, Li Fei-Fei, and Silvio Savarese. igibson 1.0: a simulation environment for interactive tasks in large realistic scenes. arXiv preprint arXiv: Arxiv-2012.02924, 2020.
[108] Tianlin Tim Shi, Andrej Karpathy, Linxi Jim Fan, Jonathan Hernandez, and Percy Liang. World of bits: an open-domain platform for web-based agents. ICML, 2017. URL https : //dl.acm.org/doi/10.5555/3305890.3306005.
[109] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019.
[110] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 10737-10746. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01075. URL https://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_ A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_ CVPR_2020_paper.html.
[111] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. arXiv preprint arXiv: Arxiv-2109.12098, 2021.
[112] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, and Demis Hassabis. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv: Arxiv-1712.01815, 2017.
[113] Laura Smith, Nikita Dhawan, Marvin Zhang, Pieter Abbeel, and Sergey Levine. Avid: Learning multi-stage tasks via pixel-level translation of human videos. arXiv preprint arXiv: Arxiv-1912.04443, 2019.
[114] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, C. Karen Liu, Silvio Savarese, Hyowon Gweon, Jiajun Wu, and Li Fei-Fei. BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments. In Aleksandra Faust, David Hsu, and Gerhard Neumann, editors, Conference on Robot Learning, 8-11 November 2021, London, UK, volume 164 of Proceedings of Machine Learning Research, pages 477-490. PMLR, 2021. URL https://proceedings.mlr.press/v164/srivastava22a.html.
[115] Bradly C. Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. arXiv preprint arXiv: Arxiv-1703.01703, 2017.
[116] Russell K Standish. Open-ended artificial evolution. International Journal of Computational Intelligence and Applications, 3(02):167-175, 2003.
[117] Kenneth O Stanley, Joel Lehman, and Lisa Soros. Open-endedness: The last grand challenge you've never heard of. O'Reilly Online,, 2017.
[118] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations using contrastive bidirectional transformer. arXiv preprint arXiv: Arxiv-1906.05743, 2019 .</p>            </div>
        </div>

    </div>
</body>
</html>
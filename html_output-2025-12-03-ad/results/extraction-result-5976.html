<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5976 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5976</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5976</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-119.html">extraction-schema-119</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-abe19d0bab2fcf494eaaca33d0c6e622384985bf</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/abe19d0bab2fcf494eaaca33d0c6e622384985bf" target="_blank">DARWIN Series: Domain Specific Large Language Models for Natural Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models, and showcases the ability of LLM in the scientific domain.</p>
                <p><strong>Paper Abstract:</strong> Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichment of automation of the discovery process, we present DARWIN, a series of tailored LLMs for natural science, mainly in physics, chemistry, and material science. This series relies on open-source LLM, incorporating structured and unstructured scientific knowledge from public datasets and literature. We fine-tuned the models using over 60,000 instruction data points, emphasizing factual correctness. During the fine-tuning, we introduce the Scientific Instruction Generation (SIG) model, automating instruction generation from scientific texts. This eliminates the need for manual extraction or domain-specific knowledge graphs and efficiently injects scientific knowledge into the model. We also explore multi-task training strategies, revealing interconnections between scientific tasks. DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models. Our research showcases the ability of LLM in the scientific domain, with the overarching goal of fostering prosperity within the broader AI for science community.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5976.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5976.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN-SIG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN Scientific Instruction Generation (SIG) model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuned open-source LLM generator that converts full-text scientific papers into high-quality question-answer pairs, extracting factual details including chemical elements and numerical values to create instruction data for downstream model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN-SIG (fine-tuned Vicuna-7B; seed QA from GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7B-based model fine-tuned to generate instruction-style Q&A from segmented full-text scientific papers; initial seed Q&A generated with GPT-4 and then used to supervise Vicuna-7B; segmentation using a '[TBC]' token to handle long texts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Materials science, chemistry, physics (general natural science literature)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Full-text scholarly literature subset assembled from Web of Science (paper collection described as ~6 million records filtered to materials-related disciplines and then pruned for similarity via MatBERT embeddings). Seed papers were selected for GPT-4 QA generation; remaining papers were processed by SIG to produce large numbers of Q&A pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-engineered supervised generation: human-designed prompts → seed QA via GPT-4 → fine-tune Vicuna-7B to map (prompt, segmented full-text) → generate Q&A at scale; uses segmentation connectors and iterative prompt refinement. Output is Q&A extraction rather than symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Extraction of numeric facts and empirical property-value relations presented as question-answer pairs (numerical facts, property mappings), not explicit algebraic law discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>Example outputs include numeric property extractions in Q&A form (e.g., OPV example: question about power conversion efficiency for a donor/acceptor pair with output '0.96478299093449994'; regression examples such as aqueous solubility outputs like '-3.8').</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation of generated Q&A quality and specificity; comparison versus a baseline 'self-questioning' approach (GPT-3.5) in terms of informativeness and specificity; use of generated Q&A downstream for fine-tuning and evaluating final models on standard benchmarks (SciQ, FAIR dataset tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SIG-generated Q&A were judged more informative and specific than GPT-3.5 self-questioning baseline, producing answers that preserve precise elements and numerical values from source papers; SIG enabled cost-effective scaling of instruction data used to improve downstream DARWIN models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Depends on seed generation quality (initial GPT-4 seed); risk of propagated hallucinations if generator is imperfect; handling of long documents requires segmentation heuristics; possible model collapse if only model-generated data are used without care (paper notes this risk generally).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to a self-questioning baseline using GPT-3.5, SIG (Vicuna-7B fine-tuned) produced more informative, specific Q&A (including explicit numeric values); SIG is presented as cheaper than using GPT-4 at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DARWIN Series: Domain Specific Large Language Models for Natural Science', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5976.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5976.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DARWIN-MDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DARWIN Material & Device Prediction (MDP) model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned LLaMA-7B derived model trained on >60,000 scientific instruction demonstrations (SciQ, SIG-generated Q&A, and 16 FAIR datasets) to perform scientific tasks including regression (property prediction), classification, and inverse design in materials and device domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DARWIN-MDP (fine-tuned LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Based on LLaMA-7B (7 billion parameters), fine-tuned with instruction-following demonstrations (~60k) using supervised instruction-tuning (BF16, 3 epochs, learning rate 2e-5, gradient accumulation). Further fine-tuned on mixed tasks from 16 FAIR datasets to support classification, regression, and inverse design prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Materials science, chemistry, physics, photovoltaics, metal-organic frameworks, molecular property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Training signals derived from: SciQ question-answer dataset (13,679 Qs with supporting paragraphs), SIG-generated Q&A from a corpus assembled from Web of Science (filtered full-text material-science papers), plus 16 open FAIR datasets (ChEMBL, ESOL, OPV, Matbench suites, Moosavi datasets, etc.). The paper reports assembling instruction data totaling >60,000 examples.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised instruction-tuning on large curated Q&A and structured dataset-derived instructions (multi-task learning across classification, regression, inverse design), i.e., the model learns to map inputs (SMILES, compositions, paper paragraphs) to numeric property outputs or labels—no explicit symbolic regression or equation discovery methods were described.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Empirical predictive mappings/regression models learned by the LLM that map structured textual/structural inputs (SMILES, composition strings, descriptors) to numeric properties (PCE, solubility, band gap, heat capacity), presented as numeric outputs rather than closed-form equations.</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>Examples reported as model outputs: OPV PCE prediction for a given donor/acceptor → '0.96478299093449994'; ESOL aqueous solubility output '7.762471166286911e-05'; Matbench_expt_gap for Sr5Cd2Sb5F → '0.0'; MoosaviCp heat capacity prediction → '4.460577901'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Downstream benchmarking using standard metrics: SciQ accuracy for QA (random 1,000 question test), Mean Absolute Error (MAE) for regression tasks, and F1 (binary/macro) for classification tasks; comparisons made against baseline LLaMA variants and dedicated ML models (e.g., MODNet, MolCLR, gptchem).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DARWIN-BASE achieved 96.9% accuracy on SciQ (test subset). DARWIN-MDP yielded lower MAE on many regression tasks (e.g., OPV MAE 0.54 vs gptchem 0.65 and LLaMA-all 0.56) and high classification F1 scores (e.g., Pei F1 = 97.2%, WaterStability F1 = 93.3%), demonstrating competitive or superior performance relative to baseline LLaMA variants and some dedicated ML baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>LLMs are noted as limited in producing high-precision real numbers—training uses rounded values; inverse design evaluation is left for future work due to difficulty of evaluation; reliance on a limited number of Q&A pairs and FAIR datasets may constrain generality; potential for hallucination and dependence on the quality of instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared against LLaMA-single/all and gptchem: DARWIN-MDP often achieved lower MAE and higher F1 on many tasks; performance is comparable to or better than some dedicated machine-learning methods on specific datasets as reported in the paper (tables compare MAE/F1 across models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DARWIN Series: Domain Specific Large Language Models for Natural Science', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5976.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5976.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SII</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured Information Inference (SII)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An information-extraction approach (cited by the authors) that extracts structured device- and materials-level information from literature and uses the extracted information as instructive data to fine-tune LLMs for property prediction tasks such as power conversion efficiency (PCE).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models as master key: Unlocking the secrets of materials science with gpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SII (used to generate instructive structured data; fine-tuning applied to GPT-3 in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SII is described as a structured information-extraction pipeline that produces instruction-format data from scientific texts; in the cited work the extracted structured information was used to fine-tune GPT-3 for device property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Materials science, device-level property prediction (e.g., photovoltaics)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in detail in this paper; described generally as extracted information from literature at the device/materials level and used as training/instruction data for fine-tuning LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Structured information extraction from papers to produce instructive training examples for LLM fine-tuning; i.e., transforming literature into structured (input, target) pairs for supervised learning rather than explicit symbolic law extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Empirical predictive relationships (property prediction models learned from literature-derived structured data), rather than explicit closed-form equations.</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>Cited application: extracted info used to fine-tune GPT-3 to predict power conversion efficiency (PCE) at the device level; specific equations are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>As reported by the cited work: evaluate fine-tuned LLM predictive performance on relevant device/property benchmarks (paper does not give numeric details here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as enabling GPT-3 fine-tuning for device prediction (PCE), facilitating predictive performance; used in prior work referenced by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper notes that structured extraction approaches are less common in general natural science compared to e.g., medical field; specific limitations of SII are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Described as a way to produce instructive data for LLMs, complementing other self-instruct or GPT-based approaches; comparative metrics are in the cited work rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DARWIN Series: Domain Specific Large Language Models for Natural Science', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5976.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5976.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 fine-tuned for low-data discovery in chemistry (Jablonka et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (cited) fine-tuned GPT-3 on chemistry tasks (regression, classification, inverse design) and reported that fine-tuned GPT-3 often matches or exceeds performance of conventional ML models while requiring less training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is GPT-3 all you need for low-data discovery in chemistry ?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (175B, fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is a 175-billion-parameter autoregressive transformer; in the cited study it was fine-tuned for chemistry-related tasks including property regression, classification, and inverse molecular design.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Chemistry, molecular property prediction and inverse design</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>As reported in the cited study: fine-tuning datasets comprising chemical examples (SMILES, property labels) for low-data discovery tasks; exact corpus details not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning on task-specific examples to map molecular representations (SMILES) to properties or to generate candidate molecules (inverse design).</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Empirical predictive mappings (regression/classification) from molecular structure to properties; inverse-design generative outputs rather than explicit symbolic equations.</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>Paper reports GPT-3 was used for regression and inverse design of molecular properties; no explicit algebraic laws are reported in this citing paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Comparison of predictive performance and data-efficiency against conventional ML baselines as reported in Jablonka et al.; details are in the cited reference.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited conclusion: fine-tuned GPT-3 often performs on par with or better than conventional models for low-data chemical discovery tasks and can require less training data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed in this paper; general concerns include model accessibility and calibration of confidence, and the need for domain-specific data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Reported by the cited study to be comparable or superior to conventional machine-learning models on certain tasks (as summarized in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DARWIN Series: Domain Specific Large Language Models for Natural Science', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5976.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5976.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or AI systems being used to extract, discover, or distill quantitative laws, mathematical relationships, or empirical equations from large collections of scientific or scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-trained LLM trained on a massive corpus of scientific articles, web content, and textbooks (reported training corpus of ~48 million scientific examples); claimed capabilities include solving mathematical problems and annotating molecular/protein structures but criticized for hallucination and inability to reliably distinguish truth from falsehood.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica (domain-trained large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based LLM trained on a large scientific corpus (paper states training data included ~48 million examples spanning scientific articles, textbooks, lecture notes, etc.); intended to support scientific tasks like math problem solving and domain annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Broad scientific literature (multidisciplinary), including mathematics and molecular/protein annotation</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Trained on a corpus described in the cited work as ~48 million scientific articles, web pages, textbooks, lecture notes and encyclopedic references; specifics are in the Galactica paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Pretraining on large scientific corpora to internalize patterns and facts; not described as performing explicit symbolic regression or automated law discovery in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_law_type</strong></td>
                            <td>Potentially able to reflect numerical and mathematical relationships learned in pretraining, but the paper notes Galactica struggles to reliably distinguish truth from falsehood (hallucinations) so explicit law distillation is unreliable.</td>
                        </tr>
                        <tr>
                            <td><strong>example_law_extracted</strong></td>
                            <td>No explicit equations or laws are reported in this paper as distilled by Galactica; the paper only cites Galactica's claimed capabilities and its limitation (hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced qualitatively here: cautionary note about hallucination and inability to consistently separate correct from incorrect scientific claims; quantitative evaluations are in the Galactica paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Galactica is cited as an ambitious domain-trained scientific LLM with broad capabilities, but criticized for unreliable factual accuracy (hallucination), limiting trust in any distilled laws without external validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Hallucination and inability to discriminate truth from falsehood when reporting scientific claims; thus outputs (including any candidate laws) require careful validation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Mentioned as a domain-trained LLM contrasted with approaches that integrate external APIs or knowledge graphs to improve reliability; no direct numeric comparison provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DARWIN Series: Domain Specific Large Language Models for Natural Science', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models as master key: Unlocking the secrets of materials science with gpt <em>(Rating: 2)</em></li>
                <li>Is GPT-3 all you need for low-data discovery in chemistry ? <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 1)</em></li>
                <li>SciBERT: A pretrained language model for scientific text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5976",
    "paper_id": "paper-abe19d0bab2fcf494eaaca33d0c6e622384985bf",
    "extraction_schema_id": "extraction-schema-119",
    "extracted_data": [
        {
            "name_short": "DARWIN-SIG",
            "name_full": "DARWIN Scientific Instruction Generation (SIG) model",
            "brief_description": "A fine-tuned open-source LLM generator that converts full-text scientific papers into high-quality question-answer pairs, extracting factual details including chemical elements and numerical values to create instruction data for downstream model fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DARWIN-SIG (fine-tuned Vicuna-7B; seed QA from GPT-4)",
            "model_description": "Vicuna-7B-based model fine-tuned to generate instruction-style Q&A from segmented full-text scientific papers; initial seed Q&A generated with GPT-4 and then used to supervise Vicuna-7B; segmentation using a '[TBC]' token to handle long texts.",
            "task_domain": "Materials science, chemistry, physics (general natural science literature)",
            "input_corpus_description": "Full-text scholarly literature subset assembled from Web of Science (paper collection described as ~6 million records filtered to materials-related disciplines and then pruned for similarity via MatBERT embeddings). Seed papers were selected for GPT-4 QA generation; remaining papers were processed by SIG to produce large numbers of Q&A pairs.",
            "distillation_method": "Prompt-engineered supervised generation: human-designed prompts → seed QA via GPT-4 → fine-tune Vicuna-7B to map (prompt, segmented full-text) → generate Q&A at scale; uses segmentation connectors and iterative prompt refinement. Output is Q&A extraction rather than symbolic regression.",
            "quantitative_law_type": "Extraction of numeric facts and empirical property-value relations presented as question-answer pairs (numerical facts, property mappings), not explicit algebraic law discovery.",
            "example_law_extracted": "Example outputs include numeric property extractions in Q&A form (e.g., OPV example: question about power conversion efficiency for a donor/acceptor pair with output '0.96478299093449994'; regression examples such as aqueous solubility outputs like '-3.8').",
            "evaluation_method": "Human evaluation of generated Q&A quality and specificity; comparison versus a baseline 'self-questioning' approach (GPT-3.5) in terms of informativeness and specificity; use of generated Q&A downstream for fine-tuning and evaluating final models on standard benchmarks (SciQ, FAIR dataset tasks).",
            "results_summary": "SIG-generated Q&A were judged more informative and specific than GPT-3.5 self-questioning baseline, producing answers that preserve precise elements and numerical values from source papers; SIG enabled cost-effective scaling of instruction data used to improve downstream DARWIN models.",
            "limitations_challenges": "Depends on seed generation quality (initial GPT-4 seed); risk of propagated hallucinations if generator is imperfect; handling of long documents requires segmentation heuristics; possible model collapse if only model-generated data are used without care (paper notes this risk generally).",
            "comparison_to_baselines": "Compared to a self-questioning baseline using GPT-3.5, SIG (Vicuna-7B fine-tuned) produced more informative, specific Q&A (including explicit numeric values); SIG is presented as cheaper than using GPT-4 at scale.",
            "uuid": "e5976.0",
            "source_info": {
                "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "DARWIN-MDP",
            "name_full": "DARWIN Material & Device Prediction (MDP) model",
            "brief_description": "An instruction-tuned LLaMA-7B derived model trained on &gt;60,000 scientific instruction demonstrations (SciQ, SIG-generated Q&A, and 16 FAIR datasets) to perform scientific tasks including regression (property prediction), classification, and inverse design in materials and device domains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DARWIN-MDP (fine-tuned LLaMA-7B)",
            "model_description": "Based on LLaMA-7B (7 billion parameters), fine-tuned with instruction-following demonstrations (~60k) using supervised instruction-tuning (BF16, 3 epochs, learning rate 2e-5, gradient accumulation). Further fine-tuned on mixed tasks from 16 FAIR datasets to support classification, regression, and inverse design prompts.",
            "task_domain": "Materials science, chemistry, physics, photovoltaics, metal-organic frameworks, molecular property prediction",
            "input_corpus_description": "Training signals derived from: SciQ question-answer dataset (13,679 Qs with supporting paragraphs), SIG-generated Q&A from a corpus assembled from Web of Science (filtered full-text material-science papers), plus 16 open FAIR datasets (ChEMBL, ESOL, OPV, Matbench suites, Moosavi datasets, etc.). The paper reports assembling instruction data totaling &gt;60,000 examples.",
            "distillation_method": "Supervised instruction-tuning on large curated Q&A and structured dataset-derived instructions (multi-task learning across classification, regression, inverse design), i.e., the model learns to map inputs (SMILES, compositions, paper paragraphs) to numeric property outputs or labels—no explicit symbolic regression or equation discovery methods were described.",
            "quantitative_law_type": "Empirical predictive mappings/regression models learned by the LLM that map structured textual/structural inputs (SMILES, composition strings, descriptors) to numeric properties (PCE, solubility, band gap, heat capacity), presented as numeric outputs rather than closed-form equations.",
            "example_law_extracted": "Examples reported as model outputs: OPV PCE prediction for a given donor/acceptor → '0.96478299093449994'; ESOL aqueous solubility output '7.762471166286911e-05'; Matbench_expt_gap for Sr5Cd2Sb5F → '0.0'; MoosaviCp heat capacity prediction → '4.460577901'.",
            "evaluation_method": "Downstream benchmarking using standard metrics: SciQ accuracy for QA (random 1,000 question test), Mean Absolute Error (MAE) for regression tasks, and F1 (binary/macro) for classification tasks; comparisons made against baseline LLaMA variants and dedicated ML models (e.g., MODNet, MolCLR, gptchem).",
            "results_summary": "DARWIN-BASE achieved 96.9% accuracy on SciQ (test subset). DARWIN-MDP yielded lower MAE on many regression tasks (e.g., OPV MAE 0.54 vs gptchem 0.65 and LLaMA-all 0.56) and high classification F1 scores (e.g., Pei F1 = 97.2%, WaterStability F1 = 93.3%), demonstrating competitive or superior performance relative to baseline LLaMA variants and some dedicated ML baselines.",
            "limitations_challenges": "LLMs are noted as limited in producing high-precision real numbers—training uses rounded values; inverse design evaluation is left for future work due to difficulty of evaluation; reliance on a limited number of Q&A pairs and FAIR datasets may constrain generality; potential for hallucination and dependence on the quality of instruction data.",
            "comparison_to_baselines": "Compared against LLaMA-single/all and gptchem: DARWIN-MDP often achieved lower MAE and higher F1 on many tasks; performance is comparable to or better than some dedicated machine-learning methods on specific datasets as reported in the paper (tables compare MAE/F1 across models).",
            "uuid": "e5976.1",
            "source_info": {
                "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "SII",
            "name_full": "Structured Information Inference (SII)",
            "brief_description": "An information-extraction approach (cited by the authors) that extracts structured device- and materials-level information from literature and uses the extracted information as instructive data to fine-tune LLMs for property prediction tasks such as power conversion efficiency (PCE).",
            "citation_title": "Large language models as master key: Unlocking the secrets of materials science with gpt",
            "mention_or_use": "mention",
            "model_name": "SII (used to generate instructive structured data; fine-tuning applied to GPT-3 in cited work)",
            "model_description": "SII is described as a structured information-extraction pipeline that produces instruction-format data from scientific texts; in the cited work the extracted structured information was used to fine-tune GPT-3 for device property prediction.",
            "task_domain": "Materials science, device-level property prediction (e.g., photovoltaics)",
            "input_corpus_description": "Not specified in detail in this paper; described generally as extracted information from literature at the device/materials level and used as training/instruction data for fine-tuning LLMs.",
            "distillation_method": "Structured information extraction from papers to produce instructive training examples for LLM fine-tuning; i.e., transforming literature into structured (input, target) pairs for supervised learning rather than explicit symbolic law extraction.",
            "quantitative_law_type": "Empirical predictive relationships (property prediction models learned from literature-derived structured data), rather than explicit closed-form equations.",
            "example_law_extracted": "Cited application: extracted info used to fine-tune GPT-3 to predict power conversion efficiency (PCE) at the device level; specific equations are not provided in this paper.",
            "evaluation_method": "As reported by the cited work: evaluate fine-tuned LLM predictive performance on relevant device/property benchmarks (paper does not give numeric details here).",
            "results_summary": "Cited as enabling GPT-3 fine-tuning for device prediction (PCE), facilitating predictive performance; used in prior work referenced by the authors.",
            "limitations_challenges": "Paper notes that structured extraction approaches are less common in general natural science compared to e.g., medical field; specific limitations of SII are not detailed here.",
            "comparison_to_baselines": "Described as a way to produce instructive data for LLMs, complementing other self-instruct or GPT-based approaches; comparative metrics are in the cited work rather than this paper.",
            "uuid": "e5976.2",
            "source_info": {
                "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 (Jablonka et al.)",
            "name_full": "GPT-3 fine-tuned for low-data discovery in chemistry (Jablonka et al.)",
            "brief_description": "Prior work (cited) fine-tuned GPT-3 on chemistry tasks (regression, classification, inverse design) and reported that fine-tuned GPT-3 often matches or exceeds performance of conventional ML models while requiring less training data.",
            "citation_title": "Is GPT-3 all you need for low-data discovery in chemistry ?",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (175B, fine-tuned)",
            "model_description": "GPT-3 is a 175-billion-parameter autoregressive transformer; in the cited study it was fine-tuned for chemistry-related tasks including property regression, classification, and inverse molecular design.",
            "task_domain": "Chemistry, molecular property prediction and inverse design",
            "input_corpus_description": "As reported in the cited study: fine-tuning datasets comprising chemical examples (SMILES, property labels) for low-data discovery tasks; exact corpus details not provided in this paper.",
            "distillation_method": "Supervised fine-tuning on task-specific examples to map molecular representations (SMILES) to properties or to generate candidate molecules (inverse design).",
            "quantitative_law_type": "Empirical predictive mappings (regression/classification) from molecular structure to properties; inverse-design generative outputs rather than explicit symbolic equations.",
            "example_law_extracted": "Paper reports GPT-3 was used for regression and inverse design of molecular properties; no explicit algebraic laws are reported in this citing paper.",
            "evaluation_method": "Comparison of predictive performance and data-efficiency against conventional ML baselines as reported in Jablonka et al.; details are in the cited reference.",
            "results_summary": "Cited conclusion: fine-tuned GPT-3 often performs on par with or better than conventional models for low-data chemical discovery tasks and can require less training data.",
            "limitations_challenges": "Not detailed in this paper; general concerns include model accessibility and calibration of confidence, and the need for domain-specific data.",
            "comparison_to_baselines": "Reported by the cited study to be comparable or superior to conventional machine-learning models on certain tasks (as summarized in this paper).",
            "uuid": "e5976.3",
            "source_info": {
                "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Galactica",
            "name_full": "Galactica: A large language model for science",
            "brief_description": "A domain-trained LLM trained on a massive corpus of scientific articles, web content, and textbooks (reported training corpus of ~48 million scientific examples); claimed capabilities include solving mathematical problems and annotating molecular/protein structures but criticized for hallucination and inability to reliably distinguish truth from falsehood.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "model_name": "Galactica (domain-trained large language model)",
            "model_description": "A transformer-based LLM trained on a large scientific corpus (paper states training data included ~48 million examples spanning scientific articles, textbooks, lecture notes, etc.); intended to support scientific tasks like math problem solving and domain annotation.",
            "task_domain": "Broad scientific literature (multidisciplinary), including mathematics and molecular/protein annotation",
            "input_corpus_description": "Trained on a corpus described in the cited work as ~48 million scientific articles, web pages, textbooks, lecture notes and encyclopedic references; specifics are in the Galactica paper.",
            "distillation_method": "Pretraining on large scientific corpora to internalize patterns and facts; not described as performing explicit symbolic regression or automated law discovery in this paper.",
            "quantitative_law_type": "Potentially able to reflect numerical and mathematical relationships learned in pretraining, but the paper notes Galactica struggles to reliably distinguish truth from falsehood (hallucinations) so explicit law distillation is unreliable.",
            "example_law_extracted": "No explicit equations or laws are reported in this paper as distilled by Galactica; the paper only cites Galactica's claimed capabilities and its limitation (hallucination).",
            "evaluation_method": "Referenced qualitatively here: cautionary note about hallucination and inability to consistently separate correct from incorrect scientific claims; quantitative evaluations are in the Galactica paper itself.",
            "results_summary": "Galactica is cited as an ambitious domain-trained scientific LLM with broad capabilities, but criticized for unreliable factual accuracy (hallucination), limiting trust in any distilled laws without external validation.",
            "limitations_challenges": "Hallucination and inability to discriminate truth from falsehood when reporting scientific claims; thus outputs (including any candidate laws) require careful validation.",
            "comparison_to_baselines": "Mentioned as a domain-trained LLM contrasted with approaches that integrate external APIs or knowledge graphs to improve reliability; no direct numeric comparison provided here.",
            "uuid": "e5976.4",
            "source_info": {
                "paper_title": "DARWIN Series: Domain Specific Large Language Models for Natural Science",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models as master key: Unlocking the secrets of materials science with gpt",
            "rating": 2
        },
        {
            "paper_title": "Is GPT-3 all you need for low-data discovery in chemistry ?",
            "rating": 2
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2
        },
        {
            "paper_title": "Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature",
            "rating": 2
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 1
        },
        {
            "paper_title": "SciBERT: A pretrained language model for scientific text",
            "rating": 1
        }
    ],
    "cost": 0.017946,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DARWIN SERIES: DOMAIN SPECIFIC LARGE LANGUAGE MODELS FOR NATURAL SCIENCE</h1>
<p>A Preprint<br>Tong Xie ${ }^{1,2,<em>, * </em>}$, Yuwei Wan ${ }^{2,3 <em>}$, Wei Huang ${ }^{2,6}$, Zhenyu Yin ${ }^{2,6}$, Yixuan Liu ${ }^{2,6}$, Shaozhou Wang ${ }^{1,2}$, Qingyuan Linghu ${ }^{2,6}$, Chunyu Kit ${ }^{3}$, Clara Grazian ${ }^{4,5}$, Wenjie Zhang ${ }^{6}$, Imran Razzak ${ }^{6}$, and Bram Hoex ${ }^{1, * </em>}$<br>${ }^{1}$ School of Photovoltaic and Renewable Energy Engineering, University of New South Wales, Kensington, NSW, Australia<br>${ }^{2}$ GreenDynamics Pty. Ltd, Kensington, NSW, Australia<br>${ }^{3}$ Department of Linguistics and Translation, City University of Hong Kong, Hong Kong, China<br>${ }^{4}$ School of Mathematics and Statistics, University of Sydney, Camperdown, NSW, Australia<br>${ }^{5}$ DARE ARC Training Centre in Data Analytics for Resources and Environments, Australia<br>${ }^{6}$ School of Computer Science and Engineering, University of New South Wales, Kensington, NSW, Australia<br>${ }^{7}$ School of Computing and Information Technology, University of Melbourne, Parkville, VIC, Australia<br>${ }^{8}$ Authors contributed equally<br>${ }^{<em> </em>}$ Corresponding author: tong.xie@unsw.edu.au, b.hoex@unsw.edu.au</p>
<p>August 24, 2023</p>
<h4>Abstract</h4>
<p>Emerging tools bring forth fresh approaches to work, and the field of natural science is no different. In natural science, traditional manual, serial, and labour-intensive work is being augmented by automated, parallel, and iterative processes driven by artificial intelligence-based experimental automation and more. To add new capabilities in natural science, enabling the acceleration and enrichment of automation of the discovery process, we present DARWIN, a series of tailored LLMs for natural science, mainly in physics, chemistry, and material science. This series relies on opensource LLM, incorporating structured and unstructured scientific knowledge from public datasets and literature. We fine-tuned the models using over 60,000 instruction data points, emphasizing factual correctness. During the fine-tuning, we introduce the Scientific Instruction Generation (SIG) model, automating instruction generation from scientific texts. This eliminates the need for manual extraction or domain-specific knowledge graphs and efficiently injects scientific knowledge into the model. We also explore multi-task training strategies, revealing interconnections between scientific tasks. DARWIN series not only achieves state-of-the-art results on various scientific tasks but also diminishes reliance on closed-source AI models. Our research showcases the ability of LLM in the scientific domain, with the overarching goal of fostering prosperity within the broader AI for science community.</p>
<p>Keywords Natural Language Processing $\cdot$ Large Language Model $\cdot$ Instruction Generation $\cdot$ AI for Science</p>
<h2>1 Introduction</h2>
<p>Code is the language of computers, and molecular atomic structure is the language of material science and chemistry [1, 2]. Discovering molecular atomic structures and their properties and utilizing them for practical applications is extremely time-consuming and labour-intensive. AI-driven approaches accelerate this by harnessing the material science knowledge developed over centuries through rigorous scientific procedures ensuring a cohesive and seamless framework. For example, AI can learn from large molecular database structures and then can be used to provide viable chemical compounds and their properties, which may accelerate the discovery of new materials or their development.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: DARWIN vs GPT-4 comparative analysis in natural science tasks
Instruction-tuned Large Language Models (LLMs), represented by InstructGPT [3], have captivated the academic community due to their proficiency in understanding directives and generating responses akin to human conversation. Compared to their smaller counterparts, LLMs demonstrate potent generalisation across various Natural Language Processing (NLP) tasks, illustrating their capacity to resolve unseen or intricate challenges in natural science. Even though fine-tuned GPT Models have shown an impressive ability to both domain-specific NLP tasks along with material and device prediction (MDP) [4, 5], the core issue with these models lies in the fact that they remain inaccessible as open-source platforms, consequently compelling each user to engage in the laborious and financially burdensome task of individually fine-tuning the model on OpenAI's servers. Additionally, they fail to offer a robust degree of confidence in their precision, an aspect that can be viewed as a significant shortfall. Such constraints have the potential to impose a brake on the momentum at which LLMs are propelling scientific discovery.
The emergence of open-source alternatives, like LLaMA [6] and RWKV [7], has provided several methods to finetune foundational Large Language Models, like Alpaca [8] and Vicuna [9]. However, most of these instruction datasets are self-instructed by GPT-3.5 [3] or GPT-4 [10], which can lead to a reduced prediction accuracy [11] (see Figure 1). This problem is particularly significant in natural science topics, as the above models are not explicitly designed for scientific fields such as materials science and chemistry. While there are examples in the medical field where knowledge graphs have been used to generate instruction data [12], this is not common in natural science. Another costly approach involves GPT-3.5 or GPT-4 for multiple rounds of question-answering sessions with cancer-related papers. Despite these challenges, the evolution and improvement of LLM continue to open promising opportunities in various fields.
This study introduces DARWIN, a series of LLM meticulously tailored for the scientific domain, particularly for material science, chemistry, and physics. The models build upon the open-source LLaMA-7B [6] and achieve state-of-the-art results in various scientific tasks. In the fine-tuning process, we introduced the Scientific Instruction Generation (SIG) model, which automates the generation of instructions from scientific texts. This innovative approach streamlines the infusion of scientific knowledge into the model. By harnessing open-sourced scientific FAIR (Finable, Accessible, Interpreable, and Reuseable) datasets [13] and the scientific literature, we generated diverse instruction data, emphasising the factual correctness of the models' responses. The key contributions of this work are:</p>
<ul>
<li>We present a novel LLM series named DARWIN that enhances performance across benchmarks in the scientific domain. This achievement represents a significant stride in our collective efforts to leverage LLMs for scientific inquiry, further endorsing their potential role in accelerating scientific discovery. Github: https://github.com/MasterAI-EAM/Darwin</li>
<li>We introduce a cost-effective instruction generation approach based on high-quality content. By incorporating the academic papers into the instructions, the reliability of the training dataset is significantly enhanced. Furthermore, we employ the binding "To be continued" tokens for lengthy text input, circumventing the input token limitation inherent to LLaMA.</li>
<li>We use open-source base models for training and public scientific benchmarks for evaluation, enforcing reproducibility in the long run. This open-source paradigm augments the accessibility of models and code</li>
</ul>
<p>and lowers the financial barriers for researchers, consequently fostering the development of the entire AI for Science community.</p>
<h1>2 Related Work</h1>
<h3>2.1 Large language models</h3>
<p>The development of LLM has been an exciting journey, fueled by innovation and a constant push to improve capabilities while addressing critical concerns such as robustness, safety, and ethical implications. It began with simpler models such as Word2Vec [14] and GloVe [15], which fundamentally changed how machines understood and processed natural language by enabling them to capture semantic and syntactic relationships between words. The advent of transformer models like BERT [16] marked a key milestone in this journey, showcasing the power of attention mechanisms and the potential for pre-training on vast amounts of data. It set the stage for GPT, a series of models developed by OpenAI that pushed the boundaries of what LLMs could do, culminating in the groundbreaking GPT-3, a model with 175 billion parameters that showcased impressive capabilities in generating human-like text.
In the spirit of open research and a commitment to make LLMs task-adaptable, instruction-tuning [3] is employed. The instructions are generated from human-annotated prompts and feedback [3], or public benchmarks and datasets with manual or automatic augmentation [17]. Compared to the above approaches, self-instruct [11] generates new tasks (instructions along with instances) from scratch and has been quickly adopted for its superior performance and low cost. For instance, Stanford Alpaca [8] is trained on 52 k instructions generated by GPT-3.5, and Vicunna [9] uses around 700k instructions from conversations gathered from ShareGPT.com. There is a direct correlation between the size and diversity of the instruction data and the generalizability of resulting models to unseen tasks [18, 19]. While self-instruct tuning brings qualitative behaviours to LLMs, researchers find an effect of model collapse that the use of model-generated content in training may cause irreversible defects in the resulting models, where tails of the original content distribution disappear [20].</p>
<h3>2.2 Scientific pretrained models</h3>
<p>Before the era of billion-parameter models, there have been several research efforts to use scientific corpora for language modeling in natural science, such as SciBERT [21] and PubMedBERT [22], or in specific domains, such as MatBERT [23] and DNABERT [24]. Characterized by the larger computational clusters, expanded datasets, and increased model parameters, the advent of the GPT-3 [25] has been a game-changer. This model has been adapted to tackle many NLP tasks in the form of question-answering, for example, merely prompting it with chemical queries in natural language.
Some studies train LLMs tailored to specific scientific domains, exemplified by endeavors such as PMC-LLaMA [26] and Med-PaLM [27] in the medical field. Within the broader natural science domain, Galactica [28] trained on a corpus encompassing 48 million examples of scientific articles, web content, textbooks, lecture notes, and encyclopedic references, claims to have multifaceted capability like solving mathematical problems and annotating molecular and protein structures. However, a fundamental problem with Galactica is that it is not able to distinguish truth from falsehood, which is normally called halluciation. In response to this challenge, some models have opted to integrate external APIs [29] or knowledge graphs [12] to improve the reliability of generated content.
Simultaneously, scientific LLMs trained for specific tasks have demonstrated promising achievements. Notably, in the realm of biology, BioGPT [30] pre-trained on an extensive corpus of biomedical literature, has exhibited superior performance across various biomedical natural language processing (NLP) tasks, including relation extraction, document classification, and question-answering. This progress is further complemented by the field of chemistry. Jablonka et al. [5] finetuned GPT-3 to solve tasks involving regression, classification, or inverse design and found finetuned GPT-3 often performs on par with, or even surpasses, conventional machine learning models while frequently requiring less training data. In addition, novel NLP tasks have been introduced to address domain-specific challenges. For instance, structured information inference (SII) [4] was devised to tackle the information extraction and inference at the device level within the realm of materials science, and extracted information serves as instructive data for finetuning GPT-3, facilitating prediction of power conversion efficiency (PCE).</p>
<h2>3 Methods</h2>
<p>In this section, we will introduce the training process of the series of DARWIN model, including datasets and modelling.</p>
<h1>3.1 Datasets</h1>
<p>Various science-related datasets were used in tuning to align LLM to the natural science domain. These generally include science exam questions answering dataset, retrieved questions and corresponding answers from scientific papers, and FAIR datasets.</p>
<h3>3.1.1 Scientific knowledge dataset</h3>
<p>SciQ dataset. Standardized tests drive progress in AI by challenging language understanding and pushing the boundaries of knowledge, modelling, and reasoning [31]. The SciQ dataset [32] is a large crowdsourced science Questions \&amp; Answers (QA) dataset comprising 13,679 science exam questions covering subjects like physics, chemistry, and biology. The questions are presented in multiple-choice format with four answer options, and most of them are accompanied by an additional paragraph containing supporting evidence for the correct answer. Since SciQ used science study textbooks as a base corpus, it provides broad and rather simple science knowledge.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Scientific paper datasets Utilizing the Web of Science, a collection of scholarly literature comprising 6 million papers was assembled, adhering to the following criteria: 1) categorized in the scientific disciplines of material science (see Figure 2), like chemistry, physics, energy, etc.; 2) available in full-text format from publishers such as Elsevier, Springer Nature, American Chemical Society, and Royal Society of Chemistry; and 3) written in the English language. We then proceeded to identify highly cited papers and utilized our web-scraping tool SciCrawler ${ }^{1}$ to download the content from different publishers and convert it into plain text. We finally employed cosine similarity of MatBert embeddings to remove papers with overly similar topics. This self-constructed paper dataset aims to enhance the model with professional science knowledge and facilitate significant progress in various domains.</p>
<h3>3.1.2 FAIR datasets</h3>
<p>FAIR stands for 'Findable, Accessible, Interoperable, and Reusable', which is a set of principles for enhancing the value and accessibility of data [13]. Due to the strong impact of 4 V (volume, variety, velocity, and veracity) of Big Data on materials science, efforts have been made in recent years to collect comprehensive data from research groups worldwide, including unpublished data, and ensure its FAIRness [33]. We collected 16 open-accessed FAIR datasets with multidisciplinary topics in physics, chemistry, and materials science, including but not limited to the physical structure, properties, synthesis and even material design (see Figure 3. The details of these datasets and which tasks they are used to derive instructions can be found in Appendix Table 4.</p>
<h3>3.2 Modeling</h3>
<p>For our model DARWIN, we developed a comprehensive training pipeline (see Figure 9) utilizing different large language models, LLaMA-7B and Vicuna-7B models, to balance performance and cost. The whole training pipeline involves three LLMs: DARWIN-SIG, DARWIN-BASE and DARWIN-MDP.</p>
<h3>3.2.1 DARWIN-SIG</h3>
<p>Since pretraining on domain texts may not mitigate hallucination issues, we propose a cost-effective approach for generating instruction data using open-source model. The main idea is to train a generator for converting full-text</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>scientific papers into question-answer pairs. By integrating these knowledge-based content into the instructions, we enhance the reliability of the training dataset and avoid model collapse induced by the utilization of training data fully generated by LLMs. The process of SIG can be break down as following steps:</p>
<ol>
<li>Prompt design: Through a process of iterative manual examination and seeking advice from domain experts, we were able to design an effective prompt for LLM to generate question-answer pairs with detailed information extracted from scientific papers.
<img alt="img-2.jpeg" src="img-2.jpeg" /></li>
</ol>
<p>Figure 4: DARWIN Series Model Structure and Training Pipeline
2. Seed QA generation: We select seed papers from the scientific paper dataset in Section 3.1.1 and the rest are training papers. The seed papers were utilized as input to produce seed question-answer pairs by prompting GPT-4 (using prompt designed in step 1).
3. DARWIN-SIG: Our SIG model was developed by fine-tuning the LLM using seed papers and their corresponding question-answer pairs. Given a prompt $p$ and input text $x$, the goal of SIG is to generate a set of instructions $I$, each of which is a pair of question and corresponding answer $\left(q_{i}, a_{i}\right)$ related to $x: \operatorname{SIG}(p, x)=I$. The initial prompt was converted into an instruction, while the seed papers served as input to the model.
4. Training QA generation: Utilizing the trained DARWIN-SIG, the large amount of training papers were subsequently used as input to automatically generate question-answer pairs with low cost. These QA pairs can be directly converted into instruction data.</p>
<p>During the SIG process, there are some details that merit discussion. The first is the choice of base model. Considering the potential inference cost associated with GPT-3.5/4, we turn to use open-source model for training generator. And after human evaluation of performance, we chose Vicuna-7B model as the base model instead of LLaMA or Alpaca. The second is the length limit. The full-text papers are usually longer than limit of 2,048 tokens and for those lengthy ones, we segmented texts and continuously input them by using the "[TBC]" token as a connector (see Figure 5). Thus, the model could output question-answer pairs aligned with the complete content of the papers.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Darwin-SIG structure and comparison
We compare with self-questioning method [34] (see Appendix B). Self-questioning also generates factual questionanswer pairs based on the given abstract by prompting GPT-3.5.it can be observed that question-answer pairs generated</p>
<p>by DARWIN-SIG are more informative and specific to the content of the input paper. Rather than being limited to simple definitions, the questions are complex enough to cover many aspects, including but not limited to the properties relevant to the application, the influence of strategies, structural differences, challenges, and experimental insights. Correspondingly, the answers generated are not overly generalized. Instead, they provided specific details, such as mentioning precise chemical elements and numerical values referenced in the original scientific papers. The high-quality Q\&amp;A generation ensured the preservation of essential knowledge within lengthy scientific papers.</p>
<h1>3.2.2 DARWIN-BASE</h1>
<p>We generated knowledge-based instruction data for DARWIN based on SciQ dataset in section 3.1.1 and generated question-answer pairs in 3.2.1. The answers in the SciQ dataset include correct options and provide support, which can be used as background knowledge input or as an explanation of the reasoning process. Based on this scenario, we generated a mixed set of instruction data consisting of the following patterns, arranged in increasing levels of difficulty: 1) Single-turn question-answering, where the model selects an answer based on the question and the provided background knowledge; 2) Single-turn question-answering, where the model selects an answer based on the question and also provides an explanation for the chosen answer; 3) Multi-turn dialogue, where the model selects an answer based on the question and provides an explanation for the chosen answer in response to user follow-up questions. Examples of the above three types are available in the Appendix (see Appendix C). Without any input, each question became an instruction, and its corresponding answer became an output. We fine-tuned the LLaMA-7B model using the two kinds of knowledge-based instruction data mentioned above, totalling over 20,000 instances. This process resulted in the development of DARWIN-BASE, a model equipped to engage in scientific question-answering.</p>
<h3>3.2.3 DARWIN-MDP</h3>
<p>In addition to DARWIN which can conduct the scientific Q\&amp;A task, we also involve 16 FAIR datasets (4) to generate other instructions to further fine-tune DARWIN-BASE as DARWIN-MDP which can conduct the Material \&amp; Device Predictions tasks, such as classification, regression, and design etc.</p>
<h2>- Classification</h2>
<p>"instruction": "Tell me if given composition has glass-forming ability." "input":"Cr20Ni61P19"
"output": "Yes, Cr20Ni61P19 has glass-forming ability."</p>
<p>The classification task of scientific language models involves the categorization or labelling of scientific text data into predefined classes or categories. For example, DARWIN-MDP can be trained to classify chemical compounds based on their properties, such as solubility, toxicity, or stability. This can assist in drug discovery, material science, or chemical engineering applications. An instance of the instruction of classification is shown above.</p>
<h2>- Regression</h2>
<p>"instruction": "Write aqueous solubility expressed as a logarithm in mol/L of given SMILES in $25{ }^{\circ} \mathrm{C}$."
"input":"CN(C)C(=O)Nc1ccc (Cl)c(Cl)c1"
"output": " $-3.8 "$
A more demanding task than classification involves developing a regression model capable of predicting continuous property values, like open-circuit voltage (Voc), short-circuit current density (Jsc), fill factor (FF), and PCE etc. Although LLMs are unable to predict real numbers in regression tasks with high precision, they can still produce predictions of acceptable accuracy by employing rounded values during training. For device performance data, a precision of two decimal points is deemed sufficient. An example of the instruction of regression is shown above.</p>
<h2>- Inverse Design</h2>
<p>"instruction": "Design a donor with acceptor PC61BM, PCE=0 for a organic solar cell with specific requirement"</p>
<div class="codehilite"><pre><span></span><code>&quot;input&quot;:&quot;under AM1.5 and 100 light intensity&quot;
&quot;output&quot;: &quot;COC(=O)c1sc2csc(c3sc4c(c3F) c(OC)c3scc(F)c3c4OC)c2c1F&quot;
</code></pre></div>

<p>In the context of material science, inverse design refers to a computational approach that aims to design new materials or optimize existing ones by working backward from desired properties or functionalities [5]. Instead of starting with a specific material and analyzing its properties, inverse design begins with a desired set of properties or performance targets and seeks to identify the material structure or composition that would exhibit those properties. By utilizing inverse design techniques, researchers can potentially discover novel materials with tailored properties that are challenging or time-consuming through traditional experimental approaches. The inverse design has shown promise in various areas of material science, including photovoltaics, catalysts, batteries, thermoelectric materials, and more. An example of the instruction of inverse design is shown above.</p>
<h1>4 Experiments</h1>
<h3>4.1 Baselines</h3>
<p>To demonstrate the performance of DARWIN on different tasks, we compared the performance of DARWIN with 2 baseline models.</p>
<ul>
<li>LLaMA is an LLM released by Meta AI with a variety of model sizes from 7 billion to 65 billion parameters. LLaMA-7B version is used as our base model out of its ease of training.</li>
<li>GPT-3 is the third-generation language model with 175 billion parameters created by OpenAI.</li>
</ul>
<p>In addition to the aforementioned two baselines, we included results from several dedicated machine learning models as references, like MODNet (v0.1.1), MolCLR and AMMExpress v2020. Those results were from matbench [35]. It should be noted that each model was individually trained on a specific FAIR dataset.</p>
<h3>4.2 Fine-tuning procedure</h3>
<p>We conducted fine-tuning of the LLaMA-7B model with instruction tuning on an 8-A100 GPU. The hyperparameters we used were similar to the ones used with Alpaca: the fine-tuning procedure included training for 3 epochs with a batch size of 4 for both training and evaluation. Gradients were accumulated over 8 steps, the learning rate was set to 2e-5 with no weight decay, and a warm-up ratio of 0.03 . We utilized the Brain Floating Point 16 (BF16) data format to strike a balance between precision and computational cost. We fine-tuned LLaMA-7B on subsets or combinations of instruction data and named them differently as follows:</p>
<ul>
<li>gptchem was fine-tuned on several FAIR datasets separately, using GPT-3 as base model [36].</li>
<li>LLaMA-single was fine-tuned on instruction data from each dataset of 12 FAIR datasets, which means there are 12 LLaMA-single.</li>
<li>LLaMA-all was fine-tuned on mixed instruction data of 9 FAIR datasets.</li>
<li>DARWIN was fine-tuned on instruction data from SciQ dataset and question-answer pairs generated by SIG.</li>
<li>DARWIN-MDP was further fine-tuned on DARWIN on mixed instruction data of 16 FAIR datasets.</li>
</ul>
<p>Our final model DARWIN-MDP is a language model fine-tuned using supervised learning from a LLaMA-7B model on over 60,000 instruction-following demonstrations derived from scientific literature and datasets.</p>
<h3>4.3 Metrics</h3>
<p>We use accuracy of multiple choice of SciQ as metric to evaluate performance in fine-tuning on scientific knowledge. If the model can choose the correct option and generate the correct corresponding answer, then it is counted as correct:</p>
<p>$$
\text { Accuracy }=\frac{\text { # of correctly answered questions }}{\text { total # of questions }}
$$</p>
<p>We also evaluated the performance of the fine-tuned DARWIN model on each MDP task separately. We only evaluate classification and regression task, but leave design task for future work due to its difficulty of evaluation.</p>
<p>For the classification task, we evaluted the performance of the model on each dataset using F1 score. When dealing with boolean-type labels (as exemplified in Section 3.2.3), we compare the predictions, expressed as 'Yes' or 'No', with the ground truth values ('True' or 'False'), and evaluate the F1-binary metric specifically for the class indicated by pos_label. For multi-category labels, we use macro-F1 score (unweighted mean of the F1 scores calculated per class).</p>
<p>For the regression task, we use Mean Absolute Error (MAE) to measure the difference between the predicted value and the true value:</p>
<p>$$
\mathrm{MAE}=\frac{1}{n} \sum_{i=1}^{n}\left|y_{i}-\hat{y}_{i}\right|
$$</p>
<h1>5 Results</h1>
<p>In this section, we present the performance of the DARWIN-BASE model and the DARWIN-MDP model, comparing them with the existing metrics from a previous study.</p>
<h3>5.1 DARWIN-BASE Model Performance</h3>
<p>Table 1 presents a comparative analysis of zero-shot and training outcomes concerning the SciQ dataset. SciQ serves as a widely recognized benchmark for scientific question-answering tasks, and numerous large language models (LLMs) employ it for assessing their zero-shot performance. Notably, the metrics reported in previous studies predominantly reflect zero-shot results, which tend to exhibit relatively lower accuracy.
For our evaluation, we conducted a random selection of 1000 questions from the SciQ dataset to serve as a test set. Remarkably, the performance of the DARWIN model on the SciQ dataset demonstrates an impressive accuracy rate of $96.9 \%$, despite having a model size of only 7 billion parameters. It is crucial to emphasize that these results were obtained from previously unseen data, despite DARWIN's fine-tuning on a subset of the SciQ dataset. This underscores the exceptional performance of DARWIN, particularly when juxtaposed with models that lack domain-specific fine-tuning.</p>
<p>Table 1: Comparative evaluation of DARWIN and SOTA models on SciQ dataset</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Number of Examples</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">zero-shot</td>
<td style="text-align: center;">BLOOM (7 B)</td>
<td style="text-align: center;">89.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">OPT (6.7 B)</td>
<td style="text-align: center;">90.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pythia (Deduplicated) (6.9 B)</td>
<td style="text-align: center;">91.1</td>
</tr>
<tr>
<td style="text-align: center;">five-shot</td>
<td style="text-align: center;">Pythia (Deduplicated) (6.9 B)</td>
<td style="text-align: center;">95.2</td>
</tr>
<tr>
<td style="text-align: center;">training</td>
<td style="text-align: center;">DARWIN-BASE (7 B)</td>
<td style="text-align: center;">96.9</td>
</tr>
</tbody>
</table>
<h3>5.2 DARWIN-MDP Model Performance</h3>
<p>Table 2 and Table 3 present the performance metrics of the DARWIN-MDP model across various regression and classification tasks. Notably, DARWIN-MDP consistently achieves the lowest Mean Absolute Error (MAE) in most regression tasks and the highest F1 score in most classification tasks. Comparing LLaMA-single and LLaMA-all, we observed that fine-tuning on multiple tasks together (LLaMA-all) yields superior performance compared to fine-tuning on individual tasks separately (LLaMA-single). This highlights the effectiveness of multi-task learning in enhancing the model's overall capabilities. Furthermore, when comparing LLaMA-all with DARWIN-MDP, we find that the initial stage of fine-tuning scientific knowledge significantly contributes to further improvements in performance. This underscores the importance of domain-specific pre-training, as DARWIN-MDP, with its initial training on scientific knowledge, outperforms the LLaMA-all model. Finally, the performance of DARWIN-MDP is comparable with the dedicated machine learning model for specific tasks, which provides a novel material design method, one model could finish multiple scientific tasks with simple prompting.
Overall, the results demonstrate that both the DARWIN series exhibit exceptional performance, surpassing the existing metrics reported in the previous study. These findings validate the effectiveness of domain-specific fine-tuning and multi-task learning, reinforcing the potential of these models for a wide range of natural language processing and scientific tasks.</p>
<p>Table 2: Comparative evaluation of DARWIN and SOTA models for Regression Task (in MAE).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">OPV</th>
<th style="text-align: center;">steels</th>
<th style="text-align: center;">ChEML</th>
<th style="text-align: center;">expt_gap</th>
<th style="text-align: center;">ESOL</th>
<th style="text-align: center;">Naga</th>
<th style="text-align: center;">Diversity</th>
<th style="text-align: center;">Cp</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MODNet (v0.1.12)</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$\mathbf{8 8}$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MolCLR</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0.6706</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">0.9345</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">gptchem</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">0.9284</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">1.1544</td>
<td style="text-align: center;">$\mathbf{1 . 9 7}$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-single</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">229.88</td>
<td style="text-align: center;">0.907</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">0.775</td>
<td style="text-align: center;">2.52</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">4.89</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-all</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">160.81</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">$\mathbf{0 . 3 1}$</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
</tr>
<tr>
<td style="text-align: center;">DARWIN-MDP</td>
<td style="text-align: center;">$\mathbf{0 . 5 4}$</td>
<td style="text-align: center;">125.97</td>
<td style="text-align: center;">$\mathbf{0 . 7 9 8}$</td>
<td style="text-align: center;">0.317</td>
<td style="text-align: center;">$\mathbf{0 . 7 1}$</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">$\mathbf{0 . 3 5}$</td>
<td style="text-align: center;">$\mathbf{4 . 7 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparative Evaluation of DARWIN and SOTA models for Classification Task (in F1 Score).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Pei</th>
<th style="text-align: center;">waterStability</th>
<th style="text-align: center;">matbench_is_metal</th>
<th style="text-align: center;">matbench_glass</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MODNet (v0.1.12)</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">AMMExpress v2020</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">$/$</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">90.4</td>
</tr>
<tr>
<td style="text-align: center;">gptchem</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">88.0</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-single</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">86.7</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-all</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">87.7</td>
</tr>
<tr>
<td style="text-align: center;">DARWIN-MDP</td>
<td style="text-align: center;">$\mathbf{9 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 3}$</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">88.0</td>
</tr>
</tbody>
</table>
<h1>6 Conclusion</h1>
<p>The emergence of foundational AI models and expansive language models offers the remarkable potential to redefine the advancement of scientific AI, making it more accessible, accurate, and cross-domain. However, the scientific domain, with its inherent complexities, presents unique challenges when it comes to implementing large language models. Our research offers a glimpse into the possibilities and hurdles of employing these technologies in scientific applications. In this study, we incorporated existing knowledge bases effectively into LLM and created the DARWIN series which achieved SOTA results on multiple scientific tasks. We also introduced a SIG model that can automatically convert knowledge from a large corpus of papers. Furthermore, we explored different multi-task training strategies and found that training on knowledge-based questions and answers first can improve prediction performance on FAIR datasets, and combining multiple tasks proved to be more effective than individual training, highlighting the underlying connections between seemingly independent tasks.</p>
<h2>7 Limitations</h2>
<p>Data: A key limitation of this study stems from the limited quantity of question-answer pairs and dependence on the somewhat confined FAIR datasets, potentially inhibiting the model's applicability and effectiveness. To address this, future research could focus on amplifying the volume of tasks, generating more question-answer pairs via SIG, thereby enhancing dataset representativeness and improving model performance across various domains.
Model: In striving for desired performance, this study utilizes multiple models instead of a unified one, introducing complexity and potential inefficiencies. To overcome this, future research should focus on scaling up the model using efficient tuning methods for comparable or superior performance with limited resources. Also, the impact of fine-tuning on large-scale scientific text as a means of injecting knowledge into the model remains untested. Future work will address this through comparative analysis, providing valuable insights into the model's ability to leverage additional knowledge sources and improve performance in scientific fields.
Evaluation: DARWIN-BASE is employed as the foundational model, with its performance evaluated in terms of multiple-choice question answering. A comprehensive evaluation mechanism for the question-answering task is warranted to appraise the efficacy of knowledge injection from diverse perspectives. We intend to formulate a modelgraded evaluation mechanism that segregates answers into various categories contingent on their quality and compliance with specific criteria, such as misunderstanding, relevance, or correctness. This methodology would afford a more exhaustive assessment of the model's performance, obviating the need for extensive manual intervention.</p>
<h2>References</h2>
<p>[1] L Weston, V Tshitoyan, J Dagdelen, O Kononova, A Trewartha, K A Persson, G Ceder, and A Jain. Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature. Journal of Chemical Information and Modeling, 59(9):3692-3702, sep 2019.</p>
<p>[2] Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. Patterns, 3(4):100488, 2022.
[3] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, and Alex Ray. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.
[4] Tong Xie, Yuwei Wan, Wei Huang, Yufei Zhou, Yixuan Liu, Qingyuan Linghu, Shaozhou Wang, Chunyu Kit, Clara Grazian, Wenjie Zhang, and Bram Hoex. Large language models as master key: Unlocking the secrets of materials science with gpt, 2023.
[5] Kevin Maik Jablonka, Philippe Schwaller, and Andres Ortega-guerrero. Is GPT-3 all you need for low-data discovery in chemistry ? ChemRxiv, pages 1-32, 2023.
[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, and Faisal Azhar. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[7] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, and Kranthi Kiran GV. RWKV: Reinventing RNNs for the Transformer Era. arXiv preprint arXiv:2305.13048, 2023.
[8] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanford_alpaca, 2023.
[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023.
[10] OpenAI. Gpt-4 technical report, 2023.
[11] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Model with Self Generated Instructions. arXiv preprint arXiv:2212.10560, 2022.
[12] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge. arXiv preprint arXiv:2304.06975, 2023.
[13] Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E Bourne, Jildau Bouwman, Anthony J Brookes, Tim Clark, Mercè Crosas, Ingrid Dillo, Olivier Dumon, Scott Edmunds, Chris T Evelo, Richard Finkers, Alejandra Gonzalez-Beltran, Alasdair J G Gray, Paul Groth, Carole Goble, Jeffrey S Grethe, Jaap Heringa, Peter A C 't Hoen, Rob Hooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J Lusher, Maryann E Martone, Albert Mons, Abel L Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene van Schaik, Susanna-Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn, Morris A Swertz, Mark Thompson, Johan van der Lei, Erik van Mulligen, Jan Velterop, Andra Waagmeester, Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3(1):160018, 2016.
[14] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26, 2013.
[15] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages $1532-1543,2014$.
[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
[17] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. arXiv e-prints, pages arXiv-2204, 2022.
[18] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022.</p>
<p>[19] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.
[20] Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. The curse of recursion: Training on generated data makes models forget. arXiv preprint arxiv:2305.17493, 2023.
[21] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676, 2019.
[22] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23, 2021.
[23] Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. Patterns, 3(4), 2022.
[24] Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, 37(15):2112-2120, 2021.
[25] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and Amanda Askell. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[26] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023.
[27] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022.
[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[29] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.
[30] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), 2022.
[31] Peter Clark. Elementary school science and math tests as a driver for ai: Take the aristo challenge! In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, pages 4019-4021, 2015.
[32] Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017.
[33] Matthias Scheffler, Martin Aeschlimann, Martin Albrecht, Tristan Bereau, Hans-Joachim Bungartz, Claudia Felser, Mark Greiner, Axel Groß, Christoph T Koch, Kurt Kremer, et al. Fair data enabling new horizons for materials research. Nature, 604(7907):635-642, 2022.
[34] Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018, 2023.
[35] Alexander Dunn, Qi Wang, Alex Ganose, Daniel Dopp, and Anubhav Jain. Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm. npj Computational Materials, $6(1): 138,2020$.
[36] Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, and Berend Smit. Is gpt-3 all you need for low-data discovery in chemistry? 2023.
[37] Anna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, and Bissan Al-Lazikani. ChEMBL: a large-scale bioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100-D1107, 2012.
[38] Xixi Qin, Xiaochen Du, Sampreeti Bhattacharya, Connor Clayton, Jun Hu, Manoj Jana, Svenja Janke, Rebecca Lau, Raul Laasner, and Andrew Levin. HybriD 3: Materials Property Database for Hybrid Organic-Inorganic Perovskites. In APS March Meeting Abstracts, volume 2021, pages X54-007, 2021.
[39] John S Delaney. ESOL: estimating aqueous solubility directly from molecular structure. Journal of chemical information and computer sciences, 44(3):1000-1005, 2004.</p>
<p>[40] Osbel Almora, Derya Baran, Guillermo C Bazan, Carlos I Cabrera, Sule Erten-Ela, Karen Forberich, Fei Guo, Jens Hauch, Anita W Y Ho-Baillie, and T Jesper Jacobsson. Device performance of emerging photovoltaic materials (Version 3). Advanced Energy Materials, 13(1):2203313, 2023.
[41] Masahiro Sakurai, Renhai Wang, Timothy Liao, Chao Zhang, Huaijun Sun, Yang Sun, Haidi Wang, Xin Zhao, Songyou Wang, and Balamurugan Balasubramanian. Discovering rare-earth-free magnetic materials through the development of a database. Physical review materials, 4(11):114408, 2020.
[42] Ya Zhuo, Aria Mansouri Tehrani, and Jakoah Brgoch. Predicting the band gaps of inorganic solids by machine learning. The journal of physical chemistry letters, 9(7):1668-1673, 2018.
[43] Yoshiyuki Kawazoe, J-Z Yu, A-P Tsai, and T Masumoto. Nonequilibrium phase diagrams of ternary amorphous alloys, 1997.
[44] Seyed Mohamad Moosavi, Aditya Nandy, Kevin Maik Jablonka, Daniele Ongari, Jon Paul Janet, Peter G Boyd, Yongjin Lee, Berend Smit, and Heather J Kulik. Understanding the diversity of the metal-organic framework ecosystem. Nature communications, 11(1):1-10, 2020.
[45] Shinji Nagasawa, Eman Al-Naamani, and Akinori Saeki. Computer-aided screening of conjugated polymers for organic solar cell: classification by random forest. The Journal of Physical Chemistry Letters, 9(10):2639-2646, 2018.
[46] Zongrui Pei, Junqi Yin, Jeffrey A Hawk, David E Alman, and Michael C Gao. Machine-learning informed prediction of high-entropy solid solution formation: Beyond the Hume-Rothery rules. npj Computational Materials, 6(1):50, 2020.
[47] Daniel Hickox-Young, Danilo Puggioni, and James M Rondinelli. Polar metals taxonomy for materials classification and discovery. Physical Review Materials, 7(1):10301, 2023.
[48] Rohit Batra, Carmen Chen, Tania G Evans, Krista S Walton, and Rampi Ramprasad. Prediction of water stability of metal-organic frameworks using machine learning. Nature Machine Intelligence, 2(11):704-710, 2020.</p>
<h1>A Categories of collected papers</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Web of Science Categories</th>
<th style="text-align: center;">Record Count</th>
<th style="text-align: center;">Ratio</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Materials Science Multidisciplinary</td>
<td style="text-align: center;">969264</td>
<td style="text-align: center;">16.002</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Physical</td>
<td style="text-align: center;">919588</td>
<td style="text-align: center;">15.182</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Multidisciplinary</td>
<td style="text-align: center;">788218</td>
<td style="text-align: center;">13.013</td>
</tr>
<tr>
<td style="text-align: center;">Energy Fuels</td>
<td style="text-align: center;">403050</td>
<td style="text-align: center;">6.654</td>
</tr>
<tr>
<td style="text-align: center;">Physics Applied</td>
<td style="text-align: center;">366015</td>
<td style="text-align: center;">6.043</td>
</tr>
<tr>
<td style="text-align: center;">Engineering Chemical</td>
<td style="text-align: center;">355343</td>
<td style="text-align: center;">5.866</td>
</tr>
<tr>
<td style="text-align: center;">Nanoscience Nanotechnology</td>
<td style="text-align: center;">298217</td>
<td style="text-align: center;">4.923</td>
</tr>
<tr>
<td style="text-align: center;">Physics Condensed Matter</td>
<td style="text-align: center;">277983</td>
<td style="text-align: center;">4.589</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Organic</td>
<td style="text-align: center;">274913</td>
<td style="text-align: center;">4.539</td>
</tr>
<tr>
<td style="text-align: center;">Environmental Sciences</td>
<td style="text-align: center;">262377</td>
<td style="text-align: center;">4.332</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Analytical</td>
<td style="text-align: center;">246136</td>
<td style="text-align: center;">4.063</td>
</tr>
<tr>
<td style="text-align: center;">Biochemistry Molecular Biology</td>
<td style="text-align: center;">242381</td>
<td style="text-align: center;">4.001</td>
</tr>
<tr>
<td style="text-align: center;">Physics Atomic Molecular Chemical</td>
<td style="text-align: center;">201022</td>
<td style="text-align: center;">3.319</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Inorganic Nuclear</td>
<td style="text-align: center;">199876</td>
<td style="text-align: center;">3.300</td>
</tr>
<tr>
<td style="text-align: center;">Polymer Science</td>
<td style="text-align: center;">173356</td>
<td style="text-align: center;">2.862</td>
</tr>
<tr>
<td style="text-align: center;">Metallurgy Metallurgical Engineering</td>
<td style="text-align: center;">170305</td>
<td style="text-align: center;">2.812</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Applied</td>
<td style="text-align: center;">162747</td>
<td style="text-align: center;">2.687</td>
</tr>
<tr>
<td style="text-align: center;">Electrochemistry</td>
<td style="text-align: center;">153127</td>
<td style="text-align: center;">2.528</td>
</tr>
<tr>
<td style="text-align: center;">Engineering Environmental</td>
<td style="text-align: center;">148581</td>
<td style="text-align: center;">2.453</td>
</tr>
<tr>
<td style="text-align: center;">Physics Particles Fields</td>
<td style="text-align: center;">137267</td>
<td style="text-align: center;">2.266</td>
</tr>
<tr>
<td style="text-align: center;">Thermodynamics</td>
<td style="text-align: center;">135296</td>
<td style="text-align: center;">2.234</td>
</tr>
<tr>
<td style="text-align: center;">Mechanics</td>
<td style="text-align: center;">133424</td>
<td style="text-align: center;">2.203</td>
</tr>
<tr>
<td style="text-align: center;">Nuclear Science Technology</td>
<td style="text-align: center;">123304</td>
<td style="text-align: center;">2.036</td>
</tr>
<tr>
<td style="text-align: center;">Chemistry Medicinal</td>
<td style="text-align: center;">123152</td>
<td style="text-align: center;">2.033</td>
</tr>
<tr>
<td style="text-align: center;">Food Science Technology</td>
<td style="text-align: center;">118760</td>
<td style="text-align: center;">1.961</td>
</tr>
</tbody>
</table>
<p>Showing 25 out of 245 entries.</p>
<h2>B Comparison of Self-questioning and SIG</h2>
<p>Self-questioning:</p>
<h2>Prompt:</h2>
<p>Given a paragraph of paper, can you ask five questions and give answers that can be directed to the paragraph? List in the format of Question-Answer pair.
Paragraph: One- and two-dimensional carbon nanomaterials are attracting considerable attention because of their extraordinary electrical, mechanical and thermal properties, which could lead to a range of important potential applications. Synthetic processes associated with making these materials can be quite complex and also consume large amounts of energy, so a major challenge is to develop simple and efficient methods to produce them. Here, we present a self-templated, catalyst-free strategy for the synthesis of one-dimensional carbon nanorods by morphology-preserved thermal transformation of rod-shaped metal-organic frameworks. The as-synthesized non-hollow (solid) carbon nanorods can be transformed into two- to six-layered graphene nanoribbons through sonochemical treatment followed by chemical activation. The performance of these metal-organic framework-derived carbon nanorods and graphene nanoribbons in supercapacitor electrodes demonstrates that this synthetic approach can produce functionally useful materials. Moreover, this approach is readily scalable and could be used to produce carbon nanorods and graphene nanoribbons on industrial levels.</p>
<h2>GPT-3.5 completion:</h2>
<p>Q1: What is being discussed in this paragraph?
A1: This paragraph is discussing the synthesis of one-dimensional carbon nanorods through a self-templated, catalyst-free strategy and the potential application of these nanorods and graphene nanoribbons in supercapacitor electrodes.</p>
<p>Q2: What transforms the nanorods into graphene nanoribbons?
A2: Sonochemical treatment followed by chemical activation transforms the nanorods into graphene nanoribbons.
Q3: What properties of nanomaterials makes them attractive?</p>
<p>A3: The extraordinary electrical, mechanical and thermal properties of one- and two-dimensional carbon nanomaterials makes them attractive.</p>
<p>Q4: What is the major challenge associated with the synthesis of these materials?
A4: The major challenge in the synthesis of these materials is to develop simple and efficient methods that consume low amounts of energy.</p>
<p>Q5: What advantages does this particular synthetic approach offer?
A5: This particular synthetic approach offers advantages such as scalability and the production of functionally useful materials on industrial levels.</p>
<h1>SIG:</h1>
<h2>Prompt:</h2>
<p>Please generate 10 scientific Q\&amp;A (prompts with outputs) related with "six-layered graphene nanoribbons", "one-dimensional carbon nanorods", "fig", "mof-74", "mof-74-rod", "synthesis", "carbon nanotubes", "excellent capacitive behaviour", "formation", "materials", "mpc", "acid", "chemical activation", "mofs", "useful materials". Don't ask very simple questions, like definition questions (e.g. What is XXX). You should generate more complex problems. Answer using the data from provided information. Add detail to answers as much as possible, such as answer the specific chemical elements and numbers. (input is the full text of paper)</p>
<h2>SIG Completion:</h2>
<p>Q1: What are some potential applications of six-layered graphene nanoribbons synthesized by chemical activation of a defective graphene layer?
A1: Six-layered graphene nanoribbons have a high surface area, which makes them useful for gas storage and catalyst support. They can also be used in electrochemical sensing and separation, and as a special template.</p>
<p>Q2: How does the chemical activation process work in the synthesis of six-layered graphene nanoribbons?
A2: The chemical activation process involves treating a graphene layer with a strong acid, such as sulfuric acid or nitric acid, in a vacuum atmosphere. The acid attacks the weak interlayer dangling bonds, causing them to break and forming strong covalent bonds between the layers.</p>
<p>Q3: What is the significance of the fig graphite intercalation behavior in six-layered graphene nanoribbons?
A3: The fig graphite intercalation behavior in six-layered graphene nanoribbons indicates that there are still many voids left between the layers after intercalation with KCl . This is because the intercalation process does not fill all of the defects in the graphene layer.</p>
<p>Q4: What are the key features of the MOF-74 compound and its role in six-layered graphene nanoribbons?
A4: MOF-74 is a metal-organic framework that exhibits highly connected interior cavities. It is used as a template to create the six-layered graphene nanoribbons, in which the MOF-74 molecules act as glue to hold the graphene layers together.</p>
<p>Q5: What are the key steps in the synthesis of one-dimensional carbon nanorods?
A5: The synthesis of one-dimensional carbon nanorods involves depositing carbon onto a substrate in the form of rods, heating the substrate, and then removing the substrate. This process can be carried out using a variety of substrates, such as silicon, ceramic, metal, or glass.</p>
<p>Q6: What chemical activation methods can be used for the synthesis of six-layered graphene nanoribbons?
A6: Two common chemical activation methods for the synthesis of six-layered graphene nanoribbons are treatment with sulfuric acid (H2SO4) and nitric acid (HNO3).</p>
<p>Q7: What are the main defects present in the graphene layer after chemical activation for the synthesis of six-layered nanoribbons?
A7: The main defects present in the graphene layer after chemical activation for the synthesis of six-layered nanoribbons are disconnected regions due to the separation of carbon layers from the graphene sheet, as well as chains of defects running along the graphene sheets.</p>
<p>Q8: What are the key features of the MOF-74 compound used in the six-layered graphene nanoribbon synthesis method?
A8: MOF-74 is a metal-organic framework consisting of 74 atoms, 48 of which are carbon. It has large pore sizes, with</p>
<p>a average diameter of around $10 \AA$, and high surface area, with a BET surface area of around $700 \mathrm{~m} 2 / \mathrm{g}$.
Q9: How do the electrochemical properties of MOF-74-based materials compare to those of graphene nanoribbons? A9: MOF-74-based materials have a larger specific surface area, higher oxygen storage capacity, and better electrochemical stability than graphene nanoribbons. They also exhibit superior thermal stability, with melting temperatures higher than 1,000 degrees.</p>
<p>Q10: What are the main factors that contribute to the excellent capacitive behavior of MOF-74-based materials?
A10: The main factors that contribute to the excellent capacitive behavior of MOF-74-based materials are the large specific surface area, high oxygen storage capacity, and the presence of defects in the graphene layer, which lead to a large number of trapped charge carriers.</p>
<h1>C Three patterns of SciQ instruction data</h1>
<h2>1) Open-book single-turn question-answering</h2>
<p>instruction: Read the following paragraph and choose an answer for a multiple choice question about the paragraph input: Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F 2 , are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.\n Question: Compounds that are capable of accepting electrons, such as o 2 or f2, are called what? (A) residues (B) antioxidants (C) Oxygen (D) oxidants
output: (D) oxidants.</p>
<h2>2) Closed-book single-turn question-answering</h2>
<p>instruction: Choose an answer for this multiple choice question and explain: Compounds that are capable of accepting electrons, such as o2 or f2, are called what? (A) residues (B) antioxidants (C) Oxygen (D) oxidants
input: Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F 2 , are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books. output: (D) oxidants.</p>
<h2>3) Multi-turn dialogue</h2>
<p>instruction: <user>: Choose an answer for this multiple choice question: Compounds that are capable of accepting electrons, such as o2 or f2, are called what? (A) residues (B) antioxidants (C) Oxygen (D) oxidants <bot>: (D) oxidants. <user>: Can you explain why?
input:
output: Oxidants and Reductants Compounds that are capable of accepting electrons, such as O 2 or F 2 , are calledoxidants (or oxidizing agents) because they can oxidize other compounds. In the process of accepting electrons, an oxidant is reduced. Compounds that are capable of donating electrons, such as sodium metal or cyclohexane (C6H12), are calledreductants (or reducing agents) because they can cause the reduction of another compound. In the process of donating electrons, a reductant is oxidized. These relationships are summarized in Equation 3.30: Equation 3.30 Saylor URL: http://www. saylor. org/books.</p>
<h2>D MDP regression task</h2>
<h2>OPV:</h2>
<p>instruction: what is the power conversion efficiency of organic solar cells with the donor: $\mathrm{COC}(=\mathrm{O}) \mathrm{c} 1 \mathrm{sc} 2 \mathrm{csc}(-$ c3sc4c(c3F)c(OC)c3scc(F)c3c4OC)c2c1F, and Acceptors: PC61BM?
Output: 0.96478299093449994</p>
<h2>Matbench_steels:</h2>
<p>instruction: Write a possible yield strength of given composition at $800-1200 \hat{0} 0 \mathrm{b0C}$.
input: Fe0.721C0.000476Mn0.000104Si0.000203Cr0.0560Ni0.109Mo0.0304V0.
0105Nb0.0000615Co0.0611Al0.00191Ti0.00931
output: 1040.4
ChEML:</p>
<p>instruction: Write lipophilicity of given SMILES.
input: $\mathrm{CC}(\mathrm{C}) \mathrm{C}(\mathrm{NC}(=\mathrm{O}) \mathrm{CN} 1 \mathrm{C}(=\mathrm{O}) \mathrm{C}(=\mathrm{CN}=\mathrm{C} 1 \mathrm{C} 2 \mathrm{CCCCC} 2) \mathrm{NC}(=\mathrm{O}) \mathrm{OCc} 3 \mathrm{ccccc} 3) \mathrm{C}(=\mathrm{O}) \mathrm{C}(\mathrm{F})(\mathrm{F}) \mathrm{F}$
output: 2.87
Matbench_expt_gap: instruction: What is band gap of given composition?
input: Sr5Cd2Sb5F
output: 0.0
ESOL:
instruction: What is water solubility of given SMILES in room temperature?
input: $\mathrm{CCOP}(=\mathrm{S})(\mathrm{OCC}) \mathrm{SCSCC}$
output: 7.762471166286911e-05
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: ESOL task instruction generation structure, A solid line delineates a one-to-one correspondence</p>
<h1>MoosaviCp:</h1>
<p>instruction: What is weight-averaged molecular weight of given SMILES at normal measurement conditions.?
input: $\mathrm{CC} 1=\mathrm{CC}(\mathrm{N}(\mathrm{CC}(\mathrm{CCCCCC}) \mathrm{CCCC}) \mathrm{C}(\mathrm{C} 2=\mathrm{C} 3 \mathrm{SC}(\mathrm{C} 4=\mathrm{CC} 5=\mathrm{C}(\mathrm{C}(\mathrm{SC}(\mathrm{C})=\mathrm{C} 6)=\mathrm{C} 6[\mathrm{Si}] 5$
$(\mathrm{CCCCCCCC}) \mathrm{CCCCCCCC}) \mathrm{S} 4) \mathrm{C} 2) \mathrm{O})=\mathrm{C} 3 \mathrm{~S} 1$
output: 30.5
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: OPV task instruction generation structure, A solid line delineates a one-to-one correspondence, while a dashed line symbolizes a one-to-many association.</p>
<h2>MoosaviDiversity:</h2>
<p>instruction: What is CellV for given MOF?
input: AVIVAC_clean
output: 18049.1</p>
<h2>MoosaviCp:</h2>
<p>instruction: Write the predicted gravimetric heat capacity at 350 K of given MOF with MOF structural features and topology.
input: linker $\mathrm{OC}(=\mathrm{O}) \mathrm{C} 1=\mathrm{CC}(=[\mathrm{C}] \mathrm{C}(=\mathrm{C} 1) \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{Oc} 1 \mathrm{cccc}(\mathrm{c} 1 \mathrm{C}(=\mathrm{O})[\mathrm{O}-]) \mathrm{C}(=\mathrm{O})[\mathrm{O}-]$,</p>
<p>$\mathrm{OC}(=\mathrm{O}) \mathrm{C} 1=\mathrm{CC}(=[\mathrm{C}] \mathrm{C}(=\mathrm{C} 1) \mathrm{Oc} 1 \mathrm{cccc}(\mathrm{c} 1 \mathrm{C}(=\mathrm{O})[\mathrm{O}-]) \mathrm{C}(=\mathrm{O})[\mathrm{O}-]) \mathrm{C}(=\mathrm{O}) \mathrm{O}$, $\mathrm{OC}(=\mathrm{O}) \mathrm{clcc}(\mathrm{cc}(\mathrm{c} 1) \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{Oc} 1 \mathrm{cccc}(\mathrm{c} 1 \mathrm{C}(=\mathrm{O})[\mathrm{O}-]) \mathrm{C}(=\mathrm{O})[\mathrm{O}-]$, nodes $[\mathrm{Ba}][\mathrm{OH} 2][\mathrm{Ba}]$, topology pts output: 4.460577901
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: MoosaviCp task instruction generation structure, A solid line delineates a one-to-one correspondence</p>
<h1>E MDP classification task</h1>
<h2>Pei:</h2>
<p>instruction: What is phase of given alloy?
input: Ni0.9V0.1
output: fcc
WaterStability:
instruction: How is the water stabilityof given stability at normal measurement conditions?
input: meso-tetra(4-carboxyl-phenyl)porphyrin $(\mathrm{AlOH}) 2$
output: It has high water stability with medium confidence.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: WaterStability task instruction generation structure, A solid line delineates a one-to-one correspondence
matbench is metal:
instruction: Is given composition metal?
input: In5AgTe8
output: No, In5AgTe8 is not metal.
Matbench_glass:
instruction: Does given composition have glass formation ability?
input: $\mathrm{Zr5}(\mathrm{CrFe} 14) 3$
output: Yes, $\mathrm{Zr5}(\mathrm{CrFe} 14) 3$ has glass formation ability.</p>
<h1>F MDP inverse design task</h1>
<h2>OPV:</h2>
<p>instruction: Design a donor with acceptor PC61BM, PCE=0 for a organic solar cell output: $\mathrm{COC}(=\mathrm{O}) \mathrm{c} 1 \mathrm{cc} 2 \mathrm{csc}(-\mathrm{c} 3 \mathrm{cccs} 3) \mathrm{c} 2 \mathrm{~s} 1$</p>
<h2>G Summary of FAIR Datasets</h2>
<p>Table 4: Details of FAIR datasets and associated tasks they are used to derive instructions, (R for regression; D for inverse design; C for classification)</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Datasets</th>
<th style="text-align: center;">Description</th>
<th style="text-align: center;">Task</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChEMBL [37]</td>
<td style="text-align: center;">This dataset is sourced from a curated database of bioactive molecules with drug-like properties, focusing on the lipophilicity of 1899 molecular compounds in pharmacokinetics. The water-octanol partition coefficient $(\log \mathrm{D})$ is used to describe lipophilicity.</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">HybriD3 [38]</td>
<td style="text-align: center;">TThe HybriD materials database serves as a robust repository encompassing both experimental and computational data related to crystalline organic-inorganic compounds, with a predominant focus on the perovskite model. The data from 1518 materials are used in this work.</td>
<td style="text-align: center;">R\&amp;D</td>
</tr>
<tr>
<td style="text-align: center;">ESOL [39]</td>
<td style="text-align: center;">This dataset is a compilation of measured aqueous solubility (LogS) values, a crucial factor in drug discovery. The dataset comprises 927 molecular compounds originally used for ESOL - estimated solubility.</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">EPVRI [40]</td>
<td style="text-align: center;">The dataset is from the Emerging Photovoltaics Reports Initiative (EPVRI), which is an international academic platform in the field of emerging photovoltaic materials, including but not limited to organic, perovskite, and dye-sensitized solar cells. The dataset has the performance of 1177 solar cells</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">Magnet [41]</td>
<td style="text-align: center;">The database has a total of 3826 entries of magnetic materials, including 300+ Fe-based rare-earth-free magnets discovered through adaptive genetic algorithm (AGA) searches. 140 material results are used in this work.</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">Matbench_expt_gap [42]</td>
<td style="text-align: center;">This dataset is retrieved from Zhuo et al.'s work, containing data on experimental band gaps and DFT calculated zero band gaps for 4604 compounds.</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">Matbench_glass [43]</td>
<td style="text-align: center;">This dataset is retrieved from a volume of the Landolt-Börnstein collection 'Nonequilibrium phase diagrams of ternary amorphous alloys', containing data on full bulk metallic glass formation ability for 5680 chemical formulas.</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">Matbench_is_metal [35]</td>
<td style="text-align: center;">This dataset is retrieved from Zhuo et al.'s work, containing data on classifying metallicity from composition for 4921 chemical formulas.</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">Matbench_steels [35]</td>
<td style="text-align: center;">This dataset is retrieved from Citrine informatics, containing data on steel yield strengths from composition for 312 chemical formulas</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">MoosaviDiversity [44]</td>
<td style="text-align: center;">A diverse set of structures based on the chemical and geometric descriptors</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">MoosaviCp [44]</td>
<td style="text-align: center;">Dataset for predicting the heat capacity of materials based on density functional theory simulations</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">OPV [45]</td>
<td style="text-align: center;">Experimental parameters including PCE, molecular weight, and electronic properties are manually collected from the literature and subjected to machine learning with digitized chemical structures</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">Pei [46]</td>
<td style="text-align: center;">The dataset is from a thermoelectric material review paper published in 2013, which abstracts information from over 100 publications. We used data of 1028 materials from this database.</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">PolarM [47]</td>
<td style="text-align: center;">Polar Metals Materials Database contains polar compounds exhibiting metallic conductivity as defined in the work of Hickox-Young et al. There are 191 materials in the dataset.</td>
<td style="text-align: center;">R</td>
</tr>
<tr>
<td style="text-align: center;">TEM13 [41]</td>
<td style="text-align: center;">The dataset is from the work of Pei et al., originally used for machine learning training. It comprises information on 1252 alloys and their respective phases.</td>
<td style="text-align: center;">C</td>
</tr>
<tr>
<td style="text-align: center;">WaterStability [48]</td>
<td style="text-align: center;">Consists of water stabilities for over 200 MOFs (metal-organic frameworks), alongside a comprehensive set of chemical features encompassing the metal node, organic ligand, and metal-ligand molar ratios.</td>
<td style="text-align: center;">C</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/MasterAI-EAM/SciCrawler&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
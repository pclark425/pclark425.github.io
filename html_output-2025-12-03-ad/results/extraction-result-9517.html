<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9517 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9517</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9517</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-274656829</p>
                <p><strong>Paper Title:</strong> Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in these case studies. In each, we explore the impact of changing parameters on the accuracy of LLM responses. The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions. We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions. We developed a semantic text highlighting tool to facilitate expert review of LLM responses. We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%. We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers. The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9517.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9517.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-assisted SLR (case studies)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model-assisted Systematic Literature Review for extracting thematic/principled knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper reports four case studies in which GPT-3.5 Turbo and GPT-4 Turbo were used to extract evidence, thematic patterns (e.g., enablers, constraints, health outcomes), and screening decisions from sets of scholarly papers as part of systematic literature reviews (SLRs), combining in-context prompting, quote-finding, fuzzy verification and semantic highlighting with expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3.5 Turbo (GPT3) and GPT-4 Turbo (GPT4)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>The paper uses the commercial models named GPT-3.5 Turbo and GPT-4 Turbo (referred to as GPT3 and GPT4). Models were accessed via Microsoft Azure endpoints. The paper does not report architecture details, parameter counts, or fine-tuning specific to these experiments; models were used via prompting/in-context learning with entire papers provided as context (no external fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Interdisciplinary systems science SLRs including agri-food system transitions (health impacts), coordinated crisis response (policy/enablers & constraints), sustainable transitions, and automated marking / generative AI in education.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Four case studies with different corpora: Case 1 — 10 papers (agri-food transitions) and 8 extraction tasks; Case 2 — 12 papers (coordinated responses to crises) where a researcher had manually extracted enablers/constraints; Case 3 — ~60 papers for a sustainable transitions SLR (papers grouped by geographical scale, ~20 per category in experiments); Case 4 — screening datasets of 14 arXiv papers (12 relevant) and 20 PubMed papers (3 relevant). Papers were converted from PDF to text with pdftotext; in some tasks specific pages were removed to avoid extraneous content; domain definitions were provided in prompts; entire paper text was supplied in the prompt (context windows large enough), i.e., not Retrieval-Augmented Generation (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Thematic/generalizable patterns and rules: e.g., enablers and constraints, health outcome associations, geographic scope classifications, and screening relevance rules (thematic/principled findings rather than numeric models).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Example provided: research question 'What are the health outcomes of an agri-food transition?' — LLM retrieved quote 'An overabundance of food supply alone has been identified as a key cause of the obesity epidemic' and produced the concise extracted outcome 'Obesity' (i.e., mapping textual evidence to a generalizable outcome). The study also extracts enablers/constraints and categories such as 'Foodborne illnesses significantly influence individuals' nutritional status.'</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompted, in-context use of GPT models with the entire (pdftotext-converted) paper provided as context; two-step extraction where the model first finds quotes/evidence and then (in a second call) produces the final answer; prompt variants tested (direct answer vs evidence-first; separate vs combined tasks; 'prompt relevant'/'prompt irrelevant'/'neutral' for screening); keyword-driven semantic highlighting (SpaCy POS + WordNet Wu-Palmer + vector fallback) to aid human review; fuzzy string matching (thefuzz Levenshtein) to verify quote faithfulness; no model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Primary ground-truth: domain expert review of LLM outputs. Automated metrics: fuzzy string similarity (Levenshtein) to verify exact quote reproduction; SpaCy semantic similarity (average token vectors) and transformer-based embedding cosine similarity (sentence/answer embeddings) to measure semantic agreement with experts; highlighting-rate correlation (fraction of words highlighted) as an experimental proxy for similarity. Statistical analyses included Pearson correlation and Fisher-transform uncertainty estimates. Screening metrics: true/false positive/negative rates computed against expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Key quantitative findings: GPT-4 reproduced exact quotes with ~98% accuracy and GPT-3.5 with ~95% (Levenshtein fuzzy matching). GPT-4's average accuracy in answering research questions was approximately 83% across tasks (performance decreased with task complexity and nuance). Transformer-embedding cosine similarity correlated more strongly with expert judgments than SpaCy similarity (example correlation reported 0.48 ± 0.09 in a case study; across experiments transformer correlations ranged up to ~0.77), while SpaCy similarity was sensitive to capitalization. Asking the model to provide evidence (evidence-first) increased completion tokens/cost and—unexpectedly—slightly reduced performance on all metrics relative to direct answers. Combining many tasks in one prompt (together) reduced prompt tokens/cost but caused 'frame-shift' or 'jumbling' errors and lower researcher-preferred accuracy; separate calls performed better. Screening experiments showed higher false-positive rates (models more likely to label irrelevant papers as relevant) and sensitivity to prompt framing ('expect irrelevant' prompt reduced false positives). The semantic highlighting tool was found useful anecdotally and highlighting-rate correlated with human accuracy trends but had high uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Baseline was expert human review (used as ground truth). Comparisons: GPT-4 outperformed GPT-3.5 on quote reproduction and nuanced identification (fewer errors), but neither matched perfect expert performance on highly nuanced tasks. Transformer-similarity automated metric correlated better with expert judgments than SpaCy; the study did not compare LLM outputs directly to other automated literature-review tools (e.g., Elicit, Scite) in controlled experiments, though such tools are discussed in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Reported limitations include (1) context-window limits for smaller models (GPT3 shorter windows led to partial-document failures), (2) LLM difficulty with nuanced, domain-specific distinctions requiring expert-defined definitions, (3) model 'frameshifting' when handling many tasks in one prompt, (4) need for expert-in-the-loop for keyword calibration for highlighting, (5) large uncertainty in highlighting-correlation as automated metric, and (6) computational/cost trade-offs (evidence-first increases completion token costs). The paper also notes that transformer similarity scores cluster in a narrow range (0.7–0.95) and should be rescaled for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Paper explicitly discusses hallucinations (LLMs making up information) and addresses them by verifying quotes with fuzzy string matching; notes selection bias risks (models favoring well-known authors or recent papers) mitigated by researcher-selected corpora; inability to capture nuance and domain-specific vocabulary (mitigated by providing definitions in prompts); SpaCy similarity flaws (capitalization sensitivity); and that LLMs were more prone to false positives in screening tasks. The authors caution that automated similarity metrics themselves can carry biases from training data and are not substitutes for expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LitLLM: A Toolkit for Scientific Literature Review <em>(Rating: 2)</em></li>
                <li>Artificial Intelligence for Literature Reviews: Opportunities and Challenges <em>(Rating: 2)</em></li>
                <li>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success) <em>(Rating: 2)</em></li>
                <li>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis <em>(Rating: 2)</em></li>
                <li>Human-AI Collaboration to Identify Literature for Evidence Synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9517",
    "paper_id": "paper-274656829",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "LLM-assisted SLR (case studies)",
            "name_full": "Large Language Model-assisted Systematic Literature Review for extracting thematic/principled knowledge",
            "brief_description": "This paper reports four case studies in which GPT-3.5 Turbo and GPT-4 Turbo were used to extract evidence, thematic patterns (e.g., enablers, constraints, health outcomes), and screening decisions from sets of scholarly papers as part of systematic literature reviews (SLRs), combining in-context prompting, quote-finding, fuzzy verification and semantic highlighting with expert review.",
            "citation_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3.5 Turbo (GPT3) and GPT-4 Turbo (GPT4)",
            "llm_model_description": "The paper uses the commercial models named GPT-3.5 Turbo and GPT-4 Turbo (referred to as GPT3 and GPT4). Models were accessed via Microsoft Azure endpoints. The paper does not report architecture details, parameter counts, or fine-tuning specific to these experiments; models were used via prompting/in-context learning with entire papers provided as context (no external fine-tuning reported).",
            "application_domain": "Interdisciplinary systems science SLRs including agri-food system transitions (health impacts), coordinated crisis response (policy/enablers & constraints), sustainable transitions, and automated marking / generative AI in education.",
            "input_corpus_description": "Four case studies with different corpora: Case 1 — 10 papers (agri-food transitions) and 8 extraction tasks; Case 2 — 12 papers (coordinated responses to crises) where a researcher had manually extracted enablers/constraints; Case 3 — ~60 papers for a sustainable transitions SLR (papers grouped by geographical scale, ~20 per category in experiments); Case 4 — screening datasets of 14 arXiv papers (12 relevant) and 20 PubMed papers (3 relevant). Papers were converted from PDF to text with pdftotext; in some tasks specific pages were removed to avoid extraneous content; domain definitions were provided in prompts; entire paper text was supplied in the prompt (context windows large enough), i.e., not Retrieval-Augmented Generation (RAG).",
            "qualitative_law_type": "Thematic/generalizable patterns and rules: e.g., enablers and constraints, health outcome associations, geographic scope classifications, and screening relevance rules (thematic/principled findings rather than numeric models).",
            "qualitative_law_example": "Example provided: research question 'What are the health outcomes of an agri-food transition?' — LLM retrieved quote 'An overabundance of food supply alone has been identified as a key cause of the obesity epidemic' and produced the concise extracted outcome 'Obesity' (i.e., mapping textual evidence to a generalizable outcome). The study also extracts enablers/constraints and categories such as 'Foodborne illnesses significantly influence individuals' nutritional status.'",
            "extraction_methodology": "Prompted, in-context use of GPT models with the entire (pdftotext-converted) paper provided as context; two-step extraction where the model first finds quotes/evidence and then (in a second call) produces the final answer; prompt variants tested (direct answer vs evidence-first; separate vs combined tasks; 'prompt relevant'/'prompt irrelevant'/'neutral' for screening); keyword-driven semantic highlighting (SpaCy POS + WordNet Wu-Palmer + vector fallback) to aid human review; fuzzy string matching (thefuzz Levenshtein) to verify quote faithfulness; no model fine-tuning reported.",
            "evaluation_method": "Primary ground-truth: domain expert review of LLM outputs. Automated metrics: fuzzy string similarity (Levenshtein) to verify exact quote reproduction; SpaCy semantic similarity (average token vectors) and transformer-based embedding cosine similarity (sentence/answer embeddings) to measure semantic agreement with experts; highlighting-rate correlation (fraction of words highlighted) as an experimental proxy for similarity. Statistical analyses included Pearson correlation and Fisher-transform uncertainty estimates. Screening metrics: true/false positive/negative rates computed against expert labels.",
            "results_summary": "Key quantitative findings: GPT-4 reproduced exact quotes with ~98% accuracy and GPT-3.5 with ~95% (Levenshtein fuzzy matching). GPT-4's average accuracy in answering research questions was approximately 83% across tasks (performance decreased with task complexity and nuance). Transformer-embedding cosine similarity correlated more strongly with expert judgments than SpaCy similarity (example correlation reported 0.48 ± 0.09 in a case study; across experiments transformer correlations ranged up to ~0.77), while SpaCy similarity was sensitive to capitalization. Asking the model to provide evidence (evidence-first) increased completion tokens/cost and—unexpectedly—slightly reduced performance on all metrics relative to direct answers. Combining many tasks in one prompt (together) reduced prompt tokens/cost but caused 'frame-shift' or 'jumbling' errors and lower researcher-preferred accuracy; separate calls performed better. Screening experiments showed higher false-positive rates (models more likely to label irrelevant papers as relevant) and sensitivity to prompt framing ('expect irrelevant' prompt reduced false positives). The semantic highlighting tool was found useful anecdotally and highlighting-rate correlated with human accuracy trends but had high uncertainty.",
            "comparison_to_baseline": "Baseline was expert human review (used as ground truth). Comparisons: GPT-4 outperformed GPT-3.5 on quote reproduction and nuanced identification (fewer errors), but neither matched perfect expert performance on highly nuanced tasks. Transformer-similarity automated metric correlated better with expert judgments than SpaCy; the study did not compare LLM outputs directly to other automated literature-review tools (e.g., Elicit, Scite) in controlled experiments, though such tools are discussed in related work.",
            "reported_limitations": "Reported limitations include (1) context-window limits for smaller models (GPT3 shorter windows led to partial-document failures), (2) LLM difficulty with nuanced, domain-specific distinctions requiring expert-defined definitions, (3) model 'frameshifting' when handling many tasks in one prompt, (4) need for expert-in-the-loop for keyword calibration for highlighting, (5) large uncertainty in highlighting-correlation as automated metric, and (6) computational/cost trade-offs (evidence-first increases completion token costs). The paper also notes that transformer similarity scores cluster in a narrow range (0.7–0.95) and should be rescaled for interpretability.",
            "bias_or_hallucination_issues": "Paper explicitly discusses hallucinations (LLMs making up information) and addresses them by verifying quotes with fuzzy string matching; notes selection bias risks (models favoring well-known authors or recent papers) mitigated by researcher-selected corpora; inability to capture nuance and domain-specific vocabulary (mitigated by providing definitions in prompts); SpaCy similarity flaws (capitalization sensitivity); and that LLMs were more prone to false positives in screening tasks. The authors caution that automated similarity metrics themselves can carry biases from training data and are not substitutes for expert review.",
            "uuid": "e9517.0",
            "source_info": {
                "paper_title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LitLLM: A Toolkit for Scientific Literature Review",
            "rating": 2,
            "sanitized_title": "litllm_a_toolkit_for_scientific_literature_review"
        },
        {
            "paper_title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges",
            "rating": 2,
            "sanitized_title": "artificial_intelligence_for_literature_reviews_opportunities_and_challenges"
        },
        {
            "paper_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success)",
            "rating": 2,
            "sanitized_title": "summarizing_simplifying_and_synthesizing_medical_evidence_using_gpt3_with_varying_success"
        },
        {
            "paper_title": "A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis",
            "rating": 2,
            "sanitized_title": "a_hybrid_semiautomated_workflow_for_systematic_and_literature_review_processes_with_large_language_model_analysis"
        },
        {
            "paper_title": "Human-AI Collaboration to Identify Literature for Evidence Synthesis",
            "rating": 1,
            "sanitized_title": "humanai_collaboration_to_identify_literature_for_evidence_synthesis"
        }
    ],
    "cost": 0.010614499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science ⋆
16 Mar 2025</p>
<p>Lachlan Mcginness 
Australian National University</p>
<p>Peter Baumgartner 
Australian National University</p>
<p>Esther Onyango esther.onyango@csiro.au 
Australian National University</p>
<p>Zelalem Lema zelalem.moti@csiro.au 
Australian National University</p>
<p>Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science ⋆
16 Mar 2025F7B423E57F2980CA42B4159578A5E67510.1007/978-981-96-0348-0_3arXiv:2503.16515v1[cs.CL]Systematic Literature ReviewLarge Language ModelsHighlighting
Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR).We evaluate the performance of LLMs for SLR tasks in these case studies.In each, we explore the impact of changing parameters on the accuracy of LLM responses.The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions.We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions.We developed a semantic text highlighting tool to facilitate expert review of LLM responses.We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%.We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers.The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.</p>
<p>Introduction</p>
<p>The scientific community is currently full of hype and hope for the use of Artificial Intelligence (AI) to accelerate research [28].Messeri and Crockett claim that scientists are too trusting and lack awareness of the biases and errors of Large Language Models (LLMs) [26].They present 'AI as Oracle' as a vision of the future where LLMs overcome the problem of too much literature to digest by efficiently searching and summarising information [26].Many research groups are optimising and improving LLM tools for literature review [7,18,2] including the CSIRO's (Commonwealth Scientific and Industrial Research Organisation's) Science Digital program [10].</p>
<p>Many tools exist to enhance or automate literature review, including LitLLM [1], Scite [7], Elicit [18] and Scopus AI [2].The techniques of these tools remain undisclosed as commercial secrets.However they all appear to use a combination of the same strategies: calling LLMs through APIs, prompt engineering (incontext learning), Retrieval Augmented Generation (RAG) and fine tuning [6].The lack of transparency about these tools make it difficult to determine and accelerate best practice in AI systematic review methods.Systematic Literature Review (SLR) was developed in the field of Evidence-Based Medicine as a method of reducing bias by sticking to strict protocols [6].It has since been adopted in many disciplines including social science, education and environmental science [6].SLR tasks include planning, searching, screening, extraction and synthesis [6].In the extraction phase, desired information is extracted from a set of selected studies.Few studies have attempted to objectively measure the capability of LLMs for SLR [36,41,11,32,33] and even fewer for the extraction and screening phases.Previous studies found that LLMs are unreliable SLR tools as they 'hallucinate' references that do not exist [35].</p>
<p>In this paper we evaluate the performance of GPT-3.5 Turbo, and GPT-4 Turbo, one of the best models currently available, on SLR tasks.We will refer to these models as GPT3 and GPT4 respectively.We present four case studies where LLMs were used to assist CSIRO interdisciplinary systems science researchers, including the last two authors of this paper, in different stages of SLR.In each we systematically explore the impact of changing a parameter on the accuracy of LLM responses.</p>
<p>Although automatically checking LLM responses is highly desirable, currently there are no tools that can perfectly check the correctness or semantic similarity of texts.An important and tedious step of using LLMs in SLR is verifying their responses.To make this task easier we contribute a highlighting algorithm, in analogy to highlighting with a text marker on paper.It aims to provide a human reader with visual clues to quickly scan generated text.The algorithm is driven by a small set of user-supplied keywords provided by a domain expert.We describe the algorithm and report on experiments and experiences with application to our case studies.</p>
<p>The rest of this paper is structured as follows.Section 2 summarizes our methodology for application to the case studies chosen.This includes statistical evaluation methods and LLM techniques.Section 3 introduces an algorithm for explainable text relevance in terms of semantic similarity, and communicating it through text highlighting.Section 4 reports on experimental results, and Section 5 discusses these results and Section 6 summarises the conclusions.</p>
<p>Methodology</p>
<p>We explore four case studies with interdisciplinary system scientists.All four studies used Microsoft Azure endpoints to call either GPT3 and GPT4.The first case study focuses on the health impacts of agri-food transitions.In the second case study, the researcher had performed an SLR extracting the enablers and constraints from twelve papers on coordinated responses to crises.They were interested to know if they had missed any key points when completing the SLR.The third case study verifies an SLR involving sixty papers on a sustainable transitions SLR task [27].The final case study focuses on screening papers for an SLR on the use of generative AI for marking student responses to exam questions.The studies investigate the impact on overall performance when using different models, asking LLMs to provide evidence for their answers, and splitting tasks into several calls.</p>
<p>An example research question from the first case study is "What are the health outcomes of an agri-food transition"?The LLM was tasked with answering the question and finding evidence from an academic paper to support its answer.Evidence would be a quote from the paper such as "An overabundance of food supply alone has been identified as a key cause of the obesity epidemic" and the LLM answer could be "Obesity".</p>
<p>Statistical and Evaluation Techniques</p>
<p>In this section we outline the statistical methods used to analyse the results.In cases where multiple similar data are available these results are summarised using mean (µ x ) and population standard deviation (σ x ) defined as usual.</p>
<p>In our analysis we use two methods to determine the correctness of LLM responses; expert review and automated similarity metrics of LLM and expert answers.The fist automated similarity metric is SpaCy Semantic Similarity [14].This method compares two strings the average embedding vector of the tokens in each of the strings is calculated and the cosine similarity is taken.The second method is the cosine similarity between transformer-based embeddings [30,38,19], which we will refer to as 'transformer similarity'.Transformer based embeddings have the ability to take order of words into account.</p>
<p>To compare these two metrics we use Pearson correlation coefficient shown in Equation 1.
Correlation = r = n i=1 (µ x − x i )(µ y − y i ) σ x σ y(1)
Uncertainty in correlation values was calculated using the Fisher transformation of correlation [13] with a 95% confidence interval (Z = 1.96) using Equation 2. In order to make these values more comparable to standard deviation they were divided by Z.
Correlation Uncertainty = ∆r = 1 Z tanh arctanh(r) ± Z √ n − 3(2)
One of the SLR tasks involves the using an LLM to screen papers and determine their relevance.If a paper is relevant we refer to this as positive.A true positive (TP) result is when a model classifies a relevant paper (as determined by experts) as relevant.A false positive (FP) occurs when an irrelevant paper is classified as relevant.True negatives (TN) and false negatives (FN) are defined similarly.False positive and False Negative rates in the normal way.</p>
<p>LLM Techniques</p>
<p>We converted the research papers from PDF to text with the pdftotext utility5 .Instead of using standard RAG techniques, we provided the entire paper to the LLM as the context windows were large enough.The research scientists were concerned by the biases of LLMs [5,31,4].Here we outline these and our approach to minimising them.</p>
<p>Selection bias: The LLM might favour, for example, well known authors or recent papers.To avoid this type of bias, the research scientist selected the papers for the SLRs.</p>
<p>Inability to understand nuance: There are many studies which note that Large Language Models are effective for general tasks but can struggle with domain specific knowledge and the nuance of specialised tasks [9,24,21].We were concerned that LLMs may lack the nuance to determine whether information relates to the study in the paper or referenced studies.To overcome this issue, researchers removed specific pages based on the task that the LLM needed to perform.For example, when asking about the geographic location of the paper, only the abstract and introduction were provided.We also addressed this issue by using in-context learning to provide the model with specific definitions provided by domain experts.</p>
<p>Lack of domain specific knowledge: The domain or even subject specific vocabulary in scientific literature poses challenges for LLM-based analysis.LLMs are trained on vast corpuses of which the subject specific matter is only a small portion.In agri-food transitions and co-ordinated responses to crises research litterature terms have very specific meanings and LLMs would miss the nuances of these words.We mitigated this problem by providing the LLMs with definitions of subject specific vocabulary as part of the prompts.</p>
<p>Hallucinations: It is well known that LLMs can 'make up' information [17].To manage this, tasks were broken into two steps.First, the Large Language Model was asked to find quotes that provide evidence of the desired information.Binary (yes/no) verification of the quotes failed because of trivial errors related to unicode characters.Therefore quotes were instead verified using fuzzy text matching using thefuzz implementation6 of the Levenshtein distance metric [20].The LLM was then given the quotes and asked for a final answer.</p>
<p>Semantic Text Highlighting</p>
<p>In this section we introduce a method for semantic text highlighting, or highlighting for short.The idea of semantic highlighting is not new and was originally formulated for information retrieval [16].In our context we apply highlighting to LLM retrieved evidence.Because the amount of retrieved evidence can still be overwhelming, further support is needed to aid a human reviewer.This is where highlighting comes in.</p>
<p>Like with a text marker on paper, highlighting does not need to be perfect to be useful.However, the highlighted words should be semantically related to a given specific SLR research question.Our method requires a set of keywords representing the research question.Highlighting then boils down to determining which words in a given sentence are semantically related to the keywords and conveying the findings in a useful way.For that, we rely on readily available information-theoretic similarity measures and a carefully curated corpus of English, which requires no training and is explainable.</p>
<p>Our method works as follows.The SLR researcher provides sets of keywords comprising of entities E, relations R and properties P as nouns, (possibly transitive) verbs and adjectives or adverbs, respectively.For example, in our first SLR case study on the topic influences of agri-food transitions on health outcomes the following were used:</p>
<p>Entities E: health, disease, outcome, food, lifestyle Relations R: explain, affect, improve, stimulate Properties P : environmental A word w in a given text is highlighted if it is deemed related to a keyword according to the procedure Similarity(w, C, t) in Algorithm 1, where C = E ∪R∪ P .The given text is first parsed with the part-of-speech (POS) parser SpaCy [15].It assigns a grammatic role t to every word w for determining the best suited subset of C for similarity.The algorithm computes two similarity scores between w and that subset in the range [0, 1].One of them is word vector similarity, as readily provided by spaCy, and the other is Wu-Palmer similarity [40,34] based on hypernym-reachability in WordNet [12].</p>
<p>Using WordNet for word similarity is an established and well-researched topic [23].In our algorithm, the search for similar words is broad and includes (onestep) synonyms, pertainyms, and related derived forms, weighted for each category.This was a design choice motivated by the case study in Section 4.2, where the researcher wants to ensure they did not miss any key points in their manual review.Similarity takes scores higher than given thresholds to determine whether w should be highlighted or not.We found that Wu-Palmer similarity often produces results more similar to what a human user expects from a highlighter.Hence, vector similarity acts only as a fall-back.Here are some examples for highlighted evidence text with the keywords above.
R ← ∅ // Result relation for s ∈ S do R ← R ∪ {s 1.0 −→ s} // R is reflexive for l ∈ WN-Lemmas(s) do R ← R ∪ {s P-Weight −→ WN-SynSet(v) | v ∈ WN-Pertainyms(l)} R ← R ∪ {s RF-Weight −→ WN-SynSet(v) | v ∈ WN-RelatedForms(l)} return R
2. Foodborne illnesses significantly influence individuals nutritional status.</p>
<ol>
<li>
<p>Changing lifestyles, mainly due to work commitment, have fuelled the increase in numbers eating out and the need for convenience foods.</p>
</li>
<li>
<p>Significant changes have occurred in food systems in the last decades that have contributed to widen such 'holes' in the barriers from phase to phase: agricultural intensification and industrialization causing major environmental deterioration, the increasing distance traveled by food in global markets, and the nutrition transition towards diets rich in ultra -processed food and animal protein are the three cornerstones of such changes.</p>
</li>
</ol>
<p>Entities (nouns, noun chunks) are colored red and relations (verbs) are colored blue.Additional colors are used for supporting words according to their grammatical roles.Properties (adjectives) of colored entities are purple.</p>
<p>For each word, the algorithm can provide an explanation of why it is highlighted.These explanations are helpful for customising parameter settings; for example, we get: In these annotations, NCP means 'NounChunkPart', and the similarity of the highlighted word(s) to keyword(s) is indicated as in SimilarTo(keyword, similarity, kind), where 'wup' is Wu-Palmer similarity.</p>
<p>Highlighting can be useful beyond marking up text excerpts.In one of our case studies below we take the 'highlighting rate' as a statistical measure to assess the similarity between the LLM's response and the researcher's benchmark evaluation.</p>
<p>Results</p>
<p>Case Study 1: Similarity Metrics in Agri-Food Transition SLR</p>
<p>In this first case study we determine the accuracy of the chosen metrics and explore if there is a significant difference in performance between GPT3 and GPT4.There were eight tasks (as shown in the first column of Table 1) and ten papers.GPT3 and GPT4 were given identical instructions to answer each task in separate calls.We compared the researcher's and models' answers.The models were asked to record three quotes (evidences), then give a final answer in a second call.Cases where the average fuzzy string similarity was less than 90% were manually investigated.There were 25 and 38 cases where this occurred for GPT4 and GPT3 respectively, resulting in overall error rates of 2% and 5%.</p>
<p>The semantic similarity between LLM and expert answers was calculated using transformer similarity and SpaCy similarity.An example of an LLM/expert answer could be "The Global context is Africa" which has a word count of 5.The correlation between the Transformer similarity score and the expert's judgement of the model answers was 0.48 ± 0.09 and there was almost no correlation (−0.07 ± 0.08) between the SpaCy similarity and expert judgment.</p>
<p>Case Study 2: Impact of Evidence on Coordinated Response to</p>
<p>Crisis SLR</p>
<p>In this case study we compare LLM output based on two methods.In the first method ('evidence') the LLM first obtains quotes to support its answer to the question and then writes it's answer.In the second method ('direct') the LLM writes an answer without searching for or providing evidence.</p>
<p>Providing evidence is a good way to increase the trustworthiness of LLM responses.However it will increase the number of completion tokens and therefore cost.We test our highlighting algorithm as an automated similarity metric by calculating the correlation between the highlighted fraction of each expert answer and model answers.</p>
<p>Table 2 shows that the evidence method results in slightly lower SpaCy Semantic Score, vector embedding cosine similarity, human judged accuracy and highlighting correlation.The highlighting correlation score changes by a more significant margin, but also has a greater uncertainty than the other measures.The number of prompt tokens is not significantly changed by asking the model for evidence, but the number of completion tokens increases fourfold.As the majority of tokens are prompt tokens, it might be expected that the number of completion tokens would have a small impact on the overall cost.However the computational cost of running a transformer in this case is proportional to the number of completion tokens.A technique to more effectively reduce the cost using an LLM for literature review is grouping multiple tasks into a single call as explored in the next case study.</p>
<p>Case Study 3: Impact of Combining Tasks on Sustainable</p>
<p>Transitions SLR</p>
<p>In this case study, we compare conditions that we name 'separate' and 'together'.</p>
<p>For the separate condition, GPT4 is called ten times.In each call the paper is provided and the LLM is asked to find an answer and evidence for a research question.In the together condition, the model is given all tasks in one call.The results for both experimental conditions are in Table 3.Note that papers were classified into three groups according to their geographical scale as requested by the researcher.There were twenty papers in each category.The researcher did not like the results of the together condition because GPT4 jumbled its responses resulting in a frame-shift.Despite the near tenfold increase in prompt tokens and cost, they preferred the separate condition.In the separate condition the model had no awareness of its answers to the other questions resulting in overlap of the ideas presented for each sub-question.</p>
<p>Case Study 4: Prompt Variations for Automated Screening</p>
<p>In the final case study the researcher wanted to automatically extract information from papers for the purpose of screening.The researcher had manually reviewed 14 papers from the arXiv and 20 from Pubmed of which 12 and 3 were relevant respectively.These were used as datasets to measure the performance of LLM information extraction.These datasets provide opposite extremes, one where the LLM needs to accept nearly all of the papers as relevant and another where it needs to reject nearly all the papers.This allowed for a study in simultaneously avoiding false positives and false negatives, see Table 4. Three prompts were used: 'prompt relevant' which indicated that the paper was likely to be relevant, 'prompt irrelevant', and a 'neutral prompt' which makes no indication of the paper's relevance.Table 4. Case Study 4: SpaCy and transformer similarity, expert verified accuracy, false positive and false negative rates for three different prompts on both datasets.The correlation between expert accuracy and the similarities were calculated to measure the quality of these metrics.The false positive and false negative rates in Table 4 show the LLM was likely to state that a paper was relevant when in fact it was not.The correlations show transformer similarity correlated better with expert review than SpaCy similarity.It was found that SpaCy similary is heavily impacted by changes in capitalisation.In some cases changes in capitalisation alone reduced the similarity to 0.3.</p>
<p>Discussion</p>
<p>In the first case study, GPT4 made less mistakes than GPT3 when extracting exact quotes from documents.This may be because of the quality of the model or GPT3's shorter maximum context length.The longest context length available for GPT3 models was 16,000 tokens which was not always sufficient to read an entire paper.When the paper was broken into multiple sections the model was less likely to find relevant quotes.We noticed that GPT3 was more likely to select quotes from the beginning of the context window than GPT4, whose quotes were more evenly distributed from the entire paper.</p>
<p>Across all studies it was found that transformer similarity correlates more strongly with expert opinion than than SpaCy similarity.This indicates that transformer embeddings are a better metric.This is expected given that it is able to take the positions of words into account.The correlation between amounts of text highlighted increased with increased human judged accuracy, showing the correct trend as a measure of semantic similarity.However the uncertainty values were very high and more work would need to be performed to determine if this is a suitable metric.</p>
<p>The highlighting tool's primary purpose is to aid a researcher in sifting through evidence and other LLM response text.The researchers anecdotally confirmed its value.</p>
<p>We originally anticipated that SpaCy's similarity would be a better measure of semantic similarity than transformer similarity because the transformer similarity scores only ranged between 0.7 and 1, while SpaCy similarities scores had much larger ranges.Contrary to our original expectations, transformer similarity ubiquitously correlated more strongly with expert opinion.To make transformer similarity more interpretable for humans we recommend scaling these values before interpretation.This is because a transformer similarity of 0.8 is actually low, despite normal human expectations.</p>
<p>The second case study found that when a model was asked to provide evidence for its claims, it was slightly less accurate on all metrics including expert judgement.This is an unexpected result as normally 'chain of thought' or asking a model to explain its reasoning improves performance [39,25].As trustworthiness is increased when the model provides verifiable evidence for its answers, this result indicates that there is an unfortunate trade off between accuracy and trustworthiness.The reason could be that the model is 'overloaded' when it needs to focus on multiple tasks at once.This was confirmed by the third case study (Section 4.3) where model performance also decreased when the number of tasks it was asked to complete in a single call increased.</p>
<p>In third third case study, as expected the number of prompt tokens is approximately ten times higher for the separate condition compared to the together condition.As far as the researcher was concerned the reduced number of errors in the results was worth the the extra cost.The major issue with the together condition was the possibility for tasks to be jumbled.One area for future work is to apply more advanced parsing techniques to avoid frameshift errors and therefore make the computationally cheaper technique more desirable for researchers.Another area for future work is to provide the model with specific keywords for each call to guide the LLM responses.</p>
<p>The fourth case study demonstrated that SpaCy semantic similarity can be heavily impacted by the capitalisation of words for medium sized models.This is not a desirable property for a system measuring semantic similarity; we argue that writing a sentence in all capital letters makes little change to the semantic meaning.</p>
<p>Readers may be tempted to think that scores such as SpaCy or transformer similarity could be used as better alternatives to subject expert review as they do not contain human biases.However, one needs to be careful in assuming that there is no bias when using metrics like these.There can be an illusion of objectivity, when in fact these models have been trained and validated on data which contains significant unknown biases.In this study, we highly value the opinion of experts who are part of the active research community and have observed that they demonstrate a strong awareness of their own biases.</p>
<p>False positive rates were consistently higher than false negative rates; GPT4 was more likely to think that an irrelevant paper was in fact relevant and most accurately screened papers when prompted to expect that papers may be irrelevant.</p>
<p>Overall GPT3 and GPT4 were able to find and correctly reproduce quotes from a text with 95% and 98% accuracy respectively.For low complexity tasks like finding the title or location of a paper, GPT4 performed with close to 100% accuracy, but accuracy was lower for more nuanced tasks such as identifying enablers in agri-food transitions.The overall approximate average accuracy of GPT4 in answering research questions was 83%.</p>
<p>Our highlighting workflow requires keyword calibration to determine a suitable set of keywords for each research question in an SLR.Currently, keyword calibration is semi-automated process with an expert in the loop.The expert proposes keywords, runs highlighting experiments on sample papers and adjusts the set of keywords according to their observations.</p>
<p>From our experiments we were able to derive some guidelines for calibration.A keywords set yielding a highlighting rate of around 0.4 ± 0.1 on evidence texts often seems to be a good compromise.If it is much higher, often too many irrelevant words are highlighted.If much lower, the domain has not been covered sufficiently and relevant keywords are missing.It is better to use evidence text than expert answers for keyword calibration as they are almost always proper English sentences.Expert assessments however can vary widely and sometimes are just lists of keywords.Hence the proportion of highlighted words is less reliable in this case, resulting in a less meaningful hit rate.</p>
<p>If the hit rate is unusually high this could be because of denser writing (less filler words) or because of denser information.The latter includes the possibility that surprising, additional insights have been unveiled.This helps to get a more complete picture of the problem.Conversely, we found that a much lower highlighting rate typically applies to irrelevant texts.</p>
<p>Conclusion</p>
<p>Large Language Models (LLMs) were used to assist interdisciplinary system scientists to conduct four Systematic Literature Reviews (SLR).The topics of the reviews were agri-food system transitions, coordinated responses to crises, sustainable transitions and automated marking.GPT-3.5 Turbo and GPT-4 Turbo had error rates of 5% and 2% when extracting exact quotes from research papers.Levenshtein distance accurately determined the faithfulness of quotes produced by LLMs and was robust to unexpected unicode characters and hyphenations.</p>
<p>When GPT-4 Turbo completed multiple tasks in a single prompt the number of prompt tokens decreased tenfold but with significant losses in accuracy in some cases due to frameshift errors.One area for future work would be to use more advanced parsing techniques to avoid frameshift or other 'jumbling' errors.</p>
<p>The accuracy of the models' answers was found to decrease with complexity of the task.For very simple tasks, expert rating of LLM answer correctness was close to 100% while for highly nuanced tasks it could be as low as 10%.On average it was found that GPT-4 Turbo was able to extract information from papers with approximately 83% accuracy.When screening papers it was found that GPT-4 Turbo was more likely to include irrelevant papers than exclude relevant papers.One area for future work is to provide the model with specific keywords to focus its answers, this may help a model focus on the desired ideas while trying to complete a nuanced task.</p>
<p>It was found that taking the cosine similarity of transformer embeddings of expert and LLM answers was a measure of accuracy that correlated more strongly with expert opinion than SpaCy's semantic similarity score.Nearly all of these transformer embedding cosine scores were in the range of 0.7 to 0.95.In order to make these cosine similarities more human interpretable, we recommend scaling them to take up the full range between 0 and 1.An area of future work would be to use cosine similarity of transformer embeddings for sentence-wise comparison of researcher and LLM answers in order to determine if any important pieces of information are missing.</p>
<p>Although highlighting is designed to assist researchers with manual checking of answers, correlation between amounts of highlighted text is showing some promise as an automated method for measuring the quality of LLM responses.Additional research would need to be conducted to see if this can be used as a valid similarity metric.Another idea for future work is to re-formulate the semantic similarity algorithm (Algorithm 1) with probabilistic logic programming.This would allow for a more flexible and expressive framework.In addition, weight parameters could be rephrased as probabilities and be learned from examples by maximum likelihood estimation.</p>
<p>Algorithm 1
1
Similarity Similarity(w, C, t) Input: w word, C keywords in canonical form (lemmas), t type of w (noun, verb, adjective ...) Output: Similarity score for w P-Weight ← 0.95 RF-Weight ← 0.95 WUP-Threshold ← 0.8 VEC-Threshold ← 0.95 bestwup ← max c∈C WUP-X(w, c, t) or else 0.0 bestvec ← max c∈C VEC(w, c) or else 0.0 if bestwup ≥ WUP-Threshold and bestwup ≥ bestvec then return bestwup elif bestvec ≥ VEC-Threshold then return bestvec else return 0.0 WUP-X(w, c, t) // Extended Wu-Palmer similarity, considers reachable words from w and c Sw = Extend(WN-Synsets(w, t)) // WN-Synsets returns synonyms of w Sc = Extend(WN-Synsets(c, t)) return max (sw ωw −→rw ,sc ωc −→rc)∈Sw ×Sc ωw • ωc • WN-WUP(rw, rc, t) // Wu-Palmer from WordNet VEC(w, c) // Vector similarity if w = c then return 1.0 elif both w and c have vector embeddings then return cosine-similarity of the embeddings of w and of c else return 0.0 Extend(S, t) Input: S a set of WordNet synsets Output: Weighted extension of S by pertainyms and derivationally related forms</p>
<p>2 .
2
Foodborne illnesses (NCP(Foodborne illnesses, [SimilarTo('disease', 0.95, 'wup')])) significantly influence (SimilarTo('affect, 0.84, 'wup')) individuals nutritional status (NCP(nutritional status, [SimilarTo('food', 0.91, 'wup')])).</p>
<p>Table 1 .
1
Case Study 1: Average and standard deviation for similarity scores, fuzzy matching scores, average word count and expert judged model accuracy are presented for GPT3 and GPT4.
Information Complexity ModelQuote Fuzzy Matching ScoreModel Average Word CountExpert Average Word CountTransformer SimilaritySpaCy SimilarityModel AccuracyGlobal ContextLowGPT-GPT-97 ± 5 98 ± 63.5 5.24.8 4.80.74 ± 0.25 0.58 ± 0.16 0.84 ± 0.06 0.57 ± 0.140.9 1.0Associated Health FocusLowGPT-GPT-97 ± 5 98 ± 45.6 71.6 1.60.82 ± 0.13 0.57 ± 0.16 0.75 0.85 ± 0.05 0.60 ± 0.14 0.94Transition PathwayGPT-Moderate GPT-95 ± 10 97 ± 813.4 235.4 5.40.81 ± 0.04 0.66 ± 0.08 0.65 0.85 ± 0.06 0.65 ± 0.21 1.0Agri-food BoundaryGPT-Moderate GPT-97 ± 4 98 ± 632 50.617 170.83 ± 0.04 0.77 ± 0.14 0.87 ± 0.03 0.79 ± 0.12 0.85 0.5Public Health RiskGPT-Moderate GPT-99 ± 7 97 ± 68.8 20.56.5 6.50.85 ± 0.05 0.59 ± 0.16 0.87 ± 0.06 0.74 ± 0.17 0.95 0.7SynergiesHighGPT-GPT-97 ± 5 98 ± 531.3 5826.4 26.40.83 ± 0.03 0.84 ± 0.07 0.25 0.81 ± 0.05 0.83 ± 0.07 0.1ConstraintsHighGPT-GPT-97 ± 5 98 ± 535 5918 180.82 ± 0.02 0.81 ± 0.08 0.44 0.84 ± 0.02 0.83 ± 0.07 1.0Integrated SolutionsHighGPT-GPT-97 ± 4 99 ± 328 5030 300.89 ± 0.04 0.90 ± 0.07 0.88 0.89 ± 0.05 0.89 ± 0.07 1.0</p>
<p>Table 2 .
2
Case Study 2: Prompt tokens, completion tokens, SpaCy similarity, transformer similarity, human judged accuracy and Highlighting Correlation are presented for comparison of direct and evidence-based conditions.
Experimental Prompt Tokens CompletionSpaCyTransformer Human Judged HighlightingCondition(×10 3 )TokensSimilaritySimilarityAccuracyCorrelationEvidence20.2 ± 5.7 842 ± 397 0.85 ± 0.06 0.87 ± 0.0669%−0.18 ± 0.24Direct20.0 ± 5.7 213 ± 64 0.88 ± 0.06 0.90 ± 0.0472%0.13 ± 0.25</p>
<p>Table 3 .
3
Case Study 3: Quote and final answer metrics for both conditions.The expert found significant errors in the first paper from the global scale responses for the together condition.They decided that the together method was not worth pursuing and did not evaluate the remaining together responses.
Experimental ConditionScale of paperPrompt Tokens ×10 5Completion Tokens ×10 3Fuzzy Text MatchingSpaCy SimilarityTransformer SimilarityInstances of failing to find quotesExpert Judged AccuracyGlobal1.1 ± 0.21.5 ± 0.2 98.4 ± 2.5 0.84 ± 0.4 0.85 ± 0.01082%SeparateInternational /national4.7 ± 1.71.3 ± 0.1 95.3 ± 6.8 0.79 ± 0.12 0.86 ± 0.01584%Subnational /Local2.2 ± 0.71.4 ± 0.2 99.4 ± 0.6 0.83 ± 0.03 0.84 ± 0.06275%Global0.12 ± 0.02 0.97 ± 0.37 99.5 ± 0.4 0.79 ± 0.5 0.83 ± 0.030N/ATogetherInternational /national0.48 ± 0.16 1.3 ± 0.2 93.1 ± 7.2 0.77 ± 0.16 0.84 ± 0.019N/ASubnational /Local0.22 ± 0.07 0.8 ± 0.2 98.0 ± 1.0 0.81 ± 0.03 0.84 ± 0.011N/A
https://www.xpdfreader.com/pdftotext-man.html
https://github.com/seatgeek/thefuzz
. It is also likely that climate change will contribute to novel occurrences of disease emergence and transmission.
Acknowledgements.This research was supported by funding from CSIRO Data61 and its Valuing Sustainability Future Science Platform initiative.We thank Enayat Moallemi for providing knowledge, expertise and data for the third case study.We thank Stephen Wan and Shima Khanehzar for helpful discussions.
LitLLM: A Toolkit for Scientific Literature Review. S Agarwal, I H Laradji, L Charlin, C Pal, 2024</p>
<p>Accelerating research processes with Scopus AI: A place branding case study. E Aguilera-Cora, C Lopezosa, J Fernández-Cavia, L Codina, 10.21555/rpc.v6i1.3088Revista Panamericana de Comunicación. 612024</p>
<p>Using LLM (Large Language Model) to Improve Efficiency in Literature Review for Undergraduate Research. S A Antu, H Chen, C K Richards, 2023WS on Empowering Education with LLMs</p>
<p>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, 10.1145/3442188.3445922Proc. FAccT '21. FAccT '21ACM2021</p>
<p>Language (Technology) is Power: A Critical Survey of "Bias" in NLP. S L Blodgett, S Barocas, Iii Daumé, H Wallach, H , 10.18653/v1/2020.acl-main.485Proc. 58th Annual Meeting of the ACL. ACL. 58th Annual Meeting of the ACL. ACL2020</p>
<p>F Bolanos, A Salatino, F Osborne, E Motta, arXiv:2402.08565Artificial Intelligence for Literature Reviews: Opportunities and Challenges. 2024</p>
<p>. S Brody, 10.5195/jmla.2021.1331Scite. Journal of the Medical Library Association. 10942021</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proc. NeurIPS. NeurIPS2020</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, J Wang, Y Wu, L Yang, K Zhu, H Chen, X Yi, C Wang, Y Wang, W Ye, Y Zhang, Y Chang, P S Yu, Q Yang, X Xie, 10.1145/3641289ACM Trans. Intell. Syst. Technol. 1532024</p>
<p>CINTEL. Collaborative Intelligence Future Science Platform. 2024</p>
<p>De Silva, A Wijekoon, J L Liyanarachchi, R Panchendrarajan, R Rajapaksha, W , arXiv:2403.03293AI Insights: A Case Study on Utilizing ChatGPT Intelligence for Research Paper Analysis. 2024</p>
<p>WordNet: An electronic lexical database. C Fellbaum, 1998Bradford Books</p>
<p>Frequency Distribution of the Values of the Correlation Coefficient in Samples from an Indefinitely Large Population. R A Fisher, 10.2307/2331838Biometrika. 1041915</p>
<p>M Honnibal, I Montani, Linguistic Features • spaCy Usage Documentation. </p>
<p>spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. M Honnibal, I Montani, 2017</p>
<p>Semantic highlighting. A Hussam, B Ford, J Hyde, A Merayyan, B Plummer, T Anderson, 10.1145/286498.286667CHI 98 Conference Summary on Human Factors in Computing Systems. CHI '98. ACM1998</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, 10.1145/3571730ACM Comput. Surv. 55122023</p>
<p>Journal of the Canadian Health Libraries Association. J Kung, 10.29173/jchla29657Journal de l'Association des bibliothèques de la santé du Canada. 4412023Elicit (product review)</p>
<p>Contextualized Embeddings based Transformer Encoder for Sentence Similarity Modeling in Answer Selection Task. M T R Laskar, J X Huang, E Hoque, Proc. Twelfth LREC. ELRA. Twelfth LREC. ELRA2020</p>
<p>Binary codes capable of correcting deletions, insertions and reversals. V I Levenshtein, Soviet Physics Doklady. 107071966</p>
<p>Nuances are the Key: Unlocking ChatGPT to Find Failure-Inducing Tests with Differential Prompting. T O Li, W Zong, Y Wang, H Tian, Y Wang, S C Cheung, J Kramer, 10.1109/ASE56229.2023.0008938th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2023. 2023</p>
<p>Y Li, L Chen, A Liu, K Yu, L Wen, arXiv:2403.02574ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary. 2024</p>
<p>Fuzzy word similarity: A semantic approach using WordNet. S Manna, B S U Mendis, 10.1109/FUZZY.2010.5584785International Conference on Fuzzy Systems. Barcelona, SpainIEEE2010</p>
<p>The application of large language models in medicine: A scoping review. X Meng, X Yan, K Zhang, D Liu, X Cui, Y Yang, M Zhang, C Cao, J Wang, X Wang, J Gao, Y G S Wang, J Ji, Z Qiu, M Li, C Qian, T Guo, S Ma, Z Wang, Z Guo, Y Lei, C Shao, W Wang, H Fan, Y D Tang, 10.1016/j.isci.2024.109713iScience. 2752024</p>
<p>Automated Theorem Provers Help Improve Large Language Model Reasoning. L Mcginness, P Baumgartner, 10.29007/2n9mProceedings of 25th Conference on Logic for Programming. N Bjørner, M Heule, A Voronkov, 25th Conference on Logic for ProgrammingEasyChair2024100</p>
<p>Artificial intelligence and illusions of understanding in scientific research. L Messeri, M J Crockett, 10.1038/s41586-024-07146-0Nature. 6272024</p>
<p>Entry points for accelerating transitions towards a more sustainable future. E Moallemi, M Miller, K Szetey, S Chakori, J Palmer, M Battaglia, B A Bryan, L Gao, A Hall, P Leith, R Raven, P M Reed, 10.31223/X5C68X2024EarthArXiv pre-print</p>
<p>Why scientists trust AI too much -and what to do about it. 10.1038/d41586-024-00639-yNature Editorial. 62780032024Nature</p>
<p>Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis. M Ormerod, J Martínez Del Rincón, B Devereux, JMIR Medical Informatics. 95e230992021</p>
<p>Changing the World by Changing the Data. A Rogers, 10.18653/v1/2021.acl-long.170Proc. 59th Annual Meeting of the ACL and the 11th IJCNLP. ACL. 59th Annual Meeting of the ACL and the 11th IJCNLP. ACL2021</p>
<p>System for systematic literature review using multiple AI agents: Concept and an empirical evaluation. A M Sami, Z Rasheed, K.-K Kemell, M Waseem, T Kilamo, M Saari, A N Duc, K Systä, P Abrahamsson, 2024</p>
<p>Summarizing, Simplifying, and Synthesizing Medical Evidence using GPT-3 (with Varying Success). C Shaib, M Li, S Joseph, I Marshall, J J Li, B Wallace, Proc. 61st Annual Meeting of the ACL (Short Papers). 61st Annual Meeting of the ACL (Short Papers)ACL2023</p>
<p>A New Similarity measure for taxonomy based on edge counting. M Shenoy, 10.5121/ijwest.2012.3403International journal of Web &amp; Semantic Technology. 342012</p>
<p>Reviews and Reviewing: Approaches to Research Synthesis. An Annual Review of Information Science and Technology (ARIST) paper. L C Smith, 10.1002/asi.24851Journal of the ASIS&amp;T. 7532024</p>
<p>Human-AI Collaboration to Identify Literature for Evidence Synthesis. S Spillias, P Tuohy, M Andreotta, R Annand-Jones, F Boschetti, C Cvitanovic, J Duggan, E Fulton, D Karcher, C Paris, R Shellock, R Trebilco, 10.21203/rs.3.rs-3099291/v1Research Square. 2023</p>
<p>Artificial intelligence to automate the systematic review of scientific literature. J De La Torre-López, A Ramírez, J R Romero, 10.1007/s00607-023-01181-xComputing. 105102023</p>
<p>Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings. J Turton, R E Smith, D Vinson, 10.18653/v1/2021.repl4nlp-1.26Proc. RepL4NLP-2021. ACL. RepL4NLP-2021. ACL2021</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. 35NeurIPS 2022. 2022</p>
<p>Verb Semantics and Lexical Selection. Z Wu, M Palmer, 10.3115/981732.98175132nd Annual Meeting of the ACL. ACL. 1994</p>
<p>A Hybrid Semi-Automated Workflow for Systematic and Literature Review Processes with Large Language Model Analysis. A Ye, A Maiti, M Schmidt, S J Pedersen, 10.3390/fi16050167Future Internet. 1651672024</p>
<p>Divide and Conquer: Text Semantic Matching with Disentangled Keywords and Intents. Y Zou, H Liu, T Gui, J Wang, Q Zhang, M Tang, H Li, D Wang, Findings of the ACL. ACL. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
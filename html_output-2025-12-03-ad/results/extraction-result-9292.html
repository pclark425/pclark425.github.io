<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9292 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9292</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9292</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-c6f7bfaf00fcef358a4774580f42478f712f073a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c6f7bfaf00fcef358a4774580f42478f712f073a" target="_blank">Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a novel framework called CFDet for fine-grained anomalous entry detection that makes use of the deep support vector data description approach to detect anomalous sequences and proposes a novel counterfactual interpretation-based approach to identify anomalous entries in the sequences.</p>
                <p><strong>Paper Abstract:</strong> Anomaly detection in sequential data has been studied for a long time because of its potential in various applications, such as detecting abnormal system behaviors from log data. Although many approaches can achieve good performance on anomalous sequence detection, how to identify the anomalous entries in sequences is still challenging due to a lack of information at the entry-level. In this work, we propose a novel framework called CFDet for fine-grained anomalous entry detection. CFDet leverages the idea of interpretable machine learning. Given a sequence that is detected as anomalous, we can consider anomalous entry detection as an interpretable machine learning task because identifying anomalous entries in the sequence is to provide an interpretation to the detection result. We make use of the deep support vector data description (Deep SVDD) approach to detect anomalous sequences and propose a novel counterfactual interpretation-based approach to identify anomalous entries in the sequences. Experimental results on three datasets show that CFDet can correctly detect anomalous entries.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9292.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9292.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFDet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Counterfactual Detection (CFDet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-phase framework that first detects anomalous sequences via Deep SVDD with an LSTM encoder and then identifies fine-grained anomalous entries by training an LSTM-based entry detector using counterfactual explanations, triplet/continuity/sparsity losses and policy-gradient optimization for discrete indicators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CFDet (Deep SVDD with LSTM encoder + LSTM anomalous-entry detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM (sequence encoder) + Deep SVDD (one-class objective); LSTM + logistic regression for entry detector; policy-gradient for discrete outputs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete event sequences (sliding-window sequences of log templates or user activity tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird) and synthetic insider-activity logs (CERT)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous sequences and anomalous entries (including contextual and group anomalies, rare events/errors)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train an LSTM encoder f(·) on normal sequences with Deep SVDD objective to learn a center c; label sequences with large distance to c as anomalous; for each detected anomalous sequence Z train an LSTM-based entry detector g(·) that outputs a binary indicator sequence A such that Z^+ = (1-A)⊙Z is close to c (normality) while Z^- = A⊙Z is far from c (comprehensiveness), with continuity and sparsity regularizers; train g using policy-gradient RL because outputs are discrete.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Attention-based interpretation (LSTM+attention), Shapley-value attribution, One-Class SVM (OCSVM), Isolation Forest (iForest)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score, AUC (ROC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Anomalous-entry detection on unlabeled U (mean ± std): BGL: Precision 98.91±0.91, Recall 98.56±1.52, F1 98.73±0.82, AUC 99.07±0.74; Thunderbird: Precision 90.99±10.33, Recall 100.00±0.00, F1 94.97±6.41, AUC 99.33±0.97; CERT: Precision 99.31±2.19, Recall 55.31±0.10, F1 71.04±0.50, AUC 77.65±0.04. Anomalous-sequence detection (Deep SVDD) on U: BGL F1 98.41±0.56, Thunderbird F1 97.19±1.96, CERT F1 77.88±0.00.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>CFDet outperforms baselines (Attention, Shapley, OCSVM, iForest) on F1 and AUC across datasets; Shapley performs well on BGL but overall CFDet achieves highest F1/AUC. Traditional OCSVM/iForest perform poorly on entry-level detection due to feature mismatch and context dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Dependence on initial anomalous-sequence detector: if Deep SVDD misses anomalous sequences, their entries cannot be identified (explains low recall on CERT); requires tuning of triplet/continuity/sparsity hyperparameters; policy-gradient training for discrete indicators can be more complex/stochastic; precision/recall tradeoffs vary with sequence length; approach designed for discrete sequences (logs/activities) and may need adaptation for continuous/tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Formulates fine-grained entry detection as counterfactual explanation: the normal subsequence should move close to the Deep SVDD center while the anomalous subsequence moves away; uses triplet loss with center as anchor plus continuity and sparsity priors to enforce concise contiguous anomalous segments; trains discrete selectors via policy-gradient, enabling post-hoc interpretations in an unsupervised/one-class setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9292.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep SVDD (LSTM encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Support Vector Data Description with LSTM encoder</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-class classification approach that minimizes the volume of a hypersphere enclosing normal-data representations; implemented here with an LSTM encoder to map sequences to latent representations and compute distances to a center c for anomaly scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep One-Class Classification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deep SVDD (with LSTM encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>One-class Deep neural network objective; LSTM sequence encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequential discrete data (log-template sequences, user-activity sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs and user activity logs (BGL, Thunderbird, CERT)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous sequences (out-of-distribution/aberrant sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train LSTM encoder f(·) on normal sequences to minimize squared distance of representations to center c (SVDD loss) plus weight regularization; derive anomaly score as squared distance to c; sequences with score > ε labeled anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared implicitly against OCSVM/iForest for sequence/entry detection tasks in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1-score, AUC for anomalous-sequence detection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Anomalous-sequence detection on U (mean ± std): BGL Precision 98.81±0.39, Recall 98.02±1.14, F1 98.41±0.56, AUC 98.73±0.54; Thunderbird Precision 94.59±3.62, Recall 100.00±0.00, F1 97.19±1.96, AUC 93.75±4.55; CERT Precision 100.00±0.00, Recall 63.77±0.00, F1 77.88±0.00, AUC 81.88±0.00.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Deep SVDD (LSTM) provided strong anomalous-sequence detection performance and served as the foundation for entry-level counterfactual detection; better recall/precision than baselines used for entry detection when aggregated into CFDet.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Missed anomalous sequences reduce downstream entry detection recall (observed on CERT); performance sensitive to sequence-length choices and center update schedule; assumes normal representations cluster near a center (isotropic Gaussian assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Using Deep SVDD latent center as anchor for counterfactual and triplet losses enables an unsupervised mechanism to judge normality of subsequences and to drive counterfactual explanation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9292.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM entry detector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM-based anomalous-entry detector (g)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM that computes per-entry hidden states followed by a logistic regression to predict per-entry anomalous probabilities; outputs are discretized and trained via a reinforcement-learning (policy-gradient) objective to satisfy counterfactual properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM + logistic regression (entry-level detector)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM sequence model with per-step logistic output; trained with policy-gradient (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete sequences of entries (log templates / activity tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs, user activity logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous sequence entries (contiguous or sparse abnormal segments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute hidden state h_l per entry via LSTM(e_l, h_{l-1}), compute p_l = q(h_l) via logistic regression, discretize p_l≥0.5 to a_l ∈ {0,1}; train g to maximize reward (negative of combined losses: normality, triplet, continuity, sparsity) using policy-gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to interpretation-based attributions (Attention, Shapley) and classical detectors (OCSVM, iForest) in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1, AUC on entry-level detection (reported as part of CFDet)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>When integrated in CFDet produced the high entry-level results reported under CFDet (see CFDet entry for full numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Provides higher-fidelity, post-hoc entry explanations compared to attention and Shapley baselines in experiments when operating on detected anomalous sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Discrete output necessitates policy-gradient training which can add variance and training complexity; performance depends on quality of detected anomalous sequences used for training; hyperparameter sensitivity (triplet/continuity/sparsity weights).</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Learning discrete selectors for entries as actions optimized for counterfactual normality/comprehensiveness is effective for fine-grained anomaly localization in sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9292.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM with Attention (interpretation-based baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LSTM sequence model augmented with an attention mechanism; attention weights are interpreted as contribution scores for entries and thresholded to mark anomalous entries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSTM + Attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LSTM with attention mechanism (sequence-to-vector with attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete log-template sequences</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird, CERT)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous entries as high-contribution tokens to anomaly detection</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Train anomalous-sequence detector with attention-weighted aggregation; use attention weights as post-hoc interpretability scores and threshold them (threshold 0.05 used for sequence length 20) to label anomalous entries.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against CFDet, Shapley, OCSVM, iForest</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1, AUC for entry detection</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Entry-detection on U (mean ± std): BGL F1 72.52±16.28, AUC 81.70±10.70; Thunderbird F1 31.44±16.34, AUC 63.22±10.12; CERT F1 46.33±17.52, AUC 67.52±7.27.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Better than OCSVM/iForest in many entry-detection cases but worse than CFDet and Shapley on several datasets; attention scores do not strictly correspond to anomalous outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Attention indicates contribution to next-event prediction or model decision but may not directly correlate with anomalous outcome; thresholding attention can be unstable and dataset-dependent; variable performance with high variance on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9292.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Shapley baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shapley-value-based attribution (Shapley)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A perturbation-based post-hoc attribution method (Shapley values / SHAP) applied to sequence entries: entries with positive Shapley values are marked anomalous because replacing them with entries from normal sequences reduces distance to normal center.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Unified Approach to Interpreting Model Predictions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Shapley values / SHAP applied to sequence entries</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Perturbation-based attribution (game-theoretic Shapley values)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Discrete sequences (log templates)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Entries contributing positively to anomalous-sequence score</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Compute Shapley values for entries in an anomalous sequence relative to the anomalous-sequence detector; entries with positive Shapley values considered responsible for anomaly and labeled anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against CFDet, Attention, OCSVM, iForest</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1, AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Entry-detection on U (mean ± std): BGL F1 97.79±0.77, AUC 98.41±0.83; Thunderbird F1 77.96±11.45, AUC 94.17±4.13; CERT F1 57.64±3.97, AUC 77.52±0.04.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Shapley performs well, especially on BGL where anomalous-entry ratio is high; CFDet slightly outperforms Shapley overall, particularly on Thunderbird and overall stability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Computationally expensive for sequences (needs many perturbations); performance varies with anomaly sparsity and contextual anomalies; requires access to the sequence detector and a perturbation model of replacements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9292.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OCSVM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-Class Support Vector Machine (OCSVM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical one-class anomaly detection algorithm trained on normal samples to estimate the support of the distribution and flag outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Estimating the Support of a High-Dimensional Distribution</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OCSVM</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Kernel-based one-class SVM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Bag-of-words vector features derived from log messages (per-entry features)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird) — not used on CERT due to feature issues</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Outlier entries</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Build bag-of-words vectors for entries and apply OCSVM to detect anomalous entries</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared as a baseline to CFDet and interpretation methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1, AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Entry-detection on U (mean ± std): BGL Precision 11.89±0.00, Recall 14.33±0.00, F1 13.00±0.00, AUC 52.93±0.00; Thunderbird Precision 8.26±0.00, Recall 100.00±0.00, F1 15.27±0.00, AUC 73.63±0.00. (Not applicable on CERT.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Performs poorly for entry-level detection relative to interpretation-based methods and CFDet, likely due to poor per-entry features and inability to capture contextual anomalies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Bag-of-words per-entry features insufficient for contextual/group anomalies; high false positives or low precision in experiments; not applied on CERT due to inability to featurize activity tokens properly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9292.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>iForest</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Isolation Forest (iForest)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised ensemble method that isolates anomalies via random partitioning (binary trees); used here as a baseline on per-entry bag-of-words features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Isolation Forest</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Isolation Forest</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Ensemble of randomized tree-based anomaly detector</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Bag-of-words vectors for log entries</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs (BGL, Thunderbird); not applied to CERT</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Outlier entries</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Apply Isolation Forest to per-entry bag-of-words features to rank/flag anomalous entries</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against CFDet, Attention, Shapley, OCSVM</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision, Recall, F1, AUC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Entry-detection on U (mean ± std): BGL Precision 19.47±0.00, Recall 23.82±0.00, F1 21.43±0.00, AUC 57.98±0.00; Thunderbird Precision 42.23±0.00, Recall 99.99±0.00, F1 59.39±0.00, AUC 96.75±0.00. (Not applicable on CERT.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Better than OCSVM in some recall-heavy cases but still substantially worse than CFDet and interpretation-based baselines in F1 and AUC for entry localization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same featurization limitations as OCSVM; contextual anomalies not captured by per-entry bag-of-words; not suitable for CERT features.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9292.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogBERT: Log anomaly detection via BERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based (BERT) approach for log anomaly detection mentioned in related work as an example of using BERT-like language models for logs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logbert: Log anomaly detection via bert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogBERT (BERT-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (BERT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Log sequences (text), log templates</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous sequences/events in logs</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in related work as an approach that applies BERT-style pretraining/fine-tuning to log data for anomaly detection (paper not used in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only cited in related work; no experimental details or limitations discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Paper cites transformers/BERT variants as viable approaches for log anomaly detection but does not evaluate them; highlights diversity of sequence-modeling approaches in related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9292.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior RNN-based approach for log anomaly detection referenced in related work; trains RNN to predict next log event and flags anomalies when predictions fail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepLog (RNN-based next-event prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>RNN (LSTM) predictive model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Sequences of log events</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>System logs</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Anomalous sequences/events identified via prediction failure</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an example where an RNN is trained on normal sequences to predict next log entries; sequences deviating from predictions are considered anomalous.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned for motivation/background; no direct experimental comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Serves as contextual prior work emphasizing predictive RNN approaches for sequence anomaly detection; CFDet differs by using Deep SVDD and counterfactual entry localization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9292.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9292.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OmniAnomaly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OmniAnomaly (Stochastic Recurrent Neural Network for multivariate time series anomaly detection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational recurrent neural network approach (VAE+GRU) for multivariate time-series anomaly detection with interpretation via reconstruction probability; cited in related work as an interpretable anomaly model for continuous multivariate time series.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OmniAnomaly (VAE + GRU)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational recurrent neural network (GRU + VAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multivariate time series (continuous)</td>
                        </tr>
                        <tr>
                            <td><strong>data_domain</strong></td>
                            <td>Sensor-like / system metrics (time series)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Multivariate time-series anomalies</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as an interpretable anomaly detection approach that uses reconstruction probability per dimension for interpretation; cited to contrast discrete-sequence setting where gradient approaches are hard.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mentioned to highlight differences between continuous multivariate time-series interpretability and discrete sequence interpretability challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>unique_insights</strong></td>
                            <td>Indicates gradient/reconstruction-probability approaches are effective for continuous multivariate series but less applicable for discrete log sequences, motivating counterfactual approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning <em>(Rating: 2)</em></li>
                <li>Logbert: Log anomaly detection via bert <em>(Rating: 2)</em></li>
                <li>Deep One-Class Classification <em>(Rating: 2)</em></li>
                <li>Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network <em>(Rating: 2)</em></li>
                <li>LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9292",
    "paper_id": "paper-c6f7bfaf00fcef358a4774580f42478f712f073a",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [
        {
            "name_short": "CFDet",
            "name_full": "Counterfactual Detection (CFDet)",
            "brief_description": "A two-phase framework that first detects anomalous sequences via Deep SVDD with an LSTM encoder and then identifies fine-grained anomalous entries by training an LSTM-based entry detector using counterfactual explanations, triplet/continuity/sparsity losses and policy-gradient optimization for discrete indicators.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "CFDet (Deep SVDD with LSTM encoder + LSTM anomalous-entry detector)",
            "model_type": "LSTM (sequence encoder) + Deep SVDD (one-class objective); LSTM + logistic regression for entry detector; policy-gradient for discrete outputs",
            "model_size": null,
            "data_type": "Discrete event sequences (sliding-window sequences of log templates or user activity tokens)",
            "data_domain": "System logs (BGL, Thunderbird) and synthetic insider-activity logs (CERT)",
            "anomaly_type": "Anomalous sequences and anomalous entries (including contextual and group anomalies, rare events/errors)",
            "method_description": "Train an LSTM encoder f(·) on normal sequences with Deep SVDD objective to learn a center c; label sequences with large distance to c as anomalous; for each detected anomalous sequence Z train an LSTM-based entry detector g(·) that outputs a binary indicator sequence A such that Z^+ = (1-A)⊙Z is close to c (normality) while Z^- = A⊙Z is far from c (comprehensiveness), with continuity and sparsity regularizers; train g using policy-gradient RL because outputs are discrete.",
            "baseline_methods": "Attention-based interpretation (LSTM+attention), Shapley-value attribution, One-Class SVM (OCSVM), Isolation Forest (iForest)",
            "performance_metrics": "Precision, Recall, F1-score, AUC (ROC)",
            "performance_results": "Anomalous-entry detection on unlabeled U (mean ± std): BGL: Precision 98.91±0.91, Recall 98.56±1.52, F1 98.73±0.82, AUC 99.07±0.74; Thunderbird: Precision 90.99±10.33, Recall 100.00±0.00, F1 94.97±6.41, AUC 99.33±0.97; CERT: Precision 99.31±2.19, Recall 55.31±0.10, F1 71.04±0.50, AUC 77.65±0.04. Anomalous-sequence detection (Deep SVDD) on U: BGL F1 98.41±0.56, Thunderbird F1 97.19±1.96, CERT F1 77.88±0.00.",
            "comparison_to_baseline": "CFDet outperforms baselines (Attention, Shapley, OCSVM, iForest) on F1 and AUC across datasets; Shapley performs well on BGL but overall CFDet achieves highest F1/AUC. Traditional OCSVM/iForest perform poorly on entry-level detection due to feature mismatch and context dependence.",
            "limitations_or_failure_cases": "Dependence on initial anomalous-sequence detector: if Deep SVDD misses anomalous sequences, their entries cannot be identified (explains low recall on CERT); requires tuning of triplet/continuity/sparsity hyperparameters; policy-gradient training for discrete indicators can be more complex/stochastic; precision/recall tradeoffs vary with sequence length; approach designed for discrete sequences (logs/activities) and may need adaptation for continuous/tabular data.",
            "unique_insights": "Formulates fine-grained entry detection as counterfactual explanation: the normal subsequence should move close to the Deep SVDD center while the anomalous subsequence moves away; uses triplet loss with center as anchor plus continuity and sparsity priors to enforce concise contiguous anomalous segments; trains discrete selectors via policy-gradient, enabling post-hoc interpretations in an unsupervised/one-class setting.",
            "uuid": "e9292.0",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Deep SVDD (LSTM encoder)",
            "name_full": "Deep Support Vector Data Description with LSTM encoder",
            "brief_description": "One-class classification approach that minimizes the volume of a hypersphere enclosing normal-data representations; implemented here with an LSTM encoder to map sequences to latent representations and compute distances to a center c for anomaly scoring.",
            "citation_title": "Deep One-Class Classification",
            "mention_or_use": "use",
            "model_name": "Deep SVDD (with LSTM encoder)",
            "model_type": "One-class Deep neural network objective; LSTM sequence encoder",
            "model_size": null,
            "data_type": "Sequential discrete data (log-template sequences, user-activity sequences)",
            "data_domain": "System logs and user activity logs (BGL, Thunderbird, CERT)",
            "anomaly_type": "Anomalous sequences (out-of-distribution/aberrant sequences)",
            "method_description": "Train LSTM encoder f(·) on normal sequences to minimize squared distance of representations to center c (SVDD loss) plus weight regularization; derive anomaly score as squared distance to c; sequences with score &gt; ε labeled anomalous.",
            "baseline_methods": "Compared implicitly against OCSVM/iForest for sequence/entry detection tasks in experiments",
            "performance_metrics": "Precision, Recall, F1-score, AUC for anomalous-sequence detection",
            "performance_results": "Anomalous-sequence detection on U (mean ± std): BGL Precision 98.81±0.39, Recall 98.02±1.14, F1 98.41±0.56, AUC 98.73±0.54; Thunderbird Precision 94.59±3.62, Recall 100.00±0.00, F1 97.19±1.96, AUC 93.75±4.55; CERT Precision 100.00±0.00, Recall 63.77±0.00, F1 77.88±0.00, AUC 81.88±0.00.",
            "comparison_to_baseline": "Deep SVDD (LSTM) provided strong anomalous-sequence detection performance and served as the foundation for entry-level counterfactual detection; better recall/precision than baselines used for entry detection when aggregated into CFDet.",
            "limitations_or_failure_cases": "Missed anomalous sequences reduce downstream entry detection recall (observed on CERT); performance sensitive to sequence-length choices and center update schedule; assumes normal representations cluster near a center (isotropic Gaussian assumption).",
            "unique_insights": "Using Deep SVDD latent center as anchor for counterfactual and triplet losses enables an unsupervised mechanism to judge normality of subsequences and to drive counterfactual explanation learning.",
            "uuid": "e9292.1",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LSTM entry detector",
            "name_full": "LSTM-based anomalous-entry detector (g)",
            "brief_description": "An LSTM that computes per-entry hidden states followed by a logistic regression to predict per-entry anomalous probabilities; outputs are discretized and trained via a reinforcement-learning (policy-gradient) objective to satisfy counterfactual properties.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSTM + logistic regression (entry-level detector)",
            "model_type": "LSTM sequence model with per-step logistic output; trained with policy-gradient (RL)",
            "model_size": null,
            "data_type": "Discrete sequences of entries (log templates / activity tokens)",
            "data_domain": "System logs, user activity logs",
            "anomaly_type": "Anomalous sequence entries (contiguous or sparse abnormal segments)",
            "method_description": "Compute hidden state h_l per entry via LSTM(e_l, h_{l-1}), compute p_l = q(h_l) via logistic regression, discretize p_l≥0.5 to a_l ∈ {0,1}; train g to maximize reward (negative of combined losses: normality, triplet, continuity, sparsity) using policy-gradient.",
            "baseline_methods": "Compared to interpretation-based attributions (Attention, Shapley) and classical detectors (OCSVM, iForest) in experiments",
            "performance_metrics": "Precision, Recall, F1, AUC on entry-level detection (reported as part of CFDet)",
            "performance_results": "When integrated in CFDet produced the high entry-level results reported under CFDet (see CFDet entry for full numbers).",
            "comparison_to_baseline": "Provides higher-fidelity, post-hoc entry explanations compared to attention and Shapley baselines in experiments when operating on detected anomalous sequences.",
            "limitations_or_failure_cases": "Discrete output necessitates policy-gradient training which can add variance and training complexity; performance depends on quality of detected anomalous sequences used for training; hyperparameter sensitivity (triplet/continuity/sparsity weights).",
            "unique_insights": "Learning discrete selectors for entries as actions optimized for counterfactual normality/comprehensiveness is effective for fine-grained anomaly localization in sequences.",
            "uuid": "e9292.2",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Attention baseline",
            "name_full": "LSTM with Attention (interpretation-based baseline)",
            "brief_description": "An LSTM sequence model augmented with an attention mechanism; attention weights are interpreted as contribution scores for entries and thresholded to mark anomalous entries.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LSTM + Attention",
            "model_type": "LSTM with attention mechanism (sequence-to-vector with attention)",
            "model_size": null,
            "data_type": "Discrete log-template sequences",
            "data_domain": "System logs (BGL, Thunderbird, CERT)",
            "anomaly_type": "Anomalous entries as high-contribution tokens to anomaly detection",
            "method_description": "Train anomalous-sequence detector with attention-weighted aggregation; use attention weights as post-hoc interpretability scores and threshold them (threshold 0.05 used for sequence length 20) to label anomalous entries.",
            "baseline_methods": "Compared against CFDet, Shapley, OCSVM, iForest",
            "performance_metrics": "Precision, Recall, F1, AUC for entry detection",
            "performance_results": "Entry-detection on U (mean ± std): BGL F1 72.52±16.28, AUC 81.70±10.70; Thunderbird F1 31.44±16.34, AUC 63.22±10.12; CERT F1 46.33±17.52, AUC 67.52±7.27.",
            "comparison_to_baseline": "Better than OCSVM/iForest in many entry-detection cases but worse than CFDet and Shapley on several datasets; attention scores do not strictly correspond to anomalous outcome.",
            "limitations_or_failure_cases": "Attention indicates contribution to next-event prediction or model decision but may not directly correlate with anomalous outcome; thresholding attention can be unstable and dataset-dependent; variable performance with high variance on some datasets.",
            "uuid": "e9292.3",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Shapley baseline",
            "name_full": "Shapley-value-based attribution (Shapley)",
            "brief_description": "A perturbation-based post-hoc attribution method (Shapley values / SHAP) applied to sequence entries: entries with positive Shapley values are marked anomalous because replacing them with entries from normal sequences reduces distance to normal center.",
            "citation_title": "A Unified Approach to Interpreting Model Predictions",
            "mention_or_use": "use",
            "model_name": "Shapley values / SHAP applied to sequence entries",
            "model_type": "Perturbation-based attribution (game-theoretic Shapley values)",
            "model_size": null,
            "data_type": "Discrete sequences (log templates)",
            "data_domain": "System logs",
            "anomaly_type": "Entries contributing positively to anomalous-sequence score",
            "method_description": "Compute Shapley values for entries in an anomalous sequence relative to the anomalous-sequence detector; entries with positive Shapley values considered responsible for anomaly and labeled anomalous.",
            "baseline_methods": "Compared against CFDet, Attention, OCSVM, iForest",
            "performance_metrics": "Precision, Recall, F1, AUC",
            "performance_results": "Entry-detection on U (mean ± std): BGL F1 97.79±0.77, AUC 98.41±0.83; Thunderbird F1 77.96±11.45, AUC 94.17±4.13; CERT F1 57.64±3.97, AUC 77.52±0.04.",
            "comparison_to_baseline": "Shapley performs well, especially on BGL where anomalous-entry ratio is high; CFDet slightly outperforms Shapley overall, particularly on Thunderbird and overall stability.",
            "limitations_or_failure_cases": "Computationally expensive for sequences (needs many perturbations); performance varies with anomaly sparsity and contextual anomalies; requires access to the sequence detector and a perturbation model of replacements.",
            "uuid": "e9292.4",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "OCSVM",
            "name_full": "One-Class Support Vector Machine (OCSVM)",
            "brief_description": "A classical one-class anomaly detection algorithm trained on normal samples to estimate the support of the distribution and flag outliers.",
            "citation_title": "Estimating the Support of a High-Dimensional Distribution",
            "mention_or_use": "use",
            "model_name": "OCSVM",
            "model_type": "Kernel-based one-class SVM",
            "model_size": null,
            "data_type": "Bag-of-words vector features derived from log messages (per-entry features)",
            "data_domain": "System logs (BGL, Thunderbird) — not used on CERT due to feature issues",
            "anomaly_type": "Outlier entries",
            "method_description": "Build bag-of-words vectors for entries and apply OCSVM to detect anomalous entries",
            "baseline_methods": "Compared as a baseline to CFDet and interpretation methods",
            "performance_metrics": "Precision, Recall, F1, AUC",
            "performance_results": "Entry-detection on U (mean ± std): BGL Precision 11.89±0.00, Recall 14.33±0.00, F1 13.00±0.00, AUC 52.93±0.00; Thunderbird Precision 8.26±0.00, Recall 100.00±0.00, F1 15.27±0.00, AUC 73.63±0.00. (Not applicable on CERT.)",
            "comparison_to_baseline": "Performs poorly for entry-level detection relative to interpretation-based methods and CFDet, likely due to poor per-entry features and inability to capture contextual anomalies.",
            "limitations_or_failure_cases": "Bag-of-words per-entry features insufficient for contextual/group anomalies; high false positives or low precision in experiments; not applied on CERT due to inability to featurize activity tokens properly.",
            "uuid": "e9292.5",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "iForest",
            "name_full": "Isolation Forest (iForest)",
            "brief_description": "An unsupervised ensemble method that isolates anomalies via random partitioning (binary trees); used here as a baseline on per-entry bag-of-words features.",
            "citation_title": "Isolation Forest",
            "mention_or_use": "use",
            "model_name": "Isolation Forest",
            "model_type": "Ensemble of randomized tree-based anomaly detector",
            "model_size": null,
            "data_type": "Bag-of-words vectors for log entries",
            "data_domain": "System logs (BGL, Thunderbird); not applied to CERT",
            "anomaly_type": "Outlier entries",
            "method_description": "Apply Isolation Forest to per-entry bag-of-words features to rank/flag anomalous entries",
            "baseline_methods": "Compared against CFDet, Attention, Shapley, OCSVM",
            "performance_metrics": "Precision, Recall, F1, AUC",
            "performance_results": "Entry-detection on U (mean ± std): BGL Precision 19.47±0.00, Recall 23.82±0.00, F1 21.43±0.00, AUC 57.98±0.00; Thunderbird Precision 42.23±0.00, Recall 99.99±0.00, F1 59.39±0.00, AUC 96.75±0.00. (Not applicable on CERT.)",
            "comparison_to_baseline": "Better than OCSVM in some recall-heavy cases but still substantially worse than CFDet and interpretation-based baselines in F1 and AUC for entry localization.",
            "limitations_or_failure_cases": "Same featurization limitations as OCSVM; contextual anomalies not captured by per-entry bag-of-words; not suitable for CERT features.",
            "uuid": "e9292.6",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "LogBERT",
            "name_full": "LogBERT: Log anomaly detection via BERT",
            "brief_description": "A transformer-based (BERT) approach for log anomaly detection mentioned in related work as an example of using BERT-like language models for logs.",
            "citation_title": "Logbert: Log anomaly detection via bert",
            "mention_or_use": "mention",
            "model_name": "LogBERT (BERT-based)",
            "model_type": "Transformer (BERT-style)",
            "model_size": null,
            "data_type": "Log sequences (text), log templates",
            "data_domain": "System logs",
            "anomaly_type": "Anomalous sequences/events in logs",
            "method_description": "Mentioned in related work as an approach that applies BERT-style pretraining/fine-tuning to log data for anomaly detection (paper not used in experiments here).",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": "Only cited in related work; no experimental details or limitations discussed in this paper.",
            "unique_insights": "Paper cites transformers/BERT variants as viable approaches for log anomaly detection but does not evaluate them; highlights diversity of sequence-modeling approaches in related literature.",
            "uuid": "e9292.7",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "DeepLog",
            "name_full": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "brief_description": "A prior RNN-based approach for log anomaly detection referenced in related work; trains RNN to predict next log event and flags anomalies when predictions fail.",
            "citation_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "mention_or_use": "mention",
            "model_name": "DeepLog (RNN-based next-event prediction)",
            "model_type": "RNN (LSTM) predictive model",
            "model_size": null,
            "data_type": "Sequences of log events",
            "data_domain": "System logs",
            "anomaly_type": "Anomalous sequences/events identified via prediction failure",
            "method_description": "Referenced as an example where an RNN is trained on normal sequences to predict next log entries; sequences deviating from predictions are considered anomalous.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": "Mentioned for motivation/background; no direct experimental comparison in this paper.",
            "unique_insights": "Serves as contextual prior work emphasizing predictive RNN approaches for sequence anomaly detection; CFDet differs by using Deep SVDD and counterfactual entry localization.",
            "uuid": "e9292.8",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "OmniAnomaly",
            "name_full": "OmniAnomaly (Stochastic Recurrent Neural Network for multivariate time series anomaly detection)",
            "brief_description": "A variational recurrent neural network approach (VAE+GRU) for multivariate time-series anomaly detection with interpretation via reconstruction probability; cited in related work as an interpretable anomaly model for continuous multivariate time series.",
            "citation_title": "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network",
            "mention_or_use": "mention",
            "model_name": "OmniAnomaly (VAE + GRU)",
            "model_type": "Variational recurrent neural network (GRU + VAE)",
            "model_size": null,
            "data_type": "Multivariate time series (continuous)",
            "data_domain": "Sensor-like / system metrics (time series)",
            "anomaly_type": "Multivariate time-series anomalies",
            "method_description": "Referenced as an interpretable anomaly detection approach that uses reconstruction probability per dimension for interpretation; cited to contrast discrete-sequence setting where gradient approaches are hard.",
            "baseline_methods": null,
            "performance_metrics": null,
            "performance_results": null,
            "comparison_to_baseline": null,
            "limitations_or_failure_cases": "Mentioned to highlight differences between continuous multivariate time-series interpretability and discrete sequence interpretability challenges.",
            "unique_insights": "Indicates gradient/reconstruction-probability approaches are effective for continuous multivariate series but less applicable for discrete log sequences, motivating counterfactual approach.",
            "uuid": "e9292.9",
            "source_info": {
                "paper_title": "Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
            "rating": 2,
            "sanitized_title": "deeplog_anomaly_detection_and_diagnosis_from_system_logs_through_deep_learning"
        },
        {
            "paper_title": "Logbert: Log anomaly detection via bert",
            "rating": 2,
            "sanitized_title": "logbert_log_anomaly_detection_via_bert"
        },
        {
            "paper_title": "Deep One-Class Classification",
            "rating": 2,
            "sanitized_title": "deep_oneclass_classification"
        },
        {
            "paper_title": "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network",
            "rating": 2,
            "sanitized_title": "robust_anomaly_detection_for_multivariate_time_series_through_stochastic_recurrent_neural_network"
        },
        {
            "paper_title": "LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs",
            "rating": 1,
            "sanitized_title": "loganomaly_unsupervised_detection_of_sequential_and_quantitative_anomalies_in_unstructured_logs"
        }
    ],
    "cost": 0.017818749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fine-grained Anomaly Detection in Sequential Data via Counterfactual Explanations</h1>
<p>He Cheng<em>, Depeng Xu ${ }^{\dagger}$, Shuhan Yuan</em>, Xintao Wu ${ }^{\ddagger}$<br>* Utah State University, Logan, USA, ${ }^{\dagger}$ University of North Carolina at Charlotte, Charlotte, USA, ${ }^{\ddagger}$ University of Arkansas, Fayetteville, AR<br>Email: {he.cheng,shuhan.yuan}@usu.edu, depeng.xu@uncc.edu, xintaowu@uark.edu</p>
<h4>Abstract</h4>
<p>Anomaly detection in sequential data has been studied for a long time because of its potential in various applications, such as detecting abnormal system behaviors from log data. Although many approaches can achieve good performance on anomalous sequence detection, how to identify the anomalous entries in sequences is still challenging due to a lack of information at the entry-level. In this work, we propose a novel framework called CFDet for fine-grained anomalous entry detection. CFDet leverages the idea of interpretable machine learning. Given a sequence that is detected as anomalous, we can consider anomalous entry detection as an interpretable machine learning task because identifying anomalous entries in the sequence is to provide an interpretation to the detection result. We make use of the deep support vector data description (Deep SVDD) approach to detect anomalous sequences and propose a novel counterfactual interpretation-based approach to identify anomalous entries in the sequences. Experimental results on three datasets show that CFDet can correctly detect anomalous entries.</p>
<p>Index Terms-anomaly detection, counterfactual explanations, sequential data</p>
<h2>I. INTRODUCTION</h2>
<p>Anomalous sequence detection has received a lot of attention recently because of wide applications, such as detecting anomalous log sequences or user activity sequences [1, 2, 3, 4, $5,6,7]$. For example, log messages generated by computing systems are critical resources for debugging the abnormal patterns of systems or detecting novel attacks. Identifying the anomalous log sequences generated by computing systems in a timely manner is important to build stable systems [1, 2, 3, 7].</p>
<p>However, the current approaches usually focus on anomalous sequence detection while no much work targets the finegrained anomalous entry detection. In practice, given detected anomalous sequences, it is critical for the domain users to understand why the detection model made such predictions. Identifying the anomalous entries in the sequence can help the domain users locate the exact issues. For example, if a user activity sequence is labeled as anomalous, it is important to know which activities in the sequences are anomalous that lead to an anomalous outcome. On the other hand, due to the limited information at the entry-level, it is not straightforward to detect anomalous entries from the sequences. Especially, for the anomalies like group anomalies (consisting of a collection of two or more anomalous points) or contextual anomalies (being abnormal because of their contexts), the traditional point
anomaly detection approaches are not suitable for identifying the anomalous entries in sequences.</p>
<p>In this work, we propose to leverage the idea of interpretable machine learning to achieve fine-grained anomalous entry detection. This is because identifying anomalous entries from a detected anomalous sequence is to explain why the given sequence is labeled as anomalous. Specially, we propose a counterfactual explanation approach, called CFDet, to detect anomalous entries via searching for counterfactual samples. The idea of counterfactual explanation is to identify the "smallest change" to anomalous sequences that could change the prediction to normal. Here, the changes indicate anomalous entries, because by removing the anomalous entries, we can "change" a sequence from anomalous to normal. Then, we can achieve fine-grained anomalous entry detection.</p>
<p>One challenge of leveraging interpretable machine learning techniques is that current interpretable models are usually developed for supervised learning models [8]. For the counterfactual explanation approach, a classifier is usually trained to distinguish the original samples from the counterfactual samples. However, in the anomaly detection scenario, due to limited anomalous samples, it is hard to train a supervised classification model, and most anomaly detection approaches are unsupervised or one-class classification models [1, 9, 10]. To tackle this challenge, we propose a framework only using the normal samples, where the counterfactuals are generated based on the distances to normal samples. In particular, we can divide an anomalous sequence into two parts, a subsequence with normal entries and a subsequence with anomalous entries. We consider the subsequence with normal entries as the counterfactual sample of the original anomalous sequence by imagining that the anomalous entries had not occurred. Inspired by the deep support vector data description (Deep SVDD) [9], where the basic assumption is that the normal samples enclose to the center of a hypersphere, we aim at identifying the subsequence with anomalous entries having a large distance to the center while the counterfactual is close to the center.</p>
<p>The main contributions of this paper are as follows. First, we propose CFDet, a novel anomalous sequence and fine-grained entry detection framework only using normal sequences for training. Considering the normal sequences are usually easy to obtain, our framework meets the requirements of realworld scenarios. Second, we develop a novel anomalous</p>
<p>entry detection approach based on the idea of counterfactual explanations, which considers anomalous entry detection as a task of providing interpretations to the detected anomalous sequences. Third, the experimental results on three datasets show that CFDet can detect anomalous sequences as well as fine-grained anomalous entries with high accuracy.</p>
<h2>II. Related Work</h2>
<p>Anomaly Detection in Sequential Data. As sequential data become more and more ubiquitous, such as time series, video frames, or event data, sequential anomaly detection plays an important role in a wide spectrum of application scenarios [11, 12, 13, 14]. Currently, due to a limited number of anomalies, many unsupervised or one-class deep learning approaches are proposed to detect anomalous sequences by identifying the differences between normal and anomalous patterns [1, 2, 3, 15]. A typical idea is to make use of recurrent neural networks (RNNs) to capture the normal patterns from normal sequences. Then, an anomalous sequence can be detected with deviate patterns [1, 3]. For example, DeepLog [1] is trained to predict the log entry by an RNN model based on a large number of normal sequences so that RNN is able to capture the normal patterns of sequences. The anomalous sequence can then be detected when the RNN cannot correctly predict the log entries, meaning the sequence does not follow the normal patterns. However, the majority of approaches proposed so far only focus on detecting the anomalous sequences and cannot point out fine-grained subsequences or entries in the sequences that actually lead to the anomalous outcomes.
Interpretable Anomaly Detection. Interpretability in machine learning is crucial for high-stakes decisions and troubleshooting [8]. Interpretable machine learning techniques can be categorized into two types, intrinsic interpretability and post-hoc interpretability [16]. Intrinsic interpretability indicates self-explanatory models that achieve interpretability directly based on their structures, while post-hoc interpretability means the interpretability is achieved by applying another model to provide explanations. There are two typical approaches to achieve the post-hoc interpretability, perturbationbased and gradient-based approaches. The perturbation-based approaches find the important features based on their impact on the decision outcome by perturbation functions, such as LIME and SHAP [17, 18], while the gradient-based approaches identify the important features based on the gradient magnitudes, such as Grad-CAM and Integrated Gradients [19, 20]. Many sequential anomaly detection models are deployed on safety-critical systems. Hence, once anomalous behaviors are detected, understanding them is imperative for the domain users to locate the problems.</p>
<p>Only a few studies target interpretable anomaly detection [21, 22]. To achieve intrinsic interpretation, the explainable deep one-class classification model [22] provides intrinsic interpretability for anomaly detection on image data but cannot identify the discrete anomalous entries in sequences. Meanwhile, the attention mechanism, which also provide intrinsic interpretation based on the attention weights, is also adopted fdetecting anomalous events from sequential data [21]. However, the attention scores derived in the proposed approach indicate the contributions to predicting the next event in the sequence and are not strictly related to the anomalous outcome. Some studies also achieve the post-hoc interpretation based on the perturbation-based or gradientbased interpretation approaches. Research in [23] develops interpretable autoencoder models to identify features leading to high reconstruction errors using Shapley values. Similarly, research in [24] adopts variational autoencoder as the anomaly detection model and identifies important features based on the gradient values. OmniAnomaly achieves the interpretable anomaly detection for multivariate time series data by a neural network combining GRU and VAE, where the interpretation is achieved based on the reconstruction probability of each dimension of input data [25]. In this work, we target on detecting anomalies in discrete sequence data, it is hard to leverage the gradient-based approaches to achieve interpretation. We leverage the idea of counterfactual interpretation to achieve fine-grained anomaly detection, which provides human-understandable post-hoc interpretations to anomalous sequence detection.</p>
<h2>III. FRAMEWORK</h2>
<h2>A. Overview</h2>
<p>We denote a sequence with length $L$ as $S=\left{e_{l}\right}<em l="l">{l=1}^{L}$ where $e</em>\right}}$ indicates the $l$-th entry. We use $e^{+}$and $e^{-}$ to denote normal and anomalous entries respectively. In this work, we assume that no labeled anomalous sequences/entries are available as training signals. Formally, given a set of normal sequences $\mathcal{P}=\left{S_{n}^{+<em m="m">{n=1}^{N}$ and another set of unlabeled sequences $\mathcal{U}=\left{S</em>$ as well as their corresponding anomalous entries.}\right}_{m=1}^{M}$ with a mixture of normal and anomalous sequences, i.e., $\mathcal{U}=\mathcal{U}^{+}\cup \mathcal{U}^{-}$, we aim at detecting the anomalous sequences in $\mathcal{U</p>
<p>We propose a two-phase framework called CFDet, as shown in Figure 1. We first adopt the Deep SVDD approach to derive an anomalous sequence detector $f(\cdot)$ based on the normal sequence set $\mathcal{P}$. Specifically, Deep SVDD is to minimize the volume of a data-enclosing hypersphere in a latent space with a center point $\mathbf{c}$ based on $\mathcal{P}$. Therefore, the anomalies can be detected with a large distance to the center. Then, we deploy the detector $f(\cdot)$ to classify the sequences in the unlabeled set $\mathcal{U}$ into a subset of anomalous sequences $\tilde{\mathcal{U}}^{-}$and a subset of normal sequences $\tilde{\mathcal{U}}^{+}$. For each detected anomalous sequence set $Z \in \tilde{\mathcal{U}}^{-}$, we further identify the fine-grained anomalous entries. We propose a novel self-supervised learning approach based on the idea of counterfactual explanation to train an anomalous entry detector $g(\cdot)$. Then, we are able to identify the anomalous entries from the anomalous sequences.
The Key Idea of Anomalous Entry Detection. Given a detected anomalous sequence $Z$, we denote the subsequence consisting of only anomalous entries in $Z$ as $Z^{-}$. Formally we have</p>
<p>$$
Z^{-}=A \odot Z
$$</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Illustration of CFDet for anomalous sequence and entry detection.</p>
<p>where $A = {a_l}<em l_-="l^-">{l=1}^L$ is an indicator sequence with each $a_l \in {0, 1}$ being a binary indicator, and $\odot$ indicates the element-wise product. If the entry is an anomalous one $e</em>$, the corresponding indicator $a_l = 1$; otherwise, $a_l = 0$. Our goal is to learn an anomalous entry detector $g(\cdot) : Z \rightarrow A$ that can properly generate the indicator sequence $A$.</p>
<p>To detect the fine-grained anomalous entries from sequences, we leverage the idea of counterfactual explanations. Counterfactual explanations describe a situation in the form: "If X had not occurred, Y would not have occurred" [26]. In the anomaly detection scenario, we can rephrase the above statement as: if there had been no anomalous entries in a sequence, the sequence would not be anomalous. For the anomalous sequence $Z$, we also denote its subsequence with normal entries as $Z^+$, i.e., $Z^+ = (1 - A) \odot Z$. Following the notion of counterfactual explanation, we aim at identifying and removing all the anomalous entries from the sequence $Z$. Once the anomalous entries are removed from the sequence, the corresponding counterfactual sequence should be normal and is actually the normal subsequence $Z^+$.</p>
<p><strong>Properties of Counterfactual Sequences.</strong> Because given a sequence $Z$, $Z^+$ is the complement of $Z^-$, generating a good counterfactual sequence also means achieving anomalous entry detection. An ideal counterfactual sequence of an anomalous sequence should satisfy the following properties.</p>
<ul>
<li><strong>Normality:</strong> The counterfactual sequence $Z^+$ should be normal. As illustrated in [10], by assuming that the latent representations derived by $f(\cdot)$ follow an isotropic Gaussian distribution, Deep SVDD is to minimize the upper bound on the entropy of the Gaussian. The hidden representations of counterfactual sequences should have minimal entropy:</li>
</ul>
<p>$$
\mathcal{H}(f(Z^+)) \leq \tau, \tag{2}
$$</p>
<p>where $\mathcal{H}(\cdot)$ indicates the entropy and $\tau$ is a constant.</p>
<ul>
<li><strong>Comprehensiveness:</strong> After removing $Z^+$ from $Z$, $Z^-$ should be abnormal:</li>
</ul>
<p>$$
\mathcal{H}(f(Z^-)) \geq \mathcal{H}(f(Z^+)) + t, \tag{3}
$$</p>
<p>where $t$ is a constant margin. It requires $Z^-$ as anomalous entries in a sequence $Z$ should have a higher entropy with a margin compared with the counterfactual sequence. Therefore, we can ensure that $Z^+$ includes all the normal entries while $Z^-$ has all the anomalous entries.</p>
<ul>
<li><strong>Conciseness:</strong> The counterfactual sample means the minimum changes on the original sequence, so the anomalous entries $Z^-$ should be consecutive and sparse,</li>
</ul>
<p>$$
\sum_l |a_l - a_{l-1}| \leq c \quad \sum_l a_l \leq s, \tag{4}
$$</p>
<p>where $c$ and $s$ are both constants.</p>
<p>Both $Z^+$ and $Z^-$ can be derived based on the indicator sequence $A$ that is generated by the anomalous entry detector $g(\cdot)$. Therefore, given an anomalous sequence $Z$, the objective of training $g(\cdot)$ is to meet the above three properties of counterfactual sequences.</p>
<h3>B. Anomalous Sequence Detection</h3>
<p>Given a set of normal sequences $\mathcal{P} = {S_n^+ }_{n=1}^N$, we derive an anomaly detection model based on the idea of Deep SVDD [9]. First, given a normal sequence $S_n^+$, we adopt a long short-term memory (LSTM) neural network to encode the sequence into an embedding space and derive its representation as $\mathbf{r}_n^+ = f(S_n^+)$, where $f(\cdot)$ indicates the LSTM model and $\mathbf{r}_n^+$ is the last hidden state of LSTM. In order to train the LSTM model, Deep SVDD minimizes the volume of a hypersphere that encloses normal data. In other words, Deep SVDD aims at making the normal data close to the center of the hypersphere. We derive the center $\mathbf{c}$ of normal sequences in $\mathcal{P}$ by a mean operation, i.e., $\mathbf{c} = \text{Mean}(\mathbf{r}_n^+)$. Formally, the objective function of Deep SVDD is defined as:</p>
<p>$$
\mathcal{L}<em n="1">{SVDD} = \frac{1}{N} \sum</em>}^N |f(\mathbf{r<em 2="2">n^+; \Theta) - \mathbf{c} |</em>
$$}^2 + \lambda |\Theta|_{F}^2, \tag{5</p>
<p>where $\Theta$ denotes the parameters in the LSTM model. Deep SVDD employs a quadratic loss for penalizing distances of normal data representations to the center $\mathbf{c}$. Therefore, it can jointly learn the LSTM model together with minimizing the volume of a data-enclosing hypersphere in the latent space. To ensure the normal sequences close to the center, the LSTM model is to extract the common factors of normal sequences.</p>
<p>After training on the normal sequences $\mathcal{P}$, the LSTM model is able to map the normal sequences close to the center of the hypersphere and also make the anomalous sequences with deviate patterns far from the center.</p>
<p>Therefore, we can deploy the LSTM model $f(\cdot)$ to detect the anomalous sequences in the unlabeled set $\mathcal{U}$. Given a sequence $S_{m} \in \mathcal{U}$ and its representation $\mathbf{r}<em m="m">{m}=f\left(S</em>\right)$, we can derive the anomaly score as the distance of a sequence to the center of the hypersphere:</p>
<p>$$
s\left(\mathbf{r}<em m="m">{m}\right)=\left|f\left(\mathbf{r}</em>
$$}\right)-\mathbf{c}\right|_{2}^{2</p>
<p>If the representation of the sequence $\mathbf{r}<em m="m">{m}$ falls outside the hypersphere, i.e., $s\left(\mathbf{r}</em>$.}\right)&gt;\epsilon$, where $\epsilon$ indicates the threshold, we will label the sequence $S_{m}$ as anomalous. Then, we can obtain a set of sequence $\tilde{\mathcal{U}}^{-}$with detected anomalous sequences from $\mathcal{U</p>
<h2>C. Anomalous Entry Detection</h2>
<p>After obtaining a set of detected anomalous sequences $\tilde{\mathcal{U}}^{-}$, we further aim at detecting the anomalous entries in the sequences. In this work, we propose to achieve anomalous entry detection via counterfactual explanations. In the anomaly detection scenario, the counterfactual example of an anomalous sequence is its subsequence with normal entries, so the counterfactual example and the subsequence with anomalous entries are complementary to each other. In other words, following the idea of counterfactual explanations, we can generate the counterfactual of an anomalous sequence by detecting the anomalous entries.</p>
<p>Given an anomalous sequence $Z \in \tilde{\mathcal{U}}^{-}$, in order to generate the counterfactual with normal entries $Z^{+}$as well as the subsequence with anomalous entries $Z^{-}$, we propose to train the anomalous entry detector $g(\cdot)$ to generate the indicator sequence $A$, i.e., $A=g(Z)$. We use another LSTM model as a part of the implementation of $g(\cdot)$. Based on the LSTM model, we can derive the hidden state $\mathbf{h}<em l="l">{l}$ for each entry $e</em>} \in Z$. Then, we apply a logistic regression model $q(\cdot)$ on $\mathbf{h<em l="l">{l}$ to predict the probability $p</em>$ as anomalous:}$ of the entry $e_{l</p>
<p>$$
\mathbf{h}<em l="l">{l}=L S T M\left(\mathbf{e}</em>}, \mathbf{h<em l="l">{l-1}\right) \quad p</em>\right)
$$}=q\left(\mathbf{h}_{l</p>
<p>where $\mathbf{e}<em l="l">{l}$ denotes the representation of entry $e</em>$ :}$. After rounding the probability $p_{l}$ to the 0-1 binary value, we get the indicator $a_{l}$ for the entry $e_{l</p>
<p>$$
a_{l}= \begin{cases}1, &amp; \text { if } p_{l} \geq 0.5 \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Equations 7 and 8 are the implementation of the anomalous entry detector $g(\cdot)$.</p>
<p>In order to train $g(\cdot)$ to accurately generate $A$ so that the counterfactual sequences meet the properties of normality, comprehensiveness, and conciseness, we train $g(\cdot)$ on the detected anomalous sequence set $\tilde{\mathcal{U}}^{-}$by the following objective function:</p>
<p>$$
\mathcal{L}=\mathcal{L}<em t="t">{n}+\alpha \mathcal{L}</em>}+\beta \mathcal{L<em s="s">{c}+\gamma \mathcal{L}</em>
$$</p>
<p>where $\mathcal{L}<em t="t">{n}$ is a Deep SVDD-based loss to ensure the normality of generated counterfactual sequences; $\mathcal{L}</em>}$ indicates the triplet loss that is to ensure the comprehensiveness; $\mathcal{L<em s="s">{c}$ indicates the continuity loss that is to ensure the continuity; $\mathcal{L}</em>$ indicates the sparsity loss that is to ensure the generated counterfactual sample with the minimum change; $\alpha, \beta$, and $\gamma$ are hyperparameters to balance the weight of each loss term.</p>
<p>Normality loss $\mathcal{L}<em z_="z^{+">{n}$. In order to meet the property of normality about the counterfactual sequence defined in Equation 2, given a detected anomalous sequence, the objective is to make the representation of counterfactual sequence $Z^{+}$close to the center $\mathbf{c}$ (the center of the hypersphere with normal sequences). Therefore, we first derive the representations of counterfactual sequence $Z^{+}$based on the LSTM model $f(\cdot)$ as $\mathbf{r}</em>\right)$. Then, the loss is defined as}}=f\left(Z^{+</p>
<p>$$
\mathcal{L}<em z_="z^{+">{n}=\left|\mathbf{r}</em>
$$}}-\mathbf{c}\right|_{2}^{2</p>
<p>Triplet loss $\mathcal{L}<em z_-="z^{-">{t}$. To meet comprehensiveness defined in Equation 3, we also need to make sure $Z^{+}$include all normal entries so that $Z^{-}$only consists of anomalous entries. The idea is to make the representation of the subsequence with anomalous entries $Z^{-}$far from the center $\mathbf{c}$ while the representation of the counterfactual sequence $Z^{+}$close to the center. To this end, we also derive the representation of anomalous entries $Z^{-}$by $f(\cdot)$, i.e., $\mathbf{r}</em>}}=f\left(Z^{-}\right)$. Then, we consider the center $\mathbf{c}$ as an anchor, the representation of counterfactual $\mathbf{r<em z_-="z^{-">{z^{+}}$as a positive sample and the representation of the anomalous subsequence $\mathbf{r}</em>$as a negative sample. The triplet loss is adopted to stipulate the comprehensiveness property:}</p>
<p>$$
\mathcal{L}<em z_="z^{+">{t}=\max \left{\left|\mathbf{c}-\mathbf{r}</em>\right|}<em z_-="z^{-">{2}^{2}-\left|\mathbf{c}-\mathbf{r}</em>\right|}<em t="t">{2}^{2}+\lambda</em>, 0\right}
$$</p>
<p>where $\lambda_{t}$ is a margin between positive and negative pairs. Intuitively, if the distance between $\mathbf{c}$ and $\mathbf{r}<em z_="z^{+">{z^{-}}$is larger than the distance between $\mathbf{c}$ and $\mathbf{r}</em>$should only have anomalous entries.}}$with a margin, $\mathbf{r}_{z^{-}</p>
<p>Continuity loss $\mathcal{L}_{c}$. Meanwhile, the abnormal entries in a sequence are usually coherent. For example, if a system is under attack, the abnormal log entries are often consecutive. Hence, to ensure the generated indicator sequence $A$ with consecutive selection on the abnormal entries, inspired by [27], we also incorporate the continuity loss:</p>
<p>$$
\mathcal{L}<em l="l">{c}=\max \left{\sum</em>, 0\right}
$$}\left|a_{l}-a_{l-1}\right|-\lambda_{c</p>
<p>where $\lambda_{c}$ is a hyperparameter that controls the continuity of the indicator sequence. Minimizing the continuity loss defined in Equation 12 ensures the indicator sequence $A$ with minimum number of small pieces controlled by $\lambda_{c}$.</p>
<p>Sparsity loss $\mathcal{L}_{s}$. The counterfactual explanation usually expects the "minimum" change on the original sample. In our scenario, we expect that removing the detected anomalous entries is just enough to change the anomalous sequence to a normal one. We do not want to remove the normal entries which could lead to false positive detection. Moreover,</p>
<p>in most scenarios, the anomalous entries should be sparse compared with the normal entries in a sequence. Hence, we also incorporate the sparsity loss in the loss function:</p>
<p>$\mathcal{L}<em l="l">{s}=\max{\sum</em>, 0}$ (13)}a_{l}-\lambda_{s</p>
<p>where $\lambda_{s}$ is a hyperparameter that indicates the expectation of anomalous entries in the sequence. Minimizing the sparsity loss is to make the number of detected anomalous entries close to a pre-set value.</p>
<p>Training. Because the indicator sequence $A$ is a sequence with binary values, the regular gradient descent algorithm cannot be used to optimize the anomalous entry detector $g(\cdot)$. Here, we use the policy gradient algorithm used in reinforcement learning [28] to train CFDet. To this end, we can consider the negative loss of the objective function in Equation 9 as the reward function of a reinforcement learning model, and $g(\cdot)$ as an agent which takes an action ${0,1}$ on an entry $e_{l}$ based on the current state (i.e., the hidden representation of an entry $\mathbf{h}_{l}$). The anomalous entry detector is then trained to maximize the reward function.</p>
<p>Algorithm 1 shows the pseudocode of CFDet to achieve anomalous entry detection on an unlabeled dataset by only using the normal dataset $\mathcal{P}$. CFDet consists of two training phases. First, we train anomalous sequence detector $f(\cdot)$ based on the idea of Deep SVDD on a normal data set $\mathcal{P}$ (lines 1-2). After training, we deploy $f(\cdot)$ on the unlabeled dataset $\mathcal{U}$ to compose the detected anomalous sequence dataset $\tilde{\mathcal{U}}^{-}$ (line 3). Then, we train the anomalous entry detector $g(\cdot)$ on $\tilde{\mathcal{U}}^{-}$ (lines 4-5). Finally, we can deploy $g(\cdot)$ to detect the anomalous entries for sequences in $\tilde{\mathcal{U}}^{-}$ (line 6). It is worth noting that for the sequences detected as normal based on $f(\cdot)$, we assume all the entries are normal.</p>
<p>1
2
3
Algorithm 1 Anomalous Sequence and Entry Detection
Inputs : Normal Dataset $\mathcal{P}$ and Unlabeled Dataset $\mathcal{U}$
Outputs: Detected anomalous sequence and entries in $\mathcal{U}$
4 for $k=1$ $\rightarrow$ $K_{1}$ do // epoch $k$
5 Train anomalous sequence detector $f(\cdot)$ on $\mathcal{P}={S_{n}^{+}}<em 2="2">{n=1}^{N}$ via the objective function Eq. 5
6 Deploy $f(\cdot)$ to detect the anomalous sequences in $\mathcal{U}$ based on Eq. 6 and get $\tilde{\mathcal{U}}^{-}$
7
8
9for $k=1$ $\rightarrow$ $K</em>$ do // epoch $k$
10 Train anomalous entry detector $g(\cdot)$ on $\tilde{\mathcal{U}}^{-}$ via the objective function Eq. 9
11 Deploy $g(\cdot)$ to detect anomalous entries for sequences in $\tilde{\mathcal{U}}^{-}$
12
13 return detected anomalous sequences and entries in $\mathcal{U}$
14</p>
<h2>IV. EXPERIMENTS</h2>
<h3>A. Experimental Setting</h3>
<p>Datasets. We evaluate our model on the following three datasets, which all provide entry-level labels:</p>
<p>TABLE I: Statistics of Three Datasets</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Normal Dataset $\mathcal{P}$ (seq)</th>
<th>Unlabeled Dataset $\mathcal{U}$</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Normal (seq)</td>
<td>Anomalous (seq/entry)</td>
</tr>
<tr>
<td>BGL</td>
<td>344576</td>
<td>77548</td>
<td>36470/627373</td>
</tr>
<tr>
<td>Thunderbird</td>
<td>280064</td>
<td>63126</td>
<td>134365/408202</td>
</tr>
<tr>
<td>CERT</td>
<td>1391104</td>
<td>572560</td>
<td>52033/121751</td>
</tr>
</tbody>
</table>
<ul>
<li>BlueGene/L (BGL) [29] contains log messages collected from a BlueGeme/L supercomputer system at Lawrence Livermore National Labs. The log messages can be categorized into alert and not-alert messages.</li>
<li>Thunderbird [29] is a large-scale system log dataset that is collected from a Thunderbird supercomputer system at Sandia National Labs.</li>
<li>CERT Insider Threat Dataset (CERT) [30] is a synthetic dataset consisting of log files that record the computer-based activities for all employees in an institution, such as logon, logoff, email, http visit. The CERT dataset contains 3995 benign employees and 5 insiders. On average, the number of activities for each employee is around 40000. We use version 4.2 of the CERT dataset.</li>
</ul>
<p>For BGL and Thunderbird, we apply the log parser, Drain developed in [31], to transfer the raw unstructured log messages to log templates and represent the log sequences as the template sequences. For CERT, we use the user activities to compose the sequences. For all three datasets, we adopt a sliding window with size 20 to split the log files into sequences and set the step size as 10.</p>
<p>Table I shows the statistics of the normal dataset $\mathcal{P}$ and unlabeled datasets $\mathcal{U}$, where the last column indicates the numbers of anomalous sequences as well as the anomalous entries in the unlabeled datasets $\mathcal{U}$. For BGL, Thunderbird, and CERT, the ratios of anomalous entries in anomalous sequences are 0.86, 0.15, and 0.12, respectively. It is worth noting that the ground-truth about the anomalous sequences and entries in the unlabeled dataset $\mathcal{U}$ is not available during the training phase. We also build a small validation set for each dataset to tune the hyper-parameters in CFDet as well as baselines for the anomalous sequence and entry detection, where the normal/anomalous sequences on BGL, Thunderbird, and CERT are 8617/4053, 7015/14930, 63618/5782, respectively. We deploy the models with the best performance on the validation set to detect anomalous sequences and entries on $\mathcal{U}$, and report their results throughout this section.</p>
<p>Baselines. We compare our CFDet with two types of baselines that can achieve entry-level anomaly detection. The first one is interpretation-based approaches and the other is traditional anomaly detection approaches.</p>
<p>For interpretation-based approaches, we implement the following two baselines.</p>
<ul>
<li>Attention. The attention mechanism is often used to provide interpretations to deep learning models based on the attention weights [32]. We improve the LSTM model $f(\cdot)$ by adding the attention mechanism as a new</li>
</ul>
<p>TABLE II: Results of anomalous entry detection on the unlabeled dataset $\mathcal{U}$ (mean $\pm$ std.). All values are percentages.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Metric</th>
<th>OCSVM</th>
<th>iForest</th>
<th>Attention</th>
<th>Shapley</th>
<th>CFDet</th>
</tr>
</thead>
<tbody>
<tr>
<td>BGL</td>
<td>Precision</td>
<td>$11.89 \pm 0.00$</td>
<td>$19.47 \pm 0.00$</td>
<td>$70.19 \pm 18.57$</td>
<td>$98.05 \pm 0.98$</td>
<td>$\mathbf{9 8 . 9 1} \pm 0.91$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>$14.33 \pm 0.00$</td>
<td>$23.82 \pm 0.00$</td>
<td>$76.57 \pm 17.45$</td>
<td>$97.56 \pm 1.86$</td>
<td>$\mathbf{9 8 . 5 6} \pm 1.52$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>$13.00 \pm 0.00$</td>
<td>$21.43 \pm 0.00$</td>
<td>$72.52 \pm 16.28$</td>
<td>$97.79 \pm 0.77$</td>
<td>$\mathbf{9 8 . 7 3} \pm 0.82$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>$52.93 \pm 0.00$</td>
<td>$57.98 \pm 0.00$</td>
<td>$81.70 \pm 10.70$</td>
<td>$98.41 \pm 0.83$</td>
<td>$\mathbf{9 9 . 0 7} \pm 0.74$</td>
</tr>
<tr>
<td>Thunderbird</td>
<td>Precision</td>
<td>$8.26 \pm 0.00$</td>
<td>$42.23 \pm 0.00$</td>
<td>$33.40 \pm 28.66$</td>
<td>$67.22 \pm 14.59$</td>
<td>$\mathbf{9 0 . 9 9} \pm 10.33$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>$100.00 \pm 0.00$</td>
<td>$99.99 \pm 0.00$</td>
<td>$41.80 \pm 24.52$</td>
<td>$94.17 \pm 5.33$</td>
<td>$\mathbf{1 0 0 . 0 0} \pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>$15.27 \pm 0.00$</td>
<td>$59.39 \pm 0.00$</td>
<td>$31.44 \pm 16.34$</td>
<td>$77.96 \pm 11.45$</td>
<td>$\mathbf{9 4 . 9 7} \pm 6.41$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>$73.63 \pm 0.00$</td>
<td>$96.75 \pm 0.00$</td>
<td>$63.22 \pm 10.12$</td>
<td>$94.17 \pm 4.13$</td>
<td>$\mathbf{9 9 . 3 3} \pm 0.97$</td>
</tr>
<tr>
<td>CERT</td>
<td>Precision</td>
<td>N/A</td>
<td>N/A</td>
<td>$71.35 \pm 20.02$</td>
<td>$60.72 \pm 9.05$</td>
<td>$\mathbf{9 9 . 3 1} \pm 2.19$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>N/A</td>
<td>N/A</td>
<td>$35.17 \pm 14.53$</td>
<td>$\mathbf{5 5 . 4 1} \pm 0.20$</td>
<td>$55.31 \pm 0.10$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>N/A</td>
<td>N/A</td>
<td>$46.33 \pm 17.52$</td>
<td>$57.64 \pm 3.97$</td>
<td>$\mathbf{7 1 . 0 4} \pm 0.50$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>N/A</td>
<td>N/A</td>
<td>$67.52 \pm 7.27$</td>
<td>$77.52 \pm 0.04$</td>
<td>$\mathbf{7 7 . 6 5} \pm 0.04$</td>
</tr>
</tbody>
</table>
<p>anomalous sequence detector and train the model based on the Deep SVDD loss. The attention weights show how much contributions the entries in sequences make for the final predictions. A threshold is set to identify the anomalous entries.</p>
<ul>
<li>Shapley. The Shapley value is a classical approach that attributes the prediction of a machine-learning model on an input to its base features [26, 33]. We derive the Shapley values of entries in sequences as interpretations to the detecting results of the anomalous sequence detector. Especially, given an anomalous sequence, an entry with a high Shapley value indicates that replacing the entry with an entry in a normal sequence could significantly reduce the distance from the sequence to the normal center $\mathbf{c}$.
We highlight that the above two approaches are not "baselines" in the usual case because the idea of combining a detection model with an interpretation approach to detect anomalous entries in sequences is novel and has not been studied by earlier work.</li>
</ul>
<p>For one-class anomaly detection approaches, we use the following two classical approaches:</p>
<ul>
<li>One Class Support Vector Machines (OCSVM) is a one-class anomaly detection model trained only by known normal samples [34].</li>
<li>Isolation Forest (iForest) is a widely-used unsupervised method for outlier detection based on the ensemble of binary trees [35].
For BGL and Thunderbird, we use words in log messages as features of each entry and build bag-of-words vectors as inputs to OCSVM and iForest. For CERT, the entries in sequences are only about the user activities, such as http visit, email, and file operation, which are hard to define proper features about these activities as inputs to OCSVM and iForest. Hence, we do not have results on the CERT dataset, shown as N/A in Table II. This also shows the challenge of fine-grained anomaly detection in sequential data faced by traditional anomaly detection approaches, and the advantage of our approach.
Implementation Details. We represent the log templates in BGL and user activities in CERT as embedding vectors with a size of 50 and in Thunderbird with a size of 500. For both BGL and CERT, we set the LSTM model used as anomalous sequence detector $f(\cdot)$ as a single-layer LSTM with a hidden
size of 128, while for Thunderbird, we use a single-layer LSTM with a hidden size of 512. For all three datasets, we train $f(\cdot)$ with the Deep SVDD loss in 50 epochs and update the center $\mathbf{c}$ in the first 20 epochs. For the LSTM model used in the anomalous entry detector $g(\cdot)$, for all three datasets, we use a single layer LSTM with a hidden size as 128, which is trained in 100 epochs. The code and datasets are available online ${ }^{1}$.</li>
</ul>
<p>For the attention-based baseline, we set the threshold as 0.05 because the sequence length is 20 , which means if one entry makes a contribution higher than an average ratio, we will label it as anomalous. For the Shapley value-based baseline, once a sequence is detected as an anomaly, we consider the entries with positive Shapley values as anomalous entries. To ensure a fair comparison, all hyperparameters in baselines are also tuned based on the validation set.
Evaluation Metrics. We adopt the precision, recall, F-1 score, and Area Under Receiver Operating Characteristic Curve (AUC) to evaluate the performance of anomalous sequence and entry detection and report the mean and standard deviation after 10 times of running. Precision, recall, and F-1 score indicate the performance focusing on the anomaly class, while AUC indicates the true positives against false positives across various anomalous score thresholds.</p>
<h2>B. Experimental Results</h2>
<p>1) The Performance of Anomalous Entry Detection: Table II shows the performance of detecting anomalous entries on the unlabeled dataset $\mathcal{U}$. We notice that two traditional anomaly detection models, OCSVM and iForest, cannot achieve good performance on detecting anomalous entries due to two possible reasons: the bag-of-words vectors based on the texts in log messages are not ideal features to identify anomalies, and some contextual anomalies cannot be identified solely based on the information in each entry. Two interpretation-based baselines, Attention and Shapley, achieve better performance than OCSVM and iForest, which shows the advantage of applying interpretation-based models for anomalous entry detection. Shapley can achieve very good performance on the BGL dataset. This could be because the ratio of anomalous entries in anomalous sequences is high,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>thus making the anomalous entries easy to be detected. Our CFDet achieves the best performance in terms of F-1 score and AUC on all three datasets. Especially, on Thunderbird, CFDet detects all the anomalous entries in the unlabeled dataset. Meanwhile, CFDet also achieves perfect precision on CERT but with low recall. We will explain the reason in the following subsection. Meanwhile, the standard deviations of our CFDet are close to zero on CERT. This is because CFDet keeps stable on detecting the rare events in the sequences.</p>
<p>TABLE III: Anomalous sequence detection (mean $\pm$ std.)</p>
<table>
<thead>
<tr>
<th></th>
<th>BGL</th>
<th>Thunderbird</th>
<th>CERT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>$98.81 \pm 0.39$</td>
<td>$94.59 \pm 3.62$</td>
<td>$100.00 \pm 0.00$</td>
</tr>
<tr>
<td>Recall</td>
<td>$98.02 \pm 1.14$</td>
<td>$100.00 \pm 0.00$</td>
<td>$63.77 \pm 0.00$</td>
</tr>
<tr>
<td>F-1 score</td>
<td>$98.41 \pm 0.56$</td>
<td>$97.19 \pm 1.96$</td>
<td>$77.88 \pm 0.00$</td>
</tr>
<tr>
<td>AUC</td>
<td>$98.73 \pm 0.54$</td>
<td>$93.75 \pm 4.55$</td>
<td>$81.88 \pm 0.00$</td>
</tr>
</tbody>
</table>
<p>2) Subcomponent Evaluation: Our approach consists of two components, Deep SVDD-based anomalous sequence detection and counterfactual explanation-based anomalous entry detection. We evaluate the performance of each component.</p>
<p>Deep SVDD-based anomalous sequence detection. Table III shows the performance of anomalous sequence detection on the unlabeled dataset $\mathcal{U}$. In short, on all three datasets, the anomalous sequence detector of our CFDet achieves good performance, which provides a solid foundation for the following anomalous entry detection. For BGL and Thunderbird, the anomalous sequence detector achieves nearly perfect recall values with high precision, which means our model is able to detect all the anomalous sequences with a small number of false alerts. For CERT, all the detected anomalous sequences are truly anomalous (precision=1.0), but some anomalous sequences can not be correctly identified (recall=0.6377). This explains why the recall ( 0.5528 ) on the anomalous entry detection (shown in Table II) is not high. Note that if an anomalous sequence is falsely labeled as normal, we would falsely consider all entries in the sequence are normal, thus leading to a low recall value.</p>
<p>TABLE IV: Counterfactual explanation-based anomalous entry detection on the detected anomalous sequences $\hat{\mathcal{U}}^{-}$(mean $\pm$ std.)</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Precision</td>
<td>70.19 $\pm 18.57$</td>
<td>$98.05 \pm 0.98$</td>
<td>$\mathbf{9 8 . 9 1} \pm 0.91$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>77.17 $\pm 17.62$</td>
<td>$98.37 \pm 1.84$</td>
<td>$\mathbf{9 9 . 3 8} \pm 1.51$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>72.80 $\pm 16.39$</td>
<td>$98.19 \pm 0.77$</td>
<td>$\mathbf{9 9 . 1 3} \pm 0.83$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>55.99 $\pm 17.45$</td>
<td>$93.31 \pm 2.51$</td>
<td>$\mathbf{9 6 . 3 3} \pm 2.81$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Precision</td>
<td>33.40 $\pm 28.66$</td>
<td>$67.22 \pm 14.59$</td>
<td>$\mathbf{9 0 . 9 9} \pm 10.33$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>51.79 $\pm 17.08$</td>
<td>$94.17 \pm 5.33$</td>
<td>$\mathbf{1 0 0 . 0 0} \pm 0.00$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>34.61 $\pm 15.20$</td>
<td>$77.96 \pm 11.45$</td>
<td>$\mathbf{9 4 . 9 7} \pm 6.41$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>58.87 $\pm 11.48$</td>
<td>$92.82 \pm 4.88$</td>
<td>$\mathbf{9 9 . 0 4} \pm 1.38$</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Precision</td>
<td>71.35 $\pm 20.02$</td>
<td>$60.72 \pm 9.05$</td>
<td>$\mathbf{9 9 . 3 1} \pm 2.19$</td>
</tr>
<tr>
<td></td>
<td>Recall</td>
<td>58.94 $\pm 24.37$</td>
<td>$\mathbf{9 1 . 6 0} \pm 0.33$</td>
<td>$90.73 \pm 0.16$</td>
</tr>
<tr>
<td></td>
<td>F-1 score</td>
<td>63.26 $\pm 23.02$</td>
<td>$72.67 \pm 6.39$</td>
<td>$\mathbf{9 4 . 8 1} \pm 0.94$</td>
</tr>
<tr>
<td></td>
<td>AUC</td>
<td>78.20 $\pm 12.25$</td>
<td>$91.92 \pm 1.26$</td>
<td>$\mathbf{9 5 . 3 2} \pm 0.05$</td>
</tr>
</tbody>
</table>
<p>Counterfactual explanation-based anomalous entry detection. We further analyze the counterfactual explanation-based anomalous entry detection. Table IV shows the anomalous entry detection on the detected anomalous sequences $\hat{\mathcal{U}}^{-}$ whereas Table II shows results on the whole unlabeled dataset $\mathcal{U}$. Both OCSVM and iForest directly detect anomalous entries and they do not depend on the prediction results from Deep SVDD. Hence we do not report the results of OCSVM and iForest in Table IV. On the set of detected anomalous sequences $\hat{\mathcal{U}}^{-}$, CFDet achieves extremely high F-1 scores and AUC on all three datasets. It means CFDet can provide the post-hoc interpretation to the anomalous sequence detection with high fidelity. Especially, for CERT, we can conclude that once a sequence is detected as anomalous, CFDet can accurately detect its anomalous entries. We also observe similar results on Shapley. The performance on $\hat{\mathcal{U}}^{-}$ is better than the unlabeled dataset $\mathcal{U}$. This shows again that a good interpretable model can identify anomalous entries by providing interpretation to the detection results.</p>
<p>Based on the results shown in Tables II, III, and IV, we conclude that CFDet is able to provide good explanations to the results of anomalous sequence detection. When the task is to identify the anomalous entries in unlabeled data, the performance of the anomalous sequence detection is crucial. <img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Hyperparameter sensitivity on the weight of each loss term in Equation 9 3) Sensitivity Analysis: We adopt Thunderbird as an example to analyze how triplet loss, continuity loss and sparsity loss affect anomalous entry detection. Sensitivity analysis on the weight of each loss term in Equation 9. We first analyze the effects on performance from the weights of triplet loss $\alpha$, the continuity loss $\beta$, and sparsity loss $\gamma$ defined in Equation 9. First, Figure 2 shows that triplet loss, continuity loss, and sparsity loss are critical for anomalous entry detection. If we set the weight of one loss to $0(\alpha=0, \beta=0$ or $\gamma=0)$, i.e., removing one loss term from the objective function, the F-1 score and AUC are low. Especially, for the triplet weight, when $\alpha=0$, the AUC value is just around $50 \%$, and the F-1 score is around $10 \%$ (shown in Figure 2a), meaning the critical of triplet loss for accurately detecting anomalous entries. We can have a similar observation for the sparsity loss shown in Figure 2c, i.e., no sparsity loss</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" />
(a) Triplet param. $\lambda_{t}$ in Eq. 11
(b) Cont. param. $\lambda_{c}$ in Eq. 12
<img alt="img-3.jpeg" src="img-3.jpeg" />
(c) Sparsity param. $\lambda_{s}$ in Eq. 13</p>
<p>Fig. 3: Hyperparameter sensitivity on the constant terms in losses defined in Equations 11 - 13
$(\gamma=0)$ leading to low F-1 and AUC. Meanwhile, we can also notice that both the triplet weight $\alpha$ and sparsity weight $\gamma$ do not have significant impacts on the performance as long as the values are not zero. On the other hand, the performance is sensitive to the continuity weight $\beta$. As shown in Figure 2b, when the $\beta$ value keeps increasing, the performance becomes worse.
Sensitivity analysis on the constant terms in loss terms defined in Equations 11 - 13. We then analyze the sensitivity of the triplet parameter $\lambda_{t}$ (Equation 11), continuity parameter $\lambda_{c}$ (Equation 12) and sparsity parameter $\lambda_{s}$ (Equation 13). $\lambda_{t}$ indicates the margin between the distance from $\mathbf{r}<em z_-="z^{-">{z^{+}}$to $\mathbf{c}$ and distance from $\mathbf{r}</em>=1$ indicates we expect only one anomalous entry in the sequence.}}$to $\mathbf{c}$. $\lambda_{c}$ indicates the expected number of the anomalous subsequences. For example, in our experiments, the sequence length is $20 . \lambda_{c}=0$ means all entries in the anomalous sequence are anomalous, while $\lambda_{c}=19$ means no consecutive anomalous entries in the sequence. $\lambda_{s}$ indicates the expected number of anomalous entries. $\lambda_{s</p>
<p>Figure 3a shows that when the margin is $0\left(\lambda_{t}=0\right)$, CFDet cannot get a reasonable result, which meets the expectation that we need to ensure the detected anomalous subsequence have a large distance to the center. Once the margin is greater than $0\left(\lambda_{t}&gt;0\right)$, CFDet achieves much better performance. Figures 3b and 3c show the performance of anomalous entry detection with values ranging of $\lambda_{c}$ from 0 to 19 and $\lambda_{s}$ from 1 to 19, respectively. We observe that when $\lambda_{c}$ and $\lambda_{s}$ are small, our approach achieves very good performance. On the other hand, with the increase of $\lambda_{c}$ and $\lambda_{s}$, the performance becomes worse. This is because for Thunderbird, the ratio of anomalous entries is small ( 0.15 ). Therefore, once we set $\lambda_{c}$ and $\lambda_{s}$ in a reasonable range, our approach can achieve good performance. On the other hand, if we keep increasing the values $\lambda_{c}$ and $\lambda_{s}$, the performance will get worse. This is because a large $\lambda_{c}$ or $\lambda_{s}$ would force the model to select more entries as anomalous, leading to high false alerts.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Anomalous Sequence Detection on $\mathcal{U}$
(b) Anomalous Entry Detection on $\tilde{\mathcal{U}}^{-}$</p>
<p>Fig. 4: Performance of anomalous sequence and entry detection with various sequence lengths on Thunderbird.
4) Various Sequence Lengths: In our experiments, the default sequence length is 20 . We further analyze the performance of our framework on the anomalous sequence and entry detection with various sequence lengths on the Thunderbird dataset. Figure 4 shows the evaluation results.</p>
<p>Figure 4a shows the experimental results of anomalous sequence detection based on Deep SVDD. We can observe that the performance for the anomalous sequence detection keeps stable when the sequence lengths range from 10 to 45 for Thunderbird. When the sequence length is 50 , the detection results from Deep SVDD got decreased. Figure 4b shows the results of anomalous entry detection on the detected anomalous sequence set $\tilde{\mathcal{U}}^{-}$. We can observe that the recall and AUC values are high when the sequence lengths increase from 10 to 50 , but the precision as well as the F-1 scores keep reducing once the length is great than 15 . It means on the Thunderbird dataset, CFDet is able to identify all the abnormal entries but predict some normal entries as abnormal (false positive) when the sequence length is large.
5) Visualization: We further visualize representations of normal and anomalous sequences in BGL, Thunderbird, and CERT datasets. We randomly select 500 detected normal and anomalous sequences, respectively. For the detected anomalous sequence $Z$ from each dataset, we then get the anomalous subsequence $Z^{-}$and the normal subsequence $Z^{+}$. After deriving the representations of $S^{+}, Z, Z^{-}$, and $Z^{+}$ based on $f(\cdot)$, we adopt the t-SNE [36] to map the representations into a two-dimensional space. Figure 5 shows the visualization plots. Overall, as expected, $S^{+}$and $Z^{+}$as normal sequences and subsequences are grouped together and located around the normal center $\mathbf{c}$, while $Z$ and $Z^{-}$as anomalous sequences and subsequences are close but far from the normal center. It also demonstrates the detected $Z^{+}$ only contains the normal entries, while the corresponding $Z^{-}$has the anomalous entries. Meanwhile, on both Thunderbird and CERT, the anomalous sequences and the corresponding subsequences are much more diverse compared with normal sequences.
6) Case Study: Detecting Anomalous Log Entries: We further conduct a case analysis on Thunderbird for anomalous log entry detection. In the Thunderbird dataset, CFDNet can detect all anomalous entries ( $100 \%$ recall) with high precision. Hence, as shown in Figure 6, we can notice that CFDNet correctly labels all four log messages about fatal</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 5: Visualization of the center (black), detected normal sequences <em>S<sup>+</sup></em> (cyan), detected anomalous sequences <em>Z</em> (yellow) and the corresponding normal <em>Z<sup>+</sup></em> (green) and anomalous <em>Z<sup>−</sup></em> (red) subsequences.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Fig. 6: Case study of abnormal log entry detection on Thunderbird. Checkmark indicates an anomalous entry while X mark indicates a normal entry.</p>
<p>errors as anomalous entries and correctly predict other entries as normal. On the other hand, the two baselines, Attention and Shapley, cannot label all the anomalous entries and also mislabel some normal entries as anomalous.</p>
<h2>V. CONCLUSION</h2>
<p>In this paper, we have developed CFDet for fine-grained anomaly detection in sequential data based on the idea of counterfactual interpretation. CFDet is able to identify anomalous entries in sequences without using any labeled anomalous sequences/entries for training. In our CFDet, we first build an anomalous sequence detection model based on normal sequences to detect the anomalous sequences. We then develop a self-supervised anomalous entry detection model to identify the anomalous entries. The core idea is that the anomalous subsequence in the anomalous sequence should be far from the center of normal samples while the rest normal subsequence, considered as the counterfactual sequence of the original anomalous sequence, should be close to the center. Experiments on three datasets show that our model can identify anomalous entries with high accuracy. A potential future direction is to design an end-to-end anomalous entry detection approach from sequences without relying on the performance of anomalous sequence detection. For some types of sequential data, such as time series data, the data distributions are governed by the underlying causal structure. Another interesting direction is to consider the causal relationships when deriving the counterfactual explanations.</p>
<h2>REFERENCES</h2>
<ul>
<li>[1] M. Du, F. Li, G. Zheng, and V. Srikumar, “DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning,” in <em>Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</em>, 2017.</li>
<li>[2] F. Liu, Y. Wen, D. Zhang, X. Jiang, X. Xing, and D. Meng, “Log2vec: A Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats within Enterprise,” in <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, ser. CCS ’19. London, United Kingdom: Association for Computing Machinery, Nov. 2019, pp. 1777–1794.</li>
<li>[3] R. Zhou, P. Sun, S. Tao, R. Zhang, W. Meng, Y. Liu, Y. Zhu, Y. Liu, D. Pei, S. Zhang, and Y. Chen, “LogAnomaly: Unsupervised Detection of Sequential and Quantitative Anomalies in Unstructured Logs,” in <em>IJCAI</em>, 2019, pp. 4739–4745.</li>
<li>[4] M. Du, Z. Chen, C. Liu, R. Oak, and D. Song, “Lifelong Anomaly Detection Through Unlearning,” in <em>Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</em>, ser. CCS ’19. London, United Kingdom: Association for Computing Machinery, Nov. 2019, pp. 1283–1297.</li>
<li>[5] Shuhan Yuan, Panpan Zheng, Xintao Wu, and Hanghang Tong, “Few-shot Insider Threat Detection,” in <em>ACM International Conference on Information and Knowledge Management</em>, 2020.</li>
<li>[6] Z. Wang, Z. Chen, J. Ni, H. Liu, H. Chen, and J. Tang, “Multi-Scale One-Class Recurrent Neural Networks for Discrete Event Sequence Anomaly Detection,” in <em>KDD</em>, 2021.</li>
<li>[7] H. Guo, S. Yuan, and X. Wu, “Logbert: Log anomaly detection via bert,” in <em>2021 International Joint Conference on Neural Networks (IJCNN)</em>. IEEE, 2021, pp. 1–8.</li>
<li>[8] C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and C. Zhong, “Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges,” <em>arXiv:2103.11251 [cs, stat]</em>, Mar. 2021.</li>
<li>[9] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui, A. Binder, E. Müller, and M. Kloft, “Deep One-Class Classification,” in <em>International Conference on Machine Learning</em>. PMLR, Jul. 2018, pp. 4393–4402.</li>
<li>[10] L. Ruff, R. A. Vandermeulen, N. Görnitz, A. Binder,</li>
</ul>
<p>E. Müller, K.-R. Müller, and M. Kloft, "Deep SemiSupervised Anomaly Detection," in ICLR, Feb. 2020.
[11] L. Ruff, J. R. Kauffmann, R. A. Vandermeulen, G. Montavon, W. Samek, M. Kloft, T. G. Dietterich, and K.-R. Müller, "A unifying review of deep and shallow anomaly detection," Proceedings of the IEEE, 2021.
[12] A. Blázquez-García, A. Conde, U. Mori, and J. A. Lozano, "A review on outlier/anomaly detection in time series data," ACM Computing Surveys (CSUR), vol. 54, no. 3, pp. 1-33, 2021.
[13] J. Soldani and A. Brogi, "Anomaly detection and failure root cause analysis in (micro) service-based cloud applications: A survey," ACM Computing Surveys (CSUR), vol. 55, no. 3, pp. 1-39, 2022.
[14] K. Santhoshk, P. Dograd, and P. Royp, "Anomaly detection in road traffic using visual surveillance [j]," ACM Computing Surveys, vol. 53, no. 6, pp. 1-26, 2020.
[15] X. Zhang, Y. Xu, Q. Lin, B. Qiao, H. Zhang, Y. Dang, C. Xie, X. Yang, Q. Cheng, Z. Li, J. Chen, X. He, R. Yao, J.-G. Lou, M. Chintalapati, F. Shen, and D. Zhang, "Robust log-based anomaly detection on unstable log data," in Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ser. ESEC/FSE 2019. New York, NY, USA: Association for Computing Machinery, Aug. 2019, pp. 807-817.
[16] C. Chen, O. Li, C. Tao, A. J. Barnett, J. Su, and C. Rudin, "This Looks Like That: Deep Learning for Interpretable Image Recognition," in NeurIPS, Dec. 2019.
[17] M. T. Ribeiro, S. Singh, and C. Guestrin, ""Why Should I Trust You?": Explaining the Predictions of Any Classifier," in Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16. New York, NY, USA: ACM, 2016, pp. 1135-1144.
[18] S. M. Lundberg and S.-I. Lee, "A Unified Approach to Interpreting Model Predictions," in NIPS, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4765-4774.
[19] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," in 2017 IEEE International Conference on Computer Vision (ICCV), Oct. 2017, pp. 618-626.
[20] M. Sundararajan, A. Taly, and Q. Yan, "Axiomatic Attribution for Deep Networks," in ICML, Jun. 2017.
[21] A. Brown, A. Tuor, B. Hutchinson, and N. Nichols, "Recurrent Neural Network Attention Mechanisms for Interpretable System Log Anomaly Detection," in Proceedings of the First Workshop on Machine Learning for Computing Systems, ser. MLCS'18. New York, NY, USA: Association for Computing Machinery, Jun. 2018, pp. 1-8.
[22] P. Liznerski, L. Ruff, R. A. Vandermeulen, B. J. Franks, M. Kloft, and K.-R. Müller, "Explainable Deep One-</p>
<p>Class Classification," in ICLR, Mar. 2021.
[23] L. Antwarg, R. M. Miller, B. Shapira, and L. Rokach, "Explaining Anomalies Detected by Autoencoders Using SHAP," arXiv:1903.02407 [cs, stat], Jun. 2020.
[24] Q. P. Nguyen, K. W. Lim, D. M. Divakaran, K. H. Low, and M. C. Chan, "GEE: A Gradient-based Explainable Variational Autoencoder for Network Anomaly Detection," in 2019 IEEE Conference on Communications and Network Security (CNS), Jun. 2019, pp. 91-99.
[25] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network," in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, ser. KDD '19. Anchorage, AK, USA: Association for Computing Machinery, Jul. 2019, pp. 2828-2837.
[26] C. Molnar, Interpretable Machine Learning, 2019.
[27] M. Yu, S. Chang, Y. Zhang, and T. S. Jaakkola, "Rethinking Cooperative Rationalization: Introspective Extraction and Complement Control," in EMNLP, Dec. 2019.
[28] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction. MIT press, 2018.
[29] A. Oliner and J. Stearley, "What supercomputers say: A study of five system logs," in 37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07). IEEE, 2007, pp. 575-584.
[30] J. Glasser and B. Lindauer, "Bridging the Gap: A Pragmatic Approach to Generating Insider Threat Data," in 2013 IEEE Security and Privacy Workshops, 2013, pp. 98-104.
[31] P. He, J. Zhu, Z. Zheng, and M. R. Lyu, "Drain: An Online Log Parsing Approach with Fixed Depth Tree," in 2017 IEEE International Conference on Web Services (ICWS), Jun. 2017, pp. 33-40.
[32] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio, "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention," in International Conference on Machine Learning. PMLR, Jun. 2015, pp. 2048-2057.
[33] M. Sundararajan and A. Najmi, "The Many Shapley Values for Model Explanation," in International Conference on Machine Learning. PMLR, Nov. 2020, pp. 92699278.
[34] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson, "Estimating the Support of a HighDimensional Distribution," Neural Computation, vol. 13, no. 7, pp. 1443-1471, Jul. 2001.
[35] F. T. Liu, K. M. Ting, and Z. Zhou, "Isolation Forest," in 2008 Eighth IEEE International Conference on Data Mining, Dec. 2008, pp. 413-422.
[36] L. Van der Maaten and G. Hinton, "Visualizing data using t-sne." Journal of machine learning research, vol. 9, no. 11, 2008.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://bit.ly/3dTykgj&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
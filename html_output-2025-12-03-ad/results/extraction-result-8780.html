<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8780 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8780</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8780</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-ea3928baba12de2fba9ce76e6804bfe50fe1cef3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ea3928baba12de2fba9ce76e6804bfe50fe1cef3" target="_blank">Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work presents a novel training procedure that can lift the limitation of the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs, and presents strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.</p>
                <p><strong>Paper Abstract:</strong> Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the non-sequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequence-based AMR models are robust against ordering variations of graph-to-sequence conversions.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8780.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8780.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AMR DFS Linearization + Scope</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first traversal linearization with scope markers and rendering function (AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-sequence representation that linearizes AMR graphs by a depth-first preorder traversal (including backward steps) and emits tokens for nodes and labeled edges with explicit scope markers (parentheses) according to a rendering function; used together with anonymization and other preprocessing for seq2seq training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Depth-first linearization with scope markers</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse the rooted AMR graph in depth-first preorder (including backtracking steps). For each visited element: if node -> emit node type token; if edge -> emit edge type token and recursively emit a bracketed (scope-marked) string for the child concept. Omit scope markers when a node has a single child to reduce length. The rendering function produces a sequence of tokens representing nodes, edges and parentheses.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (rooted directed acyclic semantic graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first search traversal (pre-order) over AMR nodes/edges; render nodes and edges to tokens; include explicit scope parentheses for multi-child nodes; omit variables and instance-of markers prior to linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (graph -> sentence) and text-to-AMR parsing (sentence -> graph) via seq2seq models</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generation: BLEU 33.8 (test) for the full system (GIGA-20M pretraining); Parsing: SMATCH 62.1 (test) for the full parser (self-trained on 20M). (Numbers reported in this paper for models using this representation.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to previous graph-to-string approaches (tree-to-string, learned linearization, TSP partitioning), this linearization with seq2seq yields state-of-the-art BLEU (33.8) and competitive SMATCH (62.1) without heavy external resources. The paper shows seq2seq models are largely agnostic to child-order artifacts produced by linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, graph-isomorphic mapping that allows any depth-first order; supports randomization of child order and still performs well; concise when omitting scopes for unary branches; integrates cleanly with seq2seq models and benefits from unlabeled pretraining; explicit scope markers help capture long-range semantic relations and substantially improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization increases sequence length compared to graph, and omission of some graph annotations (variables, instance-of, senses) results in information loss; requires careful preprocessing (anonymization, scope markers) to avoid sparsity issues.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Without scope markers model performance collapses (see ablation: removing scope markers yields large drops in parsing F1/SMATCH and harms generation). Some generated outputs still show coverage issues (missing subgraphs), disfluencies caused by anonymization, and occasional wrong attachments â€” indicating limitations in fully recovering original graph content from the sequence representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8780.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-order linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-authored child order linearization for AMR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of linearization that preserves the child visiting order exactly as authored in the human AMR annotations; used as one ordering strategy for rendering AMR graphs to sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Human-order linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Traverse children in the exact order given by the human AMR annotation when performing the depth-first rendering described above, producing sequences that reflect annotation ordering biases.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal following the human-provided ordering of children in the AMR annotation</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On the reported linearization-order study (DEV): BLEU = 21.7 (Human ordering) under the experimental setup used for that comparison. (Paper also reports full-system BLEU 33.1/33.8 for the best models that use human-order among other preprocessing and pretraining.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs slightly better than global-random and per-example random child orderings (Human 21.7 vs Global-RANDOM 20.8 vs RANDOM 20.3 in DEV experiments), suggesting human ordering leaks some generation order information.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Can implicitly encode ordering cues correlated with real sentence realization (annotation bias), leading to small but consistent gains in generation quality in some setups.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on annotation artifacts (not purely graph-structural) which may not be available or desirable for applications that require order-agnostic generation; may not generalize if annotation ordering conventions differ.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The improvement over random/global order is small; when training systems intended to operate in environments without consistent ordering, reliance on human order can be a liability. Human order can 'leak' surface realization order, so it is not a purely graph-based signal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8780.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Global-Random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global random fixed edge-type ordering linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization variant where a single random global ordering of AMR edge types is fixed and used to order children across all examples during traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Global-random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct one random global ordering over all AMR edge types; for every graph, traverse children ordered by the position of the edge type in that global ordering, rendering with the same DFS + scope rendering function.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal; children ordered per a single global random permutation of edge types</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DEV BLEU = 20.8 (reported in linearization-order experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Per-example RANDOM and GLOBAL-RANDOM show very similar performance; both are only marginally worse than HUMAN ordering, showing seq2seq models are robust to ordering artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; demonstrates seq2seq models can learn to ignore arbitrary but consistent ordering artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Slightly worse than human ordering in the experimental setup; arbitrary global order gives no semantic alignment with surface realization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Minor degradation in BLEU relative to human ordering; no catastrophic failure reported but lower absolute BLEU in the ablation setup.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8780.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Per-example Random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-example random child-order linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Each AMR example is linearized using a distinct random ordering of edge types when traversing children, testing per-sample order variability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Per-example random linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each example independently, sample a random ordering over edge types and traverse children by that order during depth-first rendering; scope markers and anonymization remain in place.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Depth-first traversal with a different random child-order per example</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>DEV BLEU = 20.3 (reported in linearization-order experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Per-example RANDOM performs nearly identically to GLOBAL-RANDOM, indicating seq2seq models can tolerate both per-example and global randomizations of child order.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Demonstrates representation robustness: seq2seq models can learn to ignore per-example order artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Slightly lower BLEU than human-ordering in the experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No severe failure but shows small degradation in BLEU compared to human ordering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8780.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Anonymization (NEs & Dates)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Named-entity and date anonymization for AMR linearization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Replace AMR subgraphs headed by named-entity or date types with typed anonymization tokens (with indices) and map corresponding sentence spans to these tokens; use aligners to construct mappings and recover text at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Anonymized subgraph tokens (NE/date anonymization)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Identify AMR subgraphs with :name roles or date structures, replace them with tokens of the form <type>_i (fine-grained type plus index). During training align anonymized AMR tokens to sentence spans (using JAMR and an unsupervised aligner) and record mappings; at generation time, predicted tokens are replaced by the most frequent mapped span or copied from the AMR graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs containing named-entity/date subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>AMR subgraph replacement with typed anonymization tokens prior to linearization and parallel replacement in the sentence; special date-format tokens for year/month/day variants.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (to normalize inputs) and AMR-to-text generation (to handle rare/open-class items)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation: anonymization contributes substantially to both parsing and generation performance; exact numeric ablation values not fully tabulated in paper, but authors report anonymization and scope markers as the largest contributors to BLEU/SMATCH improvements. Without anonymization, seq2seq parsing is largely ineffective (citing Peng et al., 2017 and their own ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Similar in spirit to Peng et al. (2017) anonymization/tying of low-frequency words; differs by using large unlabeled corpora to reduce vocabulary rather than heavily collapsing vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces sparsity from open-class vocabulary (NEs, numbers, dates), enables models to handle unseen entities by copying/lookup, dramatically lowers OOV rates when combined with Gigaword pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Anonymization can introduce generation disfluencies and minor grammatical errors (examples in qualitative analysis); requires aligner outputs and mapping tables; loses surface lexical variation that must be re-inserted heuristically.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If an anonymized entity was never observed during training, the fallback is to copy from the AMR graph; mappings can still produce incorrect or unnatural surface forms and are a source of disfluency in generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8780.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph simplification (vars/senses removal)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph simplification: removal of variable names, instance-of, and senses</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preprocessing step that removes variable identifiers and the instance-of relation (/) and, for generation only, removes PropBank sense suffixes from AMR concepts to reduce linearized sequence length and vocabulary sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph simplification (remove variables, instance-of, senses)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Before linearization, strip variable names and the instance-of relation from AMR graphs; replace re-entrant variable mentions with their co-referring concept; optionally remove PropBank sense suffixes (e.g., -01) for generation to reduce vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Deterministic graph simplification applied to AMR graphs prior to DFS rendering</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and AMR parsing (indirectly, due to simplified targets/inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not given as standalone numeric results, but included in the full preprocessing pipeline that achieves BLEU 33.8 (GIGA-20M) and SMATCH 62.1; ablations show preprocessing components collectively important.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Common form of simplification also used in other neural AMR works; the paper argues that combined with anonymization and pretraining this yields stronger seq2seq performance than prior neural baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces sequence length and vocabulary size, simplifying the learning problem and helping the seq2seq model generalize from limited AMR data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Removes explicit graph annotations (variables, instance-of, senses) that could be needed to perfectly reconstruct AMR; information loss may lead to some generation/attachment errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Information loss can make certain attachments and detailed realizations harder to produce or recover, contributing to coverage errors and some wrong-attachment outputs in qualitative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8780.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pourdamghani2016 (learned ordering / PBMT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generating English from abstract meaning representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior AMR-to-text approach that learns to linearize AMR graphs in an order that follows the output sentence and uses a phrase-based MT decoder; ordering learning aims to reduce alignment crossings for the statistical decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating English from abstract meaning representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Learned linearization for PBMT (order-following)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Partition/linearize the AMR and learn a child ordering that matches output sentence order to reduce alignment crossings for a phrase-based statistical MT decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Learn an ordering of graph fragments/edges based on training data so the resulting linearization aligns well with target sentences; used with phrase-based MT decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU (Table 3): Dev 27.2, Test 26.9 (their PBMT system trained on LDC2014T12 in the paper's comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper contrasts its seq2seq approach with PBMT + learned ordering: seq2seq with DFS + preprocessing + pretraining outperforms PBMT (BLEU 33.8 vs 26.9 test).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Specifically optimizes ordering for phrase-based decoders to reduce alignment complexity; effective in SMT pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on specialized ordering learning and integration with SMT; less flexible than end-to-end neural seq2seq approaches; requires explicit alignment and ordering models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noted by authors here: SMT-based schemes need good ordering to avoid phrase alignment crossings, whereas seq2seq models can be robust to arbitrary orderings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8780.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flanigan2016 (tree-to-string)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-to-string transduction approach that extracts tree transduction rules (driven by AMR-to-sentence alignments and POS features) used in a tree-based SMT system for AMR generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generation from abstract meaning representation using tree transducers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tree-to-string transduction (tree-based SMT)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert an AMR (or a tree extracted from it) to surface strings using a set of learned transduction rules; uses alignments and POS-based features to drive a tree-based statistical MT decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (converted to tree-like structures/tree transducers)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract tree transduction rules from aligned AMR-sentence pairs; apply transducer rules in decoding to generate sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU (Table 3): Dev 23.0, Test 23.0 for TREETOSTR baseline (Flanigan et al., 2016) referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Seq2seq approach in this paper yields substantially higher BLEU (33.8) than this tree-to-string approach (23.0) in the reported comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages structural transduction rules and alignment features; well-suited to SMT pipelines that exploit syntactic structure.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires extraction of rules and external alignment resources; less end-to-end than neural seq2seq; authors report lower BLEU than their seq2seq+pretraining approach.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower BLEU in reported comparisons; depends heavily on alignment quality and handcrafted transduction heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8780.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8780.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Song2016 (TSP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AMR-to-text generation as a traveling salesman problem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation method that partitions AMR graphs into fragments, then casts finding an optimal linearization order as a traveling salesman problem (TSP) over fragments to maximize generation quality for SMT decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR-to-text generation as a traveling salesman problem</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Fragment partitioning + TSP-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Partition the AMR graph into fragments and treat ordering fragments to form a linear sequence as a TSP instance, solving for an order that optimizes alignment/translation cost for downstream decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (partitioned into fragments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Partition graph into fragments and solve a TSP to determine a linear ordering of fragments prior to generation (used with an SMT decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported BLEU (Table 3): Dev 21.1, Test 22.4 for the TSP-based method referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Per the paper, seq2seq with DFS + preprocessing + pretraining outperforms the TSP approach by a substantial margin (BLEU 33.8 vs 22.4 test).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicitly searches for a globally coherent fragment ordering which may help SMT decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Complex partitioning and TSP solving steps; tightly coupled to SMT pipelines and alignment models; poorer BLEU than the seq2seq approach in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower BLEU in comparisons; likely sensitive to fragment partitioning and scoring heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural AMR: Sequence-to-Sequence Models for Parsing and Generation', 'publication_date_yy_mm': '2017-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating English from abstract meaning representations <em>(Rating: 2)</em></li>
                <li>Generation from abstract meaning representation using tree transducers <em>(Rating: 2)</em></li>
                <li>AMR-to-text generation as a traveling salesman problem <em>(Rating: 2)</em></li>
                <li>Addressing the data sparsity issue in neural AMR parsing <em>(Rating: 1)</em></li>
                <li>RIGA at SemEval-2016 Task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8780",
    "paper_id": "paper-ea3928baba12de2fba9ce76e6804bfe50fe1cef3",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "AMR DFS Linearization + Scope",
            "name_full": "Depth-first traversal linearization with scope markers and rendering function (AMR)",
            "brief_description": "A graph-to-sequence representation that linearizes AMR graphs by a depth-first preorder traversal (including backward steps) and emits tokens for nodes and labeled edges with explicit scope markers (parentheses) according to a rendering function; used together with anonymization and other preprocessing for seq2seq training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Depth-first linearization with scope markers",
            "representation_description": "Traverse the rooted AMR graph in depth-first preorder (including backtracking steps). For each visited element: if node -&gt; emit node type token; if edge -&gt; emit edge type token and recursively emit a bracketed (scope-marked) string for the child concept. Omit scope markers when a node has a single child to reduce length. The rendering function produces a sequence of tokens representing nodes, edges and parentheses.",
            "graph_type": "Abstract Meaning Representation (rooted directed acyclic semantic graphs)",
            "conversion_method": "Depth-first search traversal (pre-order) over AMR nodes/edges; render nodes and edges to tokens; include explicit scope parentheses for multi-child nodes; omit variables and instance-of markers prior to linearization.",
            "downstream_task": "AMR-to-text generation (graph -&gt; sentence) and text-to-AMR parsing (sentence -&gt; graph) via seq2seq models",
            "performance_metrics": "Generation: BLEU 33.8 (test) for the full system (GIGA-20M pretraining); Parsing: SMATCH 62.1 (test) for the full parser (self-trained on 20M). (Numbers reported in this paper for models using this representation.)",
            "comparison_to_others": "Compared to previous graph-to-string approaches (tree-to-string, learned linearization, TSP partitioning), this linearization with seq2seq yields state-of-the-art BLEU (33.8) and competitive SMATCH (62.1) without heavy external resources. The paper shows seq2seq models are largely agnostic to child-order artifacts produced by linearization.",
            "advantages": "Simple, graph-isomorphic mapping that allows any depth-first order; supports randomization of child order and still performs well; concise when omitting scopes for unary branches; integrates cleanly with seq2seq models and benefits from unlabeled pretraining; explicit scope markers help capture long-range semantic relations and substantially improve performance.",
            "disadvantages": "Linearization increases sequence length compared to graph, and omission of some graph annotations (variables, instance-of, senses) results in information loss; requires careful preprocessing (anonymization, scope markers) to avoid sparsity issues.",
            "failure_cases": "Without scope markers model performance collapses (see ablation: removing scope markers yields large drops in parsing F1/SMATCH and harms generation). Some generated outputs still show coverage issues (missing subgraphs), disfluencies caused by anonymization, and occasional wrong attachments â€” indicating limitations in fully recovering original graph content from the sequence representation.",
            "uuid": "e8780.0",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Human-order linearization",
            "name_full": "Human-authored child order linearization for AMR",
            "brief_description": "A variant of linearization that preserves the child visiting order exactly as authored in the human AMR annotations; used as one ordering strategy for rendering AMR graphs to sequences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Human-order linearization",
            "representation_description": "Traverse children in the exact order given by the human AMR annotation when performing the depth-first rendering described above, producing sequences that reflect annotation ordering biases.",
            "graph_type": "AMR graphs",
            "conversion_method": "Depth-first traversal following the human-provided ordering of children in the AMR annotation",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "On the reported linearization-order study (DEV): BLEU = 21.7 (Human ordering) under the experimental setup used for that comparison. (Paper also reports full-system BLEU 33.1/33.8 for the best models that use human-order among other preprocessing and pretraining.)",
            "comparison_to_others": "Performs slightly better than global-random and per-example random child orderings (Human 21.7 vs Global-RANDOM 20.8 vs RANDOM 20.3 in DEV experiments), suggesting human ordering leaks some generation order information.",
            "advantages": "Can implicitly encode ordering cues correlated with real sentence realization (annotation bias), leading to small but consistent gains in generation quality in some setups.",
            "disadvantages": "Relies on annotation artifacts (not purely graph-structural) which may not be available or desirable for applications that require order-agnostic generation; may not generalize if annotation ordering conventions differ.",
            "failure_cases": "The improvement over random/global order is small; when training systems intended to operate in environments without consistent ordering, reliance on human order can be a liability. Human order can 'leak' surface realization order, so it is not a purely graph-based signal.",
            "uuid": "e8780.1",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Global-Random linearization",
            "name_full": "Global random fixed edge-type ordering linearization",
            "brief_description": "A linearization variant where a single random global ordering of AMR edge types is fixed and used to order children across all examples during traversal.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Global-random linearization",
            "representation_description": "Construct one random global ordering over all AMR edge types; for every graph, traverse children ordered by the position of the edge type in that global ordering, rendering with the same DFS + scope rendering function.",
            "graph_type": "AMR graphs",
            "conversion_method": "Depth-first traversal; children ordered per a single global random permutation of edge types",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "DEV BLEU = 20.8 (reported in linearization-order experiments).",
            "comparison_to_others": "Per-example RANDOM and GLOBAL-RANDOM show very similar performance; both are only marginally worse than HUMAN ordering, showing seq2seq models are robust to ordering artifacts.",
            "advantages": "Simple to implement; demonstrates seq2seq models can learn to ignore arbitrary but consistent ordering artifacts.",
            "disadvantages": "Slightly worse than human ordering in the experimental setup; arbitrary global order gives no semantic alignment with surface realization.",
            "failure_cases": "Minor degradation in BLEU relative to human ordering; no catastrophic failure reported but lower absolute BLEU in the ablation setup.",
            "uuid": "e8780.2",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Per-example Random linearization",
            "name_full": "Per-example random child-order linearization",
            "brief_description": "Each AMR example is linearized using a distinct random ordering of edge types when traversing children, testing per-sample order variability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Per-example random linearization",
            "representation_description": "For each example independently, sample a random ordering over edge types and traverse children by that order during depth-first rendering; scope markers and anonymization remain in place.",
            "graph_type": "AMR graphs",
            "conversion_method": "Depth-first traversal with a different random child-order per example",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "DEV BLEU = 20.3 (reported in linearization-order experiments).",
            "comparison_to_others": "Per-example RANDOM performs nearly identically to GLOBAL-RANDOM, indicating seq2seq models can tolerate both per-example and global randomizations of child order.",
            "advantages": "Demonstrates representation robustness: seq2seq models can learn to ignore per-example order artifacts.",
            "disadvantages": "Slightly lower BLEU than human-ordering in the experimental setup.",
            "failure_cases": "No severe failure but shows small degradation in BLEU compared to human ordering.",
            "uuid": "e8780.3",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Anonymization (NEs & Dates)",
            "name_full": "Named-entity and date anonymization for AMR linearization",
            "brief_description": "Replace AMR subgraphs headed by named-entity or date types with typed anonymization tokens (with indices) and map corresponding sentence spans to these tokens; use aligners to construct mappings and recover text at generation time.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Anonymized subgraph tokens (NE/date anonymization)",
            "representation_description": "Identify AMR subgraphs with :name roles or date structures, replace them with tokens of the form &lt;type&gt;_i (fine-grained type plus index). During training align anonymized AMR tokens to sentence spans (using JAMR and an unsupervised aligner) and record mappings; at generation time, predicted tokens are replaced by the most frequent mapped span or copied from the AMR graph.",
            "graph_type": "AMR graphs containing named-entity/date subgraphs",
            "conversion_method": "AMR subgraph replacement with typed anonymization tokens prior to linearization and parallel replacement in the sentence; special date-format tokens for year/month/day variants.",
            "downstream_task": "AMR parsing (to normalize inputs) and AMR-to-text generation (to handle rare/open-class items)",
            "performance_metrics": "Ablation: anonymization contributes substantially to both parsing and generation performance; exact numeric ablation values not fully tabulated in paper, but authors report anonymization and scope markers as the largest contributors to BLEU/SMATCH improvements. Without anonymization, seq2seq parsing is largely ineffective (citing Peng et al., 2017 and their own ablation).",
            "comparison_to_others": "Similar in spirit to Peng et al. (2017) anonymization/tying of low-frequency words; differs by using large unlabeled corpora to reduce vocabulary rather than heavily collapsing vocabulary.",
            "advantages": "Reduces sparsity from open-class vocabulary (NEs, numbers, dates), enables models to handle unseen entities by copying/lookup, dramatically lowers OOV rates when combined with Gigaword pretraining.",
            "disadvantages": "Anonymization can introduce generation disfluencies and minor grammatical errors (examples in qualitative analysis); requires aligner outputs and mapping tables; loses surface lexical variation that must be re-inserted heuristically.",
            "failure_cases": "If an anonymized entity was never observed during training, the fallback is to copy from the AMR graph; mappings can still produce incorrect or unnatural surface forms and are a source of disfluency in generated text.",
            "uuid": "e8780.4",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Graph simplification (vars/senses removal)",
            "name_full": "Graph simplification: removal of variable names, instance-of, and senses",
            "brief_description": "Preprocessing step that removes variable identifiers and the instance-of relation (/) and, for generation only, removes PropBank sense suffixes from AMR concepts to reduce linearized sequence length and vocabulary sparsity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Graph simplification (remove variables, instance-of, senses)",
            "representation_description": "Before linearization, strip variable names and the instance-of relation from AMR graphs; replace re-entrant variable mentions with their co-referring concept; optionally remove PropBank sense suffixes (e.g., -01) for generation to reduce vocabulary.",
            "graph_type": "AMR graphs",
            "conversion_method": "Deterministic graph simplification applied to AMR graphs prior to DFS rendering",
            "downstream_task": "AMR-to-text generation and AMR parsing (indirectly, due to simplified targets/inputs)",
            "performance_metrics": "Not given as standalone numeric results, but included in the full preprocessing pipeline that achieves BLEU 33.8 (GIGA-20M) and SMATCH 62.1; ablations show preprocessing components collectively important.",
            "comparison_to_others": "Common form of simplification also used in other neural AMR works; the paper argues that combined with anonymization and pretraining this yields stronger seq2seq performance than prior neural baselines.",
            "advantages": "Reduces sequence length and vocabulary size, simplifying the learning problem and helping the seq2seq model generalize from limited AMR data.",
            "disadvantages": "Removes explicit graph annotations (variables, instance-of, senses) that could be needed to perfectly reconstruct AMR; information loss may lead to some generation/attachment errors.",
            "failure_cases": "Information loss can make certain attachments and detailed realizations harder to produce or recover, contributing to coverage errors and some wrong-attachment outputs in qualitative examples.",
            "uuid": "e8780.5",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Pourdamghani2016 (learned ordering / PBMT)",
            "name_full": "Generating English from abstract meaning representations",
            "brief_description": "A prior AMR-to-text approach that learns to linearize AMR graphs in an order that follows the output sentence and uses a phrase-based MT decoder; ordering learning aims to reduce alignment crossings for the statistical decoder.",
            "citation_title": "Generating English from abstract meaning representations",
            "mention_or_use": "mention",
            "representation_name": "Learned linearization for PBMT (order-following)",
            "representation_description": "Partition/linearize the AMR and learn a child ordering that matches output sentence order to reduce alignment crossings for a phrase-based statistical MT decoder.",
            "graph_type": "AMR graphs",
            "conversion_method": "Learn an ordering of graph fragments/edges based on training data so the resulting linearization aligns well with target sentences; used with phrase-based MT decoding.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported BLEU (Table 3): Dev 27.2, Test 26.9 (their PBMT system trained on LDC2014T12 in the paper's comparison).",
            "comparison_to_others": "The paper contrasts its seq2seq approach with PBMT + learned ordering: seq2seq with DFS + preprocessing + pretraining outperforms PBMT (BLEU 33.8 vs 26.9 test).",
            "advantages": "Specifically optimizes ordering for phrase-based decoders to reduce alignment complexity; effective in SMT pipelines.",
            "disadvantages": "Relies on specialized ordering learning and integration with SMT; less flexible than end-to-end neural seq2seq approaches; requires explicit alignment and ordering models.",
            "failure_cases": "Noted by authors here: SMT-based schemes need good ordering to avoid phrase alignment crossings, whereas seq2seq models can be robust to arbitrary orderings.",
            "uuid": "e8780.6",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Flanigan2016 (tree-to-string)",
            "name_full": "Generation from abstract meaning representation using tree transducers",
            "brief_description": "A tree-to-string transduction approach that extracts tree transduction rules (driven by AMR-to-sentence alignments and POS features) used in a tree-based SMT system for AMR generation.",
            "citation_title": "Generation from abstract meaning representation using tree transducers",
            "mention_or_use": "mention",
            "representation_name": "Tree-to-string transduction (tree-based SMT)",
            "representation_description": "Convert an AMR (or a tree extracted from it) to surface strings using a set of learned transduction rules; uses alignments and POS-based features to drive a tree-based statistical MT decoder.",
            "graph_type": "AMR graphs (converted to tree-like structures/tree transducers)",
            "conversion_method": "Extract tree transduction rules from aligned AMR-sentence pairs; apply transducer rules in decoding to generate sentences.",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported BLEU (Table 3): Dev 23.0, Test 23.0 for TREETOSTR baseline (Flanigan et al., 2016) referenced in the paper.",
            "comparison_to_others": "Seq2seq approach in this paper yields substantially higher BLEU (33.8) than this tree-to-string approach (23.0) in the reported comparisons.",
            "advantages": "Leverages structural transduction rules and alignment features; well-suited to SMT pipelines that exploit syntactic structure.",
            "disadvantages": "Requires extraction of rules and external alignment resources; less end-to-end than neural seq2seq; authors report lower BLEU than their seq2seq+pretraining approach.",
            "failure_cases": "Lower BLEU in reported comparisons; depends heavily on alignment quality and handcrafted transduction heuristics.",
            "uuid": "e8780.7",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        },
        {
            "name_short": "Song2016 (TSP)",
            "name_full": "AMR-to-text generation as a traveling salesman problem",
            "brief_description": "A generation method that partitions AMR graphs into fragments, then casts finding an optimal linearization order as a traveling salesman problem (TSP) over fragments to maximize generation quality for SMT decoders.",
            "citation_title": "AMR-to-text generation as a traveling salesman problem",
            "mention_or_use": "mention",
            "representation_name": "Fragment partitioning + TSP-based linearization",
            "representation_description": "Partition the AMR graph into fragments and treat ordering fragments to form a linear sequence as a TSP instance, solving for an order that optimizes alignment/translation cost for downstream decoders.",
            "graph_type": "AMR graphs (partitioned into fragments)",
            "conversion_method": "Partition graph into fragments and solve a TSP to determine a linear ordering of fragments prior to generation (used with an SMT decoder).",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Reported BLEU (Table 3): Dev 21.1, Test 22.4 for the TSP-based method referenced in the paper.",
            "comparison_to_others": "Per the paper, seq2seq with DFS + preprocessing + pretraining outperforms the TSP approach by a substantial margin (BLEU 33.8 vs 22.4 test).",
            "advantages": "Explicitly searches for a globally coherent fragment ordering which may help SMT decoders.",
            "disadvantages": "Complex partitioning and TSP solving steps; tightly coupled to SMT pipelines and alignment models; poorer BLEU than the seq2seq approach in reported experiments.",
            "failure_cases": "Lower BLEU in comparisons; likely sensitive to fragment partitioning and scoring heuristics.",
            "uuid": "e8780.8",
            "source_info": {
                "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
                "publication_date_yy_mm": "2017-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating English from abstract meaning representations",
            "rating": 2,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "Generation from abstract meaning representation using tree transducers",
            "rating": 2,
            "sanitized_title": "generation_from_abstract_meaning_representation_using_tree_transducers"
        },
        {
            "paper_title": "AMR-to-text generation as a traveling salesman problem",
            "rating": 2,
            "sanitized_title": "amrtotext_generation_as_a_traveling_salesman_problem"
        },
        {
            "paper_title": "Addressing the data sparsity issue in neural AMR parsing",
            "rating": 1,
            "sanitized_title": "addressing_the_data_sparsity_issue_in_neural_amr_parsing"
        },
        {
            "paper_title": "RIGA at SemEval-2016 Task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy",
            "rating": 1,
            "sanitized_title": "riga_at_semeval2016_task_8_impact_of_smatch_extensions_and_characterlevel_neural_translation_on_amr_parsing_accuracy"
        }
    ],
    "cost": 0.01566975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</h1>
<p>Ioannis Konstas ${ }^{\dagger}$ Srinivasan Iyer ${ }^{\dagger}$ Mark Yatskar ${ }^{\dagger}$<br>Yejin Choi ${ }^{\ddagger}$ Luke Zettlemoyer ${ }^{\dagger \S}$<br>${ }^{\dagger}$ Paul G. Allen School of Computer Science \&amp; Engineering, Univ. of Washington, Seattle, WA {ikonstas, sviyer, my89, yejin,lsz}@cs.washington.edu<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence, Seattle, WA<br>lukez@allenai.org</p>
<h4>Abstract</h4>
<p>Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the nonsequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions.</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) is a semantic formalism to encode the meaning of natural language text. As shown in Figure 1, AMR represents the meaning using a directed graph while abstracting away the surface forms in text. AMR has been used as an intermediate meaning representation for several applications including machine translation (MT) (Jones et al., 2012), summarization (Liu et al., 2015), sentence compression (Takase et al., 2016), and event extraction (Huang et al., 2016). While AMR allows for rich semantic representation, annotating training data in AMR is expensive, which in turn limits the use</p>
<p>Obama was elected and his voters celebrated
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example sentence and its corresponding Abstract Meaning Representation (AMR). AMR encodes semantic dependencies between entities mentioned in the sentence, such as "Obama" being the "arg0" of the verb "elected".
of neural network models (Misra and Artzi, 2016; Peng et al., 2017; Barzdins and Gosko, 2016).</p>
<p>In this work, we present the first successful sequence-to-sequence (seq2seq) models that achieve strong results for both text-to-AMR parsing and AMR-to-text generation. Seq2seq models have been broadly successful in many other applications (Wu et al., 2016; Bahdanau et al., 2015; Luong et al., 2015; Vinyals et al., 2015). However, their application to AMR has been limited, in part because effective linearization (encoding graphs as linear sequences) and data sparsity were thought to pose significant challenges. We show that these challenges can be easily overcome, by demonstrating that seq2seq models can be trained using any graph-isomorphic linearization and that unlabeled text can be used to significantly reduce sparsity.</p>
<p>Our approach is two-fold. First, we introduce a novel paired training procedure that enhances both the text-to-AMR parser and AMR-to-text generator. More concretely, first we use self-training to</p>
<p>bootstrap a high quality AMR parser from millions of unlabeled Gigaword sentences (Napoles et al., 2012) and then use the automatically parsed AMR graphs to pre-train an AMR generator. This paired training allows both the parser and generator to learn high quality representations of fluent English text from millions of weakly labeled examples, that are then fine-tuned using human annotated AMR data.</p>
<p>Second, we propose a preprocessing procedure for the AMR graphs, which includes anonymizing entities and dates, grouping entity categories, and encoding nesting information in concise ways, as illustrated in Figure 2(d). This preprocessing procedure helps overcoming the data sparsity while also substantially reducing the complexity of the AMR graphs. Under such a representation, we show that any depth first traversal of the AMR is an effective linearization, and it is even possible to use a different random order for each example.</p>
<p>Experiments on the LDC2015E86 AMR corpus (SemEval-2016 Task 8) demonstrate the effectiveness of the overall approach. For parsing, we are able to obtain competitive performance of 62.1 SMATCH without using any external annotated examples other than the output of a NER system, an improvement of over 10 points relative to neural models with a comparable setup. For generation, we substantially outperform previous best results, establishing a new state of the art of 33.8 BLEU. We also provide extensive ablative and qualitative analysis, quantifying the contributions that come from preprocessing and the paired training procedure.</p>
<h2>2 Related Work</h2>
<p>Alignment-based Parsing Flanigan et al. (2014) (JAMR) pipeline concept and relation identification with a graph-based algorithm. Zhou et al. (2016) extend JAMR by performing the concept and relation identification tasks jointly with an incremental model. Both systems rely on features based on a set of alignments produced using bi-lexical cues and hand-written rules. In contrast, our models train directly on parallel corpora, and make only minimal use of alignments to anonymize named entities.</p>
<p>Grammar-based Parsing Wang et al. (2016) (CAMR) perform a series of shift-reduce transformations on the output of an externally-trained dependency parser, similar to Damonte et al. (2017),</p>
<p>Brandt et al. (2016), Puzikov et al. (2016), and Goodman et al. (2016). Artzi et al. (2015) use a grammar induction approach with Combinatory Categorical Grammar (CCG), which relies on pretrained CCGBank categories, like Bjerva et al. (2016). Pust et al. (2015) recast parsing as a string-to-tree Machine Translation problem, using unsupervised alignments (Pourdamghani et al., 2014), and employing several external semantic resources. Our neural approach is engineering lean, relying only on a large unannotated corpus of English and algorithms to find and canonicalize named entities.</p>
<p>Neural Parsing Recently there have been a few seq2seq systems for AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). Similar to our approach, Peng et al. (2017) deal with sparsity by anonymizing named entities and typing low frequency words, resulting in a very compact vocabulary ( 2 k tokens). However, we avoid reducing our vocabulary by introducing a large set of unlabeled sentences from an external corpus, therefore drastically lowering the out-of-vocabulary rate (see Section 6).</p>
<p>AMR Generation Flanigan et al. (2016) specify a number of tree-to-string transduction rules based on alignments and POS-based features that are used to drive a tree-based SMT system. Pourdamghani et al. (2016) also use an MT decoder; they learn a classifier that linearizes the input AMR graph in an order that follows the output sentence, effectively reducing the number of alignment crossings of the phrase-based decoder. Song et al. (2016) recast generation as a traveling salesman problem, after partitioning the graph into fragments and finding the best linearization order. Our models do not need to rely on a particular linearization of the input, attaining comparable performance even with a per example random traversal of the graph. Finally, all three systems intersect with a large language model trained on Gigaword. We show that our seq2seq model has the capacity to learn the same information as a language model, especially after pretraining on the external corpus.</p>
<p>Data Augmentation Our paired training procedure is largely inspired by Sennrich et al. (2016). They improve neural MT performance for low resource language pairs by using a back-translation MT system for a large monolingual corpus of the target language in order to create synthetic output,</p>
<p>and mixing it with the human translations. We instead pre-train on the external corpus first, and then fine-tune on the original dataset.</p>
<h2>3 Methods</h2>
<p>In this section, we first provide the formal definition of AMR parsing and generation (section 3.1). Then we describe the sequence-to-sequence models we use (section 3.2), graph-to-sequence conversion (section 3.3), and our paired training procedure (section 3.4).</p>
<h3>3.1 Tasks</h3>
<p>We assume access to a training dataset $D$ where each example pairs a natural language sentence $s$ with an AMR $a$. The AMR is a rooted directed acylical graph. It contains nodes whose names correspond to sense-identified verbs, nouns, or AMR specific concepts, for example elect. 01 , Obama, and person in Figure 1. One of these nodes is a distinguished root, for example, the node and in Figure 1. Furthermore, the graph contains labeled edges, which correspond to PropBank-style (Palmer et al., 2005) semantic roles for verbs or other relations introduced for AMR, for example, arg0 or op1 in Figure 1. The set of node and edge names in an AMR graph is drawn from a set of tokens $C$, and every word in a sentence is drawn from a vocabulary $W$.</p>
<p>We study the task of training an AMR parser, i.e., finding a set of parameters $\theta_{P}$ for model $f$, that predicts an AMR graph $\hat{a}$, given a sentence $s$ :</p>
<p>$$
\hat{a}=\underset{a}{\operatorname{argmax}} f\left(a \mid s ; \theta_{P}\right)
$$</p>
<p>We also consider the reverse task, training an AMR generator by finding a set of parameters $\theta_{G}$, for a model $f$ that predicts a sentence $\hat{s}$, given an AMR graph $a$ :</p>
<p>$$
\hat{s}=\underset{s}{\operatorname{argmax}} f\left(s \mid a ; \theta_{G}\right)
$$</p>
<p>In both cases, we use the same family of predictors $f$, sequence-to-sequence models that use global attention, but the models have independent parameters, $\theta_{P}$ and $\theta_{G}$.</p>
<h3>3.2 Sequence-to-sequence Model</h3>
<p>For both tasks, we use a stacked-LSTM sequence-to-sequence neural architecture employed in neural machine translation (Bahdanau et al., 2015; Wu
et al., 2016). ${ }^{1}$ Our model uses a global attention decoder and unknown word replacement with small modifications (Luong et al., 2015).</p>
<p>The model uses a stacked bidirectional-LSTM encoder to encode an input sequence and a stacked LSTM to decode from the hidden states produced by the encoder. We make two modifications to the encoder: (1) we concatenate the forward and backward hidden states at every level of the stack instead of at the top of the stack, and (2) introduce dropout in the first layer of the encoder. The decoder predicts an attention vector over the encoder hidden states using previous decoder states. The attention is used to weigh the hidden states of the encoder and then predict a token in the output sequence. The weighted hidden states, the decoded token, and an attention signal from the previous time step (input feeding) are then fed together as input to the next decoder state. The decoder can optionally choose to output an unknown word symbol, in which case the predicted attention is used to copy a token directly from the input sequence into the output sequence.</p>
<h3>3.3 Linearization</h3>
<p>Our seq2seq models require that both the input and target be presented as a linear sequence of tokens. We define a linearization order for an AMR graph as any sequence of its nodes and edges. A linearization is defined as (1) a linearization order and (2) a rendering function that generates any number of tokens when applied to an element in the linearization order (see Section 4.2 for implementation details). Furthermore, for parsing, a valid AMR graph must be recoverable from the linearization.</p>
<h3>3.4 Paired Training</h3>
<p>Obtaining a corpus of jointly annotated pairs of sentences and AMR graphs is expensive and current datasets only extend to thousands of examples. Neural sequence-to-sequence models suffer from sparsity with so few training pairs. To reduce the effect of sparsity, we use an external unannotated corpus of sentences $S_{e}$, and a procedure which pairs the training of the parser and generator.</p>
<p>Our procedure is described in Algorithm 1, and first trains a parser on the dataset $D$ of pairs of sentences and AMR graphs. Then it uses self-training</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="nx">Paired</span><span class="w"> </span><span class="nx">Training</span><span class="w"> </span><span class="nx">Procedure</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Training</span><span class="w"> </span><span class="nx">set</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">graphs</span><span class="w"> </span><span class="err">\</span><span class="p">((</span><span class="nx">s</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">mathcal</span><span class="p">{</span><span class="nx">D</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">unannotated</span><span class="w"> </span><span class="kd">external</span><span class="w"> </span><span class="nx">corpus</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">a</span>
<span class="w">    </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="kp">self</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">iterations</span><span class="p">,</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">sam</span><span class="o">-</span>
<span class="w">    </span><span class="nx">ple</span><span class="w"> </span><span class="nx">size</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="err">\</span><span class="p">).</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">Model</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">parser</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">AMR</span>
<span class="w">    </span><span class="nx">generator</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">).</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">parser</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">D</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">Self</span><span class="o">-</span><span class="nx">train</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">parser</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">A_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Parse</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Pre</span><span class="o">-</span><span class="nx">train</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">parser</span><span class="p">.</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">parser</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">A_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Fine</span><span class="w"> </span><span class="nx">tune</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">parser</span><span class="p">.</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">parser</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">D</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="o">+</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="mi">10</span><span class="o">^</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="nx">end</span><span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">sample</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">k</span><span class="w"> </span><span class="err">\</span><span class="nx">cdot</span><span class="w"> </span><span class="mi">10</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">new</span><span class="w"> </span><span class="nx">sentences</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Pre</span><span class="o">-</span><span class="nx">train</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">generator</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="nx">A_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Parse</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">generator</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">A_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">},</span><span class="w"> </span><span class="nx">S_</span><span class="p">{</span><span class="nx">e</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nx">N</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">        </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">triangleright</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Fine</span><span class="w"> </span><span class="nx">tune</span><span class="w"> </span><span class="nx">AMR</span><span class="w"> </span><span class="nx">generator</span><span class="p">.</span>
<span class="w">    </span><span class="err">\</span><span class="p">(</span><span class="mi">12</span><span class="p">:</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Train</span><span class="w"> </span><span class="nx">generator</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">D</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">using</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">parameters</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">P</span><span class="p">},</span><span class="w"> </span><span class="err">\</span><span class="nx">theta_</span><span class="p">{</span><span class="nx">G</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>to improve the initial parser. Every iteration of self-training has three phases: (1) parsing samples from a large, unlabeled corpus $S_{e}$, (2) creating a new set of parameters by training on $S_{e}$, and (3) fine-tuning those parameters on the original paired data. After each iteration, we increase the size of the sample from $S_{e}$ by an order of magnitude. After we have the best parser from self-training, we use it to label AMRs for $S_{e}$ and pre-train the generator. The final step of the procedure fine-tunes the generator on the original dataset $D$.</p>
<h2>4 AMR Preprocessing</h2>
<p>We use a series of preprocessing steps, including AMR linerization, anonymization, and other modifications we make to sentence-graph pairs. Our methods have two goals: (1) reduce the complexity of the linearized sequences to make learning easier while maintaining enough original information, and (2) address sparsity from certain open class vocabulary entries, such as named entities (NEs) and quantities. Figure 2(d) contains example inputs and outputs with all of our preprocessing techniques.</p>
<p>Graph Simplification In order to reduce the overall length of the linearized graph, we first remove variable names and the instance-of relation ( / ) before every concept. In case of re-entrant nodes we replace the variable mention with its co-referring concept. Even though this replacement incurs loss of information, often the
surrounding context helps recover the correct realization, e.g., the possessive role : poss in the example of Figure 1 is strongly correlated with the surface form his. Following Pourdamghani et al. (2016) we also remove senses from all concepts for AMR generation only. Figure 2(a) contains an example output after this stage.</p>
<h3>4.1 Anonymization of Named Entities</h3>
<p>Open-class types including NEs, dates, and numbers account for $9.6 \%$ of tokens in the sentences of the training corpus, and $31.2 \%$ of vocabulary $W .83 .4 \%$ of them occur fewer than 5 times in the dataset. In order to reduce sparsity and be able to account for new unseen entities, we perform extensive anonymization.</p>
<p>First, we anonymize sub-graphs headed by one of AMR's over 140 fine-grained entity types that contain a : name role. This captures structures referring to entities such as person, country, miscellaneous entities marked with <em>-enitity, and typed numerical values, </em>-quantity. We exclude date entities (see the next section). We then replace these sub-graphs with a token indicating fine-grained type and an index, $i$, indicating it is the $i$ th occurrence of that type. ${ }^{2}$ For example, in Figure 2 the sub-graph headed by country gets replaced with country_0.</p>
<p>On the training set, we use alignments obtained using the JAMR aligner (Flanigan et al., 2014) and the unsupervised aligner of Pourdamghani et al. (2014) in order to find mappings of anonymized subgraphs to spans of text and replace mapped text with the anonymized token that we inserted into the AMR graph. We record this mapping for use during testing of generation models. If a generation model predicts an anonymization token, we find the corresponding token in the AMR graph and replace the model's output with the most frequent mapping observed during training for the entity name. If the entity was never observed, we copy its name directly from the AMR graph.</p>
<p>Anonymizing Dates For dates in AMR graphs, we use separate anonymization tokens for year, month-number, month-name, day-number and day-name, indicating whether the date is mentioned by word or by number. ${ }^{3}$ In AMR gener-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>US officials held an expert group meeting in January 2002 in New York.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Preprocessing methods applied to sentence (top row) - AMR graph (left column) pairs. Sentence-graph pairs after (a) graph simplification, (b) named entity anonymization, (c) named entity clustering, and (d) insertion of scope markers.
ation, we render the corresponding format when predicted. Figure 2(b) contains an example of all preprocessing up to this stage.</p>
<p>Named Entity Clusters When performing AMR generation, each of the AMR fine-grained entity types is manually mapped to one of the four coarse entity types used in the Stanford NER system (Finkel et al., 2005): person, location, organization and misc. This reduces the sparsity associated with many rarely occurring entity types. Figure 2 (c) contains an example with named entity clusters.</p>
<p>NER for Parsing When parsing, we must normalize test sentences to match our anonymized training data. To produce fine-grained named entities, we run the Stanford NER system and first try to replace any identified span with a fine-grained category based on alignments observed during training. If this fails, we anonymize the sentence using the coarse categories predicted by the NER system, which are also categories in AMR. After parsing, we deterministically generate AMR for anonymizations using the corresponding text span.</p>
<h3>4.2 Linearization</h3>
<p>Linearization Order Our linearization order is defined by the order of nodes visited by depth first search, including backward traversing steps. For example, in Figure 2, starting at meet the order contains meet, :ARGO, person, :ARG1-of, expert, :ARG2-of, group, :ARG2-of, :ARG1-of, :ARGO. ${ }^{4}$ The order traverses children in the sequence they are presented in the AMR. We consider alternative orderings of children in Section 7 but always follow the pattern demonstrated above.</p>
<p>Rendering Function Our rendering function marks scope, and generates tokens following the pre-order traversal of the graph: (1) if the element is a node, it emits the type of the node. (2) if the element is an edge, it emits the type of the edge and then recursively emits a bracketed string for the (concept) node immediately after it. In case the node has only one child we omit the scope markers (denoted with left " (", and right ") " parentheses), thus significantly reducing the number of generated tokens. Figure 2(d) contains an example showing all of the preprocessing techniques and scope markers that we use in our full model.</p>
<h2>5 Experimental Setup</h2>
<p>We conduct all experiments on the AMR corpus used in SemEval-2016 Task 8 (LDC2015E86), which contains 16,833/1,368/1,371 train/dev/test examples. For the paired training procedure of Algorithm 1, we use Gigaword as our external corpus and sample sentences that only contain words from the AMR corpus vocabulary $W$. We subsampled the original sentence to ensure there is no overlap with the AMR training or test sets. Table 2</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Prec</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Prec</td>
<td style="text-align: center;">Rec</td>
<td style="text-align: center;">F1</td>
</tr>
<tr>
<td style="text-align: left;">SBMT (Pust et al., 2015)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">CAMR (Wang et al., 2016)</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">CCG* (Artzi et al., 2015)</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">66.3</td>
</tr>
<tr>
<td style="text-align: left;">JAMR (Flanigan et al., 2014)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">GIGA-20M</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">62.1</td>
</tr>
<tr>
<td style="text-align: left;">GIGA-2M</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">60.2</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: left;">GIGA-200k</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: left;">AMR-ONLY</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">53.1</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: left;">SEQ2SEQ (Peng et al., 2017)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">CHAR-LSTM (Barzdins and Gosko, 2016)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.0</td>
</tr>
</tbody>
</table>
<p>Table 1: SMATCH scores for AMR Parsing. *Reported numbers are on the newswire portion of a previous release of the corpus (LDC2014T12).
summarizes statistics about the original dataset and the extracted portions of Gigaword. We evaluate AMR parsing with SMATCH (Cai and Knight, 2013), and AMR generation using BLEU (Papineni et al., 2002) ${ }^{5}$.</p>
<p>We validated word embedding sizes and RNN hidden representation sizes by maximizing AMR development set performance (Algorithm 1 - line 1). We searched over the set ${128,256,500$, $1024}$ for the best combinations of sizes and set both to 500 . Models were trained by optimizing cross-entropy loss with stochastic gradient descent, using a batch size of 100 and dropout rate of 0.5 . Across all models when performance does not improve on the AMR dev set, we decay the learning rate by 0.8 .</p>
<p>For the initial parser trained on the AMR corpus, (Algorithm 1 - line 1), we use a single stack version of our model, set initial learning rate to 0.5 and train for 60 epochs, taking the best performing model on the development set. All subsequent models benefited from increased depth and we used 2-layer stacked versions, maintaining the same embedding sizes. We set the initial Gigaword sample size to $k=200,000$ and executed a maximum of 3 iterations of self-training. For pretraining the parser and generator, (Algorithm 1 lines 4 and 9), we used an initial learning rate of 1.0 , and ran for 20 epochs. We attempt to fine-tune the parser and generator, respectively, after every epoch of pre-training, setting the initial learning rate to 0.1 . We select the best performing model on the development set among all of these fine-tuning</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: LDC2015E86 AMR training set, GIGA-200k, GIGA-2M and GIGA-20M statistics; OOV@1 and OOV@5 are the out-of-vocabulary rates on the NL side with thresholds of 1 and 5, respectively. Vocabulary sizes are 13027 tokens for the AMR side, and 17319 tokens for the NL side.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Dev</th>
<th style="text-align: right;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GIGA-20M</td>
<td style="text-align: right;">33.1</td>
<td style="text-align: right;">33.8</td>
</tr>
<tr>
<td style="text-align: left;">GIGA-2M</td>
<td style="text-align: right;">31.8</td>
<td style="text-align: right;">32.3</td>
</tr>
<tr>
<td style="text-align: left;">GIGA-200k</td>
<td style="text-align: right;">27.2</td>
<td style="text-align: right;">27.4</td>
</tr>
<tr>
<td style="text-align: left;">AMR-ONLY</td>
<td style="text-align: right;">21.7</td>
<td style="text-align: right;">22.0</td>
</tr>
<tr>
<td style="text-align: left;">PBMT* (Pourdamghani et al., 2016)</td>
<td style="text-align: right;">27.2</td>
<td style="text-align: right;">26.9</td>
</tr>
<tr>
<td style="text-align: left;">TSP (Song et al., 2016)</td>
<td style="text-align: right;">21.1</td>
<td style="text-align: right;">22.4</td>
</tr>
<tr>
<td style="text-align: left;">TREETOSTR (Flanigan et al., 2016)</td>
<td style="text-align: right;">23.0</td>
<td style="text-align: right;">23.0</td>
</tr>
</tbody>
</table>
<p>Table 3: BLEU results for AMR Generation. *Model has been trained on a previous release of the corpus (LDC2014T12).
attempts. During prediction we perform decoding using beam search and set the beam size to 5 both for parsing and generation.</p>
<h2>6 Results</h2>
<p>Parsing Results Table 1 summarizes our development results for different rounds of self-training and test results for our final system, self-trained on 200k, 2M and 20M unlabeled Gigaword sentences. Through every round of self-training, our</p>
<p>parser improves. Our final parser outperforms comparable seq2seq and character LSTM models by over 10 points. While much of this improvement comes from self-training, our model without Gigaword data outperforms these approaches by 3.5 points on F1. We attribute this increase in performance to different handling of preprocessing and more careful hyper-parameter tuning. All other models that we compare against use semantic resources, such as WordNet, dependency parsers or CCG parsers (models marked with * were trained with less data, but only evaluate on newswire text; the rest evaluate on the full test set, containing text from blogs). Our full models outperform JAMR, a graph-based model but still lags behind other parser-dependent systems (CAMR ${ }^{6}$ ), and resource heavy approaches (SBMT).</p>
<p>Generation Results Table 3 summarizes our AMR generation results on the development and test set. We outperform all previous state-of-theart systems by the first round of self-training and further improve with the next rounds. Our final model trained on GIGA-20M outperforms TSP and TreeToStr trained on LDC2015E86, by over 9 BLEU points. ${ }^{7}$ Overall, our model incorporates less data than previous approaches as all reported methods train language models on the whole Gigaword corpus. We leave scaling our models to all of Gigaword for future work.</p>
<p>Sparsity Reduction Even after anonymization of open class vocabulary entries, we still encounter a great deal of sparsity in vocabulary given the small size of the AMR corpus, as shown in Table 2. By incorporating sentences from Gigaword we are able to reduce vocabulary sparsity dramatically, as we increase the size of sampled sentences: the out-of-vocabulary rate with a threshold of 5 reduces almost 5 times for GIGA-20M.</p>
<p>Preprocessing Ablation Study We consider the contribution of each main component of our preprocessing stages while keeping our linearization order identical. Figure 2 contains examples for each setting of the ablations we evaluate on. First we evaluate using linearized graphs without paren-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4: BLEU scores for AMR generation ablations on preprocessing (DEV set).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Prec</th>
<th style="text-align: center;">Rec</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FULL</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: left;">FULL - SCOPE</td>
<td style="text-align: center;">22.7</td>
<td style="text-align: center;">54.2</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: left;">FULL - SCOPE - NE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: SMATCH scores for AMR parsing ablations on preprocessing (DEV set).
theses for indicating scope, Figure 2(c), then without named entity clusters, Figure 2(b), and additionally without any anonymization, Figure 2(a).</p>
<p>Tables 4 summarizes our evaluation on the AMR generation. Each components is required, and scope markers and anonymization contribute the most to overall performance. We suspect without scope markers our seq2seq models are not as effective at capturing long range semantic relationships between elements of the AMR graph. We also evaluated the contribution of anonymization to AMR parsing (Table 5). Following previous work, we find that seq2seq-based AMR parsing is largely ineffective without anonymization (Peng et al., 2017).</p>
<h2>7 Linearization Evaluation</h2>
<p>In this section we evaluate three strategies for converting AMR graphs into sequences in the context of AMR generation and show that our models are largely agnostic to linearization orders. Our results argue, unlike SMT-based AMR generation methods (Pourdamghani et al., 2016), that seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.</p>
<h3>7.1 Linearization Orders</h3>
<p>All linearizations we consider use the pattern described in Section 4.2, but differ on the order in which children are visited. Each linearization generates anonymized, scope-marked output (see Section 4), of the form shown in Figure 2(d).</p>
<p>Human The proposal traverses children in the order presented by human authored AMR annotations exactly as shown in Figure 2(d).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Linearization Order</th>
<th style="text-align: center;">BLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HUMAN</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: left;">Global-RANDOM</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: left;">RANDOM</td>
<td style="text-align: center;">20.3</td>
</tr>
</tbody>
</table>
<p>Table 6: BLEU scores for AMR generation for different linearization orders (DEV set).</p>
<p>Global-Random We construct a random global ordering of all edge types appearing in AMR graphs and re-use it for every example in the dataset. We traverse children based on the position in the global ordering of the edge leading to a child.</p>
<p>Random For each example in the dataset we traverse children following a different random order of edge types.</p>
<h3>7.2 Results</h3>
<p>We present AMR generation results for the three proposed linearization orders in Table 6. Random linearization order performs somewhat worse than traversing the graph according to Human linearization order. Surprisingly, a per example random linearization order performs nearly identically to a global random order, arguing seq2seq models can learn to ignore artifacts of the conversion of graphs to linear sequences.</p>
<h2>Human-authored AMR leaks information</h2>
<p>The small difference between random and globalrandom linearizations argues that our models are largely agnostic to variation in linearization order. On the other hand, the model that follows the human order performs better, which leads us to suspect it carries extra information not apparent in the graphical structure of the AMR.</p>
<p>To further investigate, we compared the relative ordering of edge pairs under the same parent to the relative position of children nodes derived from those edges in a sentence, as reported by JAMR alignments. We found that the majority of pairs of AMR edges (57.6\%) always occurred in the same relative order, therefore revealing no extra generation order information. ${ }^{8}$ Of the examples corresponding to edge pairs that showed variation, $70.3 \%$ appeared in an order consistent with the order they were realized in the sentence. The relative ordering of some pairs of AMR edges was</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 7: Error analysis for AMR generation on a sample of 50 examples from the development set.
particularly indicative of generation order. For example, the relative ordering of edges with types location and time, was $17 \%$ more indicative of the generation order than the majority of generated locations before time. ${ }^{9}$</p>
<p>To compare to previous work we still report results using human orderings. However, we note that any practical application requiring a system to generate an AMR representation with the intention to realize it later on, e.g., a dialog agent, will need to be trained either using consistent, or randomderived linearization orders. Arguably, our models are agnostic to this choice.</p>
<h2>8 Qualitative Results</h2>
<p>Figure 3 shows example outputs of our full system. The generated text for the first graph is nearly perfect with only a small grammatical error due to anonymization. The second example is more challenging, with a deep right-branching structure, and a coordination of the verbs stabilize and push in the subordinate clause headed by state. The model omits some information from the graph, namely the concepts terrorist and virus. In the third example there are greater parts of the graph that are missing, such as the whole sub-graph headed by expert. Also the model makes wrong attachment decisions in the last two sub-graphs (it is the evidence that is unimpeachable and irrefutable, and not the equipment), mostly due to insufficient annotation (thing) thus making their generation harder.</p>
<p>Finally, Table 7 summarizes the proportions of error types we identified on 50 randomly selected examples from the development set. We found that the generator mostly suffers from coverage issues,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>an inability to mention all tokens in the input, followed by fluency mistakes, as illustrated above. Attachment errors are less frequent, which supports our claim that the model is robust to graph linearization, and can successfully encode long range dependency information between concepts.</p>
<h2>9 Conclusions</h2>
<p>We applied sequence-to-sequence models to the tasks of AMR parsing and AMR generation, by carefully preprocessing the graph representation and scaling our models via pretraining on millions of unlabeled sentences sourced from Gigaword corpus. Crucially, we avoid relying on resources such as knowledge bases and externally trained parsers. We achieve competitive results for the parsing task (SMATCH 62.1) and state-of-theart performance for generation (BLEU 33.8).</p>
<p>For future work, we would like to extend our work to different meaning representations such as the Minimal Recursion Semantics (MRS; Copestake et al. (2005)). This formalism tackles certain linguistic phenomena differently from AMR (e.g., negation, and co-reference), contains explicit annotation on concepts for number, tense and case, and finally handles multiple languages ${ }^{10}$ (Bender, 2014). Taking a step further, we would like to apply our models on Semantics-Based Machine Translation using MRS as an intermediate representation between pairs of languages, and investigate the added benefit compared to directly translating the surface strings, especially in the case of distant language pairs such as English and Japanese (Siegel, 2000).</p>
<h2>Acknowledgments</h2>
<p>The research was supported in part by DARPA under the DEFT program through AFRL (FA8750-13-2-0019) and the CwC program through ARO (W911NF-15-1-0543), the ARO (W911NF-16-10121), the NSF (IIS-1252835, IIS-1562364, IIS1524371), an Allen Distinguished Investigator Award, and gifts by Google and Facebook. The authors thank Rik Koncel-Kedziorski and the UW NLP group for helpful discussions, and the anonymous reviewers for their thorough and helpful comments.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup><code>limit
    :arg0 ( treaty :arg0-of ( control :arg1 arms ) )
    :arg1 ( number
    :arg1 ( weapon :mod conventional
        :arg1-of ( deploy
        :arg2 ( relative-pos :op1 loc_0 :dir west )
        :arg1-of possible ) ) )
REF: the arms control treaty limits the number of
    conventional weapons that can be deployed west of
    the Ural Mountains .
    SYS: the arms control treaty limits the number of
    conventional weapons that can be deployed west of
    Ural Mountains .
    COMMENT: disfluency
    state
    :arg0 report
    :arg1 ( obligate :arg1 ( government-organization
        :arg0-of ( govern :arg1 loc_0 ) )
        :arg2 ( help :arg1 ( and
            :op1 ( stabilize :arg1 ( state :mod weak ) )
            :op2 ( push :arg1 ( regulate
            :mod international :arg0-of ( stop
            :arg1 terrorist
            :arg2 ( use
            :arg1 ( information
                :arg2-of ( available :arg3-of free ))
            :arg2 ( and
                :op1 ( create :arg1 ( form
                    :domain ( warfare
                    :mod biology :example ( version
                    :arg1-of modify :poss other_1 ) )
                    :mod new ) )
                :op2 ( unleash :arg1 form )
            ) ) ) ) ) ) ) )
REF: the report stated British government must
help to stabilize weak states and push for
international regulations that would stop
terrorists using freely available information to
create and unleash new forms of biological
warfare such as a modified version of the
influenza virus .
    SYS: the report stated that the Britain
    government must help stabilize the weak states
    and push international regulations to stop the
    use of freely available information to create a
    form of new biological warfare such as the
    modified version of the influenza .
    COMMENT: coverage , disfluency, attachment
    state
    :arg0 ( person
        :arg0-of ( have-org-role
        :arg1 ( committee :mod technical )
        :arg3 ( expert
            :arg1 person
            :arg2 missile
            :mod loc_0 ) ) )
    :arg1 ( evidence
        :arg0 equipment
        :arg1 ( plan :arg1 ( transfer :arg1 ( contrast
            :arg1 ( missile :mod ( just :polarity - ) )
            :arg2 ( capable
                :arg1 thing
                :arg2 ( make :arg1 missile ) ) ) ) )
    :mod ( impeach :polarity - :arg1 thing )
    :mod ( refute :polarity - :arg1 thing ) )
    REF: a technical committee of Indian missile
    experts stated that the equipment was
    unimpeachable and irrefutable evidence of a plan
    to transfer not just missiles but missile-making
    capability.
    SYS: a technical committee expert on the
    technical committee stated that the equipment is
    not impeach , but it is not refutes .
    COMMENT: coverage , disfluency, attachment
    Figure 3: Linearized AMR after preprocessing,
    reference sentence, and output of the generator.
    We mark with colors common error types: disflu-
    ency, coverage (missing information from the in-
    put graph), and attachment (implying a semantic
    relation from the AMR between incorrect entities).</code></p>
<h2>References</h2>
<p>Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage CCG semantic parsing with AMR. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 1699-1710. http://aclweb.org/anthology/D15-1198.</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 2015 International Conference on Learning Representations. CBLS, San Diego, California. http://arxiv.org/abs/1409.0473.</p>
<p>Guntis Barzdins and Didzis Gosko. 2016. RIGA at SemEval-2016 Task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1143-1147. http://www.aclweb.org/anthology/S16-1176.</p>
<p>Emily M. Bender. 2014. Language CoLLAGE: Grammatical description with the LinGO grammar matrix. In Proceedings of the 9th International Conference on Language Resources and Evaluation. Reykjavik, Iceland, pages 2447-2451.</p>
<p>Johannes Bjerva, Johan Bos, and Hessel Haagsma. 2016. The Meaning Factory at SemEval-2016 Task 8: Producing AMRs with Boxer. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1179-1184. http://www.aclweb.org/anthology/S16-1182.</p>
<p>Lauritz Brandt, David Grimm, Mengfei Zhou, and Yannick Versley. 2016. ICL-HD at SemEval-2016 Task 8: Meaning representation parsing - augmenting AMR parsing with a preposition semantic role labeling neural network. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1160-1166. http://www.aclweb.org/anthology/S16-1179.</p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Sofia, Bulgaria, pages 748-752. http://www.aclweb.org/anthology/P132131.</p>
<p>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal Recursion Semantics: An introduction. Research on Language and Computation 3(2):281-332. https://doi.org/10.1007/s11168-006-6327-9.</p>
<p>Marco Damonte, Shay B. Cohen, and Giorgio Satta. 2017. An incremental parser for abstract meaning
representation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Valencia, Spain, pages 536546. http://www.aclweb.org/anthology/E17-1051.</p>
<p>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by Gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Ann Arbor, Michigan, pages 363-370. https://doi.org/10.3115/1219840.1219885.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from abstract meaning representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, San Diego, California, pages 731-739. http://www.aclweb.org/anthology/N16-1087.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Baltimore, Maryland, pages 1426-1436. http://www.aclweb.org/anthology/P14-1134.</p>
<p>James Goodman, Andreas Vlachos, and Jason Naradowsky. 2016. UCL+Sheffield at SemEval-2016 Task 8: Imitation learning for AMR parsing with an alpha-bound. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1167-1172. http://www.aclweb.org/anthology/S16-1180.</p>
<p>Lifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng Ji, Clare R. Voss, Jiawei Han, and Avirup Sil. 2016. Liberal event extraction and event schema induction. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Berlin, Germany, pages 258-268. http://www.aclweb.org/anthology/P16-1025.</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-Based Machine Translation with Hyperedge Replacement Grammars. In Proceedings of the 2012 International Conference on Computational Linguistics. Bombay, India, pages 1359-1376. http://www.aclweb.org/anthology/C12-1083.</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, OndÅ™ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine</p>
<p>translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Prague, Czech Republic, pages 177-180. http://dl.acm.org/citation.cfm?id=1557769.1557821.</p>
<p>Fei Liu, Jeffrey Flanigan, Sam Thomson, Norman Sadeh, and Noah A. Smith. 2015. Toward abstractive summarization using semantic representations. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Denver, Colorado, pages 1077-1086. http://www.aclweb.org/anthology/N15-1114.</p>
<p>Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 14121421. http://aclweb.org/anthology/D15-1166.</p>
<p>Dipendra Kumar Misra and Yoav Artzi. 2016. Neural shift-reduce CCG semantic parsing. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1775-1786. https://aclweb.org/anthology/D161183.</p>
<p>Courtney Napoles, Matthew Gormley, and Benjamin Van Durme. 2012. Annotated Gigaword. In Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction. Association for Computational Linguistics, MontrÃ©al, Canada, pages 95-100. http://www.aclweb.org/anthology/W12-3018.</p>
<p>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics 31(1):71-106. http://www.cs.rochester.edu/ gildea/palmer-propbank-cl.pdf.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, pages 311-318. https://doi.org/10.3115/1073083.1073135.</p>
<p>Xiaochang Peng, Chuan Wang, Daniel Gildea, and Nianwen Xue. 2017. Addressing the data sparsity issue in neural AMR parsing. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Valencia, Spain, pages 366-375. http://www.aclweb.org/anthology/E17-1035.</p>
<p>Nima Pourdamghani, Yang Gao, Ulf Hermjakob, and Kevin Knight. 2014. Aligning English strings with
abstract meaning representation graphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Doha, Qatar, pages 425429. http://www.aclweb.org/anthology/D14-1048.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from abstract meaning representations. In Proceedings of the 9th International Natural Language Generation conference. Association for Computational Linguistics, Edinburgh, UK, pages 21-25. http://anthology.aclweb.org/W16-6603.</p>
<p>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing english into abstract meaning representation using syntaxbased machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 11431154. https://aclweb.org/anthology/D/D15/D151136.</p>
<p>Yevgeniy Puzikov, Daisuke Kawahara, and Sadao Kurohashi. 2016. M2L at SemEval-2016 Task 8: AMR parsing with neural networks. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1154-1159. http://www.aclweb.org/anthology/S16-1178.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Berlin, Germany, pages 86-96. http://www.aclweb.org/anthology/P16-1009.</p>
<p>Melanie Siegel. 2000. HPSG Analysis of Japanese, Springer Berlin Heidelberg, pages 264-279.</p>
<p>Linfeng Song, Yue Zhang, Xiaochang Peng, Zhiguo Wang, and Daniel Gildea. 2016. AMR-to-text generation as a traveling salesman problem. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2084-2089. https://aclweb.org/anthology/D161224.</p>
<p>Sho Takase, Jun Suzuki, Naoaki Okazaki, Tsutomu Hirao, and Masaaki Nagata. 2016. Neural headline generation on abstract meaning representation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 1054-1059. https://aclweb.org/anthology/D16-1112.</p>
<p>Oriol Vinyals, Å ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar as a foreign language. In Proceedings of the</p>
<p>28th International Conference on Neural Information Processing Systems, MIT Press, pages 27732781. http://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf.</p>
<p>Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji, and Nianwen Xue. 2016. CAMR at SemEval-2016 Task 8: An extended transition-based AMR parser. In Proceedings of the 10th International Workshop on Semantic Evaluation. Association for Computational Linguistics, San Diego, California, pages 1173-1178. http://www.aclweb.org/anthology/S161181 .</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR abs/1609.08144. http://arxiv.org/abs/1609.08144.</p>
<p>Junsheng Zhou, Feiyu Xu, Hans Uszkoreit, Weiguang QU, Ran Li, and Yanhui Gu. 2016. AMR parsing with an incremental joint model. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 680-689. https://aclweb.org/anthology/D16-1065.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10} \mathrm{~A}$ list of actively maintained languages can be found here: http://moin.delph-in.net/ GrammarCatalogue&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{9}$ Consider the sentences "She went to school in New York two years ago", and "Two years ago, she went to school in New York", where "two year ago" is the time modifying constituent for the verb went and "New York" is the location modifying constituent of went.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1914 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1914</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1914</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-281525459</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.21006v1.pdf" target="_blank">AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> We address natural language pick-and-place in unseen, unpredictable indoor environments with AnywhereVLA, a modular framework for mobile manipulation. A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy. An approach planner then selects visibility and reachability aware pre grasp base poses. For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals. The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation. We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion. In this setting, the system achieves a $46\%$ overall task success rate while maintaining throughput on embedded compute. By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1914.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1914.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AnywhereVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular onboard system that combines classical SLAM, frontier-based active exploration, and a fine-tuned vision-language-action (VLA) manipulation model to perform natural-language pick-and-place in unseen large-scale indoor environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AnywhereVLA (system-level pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular pipeline integrating a 3D semantic mapping module (LiDAR+RGB SLAM + 2D detector → 3D semantic point cloud), an Active Environment Exploration (frontier-based) planner that is conditioned by parsed language target classes, an Approach module for visibility/reachability-aware base pose selection, and a VLA Manipulation head (fine-tuned SmolVLA) that outputs manipulator actions. Processes multimodal inputs (LiDAR, RGB images) and language instructions; couples classic geometric navigation with a language-conditioned policy for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>hybrid: system leverages pretrained vision-language models (VLMs) for manipulation head (SmolVLA) and classical SLAM components for spatial memory; manipulation head was fine-tuned on robot trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>The paper states the approach draws on VLMs and co-fine-tuning on robotic trajectory data and Internet-scale vision-language tasks (general formulation), but for AnywhereVLA specifically the manipulation head (SmolVLA) was fine-tuned on 50 pick-and-place episodes collected with the SO-101 manipulator; no detailed inventory of pretraining corpora is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Language-conditioned mobile pick-and-place / mobile manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Natural language pick-and-place in unseen, unpredictable indoor multi-room environments. Action space includes continuous base motion (wheel actuation for navigation) and continuous manipulator joint motions for grasp/place; environment is real-world multi-room lab with static scenes and normal human motion; objects are household-type on support surfaces; deployment is on a real mobile manipulator (HermesBot with SO-101 arm) running fully onboard.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper claims semantic alignment is achieved by leveraging pretrained VLMs' visual-linguistic knowledge and by fine-tuning on robot pick-and-place trajectories, but it does not quantify overlap between pretraining data and in-environment objects/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>System-level overall task success rate: 46% in multi-room lab evaluations; VLA Manipulation module success rate reported as 80% (Table II). System maintains ≥10 Hz inference and completes tasks under 2.5 minutes in an 80 m^2 area in reported runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported (no baseline ablation using a non-language-pretrained manipulation policy or random initialization is presented).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported as a quantitative comparison. The manipulation head was fine-tuned with 50 pick-and-place episodes (small dataset), but no direct sample-efficiency comparison versus non-pretrained or vision-only baselines is given.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported. The paper does not present attention maps or attention-based analyses for the VLA model.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported. No analysis of embedding clustering or feature-space semantic organization is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Descriptive evidence: authors state the SmolVLA manipulation head was fine-tuned to 'ground local visual context and sub-goals into grasp and place proposals' and that the system inherits geometry-based navigation reliability plus language-conditioned manipulation agility; however, empirical mechanistic evidence mapping verbs to affordances/motor primitives is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported. The paper does not analyze low-level vs high-level feature contributions nor indicate which feature levels benefit most from language pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Qualitative statements: successful transfer is aided by combining geometric spatial memory (SLAM + frontier exploration) with a fine-tuned VLA for manipulation; limitations occur for compositional spatial constraints in language (e.g., multi-step relational instructions), indicating transfer fails when instructions require relational/spatial-prepositional grounding beyond simple object-class targets.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not quantified. The paper evaluates in unseen environments but does not report separate metrics for objects seen during pretraining/fine-tuning versus novel objects.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot fine-tuning: manipulation head fine-tuned on 50 demonstrations; no zero-shot manipulation performance numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported. No ablation/freezing/probing of layers or components is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly quantified. The paper reports a limitation: the system will explore to find the first instance of a target class regardless of relational spatial qualifiers in the instruction, indicating failure modes on compositional instructions, but no evidence that pretraining harms performance relative to alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported. The paper does not include a comparison to vision-only pretrained models (e.g., ImageNet) or purely geometric controllers for the manipulation head.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported. No analysis of representational/training dynamics over fine-tuning epochs is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported. No PCA or intrinsic-dimension analyses are included.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1914.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SmolVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SmolVLA (450M parameter VLA model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compact (≈450M-parameter) vision-language-action model designed for efficient real-time robotic manipulation; in this paper it is fine-tuned as the manipulation head to produce grasp and place proposals from local visual context and language-conditioned subgoals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smolvla: A vision-language-action model for affordable and efficient robotics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SmolVLA (450M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Lightweight VLA model (≈450M parameters) intended to map multimodal visual inputs and language instructions to action outputs for manipulation; characterized in related work as competitive with larger VLAs and efficient for embedded deployment. In this work it is fine-tuned to output manipulator motor commands/grasp/place proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (implied): SmolVLA is described as a VLA derived from pretrained VLMs; the paper fine-tunes an existing SmolVLA instance on robot trajectory data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified in detail in this paper. The related-work description implies prior SmolVLA models leverage vision-language pretraining corpora; for fine-tuning here SmolVLA was trained on 50 pick-and-place episodes collected on an SO-101 manipulator (robot trajectory data).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Pick-and-place manipulation (language-conditioned)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Local manipulation tasks: generating grasps and place proposals for a tabletop or other support-surface object from wrist/base/third-person cameras and language instruction; executed on a real SO-101 manipulator mounted on a mobile base; continuous arm joint control output.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not quantitatively analyzed. The paper describes the intent to ground language into local visual context via fine-tuning, but provides no measurement of alignment between pretraining semantics and task objects/actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Module-level VLA Manipulation success rate: 80% (Table II) when used within AnywhereVLA after fine-tuning. No isolated metric for the SmolVLA component outside the integrated stack is given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided as a comparative analysis; reported fine-tuning dataset size: 50 pick-and-place episodes (small).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional evidence only: fine-tuned SmolVLA produced grasp and place proposals in real-world runs leading to 80% manipulation-module success, implying practical grounding, but no mechanistic analyses linking language verbs to affordance representations are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not explicitly studied; SmolVLA is described as efficient for embedded deployment and fine-tunable to platform trajectories, implying transfer depends on platform demonstration data and visual viewpoint coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Few-shot fine-tuning demonstrated (50 episodes), but no explicit zero-shot claims in this paper for SmolVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Fine-tuning schedule described (hyperparameters), but no representational temporal analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1914.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0 (pi-zero)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0: A vision-language-action flow model for general robot control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flow matching–based action generation VLA model that integrates Internet-scale semantic knowledge and demonstration data for zero-shot dexterous manipulation (described in related work and cited by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π 0 : A vision-language-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA flow model that generates continuous actions conditioned on visual and language inputs; described as incorporating large internet-scale semantic knowledge and demonstration data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Described as leveraging Internet-scale vision-language data plus demonstration data (vision-language and robot trajectory co-training implied), per the related-work summary in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed in this paper beyond the general statement of Internet-scale semantic knowledge and demonstration trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Dexterous manipulation / general robot control (reported in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>High-level: dexterous manipulation tasks across robots; not experimentally used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned as leveraging large-scale semantic knowledge; the present paper does not provide measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in this paper (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>π0 is described in related work as enabling zero-shot dexterous manipulation, but this paper provides no empirical values.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1914.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0.5: Co-training VLA for cross-domain generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of π0 that co-trains on heterogeneous multimodal datasets to improve generalization across tasks and robots (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA variant described as co-training on heterogeneous multimodal datasets (flow/diffusion style) to achieve broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Co-training on multimodal datasets (vision-language + demonstrations) — described in related work summary but not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified beyond 'heterogeneous multimodal datasets'.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Generalization across manipulation tasks/robots (referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned conceptually; no measurements in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Claimed improved generalization in cited work; no numbers provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1914.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BUMBLE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BUMBLE: VLM-based end-to-end building-wide mobile manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-model-based end-to-end system for building-wide mobile manipulation that demonstrates strong navigation and manipulation across many objects but depends on a known map and preprovided landmarks (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BUMBLE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An end-to-end VLM-based mobile manipulation framework that integrates vision-language reasoning with navigation and manipulation to operate at building scale; presented as requiring a full known map and landmarks in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language model based (described as VLM-driven), details not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here; cited work likely uses VLM pretraining on large image-text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Building-wide mobile manipulation (navigation + manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Large-scale navigation and manipulation of previously unseen objects across a known mapped environment; cited work relies on prior map/landmarks instead of autonomous exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Paper notes BUMBLE relies on known map and landmarks; no alignment metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Cited work described as showing strong performance; no quantitative metrics are reproduced in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not evaluated in this paper; BUMBLE is cited as an example of end-to-end VLM-based mobile manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper contrasts BUMBLE's dependence on a known map vs AnywhereVLA's autonomy via SLAM/exploration; this suggests transfer depends on map availability.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1914.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EdgeVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EdgeVLA: Efficient Vision-Language-Action models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An efficiency-focused VLA that eliminates autoregressive decoding for end-effector prediction to yield large inference speedups while maintaining task accuracy (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Edgevla: Efficient vision-language-action models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EdgeVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Efficiency-optimized VLA variant that removes autoregressive decoding for end-effector prediction to achieve faster inference (~7x speedup claimed in related work summary) while maintaining task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language / VLA pretraining (implied); precise regimen not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (manipulation domains emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Described as focused on manipulation; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Related-work claim: maintains task accuracy while achieving ~7× inference speedup; no numeric success rates provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1914.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TinyVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TinyVLA: Fast, data-efficient vision-language-action models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based lightweight VLA with a small multimodal backbone that aims to match larger VLA performance while improving inference speed and data efficiency (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tinyvla: Towards fast, dataefficient vision-language-action models for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TinyVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small VLA using a diffusion-based policy decoder and light multimodal backbone to improve speed and data efficiency; described in related work but not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Vision-language pretraining implied; exact pretraining not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Robotic manipulation (manipulation domains emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Related-work claim: matches larger VLAs while being faster and more data-efficient; no numeric values provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Cited as more data-efficient than larger models, but no quantitative numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1914.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RDT-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RDT-1B: A billion-parameter diffusion policy for bimanual manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large diffusion-based policy pretrained on multi-robot datasets to enable semantic generalization and robust bimanual skills (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rdt-1b: a diffusion foundation model for bimanual manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RDT-1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A billion-parameter diffusion transformer-based policy trained on multi-robot datasets for bimanual manipulation; discussed in related work as focusing on manipulation rather than navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Diffusion-policy pretraining on multi-robot demonstration datasets (implied by cited description).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not detailed here beyond 'multi-robot datasets'.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Bimanual manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>High-dimensional bimanual control tasks in manipulation domains (cited work); not used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1914.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AC-DiT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AC-DiT: Adaptive Coordination Diffusion Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion transformer architecture for conditioning manipulation policies in mobility contexts to coordinate base and arm control (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AC-DiT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An adaptive coordination transformer that uses diffusion-based decoding to produce coordinated base and arm control for mobile manipulators (described in related work summary).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Diffusion policy training on manipulation/mobility datasets (implied); specifics not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Coordinated mobile manipulation (base + arm)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Coupled base-and-arm control in mobile manipulators; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1914.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MoManipVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MoManipVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA adaptation for mobile manipulators that jointly plans base and arm motions to enable complex household operations (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoManipVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA approach tailored to mobile manipulators integrating planning of base and arm motions; referenced as prior work demonstrating joint planning capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Joint base-and-arm mobile manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Household operation tasks in localized settings; not used in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1914.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ASC: Adaptive Skill Coordination</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular skill-coordination framework that sequences visuomotor skills for long-horizon mobile manipulation with strong spatial memory, but with longer episode times (cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adaptive skill coordination for robotic mobile manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ASC</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A modular approach that frames long-horizon mobile manipulation as a sequence of visuomotor skills coordinated by a high-level policy; demonstrated in apartment-scale experiments in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Long-horizon mobile manipulation (skill coordination)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Apartment-scale pick-and-place tasks; cited experiments required 10–15 minutes for complete exploration and manipulation in a 185 m^2 apartment.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here; ASC is presented as a non-VLA modular baseline emphasizing skill coordination and spatial memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Paper contrasts ASC's reliable spatial memory but slower motion efficiency with AnywhereVLA's faster exploration/manipulation, implying differences in efficiency trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1914.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e1914.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language-action model initiative referenced in the literature and cited by the paper as relevant prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open-source VLA model/framework referenced in related work; presented as part of the ecosystem of VLA approaches but not used in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>Vision-language-action tasks for robotics (general)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>General VLA tasks; not evaluated in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>π 0 : A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>π 0.5 : A vision-language-action model with open-world generalization. <em>(Rating: 2)</em></li>
                <li>Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation. <em>(Rating: 2)</em></li>
                <li>Smolvla: A vision-language-action model for affordable and efficient robotics. <em>(Rating: 2)</em></li>
                <li>Edgevla: Efficient vision-language-action models. <em>(Rating: 2)</em></li>
                <li>Tinyvla: Towards fast, dataefficient vision-language-action models for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>Rdt-1b: a diffusion foundation model for bimanual manipulation. <em>(Rating: 2)</em></li>
                <li>Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation. <em>(Rating: 2)</em></li>
                <li>Adaptive skill coordination for robotic mobile manipulation. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1914",
    "paper_id": "paper-281525459",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "AnywhereVLA",
            "name_full": "AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation",
            "brief_description": "A modular onboard system that combines classical SLAM, frontier-based active exploration, and a fine-tuned vision-language-action (VLA) manipulation model to perform natural-language pick-and-place in unseen large-scale indoor environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "AnywhereVLA (system-level pipeline)",
            "model_description": "A modular pipeline integrating a 3D semantic mapping module (LiDAR+RGB SLAM + 2D detector → 3D semantic point cloud), an Active Environment Exploration (frontier-based) planner that is conditioned by parsed language target classes, an Approach module for visibility/reachability-aware base pose selection, and a VLA Manipulation head (fine-tuned SmolVLA) that outputs manipulator actions. Processes multimodal inputs (LiDAR, RGB images) and language instructions; couples classic geometric navigation with a language-conditioned policy for manipulation.",
            "pretraining_type": "hybrid: system leverages pretrained vision-language models (VLMs) for manipulation head (SmolVLA) and classical SLAM components for spatial memory; manipulation head was fine-tuned on robot trajectories.",
            "pretraining_data_description": "The paper states the approach draws on VLMs and co-fine-tuning on robotic trajectory data and Internet-scale vision-language tasks (general formulation), but for AnywhereVLA specifically the manipulation head (SmolVLA) was fine-tuned on 50 pick-and-place episodes collected with the SO-101 manipulator; no detailed inventory of pretraining corpora is provided in this paper.",
            "target_task_name": "Language-conditioned mobile pick-and-place / mobile manipulation",
            "target_task_description": "Natural language pick-and-place in unseen, unpredictable indoor multi-room environments. Action space includes continuous base motion (wheel actuation for navigation) and continuous manipulator joint motions for grasp/place; environment is real-world multi-room lab with static scenes and normal human motion; objects are household-type on support surfaces; deployment is on a real mobile manipulator (HermesBot with SO-101 arm) running fully onboard.",
            "semantic_alignment": "The paper claims semantic alignment is achieved by leveraging pretrained VLMs' visual-linguistic knowledge and by fine-tuning on robot pick-and-place trajectories, but it does not quantify overlap between pretraining data and in-environment objects/actions.",
            "performance_with_language_pretraining": "System-level overall task success rate: 46% in multi-room lab evaluations; VLA Manipulation module success rate reported as 80% (Table II). System maintains ≥10 Hz inference and completes tasks under 2.5 minutes in an 80 m^2 area in reported runs.",
            "performance_without_language_pretraining": "Not reported (no baseline ablation using a non-language-pretrained manipulation policy or random initialization is presented).",
            "sample_efficiency_comparison": "Not reported as a quantitative comparison. The manipulation head was fine-tuned with 50 pick-and-place episodes (small dataset), but no direct sample-efficiency comparison versus non-pretrained or vision-only baselines is given.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported. The paper does not present attention maps or attention-based analyses for the VLA model.",
            "embedding_space_analysis": "Not reported. No analysis of embedding clustering or feature-space semantic organization is provided.",
            "action_grounding_evidence": "Descriptive evidence: authors state the SmolVLA manipulation head was fine-tuned to 'ground local visual context and sub-goals into grasp and place proposals' and that the system inherits geometry-based navigation reliability plus language-conditioned manipulation agility; however, empirical mechanistic evidence mapping verbs to affordances/motor primitives is not provided.",
            "hierarchical_features_evidence": "Not reported. The paper does not analyze low-level vs high-level feature contributions nor indicate which feature levels benefit most from language pretraining.",
            "transfer_conditions": "Qualitative statements: successful transfer is aided by combining geometric spatial memory (SLAM + frontier exploration) with a fine-tuned VLA for manipulation; limitations occur for compositional spatial constraints in language (e.g., multi-step relational instructions), indicating transfer fails when instructions require relational/spatial-prepositional grounding beyond simple object-class targets.",
            "novel_vs_familiar_objects": "Not quantified. The paper evaluates in unseen environments but does not report separate metrics for objects seen during pretraining/fine-tuning versus novel objects.",
            "zero_shot_or_few_shot": "Few-shot fine-tuning: manipulation head fine-tuned on 50 demonstrations; no zero-shot manipulation performance numbers reported.",
            "layer_analysis": "Not reported. No ablation/freezing/probing of layers or components is provided.",
            "negative_transfer_evidence": "Not explicitly quantified. The paper reports a limitation: the system will explore to find the first instance of a target class regardless of relational spatial qualifiers in the instruction, indicating failure modes on compositional instructions, but no evidence that pretraining harms performance relative to alternatives.",
            "comparison_to_vision_only": "Not reported. The paper does not include a comparison to vision-only pretrained models (e.g., ImageNet) or purely geometric controllers for the manipulation head.",
            "temporal_dynamics": "Not reported. No analysis of representational/training dynamics over fine-tuning epochs is provided.",
            "dimensionality_analysis": "Not reported. No PCA or intrinsic-dimension analyses are included.",
            "uuid": "e1914.0"
        },
        {
            "name_short": "SmolVLA",
            "name_full": "SmolVLA (450M parameter VLA model)",
            "brief_description": "A compact (≈450M-parameter) vision-language-action model designed for efficient real-time robotic manipulation; in this paper it is fine-tuned as the manipulation head to produce grasp and place proposals from local visual context and language-conditioned subgoals.",
            "citation_title": "Smolvla: A vision-language-action model for affordable and efficient robotics.",
            "mention_or_use": "use",
            "model_name": "SmolVLA (450M)",
            "model_description": "Lightweight VLA model (≈450M parameters) intended to map multimodal visual inputs and language instructions to action outputs for manipulation; characterized in related work as competitive with larger VLAs and efficient for embedded deployment. In this work it is fine-tuned to output manipulator motor commands/grasp/place proposals.",
            "pretraining_type": "vision-language pretraining (implied): SmolVLA is described as a VLA derived from pretrained VLMs; the paper fine-tunes an existing SmolVLA instance on robot trajectory data.",
            "pretraining_data_description": "Not specified in detail in this paper. The related-work description implies prior SmolVLA models leverage vision-language pretraining corpora; for fine-tuning here SmolVLA was trained on 50 pick-and-place episodes collected on an SO-101 manipulator (robot trajectory data).",
            "target_task_name": "Pick-and-place manipulation (language-conditioned)",
            "target_task_description": "Local manipulation tasks: generating grasps and place proposals for a tabletop or other support-surface object from wrist/base/third-person cameras and language instruction; executed on a real SO-101 manipulator mounted on a mobile base; continuous arm joint control output.",
            "semantic_alignment": "Not quantitatively analyzed. The paper describes the intent to ground language into local visual context via fine-tuning, but provides no measurement of alignment between pretraining semantics and task objects/actions.",
            "performance_with_language_pretraining": "Module-level VLA Manipulation success rate: 80% (Table II) when used within AnywhereVLA after fine-tuning. No isolated metric for the SmolVLA component outside the integrated stack is given.",
            "performance_without_language_pretraining": "Not reported.",
            "sample_efficiency_comparison": "Not provided as a comparative analysis; reported fine-tuning dataset size: 50 pick-and-place episodes (small).",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Functional evidence only: fine-tuned SmolVLA produced grasp and place proposals in real-world runs leading to 80% manipulation-module success, implying practical grounding, but no mechanistic analyses linking language verbs to affordance representations are presented.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Not explicitly studied; SmolVLA is described as efficient for embedded deployment and fine-tunable to platform trajectories, implying transfer depends on platform demonstration data and visual viewpoint coverage.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Few-shot fine-tuning demonstrated (50 episodes), but no explicit zero-shot claims in this paper for SmolVLA.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Fine-tuning schedule described (hyperparameters), but no representational temporal analysis.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1914.1"
        },
        {
            "name_short": "π0 (pi-zero)",
            "name_full": "π0: A vision-language-action flow model for general robot control",
            "brief_description": "A flow matching–based action generation VLA model that integrates Internet-scale semantic knowledge and demonstration data for zero-shot dexterous manipulation (described in related work and cited by the paper).",
            "citation_title": "π 0 : A vision-language-action flow model for general robot control.",
            "mention_or_use": "mention",
            "model_name": "π0",
            "model_description": "A VLA flow model that generates continuous actions conditioned on visual and language inputs; described as incorporating large internet-scale semantic knowledge and demonstration data.",
            "pretraining_type": "Described as leveraging Internet-scale vision-language data plus demonstration data (vision-language and robot trajectory co-training implied), per the related-work summary in this paper.",
            "pretraining_data_description": "Not detailed in this paper beyond the general statement of Internet-scale semantic knowledge and demonstration trajectories.",
            "target_task_name": "Dexterous manipulation / general robot control (reported in cited work)",
            "target_task_description": "High-level: dexterous manipulation tasks across robots; not experimentally used in this paper.",
            "semantic_alignment": "Mentioned as leveraging large-scale semantic knowledge; the present paper does not provide measurements.",
            "performance_with_language_pretraining": "Not reported in this paper (reference only).",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not evaluated in this paper.",
            "hierarchical_features_evidence": "Not evaluated here.",
            "transfer_conditions": "Not evaluated in this paper.",
            "novel_vs_familiar_objects": "Not evaluated here.",
            "zero_shot_or_few_shot": "π0 is described in related work as enabling zero-shot dexterous manipulation, but this paper provides no empirical values.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.2"
        },
        {
            "name_short": "π0.5",
            "name_full": "π0.5: Co-training VLA for cross-domain generalization",
            "brief_description": "An extension of π0 that co-trains on heterogeneous multimodal datasets to improve generalization across tasks and robots (mentioned in related work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "π0.5",
            "model_description": "A VLA variant described as co-training on heterogeneous multimodal datasets (flow/diffusion style) to achieve broader generalization.",
            "pretraining_type": "Co-training on multimodal datasets (vision-language + demonstrations) — described in related work summary but not detailed here.",
            "pretraining_data_description": "Not specified beyond 'heterogeneous multimodal datasets'.",
            "target_task_name": "Generalization across manipulation tasks/robots (referenced work)",
            "target_task_description": "Not evaluated in this paper.",
            "semantic_alignment": "Mentioned conceptually; no measurements in this paper.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Claimed improved generalization in cited work; no numbers provided here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.3"
        },
        {
            "name_short": "BUMBLE",
            "name_full": "BUMBLE: VLM-based end-to-end building-wide mobile manipulation",
            "brief_description": "A vision-language-model-based end-to-end system for building-wide mobile manipulation that demonstrates strong navigation and manipulation across many objects but depends on a known map and preprovided landmarks (cited in related work).",
            "citation_title": "Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation.",
            "mention_or_use": "mention",
            "model_name": "BUMBLE",
            "model_description": "An end-to-end VLM-based mobile manipulation framework that integrates vision-language reasoning with navigation and manipulation to operate at building scale; presented as requiring a full known map and landmarks in cited work.",
            "pretraining_type": "Vision-language model based (described as VLM-driven), details not given in this paper.",
            "pretraining_data_description": "Not provided here; cited work likely uses VLM pretraining on large image-text corpora.",
            "target_task_name": "Building-wide mobile manipulation (navigation + manipulation)",
            "target_task_description": "Large-scale navigation and manipulation of previously unseen objects across a known mapped environment; cited work relies on prior map/landmarks instead of autonomous exploration.",
            "semantic_alignment": "Paper notes BUMBLE relies on known map and landmarks; no alignment metrics reported here.",
            "performance_with_language_pretraining": "Cited work described as showing strong performance; no quantitative metrics are reproduced in this paper.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this paper.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not evaluated in this paper; BUMBLE is cited as an example of end-to-end VLM-based mobile manipulation.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Paper contrasts BUMBLE's dependence on a known map vs AnywhereVLA's autonomy via SLAM/exploration; this suggests transfer depends on map availability.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.4"
        },
        {
            "name_short": "EdgeVLA",
            "name_full": "EdgeVLA: Efficient Vision-Language-Action models",
            "brief_description": "An efficiency-focused VLA that eliminates autoregressive decoding for end-effector prediction to yield large inference speedups while maintaining task accuracy (cited in related work).",
            "citation_title": "Edgevla: Efficient vision-language-action models.",
            "mention_or_use": "mention",
            "model_name": "EdgeVLA",
            "model_description": "Efficiency-optimized VLA variant that removes autoregressive decoding for end-effector prediction to achieve faster inference (~7x speedup claimed in related work summary) while maintaining task performance.",
            "pretraining_type": "Vision-language / VLA pretraining (implied); precise regimen not provided here.",
            "pretraining_data_description": "Not described in this paper.",
            "target_task_name": "Robotic manipulation (manipulation domains emphasized)",
            "target_task_description": "Described as focused on manipulation; not evaluated in this paper.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Related-work claim: maintains task accuracy while achieving ~7× inference speedup; no numeric success rates provided in this paper.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.5"
        },
        {
            "name_short": "TinyVLA",
            "name_full": "TinyVLA: Fast, data-efficient vision-language-action models",
            "brief_description": "A diffusion-based lightweight VLA with a small multimodal backbone that aims to match larger VLA performance while improving inference speed and data efficiency (cited in related work).",
            "citation_title": "Tinyvla: Towards fast, dataefficient vision-language-action models for robotic manipulation.",
            "mention_or_use": "mention",
            "model_name": "TinyVLA",
            "model_description": "A small VLA using a diffusion-based policy decoder and light multimodal backbone to improve speed and data efficiency; described in related work but not used in experiments here.",
            "pretraining_type": "Vision-language pretraining implied; exact pretraining not specified here.",
            "pretraining_data_description": "Not described in this paper.",
            "target_task_name": "Robotic manipulation (manipulation domains emphasized)",
            "target_task_description": "Not evaluated in this paper.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Related-work claim: matches larger VLAs while being faster and more data-efficient; no numeric values provided in this paper.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Cited as more data-efficient than larger models, but no quantitative numbers in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.6"
        },
        {
            "name_short": "RDT-1B",
            "name_full": "RDT-1B: A billion-parameter diffusion policy for bimanual manipulation",
            "brief_description": "A large diffusion-based policy pretrained on multi-robot datasets to enable semantic generalization and robust bimanual skills (mentioned in related work).",
            "citation_title": "Rdt-1b: a diffusion foundation model for bimanual manipulation.",
            "mention_or_use": "mention",
            "model_name": "RDT-1B",
            "model_description": "A billion-parameter diffusion transformer-based policy trained on multi-robot datasets for bimanual manipulation; discussed in related work as focusing on manipulation rather than navigation.",
            "pretraining_type": "Diffusion-policy pretraining on multi-robot demonstration datasets (implied by cited description).",
            "pretraining_data_description": "Not detailed here beyond 'multi-robot datasets'.",
            "target_task_name": "Bimanual manipulation",
            "target_task_description": "High-dimensional bimanual control tasks in manipulation domains (cited work); not used in this paper.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.7"
        },
        {
            "name_short": "AC-DiT",
            "name_full": "AC-DiT: Adaptive Coordination Diffusion Transformer",
            "brief_description": "A diffusion transformer architecture for conditioning manipulation policies in mobility contexts to coordinate base and arm control (cited in related work).",
            "citation_title": "Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation.",
            "mention_or_use": "mention",
            "model_name": "AC-DiT",
            "model_description": "An adaptive coordination transformer that uses diffusion-based decoding to produce coordinated base and arm control for mobile manipulators (described in related work summary).",
            "pretraining_type": "Diffusion policy training on manipulation/mobility datasets (implied); specifics not in this paper.",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Coordinated mobile manipulation (base + arm)",
            "target_task_description": "Coupled base-and-arm control in mobile manipulators; not evaluated in this paper.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.8"
        },
        {
            "name_short": "MoManipVLA",
            "name_full": "MoManipVLA",
            "brief_description": "A VLA adaptation for mobile manipulators that jointly plans base and arm motions to enable complex household operations (mentioned in related work).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MoManipVLA",
            "model_description": "A VLA approach tailored to mobile manipulators integrating planning of base and arm motions; referenced as prior work demonstrating joint planning capabilities.",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not specified here.",
            "target_task_name": "Joint base-and-arm mobile manipulation",
            "target_task_description": "Household operation tasks in localized settings; not used in experiments here.",
            "semantic_alignment": "Not analyzed in this paper.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.9"
        },
        {
            "name_short": "ASC",
            "name_full": "ASC: Adaptive Skill Coordination",
            "brief_description": "A modular skill-coordination framework that sequences visuomotor skills for long-horizon mobile manipulation with strong spatial memory, but with longer episode times (cited work).",
            "citation_title": "Adaptive skill coordination for robotic mobile manipulation.",
            "mention_or_use": "mention",
            "model_name": "ASC",
            "model_description": "A modular approach that frames long-horizon mobile manipulation as a sequence of visuomotor skills coordinated by a high-level policy; demonstrated in apartment-scale experiments in cited work.",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Long-horizon mobile manipulation (skill coordination)",
            "target_task_description": "Apartment-scale pick-and-place tasks; cited experiments required 10–15 minutes for complete exploration and manipulation in a 185 m^2 apartment.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Not reported here; ASC is presented as a non-VLA modular baseline emphasizing skill coordination and spatial memory.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Paper contrasts ASC's reliable spatial memory but slower motion efficiency with AnywhereVLA's faster exploration/manipulation, implying differences in efficiency trade-offs.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.10"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA",
            "brief_description": "An open-source vision-language-action model initiative referenced in the literature and cited by the paper as relevant prior work.",
            "citation_title": "Openvla: An open-source vision-language-action model.",
            "mention_or_use": "mention",
            "model_name": "OpenVLA",
            "model_description": "An open-source VLA model/framework referenced in related work; presented as part of the ecosystem of VLA approaches but not used in this paper's experiments.",
            "pretraining_type": "Not specified in this paper.",
            "pretraining_data_description": "Not provided here.",
            "target_task_name": "Vision-language-action tasks for robotics (general)",
            "target_task_description": "General VLA tasks; not evaluated in this work.",
            "semantic_alignment": "Not analyzed here.",
            "performance_with_language_pretraining": "Not reported here.",
            "performance_without_language_pretraining": "Not reported here.",
            "sample_efficiency_comparison": "Not reported here.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported here.",
            "embedding_space_analysis": "Not reported here.",
            "action_grounding_evidence": "Not reported here.",
            "hierarchical_features_evidence": "Not reported here.",
            "transfer_conditions": "Not reported here.",
            "novel_vs_familiar_objects": "Not reported here.",
            "zero_shot_or_few_shot": "Not reported here.",
            "layer_analysis": "Not reported here.",
            "negative_transfer_evidence": "Not reported here.",
            "comparison_to_vision_only": "Not reported here.",
            "temporal_dynamics": "Not reported here.",
            "dimensionality_analysis": "Not reported here.",
            "uuid": "e1914.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "π 0 : A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "π 0.5 : A vision-language-action model with open-world generalization.",
            "rating": 2
        },
        {
            "paper_title": "Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Smolvla: A vision-language-action model for affordable and efficient robotics.",
            "rating": 2
        },
        {
            "paper_title": "Edgevla: Efficient vision-language-action models.",
            "rating": 2
        },
        {
            "paper_title": "Tinyvla: Towards fast, dataefficient vision-language-action models for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Rdt-1b: a diffusion foundation model for bimanual manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Adaptive skill coordination for robotic mobile manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 2
        }
    ],
    "cost": 0.019180999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation
25 Sep 2025</p>
<p>Konstantin Gubernatorov 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Artem Voronov 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Roman Voronov 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Sergei Pasynkov 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Stepan Perminov 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Ziang Guo 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>Dzmitry Tsetserukou 
Center for Digital Engineering
Intelligent Space Robotics Laboratory
Skolkovo Institute of Science and Technology
MoscowRussia</p>
<p>AnywhereVLA: Language-Conditioned Exploration and Mobile Manipulation
25 Sep 2025A996B673179080BDB2C2B7D6772CA511arXiv:2509.21006v1[cs.RO]
We address natural language pick-and-place in unseen, unpredictable indoor environments with Any-whereVLA, a modular framework for mobile manipulation.A user text prompt serves as an entry point and is parsed into a structured task graph that conditions classical SLAM with LiDAR and cameras, metric semantic mapping, and a task-aware frontier exploration policy.An approach planner then selects visibility and reachability aware pre grasp base poses.For interaction, a compact SmolVLA manipulation head is fine tuned on platform pick and place trajectories for the SO-101 by TheRobotStudio, grounding local visual context and sub-goals into grasp and place proposals.The full system runs fully onboard on consumer-level hardware, with Jetson Orin NX for perception and VLA and an Intel NUC for SLAM, exploration, and control, sustaining real-time operation.We evaluated AnywhereVLA in a multi-room lab under static scenes and normal human motion.In this setting, the system achieves a 46% overall task success rate while maintaining throughput on embedded compute.By combining a classical stack with a fine-tuned VLA manipulation, the system inherits the reliability of geometry-based navigation with the agility and task generalization of language-conditioned manipulation.All code, models, and datasets are open source and are available on the project GitHub repository.*Denotes equal contribution.</p>
<p>I. INTRODUCTION</p>
<p>Mobile manipulation is accelerating beyond limited indoor workcells towards large unstructured environments, in which robots need to explore unfamiliar cluttered spaces and physically interact with diverse objects and people.The execution of complex mobile manipulation tasks conditional on natural language instructions has gained attention in recent years in the field of service robotics [1].Research on intelligent robotic systems in fields such as household service [2], [3], retail automation [4], [5], warehouse logistics [6], and manufacturing [7] has gained popularity, highlighting the importance of developing mobile manipulation solutions capable of operating in large-scale and open-plan indoor environments.Recent studies have increasingly focused on natural language processing to enable robots to interpret human instructions and facilitate intuitive task specification [8], positioning language-guided manipulation as a key approach for effective human-robot collaboration [9].However, unifying languagebased control, environment exploration, and manipulation in expansive environments presents a significant challenge [10].</p>
<p>Vision-language-action (VLA) models show strong generalization in various mobile manipulation tasks [11], [12], enabling robots to perform complex operations integrating perception, language, and control.Despite these advancements, several critical limitations persist for end-to-end control use for mobile robots.Most VLA models are confined to specific tasks and have limited spatial awareness [11]- [13], restricting their operational scope to localized settings and impeding their ability to navigate or manipulate objects in unseen or occluded regions of larger indoor spaces.</p>
<p>Vision-Language Navigation (VLN) approaches [14] present an end-to-end VLM-based framework that navigates building-wide environments while manipulating previously unseen household objects.However, [14], as all VLN models, require instructions about the location of the target object within the environment, which is often impractical in dynamic or unexplored settings.In contrast, classical navigation stacks [15] provide robust solutions for mapping and environment exploration, enabling robots to systematically traverse and model unknown spaces.However, these traditional systems lack the advanced language comprehension and semantic reasoning capabilities necessary to interpret complex instructions or contextual cues [8], limiting their ability to perform goal-directed tasks that require understanding natural language or high-level semantic objectives.</p>
<p>In this paper we introduce AnywhereVLA, a novel modular architecture for large-scale indoor mobile manipulation, addressing the limitations of existing Vision-Language-Action (VLA) models that adapt pretrained vision-language models (VLMs) to enable natural language-driven perception and control, but are often constrained to room-scale environments due to high computational demands.Drawing from advancements in VLA paradigms, AnywhereVLA integrates the rich visual and linguistic knowledge encoded in VLMs-leveraged through co-fine-tuning on robotic trajectory data and Internet-scale vision-language tasks-with the robust traversability afforded by classical navigation stacks, representing robot actions as tokenized sequences to facilitate end-to-end control and emergent semantic reasoning.</p>
<p>AnywhereVLA is a pipeline for large-scale indoor mobile manipulation in unseen environments.As shown in Fig. 1, AnywhereVLA combines robust traversability of classical navigation algorithms and simultaneous localization and mapping (SLAM) with the generalizable scene understanding and task grounding of VLA models.Our pipeline translates high-level language instructions into low-level control commands by generating actions via VLA model for task-Fig.1: AnywhereVLA is the modular architecture comprising VLA manipulation and environment exploration.Given the task, AnywhereVLA parses it into simpler actions which further condition Active Environment Exploration.Exploration and navigation in larger-scale indoor environments are performed within a 3D point cloud semantic map.By leveraging a purpose-built pick-and-place dataset, AnywhereVLA exhibits robust generalization capacities.specific manipulation and computes navigation trajectories via language-conditioned exploration algorithm, directly actuating the wheels of the mobile base and the joints of the manipulator.Our main contribution is the following:</p>
<p>We propose a cohesive modular framework that accepts a single language-based task instruction as input, which conditions the environment exploration and navigation modules and simultaneously drives the VLA model for manipulation task execution.Our system achieves real-time performance exceeding 10 Hz across all modules that are deployed on consumer-available edge computing units, ensuring efficient and responsive operation in dynamic settings.</p>
<p>II. RELATED WORK</p>
<p>Large Vision-Language Models (VLMs) have emerged as a transformative paradigm.Pretraining on web-scale imagetext corpora enables robust alignment between visual content and natural language semantics.When incorporated into robotic systems, these models have catalyzed a new class of policies, Vision-Language-Action (VLA) models, which extend perception-language alignment to action generation by mapping open instructions and visual context to executable behaviors [16].VLA models unify perception, natural language, and control into end-to-end policies for robots.They leverage pretrained vision-language models and robotic trajectory datasets to ground instructions into lowlevel actions.MoManipVLA [13] adapts pretrained VLA models to mobile manipulators by jointly planning base and arm motions, enabling complex household operations.π 0 [11] introduces a flow matching-based action generation approach that integrates Internet-scale semantic knowledge and demonstration data for zero-shot dexterous manipulation, while π 0.5 [12] extends this paradigm through co-training on heterogeneous multimodal datasets, achieving strong generalization across tasks and robots.These methods excel at instruction grounding and task generalization, but have limited spatial awareness.</p>
<p>Beyond this, the deployment of resource-intensive VLAs on mobile platforms requires careful optimization to ensure efficiency and maintain performance within hardware limitations.SmolVLA [17] demonstrates that a 450M-parameter VLA can achieve competitive performance compared to larger models.EdgeVLA [18] further improves efficiency by eliminating autoregressive decoding for end-effector prediction, yielding a 7× speedup in inference while maintaining task accuracy.TinyVLA [19] introduces a diffusion-based policy decoder and a lightweight multimodal backbone, that match the performance of much larger VLAs while being significantly faster and more data-efficient.These advances enable real-time operation on embedded devices, though the generalization reported is primarily within manipulation domains.</p>
<p>A rapidly expanding body of work explores diffusion transformer-based policies for language-conditioned control.RDT-1B [20] is a billion-parameter diffusion policy pretrained on multi-robot datasets, enabling broad semantic generalization and robust bimanual skills.AC-DiT [21] introduces an adaptive coordination transformer that conditions manipulation policies in the mobility context, facilitating coupled base and arm control in mobile manipulators.However, both [20] and [21] focus exclusively on manipulation and therefore require complementary modules to provide spatial memory, target discovery, and long-horizon navigation.BUMBLE [14] introduces a VLM-based end-to-end framework for building-wide mobile manipulation.As a fully integrated system, it demonstrates strong performance in navigating large-scale indoor environments while manipulating a broad and previously unseen set of everyday objects.A central limitation, however, is its reliance on the known map of the whole environment and set of preprovided landmarks.In contrast, approaches that integrate SLAM and environment exploration overcome this limitation by navigating environments autonomously without requiring prior knowledge.Furthermore, ASC [22] frames long-horizon mobile manipulation as a sequence of modular visuomotor skills coordinated by a high-level policy.Experiments in a 185 m 2 apartment show reliable pick-and-place with strong spatial memory, indicating effective maintenance of the task context in the rooms and for extended periods.However, complete exploration and manipulation in apartment-sized environments takes 10-15 minutes.Consequently, while ASC substantiates scalable skill coordination, its motion efficiency remains a bottleneck for deployment in real-world, humanpopulated spaces.</p>
<p>III. ANYWHEREVLA ARCHITECTURE AnywhereVLA framework implements an modular pipeline actuating motors of the mobile platform and manipulator's joints processing single language command and raw sensor inputs.In Fig. 2, the architecture comprises four main modules: 3D Semantic Mapping with Confidence (SM), Active Environment Exploration (AEE), Approach, and VLA Manipulation.The workflow begins with the parsing of natural language instruction, which simultaneously informs the VLA module for task-specific manipulation and conditions the AEE process.The SM module constructs a semantic 3D point cloud map by combining LiDAR-Inertial-Visual SLAM with semantic annotations from the object detection model.This map supports the AEE module, which employs frontier-based exploration conditioned on the target object class derived from the language instruction.Exploration halts once the target object is detected and localized within the semantic map.The Approach module then navigates the mobile base to the target object's location using a 2D grid map projected from a filtered LiDAR point cloud, positioning the robot for manipulation.VLA Manipulation module, leveraging a fine-tuned SmolVLA model, executes the specified task by generating actions for the manipulator's motors.</p>
<p>A. 3D Semantic Mapping with Confidence (SM)</p>
<p>Algorithm 1 presents a 3D Semantic Object Map construction by synchronizing RGB images, undistorted LiDAR point clouds, and 2D boundbox detections by projecting LiDAR points into the camera frame to associate them with detections.To mitigate the sparsity and discontinuities of spinning LiDAR, we densified the scan by interpolating between adjacent elevation rings within each azimuth bin, followed by voxelization.For each detection, near-surface points inside its enlarged 2D bounding box are backprojected to 3D, forming object-wise point sets that are transformed to the world frame and accumulated per class over time.This produces a metric-semantic map populated by per-object 3D statistics and confidence estimates.</p>
<p>B. Object Aggregation</p>
<p>Per class accumulated points are clustered via radius-based DBSCAN [23], outliers are robustly filtered, and each cluster is summarized by its centroid and covariance.Multi-view and data-driven cues produce an object-level confidence via a logistic mapping that fuses point density, angular coverage, inlier count, and detector scores.The resulting world-frame semantic object map exposes persistent objects with class labels, geometry, and confidence for downstream planning and manipulation.end for 25: end for</p>
<p>Algorithm 1 Semantic Map Construction Pipeline</p>
<p>C. LiDAR densification</p>
<p>In Fig. 3, the sparse and discontinuous LiDAR scan pattern causes few (or no) valid returns inside a 2D detection box.In Fig. 4, we interpolate between valid ring-adjacent samples within the same azimuth bin, inserting M interior points per gap (subject to range-jump and spatial-gap gates).With endpoints S and E, the interpolants are
P t = M + 1 − t M + 1 S + t M + 1 E, t = 1, . . . , M .(1)</p>
<p>D. Confidence estimation</p>
<p>Let ρ denote point density (normalized by ρ 0 ), Ω ∈ [0, 1] denote multi-view angular coverage (union of yaw arcs over 2π), N the inlier count (normalized by N 0 ) and s the mean detector score.The confidence is
C = σ w ρ 1 − e −ρ/ρ 0 + w Ω Ω + w N 1 − e −N/N 0 + w S s + b ,(2)</p>
<p>E. Active Environment Exploration (AEE)</p>
<p>In Algorithm 2, AEE module systematically maps unknown regions to locate the target object using a frontierbased strategy [24] integrated into our modular pipeline.The occupancy grids of the SLAM backend serve as input.Frontiers are extracted through 8-connected morphological dilation and clustered.For each cluster, we compute a centroid and optimize the yaw to improve field-of-view (FoV) coverage.The resulting goal is validated with the Nav2 stack [15].To suppress spurious goals and respect operational limits, we apply cluster filtering (η c = 20 px), chunking (η k = 50 px), non-maximum suppression (d min = 1 m), an exploration radius constraint R e , and FoV yaw optimization (α = 35 • , R g = 1.5 m).We also use a conservative frontier filter with a 5 × 5 kernel and a gain threshold of 50.The planner re-evaluates goals every T u = 4 s.Exploration ends upon detection of the target or upon exhaustion of valid frontiers.</p>
<p>F. Approach module</p>
<p>The approach module computes a safe approach pose suitable for VLA manipulation from a labeled object pose on a flat support surface (e.g., a tabletop marked as an obstacle in the occupancy grid).It isolates the surface region, recovers its boundary, estimates the surface normal with principal component analysis [25], and places the robot outside the edge with a user-defined offset while orienting the base perpendicular to the surface.Nav2 [15] validates reachability and collision-free access; If the pose is infeasible, the module Chunks ← AngularPartition(Pixels, η k ) 9:</p>
<p>for each chunk c j in Chunks do 10:
p i j ← Centroid(c j ) ⊕ WorldTransform 11: p i j ← Standoff(p i j , p r , d s ) 12: if ∥p i j − p r ∥ &gt; R e then 13:ψ i j ← OptimizeYaw(p i j , unknown(M ), α, R g ) 16:
if Gain(ψ i j ) &lt; 50 then Add (p i j , ψ i j ) to Candidates</p>
<p>IV. VLA MODEL FINE-TUNING</p>
<p>We fine-tuned the SmolVLA 450M model using an NVIDIA RTX 4090 GPU 16 GB VRAM.Fine-tuning was performed on a collected dataset comprising 50 pick-andplace episodes collected with SO-101 manipulator [26] by teleportation with a leader manipulator.</p>
<p>We used a batch size of 16 and a cosine decay scheduler with a learning rate of 0.0001 warmup [27].An AdamW optimizer [28] with a weight decay of 0.01 and warmup of 100 was used.Gradients were clipped to a norm of 10.0 to mitigate the explosion.</p>
<p>V. HERMESBOT MOBILE MANIPULATOR</p>
<p>To support the deployment of AnywhereVLA, we developed a mobile manipulator platform tailored to integrated multi-modal sensing and computation.In Fig. 5, The robot</p>
<p>A. Sensor Configuration</p>
<p>The sensor suite is divided into navigational and VLA subsystems.Navigation is based on a Velodyne VLP-16 LiDAR and an Intel RealSense D435i RGB-D camera, integrated with [29] for Visual-LiDAR-Inertial SLAM.</p>
<p>The VLA subsystem employs three Intel RealSense D435 cameras: a third-person-view camera angled to overview the manipulator and operating area (Fig. 6(a)), a wristmounted camera capturing the SO-101 manipulator's twofingered gripper (Fig. 6(b)), and a base-mounted camera facing forward to monitor the manipulator's workspace (Fig. 6(c)).</p>
<p>B. Computational Hardware</p>
<p>The computational setup comprises an NVIDIA Jetson Orin NX 16Gb for GPU-accelerated tasks and an Intel NUC Core i7 32Gb for CPU-intensive operations.The Jetson Orin processes the perception-heavy SM and VLA Manipulation modules, while the NUC handles SLAM and navigation.</p>
<p>Table I details the distribution of the pipeline modules, including inference frequencies and latencies measured during real-world deployment.To evaluate the feasibility of real-world deployment, we measured the total episode completion time of AnywhereVLA in a set of exploration radii R e ∈ {2.5, 5, 7.5, 10} m.In Fig. 7, each run of AnywhereVLA within 5m radius, which is equivalent to an average apartment, took under 133 seconds on average.And within 10m radius our architecture managed to successfully complete tasks under 10 minutes.</p>
<p>VII. CONCLUSION, LIMITATIONS AND FUTURE WORK</p>
<p>In this paper, we identify a shortcoming in the existing mobile manipulation architectures: the inability to explore open and large-scale indoor environments.To address this issue, we propose an AnywhereVLA, a modular framework for language-conditioned exploration and manipulation in large-scale novel indoor environments.The system combines classical SLAM, exploration, and navigation with a lightweight VLA fine-tuned for pick-and-place operations with SO-101 by TheRobotStudio.To evaluate our system, we conducted experiments in unseen dynamic university environments.Deployed fully onboard with 3D Semantic Mapping and VLA on Jetson Orin NX and Active Environment Exploration and SLAM on Intel NUC, AnywhereVLA sustains real-time operation at ≥ 10 Hz and achieves a 46% overall success rate completing tasks under 2.5 minutes in 80m 2 area.</p>
<p>While our language-conditioned exploration pipeline demonstrates robust performance in identifying and localizing target objects within unstructured environments, Any-whereVLA exhibits a notable limitation in enforcing precise spatial-semantic constraints specified in the natural language instruction.Specifically, our pipeline is unable to successfully comprehend cases such as "Pick up the bottle from the table and place it in the blue box.And bring the bottle to me."Our architecture will explore to find the first bottle it detects, no matter where it is.</p>
<p>To address this limitation, future iterations of the pipeline could incorporate hierarchical semantic parsing and relational reasoning modules to disentangle object-level detection from spatial-prepositional constraints.One promising approach involves constructing a dynamic scene graph during exploration, where nodes represent detected objects and affordances, and edges encode probabilistic spatial relations derived from multimodal perception streams.Exploration policies could then be augmented with a graph-based reward signal that evaluates path efficiency and relational fidelity, penalizing deviations from the instructed configuration..Alternatively, integrating a lightweight Vision-Language-Model for zero-shot relational query resolution (e.g., querying "is the bottle supported by the table?") could provide an end-toend differentiable check prior to success attribution, thereby enhancing compositional generalization without substantial computational overhead.These enhancements would align the system more closely with human-like instruction following, particularly in cluttered, multi-object scenes prevalent in real-world robotic deployment.</p>
<p>Fig. 2 :
2
Fig. 2: AnywhereVLA architecture.</p>
<p>5 : 9 :
59
for each adjacent ring pair (r, r+1) with points S r k , E r+1 k meeting range/gap gates do 6: insert M interpolants P t = M+1−t M+1 Voxelize: Q t ← VoxelGrid(FOV points, v) 10: Projection: 11: for each detection b ∈ D t do 12: enlarge box (w, h) ← (w+∆ x , h+∆ y ), select points in 2D box 13: depth gate: keep points with z ≤ z min (b) + δ 14: compute 3D bbox (c, s) from selected points in LiDAR frame 15: transform points to world: Q W b ← T L→W (t) Q b 16: accumulate Q W b into per-class database 17: end for 18: Object aggregation: 19: for each class κ (periodic/on-demand) do 20: aggregate points; cluster with DBSCAN(ε, minPts) 21: for each cluster C do 22: robust outlier filtering (MAD/σ ); compute mean µ and covariance Σ 23: estimate confidence C using Eq.(2); publish markers and metadata 24:</p>
<p>Fig. 3 :
3
Fig. 3: Sparse point cloud from Velodyne VLP-16 LiDAR.</p>
<p>Fig. 4 :
4
Fig. 4: Densified LiDAR point cloud after interpolation.</p>
<p>Algorithm 2
2
Frontier-Based Goal SelectionRequire: Map M , robot pose (p r , ψ r ), camera params (α, R g ) Ensure: Goal pose g or ⊥ (none)1: F ← Dilate(free(M ), 3 × 3) ∩ unknown(M ) \ Dilate(occupied(M ), 5 × 5) 2: Labels, Slices ← LabelComponents(F) 3: for each slice s i in Slices do</p>
<p>end for 22: Candidates ← NMS(Candidates, d min ); Sort by ∥p − p r ∥ 23: for each (p i , ψ i ) in Candidates do 24: feasible, len ← ComputePath(p r , p i ) 25: if feasible then 26: g ← Pose(p i , YawToQuat(ψ i )) for 31: return ⊥ iterates over nearby edge candidates and reports failure only after exhausting valid options.</p>
<p>Fig. 5 :
5
Fig. 5: HermesBot mobile manipulator.</p>
<p>Fig. 6 :
6
Fig. 6: Intel Realsense D435 camera frames.</p>
<p>Fig. 7 :
7
Fig. 7: Total episode completion time.</p>
<p>1 :
1
Inputs: image I t , detections D t , LiDAR cloud P L t , extrinsics T L→C , TF T L→W 2: FOV filter: project P L</p>
<p>t to camera: U t ← Π T L→C P L t ; keep points inside image 3: Densification: 4: for each azimuth bin k do</p>
<p>TABLE I :
I
Computational throughput of AnywhereVLA modules.
ModuleComputerFrequency (Hz) ↑ Process time (ms) ↓SLAMIntel NUC1025Semantic MapJetson Orin1545VLAJetson Orin1520</p>
<p>TABLE II :
II
Success Rate of AnywhereVLA modules.
ModuleSR (%) ↑SLAM100Active Environment Exploration75Navigation90Object Detection85VLA Manipulation80</p>
<p>A survey of wheeled mobile manipulation: A decision-making perspective. S Thakar, S Srinivasan, S Al-Hussaini, P M Bhatt, P Rajendran, Y Jung Yoon, N Dhanaraj, R K Malhan, M Schmid, V N Krovi, Journal of Mechanisms and Robotics. 152208012023</p>
<p>Tidybot++: An open-source holonomic mobile manipulator for robot learning. J Wu, W Chong, R Holmberg, A Prasad, Y Gao, O Khatib, S Song, S Rusinkiewicz, J Bohg, 2024</p>
<p>Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. Z Fu, T Z Zhao, C Finn, 2024</p>
<p>Practical insights on grasp strategies for mobile manipulation in the wild. I Huang, R Cheng, S Kim, D Kruse, C Matl, L Kaul, J Hancock, S Harikumar, M Tjersland, J Borders, D Helmick, 2025</p>
<p>M Bajracharya, J Borders, R Cheng, D Helmick, L Kaul, D Kruse, J Leichty, J Ma, C Matl, F Michel, arXiv:2401.01474Demonstrating mobile manipulation in the wild: A metrics-driven approach. 2024arXiv preprint</p>
<p>Tacmms: Tactile mobile manipulators for warehouse automation. Z He, X Zhang, S Jones, S Hauert, D Zhang, N F Lepora, IEEE Robotics and Automation Letters. 882023</p>
<p>A general mobile manipulator automation framework for flexible tasks in controlled environments. C Pu, C Yang, J Pu, R B Fisher, Advanced Engineering Informatics. 571020622023</p>
<p>Natural language instructions for intuitive human interaction with robotic assistants in field construction work. S Park, X Wang, C C Menassa, V R Kamat, J Y Chai, 10.1016/j.autcon.2024.105345May 2024161105345Automation in Construction</p>
<p>Language-guided long horizon manipulation with llm-based planning and visual perception. C Zhou, H Xu, N Gu, Z Wang, B Cheng, P Zhang, Y Dong, M Hayashibe, Y Zhou, B He, 2025</p>
<p>Large vlm-based vision-language-action models for robotic manipulation: A survey. R Shao, W Li, L Zhang, R Zhang, Z Liu, R Chen, L Nie, 2025</p>
<p>π 0 : A vision-language-action flow model for general robot control. K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, L X Shi, J Tanner, Q Vuong, A Walling, H Wang, U Zhilinsky, 2024</p>
<p>a vision-language-action model with open-world generalization. P Intelligence, K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, C Finn, N Fusai, M Y Galliker, D Ghosh, L Groom, K Hausman, B Ichter, S Jakubczak, T Jones, L Ke, D Leblanc, S Levine, A Li-Bell, M Mothukuri, S Nair, K Pertsch, A Z Ren, L X Shi, L Smith, J T Springenberg, K Stachowicz, J Tanner, Q Vuong, H Walke, A Walling, H Wang, L Yu, U Zhilinsky, 20255</p>
<p>Momanipvla: Transferring vision-language-action models for general mobile manipulation. Z Wu, Y Zhou, X Xu, Z Wang, H Yan, 2025</p>
<p>Bumble: Unifying reasoning and acting with vision-language models for building-wide mobile manipulation. R Shah, A Yu, Y Zhu, Y Zhu, R Martín-Martín, 2024</p>
<p>The marathon 2: A navigation system. S Macenski, F Martín, R White, J Ginés Clavero, 2020 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS). 2020</p>
<p>Openvla: An open-source vision-language-action model. M J Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, Q Vuong, T Kollar, B Burchfiel, R Tedrake, D Sadigh, S Levine, P Liang, C Finn, 2024</p>
<p>Smolvla: A vision-language-action model for affordable and efficient robotics. M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, M Aractingi, C Pascal, M Russi, A Marafioti, S Alibert, M Cord, T Wolf, R Cadene, 2025</p>
<p>Edgevla: Efficient vision-language-action models. P Budzianowski, W Maa, M Freed, J Mo, W Hsiao, A Xie, T Młoduchowski, V Tipnis, B Bolte, 2025</p>
<p>Tinyvla: Towards fast, dataefficient vision-language-action models for robotic manipulation. J Wen, Y Zhu, J Li, M Zhu, K Wu, Z Xu, N Liu, R Cheng, C Shen, Y Peng, F Feng, J Tang, 2025</p>
<p>Rdt-1b: a diffusion foundation model for bimanual manipulation. S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, 2025</p>
<p>Ac-dit: Adaptive coordination diffusion transformer for mobile manipulation. S Chen, J Liu, S Qian, H Jiang, L Li, R Zhang, Z Liu, C Gu, C Hou, P Wang, Z Wang, S Zhang, 2025</p>
<p>Asc: Adaptive skill coordination for robotic mobile manipulation. N Yokoyama, A Clegg, J Truong, E Undersander, T.-Y Yang, S Arnaud, S Ha, D Batra, A Rai, 2023</p>
<p>A density-based algorithm for discovering clusters in large spatial databases with noise. M Ester, H.-P Kriegel, J Sander, X Xu, Knowledge Discovery and Data Mining. 1996355163</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, Proc. IEEE Int. Symp. Computational Intelligence in Robotics and Automation (CIRA). IEEE Int. Symp. Computational Intelligence in Robotics and Automation (CIRA)July 1997</p>
<p>Principal component analysis. S Wold, K Esbensen, P Geladi, proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists. the Multivariate Statistical Workshop for Geologists and Geochemists19872Chemometrics and Intelligent Laboratory Systems</p>
<p>Lerobot: State-of-the-art machine learning for real-world robotics in pytorch. R Cadene, S Alibert, A Soare, Q Gallouedec, A Zouitine, S Palma, P Kooijmans, M Aractingi, M Shukor, D Aubakirova, M Russi, F Capuano, C Pascal, J Choghari, J Moss, T Wolf, 2024</p>
<p>A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. A Gotmare, N S Keskar, C Xiong, R Socher, 2018</p>
<p>Decoupled weight decay regularization. I Loshchilov, F Hutter, 2019</p>
<p>Fast-livo2: Fast, direct lidar-inertial-visual odometry. C Zheng, W Xu, Z Zou, T Hua, C Yuan, D He, B Zhou, Z Liu, J Lin, F Zhu, Y Ren, R Wang, F Meng, F Zhang, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9042 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9042</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9042</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-274656281</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12644v5.pdf" target="_blank">Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles</a></p>
                <p><strong>Paper Abstract:</strong> Assessing the effectiveness of large language models (LLMs) in performing different tasks is crucial for understanding their strengths and weaknesses. This paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human cognitive principles and designed to assess LLMs by examining the cognitive demands of various tasks. The HPT utilizes the Hierarchical Prompting Framework (HPF), which structures five unique prompting strategies in a hierarchical order based on their cognitive requirement on LLMs when compared to human mental capabilities. It assesses the complexity of tasks with the Hierarchical Prompting Index (HPI), which demonstrates the cognitive competencies of LLMs across diverse datasets and offers insights into the cognitive demands that datasets place on different LLMs. This approach enables a comprehensive evaluation of an LLMs problem solving abilities and the intricacy of a dataset, offering a standardized metric for task complexity. Extensive experiments with multiple datasets and LLMs show that HPF enhances LLM performance by 2% to 63% compared to baseline performance, with GSM8k being the most cognitively complex task among reasoning and coding tasks with an average HPI of 3.20 confirming the effectiveness of HPT. To support future research and reproducibility in this domain, the implementations of HPT and HPF are available here.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9042.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9042.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Massive Multitask Language Understanding (MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A wide-coverage multiple-choice benchmark assessing general knowledge and multi-domain reasoning across many subjects; used here to probe LLMs' general knowledge and low-to-moderate reasoning demands under the HPT framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated in the paper; includes proprietary models (GPT-4o, Claude 3.5 Sonnet) and open/research models (Mistral-Nemo 12B, Gemma-2 9B, Llama-3 8B, Gemma 7B, Mistral 7B). Models were evaluated under the Hierarchical Prompting Framework (HPF) using five prompting strategies and HPI as a complexity metric.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MMLU (general knowledge / multitask benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice exam-style benchmark covering many school and professional subjects; assesses general knowledge and multi-domain reasoning (aligned to HPT's lower-to-mid cognitive demands).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported per model (HPI and accuracy): GPT-4o — HPI 1.81, Accuracy 91.61%; Claude 3.5 Sonnet — HPI 1.84, Accuracy 92.16%; Mistral-Nemo 12B — HPI 2.45, Accuracy 89.75%; Gemma-2 9B — HPI 2.34, Accuracy 87.28%; Llama-3 8B — HPI 2.84, Accuracy 82.63%; Gemma 7B — HPI 2.93, Accuracy 83.31%; Mistral 7B — HPI 2.89, Accuracy 81.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Top proprietary models (Claude 3.5 Sonnet, GPT-4o) show the highest accuracies with low HPI (low cognitive prompting needed); several smaller models approach competitive accuracy but generally have higher HPI. The paper notes that best HPI (minimal prompting complexity) does not always coincide with best accuracy (e.g., GPT-4o best HPI whereas Claude 3.5 Sonnet achieves highest accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used HPF: five prompting strategies (Role, Zero-shot CoT, 3-shot CoT, Least-to-Most, Generated Knowledge Prompting); HPI is the complexity level where the model first solves an instance. LLM-as-a-Judge (GPT-4o) and human annotators (representative 5% subset) were used to calibrate HPI. Prompts and thresholds follow templates described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human accuracy baseline on MMLU provided in paper; HPI is influenced by human annotator judgments (representative set only 5%); HPI measures prompting complexity required rather than cognitive parity with humans; dataset-level metrics may mask per-item differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9042.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GSM8k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GSM8k (Grade School Math 8K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A collection of multi-step grade-school math word problems used to evaluate multi-step mathematical reasoning in LLMs; in this paper used to probe higher-order reasoning demands under HPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of instruction-tuned LLMs as above; models were evaluated across HPF prompting strategies and HPI measured for each instance to quantify cognitive demand required to solve math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>GSM8k (multi-step math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Dataset of grade-school level multi-step math problems assessing multi-step reasoning and working-memory-like decomposition of steps.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported per model (HPI and accuracy): GPT-4o — HPI 1.71, Accuracy 96.43%; Claude 3.5 Sonnet — HPI 1.35, Accuracy 97.72%; Mistral-Nemo 12B — HPI 3.01, Accuracy 86.80%; Gemma-2 9B — HPI 2.17, Accuracy 91.28%; Llama-3 8B — HPI 2.34, Accuracy 86.20%; Gemma 7B — HPI 6.70, Accuracy 27.88%; Mistral 7B — HPI 5.11, Accuracy 46.93%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Top proprietary models (Claude 3.5 Sonnet, GPT-4o) attain high accuracy with low HPI (require less complex prompting); several smaller SLMs (Gemma 7B, Mistral 7B) perform much worse and require higher HPI, indicating they need more complex prompting strategies to solve GSM8k problems.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>HPF progression applied per instance until solved; HPI records complexity of prompt needed. Zero-shot and few-shot CoT variants and more structured strategies (Least-to-Most, Generated Knowledge) were included. GSM8k considered the most cognitively complex among reasoning/coding in average HPI reported elsewhere in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human performance on GSM8k not reported here; some HPI values (very high for certain small models) indicate those models failed under the five strategies rather than true human-comparable cognitive assessment; the dataset probes mathematical reasoning but is not a formal human cognitive psychology test.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9042.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HumanEval (code generation benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code generation benchmark consisting of function-level programming tasks used to evaluate LLMs' code-writing and problem decomposition capabilities; treated as a high-cognitive-demand task in HPF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated with HPF; code generation assessed with pass@k metric (paper reports pass@1) and HPI as prompt complexity indicator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>HumanEval (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A suite of code-writing tasks measuring an LLM's ability to generate correct function implementations; evaluates multi-step synthesis and application of knowledge akin to high-level problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Pass@1 and HPI reported: GPT-4o — HPI 2.25, Pass@1 0.95; Claude 3.5 Sonnet — HPI 1.04, Pass@1 1.00; Mistral-Nemo 12B — HPI 2.07, Pass@1 0.96; Gemma-2 9B — HPI 1.01, Pass@1 0.91; Llama-3 8B — HPI 1.03, Pass@1 1.00; Gemma 7B — HPI 3.71, Pass@1 0.79; Mistral 7B — HPI 1.10, Pass@1 0.93.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Several SLMs with appropriate prompting reach pass@1 rates comparable to leading LLMs; Claude 3.5 Sonnet and Llama-3 8B achieve perfect pass@1 in reported results. HPF gave particularly large relative improvements on HumanEval for many SLMs (e.g., Mistral 7B, Gemma-2 9B).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Code tasks evaluated using pass@k (paper reports pass@1). HPF strategies systematically escalated until solution; HPI tracks complexity level needed. Prompt templates included role-based and chain-of-thought styles; generator and external-knowledge prompts (GKP) used for highest complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human programmer baseline not provided; code correctness measured automatically (pass@1) which may not map to human cognitive ability; HPI differences were small between top models, so claiming parity with human programmers is not supported by human baseline data in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9042.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BoolQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BoolQ (Boolean question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset of natural yes/no questions grounded in Wikipedia passages; used to probe simple comprehension and recall (lower cognitive load) under the HPT framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated with HPF templates for Boolean QA; often solved with low-complexity prompts per HPI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>BoolQ (boolean reading comprehension)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Yes/No questions requiring passage understanding and factual recall; corresponds to basic recall/understanding cognitive levels in HPT.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported per model (HPI and accuracy): GPT-4o — HPI 1.32, Accuracy 96.82%; Claude 3.5 Sonnet — HPI 1.20, Accuracy 99.81%; Mistral-Nemo 12B — HPI 1.75, Accuracy 99.87%; Gemma-2 9B — HPI 1.30, Accuracy 98.28%; Llama-3 8B — HPI 1.37, Accuracy 99.30%; Gemma 7B — HPI 1.45, Accuracy 99.42%; Mistral 7B — HPI 1.41, Accuracy 98.07%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Almost all evaluated models achieve near-perfect accuracy with low HPI, leading the authors to conclude BoolQ is insufficiently challenging for modern LLMs and not a good discriminator of higher cognitive abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt templates for Boolean QA used Role, Zero-shot CoT, and higher HPF strategies as needed; HPI per instance computed to reflect the minimal prompting complexity needed to answer correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline reported; near-ceiling performance makes BoolQ a poor probe of higher-level cognitive capabilities; HPI being low across models implies dataset complexity < present model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9042.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CSQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CommonsenseQA (CSQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multiple-choice commonsense reasoning benchmark designed to evaluate models' everyday commonsense inference abilities; used here to probe mid-level analysis and reasoning under HPT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs tested with HPF across prompting strategies; CSQA assesses commonsense reasoning rather than pure factual recall.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>CSQA (CommonsenseQA)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Multiple-choice questions requiring commonsense knowledge and reasoning; assesses analysis and moderate reasoning tasks in HPT.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported per model (HPI and accuracy): GPT-4o — HPI 1.65, Accuracy 92.54%; Claude 3.5 Sonnet — HPI 2.01, Accuracy 86.15%; Mistral-Nemo 12B — HPI 2.06, Accuracy 90.17%; Gemma-2 9B — HPI 1.94, Accuracy 88.86%; Llama-3 8B — HPI 2.43, Accuracy 84.76%; Gemma 7B — HPI 2.50, Accuracy 83.78%; Mistral 7B — HPI 2.49, Accuracy 82.06%.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Most models perform strongly, though proprietary models (GPT-4o) and some larger SLMs show higher accuracy; HPI differences indicate variability in how much prompting complexity is needed per model. GPT-4o shows better trade-off of low HPI and high accuracy here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Standard HPF escalation used; prompts include role, CoT variants and higher-level strategies. Both LLM-as-a-Judge and human annotators were used to validate complexity ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not include human accuracy baselines; CSQA measures commonsense reasoning but is still a dataset benchmark rather than a controlled cognitive-psychology experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9042.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IWSLT / SamSum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IWSLT-2017 en-fr (translation) and SamSum (dialogue summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Machine translation and dialogue summarization datasets used to evaluate LLMs' language encoding/decoding and summarization abilities under HPF, with thresholded scoring (BLEU/ROUGE-L) tied to HPI levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs evaluated on translation (IWSLT) and summarization (SamSum) with HPF strategies; evaluation used BLEU for translation and ROUGE-L for summarization with thresholds (0.15, 0.20, additional 0.25/0.30 explored).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>IWSLT-2017 en-fr (translation) and SamSum (dialogue summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>IWSLT: TED-talk parallel corpus used to measure translation (nuance, adequacy). SamSum: human-annotated dialogue summaries measuring summarization quality (informational condensation). Both relate to language understanding and application.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Examples from Table 3 (threshold 0.15 / 0.20): GPT-4o — IWSLT HPI 2.66 / 3.08 with BLEU ~0.32; SamSum HPI 1.11 / 1.21 with ROUGE-L ~0.30. Claude 3.5 Sonnet — IWSLT HPI 4.63 / 4.87 BLEU ~0.20; SamSum HPI 1.25 / 1.60 ROUGE-L ~0.23. Other models report varying BLEU/ROUGE-L and HPI values (see Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4o consistently achieves higher translation and summarization metrics with comparatively lower HPI in these tasks, while Claude 3.5 Sonnet is comparatively weaker on summarization/translation despite strong reasoning performance on other benchmarks. Authors note performance saturation across thresholds (increasing threshold yields little BLEU/ROUGE gains but raises HPI).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation used thresholded completion criteria (0.15 and 0.20) to define success at each HPF level; additional thresholds (0.25, 0.30) explored showing performance plateau and rising HPI. Standard HPF prompting templates for translation/summarization applied.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No human baseline scores provided; threshold-based success definitions are arbitrary and influence HPI values; translation/summarization metrics (BLEU/ROUGE-L) imperfectly map to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9042.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9042.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-Judge / HPT calibration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Judge evaluation and human HPI calibration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method for aligning HPF prompting complexity levels with human cognitive principles by using GPT-4o as an automated judge and human annotators (representative subset) to assign HPI penalties and calibrate complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (judge); Llama-3 8B (prompt-selector in Adaptive HPF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o is used as an LLM judge scoring prompting strategies against HPT criteria; Llama-3 8B used as the prompt-selector in the Adaptive HPF zero-shot experiments (noted to hallucinate).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4o: not reported; Llama-3: 8B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Human-annotator calibration / LLM-as-a-Judge scoring (applies across datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Scoring schema based on four HPT rules (Basic Recall, Understanding, Analysis & Reasoning, Application/Execution) where criteria are rated 1–5; used to assign HPI and to compare LLM and human judgments of prompt complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Paper reports that GPT-4o as an LLM judge shows consistent hierarchy with less variability than human judges; specific numeric judge scores aggregated across prompting strategies are presented in appendices and figures (e.g., Figure 7, Figure 8).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLM-as-a-Judge (GPT-4o) replicates human-annotator hierarchy with less variability, supporting HPT alignment between human cognitive principles and LLM behavior; however, the paper notes differences and that LLM judges can diverge in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>System prompt instructs GPT-4o to score each prompting strategy on the four HPT criteria (1-5). Human annotators scored a 5% representative subset to introduce HPI penalties when LLMs fail. Adaptive HPF used Llama-3 8B as an automatic prompt-selector in zero-shot mode; this selector exhibited hallucinations and higher HPI.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human annotation limited (5% representative set), introducing potential bias; LLM-as-a-Judge may be less variable but can encode model-specific biases; Adaptive HPF prompt-selector hallucinations indicate automated selection is unreliable without further safety/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Generated Knowledge Prompting for Commonsense Reasoning <em>(Rating: 2)</em></li>
                <li>Measuring Massive Multitask Language Understanding <em>(Rating: 2)</em></li>
                <li>PromptBench: A Unified Library for Evaluation of Large Language Models <em>(Rating: 1)</em></li>
                <li>Progressive-Hint Prompting Improves Reasoning in Large Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9042",
    "paper_id": "paper-274656281",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "MMLU",
            "name_full": "Massive Multitask Language Understanding (MMLU)",
            "brief_description": "A wide-coverage multiple-choice benchmark assessing general knowledge and multi-domain reasoning across many subjects; used here to probe LLMs' general knowledge and low-to-moderate reasoning demands under the HPT framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Instruction-tuned LLMs evaluated in the paper; includes proprietary models (GPT-4o, Claude 3.5 Sonnet) and open/research models (Mistral-Nemo 12B, Gemma-2 9B, Llama-3 8B, Gemma 7B, Mistral 7B). Models were evaluated under the Hierarchical Prompting Framework (HPF) using five prompting strategies and HPI as a complexity metric.",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "MMLU (general knowledge / multitask benchmark)",
            "test_description": "Multiple-choice exam-style benchmark covering many school and professional subjects; assesses general knowledge and multi-domain reasoning (aligned to HPT's lower-to-mid cognitive demands).",
            "llm_performance": "Reported per model (HPI and accuracy): GPT-4o — HPI 1.81, Accuracy 91.61%; Claude 3.5 Sonnet — HPI 1.84, Accuracy 92.16%; Mistral-Nemo 12B — HPI 2.45, Accuracy 89.75%; Gemma-2 9B — HPI 2.34, Accuracy 87.28%; Llama-3 8B — HPI 2.84, Accuracy 82.63%; Gemma 7B — HPI 2.93, Accuracy 83.31%; Mistral 7B — HPI 2.89, Accuracy 81.45%.",
            "human_baseline_performance": null,
            "performance_comparison": "Top proprietary models (Claude 3.5 Sonnet, GPT-4o) show the highest accuracies with low HPI (low cognitive prompting needed); several smaller models approach competitive accuracy but generally have higher HPI. The paper notes that best HPI (minimal prompting complexity) does not always coincide with best accuracy (e.g., GPT-4o best HPI whereas Claude 3.5 Sonnet achieves highest accuracy).",
            "experimental_details": "Evaluation used HPF: five prompting strategies (Role, Zero-shot CoT, 3-shot CoT, Least-to-Most, Generated Knowledge Prompting); HPI is the complexity level where the model first solves an instance. LLM-as-a-Judge (GPT-4o) and human annotators (representative 5% subset) were used to calibrate HPI. Prompts and thresholds follow templates described in the paper.",
            "limitations_or_caveats": "No human accuracy baseline on MMLU provided in paper; HPI is influenced by human annotator judgments (representative set only 5%); HPI measures prompting complexity required rather than cognitive parity with humans; dataset-level metrics may mask per-item differences.",
            "uuid": "e9042.0",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GSM8k",
            "name_full": "GSM8k (Grade School Math 8K)",
            "brief_description": "A collection of multi-step grade-school math word problems used to evaluate multi-step mathematical reasoning in LLMs; in this paper used to probe higher-order reasoning demands under HPT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Same set of instruction-tuned LLMs as above; models were evaluated across HPF prompting strategies and HPI measured for each instance to quantify cognitive demand required to solve math problems.",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "GSM8k (multi-step math word problems)",
            "test_description": "Dataset of grade-school level multi-step math problems assessing multi-step reasoning and working-memory-like decomposition of steps.",
            "llm_performance": "Reported per model (HPI and accuracy): GPT-4o — HPI 1.71, Accuracy 96.43%; Claude 3.5 Sonnet — HPI 1.35, Accuracy 97.72%; Mistral-Nemo 12B — HPI 3.01, Accuracy 86.80%; Gemma-2 9B — HPI 2.17, Accuracy 91.28%; Llama-3 8B — HPI 2.34, Accuracy 86.20%; Gemma 7B — HPI 6.70, Accuracy 27.88%; Mistral 7B — HPI 5.11, Accuracy 46.93%.",
            "human_baseline_performance": null,
            "performance_comparison": "Top proprietary models (Claude 3.5 Sonnet, GPT-4o) attain high accuracy with low HPI (require less complex prompting); several smaller SLMs (Gemma 7B, Mistral 7B) perform much worse and require higher HPI, indicating they need more complex prompting strategies to solve GSM8k problems.",
            "experimental_details": "HPF progression applied per instance until solved; HPI records complexity of prompt needed. Zero-shot and few-shot CoT variants and more structured strategies (Least-to-Most, Generated Knowledge) were included. GSM8k considered the most cognitively complex among reasoning/coding in average HPI reported elsewhere in paper.",
            "limitations_or_caveats": "Human performance on GSM8k not reported here; some HPI values (very high for certain small models) indicate those models failed under the five strategies rather than true human-comparable cognitive assessment; the dataset probes mathematical reasoning but is not a formal human cognitive psychology test.",
            "uuid": "e9042.1",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "HumanEval",
            "name_full": "HumanEval (code generation benchmark)",
            "brief_description": "A code generation benchmark consisting of function-level programming tasks used to evaluate LLMs' code-writing and problem decomposition capabilities; treated as a high-cognitive-demand task in HPF.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Instruction-tuned LLMs evaluated with HPF; code generation assessed with pass@k metric (paper reports pass@1) and HPI as prompt complexity indicator.",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "HumanEval (code generation)",
            "test_description": "A suite of code-writing tasks measuring an LLM's ability to generate correct function implementations; evaluates multi-step synthesis and application of knowledge akin to high-level problem solving.",
            "llm_performance": "Pass@1 and HPI reported: GPT-4o — HPI 2.25, Pass@1 0.95; Claude 3.5 Sonnet — HPI 1.04, Pass@1 1.00; Mistral-Nemo 12B — HPI 2.07, Pass@1 0.96; Gemma-2 9B — HPI 1.01, Pass@1 0.91; Llama-3 8B — HPI 1.03, Pass@1 1.00; Gemma 7B — HPI 3.71, Pass@1 0.79; Mistral 7B — HPI 1.10, Pass@1 0.93.",
            "human_baseline_performance": null,
            "performance_comparison": "Several SLMs with appropriate prompting reach pass@1 rates comparable to leading LLMs; Claude 3.5 Sonnet and Llama-3 8B achieve perfect pass@1 in reported results. HPF gave particularly large relative improvements on HumanEval for many SLMs (e.g., Mistral 7B, Gemma-2 9B).",
            "experimental_details": "Code tasks evaluated using pass@k (paper reports pass@1). HPF strategies systematically escalated until solution; HPI tracks complexity level needed. Prompt templates included role-based and chain-of-thought styles; generator and external-knowledge prompts (GKP) used for highest complexity.",
            "limitations_or_caveats": "Human programmer baseline not provided; code correctness measured automatically (pass@1) which may not map to human cognitive ability; HPI differences were small between top models, so claiming parity with human programmers is not supported by human baseline data in this paper.",
            "uuid": "e9042.2",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BoolQ",
            "name_full": "BoolQ (Boolean question answering)",
            "brief_description": "A dataset of natural yes/no questions grounded in Wikipedia passages; used to probe simple comprehension and recall (lower cognitive load) under the HPT framework.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Instruction-tuned LLMs evaluated with HPF templates for Boolean QA; often solved with low-complexity prompts per HPI.",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "BoolQ (boolean reading comprehension)",
            "test_description": "Yes/No questions requiring passage understanding and factual recall; corresponds to basic recall/understanding cognitive levels in HPT.",
            "llm_performance": "Reported per model (HPI and accuracy): GPT-4o — HPI 1.32, Accuracy 96.82%; Claude 3.5 Sonnet — HPI 1.20, Accuracy 99.81%; Mistral-Nemo 12B — HPI 1.75, Accuracy 99.87%; Gemma-2 9B — HPI 1.30, Accuracy 98.28%; Llama-3 8B — HPI 1.37, Accuracy 99.30%; Gemma 7B — HPI 1.45, Accuracy 99.42%; Mistral 7B — HPI 1.41, Accuracy 98.07%.",
            "human_baseline_performance": null,
            "performance_comparison": "Almost all evaluated models achieve near-perfect accuracy with low HPI, leading the authors to conclude BoolQ is insufficiently challenging for modern LLMs and not a good discriminator of higher cognitive abilities.",
            "experimental_details": "Prompt templates for Boolean QA used Role, Zero-shot CoT, and higher HPF strategies as needed; HPI per instance computed to reflect the minimal prompting complexity needed to answer correctly.",
            "limitations_or_caveats": "No human baseline reported; near-ceiling performance makes BoolQ a poor probe of higher-level cognitive capabilities; HPI being low across models implies dataset complexity &lt; present model capabilities.",
            "uuid": "e9042.3",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CSQA",
            "name_full": "CommonsenseQA (CSQA)",
            "brief_description": "A multiple-choice commonsense reasoning benchmark designed to evaluate models' everyday commonsense inference abilities; used here to probe mid-level analysis and reasoning under HPT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Instruction-tuned LLMs tested with HPF across prompting strategies; CSQA assesses commonsense reasoning rather than pure factual recall.",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "CSQA (CommonsenseQA)",
            "test_description": "Multiple-choice questions requiring commonsense knowledge and reasoning; assesses analysis and moderate reasoning tasks in HPT.",
            "llm_performance": "Reported per model (HPI and accuracy): GPT-4o — HPI 1.65, Accuracy 92.54%; Claude 3.5 Sonnet — HPI 2.01, Accuracy 86.15%; Mistral-Nemo 12B — HPI 2.06, Accuracy 90.17%; Gemma-2 9B — HPI 1.94, Accuracy 88.86%; Llama-3 8B — HPI 2.43, Accuracy 84.76%; Gemma 7B — HPI 2.50, Accuracy 83.78%; Mistral 7B — HPI 2.49, Accuracy 82.06%.",
            "human_baseline_performance": null,
            "performance_comparison": "Most models perform strongly, though proprietary models (GPT-4o) and some larger SLMs show higher accuracy; HPI differences indicate variability in how much prompting complexity is needed per model. GPT-4o shows better trade-off of low HPI and high accuracy here.",
            "experimental_details": "Standard HPF escalation used; prompts include role, CoT variants and higher-level strategies. Both LLM-as-a-Judge and human annotators were used to validate complexity ratings.",
            "limitations_or_caveats": "Paper does not include human accuracy baselines; CSQA measures commonsense reasoning but is still a dataset benchmark rather than a controlled cognitive-psychology experiment.",
            "uuid": "e9042.4",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "IWSLT / SamSum",
            "name_full": "IWSLT-2017 en-fr (translation) and SamSum (dialogue summarization)",
            "brief_description": "Machine translation and dialogue summarization datasets used to evaluate LLMs' language encoding/decoding and summarization abilities under HPF, with thresholded scoring (BLEU/ROUGE-L) tied to HPI levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple: GPT-4o; Claude 3.5 Sonnet; Mistral-Nemo 12B; Gemma-2 9B; Llama-3 8B; Gemma 7B; Mistral 7B",
            "model_description": "Instruction-tuned LLMs evaluated on translation (IWSLT) and summarization (SamSum) with HPF strategies; evaluation used BLEU for translation and ROUGE-L for summarization with thresholds (0.15, 0.20, additional 0.25/0.30 explored).",
            "model_size": "GPT-4o: not reported; Claude 3.5 Sonnet: not reported; Mistral-Nemo: 12B; Gemma-2: 9B; Llama-3: 8B; Gemma: 7B; Mistral: 7B",
            "test_battery_name": "IWSLT-2017 en-fr (translation) and SamSum (dialogue summarization)",
            "test_description": "IWSLT: TED-talk parallel corpus used to measure translation (nuance, adequacy). SamSum: human-annotated dialogue summaries measuring summarization quality (informational condensation). Both relate to language understanding and application.",
            "llm_performance": "Examples from Table 3 (threshold 0.15 / 0.20): GPT-4o — IWSLT HPI 2.66 / 3.08 with BLEU ~0.32; SamSum HPI 1.11 / 1.21 with ROUGE-L ~0.30. Claude 3.5 Sonnet — IWSLT HPI 4.63 / 4.87 BLEU ~0.20; SamSum HPI 1.25 / 1.60 ROUGE-L ~0.23. Other models report varying BLEU/ROUGE-L and HPI values (see Table 3).",
            "human_baseline_performance": null,
            "performance_comparison": "GPT-4o consistently achieves higher translation and summarization metrics with comparatively lower HPI in these tasks, while Claude 3.5 Sonnet is comparatively weaker on summarization/translation despite strong reasoning performance on other benchmarks. Authors note performance saturation across thresholds (increasing threshold yields little BLEU/ROUGE gains but raises HPI).",
            "experimental_details": "Evaluation used thresholded completion criteria (0.15 and 0.20) to define success at each HPF level; additional thresholds (0.25, 0.30) explored showing performance plateau and rising HPI. Standard HPF prompting templates for translation/summarization applied.",
            "limitations_or_caveats": "No human baseline scores provided; threshold-based success definitions are arbitrary and influence HPI values; translation/summarization metrics (BLEU/ROUGE-L) imperfectly map to human judgments.",
            "uuid": "e9042.5",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-as-a-Judge / HPT calibration",
            "name_full": "LLM-as-a-Judge evaluation and human HPI calibration",
            "brief_description": "Method for aligning HPF prompting complexity levels with human cognitive principles by using GPT-4o as an automated judge and human annotators (representative subset) to assign HPI penalties and calibrate complexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o (judge); Llama-3 8B (prompt-selector in Adaptive HPF)",
            "model_description": "GPT-4o is used as an LLM judge scoring prompting strategies against HPT criteria; Llama-3 8B used as the prompt-selector in the Adaptive HPF zero-shot experiments (noted to hallucinate).",
            "model_size": "GPT-4o: not reported; Llama-3: 8B",
            "test_battery_name": "Human-annotator calibration / LLM-as-a-Judge scoring (applies across datasets)",
            "test_description": "Scoring schema based on four HPT rules (Basic Recall, Understanding, Analysis & Reasoning, Application/Execution) where criteria are rated 1–5; used to assign HPI and to compare LLM and human judgments of prompt complexity.",
            "llm_performance": "Paper reports that GPT-4o as an LLM judge shows consistent hierarchy with less variability than human judges; specific numeric judge scores aggregated across prompting strategies are presented in appendices and figures (e.g., Figure 7, Figure 8).",
            "human_baseline_performance": null,
            "performance_comparison": "LLM-as-a-Judge (GPT-4o) replicates human-annotator hierarchy with less variability, supporting HPT alignment between human cognitive principles and LLM behavior; however, the paper notes differences and that LLM judges can diverge in some cases.",
            "experimental_details": "System prompt instructs GPT-4o to score each prompting strategy on the four HPT criteria (1-5). Human annotators scored a 5% representative subset to introduce HPI penalties when LLMs fail. Adaptive HPF used Llama-3 8B as an automatic prompt-selector in zero-shot mode; this selector exhibited hallucinations and higher HPI.",
            "limitations_or_caveats": "Human annotation limited (5% representative set), introducing potential bias; LLM-as-a-Judge may be less variable but can encode model-specific biases; Adaptive HPF prompt-selector hallucinations indicate automated selection is unreliable without further safety/fine-tuning.",
            "uuid": "e9042.6",
            "source_info": {
                "paper_title": "Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Generated Knowledge Prompting for Commonsense Reasoning",
            "rating": 2,
            "sanitized_title": "generated_knowledge_prompting_for_commonsense_reasoning"
        },
        {
            "paper_title": "Measuring Massive Multitask Language Understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "PromptBench: A Unified Library for Evaluation of Large Language Models",
            "rating": 1,
            "sanitized_title": "promptbench_a_unified_library_for_evaluation_of_large_language_models"
        },
        {
            "paper_title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "progressivehint_prompting_improves_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.01955775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles
21 Jul 2025</p>
<p>Devichand Budagam 
India Ashutosh Kharagpur 
Kumar 
Vinija Jain 
Aman 2025 Chadha 
Ashutosh Kumar 
Mahsa Khoshnoodi 
Sankalp Kj 
Hierarchical </p>
<p>Indian Institute of Technology</p>
<p>Rochester Institute of Technology
USA</p>
<p>Mahsa Khoshnoodi Researcher
Fatima Fellowship USA</p>
<p>Sankalp KJ AI Institute
University of South Carolina
USA</p>
<p>Amazon GenAI Stanford University
USA</p>
<p>Amazon GenAI James Silberrad Brown Center for AI</p>
<p>San Diego State University Stanford University
USA</p>
<p>Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles. In . ACM
18 pagesNew YorkNYUSA</p>
<p>Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models Aligned with Human Cognitive Principles
21 Jul 202530324FD4FA2D40191A6DE41D91B3375310.1145/nnnnnnn.nnnnnnnarXiv:2406.12644v5[cs.CL]Prompting TaxonomyCognitive DemandsPrompt Optimization
Assessing the effectiveness of large language models (LLMs) in performing different tasks is crucial for understanding their strengths and weaknesses.This paper presents Hierarchical Prompting Taxonomy (HPT), grounded on human cognitive principles and designed to assess LLMs by examining the cognitive demands of various tasks.The HPT utilizes the Hierarchical Prompting Framework (HPF), which structures five unique prompting strategies in a hierarchical order based on their cognitive requirement on LLMs when compared to human mental capabilities.It assesses the complexity of tasks with the Hierarchical Prompting Index (HPI), which demonstrates the cognitive competencies of LLMs across diverse datasets and offers insights into the cognitive demands that datasets place on different LLMs.This approach enables a comprehensive evaluation of LLM's problem-solving abilities and the intricacy of a dataset, offering a standardized metric for task complexity.Extensive experiments with multiple datasets and LLMs show that HPF enhances LLM performance by 2 → 63% compared to baseline performance, with GSM8k being the most cognitively complex task among reasoning and coding tasks with an average HPI of 3.20 confirming the effectiveness of HPT.To support future research in this domain, the implementations of HPT and HPF are publicly available 1 .CCS Concepts• Computing methodologies → Natural language generation; Reasoning about belief and knowledge; Cognitive science.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have revolutionized natural language processing (NLP), enabling significant advancements in a wide range of applications.Conventional evaluation frameworks often apply a standard prompting approach to assess different LLMs, regardless of the complexity of the task, which may result in biased and suboptimal outcomes.Moreover, applying the same prompting approach across all samples within a dataset without considering each sample's relative complexity adds to the unfair situation.To achieve a more balanced evaluation framework, it is essential to account for both the task-solving ability of LLMs and the varying cognitive complexities of the dataset samples.This limitation highlights the need for more sophisticated evaluation methods that can adapt to varying levels of sample task complexity.This study defines complexity as the cognitive demands imposed by a task or the cognitive load introduced by a prompting strategy on LLMs.Task complexity in human cognition reflects the mental effort required for processing, analyzing, and synthesizing information.As Sweller [30] noted, complexity increases with greater cognitive resource demands, engaging working memory in reasoning and problemsolving.Similarly, Anderson et al. [2] describes cognitive abilities as a continuum, from basic recall to higher-order thinking, with difficulty rising for tasks requiring analysis, synthesis, and evaluation.By mapping LLM prompting strategies onto this hierarchy, we systematically assess how LLMs handle varying cognitive loads.This framework provides a structured, cognitively grounded method for evaluating model performance across tasks of differing complexity.This study is directed by the following research questions: Research Questions:</p>
<p>• RQ1: Does cognitively inspired strategic selection of prompts enable small language models (SLMs) to match the performance of LLMs? • RQ2: How can cognitive demand measurements of LLMs guide model selection and deployment decisions beyond traditional metrics?• RQ3: How can we align prompt complexity with task demands to optimize both computational efficiency and performance?</p>
<p>This paper introduces the HPT, a set of rules that maps the human cognitive principles for assessing the complexity of different prompting strategies.It employs the HPF shown in Figure 1, a prompt selection framework that selects the prompt with the optimal cognitive load on LLM required in solving the task.The main contributions of this work are:</p>
<p>• Hierarchical Prompting Taxonomy (HPT): The paper introduces HPT, rules mapping prompting strategies to human cognitive principles, enabling a universal measure of LLMs' task complexity.• Hierarchical Prompting Framework (HPF): The HPF framework selects the best prompt from five strategies to optimize LLMs' cognitive load, improving evaluation and performance transparency.• Hierarchical Prompting Index (HPI): HPI 2 quantitatively assesses LLMs' task complexity across datasets, revealing cognitive demands on various LLMs.HPF can be compared to an "open book" exam (see Figure 2), with tasks analogous to questions and prompting strategies akin to textbooks.The exam questions, ranging from basic recall to complex analysis, parallel the cognitive challenges in HPT tasks.Similarly, textbooks offer structured support, much like HPF, which arranges prompts by complexity to assist LLMs.A glossary lookup represents a task with low complexity, whereas solving a multi-step analytical 2 HPI can be quantitatively assessed to analyze the cognitive abilities of an LLM and the cognitive demands imposed by datasets on LLMs, as both factors are interchangeably related to the complexity of tasks.HPF components (below) mirror traditional educational assessment elements (above), with parallel relationships between task complexity levels, resource utilization (prompts/textbooks), and performance metrics (HPI/student effort).This comparison demonstrates how LLM task complexity scales similarly to educational assessment complexity, from simple lookup tasks to complex synthesis problems.</p>
<p>problem indicates high complexity.The effort exerted by a student is similar to HPI, which measures the cognitive demand on LLMs.Just as structured learning materials improve students' performance, carefully crafted hierarchical prompts help LLMs in addressing increasingly complex tasks more effectively.The remainder of the paper is structured as follows: Section 2 reviews the related work on prompting and evaluation in LLMs.Section 3 details the HPT and its associated frameworks.Section 4 outlines the experimental setup, results, and ablation studies.Section 5 concludes the paper.Section 6 discusses the limitations of the work.Section 7 discusses the ethical impact of the work.</p>
<p>Related Work</p>
<p>The advent of LLMs has revolutionized NLP by demonstrating significant improvements in few-shot and zero-shot learning capabilities.Brown et al. [6] introduced GPT-3, a 175 billion parameter autoregressive model, showcasing its ability to perform a wide range of tasks such as question-answering, reading comprehension, translation, and natural language inference without fine-tuning.This study highlighted the potential of very large models for in-context learning while also identifying limitations in commonsense reasoning and specific comprehension tasks.Similarly, Liu et al. [23] surveyed prompt-based learning, emphasizing the role of prompt engineering in leveraging pre-trained models for few-shot and zero-shot adaptation to new tasks with minimal labeled data.</p>
<p>Prompt Engineering</p>
<p>Prompting plays a vital role in unlocking the full potential of LLMs.By designing specific input prompts, the LLM's responses can be guided, significantly influencing the quality and relevance of the output.Effective prompting strategies have enhanced LLM performance on tasks ranging from simple question-answering to complex reasoning and problem-solving.Recent research has explored various approaches to prompting and reasoning evaluation in LLMs.Chainof-Thought (CoT) prompting [39] elicits step-by-step reasoning, improving performance on complex tasks.Specializing smaller models [13] and using large models as reasoning teachers [16] have demonstrated the potential for enhancing reasoning capabilities.Emergent abilities in LLMs, which appear suddenly at certain scale thresholds, have also been a topic of interest.Wei et al. [38] examined these abilities in few-shot prompting, discussing the underlying factors and implications for future scaling.Complementing this, Kojima et al. [19] demonstrated that LLMs could exhibit multi-step reasoning capabilities in a zero-shot setting by simply modifying the prompt structure, thus highlighting their potential as general reasoning engines.Yao et al. [40] introduced the Tree-of-Thoughts framework, enabling LLMs to deliberate over coherent text units and perform heuristic searches for complex reasoning tasks.This approach generalizes over chain-of-thought prompting and has shown significant performance improvements in tasks requiring planning and search, such as creative writing and problem-solving games.Kong et al. [20] introduced role-play prompting to improve zero-shot reasoning by constructing role-immersion interactions, which implicitly trigger chain-of-thought processes and enhance performance across diverse reasoning benchmarks.Progressive-hint prompting [41] has been proposed to conceptualize answer generation and guide LLMs toward correct responses.Metacognitive prompting [37] incorporates self-aware evaluations to enhance understanding abilities.</p>
<p>These studies highlight progress in using innovative prompting techniques to improve LLMs' emergent abilities, reasoning, interaction strategies, robustness, and evaluation.Yet, challenges persist in prompt design, managing complex reasoning tasks, and performance evaluation across various scenarios.Although LLMs show promising emergent abilities, they frequently lack predictability and control, and their resistance to misleading prompts is still an issue.</p>
<p>Prompt Optimization and Selection</p>
<p>The challenge of optimizing prompts for LLMs has been addressed in several key studies, each contributing unique methodologies to enhance model performance and efficiency.Shen et al. [29] introduce PFLAT, a metric utilizing flatness regularization to quantify prompt utility, which leads to improved results in classification tasks.Do et al. [12] propose a structured three-step methodology that contains data clustering, prompt generation, and evaluation, effectively balancing generality and specificity in prompt selection.ProTeGi [27] offers a non-parametric approach inspired by gradient descent, leveraging natural language "gradients" to iteratively refine prompts.Wang et al. [36] present PromISe, which transforms prompt optimization into an explicit chain of thought, employing self-introspection and refinement techniques.Zhou et al. [43] proposed DYNAICL, a framework for efficient prompting that dynamically allocates incontext examples based on a meta-controller's predictions, achieving better performance-efficiency trade-offs compared to uniform example allocation.</p>
<p>These studies seek to automate prompt design, reducing reliance on manual trial-and-error while improving efficiency and scalability across tasks and models.They report performance gains of 5% to 31% across benchmarks, highlighting the growing significance of prompt optimization.Future research directions include exploring theoretical foundations, combining optimization techniques, and differentiating task-specific from general-purpose strategies.</p>
<p>Evaluation Benchmarks</p>
<p>To facilitate the evaluation and understanding of LLM capabilities, Zhu et al. [44] introduced PromptBench, a unified library encompassing a variety of LLMs, datasets, evaluation protocols, and adversarial prompt attacks.This modular and extensible tool aims to support collaborative research and advance the comprehension of LLM strengths and weaknesses.Further exploring reasoning capabilities, Qiao et al. [28] categorized various prompting methods and evaluated their effectiveness across different model scales and reasoning tasks, identifying key open questions for achieving robust and generalizable reasoning.[35] introduced a multitask benchmark for LLM robustness evaluation, which extends the original GLUE [34] benchmark to assess model robustness against adversarial inputs.It incorporates perturbed versions of existing GLUE tasks, such as paraphrasing, negation, and noise, to test models' abilities with challenging data.The study highlights that despite their success on clean datasets, state-of-the-art models often struggle with adversarial examples, underscoring the importance of robustness evaluations in model development.</p>
<p>3 Hierarchical Prompting Taxonomy 3.1 Governing Rules Figure 3 illustrates the HPT, a taxonomy that systematically reflects human cognitive functions as outlined in Bloom [4].Each rule embodies complex cognitive processes based on established principles from learning and psychology.</p>
<p>(1) Basic Recall and Reproduction: This reflects the fundamental cognitive process of remembering and reproducing factual information without analysis or interpretation, which involves mere recognition or retrieval of knowledge from memory [2].(2) Understanding and Interpretation: This corresponds to the second cognitive rule of [4], where individuals must not only recall information but also explain it in their own words, summarize key points or clarify the meaning of content.This rule demands an intermediate cognitive load involving information processing rather than retrieving it.(3) Analysis and Reasoning: This aligns with the analysis stage of [4], which involves higher cognitive functions such as comparison, contrast, and deep understanding of the underlying principles.It is more complex than mere understanding because it requires examining structure and identifying patterns and connections.(4) Application of Knowledge and Execution: This mirrors the application and evaluation stages of [4], where individuals must not only understand and analyze but also use knowledge to perform multi-step tasks, solve complex problems, and execute decisions.It represents the most cognitively complex tasks, which require synthesis of information and practical decision-making, highlighting the critical leap from understanding theory to executing it in practice.</p>
<p>Prompt Selection</p>
<p>In HPT, the progression from basic recall to application of knowledge reflects increasing cognitive complexity, consistent with educational and cognitive frameworks, where more advanced cognitive processes build on foundational ones, demanding deeper engagement and mental effort.</p>
<p>Hierarchical Prompting Framework</p>
<p>The HPF consists of five prompting strategies, each assigned a complexity level.These levels are determined by the degree to which the strategies are shaped by the four principles of the HPT.The complexity levels of the prompting strategies are assigned based on human assessment of their relative cognitive loads over a set of 7 different tasks, guaranteeing that the cognitive abilities of LLMs are in harmony with those of humans.This approach enables the assessment of tasks in terms of their complexity and the cognitive load they impose on both humans and LLMs by utilizing HPI.Section 4.5 examines the hierarchical structure of the HPF in conjunction with the LLM-as-a-Judge framework, validating that the cognitive demands on LLMs can be aligned with those of humans.</p>
<p>The five prompting strategies were selected to ensure comprehensive coverage of cognitive demands rather than maximizing the number of strategies (see Appendix A).This makes HPF adaptable, allowing for replication or expansion with similar strategies.The strategies, ordered by increasing complexity, are:</p>
<p>(1) Role Prompting [20]: Specifies the LLM's role in task resolution, exerting minimal influence from HPT principles.</p>
<p>(2) Zero-Shot Chain-of-Thought Prompting (Zero-CoT) [19]:</p>
<p>Uses "Let's think step by step" to encourage reasoning, moderately influenced by rule 3. (3) Three-Shot Chain-of-Thought Prompting (3-CoT) [39]:</p>
<p>Provides three examples to guide reasoning, strongly influenced by rules 1 and 2, with moderate influence from rule 3. (4) Least-to-Most Prompting [42]: Breaks tasks into sub-problems, requiring recall, interpretation, and analysis, exerting strong influence from rules 1, 2, &amp; 3. (5) Generated Knowledge Prompting (GKP) [22]: Integrates external knowledge, demanding correlation, application, and analysis, making it the most cognitively complex (rules 2, 3, and 4).Llama-3 8B generates the external knowledge in experiments.</p>
<p>Hierarchical Prompting Index</p>
<p>HPI is an evaluation metric for assessing the task complexity of LLMs over different datasets, which is influenced by the HPT rules.</p>
<p>A lower HPI for a dataset suggests that the corresponding LLM is more adept at solving the task with fewer cognitive processes.</p>
<p>For each dataset instance, we begin with the least complex prompting strategy and progressively move through the HPF prompting strategies until the instance is resolved.The HPI corresponds to the complexity level of the prompting strategy where the LLM first tackles the instance.Algorithm 1 illustrates the process for determining HPI, with  indicating the total levels within the HPF and  representing the number of samples in the evaluation dataset.HPI  denotes the penalty that human evaluations impose on the framework.Additional information regarding human annotation is provided in Appendix A. Large Language Models: We tested LLMs ranging from 7B to 12B parameters across open-source and proprietary models.Proprietary LLMs: GPT-4o [25], Claude 3.5 Sonnet [3].SLMs: Gemma 7B [32], Mistral 7B [17], Llama-3 8B [1], Gemma-2 9B [33], Mistral-Nemo 12B [24].</p>
<p>Additional Evaluation Metrics</p>
<p>Coding: Pass@k [9] estimates the probability of at least one correct solution among the top k outputs for code generation.Summarization: ROUGE-L [21] measures sequence-level similarity via the longest common subsequence.Machine Translation: BLEU [26] evaluates n-gram precision against reference texts.Summarization and translation tasks used thresholds of 0.15 and 0.20, respectively, to define task completion at each HPF complexity level, enabling iterative refinement of prompting strategies.</p>
<p>Results on Standard Benchmarks: MMLU, GSM8K, and Humaneval</p>
<p>The evaluation of HPF effectiveness as shown in Figure 4 spans three standard benchmarks: MMLU, GSM8k, and HumanEval.On the MMLU benchmark, which tests general knowledge across multiple domains, all models showed notable improvements over their baseline performance.Mistral-Nemo 12B demonstrated the most substantial MMLU enhancement (+21.8%), while Claude 3.5 Sonnet achieved a consistent improvement of 3.5%.In mathematical reasoning, assessed through GSM8k, the results revealed a correlation with the model scale.Larger models like GPT-4 and Claude 3.5 Sonnet showed modest gains (+4.4% and +1.3% respectively), while smaller models exhibited more variable performance.The HumanEval benchmark, which assesses code generation capabilities, revealed the most dramatic improvements across all models.Mistral 7B achieved an exception 62.5% improvement in HumanEval scores, followed by Mistral-Nemo 12B with an impressive 51.4% improvement, and Gemma-2 9B with a 50.8% enhancement.The results suggest that HPF enhances performance on all benchmarks for the majority of SLMs and achieves similar performance to LLMs such as GPT-4o and Claude 3.5 Sonnet, thereby addressing RQ1, its impact is particularly pronounced in programming tasks, suggesting that the technique may be especially valuable for enhancing code-related capabilities.Table 1 highlights the improved performance of various LLMs on MMLU, with all models showing an HPI index below three.This indicates that reasoning over most MMLU samples requires minimal cognitive effort for these models, compared to baseline multi-shot CoT methods (5 shot), which typically require more than five examples and are more cognitively demanding according to HPT.Interestingly, while Claude 3.5 Sonnet achieves the highest MMLU accuracy, GPT-4o records the best HPI score, showing that minimal cognitive effort does not necessarily equate to the best performance addressing RQ2.The enhancement in GSM8k is relatively smaller compared to MMLU, with decreased performances for both Mistral  2, Claude 3.5 Sonnet achieves a perfect pass@1 of 1.00 with low HPI values, outperforming GPT-4o, which scores 0.95 but has a higher HPI.Gemma 7B struggles with the lowest pass@1 of 0.79 and the highest HPI of 3.71, indicating a need for a more complex prompting strategy.Notably, HPF noticeably boosted the performance of the majority of LLMs on three benchmark datasets, despite the HPI difference being less than 1 compared to the top-performing LLMs.This suggests that even with a minimal number of inferences, utilizing HPF can achieve optimal performance, unlike multi-shot prompting and prompt optimization strategies, thereby addressing RQ3.This highlights that tailoring the prompting strategy to align with the complexity of each dataset instance can lead to substantial improvements, achieving performance levels comparable to state-of-the-art LLMs such as GPT-4o and Claude 3.5 Sonnet on these benchmarks.</p>
<p>Results on Other Datasets</p>
<p>Table 1 presents LLM performance on the BoolQ and CSQA datasets.While no significant insights emerge, an unexpected result is GPT-4o's poor performance, which deviates from its typical trend.With most LLMs achieving near-perfect scores, BoolQ appears insufficiently complex to serve as an effective benchmark for modern LLMs, as they excel even with minimal cognitive prompting.This highlights HPF's value in assessing dataset complexity relative to LLM capabilities, providing researchers with insights for designing more challenging and robust benchmarks.Table 3 presents the performance of LLMs on IWSLT and Sam-Sum datasets at varying thresholds.GPT-4o consistently achieved the highest scores across all thresholds, while most models, except Gemma 7B, performed similarly.Interestingly, Claude 3.5 Sonnet, which excelled in reasoning tasks, did not perform as strongly in summarization and translation tasks.The threshold selection is guided by the observed performance plateau across most LLMs as the threshold increases.</p>
<p>Threshold Selection for SamSum and IWSLT</p>
<p>In addition to the 0.15 and 0.20 thresholds presented in the main experiments, extended evaluations were conducted on the IWSLT and SamSum datasets using thresholds of 0.25 and 0.30 with GPT-4o, Mistral-Nemo 12B, and Llama-3 to assess the impact of varying thresholds on LLM performance.</p>
<p>SamSum Dataset: In the summarization task, increasing the threshold evaluates an LLM's ability to condense content while retaining key information.Higher thresholds like 0.25 and 0.30 reveal the trade-offs between conciseness and informativeness.However, as shown in Figure 5, there was no significant improvement in ROUGE-L, except for a slight increase with GPT-4o.The experiments showed a sharp rise in HPI, reflecting the increased task complexity.These results suggest that LLM performance has plateaued, with no further gains at higher thresholds.This validates that the use of 0.15 and 0.20 thresholds are sufficient for optimal LLM performance.</p>
<p>IWSLT Dataset: In machine translation, higher thresholds (0.25 and 0.30) impose stricter evaluations, assessing how well models capture the nuances of the source text.Lower thresholds (0.15 and 0.20) focus on general adequacy, while higher ones test performance under more challenging conditions.As shown in Figure 6, no BLEU improvements were observed across any LLMs, with models either reaching saturation or showing decreased performance alongside a rapid rise in HPI.This validates the selection of 0.15 and 0.20 thresholds are sufficient for optimal LLM performance.</p>
<p>Complexity Levels with LLM-as-a-Judge</p>
<p>This study evaluated prompting strategies by assessing how GPT-4o, as the LLM judge, replicates the hierarchical complexity levels of these strategies using a systematic scoring approach across tasks.Figure 7 shows a consistent hierarchy with less variability than human judges, indicating a strong alignment between LLM and human judgment.These results validate the proposed framework and demonstrate the correspondence between human cognitive principles and LLM behavior.Figure 8 shows the scoring distribution across the four HPT rules for each strategy.Further details related to evaluation dataset specifications and scoring method are in Appendix B.  thinking, involving logical arguments or intricate problems needing deliberate processing.Elevated HPF levels are used for tasks demanding deep analysis.HPF explicitly measures this transition with HPI, assessing the cognitive load required for each task.By tailoring prompting strategies to task complexity, HPF optimizes LLM performance, much like humans adaptively switch between System 1 and System 2 based on the situation.This parallel highlights how HPT bridges computational strategies with human-like cognitive models, enabling more nuanced task evaluation and resource allocation.</p>
<p>R o l e P r o m p t i n g Z e r o -C o T 3 -C o T L e a s t t o M o s t G K P</p>
<p>Adaptive HPF</p>
<p>The Adaptive HPF automates the selection of the optimal complexity level in the HPF using a prompt-selector, Llama-3 8B in a zero-shot setting, bypassing iterative steps.Figure 9 shows that Adaptive HPF yields higher HPI but lower evaluation scores than the standard HPF.This suggests that Adaptive HPF struggles to select the optimal complexity level, likely due to hallucinations by the prompt-selector when choosing the prompting strategy.For more results and ablation studies, see Appendix C.</p>
<p>The prompt-selector can dynamically select the most suitable prompting strategy for a given task's complexity from the HPF's hierarchy of complexity levels.To determine the most effective prompting strategy to complete the task, the prompt-selector was given a maximum number of iterations equivalent to the number of levels in the manual HPF.The score for ith iteration is i + x, where x is the complexity level by the prompt-selector.If the LLM fails to complete the task after all iterations, it is assigned a penalty, HPI  .Algorithm 2 demonstrates the calculation of HPI for an adaptive HPF, where  denotes the HPF level chosen by the prompt-selector at the th iteration as the task is being tackled.Here,  indicates the total number of HPF levels, and  signifies the total quantity of samples in the evaluation set.</p>
<p>Conclusion</p>
<p>The HPT offers an efficient way to evaluate LLMs by focusing on task cognitive demands.It shows that cognitively inspired selection of prompting strategies enhances LLM performance across various datasets.This method offers insights into LLM problem-solving and improves evaluation methods based on human cognition, supporting better in-context learning strategies for assessing LLMs.</p>
<p>Limitations</p>
<p>Human Annotation Constraints: A limitation of this study is the reliance on human evaluation for inducing the HPI  penalty into the HPF.While this study assessed 5% of the datasets, expanding coverage would offer a more comprehensive analysis.However, due to constraints in human resources for manual annotation, we could not include a larger portion.Future work could address this by increasing manpower or automating parts of the evaluation process.</p>
<p>HPF Optimization:</p>
<p>The effectiveness of the HPF heavily relies on the quality of the prompts used at each level of the taxonomy.Crafting high-quality prompts that accurately reflect the subtleties of each level demands considerable expertise and repeated refinement.This study only investigated a limited set of prompting strategies within the HPF, indicating a need for further research into creating diverse structural frameworks and incorporating additional prompting strategies.</p>
<p>Zero-shot Prompt Selection: HPF dynamically determines the optimal cognitive complexity level by iterating through the framework's levels, which leads to increased inference time.While this study investigated Adaptive HPF for zero-shot prompt selection, it faced considerable hallucinations.Future research should focus on automating HPF using fine-tuning or reinforcement learning-based approaches to select the appropriate complexity level without manual iteration.This strategy would optimize inference time and improve overall performance.</p>
<p>Ethical Statement</p>
<p>The HPI  assigned by experts to MMLU, GSM8k, HumanEval, BoolQ, CSQA, IWSLT, and SamSum may introduce bias due to the subjective nature of expert scoring, influenced by individual experience and perspective.However, these publicly available, widely recognized datasets help mitigate unforeseen ethical concerns.Acknowledging potential scoring bias remains essential for transparency and integrity in the analysis.Higher scores for the four rules signify a stronger influence of the respective rules, indicating that completing the task requires greater cognitive effort.The HPI  for each dataset, as shown in Table 4, was calculated by taking the mean of the values from these four criteria, acknowledging the challenge of estimating or computing the individual weights of the influence of each rule.</p>
<p>The Representative Set Size in Table 4 refers to the subset of the dataset evaluated by human annotators, ensuring that the assessment reflects the overall task.Human annotation, while timeconsuming and costly, provides a gold standard for calibrating the evaluation process of this paper.Selecting 5% of the dataset as the representative set size balances quality assessment and feasibility, capturing the dataset's diversity and ensuring that human annotations encompass a broad range of cases without needing to annotate every sample.</p>
<p>A.2 Human Judgement Policy</p>
<p>To populate the HPF with relevant prompting strategies across a wide range of strategies, human annotators who adhered to the annotation policy for assessing HPI  were instructed to follow a judgment policy for a predefined set of prompting strategies.They were instructed to evaluate the influence of the four rules of the HPT on solving the annotated tasks using each prompting strategy, rating their influence as High (H), Moderate (M), or Low (L).It's important to note that a high rating on rule 4 has a greater influence than a high rating on rule 3, and similarly for the other two rules.Considering the rating as shown in Table 5 and varying influences of these rules, five prompting strategies that prioritize comprehensive coverage of cognitive demands while ensuring the set optimally widens the variation across complexity levels were selected for populating the HPF.</p>
<p>Prompting Strategy</p>
<p>Rule 1 Rule 2 Rule 3 Rule 4
Role Prompting L L L L Emotion Prompt- ing L L M L Zero-shot CoT L L M L Meta Prompting M H M L Three-shot CoT H H M L Five-shot CoT H H H L Chain-of- Verification H H H H Least-to-Most Prompting H H H L Self-Consistency H H H M GKP L H H H
Table 5: Human judgment of influence of the rules of taxonomy on different prompting strategies in solving the tasks of the representative set.The ratings are provided based on a voting system involving all human annotators.Green represents the prompting strategies selected for populating the complexity levels of the HPF.</p>
<p>B LLM-as-a-Judge B.1 Scoring Prompt Template</p>
<p>The system prompt is designed to guide the LLM judge in evaluating different prompting strategies based on four specific criteria: Basic Recall and Reproduction, Understanding and Interpretation, Analysis and Reasoning, and Application of Knowledge and Execution.Each criterion is scored on a scale of 1-5.The evaluation uses GPT-4o as a judge, with the following system prompt:</p>
<p>You are a judge evaluating different prompting strategies and you need to score these prompting strategies based on pre-defined criteria.Different prompting strategies leverage varied amounts of intelligence from the model to achieve the required answer.So, assign the scores very carefully based on your analysis of the prompt and its effect on your intelligence to achieve the given answer as well as the number of multi-step prompts which increases the complexity of execution.The need for the model to break down complex information, understand relationships, and solve problems using logical reasoning to answer the prompt Range: 1-5 score4: Application of Knowledge and Execution Definition: The need for the model to apply knowledge in practical situations, execute multi-step processes, and solve complex tasks to answer the prompt Range: 1-5</p>
<p>B.2 Hybrid Dataset</p>
<p>The hybrid dataset is composed of 1106 samples uniformly distributed over seven different task-specific datasets, covering a wide range of language understanding and generation tasks.This diversity allows for a comprehensive evaluation of the prompting strategies across various problem types.The evaluation uses a hybrid dataset composed of samples from various task-specific datasets and each dataset contributes specific types of tasks:</p>
<p>B.3 Scoring Method</p>
<p>For each prompting strategy (Role Prompting, Zero-shot CoT, Threeshot CoT, Least to Most Prompting, Generated Knowledge Prompting), the system:</p>
<p>(1) Applies the prompting strategy to each sample in the hybrid dataset ( 2) Generates an answer using GPT-4o (3) Presents the prompt, generated answer, and correct answer to the LLM judge (4) Collects scores for each of the four criteria and the system calculates average scores for each criterion across all tasks and datasets.This study ensured that both the human judge and the LLM judge utilized the same scoring methodology to eliminate any potential bias in the comparison.</p>
<p>C Hallucination in Adaptive HPF</p>
<p>Hallucinations in prompt-selector refer to instances where the LLM generates incorrect or misleading prompting levels or nonsensical information that is not supported by the HPF.These hallucinations can occur across various tasks, including question answering, multiplechoice questions, translation, and summarization.</p>
<p>For the BoolQ task, the prompt-selector initially struggles, indicated by the iterations where it reaches Level 4 with hallucinations.However, by the fourth iteration, Llama-3 8B manages to answer correctly at Level 2. For the CSQA task, prompt-selector exhibits hallucinations initially, shown by Level 4 and Level 0 (not included in HPF) responses.Eventually, it corrects itself by the third iteration, providing the correct answer at Level 2. For the IWSLT task, promptselector demonstrates a consistent pattern of hallucinations across multiple iterations.Even though Llama-3 8B attempts the translation at Level 2 multiple times, it ultimately fails to provide a correct translation, indicating a persistent hallucination.For the SamSum task, prompt-selector shows initial hallucinations in the first three iterations (Level 4).However, by the fourth and fifth iterations, the prompt-selector starts producing lower levels.Finally, Llama-3 8B achieves the correct answer at Level 2 in the last iteration .</p>
<p>The results in Table 6 and Table 7 indicate that the prompt-selector exhibits hallucinations in selecting complexity levels across various tasks and iterations resulting in higher HPI for Adaptive HPF, with performance varying significantly.While the LLM can eventually produce correct answers, as seen in the BoolQ and SamSum tasks, it often requires multiple attempts and may still fail in tasks like IWSLT translation.</p>
<p>C.1 Prompt Template for Prompt-Selector</p>
<p>The prompt-selector in adaptive HPF selects the prompting level based on the task complexity to address the task.Llama-3 8B serves as the prompt-selector in the experiments.The prompt template was meticulously designed to ensure maximum clarity, aiming to reduce hallucinations and select the most effective prompting strategy.</p>
<p>Prompt Template: Choose the most effective prompting strategy among five available strategies for the task.Begin with the lowest indexed strategy and progress to higher indexed strategies if the earlier ones are not effective.For a given task, the prompting strategies are: • Least-to-most prompting: Uses a sequential method to derive essential insights from the task to solve it.• Generated Knowledge Prompting: Integration and application of external knowledge to accomplish the task.The external knowledge is generated using some other model based on the task.
•
Select only the index (do not provide the name) of the most effective prompting strategy.</p>
<p>D Computational Budget</p>
<p>All evaluation experiments and ablation studies were conducted on V100 GPUs (16GB and 32GB variants), utilizing a total of around 9,000 computation hours, this equates to approximately 1.125 petaflop-hours of computational resources.</p>
<p>E Large Language Models Used for Evaluation</p>
<p>The HPF supports leading open source and proprietary LLMs and includes mechanisms for optimizing performance through advanced quantization techniques.The experiments were conducted on the following instruction-tuned LLMs, and the model description and licenses are discussed in Table 8.</p>
<p>The LLMs were loaded in 4-bit precision format, with a maximum generation limit of 1024 tokens per run to ensure concise outputs.The temperature was set to 0.6 to control prediction randomness, while top-p sampling (p=0.9)enabled the exploration of diverse continuations.Additionally, a repetition penalty was applied to discourage the generation of repeated phrases, promoting coherent and varied text output.</p>
<p>F Prompt Templates F.1 Level 1: Role Prompting</p>
<p>Role prompting represents the most basic interaction with an LLM, assigning it a specific role or task without additional context or examples.This approach relies solely on the initial instruction to guide responses.For instance, asking the LLM to "act as a translator" prompts it to translate text based on its training data.While straightforward, this method may lack depth, resulting in less accurate or nuanced outputs.Table 9 shows the prompt templates used for role prompting across all datasets in the experiments.</p>
<p>IWSLT</p>
<p>Translate "english text" to french as a Translator.</p>
<p>SamSum</p>
<p>Summarize the Dialogue: "dialogue" as a Summarizer.</p>
<p>GSM8k</p>
<p>Based on the question: "question", calculate the numerical answer to the question as an expert mathematician.</p>
<p>HumanEval</p>
<p>Complete the given code based on the mentioned constraints: "code" as an expert programmer.</p>
<p>MMLU</p>
<p>Choose the answer: "question",A."option 1",B."option 2",C."option 3",D."option 4" as a critical thinker.</p>
<p>F.2 Level 2: Zero-shot Chain-of-Thought Prompting</p>
<p>Zero-shot Chain-of-Thought (CoT) prompting enhances basic role prompting by requiring the LLM to generate a reasoning process for a task, despite not being explicitly trained on similar examples.This method encourages the LLM to break down the problem and solve it step-by-step using its internal knowledge, improving response quality through logical progression and coherence.Table 10 displays the prompt templates used for Zero-CoT across all datasets in the experiments.</p>
<p>F.3 Level 3: Three-Shot Chain-of-Thought Prompting</p>
<p>Three-shot Chain-of-Thought (CoT) prompting builds on the zeroshot approach by providing the LLM with three task examples, including the reasoning steps used to reach the solution.These examples help the LLM grasp the required structure and logic, enabling it to better replicate the problem-solving process and produce more accurate, contextually relevant responses.Table 11 shows the prompt templates used for 3-CoT across all datasets in the experiments.</p>
<p>F.4 Level 4: Least-to-Most Prompting</p>
<p>Least-to-most prompting is an advanced technique that gradually increases prompt complexity, starting with simpler tasks and progressing to more complex challenges.This method allows the LLM to build confidence and leverage insights from easier prompts to tackle harder ones, enhancing its ability to generalize from straightforward examples to intricate scenarios.Table 12 displays the prompt templates used for Least-to-Most Prompting across all datasets in the experiments.</p>
<p>F.5 Level 5: Generated Knowledge Prompting</p>
<p>Generated Knowledge prompting is one of the most complex techniques in HPF, where the LLM not only addresses the task but also integrates relevant additional information to enhance its response.This method prompts another LLM to produce auxiliary knowledge, creating a richer context for understanding and solving the problem.By leveraging self-generated insights, the LLM can deliver more detailed, accurate, and nuanced answers.Table 13 shows the prompt templates used for Generated Knowledge Prompting across all datasets in the experiments.</p>
<p>Dataset Prompt BoolQ</p>
<p>Based on the passage: "passage1", answer True/False to the question: "question1".Answer: "answer1".Explanation: "explaination1".Based on the passage: "passage2", Answer True/False to the question: "question2".Answer: "answer2".Explanation: "explaination2".Based on the passage: "passage3", Answer True/False to the question: "question3".Answer: "answer3".Explanation: "explaination3".Based on the passage: "passage", Answer True/False to the question: "question".</p>
<p>CSQA Choose the answer: "question1",A."option1-1",B."option2-1",C."option3-1",D."option4-1",E."option5-1", Answer: "answer1", Explanation: "explaination1".Choose the answer: "question2", A. "option1-2",B."option2-2",C."option3-2",D."option4-2",E."option5-2", Answer: "answer2", Explanation: "explainatio n2".Choose the answer: "question3", A. "option1-3",B."option2-3",C."option3-3",D."option4-3",E."option5-3", Answer: "answer3", Explanation: "explaination3".Choose the answer: "question", "question",A."option 1",B."option 2",C."option 3",D."option 4",E."option 5".</p>
<p>IWSLT</p>
<p>Translate "english text1" to French.French: "french text1".</p>
<p>Translate "english text2" to French.French: "french text2".</p>
<p>Translate "english text3" to French.French: "french text3".</p>
<p>Translate "english text" to French.</p>
<p>SamSum</p>
<p>Summarize the Dialogue: "dialogue1".Summary: "summary1".Summarize the Dialogue: "dialogue2".Summary: "summary2".Summarize the Dialogue: "dialogue3".Summary: "summary3".Summarize the Dialogue: "dialogue".</p>
<p>GSM8k</p>
<p>Based on the question:"gsm8k_question1", calculate the numerical answer to the question.Answer: "gsm8k_ans1".Based on the question:"gsm8k_question2", calculate the numerical answer to the question.Answer: "gsm8k_ans2".Based on the question:"gsm8k_question3", calculate the numerical answer to the question.Answer: "gsm8k_ans3".Based on the question: "question", calculate the numerical answer to the question.</p>
<p>HumanEval</p>
<p>Complete the given code based on the mentioned constraints: "humaneval_code1", Code: "humaneval_sol1".Complete the given code based on the mentioned constraints: "humaneval_code2", Code: "humaneval_sol1".Complete the given "code" based on the mentioned constraints: "humaneval_code3", Code: "humaneval_sol3".</p>
<p>MMLU Choose the answer for the question: "mmlu_ques1" A.</p>
<p>Dataset Prompt</p>
<p>BoolQ prompt 1: Summarize the main points of this passage: "passage".prompt 2: Analyze this question to identify its key components: "question".prompt 3: Find the part of the passage that relates to this question: "question", Passage: "passage".IWSLT prompt 1: What is the main idea or theme of this text?"english text".prompt 2: Identify and list the key phrases or terms in this text: "english text".prompt 3: Translate the following key phrases into French: "previous response".prompt 4: Translate "english text" into French, incorporating the translations of the key phrases: "previous response".</p>
<p>SamSum prompt 1: List the main points or key ideas present in this dialogue: "dialogue".prompt 2: Elaborate on the following key points, providing additional details or context: "previous response".prompt 3: Using the listed key points and their elaborations, draft a concise summary of this text: "dialogue".prompt 4: Refine this draft summary to make it more concise and coherent, ensuring it captures the essence of the text: "dialogue".</p>
<p>GSM8k</p>
<p>Analyze the question: "question".Break the question into sub-problems: "question".Calculate answers for the subproblems of the question: "pred".Calculate the numerical answer to this question: "question" based on the previous calculations: "pred"</p>
<p>HumanEval</p>
<p>Analyze the code: "code".Break the question into sub-problems: "code".Complete code for the subproblems of the code: "pred".Complete the code based on the mentioned constraints: "code" based on the previous calculations: "pred" Table 13: Prompt templates of different datasets for Generated Knowledge Prompting.</p>
<p>MMLU</p>
<p>Dataset Prompt</p>
<p>BoolQ inference prompt: Based on the passage:"passage", answer True/False to the question: 'question' using knowledge of the passage:"knowledge" knowledge generation prompt: Generate Knowledge about the passage: "passage".</p>
<p>CSQA inference prompt: Choose the answer:"question", A. "option 1",B."option 2",C."option 3",D."option 4",E."option 5" using knowledge of the question:"knowledge" knowledge generation prompt: Generate Knowledge about the question: "question".</p>
<p>IWSLT inference prompt: Translate "english text": to French using definitions of the keywords:"knowledge" knowledge generation prompt: Generate definitions in french of each word in the text: "english text".</p>
<p>SamSum inference prompt: Summarize the Dialogue: "dialogue" using the interpretation of the dialogue:"knowledge" knowledge generation prompt: Generate interpretation about the dialogue: "dialogue".</p>
<p>GSM8k</p>
<p>Based on the question:"question", calculate the numerical answer to the question using an interpretation of the question:"pred"</p>
<p>HumanEval</p>
<p>Complete the code based on the mentioned constraints:"code" using knowledge of the constraints:"pred"</p>
<p>MMLU Choose the answer."question", options: A. "option 1" B. "option 2" C. "option 3" D. "option 4" using knowledge of the question:"pred"</p>
<p>Figure 1 :
1
Figure 1: The Hierarchical Prompting Framework includes five distinct prompting strategies, each designed for different levels of task complexity to ensure the appropriate prompt is selected for the given task.A ✓ indicates task completion, while a × signifies task incompletion.</p>
<p>Figure 2 :
2
Figure 2: Analogical framework comparing the HPF with "open book" examination methodology.The diagram illustrates howHPF components (below) mirror traditional educational assessment elements (above), with parallel relationships between task complexity levels, resource utilization (prompts/textbooks), and performance metrics (HPI/student effort).This comparison demonstrates how LLM task complexity scales similarly to educational assessment complexity, from simple lookup tasks to complex synthesis problems.</p>
<p>Figure 3 :
3
Figure 3: Hierarchical Prompting Taxonomy: A taxonomy designed to assess the complexity of prompting strategies based on the criteria: Basic Recall and Reproduction, Understanding and Interpretation, Analysis and Reasoning, and Application of Knowledge and Reasoning.</p>
<p>Algorithm 1
1
HPI Computation HPI_List = [ ] for sample  in the evaluation dataset do for level  in the HPF do if LLM resolves the task then HPI_List[] =  break end if end for if LLM failed to resolve the task then HPI_List[] =  + HPI  end Datasets We evaluated the framework on diverse datasets spanning reasoning, coding, mathematics, question-answering, summarization, and machine translation.For dataset sizes, see Appendix A. Reasoning: MMLU [15] (57 subjects, multiple-choice), CSQA [31] ( 12K commonsense questions).Coding: HumanEval [8] (164 function-based coding tasks).Mathematics: GSM8k [11] (8.5K multi-step math problems).Question-Answering: BoolQ [10] ( 16K True/False questions from Wikipedia).Summarization: SamSum [14] ( 16K human-annotated dialogue summaries).Machine Translation: IWSLT-2017 en-fr [7] (TED Talk parallel corpus).</p>
<p>Figure 4 :
4
Figure 4: Performance Comparison of HPT-based Evaluation vs. Standard Evaluation: Performance improvements (in %) when using HPT-based evaluation compared to standard evaluation across three benchmarks: MMLU, GSM8k, and HumanEval.Positive values indicate performance gains with HPT, while negative values indicate performance decreases.The baseline standard evaluation scores are sourced from Hugging Face leaderboard and official research reports.</p>
<p>Figure 5 :Figure 6 :
56
Figure 5: Comparison of HPI and ROUGE-L scores across different threshold values on SamSum dataset.</p>
<p>Figure 8 :
8
Figure 8: Scoring distribution for each of the four rules of the HPT for the prompting strategies in the HPF.</p>
<p>Figure 9 :
9
Figure 9: HPI of datasets for LLMs in Adaptive HPF.</p>
<p>( 1 ) 2 ) 3 ) 4 )
1234
Basic Understanding and Reproduction: This criterion evaluates the annotator's ability to comprehend and accurately reproduce the content.(Understanding and Interpretation: This criterion assesses the annotator's depth of understanding and the ability to interpret the information correctly.(Analysis and Reasoning: This criterion measures the annotator's ability to analyze the information and apply logical reasoning.(Application of Knowledge and Execution: This criterion evaluates the annotator's practical application of knowledge and the execution of tasks based on the relevant knowledge.</p>
<p>score1: Basic Recall and Reproduction Definition: The need of the model to remember and reproduce factual information without interpretatio or analysis to answer the prompt Range: 1-5 score2: Understanding and Interpretation Definition: The need of the model to comprehend and explain the meaning of information, summarizing or clarifying content to answer the prompt Range: 1-5 score3: Analysis and Reasoning Definition:</p>
<p>( 1 )
1
MMLU (Massive Multitask Language Understanding) (2) HumanEval (Code Generation and Completion) (3) GSM8K (Grade School Math 8K) (4) BoolQ (Boolean Questions) (5) CSQA (Commonsense Question Answering) (6) IWSLT (International Workshop on Spoken Language Translation) (7) SamSum (Dialogue Summarization)</p>
<p>Role Prompting: Defines a role for the model in solving the task.• Zero-shot Chain of Thought prompting: Stimulates reasoning and problem-solving by including the phrase 'Let's think step by step' without offering previous examples related to the task.• Three-shot Chain of Thought prompting: Offers three examples related to the task to guide the model's reasoning process.</p>
<p>[AND, NOT] B. [NOT, OR] C. [AND, OR] D. [NAND] Answer: C. Explanation: "mmlu_exp1".Choose the answer for the question "mmlu_ques2" A. The defendant's statement was involuntary.B. The defendant's statement was voluntary.C. The defendant was not in custody when the statement was made.D. The statement was not made in response to a known police interrogation.Answer: A, Explanation: "mmlu_exp2".Choose the answer for the question: "mmlu_ques3" .A. Wrong, Wrong.B. Wrong, Not wrong C.Not wrong, Wrong D.Not wrong, Not wrong.Answer: B Explanation: "mmlu_exp3".Choose the answer."question""question",A."option 1",B."option 2",C."option 3",D."option 4".</p>
<p>prompt 4 :CSQA prompt 1 : 2 : 3 : 4 :
41234
Based on the passage, what is the answer to this question: "question", Relevant Information: "previous response".Analyze this question: "question".prompt Elaborate about each option for the question: "question", options: A. "option 1",B."option 2",C."option 3",D."option 4",E."option 5".prompt Based on the analysis: "previous response", discard wrong answers among the options: A. "option 1",B."option 2",C."option 3",D."option 4",E."option 5".prompt Choose the correct answer from the options: A. "option 1",B."option 2",C."option 3",D."option 4",E."option 5".</p>
<p>Analyze the question: "question".Elaborate about each option for the question: "question", options: A. "option 1" B. "option 2" C. "option 3" D. "option 4".Based on the analysis: "question", Discard wrong answers among the options: A. "option 1" B. "option 2" C. "option 3" D. "option 4".</p>
<p>Table 1 :
1
HPI (lower is better) and accuracy of LLMs across MMLU, GSM8K, BoolQ, and CSQA datasets.Blue indicates datasets where the LLM with the best HPI does not achieve the best performance.Green indicates the LLM with the best performance over the maximum number of datasets.
DATASETSMMLUGSM8kBoolQCSQAModelsHPI Accuracy HPI Accuracy HPI Accuracy HPI AccuracyGPT-4o1.8191.611.7196.431.3296.821.6592.54Claude 3.5 Sonnet1.8492.161.3597.721.2099.812.0186.15Mistral-Nemo 12B 2.4589.753.0186.801.7599.872.0690.17Gemma-2 9B2.3487.282.1791.281.3098.281.9488.86Llama-3 8B2.8482.632.3486.201.3799.302.4384.76Gemma 7B2.9383.316.7027.881.4599.422.5083.78Mistral 7B2.8981.455.1146.931.4198.072.4982.067B and Gemma 7B. The high HPI values for Gemma 7B and Mistral7B indicate that none of the five prompting strategies in HPF posedsignificant cognitive challenges for these LLMs, i.e more cognitivelydemanding prompting strategies are needed, highlighting a limitationof the HPF. As shown in Table</p>
<p>Table 2 :
2
HPI (lower is better) and Pass@1 of LLMs on the HumanEval dataset.Blue indicates datasets where the LLM with the best HPI does not achieve the best performance.Green indicates the LLMs with the best performance over the dataset.
DATASETHumanEvalModelsHPI Pass@1GPT-4o2.250.95Claude 3.5 Sonnet1.041.00Mistral-Nemo 12B 2.070.96Gemma-2 9B1.010.91Llama-3 8B1.031.00Gemma 7B3.710.79Mistral 7B1.100.93</p>
<p>Table 3 :
3
HPI (lower is better), BLEU score for IWSLT, and ROUGE-L score for SamSum, of LLMs with thresholds.
DATASETSIWSLTSamSumHPIBLEUHPIROUGE-LModels0.150.200.150.200.150.200.150.20GPT-4o2.663.080.320.321.111.210.300.29Claude 3.5 Sonnet4.634.870.200.201.251.600.230.23Mistral-Nemo 12B2.873.400.270.271.191.470.230.24Gemma-2 9B4.404.750.210.201.301.860.220.22Llama-3 8B3.403.920.240.231.301.720.220.22Gemma 7B5.395.840.080.093.315.030.110.10Mistral 7B3.524.140.220.221.261.680.210.220.160.180.200.220.240.260.280.30</p>
<p>Table 4 :
4
HPI  scores across datasets evaluated by human annotators.The table lists the evaluation set size, representative set size, and HPI  for various datasets.HPI  scores provide a measure of task complexity relative to human annotators.
DatasetEvaluationRepresentativeHPI 𝐷𝑎𝑡𝑎𝑠𝑒𝑡Set SizeSet SizeMMLU145007253.03GSM8k1320662.14Humaneval 16084.68BoolQ32701621.71CSQA1221602.52IWSLT890451.92SamSum819402.23</p>
<p>Table 6 :
6
HPI (lower is better) of LLMs across datasets (with thresholds) for Adaptive HPF.
ModelBoolQ CSQA IWSLT (0.15) IWSLT (0.20) SamSum (0.15) SamSum (0.20)Llama-3 8B 5.2173 5.91366.20066.28415.03165.5756Mistra 7B5.0483 5.90736.24786.46044.74235.1336Phi-3 3.8B5.1386 5.67936.39556.49365.09615.7778Gemma 7B 5.1514 5.57716.59476.66055.72296.4347</p>
<p>Table 7 :
7
Performance scores of LLMs across datasetsfor Adaptive HPF.
DatasetMetricThresholdLlama-3 8BPhi-3 3.8BMistral 7BGemma 7BBoolQAccuracy-0.885770.911150.917520.91166CSQAAccuracy-0.594510.680190.601110.68549IWSLTBLEU0.15 0.20.21140 0.211460.15557 0.153540.20000 0.205680.08447 0.07730SamSumROUGE-10.15 0.20.24407 0.249810.20586 0.215800.26910 0.283350.16023 0.16179</p>
<p>Table 8 :
8
License information for LLMs used in the experiments.
ModelLicense TypeUsage RestrictionsGPT-4oProprietaryCommercial use requires paid API access, subject to OpenAI'sterms of serviceClaude 3.5 SonnetProprietaryCommercial use requires paid API access, subject to Anthropic'sterms of serviceMistral-Nemo 12BProprietaryUsage likely restricted to authorized partners or specific usecasesGemma-2 9BResearch LicenseNon-commercial use only, research purposesLlama-3 8BResearch LicenseSpecific restrictions may apply, typically for non-commercialresearch useMistral 7BOpen-sourceBroad use allowed, must include original license and noticesGemma 7BOpen-source/ResearchDepending on the license, may have non-commercial restrictionsor broad use allowedPhi-3 3.8BOpen-sourceBroad use allowed, must include original license and notices</p>
<p>Table 9 :
9
Prompt templates of different datasets for Role Prompting.
DatasetPromptBoolQBased on the passage: "passage", answer True/False to thequestion: "question" as an Omniscient person.CSQAChoose the answer: "question",A. "option 1",B. "option 2",C."option 3",D. "option 4",E. "option 5" as a critical thinker.</p>
<p>Table 10 :
10
Prompt templates of different datasets for Zero-shot Chain-of-Thought Prompting.
DatasetPromptBoolQBased on the passage: "passage", answer True/False to thequestion: "question". Let's think step by step.CSQAChoose the answer: A. "option 1",B. "option 2",C. "option 3",D."option 4",E. "option 5". Let's think step by step.IWSLTTranslate "english text" to french. Let's think step by step.SamSumSummarize the Dialogue: "dialogue". Let's think step by step.GSM8kBased on the question: "question", calculate the numericalanswer to the question. Let's think step by step.HumanEvalComplete the given code based on the mentioned constraints:"code". Let's think step by step.MMLUChoose the answer: "question",A. "option 1",B. "option 2",C.
"option 3",D."option 4".Let's think step by step.</p>
<p>Table 11 :
11
Prompt templates of different datasets for Three-Shot Chain-of-Thought Prompting.</p>
<p>Table 12 :
12
Prompt templates of different datasets for Least-to-Most Prompting.</p>
<p>A I , Meta , Llama 3 Model Card. 2024</p>
<p>A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's. Pearson. L W Anderson, D Krathwohl, K Cruikshank, P Airasian, J Raths, P Pintrich, R Mayer, M Wittrock, 2014Boston, MA</p>
<p>Anthropic, Claude 3.5 Sonnet. 2024</p>
<p>B S Bloom, Taxonomy of Educational Objectives: The Classification of Educational Goals. Number v. 1 in Taxonomy of Educational Objectives: The Classification of Educational Goals. Longmans, Green, Earth. 1956</p>
<p>Thinking fast and slow in AI. Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jonathan Lenchner, Nick Linck, Andreas Loreggia, Keerthiram Murgesan, Nicholas Mattei, Francesca Rossi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 332020. 2020</p>
<p>Overview of the IWSLT 2017 Evaluation Campaign. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, Christian Federmann, Proceedings of the 14th International Conference on Spoken Language Translation. International Workshop on Spoken Language Translation. the 14th International Conference on Spoken Language Translation. International Workshop on Spoken Language TranslationTokyo, Japan2017</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet ; Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, arXiv:2107.03374Felipe Petroski Such. Josh Achiam, Vedant Misra, Evan MorikawaJan LeikeIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, arXiv:2107.03374[cs.LGIlya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr; Bob McGrew, Dario Amodei, Sam McCandlishJan Leike</p>
<p>BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/v1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168unknownTraining Verifiers to Solve Math Word Problems. 2021. 2021arXiv preprintTo appear</p>
<p>Automatic Prompt Selection for Large Language Models. Viet-Tung Do, Van-Khanh, Duy-Hung Hoang, Shahab Nguyen, Jeff Sabahi, Hajime Yang, Minh-Tien Hotta, Hung Nguyen, Le, arXiv:2404.02717[cs.CL2024</p>
<p>Specializing Smaller Language Models towards Multi-Step Reasoning. Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, Tushar Khot, Proceedings of the 40th International Conference on Machine Learning (Proceedings of Machine Learning Research. Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, Jonathan Scarlett, the 40th International Conference on Machine Learning ( Machine Learning Research2023202PMLR, earth</p>
<p>SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, Aleksander Wawer, 10.18653/v1/D19-5409Proceedings of the 2nd Workshop on New Frontiers in Summarization. Jackie Chi, Kit Cheung, Giuseppe Carenini, Fei Liu, the 2nd Workshop on New Frontiers in SummarizationLu Wang; Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Measuring Massive Multitask Language Understanding. -pages. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021To appear</p>
<p>Large language models are reasoning teachers. -pages. Namgyu Ho, Laura Schmid, Se-Young Yun, 2022To appear</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7B. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Thinking, fast and slow. Daniel Kahneman, 2011macmillan, Earth</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 352022. 2022</p>
<p>Aobo Kong, Shiwan Zhao, Hao Chen, Qicheng Li, Yong Qin, Ruiqi Sun, Xin Zhou, Enzhi Wang, Xiaohang Dong, arXiv:2308.07702Better Zero-Shot Reasoning with Role-Play Prompting. 2024</p>
<p>ROUGE: A Package for Automatic Evaluation of Summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Generated Knowledge Prompting for Commonsense Reasoning. Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Le Ronan, Yejin Bras, Hannaneh Choi, Hajishirzi, 10.18653/v1/2022.acl-long.225Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Nakov, Aline Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Smaranda Muresan</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, arXiv:2107.13586Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. 2021</p>
<p>A I Mistral, Nvidia , Mistral NeMo 12B. 2024</p>
<p>BLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL '02). the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL '02)USAAssociation for Computational Linguistics2002</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023</p>
<p>Reasoning with Language Model Prompting: A Survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, 10.18653/v1/2023.acl-long.294Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Flatnessaware prompt selection improves accuracy and sample efficiency. -pages. Lingfeng Shen, Weiting Tan, Boyuan Zheng, Daniel Khashabi, 2023To appear</p>
<p>Cognitive load during problem solving: Effects on learning. John Sweller, 10.1016/0364-0213(88)90023-7Cognitive Science. 121988. 1988</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. 2019</p>
<p>. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Giuseppe Pier, Aakanksha Sessa, Adam Chowdhery, Aditya Roberts, Alex Barua, Alex Botev, Ambrose Castro-Ros, Amélie Slone, Andrea Héliou, Anna Tacchetti, Antonia Bulanova, Beth Paterson, Bobak Tsai, Charline Le Shahriari, Christopher A Lan, Clément Choquette-Choo, Daniel Crepy, Daphne Cer, David Ippolito, Elena Reid, Eric Buchatskaya, Eric Ni, Geng Noland, George Yan, George-Christian Tucker, Grigory Muraru, Henryk Rozhdestvenskiy, Ian Michalewski, Ivan Tenney, Jacob Grishchenko, James Austin, Jane Keeling, Jean-Baptiste Labanowski, Jeff Lespiau, Jenny Stanway, Jeremy Brennan, Johan Chen, Justin Ferret, Justin Chiu, Katherine Mao-Jones, Kathy Lee, Katie Yu, Lars Lowe Millican, Lisa Sjoesund, Lucas Lee, Machel Dixon, Maciej Reid, Mateo Mikuła, Michael Wirth, Nikolai Sharman, Nithum Chinaev, Olivier Thain, Oscar Bachem, Oscar Chang, Paige Wahltinez, Paul Bailey, Petko Michel, Rahma Yotov, Ramona Chaabouni, Reena Comanescu, Rohan Jana, Ross Anil, Ruibo Mcilroy, Ryan Liu, Mullins, L Samuel, Sebastian Smith, Sertan Borgeaud, Sholto Girgin, Shree Douglas, Siamak Pandya, Soham Shakeri, Ted De, Tom Klimenko, Vlad Hennigan, Wojciech Feinberg, Yu Stokowiec, Zafarali Hui Chen, Zhitao Ahmed, Tris Gong, Ludovic Warkentin, Minh Peran, Giang, arXiv:2403.08295Clément Farabet, Oriol Vinyals. Koray Kavukcuoglu, Demis HassabisAlek Andreev. and Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and Technology</p>
<p>. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size. -pages. To appear</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Tal Linzen, Grzegorz Chrupała, Afra Alishahi, the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models. Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li, Advances in Neural Information Processing Systems. Red Hook, NY, USACurran Associates, Inc2021To appear</p>
<p>PromISe: Releasing the Capabilities of LLMs with Prompt Introspective Search. Minzheng Wang, Nan Xu, Jiahao Zhao, Yin Luo, Wenji Mao, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta Calzolari. Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), Nicoletta CalzolariTorino, Italia2024ELRA and ICCL</p>
<p>Yuqing Wang, Yun Zhao, arXiv:2308.05342Metacognitive Prompting Improves Understanding in Large Language Models. 2024</p>
<p>Emergent Abilities of Large Language Models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, William Fedus, 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 352022. 2022</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik R Narasimhan, Thirty-seventh Conference on Neural Information Processing Systems. Red Hook, NY, USACurran Associates, Inc2023id=5Xc1ecxO1h To appear</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, arXiv:2304.09797Progressive-Hint Prompting Improves Reasoning in Large Language Models. 2023</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. Red Hook, NY, USACurran Associates, Inc2023</p>
<p>Wangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cotterell, Mrinmaya Sachan, arXiv:2305.11170Efficient Prompting via Dynamic In-Context Learning. 2023</p>
<p>Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie, arXiv:2312.07910PromptBench: A Unified Library for Evaluation of Large Language Models. 2024</p>
<p>We implemented a rigorous human annotation process to ensure the quality of HPI 𝐷𝑎𝑡𝑎𝑠𝑒𝑡 scored by human experts for the datasets. Human annotators were tasked with calculating the HPI for each sample in a given dataset. The HPI quantifies the cognitive demands imposed on human expert proficiency in completing a task, based on the HPT, where higher values indicate greater cognitive demands. A Human Annotation and Judgement Policy A.1. Human Annotation Policy HPI 𝐷𝑎𝑡𝑎𝑠𝑒𝑡 is introduced to penalize the HPI of tasks or samples unsolvable by the LLM, aligning the framework more closely with human cognitive demands and enhancing its comprehensiveness. Each sample was scored on a scale from 1 (lowest complexity level) to 5 (highest complexity level) for the following criteria</p>            </div>
        </div>

    </div>
</body>
</html>
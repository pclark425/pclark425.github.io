<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3082 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3082</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3082</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-264590648</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.225.pdf" target="_blank">Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3082.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3082.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaSLaM+GPT-3.5 (Geometry)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaSLaM (Decomposition And Solution LAnguage Models) coordinating text-davinci-003 (GPT-3.5) on Geometry problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper introduces DaSLaM, a modular system where a finetuned small decomposer LM (LLaMA-13B) generates solver-conditioned subproblems and a black-box solver LM (text-davinci-003 / GPT-3.5) answers them iteratively; applied to geometry problems from the MATH benchmark, DaSLaM improves solver accuracy by producing targeted subquestions and appended CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (referred to as GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI autoregressive large language model used here as a frozen black-box solver; used via prompting (no solver finetuning) and receives appended subproblems, their answers, and CoTs produced during the DaSLaM loop.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometry problems (subset of MATH dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual geometry problems from the MATH benchmark requiring spatial/math reasoning (e.g., reasoning about shapes, lengths, angles, discriminants and sign reasoning relevant to geometric/analytic geometry problems). They are presented as natural-language math questions rather than as images.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Pure text: problem statements (natural-language math questions) from the MATH dataset; no visual or grid inputs were used.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>DaSLaM pipeline: zero-shot evaluation where a finetuned small decomposer LM (LLaMA-13B) generates subproblems conditioned on the original question plus the solver's initial CoT/answer (feedback-driven); the solver (GPT-3.5) then answers subproblems iteratively and the original question is appended with subproblem answers and CoTs before final answer. Compared against Chain-of-Thought (CoT), Least-to-Most (L2M), Progressive-Hint Prompting (PHP), DSP, and ablations (DaSLaM-NF, finetuned-solver, prompted GPT-3.5 decomposer).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>No low-level representation or attention analyses specific to spatial knowledge were reported. The paper provides qualitative case studies (example-by-example CoT traces) showing that DaSLaM's solver-conditioned subquestions guide the solver to correct intermediate facts (e.g., correct sign/discriminant reasoning, explicit computation of geometric subvalues) and produce more stable, earlier convergence to correct reasoning steps than CoT or L2M. There is no probing of internal spatial representations or ablation of spatial-token encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported improvement on Geometry subset of MATH: DaSLaM produced a +7.2 percentage-point improvement in accuracy over standard Chain-of-Thought prompting when using text-davinci-003 as the solver. (The paper reports domain-wise relative improvements; absolute per-domain accuracy numbers are provided in their tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Authors report cases where the decomposer generates subproblems that are not readily answerable in-context (leading to failure); token-usage overhead from RL exploration; DaSLaM is limited to text-only decomposition (cannot incorporate visual inputs needed for some geometric/spatial tasks), and the system does not currently trace back errors or dynamically reorder subproblems based on intermediate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>DaSLaM-augmented GPT-3.5 outperforms the same GPT-3.5 using CoT and other prompting baselines (L2M, PHP, DSP) on geometry and other MATH domains; the finetuned small decomposer (LLaMA-13B) as a module is more effective than prompting GPT-3.5 to act as decomposer. The paper also notes DaSLaM-boosted GPT-3.5 approaches or, on some tasks, matches GPT-4 performance, but no direct human performance comparison on geometry is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3082.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3082.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaSLaM+LLaMA (Geometry / MATH)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaSLaM coordinating LLaMA-13B and LLaMA-33B solvers on MATH (including Geometry) problems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper demonstrates that the same DaSLaM decomposer (finetuned LLaMA-13B) improves performance of smaller LLaMA-family solver models (13B and 33B) on MATH benchmark tasks including geometry, showing solver-agnostic gains even for reduced-capacity LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B and LLaMA-33B (used as solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source autoregressive transformer foundation models (LLaMA family) used as black-box solvers; LLaMA-13B was also used as the decomposer base and finetuned for decomposition, while LLaMA solvers were instruction-tuned/used as provided (LLaMA-13B instruction-finetuned via Alpaca; LLaMA-33B instruction-finetuned from HuggingFace).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B and 33B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Geometry problems (and other MATH domains) from the MATH dataset</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same textual math/geometry problems as above—natural-language questions that require spatial/mathematical reasoning rather than image-based spatial puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual problem statements from MATH/AQuA/JEEBench datasets; no image or structured grid inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>DaSLaM: finetuned decomposer generates solver-conditioned subproblems using PPO-based RL fine-tuning; solver (LLaMA-13B/33B) answers subproblems iteratively and final question appended. Compared to CoT and L2M baselines; PHP failed in one-shot with these smaller solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Appendix case studies (Figures 3 and 4) show qualitatively that for smaller LLaMA solvers, CoT and Least-to-Most often fail on geometry examples while DaSLaM guides them to correct solutions. No quantitative internal probing of spatial representations is provided—analysis is behavioral and qualitative.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>The paper reports that DaSLaM 'almost doubles' performance of base CoT across multiple MATH tasks for the LLaMA solvers; concrete per-domain numbers are in their result tables (DaSLaM yields consistent improvement across tasks for both 13B and 33B solvers). Exact per-domain percentages are reported in the paper tables (see Table 2), but the key reported fact is substantial relative gains vs. CoT and failure or weak gains from L2M/PHP.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Smaller solvers still depend on decomposer quality; when decomposer lacks feedback (DaSLaM-NF) improvements are smaller; some failure cases occur when decomposer creates subproblems not answerable in context. Same text-only limitation applies for spatial tasks requiring visual reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>DaSLaM yields greater relative improvement for smaller LLaMA solvers compared to baseline prompting methods (L2M often fails with LLaMA variants). The paper emphasizes decomposer finetuning is more beneficial than solver finetuning; no human baselines are presented for these specific solver+task combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Progressive-hint prompting improves reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Have llms advanced enough? a challenging problem solving benchmark for large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3082",
    "paper_id": "paper-264590648",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "DaSLaM+GPT-3.5 (Geometry)",
            "name_full": "DaSLaM (Decomposition And Solution LAnguage Models) coordinating text-davinci-003 (GPT-3.5) on Geometry problems",
            "brief_description": "This paper introduces DaSLaM, a modular system where a finetuned small decomposer LM (LLaMA-13B) generates solver-conditioned subproblems and a black-box solver LM (text-davinci-003 / GPT-3.5) answers them iteratively; applied to geometry problems from the MATH benchmark, DaSLaM improves solver accuracy by producing targeted subquestions and appended CoTs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (referred to as GPT-3.5)",
            "model_description": "OpenAI autoregressive large language model used here as a frozen black-box solver; used via prompting (no solver finetuning) and receives appended subproblems, their answers, and CoTs produced during the DaSLaM loop.",
            "model_size": "175B",
            "puzzle_name": "Geometry problems (subset of MATH dataset)",
            "puzzle_description": "Textual geometry problems from the MATH benchmark requiring spatial/math reasoning (e.g., reasoning about shapes, lengths, angles, discriminants and sign reasoning relevant to geometric/analytic geometry problems). They are presented as natural-language math questions rather than as images.",
            "input_representation": "Pure text: problem statements (natural-language math questions) from the MATH dataset; no visual or grid inputs were used.",
            "prompting_method": "DaSLaM pipeline: zero-shot evaluation where a finetuned small decomposer LM (LLaMA-13B) generates subproblems conditioned on the original question plus the solver's initial CoT/answer (feedback-driven); the solver (GPT-3.5) then answers subproblems iteratively and the original question is appended with subproblem answers and CoTs before final answer. Compared against Chain-of-Thought (CoT), Least-to-Most (L2M), Progressive-Hint Prompting (PHP), DSP, and ablations (DaSLaM-NF, finetuned-solver, prompted GPT-3.5 decomposer).",
            "spatial_reasoning_analysis": "No low-level representation or attention analyses specific to spatial knowledge were reported. The paper provides qualitative case studies (example-by-example CoT traces) showing that DaSLaM's solver-conditioned subquestions guide the solver to correct intermediate facts (e.g., correct sign/discriminant reasoning, explicit computation of geometric subvalues) and produce more stable, earlier convergence to correct reasoning steps than CoT or L2M. There is no probing of internal spatial representations or ablation of spatial-token encodings.",
            "performance_metrics": "Reported improvement on Geometry subset of MATH: DaSLaM produced a +7.2 percentage-point improvement in accuracy over standard Chain-of-Thought prompting when using text-davinci-003 as the solver. (The paper reports domain-wise relative improvements; absolute per-domain accuracy numbers are provided in their tables.)",
            "limitations_or_failure_modes": "Authors report cases where the decomposer generates subproblems that are not readily answerable in-context (leading to failure); token-usage overhead from RL exploration; DaSLaM is limited to text-only decomposition (cannot incorporate visual inputs needed for some geometric/spatial tasks), and the system does not currently trace back errors or dynamically reorder subproblems based on intermediate answers.",
            "comparison_to_other_models_or_humans": "DaSLaM-augmented GPT-3.5 outperforms the same GPT-3.5 using CoT and other prompting baselines (L2M, PHP, DSP) on geometry and other MATH domains; the finetuned small decomposer (LLaMA-13B) as a module is more effective than prompting GPT-3.5 to act as decomposer. The paper also notes DaSLaM-boosted GPT-3.5 approaches or, on some tasks, matches GPT-4 performance, but no direct human performance comparison on geometry is provided.",
            "uuid": "e3082.0",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DaSLaM+LLaMA (Geometry / MATH)",
            "name_full": "DaSLaM coordinating LLaMA-13B and LLaMA-33B solvers on MATH (including Geometry) problems",
            "brief_description": "The paper demonstrates that the same DaSLaM decomposer (finetuned LLaMA-13B) improves performance of smaller LLaMA-family solver models (13B and 33B) on MATH benchmark tasks including geometry, showing solver-agnostic gains even for reduced-capacity LMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B and LLaMA-33B (used as solvers)",
            "model_description": "Open-source autoregressive transformer foundation models (LLaMA family) used as black-box solvers; LLaMA-13B was also used as the decomposer base and finetuned for decomposition, while LLaMA solvers were instruction-tuned/used as provided (LLaMA-13B instruction-finetuned via Alpaca; LLaMA-33B instruction-finetuned from HuggingFace).",
            "model_size": "13B and 33B",
            "puzzle_name": "Geometry problems (and other MATH domains) from the MATH dataset",
            "puzzle_description": "Same textual math/geometry problems as above—natural-language questions that require spatial/mathematical reasoning rather than image-based spatial puzzles.",
            "input_representation": "Textual problem statements from MATH/AQuA/JEEBench datasets; no image or structured grid inputs.",
            "prompting_method": "DaSLaM: finetuned decomposer generates solver-conditioned subproblems using PPO-based RL fine-tuning; solver (LLaMA-13B/33B) answers subproblems iteratively and final question appended. Compared to CoT and L2M baselines; PHP failed in one-shot with these smaller solvers.",
            "spatial_reasoning_analysis": "Appendix case studies (Figures 3 and 4) show qualitatively that for smaller LLaMA solvers, CoT and Least-to-Most often fail on geometry examples while DaSLaM guides them to correct solutions. No quantitative internal probing of spatial representations is provided—analysis is behavioral and qualitative.",
            "performance_metrics": "The paper reports that DaSLaM 'almost doubles' performance of base CoT across multiple MATH tasks for the LLaMA solvers; concrete per-domain numbers are in their result tables (DaSLaM yields consistent improvement across tasks for both 13B and 33B solvers). Exact per-domain percentages are reported in the paper tables (see Table 2), but the key reported fact is substantial relative gains vs. CoT and failure or weak gains from L2M/PHP.",
            "limitations_or_failure_modes": "Smaller solvers still depend on decomposer quality; when decomposer lacks feedback (DaSLaM-NF) improvements are smaller; some failure cases occur when decomposer creates subproblems not answerable in context. Same text-only limitation applies for spatial tasks requiring visual reasoning.",
            "comparison_to_other_models_or_humans": "DaSLaM yields greater relative improvement for smaller LLaMA solvers compared to baseline prompting methods (L2M often fails with LLaMA variants). The paper emphasizes decomposer finetuning is more beneficial than solver finetuning; no human baselines are presented for these specific solver+task combinations.",
            "uuid": "e3082.1",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Progressive-hint prompting improves reasoning in large language models",
            "rating": 1,
            "sanitized_title": "progressivehint_prompting_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Have llms advanced enough? a challenging problem solving benchmark for large language models",
            "rating": 2,
            "sanitized_title": "have_llms_advanced_enough_a_challenging_problem_solving_benchmark_for_large_language_models"
        }
    ],
    "cost": 0.01174625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</p>
<p>Gurusha Juneja 
Iit Delhi 
India Subhabrata Dutta 
Sunny Manchhanda 
Tanmoy Chakraborty </p>
<p>IIT Delhi
India</p>
<p>Soumen Chakrabarti IIT Bombay
India</p>
<p>DYSL-AI
India</p>
<p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
2D9D0B1B135200C1C577934B01BED884
Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities.Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem.A significant disadvantage is that foundational LLMs are typically not available for finetuning, making adaptation computationally prohibitive.We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM.We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps.These subproblems are answered by a solver.We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as blackbox) and guide it through subproblems, thereby rendering our method solver-agnostic.Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4.Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique.Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.</p>
<p>Introduction</p>
<p>In recent years, an astounding variety of text and NLP tasks have been accomplished by language models (LMs) (Devlin et al., 2019) -in essence, fitting continuous feature vectors to tokens and modeling smooth conditional distributions over thousands of token positions with multi-task objectives.The next generation of large LMs (LLMs) such as T5, GPT4 and Bard (Raffel et al., 2020;OpenAI, 2023) developed protean capabilities, extending to mathematical and logical ability, based on prompting and in-context learning.Chain-ofthought (CoT) prompting has been a key enabler (Wei et al., 2022;Feng et al., 2023).LLMs can solve middle-school word problems and equations reasonably well.It has also acquired the ability to invoke specialized external tools such as Wolfram Alpha (Wolfram, 2023;Schick et al., 2023).</p>
<p>Recent advances in LLMs have arisen largely from cleverly-engineered, ever-growing training data, rather than any significant change in network structure, which remains relatively regular, but with rapidly increasing network size and number of parameters.One outcome of such giant monolithic LLMs is unsustainable levels of hardware and energy (Dhar, 2020) to train them.Meanwhile, neurologists and brain scientists have known, via fMRI scans, inter alia, that cerebral functions are specialized and spatially localized (Fedorenko and Varley, 2016;Mahowald et al., 2023).</p>
<p>Many recent complex reasoning challenges thrown at LLMs have a two-level character -the input task needs to be decomposed into subtasks, then the subtasks need to be solved, and finally, subtask solutions have to be consolidated and combined to solve the input task.Existing approaches use the same LLM to both decompose and solve the task, sometimes in tangled and uninterpretable ways.Because the sharing of an LLM across these functions cannot be closely controlled, very large models are needed for this double ability (decompose and solve) to emerge.</p>
<p>Staying entirely inside the LLM regime, and avoiding the possibility of specialized tools, we Figure 1: Working example of DaSLaM on a mathematical reasoning question from the JEEBench dataset (Arora et al., 2023).In this example, the solver LM is text-davinci-003.In step 1 , the solver is prompted to answer the question (blue textbox) and it fails to answer correctly (red textbox).A problem decomposing LM generates subproblems (violet textboxes) conditioned on the original question and the initial response of the solver in step 2 .In step 3 , the solver answers these subproblems iteratively and appends to the prompt.Finally, the original problem is appended to the prompt in step 4 , and the solver answers it correctly (green textbox).ask a simple question -is it possible to offload the ability of problem decomposition to a dedicated, relatively smaller scale model, which is specialized and can act in synergy with any solver model of choice?To incorporate flexibility and better generalization, an immediate requirement of such a setup would be to enable a model-agnostic communication between the decomposer and the solver.</p>
<p>Our contributions.To study this research question, we develop DaSLaM (Decomposition And Solution LAnguage Models), in which we separate the decomposer from the solver, as shown in Figure 1.The solver LM can be conventionally trained or fine-tuned.In the illustration, when it answers a question incorrectly, the decomposer LM takes over to produce sub-questions.The partial solutions are appended and resubmitted to the solver LM, which solves the question correctly.The decomposer LM regards the solver as a black box, and uses reinforcement learning (RL) to become a specialized expert at decomposition, informed by the solver's mistakes.</p>
<p>Extensive experiments with three reasoning datasets (MATH, AQuA, and JEEBench) show that the proposed specialization improves the perfor-mance of OpenAI GPT-3 text-davinci-003 to outperform GPT-3.5 and even begins to compete with GPT-4, outperforming other similar methods.</p>
<p>DaSLaM boosts text-davinci-003 from an exact match accuracy of 41.6 to 54.5 in zero-shot regime, which is 3.9 points higher than few-shot GPT-4.Similarly, on Physics problems from JEEBench dataset, DaSLaM-augmented text-davinci-003 scores only 0.58 points short of GPT-4 while outperforming GPT-3.5.The decomposer LM in DaSLaM reduces decomposition errors, and generalizes well across diverse small-scale LMs.It is also more robust in the face of difficult datasets, where the solver gives near-random performance.</p>
<p>These results support our founding hypothesis that heterogeneous functional specialization improves model efficiency and robustness of LLMs.A crucial findings from our experiments is that finetuning the decomposer is much more powerful choice than finetuning the solver.Moreover, a finetuned decomposer is largely superior compared to an orders of magnitude larger LLM prompted to act as a decomposer.Given the prohibitive cost of finetuning LLMs like GPT 3, 3.5, or 4, we hope this method would provide us a promising direction to-wards future development of task-expert models.1</p>
<p>Related Work</p>
<p>Eliciting superior reasoning abilities in LM through specially designed prompts has found its popularity through CoT prompting (Wei et al., 2022) -asking the LM to explain the reasoning steps improves overall performance.Decomposing a complex reasoning requirement into multiple, simple steps results in superior reasoning capabilities across modalities other than free-form natural language text as well, e.g., reasoning over tabular data (Ye et al., 2023), visual question answering (Lu et al., 2022), etc.These methods generally solicit a single run of LM inference with no intermediate prompting interactions.Consequently, the LM often misses key reasoning steps or hallucinates irrelevant ones.</p>
<p>On the other hand, a prototypical prompter, sequentially interacting with the LM, has shown impressive performance.Progressive Hint Prompting (Zheng et al., 2023) uses such a setting; first, the LM is asked to provide a base answer.The prompt then uses the answer as a hint to the LM that progressively guides it to the final answer.Zhou et al. (2023) followed a similar direction by breaking down the problem itself.Their method, Least-tomost prompting, asks the LM to generate simpler, related problems from a complex problem.The final solution to the original question is generated by the LM conditioned upon the solution of the subproblems.A major bottleneck then becomes the solver's ability to identify the critical subproblems.Decomposing a complex task and then solving each task via multiple LLMs with their own in-context examples have been attempted as well (Dua et al., 2022;Khot et al., 2023).Recently, Shridhar et al. (2022) explored subquestion generation from complex questions as means of distilling reasoning abilities from larger LMs to smaller ones.</p>
<p>Our proposed method, DaSLaM makes a departure from these mentioned approaches in three particular features: (i) we seek to separate out the decomposer from the solver to get rid of the solver's limitations affecting decomposition, (ii) the decomposer acts as a plug-and-play module that can generalize to any solver, and (iii) the decomposition actuates with complete knowledge of the solver's actions.</p>
<p>When answering Q requires multistep reasoning, one can conceptualize S as a sequence of smaller steps
{S ′ 1 , S ′ 2 , • • • , S ′ n } such that the LM iteratively answers a sequence of subproblems {Q ′ 1 , • • • , Q ′ n }
to finally reach the desired answer to the original question.Eq. 1 can then be rewritten as,
Â = arg max A ′ n ∈A i p θ (A ′ i |S ′ i , Q ′ i ) arg max S ′ i p θ (S ′ i |Q ′ i ) (2)
where A ′ i is the answer to the subproblem Q ′ i .In the usual regime of CoT, the subproblems Q ′ i are implicit; the LM discovers them on its own and generates the reasoning steps and the answers.Due to the repeated multiplication in Eq. 2, any error in the initial stages quickly propagates along the chain of steps.</p>
<p>In DaSLaM, we seek to alleviate this problem by offloading the task of inferring the subproblems {Q ′ i } to a decomposer LM ϕ.For a more guided problem decomposition, DaSLaM uses the answer and the steps generated by the solver LM, θ in the naive CoT regime as described in Eq. 1 to generate a set of subproblems { Qi } i=1,...,n as follows:
Qi = arg max Q ′ i p ϕ (Q ′ i |{ Qj : j ∈ [1, i − 1]}, Q, Â0, S0),(3)
for i ∈ [1, n], where Â0 and S 0 are the initial answer and reasoning steps, respectively generated by θ.The solver LM θ then solves the subproblem set one-by-one similar to Eq. 2. However, instead of seeking to generate the final answer as a response to the last subproblem Qn , we append the original question at the end and let θ answer it directly given the context generated by the subproblems, their answers, and the corresponding CoTs.The four stages of workflow with DaSLaM, as described The first stage is straightforward with the decomposer LM being finetuned using language modeling objective.In the first stage, we seek to optimize the following objective:
min ϕ [− log(p ϕ (Q ′ gold |Q, S))]
(5)</p>
<p>This step is somewhat similar to instruction tuning, where an LM is asked to generate some text conditioned on a context, instead of following the usual sentence completion-style behavior of the LM.Intuitively, the role of this stage is to align the LM to the task of the decomposer.</p>
<p>Decomposition via policy network.The previous stage inculcates the ability to decompose a problem within ϕ.However, it is still blind to the actual errors made by the solver LM.In the next stage, we seek to make the decomposer LM work in synergy with any solver LM of choice.This solver agnosticism restrains ϕ to observe the internal representations computed by θ while solving the problem.To handle this imposed blackbox characteristics of the solver, we resort to policy gradient optimization of ϕ assuming θ to be part of the environment.We formalize the setup as follows.</p>
<p>Elements of the environment: The solver LM θ constitutes the core component of the environment.Given a question Q, the solver-generated CoT S and the answer A as sequences of tokens define the observation of the policy.</p>
<p>State and action space:</p>
<p>We define the state space as S. The initial state, s 0 ∈ S, is defined by the original question Q and the initial response from the solver, S 0 , A 0 , that are provided to the decomposer LM ϕ as input.A single timestep is defined on generation of a single token, i.e., the action a t at time t.Given the autoregressive nature of LM, we define the state at t-th timestep as s t = (s t−1 , {a t−1 }), i.e., the token generated at t − 1 appended to the tokens generated till t − 1. Trivially, the action space is the same as V, the vocabulary of ϕ.</p>
<p>Policy:</p>
<p>The decomposer LM is conceptualized as a policy network π ϕ : S − → V, i.e., it generates a token given the inputs and the token generated hitherto, till the end of episode T .Inspired by the recent success in Reinforcement Learning from Human Feedback (RLHF) with autoregressive LMs, we choose the Proximal Policy Optimization (PPO) algorithm to train the policy π ϕ (a|s).In a typical PPO setup, we define the advantage function as follows:
Ĝt = T −t+1 i=0 (γλ) i [rt+i + γV (st+i+1) − V (st+i)] (6)
where r t is the reward at step t, V (s t ) : S − → R is the value function determining the reward associated to state s t , and γ, λ are hyperparameters.We use the policy model augmented with a randomly initialized feedforward layer as the value function.A crucial component in on-policy learning regime is the reward function r t .While the end goal of the learning is to have the solver answering correctly, the decomposer LM should receive some incremental signal aligned to its generation as well for it to stably converge to the optimal policy.With this goal in mind, we construct the reward function as,
r t = R 1 + R 2 + R 3 + R 4 + R 5
(7) where R 1 to R 5 are defined as follows (see Appendix B for a detailed treatment on the reward computation method).Here cos-sim represents cosine similarity.
I(x) = 1 if x is true is the indicator function. Entity coverage: R 1 = |E Q ′ | |E Q |
, where E Q ′ and E Q are the sets of distinct entities in the generated subproblems and the original question, respectively.Consistency of answers to subproblems:
R2 = i I(ei = êi) + cos-sim(Q ′ i , Ai) (8)
where êi is the entity whose value has been asked in the subproblem Q ′ i , and e i is the entity answered.This reward penalizes the decomposer LM for generating questions whose answers are not consistent.Order of operations: R 3 = l m , where l is the number of operations matched in order between S and S gold , and m is the total number of operations in S gold .</p>
<p>CoT proximity: To ensure that the distance of reasoning produced by the model after prompting S to the gold reasoning S gold is less than the distance of reasoning produced without prompt S 0 to the gold reasoning steps S gold , we design a reward based on the cosine similarity of each step of S gold .We break S and S 0 at new-line token to form reasoning steps.At each step j, we compute c 1j = cos-sim(S j , S j gold ) and c 2j = cos-sim(S j 0 , S j gold ).The reward is
R4 = m j=0 I(c1j &gt; c2j)c1j + I(c2j &gt; c1j)(−1 − c2j),(9)
Correctness of final answer: R 5 =I( Â=A gold ).Now, we can define the PPO objective as follows:
max ϕ Et π ϕ (at|st) π ref (at|st) Ĝt − βEt <a href="10">Kt</a>
where π ref is the reference model that is initialized with supervised finetuned ϕ.
K t = KL[π ref (•|s t ), π ϕ (•|s t )]
is the KL-divergence between the reference model and the policy model.The resulting decomposer LM ϕ optimized using the above mentioned three stages of finetuning can then be used with DaSLaM.</p>
<p>Experiments</p>
<p>Training data curation.The training process of DaSLaM consists of two stages as mentioned previ-ously.In the first stage, we require the subproblems along with the reasoning steps for a given problem.We use samples from four existing datasets -MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and Strate-gyQA (Geva et al., 2021).Each question in these four datasets contains a question Q gold , a step-bystep illustration of the reasoning process S gold , and the final answer A gold .We sample 7, 000 examples from the training splits of these datasets and employ OpenAI's text-davinci-003 model to generate the corresponding subquestions.We provide the model with one-shot example illustrating how to decompose a question into subquestions based on the reasoning.In the second stage of training, we utilize the remaining training data from MATH and AQuA datasets to conduct the policy optimization since this step does not require any supervised examples of subproblems.</p>
<p>LMs used.We use LLaMA 13 billion (Touvron et al., 2023) as the decomposer LM.For the solver LM, we primarily use text-davinci-003 (henceforth, we denote it as GPT-3.5 for brevity).We also experiment with the LLaMA 13 bilion and LLaMA 33 billion models as solvers to test the model-agnostic generalizability of DaSLaM.</p>
<p>Baselines.We compare DaSLaM with four existing methods of prompting: Chain-of-thought prompting (CoT) (Wei et al., 2022), Least-to-most prompting (L2M) (Zhou et al., 2023), Progressive Hint Prompting (PHP) (Zheng et al., 2023), and, Demonstrate-Search-Predict (DSP) (Khattab et al., 2022a).The original setting of PHP requires an 8-shot prompting; however, since all other methods including DaSLaM predict in the zero-shot setting, we use PHP in 1-shot for a fairer comparison.Additionally, we experiment with three ablation variants: DaSLaM-NF does not take the solver feedback into account while generating the subproblems; Finetuned is the solver LM (LLaMA 13B in this case, we could not finetune 33B variant due to computational constraints) finetuned without any decomposer; GPT-3.5 decomposer does away with the finetuned LLaMA 13B decomposer and uses pretrained GPT-3.5 as the prompted decomposer.</p>
<p>Test datasets.For evaluation purposes, we use three datasets -MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), and JEEBench (Arora et al., 2023).For the first two datasets, only the test splits are used during evaluation since their</p>
<p>Experimental Results</p>
<p>The tasks used to evaluate the performance of DaSLaM contain questions that can be answered either of the three types -numerical, single correct answer MCQ, and multiple correct answer MCQ.DaSLaM is better than pure prompting We start with DaSLaM augmented with GPT-3.5 as the solver LM on MATH and AQuA datasets (see Table 1).The improvement achieved with DaSLaM prompting compared to standard CoT is staggering across all types of problems in the MATH dataset: +11.7 on Pre-Algebra, +8.4 on Intermediate Algebra, +7.7 on Number Theory, +7.2 on Geometry, +5.0 on Probability and Combinatorics, +5.8 on Algebra, and +4.2 on Calculus.The absolute improvement is even larger on the AQuA dataset, i.e., +12.9 over CoT.It is noticeable that the effects of DaSLaM are stronger across tasks containing algebraic reasoning (AQuA, Pre-and Intermediate-Algebra, etc.) compared to Probability and Combinatorics or Calculus, which require more implicit knowledge.The performance gain achieved via DaSLaM is significantly better compared to methods like L2M or PHP.The latter methods often fail to improve over standard CoT (e.g., on Probability and combinatorics, Number Theory, and Algebra problems, L2M shows a drop in accuracy).Even when improving over CoT, their improvement is meager compared to DaSLaM.This trend entails our earlier argument in support of offloading the problem decomposition task to a specialized LM; methods that prompt the solver LM to decompose the problem lack the expertise achieved via dedicated finetuning in DaSLaM.</p>
<p>Finetuned decomposer is essential.Despite being orders of magnitude smaller, a finetuned LLaMA 13B model delivers better performance compared to GPT-3.5 as a decomposer (DaSLaM vs. GPT-3.5generator in Table 1 and 2).This further justifies our choice of separately finetuning the decomposer and the added flexibility that it offers.In fact, finetuning the decomposer is far effective compared to finetuning the solver (DaSLaM vs Finetuned solver in Table 2).</p>
<p>Feedback from the solver is important.In the preceding paragraph, we attributed the superiority of DaSLaM over other methods to the usage of a specialized LM for problem decomposition.However, manipulating the problem decomposition upon feedback from the solver is also an important factor here.None of the existing methods does so, and therefore, remains blind towards what reasoning (and possible errors) is followed by the solver model.This is further manifested when we compare DaSLaM with itself without the feedback module, DaSLaM-NF.While DaSLaM-NF is able to improve upon basic CoT and other prompting methods, it falls short of a decomposer LM that has access to the initial response of the solver.</p>
<p>DaSLaM generalizes to smaller solvers.An important aspect of a prompting method is its ability to work with LMs of different scales.Despite being finetuned with GPT-3.5 responses only, DaSLaM is able to improve upon the base performance of smaller scale LLaMA models as well (see Table 2).L2M prompting generally fails with both LLaMA 13 billion and 33 billion variants.On the other hand, DaSLaM, with or without feedback, almost doubles the performance of the base CoT across multiple tasks of the MATH dataset.It shows substantial improvement on AQuA as well.The importance of feedback from the solver LM usually manifests strongly in proportion to the scale of the solver.</p>
<p>DaSLaM generalizes to harder problems.Since the decomposer LM ϕ is trained using a subset of the training data of MATH and AQuA, we opt for a harder (in terms of benchmark performance of different LMs) reasoning evaluation on the JEEBench dataset.Table 3 summarizes the performance of the baselines and DaSLaM with GPT-3.5 as the solver LM on Mathematics and Physics questions of the JEEBench dataset.We notice that the superiority of DaSLaM manifests even more profoundly on this task compared to the former ones.Both PHP and L2M prompting absolutely fail to improve upon basic CoT prompting, often with a sharp fall in performance (e.g., Physics MCQ questions).On the other hand, DaSLaM boosts the LMs performance, very often over 100% relative improvement (all three types of problems in Physics and numerical problems in Mathematics).Aggregated across question types, DaSLaM boosts the performance of GPT-3.5 to 22.420 in Physics and 22.07 in Mathematics.It is noteworthy that the same LM in its base setting performs near-random, i.e., 10.4 and 10.7 in Physics and Mathematics, respectively, whereas a random selection baseline gives scores of 9.6 and 10.3, respectively (Arora et al., 2023).Furthermore, GPT-3.5 with DaSLaM outperforms a better optimized candidate of the GPT series, GPT-3.5 on both these subjects (note that Arora et al. (2023) reported 18.9 and 15.7 scores with GPT-3.5 on Physics and Mathematics, respectively).Comparison with GPT-4.The colossal compute used by GPT-4 makes the comparison with any of its predecessors like GPT-3.5 quite unfair.However, it is tempting to observe that DaSLaM boosts the performance of GPT-3.5 often to the level of GPT-4.For example, on arithmetic problems of the AQuA dataset, DaSLaM surprisingly outperforms both zero-shot and few-shot  respectively, compared to 54.5 with GPT-3.5 and DaSLaM).On MATH dataset, DaSLaM augmented GPT-3.5 scores an aggregate of 30.23, which is better than ChatGPT (26.4) and close to .On JEEBench Mathematics problems, GPT-Figure 2: An example case study on a problem from the MATH dataset.GPT-3.5 is used as the solver LM with three different methods of prompting -standard CoT, Least-to-most, and DaSLaM.Only DaSLaM is able to guide the model to the correct answer.</p>
<p>4 comes up with an aggregate score of 23.1, which is pretty close to our 22.42.In Physics and Math MCQ questions, DaSLaM with GPT-3.5 outperforms GPT-4.These results definitely do not claim any assumed superiority of DaSLaM-boosted GPT-3.5 over GPT-4 since there are multiple other cases that state otherwise.Instead, we seek to demonstrate how much leftover potential these LMs possess that can be unleashed via our proposed method of feedback-guided automatic problem decomposition.</p>
<p>Case Study</p>
<p>To this point, we have compared the numbers produced by DaSLaM-boosted models across different datasets.While they provide an overall assessment, deeper analyses are needed to comprehend the actual reasoning steps adopted by these different methods.Figure 2 shows the reasoning steps generated by GPT-3.5 given an example problem from the MATH dataset with three different prompting methods -vanilla CoT, L2M, and DaSLaM.Note that DaSLaM uses the CoT output to decompose the problem.</p>
<p>Both CoT and L2M end up with the model an-swering incorrectly.With CoT, the solver wrongly assumes that the given equation must have two real roots though it should not have any real roots.Also, it mistakes the value of a 2 as a.The effect is prominent in the subproblems generated by DaSLaM as it asks to find the value of a explicitly.Furthermore, the solver LM specifically announces that y ≤ 0 to answer the first subproblem generated by DaSLaM.This helps to correct the reasoning about the sign of the discriminant.</p>
<p>With L2M, the confusion around the value of a and a 2 persists, as the solver LM substitutes a in the given equation by 49 (which is the value of a 2 ) twice in the answering process.Although it substituted the correct value of a once while answering the second question, it is not explicitly declared like in DaSLaM.We observe multiple similar failure cases with L2M.It is quite likely that prompting the model to generate the final answer after each subproblem accumulates the erroneous reasoning steps that the model falls prey to.</p>
<p>With DaSLaM, the reasoning steps followed by the solver remains robust throughout.It reaches the final answer much earlier (third and fourth subproblems).In the final answer, the solver simply reiterates the steps that it earlier generated to answer the subproblems.This is a common behavior that we observed across multiple problems from multiple datasets.In Appendix D (see Figures 3  and 4), we provide similar case studies on LLaMA 13B and 33B with different prompting methods.With reduced solver capacity, the difference between Least-to-most and CoT generated reasoning steps further diminishes with both leading to incorrect answers; DaSLaM, on the other hand, still guides the solver through correct steps.</p>
<p>An interesting observation can be made by comparing how the solver behaves with CoT vs. with DaSLaM.With DaSLaM, we do not provide any new knowledge to the solver.Yet, the same model can rectify its errors made in CoT response.This may point to the intuition that current LLMs are actually underutilized, and one can unfold even more impressive performance with cleverly composed guidance.</p>
<p>We further provide case analyses with when DaSLaM fails to guide the model (GPT 3.5 in this case) to successful final answers, in Appendix E. While we do not find any obvious pattern of errors, one can see that the decomposer generates questions that are not readily answerable within that context.DaSLaM does not use any method to trace back the error or generate subproblems based on the answers to the previous subproblems.This might raise such issues where the subproblems generated are not actually helping the solver in the right order.</p>
<p>Conclusion</p>
<p>We challenged the design of ever-larger monolithic LLMs as homogeneous network structures, where diverse aspects of problem decomposition and solution are stored in a tangled and opaque manner.The formidable general-purpose problem-solving capabilities of LLMs are exceedingly resource-hungry, dependent on immense data engineering.Inspired by brain science, we took a first step toward heterogeneity -let two different LLMs evolve independently and adapt to their roles of decomposing and solving complex reasoning problems.Through extensive experiments on several benchmarks, we showed that such a heterogeneous network can match or exceed some of the largest contemporary LLMs, at a much smaller parameter count.</p>
<p>Limitations</p>
<p>A potential limitation of DaSLaM, as with many system that uses an LLM-as-a-service API charging per token exchange, is the increased token usage because of the RL exploration.Asserting a token budget on the decomposer LM is left as an avenue for future exploration.Ideally, the decomposer LM should seamlessly invoke solvers of many forms, such as retrievers (Khattab et al., 2022b) or mathematical calculators (Schick et al., 2023;Wolfram, 2023).Future work may extend DaSLaM to such tools.DaSLaM is limited to purely text-based subproblem decomposition; it is not possible at present to incorporate reasoning through other modalities (e.g., visual inputs for geometric reasoning) into DaSLaM in its current form.</p>
<p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning (Appendix)</p>
<p>A Supervised Fine-tuning Dataset</p>
<p>The training data for supervised fine-tuning stage was generated using text-davinci-003.Since Jake owns 3 of the boots, the subset from which the 4 boots should be chosen are the 12 boots not owned by Jake from the universe of 15.The first boot can be one of the 12 from the 15 with probability 12 15 .The second boot can be one of the 11 from the 14 remaining with probability 11 14 .The third boot can be one of the 10 from the 13 remaining with probability 10</p>
<p>13 .The fourth boot can be one of the 9 from the 12 remaining with probability 9 12 .The total probability will be 12 15 • 11 14 • 10 13 • 9 12 .On cancellation, this comes to 33 91 Q ′ gold 1.How many boots did Jake own? 2. How many boots were on the field?3. How many boots did Peter take? 4. What is the probability of choosing one of the 12 boots not owned by Jake from the universe of 15? 5. What is the probability of choosing the second, third and fourth boots not owned by Jake? 6.What is the total probability?</p>
<p>B Reward Calculation</p>
<p>During policy gradient optimization of the decomposer LM ϕ, rewards were provided in an incremental fashion so that the generated sub-questions align well with the initial question, the answer generated by the solver LM θ and the sub-question solving capabilities of θ.For this, the reward was designed as a combination of five terms.The definitions of these terms are given in Section 4.Here we provide an example of the reward calculation.</p>
<p>Entity coverage Reward (R 1 ).Here, we find distinct nouns and numbers in the question and the sub-questions using the nltk library.The reward is calculated using
R 1 = |E Q ′ | |E Q |
, where E Q ′ and E Q are the sets of distinct nouns and numbers in the sub-questions and questions, respectively.An example of the same is shown below.</p>
<p>Qgold:</p>
<p>Each good worker can paint my new house alone in 12 hours.Each bad worker can paint my house alone in 36 hours.I need my house painted in 3 hours.If I can only find 3 good workers, how many bad workers must I also find in order to have my house painted on time?
Q ′ gold :
1. How many good workers are needed to paint the house in 3 hours?2. How many bad workers are needed to paint the house in 3 hours?3. What is the total number of workers needed to paint the house in 3 hours?
|E Q ′ | = 4 + 1 = 5 (11) |E Q ′ | = 6 + 3 = 9 (12) R1 = |E Q ′ | |EQ| = 5 9 = 0.55(13)
Consistency of answers to subproblems reward (R 2 ).To ensure that the sub-question is such that the solver LLM ϕ does not produce random answers, we compare the entity in the solution to the entity asked in the subquestion along with the cosine similarity of the embeddings of sub-question and sub-answers.To find the embeddings, we use the pre-trained model in the spacy library.R 2 = i (I(e i = êi ) + cosine-similarity(Q ′ i , A i )), where I(x) = 1 if x is true, êi is entity whose value has been asked in the subproblem Q ′ i , and e i is the entity answered.An example for the reward calculation is given below.</p>
<p>Q gold : Three friends Alan, Roger and Peter attempt to answer a question on an exam.Alan randomly guesses the answer, giving him a 1 5 probability of guessing correctly.Roger cheats by looking at the paper of the student in front of him, giving him a 2 3 probability of answering correctly.And Peter dutifully performs the calculations, then marks the answer, giving him a 5 6 probability of a correct answer.What is the probability that the question is answered correctly, but not via cheating?
Sgold : Prob(Alan) = 1 5 Prob(Roger) without cheating = 2 3 − 1 = 1 3 Prob(Peter) = 5 6 Total Probability = 1 5 • 1 3 • 5 6 = 118
Smodel : Alan has a 1 5 chance of getting the answer correct.Roger has a 2 3 chance of getting the answer correct.Peter has a 5 6 chance of getting the answer correct.The probability that the question is answered correctly is 1 5 + 2 3 + 5 6 = 13 12 .The probability that the question is answered correctly, but
not via cheating is1 − ( 1 5 + 2 3 + 5 6 ) = 1 − 13 12 = −112
Agold:
1 18 Amodel: −1 12 R5 = 0</p>
<p>C Hyperparameter Selection</p>
<p>We performed hyperparameter tuning on a subset of 250 examples from the training dataset.Based on the improvement in accuracy and compute resources, we converged at the following values.</p>
<p>For the supervised fine-tuning stage, we used LoRA r = 16, LoRA α = 32, LoRA droput = 0.05.For the RLMF stage, we finetuned the last 3 layers of LoRA adapters, using a batch size of 8, gradient accumulation steps=8, init kl coef=0.01,target=4.For inference, we used the following generation parameters: temperature = 0.95, top p=0.18,pad token id = 0, do sample = False, number of beams = 1, maxi length = 2048.</p>
<p>All the models were implemented using Huggingface with PyTorch, and loaded in int_8 precision.For solver-LLaMA 13B model, we instruction finetuned the LLaMA13B model using the Alpaca dataset by stanford (Taori et al., 2023).For solver-LLaMA 33 billion model, we used an instruction-finetuned version of LLAMA-33 billion from Huggingface.To implement the PPO algorithm, we used the TRL library.We used a single NVIDIA A100 (80GB) GPU for all training and inference purposes.</p>
<p>D Case Study on LLAMA Models</p>
<p>Similar to Figure 2, we performed example-wise case study on the answering process of the LLaMA solvers (13 billion and 33 billion models) with different prompting methods (see Figures 3 and 4).The example problem was selected from the MATH dataset.As we can see, with relatively small scale solvers like LLaMA, Least-to-most fails to add any improvement over basic CoT prompting.DaSLaM is able to guide the model to the correct answer in both cases.</p>
<p>E Failure cases with DaSLaM</p>
<p>E. 1
1
Case I Question A train running at a speed of 100 miles/hour, takes 10 hours to reach its destination.After covering quarter of the distance, it starts raining and the train has to be slowed to speed of 75 miles/hour.What is the total journey duration?A)10 B)11.5 C)12.5 D)13.5 E)15Initial Answer The train covers the first quarter of the distance at a speed of 100 miles/hour.So, the time taken to cover the first quarter of the distance = (1/4) x 10 hours = 2.5 hours The train covers the remaining three quarters of the distance at a speed of 75 miles/hour.So, the time taken to cover the remaining three quarters of the distance = (3/4) x 10 hours = 7.5 hours Therefore, the total journey duration = 2.5 + 7.5 = 10 hours Sub problem 1 What is the total distance to be covered?SubAnswer 1 Let the total distance to be covered be d.Time taken to cover the first quarter of the distance = (1/4)d/100 = 10/400 hours Time taken to cover the remaining distance = (3/4)d/75 = 300/400 hours Total time taken = 10/400 + 300/400 = 10/4 hours = 2.5 hours Since the train takes 10 hours to reach its destination, Total distance to be covered = 10 x 100 = 1000 miles Sub problem 2 What is the speed of the train?Sub Answer 2 After covering quarter of the distance, the train has to be slowed to speed of 75 miles/hour.Let the remaining distance be x.Therefore, x = d -(d/4) = 1000 -(1000/4) = 750 miles.The time taken to cover the remaining distance x = 750 miles at a speed of 75 miles/hour = (750/75) hours = 10 hours.Therefore, the speed of the train = x/time taken = 750/10 = 75 miles/hour.Sub problem 3 What is the time taken to cover the distance?Sub Answer 3 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours Time taken to cover the remaining distance = (3d/4)/75 hours = d/300 hours Total time taken = d/400 + d/300 = (5d + 4d)/1200 hours = 9d/1200 hours.Putting d=1000, Final answer: The time taken to cover the distance is 9 x 1000/1200 hours.Sub problem 4 What is the total duration of the journey?Sub Answer 4 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours Time taken to cover the remaining three quarters of the distance = (3d/4)/75 hours = 4d/300 hours Total time taken = d/400 + 4d/300 = (5d + 4d)/(400 + 300) = 9d/700 hours Total duration of the journey = 9d/700 hours = 9 hours and (d/700) hours = 9 hours and (100/700) hours = 9 hours and 14.29 minutes Final Answer Step 1: The train covers the first quarter of the journey at a speed of 100 miles/hour.Step 2: The time taken to cover the first quarter of the journey is given by: Time = Distance/Speed Time = (1/4) x 100/100 Time = 1/4 hour Step 3: The train covers the remaining three quarters of the journey at a speed of 75 miles/hour.Step 4: The time taken to cover the remaining three quarters of the journey is given by: Time = Distance/Speed Time = (3/4) x 75/100 Time = 3/4 hour Step 5: The total duration of the journey is the sum of the time taken to cover the first quarter and the remaining three quarters of the journey.Total Time = 1/4 + 3/4 Total Time = 4/4 Total Time = 1 hour Therefore, the total duration of the journey is 1 hour.</p>
<p>Table 1 :
1
Performance comparison on MATH and AQuA datasets using GPT-3.5 as the solver LM.See Section 5 for abbreviations.trainingsplits are used while finetuning the decomposer.The MATH dataset contains mathematical problems on multiple different domains.We report the results on each of them separately and use the following abbreviations -ALG, I-ALG, and P-ALG for Algebra, Intermediate Algebra, and Pre-
DatasetMethod CoT L2M PHP DSP GPT3.5 Decomposer DaSLaM-NF DaSLaMPnC16.4 16.010.2 16.216.020.021.4NT14.4 11.09.820.314.218.426.1ALG27.6 22.424.0 15.332.131.633.4I-ALG 16.4 16.810.0 17.018.420.824.8Calc.14.0 14.58 14.28 18.812.015.118.2P-ALG 32.3 28.026.5 28.035.538.044.0Geom. 14.2 12.514.05.222.019.0421.4AQuA 41.6 44.744.4 44.045.453.254.5
Algebra, respectively; Calc for Calculus, Geom for Geometry, PnC for Probability and Combinatorics, NT for Number theory.From the JEEBench dataset, we use the problems in Physics (Phy) and Mathematics (Math).Each of these two subjects has three types of problems -single-answer multiple-choice questions (MCQ), numerical problems (Num), and multi-answer multiple-choice questions (Multi).For all these datasets, we use exact match criteria to evaluate the correctness of the model-inferred answers.Details of training and inference hyperparameters and compute resource usage are provided in Appendix C.</p>
<p>Table 3 :
3
Performance comparison on the JEE benchmark dataset with GPT-3.5 as the solver LM. 0<em> signifies that the model was not able to answer any problem in the task correctly.
Method CoT L2M Finetuned GPT3.5 Decomposer 2.05 PnC 2.05 1.66 2.8 DaSLaM-NF 2.93 DaSLaM 4.0 CoT 2.4 L2M 2.38 4.16 NT LLaMA 13 billion Dataset ALG iALG Geom 4.0 3.12 2.4 3.2 3.2 3.33 2.8 2.0 3.6 3.57 3.2 4.1 5.0 4.68 2.8 2.08 4.8 4.68 3.2 4.0 5.6 4.70 3.4 4.3 LLaMA 33 billion 4.16 4.54 3.7 4.0 4.2 6.0 4.25 DaSLaM-NF 3.2 5.83 5.6 5.6 5.1 DaSLaM 4.0 7.36 9.09 6.02 5.3Cal 2.08 3.33 4.54 Palg AQuA 5.0 17.7 16.6 3.05 6.04 19.4 4.0 6.66 20.4 3.9 6.2 21.6 4.1 8.33 22.0 4.0 5.2 20.0 5.71 5.55 21.6 5.71 5.2 22.5 6.03 8.44 26.8Table 2: Performance on MATH and AQuA with LLaMA 13 billion and LLaMA 33 billion as solvers. PHP is not reported as one-shot PHP generated randomly with both LLaMA variants. DaSLaM provides consistent improvement across all the tasks while other baseline methods mostly fail.Method CoT PHP L2M DaSLaM-NF DaSLaM GPT-4Dataset Phy MCQ Math MCQ Phy Multi Math Multi Phy Num Math Num Phy Int Math Int 33.33 21.9 6.25 12.0 3.03 1.69 12.5 20.0 22.22 17.07 6.25 7.59 3.03 1.69 0</em> 4.0 22.22 21.9 6.25 12.5 3.03 3.38 10.0 20.0 20.8 31.7 7.5 10.12 3.03 3.38 12.5 16.0 55.55 36.5 18.75 16.0 6.06 10.16 22.5 24.0 55.55 34.14 27.5 21.5 15.15 11.8 25.0 20.0</p>
<p>Each data point in the dataset consisted of a tuple ⟨Q gold , S gold , Q ′ gold ⟩, where Q gold represents the reasoning question, S gold represents the gold reasoning steps, and Q ′ gold represent the sub-questions generated by text-davinci-003 in a one-shot setting.An example of a single data point is given below.
Q goldJohn borrowed 3 soccer boots from Jake, and forgot them on the field, if peter came across a total of 15 boots on the field Jake's boots inclusive, and he took 4 boots at random, what is the probability that Jake's boots were not amongst the 4 taken?. (A) 12 91 , (B) 3 15 , (C) 12 15 (D) 33 91 , (E) 3 91S gold
The codebase is given at: https://github.com/ LCS2-IIITD/DaSLaM
AcknowledgmentsThe authors acknowledge the financial support of DYSL-AI.Q ′1 : How many good workers are needed to paint the house in 3 hours?A1: We can calculate the amount of work done in 3 hours by a good worker by dividing 12 hours by 3 hours.We get 4 as the answer.We know that a bad worker can paint the house in 36 hours.We can calculate the amount of work done in 3 hours by a bad worker by dividing 36 hours by 3 hours.We get 12 as the answer.We need to find out how many good workers are needed to paint the house in 3 hours.We can calculate the number of good workers needed by dividing 12 by 4. Answer: 3 good workers can paint the house in 3 hours.cos-sim(Q ′ i , Ai) = 0.70457435 I(ei = êi) = 1 R2 = 1 + 0.70457435 = 1.70457435Order of operations reward (R 3 ).To evaluate the correctness of the mathematical solution produced by the model, we compare the order of operations.R 3 = l m , where l is the number of operations matched in order between S and S gold , and m is the total number of operations in S gold .Below is an example of R 3 calculation.S gold1. : We first consider the possible neighbors of the chief's mother, one must be the chief, the other is one of the remaining 5 natives.That native can, in turn, have one of 4 neighbors in addition to the chief's mother 2. Continuing around, there are 5! = 120 possible arrangements 3. The chief can be on his mother's left or right, for a total of 2 • 120 = 240 arrangements 4. Alternatively, we know that the seating arrangement is the same if it is rotated, once the chief's spot is chosen, the rotations are eliminated 5.There are 2 ways to seat his wife and his mother, and then there are 5 distinct positions for the remaining 5 natives, which makes for 2 • 5! = 240 possible arrangements S0:1. We can see that the chief must be seated between his mother and his wife.2. This means that there are 3 possible positions for the chief, and 5 possible positions for the other 6 people.3. Therefore, there are 3! • 5! = 1800 different seating arrangements.S:1.The chief must sit between his mother and his wife.2. There are 8 people in the tribe.3. So, the chief's mother and wife can be arranged in 2! ways.4. The remaining 5 people can be arranged in 5! ways. 5. Therefore, the total number of possible seating arrangements after removing rotation = 5! = 240 Cosine similarity calculation per step:1. c1j = 0.47673503, c2j = 0.44773823 2. c1j = 0.45063934, c2j = 0.47917843 3. c1j = 0.5173945, c2j = 0.20383504 4. c1j = 0.46866685, c2j = 0 5. c1j = 0.47825924, c2j = 0 Hence, R4 = 0.47673503 + (−1 − 0.47917843) + 0.5173945 + 0.46866685 + 0.47825924 = 0.46187719Correctness of final answer(R 5 ), R 5 = I( Â = A gold ).This reward checks if the final answer matches the gold answer.A negative example of the same is given below.As n ranges over the positive integers, what is the maximum possible value that the greatest common divisor of 80n + 49 and 18n + 11 can take?What is the greatest common divisor of 8n + 5 and 2n + 1?The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 8n + 5 − 2n − 1 = 6n + 4.The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 6n + 4The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 4n + 3The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 2n + 2 − 2n − 1 = 1.Hence, The greatest common divisor of 8n + 5 and 2n + 1 is 1 What is the greatest common divisor of 18n + 11 and 8n + 5?The greatest common divisor of 18n + 11 and 8n + 5 is the greatest common divisor of 8n + 5 and 18n + 11 − 8n − 5 = 10n + 6 = greatest common divisor of 8n + 5 and 10n + 6DaSLaMLeast to Most COT Figure3: A case study on LLAMA-13B.The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.What is the largest of the three consecutive integers?What is the product of three consecutive integers?The product of three consecutive integers is 120.What is the mean of the three integers?The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.So, 120/mean = 24 Mean = 120/24 = 5 What is the largest of the three consecutive integers?The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.So, 120/mean = 24 Mean = 120/24 = 5 So, the three consecutive integers are 4, 5, 6.The largest of the three consecutive integers is 6The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.The mean of the three integers is the sum of the three integers divided by 3. The sum of the three integers is 120.Therefore, the mean of the three integers is 40.The product of the three integers is 120, so the product of the three integers divided by the mean of the three integers is 3.This means that the largest of the three consecutive integers is 40.The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.The mean of the three integers is the sum of the three integers divided by 3. The sum of the three integers is 120.Therefore, the mean of the three integers is 40.The largest of the three consecutive integers is 40 + 24 = 64.DaSLaMComparing the coefficients of x on both sides, m = 2 sec θ Comparing the constant terms on both sides, c = 1 Therefore, the equation of the line x 2 − 2x sec θ + 1 = 0 is given by y = 2 sec θx + 1 Hence, the answer is y = 2 sec θx + 1.What is the equation of the line x 2 + 2x tan θ − 1 = 0? Given, − π We need to find the value of α 1 + β 2 Let us consider the equation x 2 − 2x sec θ + 1 = 0 α 1 and β 1 are the roots of the equation.Therefore, α 1 + β 1 = −2 sec θ Let us consider the equation x 2 + 2x tan θ − 1 = 0 α 2 and β 2 are the roots of the equation.Therefore,Hence, the answer is (A) 2(sec θ − tan θ).Hence, the answer is (A) 2(sec θ − tan θ).
Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>The carbon impact of artificial intelligence. Payal Dhar, Nature Machine Intelligence. 282020</p>
<p>Successive Prompting for Decomposing Complex Questions. Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner, Empirical Methods in Natural Language Processing (EMNLP). 2022</p>
<p>Language and thought are not the same thing: evidence from neuroimaging and neurological patients. Evelina Fedorenko, Rosemary A Varley, 2016Annals of the New York Academy of Sciences1369</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, 2023</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022aarXiv preprint</p>
<p>Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.14024Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. 2022barXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Dissociating language and thought in large language models: a cognitive perspective. 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Gpt-4 technical report. 2023. 202021OpenAI</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, arXiv:2212.001932022arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Stephen Wolfram, ChatGPT gets its "Wolfram superpowers"! (blog). 2023</p>
<p>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, arXiv:2301.138082023arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
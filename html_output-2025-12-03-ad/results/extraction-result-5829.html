<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5829 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5829</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5829</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-f3a332ff1b73acda482e5d83696b2c701f487819</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f3a332ff1b73acda482e5d83696b2c701f487819" target="_blank">P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The method P-Tuning v2 is an implementation of Deep Prompt Tuning optimized and adapted for NLU and can serve as an alternative to finetuning and a strong baseline for future research.</p>
                <p><strong>Paper Abstract:</strong> Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5829.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5829.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>P-tuning v2 (deep prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P-Tuning v2 (Deep Prompt Tuning applied across all layers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-tuning method that inserts trainable continuous prompt tokens into every Transformer layer (deep prompt/prefix tokens) and uses a frozen backbone with a task-specific classification head, achieving performance comparable to full fine-tuning while tuning only 0.1%–3% of parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large; RoBERTa-large; GLM_xlarge (2B); GLM_xxlarge (10B); DeBERTa_xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE classification tasks (examples: BoolQ, CB, RTE, ReCoRD, COPA, MultiRC, WiC, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard NLU classification tasks from SuperGLUE used to evaluate general natural language understanding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Continuous prompt tuning with deep prompts: trainable continuous embeddings (prefix tokens) are added to the input at every Transformer layer; backbone parameters frozen; a randomly initialized classification head is used in the full-data setting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Input-layer-only continuous prompt tuning (Lester et al. style PT) and full fine-tuning (FT).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Across many SuperGLUE tasks and model scales P-tuning v2 (PT-2) matches or nearly matches fine-tuning (FT) while input-layer-only prompt tuning (PT) often lags on small/medium models. Example: RoBERTa-large on BoolQ — FT: 86.9% accuracy; PT: 62.3%; PT-2: 84.8%. GLM_xxlarge (10B) on RTE — FT: 93.1; PT: 89.9; PT-2: 93.1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Example differences: RoBERTa-large BoolQ: PT -> PT-2 improves from 62.3% to 84.8% (PT-2 close to FT 86.9%); BERT-large BoolQ: FT 77.7, PT 67.2, PT-2 75.8.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large: PT-2 improved absolute accuracy by ~+22.5 percentage points over PT on RoBERTa-large BoolQ (62.3 -> 84.8); PT-2 is within ~1–3 points of FT in many cases on smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Adding prompts to every layer increases the capacity and gives the prompts a more direct impact on model predictions (deeper-layer prompts have stronger, more direct influence than input-layer-only prompts), reducing the gap between prompt tuning and fine-tuning especially on smaller models and on harder tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5829.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Input-layer prompt tuning (PT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Input-layer-only Continuous Prompt Tuning (as in Lester et al. 2021 / Liu et al. 2021 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that appends trainable continuous prompt embeddings only to the input embedding sequence while keeping the pretrained model frozen; shown to work well at very large scales but to underperform fine-tuning at smaller scales and on sequence-labeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The power of scale for parameter-efficient prompt tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large; RoBERTa-large; GLM_xlarge (2B); GLM_xxlarge (10B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SuperGLUE classification tasks and sequence labeling tasks (NER, Extractive QA, SRL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Various NLU tasks including simple classification and harder sequence-labeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Continuous prompts appended only to the input embedding sequence (single-layer prefix), frozen LM backbone, typically using language-modeling head with verbalizer in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to P-tuning v2 (deep prompts across layers) and full fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On SuperGLUE, PT often underperforms FT for models < ~10B; examples: RoBERTa-large BoolQ: PT 62.3 vs FT 86.9; on sequence labeling/extractive QA PT can nearly fail (very low EM/F1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>On SQuAD1.1 (extractive QA): BERT-large PT EM/F1 reported near 1.0 (i.e., effectively failed) whereas PT-2 achieved ~77.8 EM/F1 and FT ~84.2.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Very large negative effect on extractive QA in the reported experiments (e.g., PT EM ~1.0 vs PT-2 EM ~77.8 for BERT-large on SQuAD1.1).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>reduced (relative to deep prompts / fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Input-layer-only prompts have limited tunable capacity (sequence length constraint) and a more indirect influence on deeper model representations, which limits effectiveness particularly for smaller models and sequence-labeling tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>At very large scale models (around 10B), input-layer PT can be competitive with FT on some simple classification tasks (as noted in Lester et al. 2021 and confirmed partially in this paper for GLM 10B).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5829.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Extractive QA format sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-labeling (extractive QA) sensitivity to prompt presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that the presentation format (input-layer PT vs deep PT) massively affects performance on extractive QA; input-layer prompt tuning largely fails while deep prompt tuning recovers strong performance close to fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large; RoBERTa-large; DeBERTa_xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Extractive Question Answering (SQuAD 1.1 / 2.0, framed as sequence tagging)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract a contiguous answer span from context given a question; authors cast the problem as sequence tagging (start/end labels for tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt formats compared: input-layer-only continuous prompts (PT) vs deep prompts (PT-2) inserted at every layer; frozen backbone; classification head for token-level labels (sequence tagging).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>PT vs PT-2 vs FT; also multi-task PT-2 (MPT-2).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BERT-large on SQuAD1.1 dev: FT EM/F1 84.2; PT EM/F1 ~1.0 (fail); PT-2 EM/F1 ~77.8; MPT-2 EM/F1 ~86.0 (multi-task PT-2 matched or slightly exceeded FT). RoBERTa-large shows analogous patterns (PT nearly fails; PT-2 recovers high performance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Huge gap: PT to PT-2 improvement of ~+76.8 EM points for BERT-large SQuAD1.1 (1.0 -> 77.8); PT-2 approaches FT (77.8 vs 84.2) and multi-task PT-2 can surpass FT in some settings (MPT-2 ~86.0).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Very large (tens of absolute EM/F1 points); e.g., BERT-large PT -> PT-2 ~+76.8 EM on SQuAD1.1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (deep prompts dramatically improved over input-layer PT)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Sequence labeling tasks require token-level supervision and deeper influence on representations; input-layer prompts lack capacity and direct influence for token-level decisions, whereas deep prompts inserted into intermediate layers can modify hidden representations more effectively to support sequence tagging.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5829.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt length sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of continuous prompt length on performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt length (number of continuous prompt tokens per insertion) is an important hyperparameter: simple classification tasks tend to prefer short prompts (<20 tokens), while hard sequence-labeling tasks tend to prefer much longer prompts (around 100).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (ablation), results also discussed across tasks/models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various NLU tasks: RTE, BoolQ (classification); CoNLL04, CoNLL12 (sequence labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative simple classification and hard sequence-labeling tasks used to probe prompt length effects.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Varying number of continuous prompt tokens per prompt insertion (prompt lengths studied for embedding-based and MLP reparameterized prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Short prompts (<20) vs long prompts (~100) and interaction with reparameterization (MLP vs direct embedding).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: simple NLU tasks reach best performance with shorter prompts; sequence-labeling tasks benefit from longer prompts (around 100). Exact optimal lengths vary by dataset; see Figure 4 and Appendix B for curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (shorter preferred for classification, longer preferred for sequence-labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Longer prompts increase the tunable capacity and allow more expressive task-specific prefixes—important for complex token-level tasks—while simple tasks do not need large prompt capacity and may even degrade with unnecessarily long prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Optimal prompt length is dataset-dependent and not uniform; reparameterization interacts with optimal length (MLP sometimes achieves optimum at smaller lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5829.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reparameterization (MLP vs embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt reparameterization: learned MLP encoder vs directly learned embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an MLP to reparameterize prompt embeddings can improve training speed, robustness, and sometimes performance, but effects are inconsistent across datasets: helpful on some (RTE, CoNLL04), neutral on others (BoolQ), and harmful on some (CoNLL12).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large (explicit ablations reported); other models used in main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>RTE (classification), BoolQ (classification), CoNLL04 and CoNLL12 (sequence labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Representative classification and sequence-labeling datasets used in ablations of reparameterization choice.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt parameterization: direct trainable prompt embeddings vs an MLP (reparameterization encoder) that maps a smaller parameter vector to the prompt embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Embedding (direct) vs MLP reparameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dataset-dependent: MLP improved performance for RTE and CoNLL04 across prompt lengths; in BoolQ embedding and MLP performed competitively; in CoNLL12 embedding outperformed MLP consistently (see Figure 4 and Appendix B).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative differences across datasets (examples: RTE and CoNLL04: MLP > embedding; CoNLL12: embedding > MLP). No single uniform numerical effect reported across all tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>varied (improved for some tasks; reduced for others; neutral for some)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Reparameterization can provide inductive biases and smoother parameterization that aids optimization, but the benefit depends on the dataset and how the prompt interacts with the task; it also interacts with optimal prompt length (MLP tends to reach optimum at shorter prompt lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>On CoNLL12, MLP reparameterization consistently reduced performance relative to direct embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5829.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbalizer (LM-head) vs classification head</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using a language-modeling verbalizer head vs a randomly initialized classification head for prompt tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper finds that using the LM head to predict verbalizers (a common prompt-tuning design) is unnecessary in full-data settings and incompatible with sequence-labeling tasks; instead, a randomly initialized classification head on top of token representations is used for P-tuning v2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large; RoBERTa-large; general across models used in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Full-data classification and sequence-labeling tasks (e.g., SuperGLUE, NER, Extractive QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where outputs are class labels (classification) or token-level labels (sequence tagging).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Output head design: (1) LM head with discrete/continuous verbalizers (predicting tokens mapped to labels) vs (2) task-specific randomly initialized classification head mapping token hidden states to label logits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Verbalizer-based LM-head vs classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Authors report that in full-data experiments the LM verbalizer setup is unnecessary and does not work well for sequence labeling; P-tuning v2 uses classification heads to handle token-level prediction tasks successfully.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (classification head enabled sequence labeling; verbalizer was incompatible)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Verbalizer approaches map labels to surface tokens and rely on LM-head logits, which is awkward for sequence-labeling problems (token-wise labels) and can limit expressivity in full-data supervised settings; a direct classification head is simpler and more suitable.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5829.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5829.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-task shared prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-task Prompt Pretraining with shared continuous prompts (MPT-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Jointly optimizing shared continuous prompts across multiple related datasets/tasks before per-task tuning can provide better initialization and performance boosts for P-tuning v2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large; RoBERTa-large; DeBERTa-xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Sequence labeling tasks (e.g., NER datasets CoNLL03, OntoNotes 5.0, CoNLL04) and SQuAD QA multi-task pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-task setting: combine training sets of multiple datasets of the same task type and share continuous prompts while using separate linear classifiers per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Shared continuous prompts across tasks (multi-task pretraining) vs single-task prompt initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>PT-2 (single-task) vs MPT-2 (multi-task PT-2) vs FT</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Example: CoNLL03 NER with BERT-large — FT 92.8; PT (input-layer) 81.9; PT-2 90.2; MPT-2 91.0 (MPT-2 improved PT-2 by +0.8 and further closed gap to FT). RoBERTa-large CoNLL03 PT-2 92.8, MPT-2 92.8 (no change).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Multi-task pretraining sometimes improves PT-2 (e.g., BERT-large CoNLL03 +0.8 absolute F1) but gains are dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+0.8 absolute F1 for BERT-large CoNLL03 in reported results; other datasets show smaller or no changes.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (sometimes)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Multi-task training provides better shared initialization for the prompt parameters, improving generalization and optimization for related datasets, though benefits vary by dataset composition and similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>Some datasets showed little or no improvement from multi-task pretraining (e.g., RoBERTa-large CoNLL03 PT-2 vs MPT-2 showed no change).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The power of scale for parameter-efficient prompt tuning. <em>(Rating: 2)</em></li>
                <li>Prefixtuning: Optimizing continuous prompts for generation. <em>(Rating: 2)</em></li>
                <li>Gpt understands, too. <em>(Rating: 2)</em></li>
                <li>Learning how to ask: Querying lms with mixtures of soft prompts. <em>(Rating: 2)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners. <em>(Rating: 1)</em></li>
                <li>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5829",
    "paper_id": "paper-f3a332ff1b73acda482e5d83696b2c701f487819",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "P-tuning v2 (deep prompts)",
            "name_full": "P-Tuning v2 (Deep Prompt Tuning applied across all layers)",
            "brief_description": "A prompt-tuning method that inserts trainable continuous prompt tokens into every Transformer layer (deep prompt/prefix tokens) and uses a frozen backbone with a task-specific classification head, achieving performance comparable to full fine-tuning while tuning only 0.1%–3% of parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-large; RoBERTa-large; GLM_xlarge (2B); GLM_xxlarge (10B); DeBERTa_xlarge",
            "model_size": null,
            "task_name": "SuperGLUE classification tasks (examples: BoolQ, CB, RTE, ReCoRD, COPA, MultiRC, WiC, WSC)",
            "task_description": "Standard NLU classification tasks from SuperGLUE used to evaluate general natural language understanding performance.",
            "problem_format": "Continuous prompt tuning with deep prompts: trainable continuous embeddings (prefix tokens) are added to the input at every Transformer layer; backbone parameters frozen; a randomly initialized classification head is used in the full-data setting.",
            "comparison_format": "Input-layer-only continuous prompt tuning (Lester et al. style PT) and full fine-tuning (FT).",
            "performance": "Across many SuperGLUE tasks and model scales P-tuning v2 (PT-2) matches or nearly matches fine-tuning (FT) while input-layer-only prompt tuning (PT) often lags on small/medium models. Example: RoBERTa-large on BoolQ — FT: 86.9% accuracy; PT: 62.3%; PT-2: 84.8%. GLM_xxlarge (10B) on RTE — FT: 93.1; PT: 89.9; PT-2: 93.1.",
            "performance_comparison": "Example differences: RoBERTa-large BoolQ: PT -&gt; PT-2 improves from 62.3% to 84.8% (PT-2 close to FT 86.9%); BERT-large BoolQ: FT 77.7, PT 67.2, PT-2 75.8.",
            "format_effect_size": "Large: PT-2 improved absolute accuracy by ~+22.5 percentage points over PT on RoBERTa-large BoolQ (62.3 -&gt; 84.8); PT-2 is within ~1–3 points of FT in many cases on smaller models.",
            "format_effect_direction": "improved",
            "explanation_or_hypothesis": "Adding prompts to every layer increases the capacity and gives the prompts a more direct impact on model predictions (deeper-layer prompts have stronger, more direct influence than input-layer-only prompts), reducing the gap between prompt tuning and fine-tuning especially on smaller models and on harder tasks.",
            "counterexample_or_null_result": null,
            "uuid": "e5829.0",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Input-layer prompt tuning (PT)",
            "name_full": "Input-layer-only Continuous Prompt Tuning (as in Lester et al. 2021 / Liu et al. 2021 baseline)",
            "brief_description": "A method that appends trainable continuous prompt embeddings only to the input embedding sequence while keeping the pretrained model frozen; shown to work well at very large scales but to underperform fine-tuning at smaller scales and on sequence-labeling tasks.",
            "citation_title": "The power of scale for parameter-efficient prompt tuning.",
            "mention_or_use": "use",
            "model_name": "BERT-large; RoBERTa-large; GLM_xlarge (2B); GLM_xxlarge (10B)",
            "model_size": null,
            "task_name": "SuperGLUE classification tasks and sequence labeling tasks (NER, Extractive QA, SRL)",
            "task_description": "Various NLU tasks including simple classification and harder sequence-labeling tasks.",
            "problem_format": "Continuous prompts appended only to the input embedding sequence (single-layer prefix), frozen LM backbone, typically using language-modeling head with verbalizer in prior work.",
            "comparison_format": "Compared to P-tuning v2 (deep prompts across layers) and full fine-tuning.",
            "performance": "On SuperGLUE, PT often underperforms FT for models &lt; ~10B; examples: RoBERTa-large BoolQ: PT 62.3 vs FT 86.9; on sequence labeling/extractive QA PT can nearly fail (very low EM/F1).",
            "performance_comparison": "On SQuAD1.1 (extractive QA): BERT-large PT EM/F1 reported near 1.0 (i.e., effectively failed) whereas PT-2 achieved ~77.8 EM/F1 and FT ~84.2.",
            "format_effect_size": "Very large negative effect on extractive QA in the reported experiments (e.g., PT EM ~1.0 vs PT-2 EM ~77.8 for BERT-large on SQuAD1.1).",
            "format_effect_direction": "reduced (relative to deep prompts / fine-tuning)",
            "explanation_or_hypothesis": "Input-layer-only prompts have limited tunable capacity (sequence length constraint) and a more indirect influence on deeper model representations, which limits effectiveness particularly for smaller models and sequence-labeling tasks.",
            "counterexample_or_null_result": "At very large scale models (around 10B), input-layer PT can be competitive with FT on some simple classification tasks (as noted in Lester et al. 2021 and confirmed partially in this paper for GLM 10B).",
            "uuid": "e5829.1",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Extractive QA format sensitivity",
            "name_full": "Sequence-labeling (extractive QA) sensitivity to prompt presentation",
            "brief_description": "The paper shows that the presentation format (input-layer PT vs deep PT) massively affects performance on extractive QA; input-layer prompt tuning largely fails while deep prompt tuning recovers strong performance close to fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-large; RoBERTa-large; DeBERTa_xlarge",
            "model_size": null,
            "task_name": "Extractive Question Answering (SQuAD 1.1 / 2.0, framed as sequence tagging)",
            "task_description": "Extract a contiguous answer span from context given a question; authors cast the problem as sequence tagging (start/end labels for tokens).",
            "problem_format": "Prompt formats compared: input-layer-only continuous prompts (PT) vs deep prompts (PT-2) inserted at every layer; frozen backbone; classification head for token-level labels (sequence tagging).",
            "comparison_format": "PT vs PT-2 vs FT; also multi-task PT-2 (MPT-2).",
            "performance": "BERT-large on SQuAD1.1 dev: FT EM/F1 84.2; PT EM/F1 ~1.0 (fail); PT-2 EM/F1 ~77.8; MPT-2 EM/F1 ~86.0 (multi-task PT-2 matched or slightly exceeded FT). RoBERTa-large shows analogous patterns (PT nearly fails; PT-2 recovers high performance).",
            "performance_comparison": "Huge gap: PT to PT-2 improvement of ~+76.8 EM points for BERT-large SQuAD1.1 (1.0 -&gt; 77.8); PT-2 approaches FT (77.8 vs 84.2) and multi-task PT-2 can surpass FT in some settings (MPT-2 ~86.0).",
            "format_effect_size": "Very large (tens of absolute EM/F1 points); e.g., BERT-large PT -&gt; PT-2 ~+76.8 EM on SQuAD1.1.",
            "format_effect_direction": "improved (deep prompts dramatically improved over input-layer PT)",
            "explanation_or_hypothesis": "Sequence labeling tasks require token-level supervision and deeper influence on representations; input-layer prompts lack capacity and direct influence for token-level decisions, whereas deep prompts inserted into intermediate layers can modify hidden representations more effectively to support sequence tagging.",
            "counterexample_or_null_result": null,
            "uuid": "e5829.2",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Prompt length sensitivity",
            "name_full": "Effect of continuous prompt length on performance",
            "brief_description": "Prompt length (number of continuous prompt tokens per insertion) is an important hyperparameter: simple classification tasks tend to prefer short prompts (&lt;20 tokens), while hard sequence-labeling tasks tend to prefer much longer prompts (around 100).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (ablation), results also discussed across tasks/models",
            "model_size": "355M",
            "task_name": "Various NLU tasks: RTE, BoolQ (classification); CoNLL04, CoNLL12 (sequence labeling)",
            "task_description": "Representative simple classification and hard sequence-labeling tasks used to probe prompt length effects.",
            "problem_format": "Varying number of continuous prompt tokens per prompt insertion (prompt lengths studied for embedding-based and MLP reparameterized prompts).",
            "comparison_format": "Short prompts (&lt;20) vs long prompts (~100) and interaction with reparameterization (MLP vs direct embedding).",
            "performance": "Qualitative: simple NLU tasks reach best performance with shorter prompts; sequence-labeling tasks benefit from longer prompts (around 100). Exact optimal lengths vary by dataset; see Figure 4 and Appendix B for curves.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "varied (shorter preferred for classification, longer preferred for sequence-labeling)",
            "explanation_or_hypothesis": "Longer prompts increase the tunable capacity and allow more expressive task-specific prefixes—important for complex token-level tasks—while simple tasks do not need large prompt capacity and may even degrade with unnecessarily long prompts.",
            "counterexample_or_null_result": "Optimal prompt length is dataset-dependent and not uniform; reparameterization interacts with optimal length (MLP sometimes achieves optimum at smaller lengths).",
            "uuid": "e5829.3",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Reparameterization (MLP vs embedding)",
            "name_full": "Prompt reparameterization: learned MLP encoder vs directly learned embeddings",
            "brief_description": "Using an MLP to reparameterize prompt embeddings can improve training speed, robustness, and sometimes performance, but effects are inconsistent across datasets: helpful on some (RTE, CoNLL04), neutral on others (BoolQ), and harmful on some (CoNLL12).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large (explicit ablations reported); other models used in main experiments",
            "model_size": "355M",
            "task_name": "RTE (classification), BoolQ (classification), CoNLL04 and CoNLL12 (sequence labeling)",
            "task_description": "Representative classification and sequence-labeling datasets used in ablations of reparameterization choice.",
            "problem_format": "Prompt parameterization: direct trainable prompt embeddings vs an MLP (reparameterization encoder) that maps a smaller parameter vector to the prompt embeddings.",
            "comparison_format": "Embedding (direct) vs MLP reparameterization.",
            "performance": "Dataset-dependent: MLP improved performance for RTE and CoNLL04 across prompt lengths; in BoolQ embedding and MLP performed competitively; in CoNLL12 embedding outperformed MLP consistently (see Figure 4 and Appendix B).",
            "performance_comparison": "Qualitative differences across datasets (examples: RTE and CoNLL04: MLP &gt; embedding; CoNLL12: embedding &gt; MLP). No single uniform numerical effect reported across all tasks.",
            "format_effect_size": null,
            "format_effect_direction": "varied (improved for some tasks; reduced for others; neutral for some)",
            "explanation_or_hypothesis": "Reparameterization can provide inductive biases and smoother parameterization that aids optimization, but the benefit depends on the dataset and how the prompt interacts with the task; it also interacts with optimal prompt length (MLP tends to reach optimum at shorter prompt lengths).",
            "counterexample_or_null_result": "On CoNLL12, MLP reparameterization consistently reduced performance relative to direct embeddings.",
            "uuid": "e5829.4",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Verbalizer (LM-head) vs classification head",
            "name_full": "Using a language-modeling verbalizer head vs a randomly initialized classification head for prompt tuning",
            "brief_description": "The paper finds that using the LM head to predict verbalizers (a common prompt-tuning design) is unnecessary in full-data settings and incompatible with sequence-labeling tasks; instead, a randomly initialized classification head on top of token representations is used for P-tuning v2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-large; RoBERTa-large; general across models used in this paper",
            "model_size": null,
            "task_name": "Full-data classification and sequence-labeling tasks (e.g., SuperGLUE, NER, Extractive QA)",
            "task_description": "Tasks where outputs are class labels (classification) or token-level labels (sequence tagging).",
            "problem_format": "Output head design: (1) LM head with discrete/continuous verbalizers (predicting tokens mapped to labels) vs (2) task-specific randomly initialized classification head mapping token hidden states to label logits.",
            "comparison_format": "Verbalizer-based LM-head vs classification head.",
            "performance": "Authors report that in full-data experiments the LM verbalizer setup is unnecessary and does not work well for sequence labeling; P-tuning v2 uses classification heads to handle token-level prediction tasks successfully.",
            "performance_comparison": null,
            "format_effect_size": null,
            "format_effect_direction": "improved (classification head enabled sequence labeling; verbalizer was incompatible)",
            "explanation_or_hypothesis": "Verbalizer approaches map labels to surface tokens and rely on LM-head logits, which is awkward for sequence-labeling problems (token-wise labels) and can limit expressivity in full-data supervised settings; a direct classification head is simpler and more suitable.",
            "counterexample_or_null_result": null,
            "uuid": "e5829.5",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Multi-task shared prompts",
            "name_full": "Multi-task Prompt Pretraining with shared continuous prompts (MPT-2)",
            "brief_description": "Jointly optimizing shared continuous prompts across multiple related datasets/tasks before per-task tuning can provide better initialization and performance boosts for P-tuning v2.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-large; RoBERTa-large; DeBERTa-xlarge",
            "model_size": null,
            "task_name": "Sequence labeling tasks (e.g., NER datasets CoNLL03, OntoNotes 5.0, CoNLL04) and SQuAD QA multi-task pretraining",
            "task_description": "Multi-task setting: combine training sets of multiple datasets of the same task type and share continuous prompts while using separate linear classifiers per dataset.",
            "problem_format": "Shared continuous prompts across tasks (multi-task pretraining) vs single-task prompt initialization.",
            "comparison_format": "PT-2 (single-task) vs MPT-2 (multi-task PT-2) vs FT",
            "performance": "Example: CoNLL03 NER with BERT-large — FT 92.8; PT (input-layer) 81.9; PT-2 90.2; MPT-2 91.0 (MPT-2 improved PT-2 by +0.8 and further closed gap to FT). RoBERTa-large CoNLL03 PT-2 92.8, MPT-2 92.8 (no change).",
            "performance_comparison": "Multi-task pretraining sometimes improves PT-2 (e.g., BERT-large CoNLL03 +0.8 absolute F1) but gains are dataset-dependent.",
            "format_effect_size": "+0.8 absolute F1 for BERT-large CoNLL03 in reported results; other datasets show smaller or no changes.",
            "format_effect_direction": "improved (sometimes)",
            "explanation_or_hypothesis": "Multi-task training provides better shared initialization for the prompt parameters, improving generalization and optimization for related datasets, though benefits vary by dataset composition and similarity.",
            "counterexample_or_null_result": "Some datasets showed little or no improvement from multi-task pretraining (e.g., RoBERTa-large CoNLL03 PT-2 vs MPT-2 showed no change).",
            "uuid": "e5829.6",
            "source_info": {
                "paper_title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning.",
            "rating": 2
        },
        {
            "paper_title": "Prefixtuning: Optimizing continuous prompts for generation.",
            "rating": 2
        },
        {
            "paper_title": "Gpt understands, too.",
            "rating": 2
        },
        {
            "paper_title": "Learning how to ask: Querying lms with mixtures of soft prompts.",
            "rating": 2
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners.",
            "rating": 1
        },
        {
            "paper_title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts.",
            "rating": 1
        }
    ],
    "cost": 0.01711225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks</h1>
<p>Xiao Liu ${ }^{1,2 <em>}$, Kaixuan Ji ${ }^{1 </em>}$, Yicheng Fu ${ }^{1 *}$, Weng Lam Tam ${ }^{1}$, Zhengxiao Du ${ }^{1,2}$, Zhilin Yang ${ }^{1,3 \dagger}$, Jie Tang ${ }^{1,2 \dagger}$<br>${ }^{1}$ Tsinghua University, KEG ${ }^{2}$ Beijing Academy of Artificial Intelligence (BAAI)<br>${ }^{3}$ Shanghai Qi Zhi Institute<br>{liuxiao21,jkx19,fyc19}@mails.tsinghua.edu.cn</p>
<h4>Abstract</h4>
<p>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only $0.1 \%-3 \%$ tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (Li and Liang, 2021; Qin and Eisner, 2021) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Pretrained language models (Radford et al., 2019; Devlin et al., 2018; Yang et al., 2019; Raffel et al., 2019) improve performance on a wide range of natural language understanding (NLU) tasks. A widely-used method, fine-tuning, updates the entire set of model parameters for a target task. While fine-tuning obtains good performance, it is memory-consuming during training because gradients and optimizer states for all parameters must be stored. Moreover, keeping a copy of model parameters for each task during inference is inconvenient since pre-trained models are usually large.</p>
<p>Prompting, on the other hand, freezes all parameters of a pre-trained model and uses a natural lan-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Average scores on RTE, BoolQ and CB of SuperGLUE dev. With $0.1 \%$ task-specific parameters, P-tuning v2 can match fine-tuning across wide scales of pre-trained models, while Lester et al. (2021) \&amp; Ptuning can make it conditionally at 10B scale.
guage prompt to query a language model (Brown et al., 2020). For example, for sentiment analysis, we can concatenate a sample (e.g., "Amazing movie!") with a prompt "This movie is [MASK]" and ask the pre-trained language model to predict the probabilities of masked token being "good" and "bad" to decide the sample's label. Prompting requires no training at all and stores one single copy of model parameters. However, discrete prompting (Shin et al., 2020; Gao et al., 2020) can lead to suboptimal performance in many cases compared to fine-tuning.</p>
<p>Prompt tuning ${ }^{2}$ is an idea of tuning only the continuous prompts. Specifically, Liu et al. (2021); Lester et al. (2021) proposed to add trainable continuous embeddings (also called continuous prompts) to the original sequence of input word embeddings. Only the continuous prompts are updated during training. While prompt tuning improves over prompting on many tasks (Liu et al., 2021; Lester et al., 2021; Zhong et al., 2021), it still underperforms fine-tuning when the model size is not large, specifically less than 10 billion parameters (Lester et al., 2021). Moreover, as shown in our experiments, prompt tuning performs poorly compared to fine-tuning on several hard sequence labeling tasks such as extractive question answer-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>ing (Cf. Section 4.2).
Our main contribution in this paper is a novel empirical finding that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks. In contrast to observations in prior work, our discovery reveals the universality and potential of prompt tuning for NLU.
Technically, our approach P-tuning v2 is not conceptually novel. It can be viewed as an optimized and adapted implementation of Deep Prompt Tuning (Li and Liang, 2021; Qin and Eisner, 2021) designed for generation and knowledge probing. The most significant improvement originates from appling continuous prompts for every layer of the pretrained model, instead of the mere input layer. Deep prompt tuning increases the capacity of continuous prompts and closes the gap to fine-tuning across various settings, especially for small models and hard tasks. Moreover, we present a series of critical details of optimization and implementation to ensure finetuning-comparable performance.
Experimental results show that P-tuning v2 matches the performance of fine-tuning at different model scales ranging from 300M to 10B parameters and on various hard sequence tagging tasks such as extractive question answering and named entity recognition. P-tuning v2 has 0.1\% to 3\% trainable parameters per task compared to fine-tuning, which substantially reduces training time memory cost and per-task storage cost.</p>
<h2>2 Preliminaries</h2>
<p>NLU Tasks. In this work, we categorize NLU challenges into two families: simple classification tasks and hard sequence labeling tasks. ${ }^{3}$ Simple classification tasks involve classification over a label space. Most datasets from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) are in this category. Hard sequence labeling tasks involve classification over a sequence of tokens, such as named entity recognition and extractive question answering.</p>
<p>Prompt Tuning. Let $\mathcal{V}$ be the vocabulary of a language model $\mathcal{M}$ and let $\mathbf{e}$ be the embedding layer of $\mathcal{M}$. In the case of discrete prompting (Schick and Schütze, 2020), prompt tokens {"It", "is", "[MASK]"} $\subset \mathcal{V}$ can be</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>used to classify a movie review. For example, given the input text $\mathbf{x}=$ "Amazing movie!", the input embedding sequence is formulated as $[\mathbf{e}(\mathbf{x}), \mathbf{e}(" I t "), \mathbf{e}(" i s "), \mathbf{e}("[M A S K]")]$.</p>
<p>Lester et al. (2021) and Liu et al. (2021) introduce trainable continuous prompts as a substitution to natural language prompts for NLU with the parameters of pretrained language models frozen. Given the trainable continuous embeddings $\left[h_{0}, \ldots, h_{i}\right]$, the input embedding sequence is written as $\left[\mathbf{e}(\mathbf{x}), h_{0}, \ldots, h_{i}, \mathbf{e}("[\mathrm{MASK}]")\right]$, as illustrated in Figure 2. Prompt tuning has been proved to be comparable to fine-tuning on 10-billion-parameter models on simple classification tasks (Lester et al., 2021; Kim et al., 2021; Liu et al., 2021).</p>
<h2>3 P-Tuning v2</h2>
<h3>3.1 Lack of Universality</h3>
<p>Lester et al. (2021); Liu et al. (2021) have been proved quite effective in many NLP applications (Wang et al., 2021a,b; Chen et al., 2021; Zheng et al., 2021; Min et al., 2021), but still fall short at replacing fine-tuning due to lack of universality, as discussed below.</p>
<p>Lack of universality across scales. Lester et al. (2021) shows that prompt tuning can be comparable to fine-tuning when the model scales to over 10 billion parameters. However, for medium-sized models (from 100M to 1B) that are widely used, prompt tuning performs much worse than fine-tuning.</p>
<p>Lack of universality across tasks. Though Lester et al. (2021); Liu et al. (2021) have shown superiority on some of the NLU benchmarks, the effectiveness of prompt tuning on hard sequence tagging tasks is not verified. Sequence tagging predicts a sequence of labels for each input token, which can be harder and incompatible with verbalizers (Schick and Schütze, 2020). In our experiments (Cf. Section 4.2 and Table 3), we show that Lester et al. (2021); Liu et al. (2021) perform poorly on typical sequence tagging tasks compared to fine-tuning.</p>
<p>Considering these challenges, we propose Ptuning v2, which adapts deep prompt tuning ( Li and Liang, 2021; Qin and Eisner, 2021) as a universal solution across scales and NLU tasks.</p>
<h3>3.2 Deep Prompt Tuning</h3>
<p>In (Lester et al., 2021) and (Liu et al., 2021), continuous prompts are only inserted into the input</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: From Lester et al. (2021) &amp; P-tuning to P-tuning v2. Orange blocks (i.e., <em>h₀, ..., hᵢ</em>) refer to trainable prompt embeddings; blue blocks are embeddings stored or computed by frozen pre-trained language models.</p>
<p>embedding sequence (Cf. Figure 2 (a)). This leads to two challenges. First, the number of tunable parameters is limited due to the constraints of sequence length. Second, the input embeddings have relatively indirect impact on model predictions.</p>
<p>To address these challenges, P-tuning v2 employs the idea of deep prompt tuning (Li and Liang, 2021; Qin and Eisner, 2021). As illustrated in Figure 2, prompts in different layers are added as prefix tokens. On one hand, P-tuning v2 have more tunable task-specific parameters (from 0.01% to 0.1%-3%) to allow more per-task capacity while being parameter-efficient; on the other hand, prompts added to deeper layers have more direct impact on model predictions (see analysis in Appendix B).</p>
<h3>3.3 Optimization and Implementation</h3>
<p>There are a few useful details of optimization and implementation for achieving the best performance.</p>
<p><strong>Reparameterization.</strong> Prior works usually leverage a reparameterization encoder such as an MLP (Li and Liang, 2021; Liu et al., 2021) to transform trainable embeddings. However, for NLU, we discover that its usefulness depends on tasks and datasets. For some datasets (e.g., RTE and CoNLL04), MLP brings a consistent improvement; for the others, MLP leads to minimal or even negative effects on the results (e.g., BoolQ and CoNLL12). See Appendix B for more analysis.</p>
<p><strong>Prompt Length.</strong> The prompt length plays a critical role in P-Tuning v2. We find that different NLU tasks usually achieve their best performance with different prompt lengths (Cf. Appendix B). Generally, simple classification tasks prefer shorter prompts (less than 20); hard sequence labeling tasks prefer longer ones (around 100).</p>
<p><strong>Multi-task Learning.</strong> Multi-task learning jointly optimizes multiple tasks with shared continuous prompts before fine-tuning for individual tasks.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Task</th>
<th>Re-param.</th>
<th>Deep PT</th>
<th>Multi-task</th>
<th>No verb.</th>
</tr>
</thead>
<tbody>
<tr>
<td>P-tuning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Liu et al., 2021)</td>
<td>KP</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>NLU</td>
<td>LSTM</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>PromptTuning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Lester et al., 2021)</td>
<td>NLU</td>
<td>-</td>
<td>-</td>
<td>✓</td>
<td>-</td>
</tr>
<tr>
<td>Prefix Tuning</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Li and Liang, 2021)</td>
<td>NLG</td>
<td>MLP</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Soft Prompts</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Qin and Eisner, 2021)</td>
<td>KP</td>
<td>-</td>
<td>✓</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>P-tuning v2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(Ours)</td>
<td>NLU</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SeqTag</td>
<td>(depends)</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1: Conceptual comparison between P-tuning v2 and existing Prompt Tuning approaches (KP: Knowledge Probe; SeqTag: Sequence Tagging; Re-param.: Reparameterization; No verb.: No verbalizer).</p>
<p>Multi-task is optional for P-Tuning v2 but can be used for further boost performance by providing a better initialization (Gu et al., 2021).</p>
<p><strong>Classification Head.</strong> Using a language modeling head to predict verbalizers (Schick and Schütze, 2020) has been central for prompt tuning (Liu et al., 2021), but we find it unnecessary in a full-data setting and incompatible with sequence labeling. P-tuning v2 instead applies a randomly-initialized classification head on top of the tokens as in BERT (Devlin et al., 2018) (Cf. Figure 2).</p>
<p>To clarify P-tuning v2's major contribution, we present a conceptual comparison to existing prompt tuning approaches in Table 1.</p>
<h3>4 Experiments</h3>
<p>We conduct extensive experiments over different commonly-used pre-trained models and NLU tasks to verify the effectiveness of P-tuning v2. In this work, all methods except for fine-tuning are conducted with <strong>frozen language model backbones</strong>, which accords with Lester et al., 2021's setting but differs from Liu et al., 2021's tuned setting.</p>
<table>
<thead>
<tr>
<th></th>
<th>#Size</th>
<th>BoolQ</th>
<th></th>
<th></th>
<th>CB</th>
<th></th>
<th></th>
<th>COPA</th>
<th></th>
<th></th>
<th>MultiRC (F1a)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
</tr>
<tr>
<td>BERT ${ }_{\text {large }}$</td>
<td>335M</td>
<td>77.7</td>
<td>67.2</td>
<td>75.8</td>
<td>94.6</td>
<td>80.4</td>
<td>94.6</td>
<td>69.0</td>
<td>55.0</td>
<td>73.0</td>
<td>70.5</td>
<td>59.6</td>
<td>70.6</td>
</tr>
<tr>
<td>RoBERTa ${ }_{\text {large }}$</td>
<td>355M</td>
<td>86.9</td>
<td>62.3</td>
<td>84.8</td>
<td>98.2</td>
<td>71.4</td>
<td>100</td>
<td>94.0</td>
<td>63.0</td>
<td>93.0</td>
<td>85.7</td>
<td>59.9</td>
<td>82.5</td>
</tr>
<tr>
<td>GLM ${ }_{\text {xlarge }}$</td>
<td>2B</td>
<td>88.3</td>
<td>79.7</td>
<td>87.0</td>
<td>96.4</td>
<td>76.4</td>
<td>96.4</td>
<td>93.0</td>
<td>92.0</td>
<td>91.0</td>
<td>84.1</td>
<td>77.5</td>
<td>84.4</td>
</tr>
<tr>
<td>GLM ${ }_{\text {xxlarge }}$</td>
<td>10B</td>
<td>88.7</td>
<td>88.8</td>
<td>88.8</td>
<td>98.7</td>
<td>98.2</td>
<td>96.4</td>
<td>98.0</td>
<td>98.0</td>
<td>98.0</td>
<td>88.1</td>
<td>86.1</td>
<td>88.1</td>
</tr>
<tr>
<td></td>
<td>#Size</td>
<td>ReCoRD (F1)</td>
<td></td>
<td></td>
<td>RTE</td>
<td></td>
<td></td>
<td>WiC</td>
<td></td>
<td></td>
<td>WSC</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
<td>FT</td>
<td>PT</td>
<td>PT-2</td>
</tr>
<tr>
<td>BERT ${ }_{\text {large }}$</td>
<td>335M</td>
<td>70.6</td>
<td>44.2</td>
<td>72.8</td>
<td>70.4</td>
<td>53.5</td>
<td>78.3</td>
<td>74.9</td>
<td>63.0</td>
<td>75.1</td>
<td>68.3</td>
<td>64.4</td>
<td>68.3</td>
</tr>
<tr>
<td>RoBERTa ${ }_{\text {large }}$</td>
<td>355M</td>
<td>89.0</td>
<td>46.3</td>
<td>89.3</td>
<td>86.6</td>
<td>58.8</td>
<td>89.5</td>
<td>75.6</td>
<td>56.9</td>
<td>73.4</td>
<td>63.5</td>
<td>64.4</td>
<td>63.5</td>
</tr>
<tr>
<td>GLM ${ }_{\text {xlarge }}$</td>
<td>2B</td>
<td>91.8</td>
<td>82.7</td>
<td>91.9</td>
<td>90.3</td>
<td>85.6</td>
<td>90.3</td>
<td>74.1</td>
<td>71.0</td>
<td>72.0</td>
<td>95.2</td>
<td>87.5</td>
<td>92.3</td>
</tr>
<tr>
<td>GLM ${ }_{\text {xxlarge }}$</td>
<td>10B</td>
<td>94.4</td>
<td>87.8</td>
<td>92.5</td>
<td>93.1</td>
<td>89.9</td>
<td>93.1</td>
<td>75.7</td>
<td>71.8</td>
<td>74.0</td>
<td>95.2</td>
<td>94.2</td>
<td>93.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on SuperGLUE development set. P-tuning v2 surpasses P-tuning \&amp; Lester et al. (2021) on models smaller than 10B, matching the performance of fine-tuning across different model scales. (FT: fine-tuning; PT: Lester et al. (2021) \&amp; P-tuning; PT-2: P-tuning v2; bold: the best; underline: the second best).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">#Size</th>
<th style="text-align: center;">CoNLL03</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">OntoNotes 5.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CoNLL04</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT ${ }_{\text {large }}$</td>
<td style="text-align: center;">335M</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa ${ }_{\text {large }}$</td>
<td style="text-align: center;">355M</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;">89.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DeBERTa ${ }_{\text {xlarge }}$</td>
<td style="text-align: center;">750M</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">90.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Size</td>
<td style="text-align: center;">SQuAD 1.1 dev (EM / F1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SQuAD 2.0 dev (EM / F1)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT ${ }_{\text {large }}$</td>
<td style="text-align: center;">335M</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa ${ }_{\text {large }}$</td>
<td style="text-align: center;">355M</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">83.4</td>
</tr>
<tr>
<td style="text-align: center;">DeBERTa ${ }_{\text {xlarge }}$</td>
<td style="text-align: center;">750M</td>
<td style="text-align: center;">90.1</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">50.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">#Size</td>
<td style="text-align: center;">CoNLL12</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoNLL05 WSJ</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CoNLL05 Brown</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">PT</td>
<td style="text-align: center;">PT-2</td>
<td style="text-align: center;">MPT-2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERT ${ }_{\text {large }}$</td>
<td style="text-align: center;">335M</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">64.5.</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa ${ }_{\text {large }}$</td>
<td style="text-align: center;">355M</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DeBERTa ${ }_{\text {xlarge }}$</td>
<td style="text-align: center;">750M</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3: Results on Named Entity Recognition (NER), Question Answering (Extractive QA), and Semantic Role Labeling (SRL). All metrics in NER and SRL are micro-f1 score. (FT: fine-tuning; PT: P-tuning \&amp; Lester et al. (2021); PT-2: P-tuning v2; MPT-2: Multi-task P-tuning v2; bold: the best; underline: the second best).</p>
<p>Ratios of task-specific parameters (e.g., $0.1 \%$ ) are derived from comparing continuous prompts' parameters with transformers' parameters. Another thing to notice is that our experiments are all conducted in the fully-supervised setting rather than few-shot setting.</p>
<p>NLU Tasks. First, we include datasets from SuperGLUE (Wang et al., 2019) to test P-tuning v2's general NLU ability. Additionally, we introduce a suite of sequence labeling tasks, including named entity recognition (Sang and De Meulder, 2003; Weischedel et al., 2013; Carreras and Màrquez, 2004), extractive Question Answering (Rajpurkar et al., 2016), and semantic role labeling (Carreras
and Màrquez, 2005; Pradhan et al., 2012)).
Pre-trained Models. We include BERT-large (Devlin et al., 2018), RoBERTa-large (Liu et al., 2019), DeBERTa-xlarge (He et al., 2020), GLMxlarge/xxlarge (Du et al., 2021) for evaluation. They are all bidirectional models designed for NLU tasks, covering a wide range of sizes from about 300 M to 10B.</p>
<p>Multitask Learning. For the multi-task setting, we combine the training sets of the datasets in each task type (e.g., combing all training sets of semantic role labeling). We use separate linear classifiers for each dataset while sharing the continuous prompts (Cf. Appendix A).</p>
<table>
<thead>
<tr>
<th></th>
<th>SST-2</th>
<th>RTE</th>
<th>BoolQ</th>
<th>CB</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2>References</h2>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Xavier Carreras and Lluís Márquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HLT-NAACL 2004, pages 89-97, Boston, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Xavier Carreras and Lluís Márquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 152-164, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen. 2021. Adaprompt: Adaptive promptbased finetuning for relation extraction. arXiv preprint arXiv:2104.07650.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv e-prints.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. All nlp tasks are generation tasks: A general pretraining framework. arXiv preprint arXiv:2103.10360.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723.</p>
<p>Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. 2021. Ppt: Pre-trained prompt tuning for few-shot learning. arXiv preprint arXiv:2109.04332.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention.</p>
<p>Boseop Kim, HyoungSeok Kim, Sang-Woo Lee, Gichang Lee, Donghyun Kwak, Dong Hyeon Jeon, Sunghyun Park, Sungju Kim, Seonhoon Kim, Dongpil Seo, et al. 2021. What changes can large-scale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers. arXiv preprint arXiv:2109.04650.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefixtuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.</p>
<p>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. arXiv:2103.10385.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv e-prints.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2021. Noisy channel language model prompting for few-shot text classification. arXiv preprint arXiv:2108.04106.</p>
<p>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL - Shared Task, pages 1-40, Jeju Island, Korea. Association for Computational Linguistics.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text.</p>
<p>Erik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Languageindependent named entity recognition. arXiv preprint cs/0306050.</p>
<p>Timo Schick and Hinrich Schütze. 2020. It's not just size that matters: Small language models are also few-shot learners. arXiv preprint arXiv:2009.07118.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. SuperGLUE:</p>
<p>A Stickier Benchmark for General-Purpose Language Understanding Systems. In NeurIPS 2019, pages $3261-3275$.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv e-prints.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv e-prints.</p>
<p>Hongru Wang, Mingyu Cui, Zimo Zhou, Gabriel Pui Cheong Fung, and Kam-Fai Wong. 2021a. Topicrefine: Joint topic prediction and dialogue response generation for multi-turn end-to-end dialogue system. arXiv preprint arXiv:2109.05187.</p>
<p>Shuo Wang, Zhaopeng Tu, Zhixing Tan, Wenxuan Wang, Maosong Sun, and Yang Liu. 2021b. Language models are good translators. arXiv preprint arXiv:2106.13627.</p>
<p>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, et al. 2013. Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia, PA, 23.</p>
<p>Lu Xu, Zhanming Jie, Wei Lu, and Lidong Bing. 2021. Better feature integration for named entity recognition. arXiv preprint arXiv:2104.05316.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237.</p>
<p>Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, and Zhilin Yang. 2021. Fewnlu: Benchmarking state-of-the-art methods for few-shot natural language understanding. arXiv preprint arXiv:2109.12742.</p>
<p>Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]: Learning vs. learning to recall. arXiv preprint arXiv:2104.05240.</p>
<h2>A Problem Formulation on Sequence Tagging</h2>
<p>Name entity recognition (NER). NER aims to predict all spans of words that represent some given classes of entity with a sentence. We adopted CoNLL03 (Sang and De Meulder, 2003), OntoNotes 5.0 (Weischedel et al., 2013) and CoNLL04 (Carreras and Màrquez, 2004). For CoNLL03 and CoNLL04, we trained our model on the standard train-develop-test split. For OntoNotes 5.0, we use the same train, develop, test split as ( Xu et al., 2021). All the datasets are labeled in IOB2 format. We use sequence tagging to solve NER tasks by assigning labels marking the beginning and inside some classes of entity. The language models generate a representation for each token, and we use a linear classifier to predict the labels. We use the official scripts to evaluate the results. For the multi-task setting, we combine the training set of the three datasets for pre-training. We use different linear classifiers for each dataset while sharing the continuous prompts.
(Extractive) Question Answering (QA). Extractive QA is designed to extract the answer from the context given the context and a question. We use SQuAD (Rajpurkar et al., 2016) 1.1 and 2.0, in which each answer is within a continuous span of the context. Following tradition, we formulate the problem as sequence tagging by assigning one of the two labels: 'start' or 'end' to each token and at last selecting the span of the most confident startend pair as the extracted answer. If the probability of the most confident pair is lower than a threshold, the model will assume the question unanswerable. For the multi-task setting, our training set for pretraining combines the training sets of SQuAD 1.1 and 2.0. When pre-training, we assume that all the questions, regardless of their origin, are possibly unanswerable.</p>
<p>Semantic Role Labeling (SRL). SRL assigns labels to words or phrases in a sentence that indicate their semantic roles in the sentence. We evaluate P-tuning v2 on CoNLL05 (Carreras and Màrquez, 2005) and CoNLL12 (Pradhan et al., 2012). Since a sentence can have multiple verbs, we add the target verb token to the end of each sentence to help recognize which verb is used for prediction. We classify each word with a linear classifier based on the corresponding semantic role representation. For multitask setting, the pre-train training set is a combina-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Ablation study on prompt length and reparamerization using RoBERTa-large. The conclusion can be very different given certain NLU task and dataset. (MQA: Multiple-choice QA)</p>
<p>The results of the training set of CoNLL05 (Carreras and Màrquez, 2005), CoNLL12 (Pradhan et al., 2012) and propbank-release (a common extend data used for training SRL). The multi-task training strategy is similar to NER.</p>
<h3>B More Ablation Study</h3>
<p>Due to the page limit, we present hyper-parameters and architecture designs ablations regarding reparameterization and prompt length in this section.</p>
<p><strong>Embedding v.s. MLP reparameterization.</strong> In both prefix-tuning (Li and Liang, 2021) and P-tuning (Liu et al., 2021), authors discover the reparameterization to be useful in improving training speed, robustness, and performance. However, we conduct experiments to show that the reparameterization effect is inconsistent across different NLU tasks and datasets.</p>
<p>As shown in Figure 4, in RTE and CoNLL04, MLP reparameterization generally indicates better performance than embedding for almost all prompt lengths. However, in BoolQ, MLP, and embedding's results are competitive; in CoNLL12, the embedding consistently outperforms MLP.</p>
<p><strong>Prompt Length.</strong> Prompt length is yet another influential hyper-parameter for P-tuning v2, and its optimal value varies from task to task. From Figure 4, we observe that for simple NLU tasks, usually, a shorter prompt is enough for the best performance; for hard sequence tasks, usually, a longer prompt than 100 would be helpful.</p>
<p>We also discover that reparameterization has a close bond with optimal prompt length. For example, in RTE, CoNLL04, and BoolQ, MLP reparameterization achieves its optimal result earlier than embedding. This conclusion may contribute some thoughts on P-tuning's optimization properties.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Note that the notions of "simple" and "hard" are specific to prompt tuning, because we find sequence labeling tasks are more challenging for prompt tuning.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ We use "prompt tuning" to refer to a class of methods rather than a particular method.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
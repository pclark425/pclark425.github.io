<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6545 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6545</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6545</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-266520916</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.14890v4.pdf" target="_blank">NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes</a></p>
                <p><strong>Paper Abstract:</strong> Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6545.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6545.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic, automatically generated benchmark of 900 algorithmic reasoning problems spanning P, NP-complete, and NP-hard classes with 10 difficulty levels, designed to evaluate LLMs' logical reasoning and to reduce overfitting via monthly refreshes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>algorithmic-complexity-based evaluation (dynamic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>benchmark/dataset</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Algorithmic decision and optimization tasks (Sorted Array Search, Edit Distance, Shortest Path, TSP decision/optimization, Graph Coloring decision/optimization, Knapsack, Meeting Scheduling) across difficulty levels mapped to complexity classes (P, NP-complete, NP-hard).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy; Failure Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Benchmark is automatically checkable (algorithmic answers), excludes math‑intensive tasks, is refreshed monthly to mitigate overfitting, and structures tasks by complexity class and graded difficulty to measure reasoning rather than calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6545.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot (P)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting evaluated on P-class tasks (average across models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot prompting applied to all evaluated LLMs on P-class tasks (SAS, EDP, SPP) with reported average weighted accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated models (12)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Polynomial-time algorithmic tasks (Sorted Array Search, Edit Distance, Shortest Path).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>24.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot on NP-Hard tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>22.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Average weighted accuracy across P tasks ~0.24; models generally perform substantially better on P tasks than on NP-Hard tasks (statistically significant difference reported via Wilcoxon test).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6545.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot (NP-Complete)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting evaluated on NP-complete tasks (average across models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot prompting applied to all evaluated LLMs on NP-complete decision problems with reported average weighted accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated models (12)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NP-complete decision tasks (TSP-D, GCP-D, KSP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>25.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot on NP-Hard tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>23.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Average weighted accuracy across NP-complete tasks ~0.25; drop to NP-Hard is statistically significant (Wilcoxon p < 0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6545.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot (NP-Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting evaluated on NP-hard tasks (average across models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline zero-shot prompting applied to all evaluated LLMs on NP-hard optimization tasks showing a marked performance drop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>All evaluated models (12)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NP-hard optimization tasks (TSP optimization, Graph Coloring optimization, Meeting Scheduling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>2.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot on P and NP-Complete tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>-22.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mean weighted accuracy on NP-hard tasks is extremely low (~0.02), indicating models struggle fundamentally with these optimization problems under zero-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6545.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot In-Context (SAS/EDP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot in-context learning with varying example difficulty (applied to SAS and EDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic few-shot prompts that vary the difficulty of examples relative to the test question (same difficulty, easier, harder) to test whether LLMs truly learn algorithmic skills or merely mimic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Selected models (closed- and open-source groups)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Few-shot in-context learning (difficulty-graded examples)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / in-context</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval (SAS, EDP subsets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>In-context learning on Sorted Array Search (SAS) and Edit Distance Problem (EDP) with example difficulties varied ±1..5 levels.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy; Failure Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Closed-source models (e.g., GPT-4 Turbo, Claude 2) show minimal performance variation across example difficulty (interpreted as genuine learning); open-source models show heterogeneous behavior: some generalize from harder-to-easier examples but struggle from easier-to-harder, consistent with partial 'mimicking' rather than robust algorithmic learning.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6545.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning (QLoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parameter-efficient finetuning via QLoRA on benchmark versions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential fine-tuning (QLoRA) of open-source models on prior monthly benchmark versions to test 'hacking' the benchmark and generalization to new versions/difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-2; Mistral-7b; Qwen-14b</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Finetuning with QLoRA (question-answer pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>weight-updating / supervised finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval (multiple historical versions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Supervised finetuning on prior benchmark versions using question-answer pairs; evaluate on newer versions with same/different difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Unfinetuned checkpoints</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Finetuning improved performance on P problems (in-distribution difficulty) but did not generalize to NP-complete/NP-hard and sometimes degraded OOD performance (example: Qwen-14b improved on SPP levels 1-10 to near GPT-4 performance but regressed on harder SPP levels 11-20). Authors conclude basic Q&A finetuning cannot 'hack' NP-complete/NP-hard tasks and can overfit P tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6545.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting technique that elicits step-by-step intermediate reasoning from LLMs using prompts like 'Let's think step by step'.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / stepwise</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-step reasoning by eliciting intermediate steps in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned in related work as effective prompting technique; not specifically applied as an experimental variable in this paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6545.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thought / Graph-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thought and Graph-of-Thought prompting variations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Higher-level prompting frameworks that explore branching (tree) or graph search in the model's thought process to improve complex problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thought; Graph-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search; graph-search</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Deliberative search in thought-space via branching or graph exploration to solve complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Cited in related work as examples of structured reasoning approaches; not directly tested in the NPHardEval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6545.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-critique / RCI / Backward verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-critique (RCI) and backward verification methods</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative self-evaluation and correction strategies (e.g., Recursively Criticizes and Improves, backward verification) that let models refine and verify outputs to reduce reasoning errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>RCI; backward verification; self-critique</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>iterative refinement / self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Systems where models iteratively critique and improve their outputs or verify conclusions backward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Mentioned as related work improving reasoning performance; not incorporated in the benchmark experiments here (no chain-of-thought annotations were provided for finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6545.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct and Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning+action) and Reflexion (feedback-based refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ReAct integrates reasoning and actions (tool use) for better problem solving; Reflexion uses linguistic feedback to reinforce model behavior without weight updates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>ReAct; Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>reasoning-action hybrid; feedback-based</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Combining internal reasoning with external actions or feedback loops to improve task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Referenced as examples of methods that combine reasoning with actions or feedback; not directly evaluated in NPHardEval experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6545.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Closed-source models (group)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Closed-source foundation models (GPT-4 Turbo, Claude 2, GPT-3.5 Turbo, PaLM 2, Claude Instant 1.2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary LLMs evaluated on NPHardEval; reported to have higher weighted accuracy and lower failure rates than open-source counterparts and more consistent few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo; Claude 2; GPT-3.5 Turbo; PaLM 2; Claude Instant 1.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot / Few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / in-context</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>homogeneous</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated across P, NP-complete, NP-hard tasks and difficulty levels in NPHardEval.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy; Failure Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Open-source models (group)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Closed-source models generally showed higher accuracy and lower failure rates; GPT-4 Turbo often the top performer, Claude 2 strong on medium NP-complete zero-shot; closed-source models showed minimal performance variation across few-shot example difficulty (interpreted as better learning rather than mere mimicking).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6545.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e6545.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open-source models (group)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-source foundation models (Yi-34b, Qwen-14b, Mistral-7b, Phi-2, MPT-30b, Vicuna-13b, Phi-1.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLMs evaluated on NPHardEval; a subset (Yi-34b, Qwen-14b, Mistral-7b) achieved strong open-source performance, but overall open-source models had higher failure rates and more variable few-shot generalization patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-34b; Qwen-14b; Mistral-7b; Phi-2; MPT-30b; Vicuna-13b; Phi-1.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Zero-shot / Few-shot prompting; Finetuning (selected)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential / in-context; finetuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>NPHardEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on same algorithmic tasks and difficulty levels as closed-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Weighted Accuracy; Failure Rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Closed-source models (group)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Yi-34b, Qwen-14b, and Mistral-7b outperformed other open-source models but still trailed closed-source overall; open-source models sometimes generalize from harder to easier examples but often fail to generalize from easier to harder examples, interpreted as partial mimicry.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Graph of thoughts: Solving elaborate problems with large language models <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
                <li>Larger language models do in-context learning differently <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6545",
    "paper_id": "paper-266520916",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "NPHardEval",
            "name_full": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
            "brief_description": "A dynamic, automatically generated benchmark of 900 algorithmic reasoning problems spanning P, NP-complete, and NP-hard classes with 10 difficulty levels, designed to evaluate LLMs' logical reasoning and to reduce overfitting via monthly refreshes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "algorithmic-complexity-based evaluation (dynamic benchmark)",
            "reasoning_method_type": "benchmark/dataset",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "NPHardEval",
            "task_description": "Algorithmic decision and optimization tasks (Sorted Array Search, Edit Distance, Shortest Path, TSP decision/optimization, Graph Coloring decision/optimization, Knapsack, Meeting Scheduling) across difficulty levels mapped to complexity classes (P, NP-complete, NP-hard).",
            "performance_metric": "Weighted Accuracy; Failure Rate",
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Benchmark is automatically checkable (algorithmic answers), excludes math‑intensive tasks, is refreshed monthly to mitigate overfitting, and structures tasks by complexity class and graded difficulty to measure reasoning rather than calculation.",
            "ablation_study_present": false,
            "uuid": "e6545.0",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zero-shot (P)",
            "name_full": "Zero-shot prompting evaluated on P-class tasks (average across models)",
            "brief_description": "Baseline zero-shot prompting applied to all evaluated LLMs on P-class tasks (SAS, EDP, SPP) with reported average weighted accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated models (12)",
            "model_size": null,
            "reasoning_method_name": "Zero-shot prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "NPHardEval",
            "task_description": "Polynomial-time algorithmic tasks (Sorted Array Search, Edit Distance, Shortest Path).",
            "performance_metric": "Weighted Accuracy",
            "performance_value": 24.0,
            "comparison_target_method": "Zero-shot on NP-Hard tasks",
            "performance_difference": 22.0,
            "statistical_significance": true,
            "analysis_notes": "Average weighted accuracy across P tasks ~0.24; models generally perform substantially better on P tasks than on NP-Hard tasks (statistically significant difference reported via Wilcoxon test).",
            "ablation_study_present": false,
            "uuid": "e6545.1",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zero-shot (NP-Complete)",
            "name_full": "Zero-shot prompting evaluated on NP-complete tasks (average across models)",
            "brief_description": "Baseline zero-shot prompting applied to all evaluated LLMs on NP-complete decision problems with reported average weighted accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated models (12)",
            "model_size": null,
            "reasoning_method_name": "Zero-shot prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "NPHardEval",
            "task_description": "NP-complete decision tasks (TSP-D, GCP-D, KSP).",
            "performance_metric": "Weighted Accuracy",
            "performance_value": 25.0,
            "comparison_target_method": "Zero-shot on NP-Hard tasks",
            "performance_difference": 23.0,
            "statistical_significance": true,
            "analysis_notes": "Average weighted accuracy across NP-complete tasks ~0.25; drop to NP-Hard is statistically significant (Wilcoxon p &lt; 0.05).",
            "ablation_study_present": false,
            "uuid": "e6545.2",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zero-shot (NP-Hard)",
            "name_full": "Zero-shot prompting evaluated on NP-hard tasks (average across models)",
            "brief_description": "Baseline zero-shot prompting applied to all evaluated LLMs on NP-hard optimization tasks showing a marked performance drop.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "All evaluated models (12)",
            "model_size": null,
            "reasoning_method_name": "Zero-shot prompting",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "NPHardEval",
            "task_description": "NP-hard optimization tasks (TSP optimization, Graph Coloring optimization, Meeting Scheduling).",
            "performance_metric": "Weighted Accuracy",
            "performance_value": 2.0,
            "comparison_target_method": "Zero-shot on P and NP-Complete tasks",
            "performance_difference": -22.0,
            "statistical_significance": true,
            "analysis_notes": "Mean weighted accuracy on NP-hard tasks is extremely low (~0.02), indicating models struggle fundamentally with these optimization problems under zero-shot prompting.",
            "ablation_study_present": false,
            "uuid": "e6545.3",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Few-shot In-Context (SAS/EDP)",
            "name_full": "Few-shot in-context learning with varying example difficulty (applied to SAS and EDP)",
            "brief_description": "Systematic few-shot prompts that vary the difficulty of examples relative to the test question (same difficulty, easier, harder) to test whether LLMs truly learn algorithmic skills or merely mimic examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Selected models (closed- and open-source groups)",
            "model_size": null,
            "reasoning_method_name": "Few-shot in-context learning (difficulty-graded examples)",
            "reasoning_method_type": "sequential / in-context",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "NPHardEval (SAS, EDP subsets)",
            "task_description": "In-context learning on Sorted Array Search (SAS) and Edit Distance Problem (EDP) with example difficulties varied ±1..5 levels.",
            "performance_metric": "Weighted Accuracy; Failure Rate",
            "performance_value": null,
            "comparison_target_method": "Zero-shot prompting",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Closed-source models (e.g., GPT-4 Turbo, Claude 2) show minimal performance variation across example difficulty (interpreted as genuine learning); open-source models show heterogeneous behavior: some generalize from harder-to-easier examples but struggle from easier-to-harder, consistent with partial 'mimicking' rather than robust algorithmic learning.",
            "ablation_study_present": false,
            "uuid": "e6545.4",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Finetuning (QLoRA)",
            "name_full": "Parameter-efficient finetuning via QLoRA on benchmark versions",
            "brief_description": "Sequential fine-tuning (QLoRA) of open-source models on prior monthly benchmark versions to test 'hacking' the benchmark and generalization to new versions/difficulties.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-2; Mistral-7b; Qwen-14b",
            "model_size": null,
            "reasoning_method_name": "Finetuning with QLoRA (question-answer pairs)",
            "reasoning_method_type": "weight-updating / supervised finetuning",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "NPHardEval (multiple historical versions)",
            "task_description": "Supervised finetuning on prior benchmark versions using question-answer pairs; evaluate on newer versions with same/different difficulty.",
            "performance_metric": "Weighted Accuracy",
            "performance_value": null,
            "comparison_target_method": "Unfinetuned checkpoints",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Finetuning improved performance on P problems (in-distribution difficulty) but did not generalize to NP-complete/NP-hard and sometimes degraded OOD performance (example: Qwen-14b improved on SPP levels 1-10 to near GPT-4 performance but regressed on harder SPP levels 11-20). Authors conclude basic Q&A finetuning cannot 'hack' NP-complete/NP-hard tasks and can overfit P tasks.",
            "ablation_study_present": false,
            "uuid": "e6545.5",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Chain-of-Thought",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Prompting technique that elicits step-by-step intermediate reasoning from LLMs using prompts like 'Let's think step by step'.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT) prompting",
            "reasoning_method_type": "sequential / stepwise",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "Multi-step reasoning by eliciting intermediate steps in natural language.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned in related work as effective prompting technique; not specifically applied as an experimental variable in this paper's evaluations.",
            "ablation_study_present": false,
            "uuid": "e6545.6",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Tree-of-Thought / Graph-of-Thought",
            "name_full": "Tree-of-Thought and Graph-of-Thought prompting variations",
            "brief_description": "Higher-level prompting frameworks that explore branching (tree) or graph search in the model's thought process to improve complex problem solving.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "Tree-of-Thought; Graph-of-Thought",
            "reasoning_method_type": "tree-search; graph-search",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "",
            "task_description": "Deliberative search in thought-space via branching or graph exploration to solve complex tasks.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Cited in related work as examples of structured reasoning approaches; not directly tested in the NPHardEval experiments.",
            "ablation_study_present": false,
            "uuid": "e6545.7",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-critique / RCI / Backward verification",
            "name_full": "Self-critique (RCI) and backward verification methods",
            "brief_description": "Iterative self-evaluation and correction strategies (e.g., Recursively Criticizes and Improves, backward verification) that let models refine and verify outputs to reduce reasoning errors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "RCI; backward verification; self-critique",
            "reasoning_method_type": "iterative refinement / self-verification",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "",
            "task_description": "Systems where models iteratively critique and improve their outputs or verify conclusions backward.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Mentioned as related work improving reasoning performance; not incorporated in the benchmark experiments here (no chain-of-thought annotations were provided for finetuning).",
            "ablation_study_present": false,
            "uuid": "e6545.8",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ReAct and Reflexion",
            "name_full": "ReAct (reasoning+action) and Reflexion (feedback-based refinement)",
            "brief_description": "ReAct integrates reasoning and actions (tool use) for better problem solving; Reflexion uses linguistic feedback to reinforce model behavior without weight updates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "reasoning_method_name": "ReAct; Reflexion",
            "reasoning_method_type": "reasoning-action hybrid; feedback-based",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "",
            "task_description": "Combining internal reasoning with external actions or feedback loops to improve task performance.",
            "performance_metric": null,
            "performance_value": null,
            "comparison_target_method": null,
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Referenced as examples of methods that combine reasoning with actions or feedback; not directly evaluated in NPHardEval experiments.",
            "ablation_study_present": false,
            "uuid": "e6545.9",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Closed-source models (group)",
            "name_full": "Closed-source foundation models (GPT-4 Turbo, Claude 2, GPT-3.5 Turbo, PaLM 2, Claude Instant 1.2)",
            "brief_description": "Proprietary LLMs evaluated on NPHardEval; reported to have higher weighted accuracy and lower failure rates than open-source counterparts and more consistent few-shot generalization.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo; Claude 2; GPT-3.5 Turbo; PaLM 2; Claude Instant 1.2",
            "model_size": null,
            "reasoning_method_name": "Zero-shot / Few-shot prompting",
            "reasoning_method_type": "sequential / in-context",
            "reasoning_style_diversity": "homogeneous",
            "benchmark_name": "NPHardEval",
            "task_description": "Evaluated across P, NP-complete, NP-hard tasks and difficulty levels in NPHardEval.",
            "performance_metric": "Weighted Accuracy; Failure Rate",
            "performance_value": null,
            "comparison_target_method": "Open-source models (group)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Closed-source models generally showed higher accuracy and lower failure rates; GPT-4 Turbo often the top performer, Claude 2 strong on medium NP-complete zero-shot; closed-source models showed minimal performance variation across few-shot example difficulty (interpreted as better learning rather than mere mimicking).",
            "ablation_study_present": false,
            "uuid": "e6545.10",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Open-source models (group)",
            "name_full": "Open-source foundation models (Yi-34b, Qwen-14b, Mistral-7b, Phi-2, MPT-30b, Vicuna-13b, Phi-1.5)",
            "brief_description": "Open-source LLMs evaluated on NPHardEval; a subset (Yi-34b, Qwen-14b, Mistral-7b) achieved strong open-source performance, but overall open-source models had higher failure rates and more variable few-shot generalization patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-34b; Qwen-14b; Mistral-7b; Phi-2; MPT-30b; Vicuna-13b; Phi-1.5",
            "model_size": null,
            "reasoning_method_name": "Zero-shot / Few-shot prompting; Finetuning (selected)",
            "reasoning_method_type": "sequential / in-context; finetuning",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "NPHardEval",
            "task_description": "Evaluated on same algorithmic tasks and difficulty levels as closed-source models.",
            "performance_metric": "Weighted Accuracy; Failure Rate",
            "performance_value": null,
            "comparison_target_method": "Closed-source models (group)",
            "performance_difference": null,
            "statistical_significance": null,
            "analysis_notes": "Yi-34b, Qwen-14b, and Mistral-7b outperformed other open-source models but still trailed closed-source overall; open-source models sometimes generalize from harder to easier examples but often fail to generalize from easier to harder examples, interpreted as partial mimicry.",
            "ablation_study_present": false,
            "uuid": "e6545.11",
            "source_info": {
                "paper_title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of thoughts: Solving elaborate problems with large language models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Larger language models do in-context learning differently",
            "rating": 2,
            "sanitized_title": "larger_language_models_do_incontext_learning_differently"
        }
    ],
    "cost": 0.01943375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes
12 Feb 2024</p>
<p>Lizhou Fan lizhouf@umich.edu 
School of Information
University of Michigan
48103Ann ArborMI</p>
<p>Lizhou Fan and Wenyue Hua contribute equally</p>
<p>Wenyue Hua wenyue.hua@rutgers.edu 
Department of Computer Science
Rutgers University
08854New BrunswickNJ</p>
<p>Lizhou Fan and Wenyue Hua contribute equally</p>
<p>Lingyao Li lingyaol@umich.edu 
School of Information
University of Michigan
48103Ann ArborMI</p>
<p>Haoyang Ling 
Yongfeng Zhang yongfeng.zhang@rutgers.edu 
School of Information
University of Michigan
48103Ann ArborMI</p>
<p>Department of Computer Science
Rutgers University
08854New BrunswickNJ</p>
<p>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes
12 Feb 2024C53DD9D0E2D92F29D7A649C3B0FB82A4arXiv:2312.14890v4[cs.AI]
Complex reasoning ability is one of the most important features of current Large Language Models (LLMs), which has also been leveraged to play an integral role in complex decision-making tasks.Therefore, the investigation into the reasoning capabilities of LLMs is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs.However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving.They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance.Addressing these limitations, our research introduces a new benchmark, named NPHardEval.This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class.These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs.Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes.Our findings contribute significantly to understanding the current capabilities of LLMs in reasoning tasks and lay the groundwork for future advancements in enhancing the reasoning abilities of these models.Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis.Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities.The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.</p>
<p>Introduction</p>
<p>The advancement of LLMs has ushered in a transformative era in Artificial Intelligence (AI) research [1].One major advantage, believed by many researchers, is the unparalleled reasoning capabilities showcased by these models [2].Despite the implementation of various benchmarks for evaluating reasoning ability [3,4,5,6,7], existing methods reveal certain limitations.These include inadequacies in the precise characterization of reasoning abilities, the risk of models overfitting to specific benchmarks [8], and in some cases, the dependency on manual evaluation methods [9].Additionally, it is theoretically interesting to examine the extent to which LLMs can address problems in the computational complexity hierarchy [10], especially NP-hard or NP-complete problems.In response Another novel feature of our benchmark is its end-to-end automation, encompassing both the generation of tasks and the verification of results.This automation is facilitated by the use of well-known tasks within the benchmark, for which mature and established algorithms have been developed to provide solutions.This systematic approach ensures a high degree of accuracy and reliability in the evaluation process, which also enables easy update of datapoints in the benchmark.This automated framework facilitates an effortless updating of datapoints within the benchmark.As a result, we design the benchmark to refresh its datapoints monthly, effectively reducing the likelihood of the model overfitting the dataset.This dynamic updating mechanism is crucial in maintaining the rigor and relevance of the benchmark over time.We welcome open submissions of model performance to our benchmark directly through the NPHardEval leaderboard on HuggingFace: https://huggingface.co/spaces/NPHardEval/NPHardEval-leaderboard.</p>
<p>In general, our benchmark offers several advantages compared with current benchmarks:</p>
<p>• The questions in the benchmark utilized are grounded in the established computational complexity hierarchy, a concept extensively studied in theoretical computer science.This foundation enables us to leverage existing research to rigorously and quantitatively measure an LLM's logical reasoning extent.• We incorporate automatic checking mechanisms for these questions, as they are based on algorithmically computable problems.Human intervention is not required to determine the correctness of the LLM's responses.• The method allows for the automatic generation of questions so that we can update the benchmark on a monthly basis.This monthly-refreshed benchmark helps prevent model's overfitting as we can always generate novel questions with varying difficulty levels for evaluation.• The benchmark excludes numerical computation from the questions, which is notably difficult for LLM.This focus allows for a more accurate evaluation of an LLM's pure logical reasoning ability, as numerical computation can obscure this assessment.• Our methodology offers insights into a long-standing and intriguing question within the field: the degree to which LLMs are capable of tackling problems classified as NP-hard or NP-complete.</p>
<p>In our research utilizing the benchmark, we aim to address three critical aspects to evaluate and understand the reasoning abilities of LLMs (Foundation Models):</p>
<ol>
<li>Model Performance Comparison: Our benchmark compares the reasoning ability of 12 closedsource models (including as GPT 4 Turbo, Claude 2, GPT 3.5 Turbo, Claude Instant, and PaLM 2) and open-source models (including Yi-34b, Qwen-14b, Mistral-7b, Phi-2, MPT-30b, Vicuna-13b, and Phi-1.5)across three complexity classes (P, NP-complete, and NP-hard) and 10 difficulty levels.This comparison can shed light on the relative strengths and weaknesses of these models and determine the proficiency of them in solving progressively challenging problems, thereby gauging their capability to handle tasks with escalating complexity.2. Robustness of Benchmark Assessments: This study examines whether the frequent updating of algorithmic benchmarks can effectively prevent the risk of "hacking" the benchmark.The Figure 1: Computational complexity classes P, NP-complete, and NP-hard and corresponding tasks dynamic updating of benchmarks is proposed as a strategy to reduce the likelihood of LLMs overfitting to these benchmarks.However, a pertinent question arises: does finetuning LLMs on benchmarks from the previous month lead to overfitting specific problem types?To explore this, we conducted an experiment where three open-source models --Phi-2, Mistral-7b, and Qwen-14b --were finetuned on five distinct versions of the benchmarks.The performance of these models was evaluated on two versions of the benchmark, each differing in difficulty level.This approach allowed us to assess whether finetuning enables models to "hack" benchmarks of varying complexity.</li>
</ol>
<p>Generalization through In-context Learning:</p>
<p>Given examples in the context, can LLMs genuinely learn and apply algorithmic skills presented in contextual examples as opposed to merely mimicking problem-solving processes [11,12]?We differentiate between "learning" and "mimicking" by evaluating whether LLMs can generalize solutions to new problems of varying difficulty levels within the same task, after being exposed to examples.Our hypothesis is that if an LLM has truly learned the underlying algorithmic skill, it should be able to tackle problems across different difficulty levels within the same task.Conversely, if an LLM is merely mimicking, its performance may falter when faced with variations in problem difficulty.</p>
<p>The contribution of this paper is as follows: We present the first complexity classes-based reasoning benchmark, NPHardEval.This benchmark enables a rigorous evaluation of LLMs' reasoning capabilities on a wide range of complex reasoning tasks with varying difficulty.Through the above research questions, we aim to provide a comprehensive analysis of the reasoning capabilities of LLMs, exploring their potential for genuine understanding and application of complex problem-solving skills.</p>
<p>2 Related Work</p>
<p>Reasoning ability of LLMs</p>
<p>LLMs [13,14,15] have made significant advancements in natural language processing and related fields.Recent research underscores the unprecedented reasoning abilities of LLMs in various fields, from biomedical and human-computer interaction research to humanities and social studies [16,17,18,19,20].It has been discussed that these models exhibit "emergent" behaviors, including the ability to "reason" when they are large enough [21,22].By providing the models with the chain of thoughts with a simple prompt "Let's think step by step", these models are able to answer questions with explicit reasoning steps [23].This has sparked considerable interest in the community since reasoning ability is a hallmark of human intelligence.Various variations of chain-of-thought have been developed to prompt models' reasoning ability [24,25,26], such as tree of thought [27], graph of thought [28], self-inspring technique [29].</p>
<p>Later, various self-critique methods have been proposed to enhance LLM's reasoning performance.The Recursively Criticizes and Improves (RCI) approach, for example, iteratively refines outputs, proving more effective in automating computer tasks and elevating reasoning capabilities [30].Similarly, backward verification proposes an intuitive human-like mechanism for LLMs to self-check and improve their conclusions, reducing errors in reasoning tasks [31].Moreover, the interplay of reasoning and action showcases LLMs' outstanding synergistic ability.For instance, the "ReAct" approach highlights that reasoning can enhance action plans, while actions can help the model interface with external sources for better reasoning [32].In addition, the capability of LLMs to learn from feedback also indicates their reasoning potential."Reflexion" is a workflow that reinforces LLMs through linguistic feedback without updating model weights [33].In robotic contexts, LLMs demonstrate enhanced performance with environment feedback, creating an internal "monologue" to assist decision-making [34].</p>
<p>Despite the impressive performance exhibited by LLMs, there remains a gap in our rigorous understanding of the extent and depth of reasoning these models are capable of.Our paper aims to address this gap by providing a framework to study the reasoning abilities of LLMs within the well-established hierarchy of computational complexity.This approach seeks to systematically evaluate and quantify their reasoning capabilities in a more structured and academically rigorous manner.</p>
<p>Benchmarks of LLMs' Performance</p>
<p>The advancement of LLMs has catalyzed the evolution of a range of general-purpose AI technologies, underscoring the importance of accurately assessing these models' reasoning capabilities.Existing evaluation approaches predominantly rely on datasets comprising human-generated questions and their standard answers.For instance, MMLU [6] and GAOKAO [35] both utilize human exam questions in their automated evaluations.Additionally, datasets such as the French National Math Exam, Hungarian National High School Exam1 , and GHOST (Graduate-Level High-Order Skill Tests) [9] are utilized to assess LLMs' reasoning proficiency.[36] proposes a dynamic graph-based reasoning benchmark.These sources aim to ensure the absence of data leakage.Nonetheless, the requirement for manual verification of answers in these datasets limits their practical utility.In general, these datasets are commonly employed as benchmarks in the field; however, they lack a quantitative metric for assessing the difficulty level of the questions and the extent of reasoning necessary to answer them.This absence of precise measurement criteria results in a limited understanding of the logical reasoning capabilities of large language models.</p>
<p>Other Benchmarks such as AlpacaEval [37] and SuperCLUE [38] have attempted to incorporate openended questions in English and Chinese, respectively, to capture a diverse breadth of possible answers and enhance the comprehensiveness of LLM's evaluation.However, they are not universal and are often constrained by language barriers and cultural contexts, potentially skewing the evaluation of reasoning abilities toward a specific scenario.Reasoning tasks should transcend linguistic and cultural specifics, focusing instead on universal logical principles.Big-Bench Hard [39], DROP [40], and HellaSwag [41], while valuable, predominantly target multi-step reasoning, reading comprehension, and commonsense reasoning, respectively.They do not adequately prioritize complex logical reasoning in their assessment criteria.</p>
<p>The prevalent focus on question answering and math problems in current benchmarks may insufficiently capture the essence of reasoning -the ability to logically process and deduce information beyond memorized knowledge.It also falls short on providing a rigorous metric on the reasoning ability.This gap highlights the need for a paradigm expansion in LLM evaluation, calling for logic-based reasoning benchmarks to complete the traditional utility-based approach, where we have quantitative evaluation on the computational complexity of the questions, indicating the amount of reasoning ability required.</p>
<p>3 Benchmark Construction</p>
<p>Complexity Classes</p>
<p>In our study, we employ the concept of complexity classes to categorize the reasoning tasks for LLMs.These classes are defined based on the computational resources, such as time or memory, required to solve the problems they contain [10].Primarily, most complexity classes comprise decision problems that can be solved using a Turing machine, with differentiation based on their time or space (memory) requirements.For example, the class P includes decision problems that a deterministic Turing machine can solve in polynomial time.Tasks within this class often pose multi-dimensional cognitive challenges, enriching the evaluation framework of LLMs.This structured approach not only aids in assessing the reasoning capabilities of LLMs but also holds substantial relevance in various practical applications, particularly in optimization and high-level decision-making scenarios.</p>
<p>In particular, we use three complexity classes to define the task complexity in the benchmark, including P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard, which are increasingly complex in both the intrinsic difficulty and the resources needed to solve them.Figure 1 shows their relation regarding computational complexity in an Euler diagram.This approach aims to delineate the extent of complex reasoning achievable by LLMs, thus for each complexity class, we only choose tasks from the non-overlapping subset of the complexity class.In our selection criteria, we intentionally exclude tasks that demand intensive mathematical computations, such as matrix multiplication and logarithmic calculations.Thus, we do not list NP class (questions in NP but not P and not NP-complete), which is exemplified by the discrete logarithm and integer factorization problems, as the majority of such problems are characterized by their calculation-intensive nature (see details in Appendix B).</p>
<p>P (Polynomial time) Tasks</p>
<p>This class consists of tasks that can be solved by a deterministic Turing machine in polynomial time.Essentially, it represents tasks that are efficiently solvable.We include three P problems in the benchmark, namely Sorted Array Search (SAS), Edit Distance Problem (EDP), and Shortest Path Problem (SPP).</p>
<p>Sorted Array Search (SAS) SAS is about finding the position of a target value after sorting a given array.Given an array A of n elements and a target value T , the goal is to determine the index at which T is located in A after sorting.Renowned algorithms like binary search efficiently accomplish this task by iteratively halving the search interval, operating in logarithmic time.The problem can be formally stated as finding an index i such that A[i] = T , or determining that no such index exists.It is commonly used in databases and search engines to quickly find specific data within a large dataset [42].</p>
<p>Edit Distance Problem (EDP) EDP is about finding the minimum number of operations required to transform one string into another.Given two strings, A and B, of lengths m and n respectively, the aim is to determine the minimum number of operations needed to convert A into B. The allowable operations are insertion, deletion, and substitution of a single character.Formally, the problem can be defined as finding a minimum number d such that string A can be transformed into string B using d operations.This algorithm has a time complexity of O(ab) where a and b are the lengths of the strings.When the full dynamic programming table is constructed, its space complexity is also O(ab).EDP has widespread applications, especially in fields like computational biology for sequence alignment, natural language processing for spell checking and correction, and in data analysis for measuring similarity between data strings.</p>
<p>Shortest Path Problem (SPP) SPP is about finding the shortest path between two nodes in a non-negative weighted graph.In our experiments, we ask for the shortest path between the first and last nodes.Given a graph G = (V, E) with a weight function w : E → R assigning weights to edges, and two vertices u and v in V , the task is to find the path from u to v that minimizes the total weight.This is often solved using Dijkstra's algorithm which systematically expands the shortest path from the starting node until it reaches the target node.Formally, the problem is to find a path P = (v 1 , v 2 , ..., v k ), where v 1 = u and v k = v, such that the sum of weights of consecutive edges in P ,
k−1 i=1 w(v i , v i+1
), is minimized.This problem can be used in network routing, GPS navigation systems, and logistics to find the shortest or most efficient path between two points.It helps in reducing travel time and costs in transportation and communication networks.</p>
<p>NP-complete problems</p>
<p>This is a subset of NP.A problem is NP-complete if it is in NP and as hard as any problem in NP.If any NP-complete problem can be solved in polynomial time, then every problem in NP can also be solved in polynomial time.We include three NP-complete problems that are not in P in the benchmark, namely Traveling Salesman Problem Decision Version (TSP-D), Graph Coloring Problem Decision Version (GCP-D), and Knapsack Problem (KSP).</p>
<p>Traveling Salesman Problem (Decision Version, TSP-D) TSP-D is concerned with determining if a salesman can complete a route, visiting each city at least once, with the total travel distance being less than a specified value.Given a complete graph G = (V, E) with vertices V representing cities and edges E representing paths between cities, each edge (i, j) is assigned a distance d(i, j).The decision version of this problem asks whether there exists a tour (a sequence of cities) such that the total distance of the tour is less than or equal to a given value D. Formally, the problem can be stated as finding a permutation P of the set of cities 1, 2, ..., n that satisfies the condition n−1 i=1 d(P (i), P (i + 1)) + d(P (n), P (1)) ≤ D. This problem is useful in logistics and supply chain management in planning efficient delivery routes and schedules [43].</p>
<p>Graph Coloring Problem (Decision Version, GCP-D) GCP-D involves determining if it is possible to color the vertices of a graph using a given number of colors so that no two adjacent vertices share the same color.Given an undirected graph G = (V, E), with V representing vertices and E representing edges, the goal is to find out if there is a way to assign one of k colors to each vertex such that for any edge (u, v) ∈ E, the vertices u and v have different colors.The formal statement is to determine if there exists a coloring function c : V → 1, 2, ..., k such that for every edge (u, v) ∈ E, c(u) ̸ = c(v).It has wide applications in Round-Robin Sports Scheduling, Aircraft scheduling, and Biprocessor tasks [44].</p>
<p>Knapsack Problem (KSP) KSP asks whether a subset of items can be chosen to fit into a knapsack of fixed capacity without exceeding it, while also maximizing the total value of the selected items.Consider a set of items, each with a weight w i and a value v i , and a knapsack with a weight capacity W .The problem is to select a subset of these items such that the total weight does not exceed W and the total value is maximized.Formally, let x i be a binary variable indicating whether item i is included in the knapsack (x i = 1) or not (x i = 0).The problem can be stated as maximizing n i=1 v i x i subject to the constraint n i=1 w i x i ≤ W , where n is the number of items.It is used in resource allocation and budgeting where the goal is to maximize the total value of a selection under a weight or cost constraint.Applications include cargo loading, and electric vehicle charging [45,46].</p>
<p>NP-hard problems</p>
<p>These problems are at least as hard as the hardest problems in NP.They may not necessarily be in NP (i.e., they may not have solutions verifiable in polynomial time) but solving an NP-hard problem in polynomial time would imply that P = NP.We include three NP-hard problems that are not reducible to NP-complete problems in the benchmark, namely Traveling Salesman Problem Optimization Version (TSP), Graph Coloring Problem Optimization Version (GCP), and Meeting Scheduling Problem (MSP).</p>
<p>Traveling Salesman Problem (Optimization Version, TSP) TSP-O involves finding the shortest route for a salesman to visit each city exactly once and return to the starting city.Given a complete graph K n with n vertices, where each vertex represents a city and each edge (i, j) is assigned a non-negative cost or distance d(i, j), the problem is to find the shortest possible route that visits each city exactly once and returns to the origin city.Formally, let P be a permutation of the set of cities 1, 2, ..., n representing the order in which the cities are visited.The traveling salesman problem can be formulated as finding the permutation P that minimizes the total travel cost, given by the function f (P ) = d(P (n), P (1)) + n−1 i=1 d(P (i), P (i + 1)).This problem is important in operational research and logistics to find the most efficient route to visit multiple locations and return to the origin, particularly route planning for delivery services, maintenance operations, and sales.</p>
<p>Graph Coloring Problem (Optimization Version, GCP) GCP-O refers to the problem of coloring vertices of a graph in such a way that no two adjacent vertices have the same color.Given an undirected graph G = (V, E), where V is the set of vertices and E is the set of edges, assign a color to each vertex such that no two adjacent vertices have the same color.Formally, let c : V → C be a function that assigns a color from a set of colors C to each vertex in V .The graph coloring problem can be formulated as finding a proper coloring, i.e., a function c such that for every edge
(u, v) ∈ E, c(u) ̸ = c(v).
This problem is used in constraint satisfaction problems and applied in exam timetabling and register allocation in compilers [47].</p>
<p>Meeting Scheduling Problem (MSP) MSP deals with allocating time slots for meetings such that all constraints, including participant availability and room capacity, are satisfied without overlaps.Given a set of n participants and their availability for m time slots, find a schedule that maximizes the number of participants who can attend the meeting.Formally, let A = a 1 , a 2 , ..., a n be the set of participants and T = t 1 , t 2 , ..., t m be the set of time slots.For each participant a i , let S i be a subset of T representing the times when a i is available and m i be a subset of meetings that are required to attend.The meeting scheduling problem can be formulated as finding a subset S ⊆ T such that |a i ∈ A|S i ∩ S ̸ = ∅| is maximized.In other words, the aim is to find a scheduling subset S i where the collective availability of participants intersects with S i , ensuring maximum participation.This problem is crucial in organizational management for scheduling meetings involving multiple participants with varying availability.It ensures optimal utilization of time and resources and is used in corporate scheduling systems and collaborative software [48].</p>
<p>Difficulty Level for Tasks</p>
<p>NPHardEval categorizes the challenges it presents into a hierarchy of difficulty, spanning from the simplest to the most complex.This structure is divided into 10 distinct levels of difficulty for each task, with the initial level being designed as the most basic challenge that an LLM might face.This gradation allows for a nuanced assessment of an LLM's problem-solving abilities across a spectrum of increasingly complex tasks.For instance, the GCP-D problem has difficulty levels 1 to 10 with questions of 6, 8, 10, 12, 14, 16, 18, 20, 22, and 24 average edges and 6, 7, 8, 9, 10, 11, 12, 13, 14, and 15 nodes.Beginning with graphs of 6 nodes and 6 edges, each subsequent level incorporates an additional 2 edges and 1 node, culminating in graphs of 24 edges and 15 nodes at the most challenging level.</p>
<p>The difficulty level is not strictly bound to a linear scaling of difficulty; rather, it is designed to explore the nuances of performance degradation.By observing how LLMs cope with an escalating series of challenges, we aim to identify the inflection point where the performance notably diminishes.This approach provides a comprehensive understanding of where LLMs excel and where they falter, informing potential pathways for the enhancement of their reasoning capabilities.</p>
<p>Data Synthesis</p>
<p>In the context of data synthesis for complex tasks, the approach can be categorized into two distinct methodologies, each corresponding to a different type of data structure: graph data (e.g., GCP) and linear data (e.g., MSP).The synthesis process in both cases is governed by a progression of complexity across a spectrum of predefined levels.This structured approach enables the creation of diverse datasets, suitable for evaluating and benchmarking LLMs' reasoning ability.We provide examples of the synthesized data and how thay are used in prompts in Appendix A.</p>
<p>Graph Data Synthesis</p>
<p>The complexity in graph data synthesis escalates through a series of levels, each defined by a set of parameters that dictate the graph's size and intricacy.These parameters typically include the number of vertices, the number of edges, and the range of edge weights.At lower levels, graphs are simpler with fewer vertices and edges, and a limited range of edge weights.As the level increases, the graphs become progressively more complex, featuring more vertices, a higher density of edges, and a wider variety of edge weights.The synthesis process is as follows:</p>
<p>• A generative function is employed to construct individual graph instances.This function adheres to the principles of graph theory, ensuring the creation of simple graphs without self-loops and duplicate edges, and respecting the parameters dictated by the current difficulty level.</p>
<p>• A batch synthesis function then iteratively employs the generative function to produce multiple graph instances across the spectrum of difficulty levels.</p>
<p>• Finally, the synthesized graph instances are preserved in a tabulated format (in a CSV file), facilitating subsequent utilization and analysis.</p>
<p>Linear Data Synthesis In linear data synthesis, complexity is modulated by manipulating the length of the data array and the range of its constituent elements.Initial levels are characterized by shorter arrays with elements drawn from a narrow range.As the difficulty level ascends, the arrays lengthen, and the range of possible element values expands, thus introducing greater variability and complexity to the problem.The synthesis process is as follows:</p>
<p>• A linear data instance generation function is first utilized.This function produces sorted arrays of random numbers within a defined range, and selects a target number, ensuring its presence within the array to guarantee solvability.</p>
<p>• Multiple instances are generated through an iterative process, adhering to the difficulty levels outlined.</p>
<p>• These instances are then systematically recorded in a structured format (in a JSON file) for easy access and analysis.</p>
<p>Experimental Setting</p>
<p>This section presents the experiment setting to answer the three research questions.Our approach assessed 10 distinct LLMs, with a dichotomy between five proprietary (closed-source) models and five open-source models, including GPT 4 Turbo, Claude 2, GPT 3.5 Turbo, Claude Instant 1.2, PaLM 2, Vicuna-13b, Yi-34b, Mistral-7b, MPT-30b, and Phi-1.5.</p>
<p>Experiment 1: Model Performance Comparison</p>
<p>To evaluate the reasoning abilities of different LLMs through the NPHardEval benchmark, we employed a comparative experimental design.We use zero-shot prompts as the foundational measure of performance.These prompts comprise a task description and a specific question, presented without any preceding examples, to gauge the base capability of the model.</p>
<p>The complexity of the problems presented to the models spanned from polynomial-time (P) to NP-complete and NP-hard levels.To ensure comprehensive coverage, we utilize the full set of 900 problems in NPHardEval, capturing the multifaceted nature of real-world challenges that typically exceed the capabilities of straightforward algorithmic approaches.Each model's performance was evaluated based on two primary metrics: weighted accuracy and failure rate across the different complexity classes of problems, as we discussed in Section 4.4.</p>
<p>To evaluate across task complexity, specifically comparing the complexity among P, NP-Complete, and NP-Hard pairs, we initially pinned the data based on complexity levels.Subsequently, we applied the Wilcoxon test to each pair of complexity sets.Wilcoxon is a non-parametric statistical hypothesis test that allows us to compare two populations with matched samples.To evaluate problem difficulty, aiming to discern differences among problems within the complexity category, we pinned the data based on the specific problems and then used the Wilcoxon test to compare pairs of different problem sets.</p>
<p>Experiment 2: Benchmark Robustness</p>
<p>The primary objective of this experiment is to ascertain whether it is possible to "hack" our benchmark by finetuning models on its previous versions.To simulate this, we constructed five versions of the benchmark, maintaining a consistent difficulty level.Additionally, we utilize two distinct versions of the benchmark, each varying in difficulty, to evaluate the potential for hacking under varying conditions.To replicate the progression of time, models were finetuned sequentially on one to five benchmarks, each finetuned checkpoint is tested on the two distinct benchmarks for evaluation.</p>
<p>The experiment involved finetuning three high-performing open-source models: Phi-2, Mistral-7b, and Qwen-14b.Due to constraints in computing resources, the Yi-34b model was not included in the finetuning process.For the finetuning process, we employed the QLoRA technique, applying specific hyperparameters: batch size set to 8, a single epoch, a warmup proportion of 0.03, a learning rate of 1e-4, lora_r at 64, lora_alpha at 16, and a lora_dropout of 0.1.This approach aims to rigorously test the robustness of our benchmark against potential overfitting strategies.</p>
<p>Experiment 3: Comparative Analysis of Learnability by In-context Learning</p>
<p>A prevalent approach in current few-shot learning involves using examples that bear similarity to the test question.However, this raises a question about the extent to which the model is replicating the problem-solving process from the examples as opposed to genuinely acquiring reasoning skills.Consequently, it becomes pertinent to investigate whether the problem-solving abilities developed through example-based learning are generalizable.</p>
<p>To delve deeper into the models' in-context learning abilities, we utilize various few-shot in-context learning prompts to discern whether the model is "learning" from the few-shot examples or merely "mimicking" the behavior.In our benchmark, since we distinctly classify the difficulty level of each question, it allows for the use of questions from the same task but with varying difficulty levels as few-shot examples.The crux of this analysis lies in varying the difficulty levels of examples within the prompts.Since the fundamental algorithmic skill required to solve a question remains constant across varying difficulty levels under the same task, a model that truly learns this skill should show consistent performance irrespective of the example difficulty in the prompt.We propose the following hypotheses about the relationship between in-context learning ability and the difference of difficulty level between the given examples and the question being asked in context: It points to an absence of transferable, logic-learning skills, reflecting a superficial form of learning that is limited to surface-level imitation rather than a deeper, conceptual grasp.</p>
<p>We categorize the few-shot prompts into three types:</p>
<p>• Few-shot prompts with examples of the same difficulty level: Here, the model is provided with five examples in the prompt, all of which are at the same difficulty level and distinct from the question being asked.</p>
<p>• Few-shot prompts with examples that are easier than the question: This set comprises five variations of prompts, each with examples that are 1, 2, 3, 4, and 5 levels easier than the question, respectively.</p>
<p>• Few-shot prompts with examples that are more challenging than the question: Similarly, we prepare five sets of prompts, each containing examples that are 1, 2, 3, 4, and 5 levels more difficult than the question, offering a gradient of increased challenge.</p>
<p>Through this diverse array of prompts, we aim to provide a nuanced understanding of the LLMs' ability to learn from examples, thereby offering valuable insights into their underlying learning capabilities.</p>
<p>Evaluation Metrics</p>
<p>To evaluate the reasoning ability of LLMs, we utilize two metrics, the Weighted Accuracy and the Failure Rate, to comprehensively quantify the correctness of LLMs' reasoning outputs.</p>
<p>Weighted Accuracy (WA) is calculated for each problem either through the comparison with the correct answer or through step-by-step results checking, for those problems without the only answer.</p>
<p>To better represent the comparative accuracy, we assign weights to different difficulty levels so that each level has a weight corresponding to its relative importance or challenge.Higher difficulty levels are given more weight in a linear manner (e.g., level 1 has weight 1, level 2 has weight 2, etc.).The Weighted Accuracy is formally defined as:
W A = 10 i=1 (w i × A i )10i=1
w i where w i represents the weight assigned to difficulty level i, from 1 to 10, and A i is the accuracy at that level.</p>
<p>Failure Rate (FR) is a measure used to assess the frequency of unsuccessful outcomes across the different problems and difficulty levels.It is particularly useful for identifying cases where an LLM's result does not comply with the expected output format.The Failure Rate is calculated by considering the proportion of failed attempts relative to the total number of attempts for each difficulty level.An attempt is defined as failed if the model generates results that cannot be successfully parsed in all endpoint calls, and we set the maximum times of try as 10.For each problem, the Failure Rate is then aggregated across all difficulty levels, taking into account the total 10 attempts at each level.The formal definition of Failure Rate is given by:
F R = 10 i=1 F i 100
where F i denotes the number of failed attempts at difficulty level i.</p>
<p>Results</p>
<p>Reasoning Ability of Foundation Models</p>
<p>Experiment 1 focuses on a comprehensive comparison among various foundation models and across complexity classes and difficulty levels.In Figure 2, we present the overall zero-shot accuracy for each problem, providing a visual representation of the performance of different models.</p>
<p>Our observations reveal that closed-source models generally demonstrate higher accuracy and a lower rate of failure compared to their open-source counterparts.Notably, GPT-4 Turbo often emerges as the frontrunner in performance across the majority of tasks, indicating its superior problemsolving capabilities, while Claude 2, on the other hand, often performs the best on medium-level (NP-complete) complexity in zero-shot settings.</p>
<p>Within the realm of open-source models, Yi-34b, Qwen-14b, and Mistral-7b distinguish themselves by significantly outperforming other models in this category.We observe a disparity between the performance of these three models and other open-source options, highlighting a notable performance gap and suggesting that these models possess more advanced reasoning abilitie.</p>
<p>In particular, we use the weighted accuracy and the failure rate metrics to further quantify different models' performance.The trends observed below in both weighted accuracy and failure rates point to a nuanced understanding of the capabilities and limitations of current LLMs.We also utilize performance comparison tests within and across complexity classes, to further explore the model performance differences among complexity classes.</p>
<p>Weighted Accuracy As Figure 3(a) shows, upon analysis of the weighted accuracy for different models across problem complexities, we observed a general trend where all models experienced a decrease in accuracy as problem complexity increased.Notably, there are two detailed findings for overall reasoning ability change.First, regarding the performance decay speed, among the 12 models we tested, the average performance demonstrated a higher accuracy at the P and NP-Complete complexity levels (with similar weighted accuracies of 0.24 and 0.25) but saw a sharper decline as the problems became more complex when proceeding to the NP-hard level (with a weighted accuracy of 0.02).There is a performance decay on average when models are tested against NP-Hard problems.Second, close-source models usually perform better than open-source models -there are more triangles in the upper locations than squares in Figure 3(a).Failure Rate As Figure 3(b) indicates, the failure rates mirrored the trends observed in weighted accuracy but in reverse.On average, the models showed an increase in failure rates corresponding to the complexity of the problems.Similar to the weighted accuracy, open-source models fail more often (with more squares on the top) than the close-source models (with more triangles on the bottom), indicating close-source's models advanced ability in following the prompt to understand the reasoning problems and generate answers with correct format.</p>
<p>Performance across Task Complexity and Difficulty Levels</p>
<p>Figure 4 shows the accuracy of each model across different complexity levels.The test results reveal statistical significance (p &lt; 0.05) in the p-values between P and NP-Hard, as well as NP-Complete and NP-Hard.These findings indicate that our investigated LLMs performed significantly worse when confronted with NP-Hard problems compared to P and NP-Complete problems.</p>
<p>Figure 6 presents the accuracy of each model across various problems associated with P, NP-Complete, and NP-Hard complexities.Regarding P complexity, notable differences emerged among the models.GPT 3.5 Turbo, GPT 4 Turbo, Yi-34b, and Qwen-14b models exhibited significantly superior performance on the SAS problem compared to the other two problems.GPT 3.5 Turbo, Yi-34b, and Vicuna-13b models demonstrated markedly better performance on the EDP problem compared to the SPP problem.Only the Vicuna-13b model displayed slightly better performance, although not significant, on the EDP problem compared to SAS across all investigated models.</p>
<p>Other observations include: GPT 4 Turbo showcased very similar performance between the EDP and SPP problems, while Claude Instant 1.2 exhibited similar performance for all these three problems.Yi-34b, Owen-14b, GPT 3.5 Turbo, and GPT 4 Turbo displayed remarkably high accuracy specifically for the SAS task.MPT-30b and Phi-1.5 showed very limited performance in identifying these three problems.Considering NP-Hard complexity as the most intricate task set among the three (as evidenced in Figure 4), many of the examined models encountered challenges in identifying tasks within this complexity.For the GCP task, Mistral-7b, PaLM 2, GPT 3.5 Turbo, and GPT 4 Turbo exhibited some potential, while Vicuna-13b and Claude Instant 1.2 showed limited performance.For the TSP task, identification was observed only in Claude 2 and GPT 4 Turbo.Of all the investigated models, GPT 4 Turbo exhibited promise in identifying these three tasks within the NP-Hard complexity.However, the performance in GCP and TSP identification significantly surpassed that of the MSP task across these models.For the MSP task, only GPT 4 Turbo displayed some ability for identification, while with notably low accuracy.</p>
<p>Evaluating Benchmark Robustness</p>
<p>In Experiment 2, we explore the robustness of benchmark against hacking attempts through a process of finetuning on pairs of question and gold answer.We experiment using 3 well-performing opensource models: Qwen-14b, Mistral-7b, and Phi-2 on two versions of benchmarks.Figure 6 presents the result2 : each problem has two graph with one displaying evaluation results at difficulty levels 1-10 and one displaying evaluation results at difficulty levels 11-20.In each graph, the first row of indicate the accuracy mean of each model, averaged over outcomes at 5 finetuning checkpoints, ranging from tuning using zero (no finetuning) to five distinct benchmarks.Our findings are twofold: (1) While finetuning yields improvements in solving polynomial-time problems, its impact on the more complex NP-complete and NP-hard problems are negative.This suggests the inherent difficulty of hacking NP-complete, and potentially NP-hard, problems through the basic finetuning with question-and-answer approach.Manual annotation of the chain-of-thought, which is not provided in the benchmarks, could potentially enhance effectiveness, albeit with challenges in annotation.(2) Finetuning appears beneficial for performance within the same difficulty level all P problems, yet shows limited out-of-distribution (OOD) adaptability and struggles to generalize to more difficult problems (as evidenced in graphs a and c) except SAS.For instance, Qwen-14b demonstrates notable proficiency on SPP challenges at levels 1-10 following finetuning; its performance is comparable to that of GPT-4.However, its performance significantly diminishes on SPP problems at levels 11-20, even underperforming compared to its unfinetuned checkpoint.This indicates that finetuning on these benchmarks can only benefit very simple questions such as SAS but could potentially impede generalization capabilities and renders finetuning hacking useless.</p>
<p>In conclusion, our benchmark is challenging to hack due to two primary factors: (1) the inherent complexity of NP-complete and NP-hard problems, which are difficult to learn solely from questionanswer pairs, and (2) the propensity for P problems to become overfitted through finetuning on these pairs, while the real "reasoning" ability can be easily exposed by increasing the problem difficulty level.Based on these conclusions, we intend to periodically update our benchmarks strategetically with varying difficulty levels to minimize the potential for hacking.</p>
<p>Effects of Few-shot Examples' Difficulty on Reasoning Ability Enhancement</p>
<p>In Experiment 3, we focused on the tasks of SAS and EDP to investigate the nature of the in-context learning capabilities of LLMs.This experiment empirically distinguishes between "learning" and "mimicking" as exhibited by LLMs during in-context learning scenarios.Our findings also revealed a clear dichotomy in the approach to learning and generalization from examples between closed-source and open-source models.With regard to models' performance across different complexity classes and difficulty levels, all models show decreased accuracy and increased failure rates as task complexity rises, with a marked performance decay at NP-Hard complexity levels.But the transition from P to NP-Complete complexity did not uniformly affect model performance; while some models showed little difference, others exhibited significant performance variations.Specifically, models like GPT 4 Turbo and Claude Instant 1.2 showed noteworthy performance shifts between these two complexity classes.Detailed performance analysis across specific tasks revealed that certain models had strengths in particular types of tasks within each complexity category, with a notable decline in model performance as they addressed more complex NP-Hard tasks.</p>
<p>Finally, we used the tasks of SAS and EDP to understand how the difficulty of few-shot examples affects the in-context learning capabilities.Closed-source models like GPT 4 Turbo and Claude 2 demonstrated minimal performance variation and high consistency across different difficulty levels, suggesting a robust ability to learn algorithmic skills from examples.Conversely, open-source models showed varied adaptability, with some like Yi-34b and Mistral-7b performing well on more challenging examples but struggling with simpler ones.</p>
<p>Table 2: Weighted failure rate of Zero-shot and Few-shot on SAS and EDP.</p>
<p>Limitations</p>
<p>While our study offers a novel approach to assessing the reasoning abilities of LLMs, it is paramount to reflect on the limitations of our current methodology to provide a comprehensive understanding and guide future research.Task Complexity's Comparison A significant limitation lies in the scope of our task selection and the definition of complexity within our benchmark.While we have delineated criteria for task selection in the appendix, a more resource-intensive approach could involve the inclusion of a larger variety of questions for each task type, enhancing the depth and breadth of our evaluation.Additionally, our current approach to defining complexity is based on a linear increment of weights.This simplistic weighting heuristic may not accurately represent the nuanced complexity increase in real-world tasks.More experimental work is needed to refine this approach and determine the most effective weight assignment that truly reflects the intricacies of task complexity.</p>
<p>Randomness Another critical aspect to consider is the inherent randomness in the generation of responses by LLMs.This randomness can introduce variability in performance, making it challenging to draw consistent conclusions about a model's reasoning capabilities.Notably, decision questions in the NP-complete level, including GCP-D and TSP-D, use true or false results as the evaluation criteria.Thus, it is hard to directly rule out the random positive cases, although the model may not go through a correct reasoning process, leading to potentially inflated performance.Addressing this issue requires a more nuanced approach to evaluating responses, possibly through repeated trials or the incorporation of statistical methods to account for this variability.</p>
<p>Model Updates and Emergence</p>
<p>The fast-paced evolution of LLMs also presents a significant challenge.With the continuous version updates and emergence of advanced models like Gemini Ultra [49] and Phi-2 [50], as well as an increasing number of open-source options, the analysis based on our benchmark may quickly become outdated.Thus we will monitor and experiment on new models, together with the LLMs research community, to keep pace with these rapid developments is crucial for maintaining the relevance and applicability of our findings.This dynamic nature of the field necessitates a flexible and adaptable approach to benchmarking, where updates and revisions are integral to the evaluation process.</p>
<p>Future research should aim to expand the scope and depth of task selection, refine the complexity definition, account for generation randomness, and adapt to the evolving landscape of LLMs.Addressing these challenges will enhance the accuracy and relevance of our benchmark, contributing to the development of LLMs that are capable of sophisticated reasoning in complex, real-world scenarios.</p>
<p>Research Outlook</p>
<p>Our research outlook includes future investigations that can extend and enrich our understanding of the reasoning abilities of LLMs.</p>
<p>Fine-grained Time Complexity under Polynomial (P) with Big O notation We will further the investigation of the P complexity class with fine-grained time complexity notation, the Big O notation.For example, the time complexity of SAS is O(log n), while the time complexity of the Dijkstra algorithm, the solution to SPP, is O(V log V + E) with Fibonacci heaps [51].This approach will enable a detailed evaluation of models within the same complexity, proving a complement perspective to the current difficulty levels and enabling a possible cross-comparison among different tasks' difficulty levels.</p>
<p>Self-correction for Reasoning Another promising avenue is the enhancement of LLM reasoning abilities.A key strategy here is the implementation of iterative self-correction mechanisms.Pioneered by self-correction experiments in [52,53], allowing LLMs to go through multiple rounds (e.g., ranging from 1 to 10) of self-correction, we can observe how the refinement process affects the accuracy and sophistication of their responses.This iterative process mimics human problem-solving, where multiple drafts and revisions lead to improved outcomes.</p>
<p>Multi-agent Systems for Reasoning Moreover, exploring a multi-agent system [54,55,56,57] approach could significantly advance LLMs' reasoning abilities.In such a system, different LLM agents, each potentially specialized in certain types of reasoning or knowledge areas, collaborate to solve complex problems.This collaborative approach could mimic a team of experts, each contributing their expertise, leading to more comprehensive and nuanced solutions.It also opens the door to understanding how LLMs can interact and augment each other's capabilities, which is crucial for their application in real-world, multi-faceted problem-solving scenarios.These future research directions hold the potential not only to deepen our understanding of the current capabilities and limitations of LLMs but also to drive forward the development of more sophisticated and reliable AI systems.By focusing on robustness testing and enhancing reasoning abilities through innovative methods like iterative self-correction and multi-agent systems, we can make significant strides towards realizing the full potential of LLMs in complex decision-making and problem-solving tasks.</p>
<p>Second , the final output of all vertex numbers and their associated colors , wrapped by final_answer tag , like &lt; final_answer &gt; {0: ' COLOR_1 ' , 1: ' COLOR_2 ' , ...} &lt;/ final_answer &gt;. 14 The graph is below :</p>
<p>15 Vertex 1 is connected to vertex 6.</p>
<p>16</p>
<p>Vertex 2 is connected to vertex 6.</p>
<p>B Choices of Problems</p>
<p>In the benchmark, we exclude calculation-only (math intensive) tasks for each of the complexity classes, due to the overlap with already exist benchmarks and the known uncertainty of LLMs' math ability.For other reasoning, we provide detailed explanations and highlight them in bold.</p>
<p>B.1 Excluded P problems</p>
<p>Prime Number Determination Using algorithms like AKS primality test to determine if a given number is prime.Reason: Math-intensive.</p>
<p>Solving Linear Equations Finding solutions for a system of linear equations.Reason: Mathintensive.</p>
<p>Maximum Flow Problem Finding the maximum flow from a source node to a sink node in a flow network.A flow network is a directed graph G = (V, E) where each edge (u, v) ∈ E has a capacity c(u, v) and flow f (u, v), with a designated source s and sink t.The objective is to maximize the total flow from s to t under the constraints that the flow on an edge does not exceed its capacity and the incoming flow is equal to the outgoing flow for every vertex except s and t.Reason: Most open source algorithms cannot follow the question and the prompt to provide outputs with mostly correct formats.B.2 Excluded NP-Complete problems 3-SAT Problem Deciding whether a given Boolean formula in conjunctive normal form with three literals per clause is satisfiable.Reason: Math-intensive.</p>
<p>B.3 Excluded NP-hard problems</p>
<p>Integer Linear Programming Finding the best integer solution for a set of linear equations and inequalities.Reason: Math-intensive.</p>
<p>Figure 2 :
2
Figure 2: Zero-shot model performance on the nine tasks from P to NP-Complete bottom-up.</p>
<p>Figure 3 :
3
Figure 3: Model performance on different complexity problems: (a) weighted accuracy (b) (weighted) failure rate.Open models are denoted in squares and close models are denoted in triangles.Trends of metrics are demonstrated for models with outstanding performances in both weighted accuracy and failure rate, including both close-source (GPT 4 Turbo and Claude 2) and open-source (Mistral-7B and Phi-2) models.</p>
<p>Figure 4 :
4
Figure 4: Models' performance on each complexity level.(a) GPT 4 Turbo.(b) Claude 2. (c) GPT 3.5 Turbo.(d) Claude Instant 1.2.(e) PaLM 2. (f) Yi-34b.(g) Qwen-14b.(h) Mistral-7b.(i) Phi-2.(j) MPT-30b.(k) Vicuna-13b.(l) Phi-1.5.</p>
<p>Figure 5 :
5
Figure 5: Models' performance on tasks across complexity levels.(a) GPT 4 Turbo.(b) Claude 2. (c) GPT 3.5 Turbo.(d) Claude Instant 1.2.(e) PaLM 2. (f) Yi-34b.(g) Qwen-14b.(h) Mistral-7b.(i) Phi-2.(j) MPT-30b.(k) Vicuna-13b.(l) Phi-1.5.</p>
<p>Figure 6 :
6
Figure 6: Model's robustness on different problems and difficulty levels.</p>
<p>Figure 7 :
7
Figure 7: Heatmap of SAS and EDP task for each model.</p>
<p>17
17</p>
<p>Vertex 3
3
is connected to vertex 4.</p>
<p>18
18</p>
<p>Vertex 3
3
is connected to vertex 5.</p>
<p>19 20 # 24 &lt;
2024
Output ( formatted for easier reading ) 21 &lt; root &gt; 22 &lt; reasoning &gt; 23 Start with vertex 1 , color it A ; color adjacent vertex 3 with B ; vertex 2 can be A as it ' s not adjacent to 1; vertex 4 connected to 2 must be different , use B ; vertex 5 connected to both 2 and 3 , use C ; vertex 6 connected to 3 and 4 , use A .A ' , 2: 'A ' , 3: 'B ' , 4: 'B ' , 5: 'C ' , 6: 'A '} 27 &lt;/ final_answer &gt; 28 &lt;/ root &gt; Listing 2: GCP Example</p>
<p>•</p>
<p>Models possessing optimal generalization capabilities should demonstrate consistent performance improvement regardless of the difficulty level of the prompt examples in context.This assumption is based on the premise that models with robust learning abilities are capable of discerning and applying the intrinsic problem-solving skills learned in the examples.Given that questions within the same task fundamentally require similar skills, variations in difficulty are unlikely to significantly affect the model's performance.</p>
<p>• If a model exhibits the ability to generalize only from some types of examples but is unable to extend this learning to others, it reveals a deficiency in its capacity for generalization in terms of reasoning.This suggests that the model is not genuinely acquiring problem-solving skills from the examples but merely recognizing and applying patterns from examples that are of equal or greater complexity to the problem at hand.• If a model is unable to generalize from either more difficult or easier examples and is restricted to examples of the same difficulty level, it strongly suggests that the model is merely replicating the process presented in the context rather than internalizing any fundamental problem-solving techniques or pattern recognition embedded within the examples.This behavior indicates a profound deficiency in the model's ability to comprehend and understand the underlying principles.</p>
<p>https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam
We do not present the result on MSP as this problem does not have a fixed solution and we do not conduct finetuning on it.
Your output should contain two parts enclosed by &lt; root &gt; &lt;/ root &gt;. First , your step by step reasoning wraped by &lt; reasoning &gt; &lt;/ reasoning &gt;.
AcknowledgementWe extend our sincere gratitude to Libby Hemphill for her invaluable support in this work.We also thank Jinkui Chi, who kindly contributed to the maintenance of the benchmark's code repository and user guidance.Additionally, we are grateful for the diverse feedback we received from Siqi Liu and many others, which illuminated our path towards enhancing the quality of this paper.For closed-source models, including GPT 4 Turbo, Claude 2, GPT 3.5 Turbo, PaLM 2, and Claude Instant 1.2, the results were notably close to the ideal scenario.We observed minimal variation in performance across different levels of difficulty in the examples provided.This consistency suggests that these models are not merely mimicking the solutions but are indeed learning the algorithmic skills presented in the context of the examples.In contrast, the performance of open-source models, particularly Yi-34b and Mistral-7b, exhibits a clear pattern where the models generally generalize well from examples that are more challenging than the given question, yet they struggle to do so from simpler examples.Other open-source models display less distinct patterns, but a notable trend is still evident: these models demonstrate some capacity to generalize from more challenging to simpler questions, but they are less successful in generalizing from simpler to more complex questions.An exception is observed with the Phi-1.5 model in EDP, where it appears to generalize better from easier examples than from harder examples at certain difficulty levels.However, broadly speaking, none of the open-source models consistently learn from both harder and easier examples.The difficulty level significantly influences the models' performance, suggesting a tendency for these models to mimic patterns rather than engage in genuine learning from the context.This phenomenon underscores that the differentiation between powerful closed-source and opensource models lies not only in their raw reasoning ability but also significantly in their capacity to learn from in-context examples.This insight highlights the importance of considering both reasoning and learning abilities when evaluating the effectiveness and potential applications of LLMs.Conclusions and DiscussionIn this study, we present a novel benchmark, NPHardEval, designed to rigorously evaluate LLMs' reasoning capabilities across a spectrum of complex tasks, up to the complexity class of NP-hard.By eschewing standard QA formats in favor of complex, logic-oriented problems, this benchmark aims to provide a more accurate measure of a model's reasoning prowess.This approach is crucial for developing LLMs capable of handling sophisticated, real-world tasks that demand high-level cognitive processing, steering the evaluation of LLMs from potentially "useful" to fundamentally "logical".In addition to developing the benchmark, we compare different foundation models' reasoning ability across task complexity and experimented with different prompt styles to understand their in-context learnability.Our study reveals a notable disparity in performance between closed-source and opensource models not only on general reasoning ability but also the disparity between "learning" and "mimicking".To transform \ " cef \ " into \ " ccb \ " , we can follow these steps :\ n
A bibliometric review of large language models research from. Lizhou Fan, Lingyao Li, Zihui Ma, Sanggyu Lee, Huizi Yu, Libby Hemphill, arXiv:2304.020202017 to 2023. 2023arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, Tony Xia, arXiv:2305.12524Theoremqa: A theorem-driven question answering dataset. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>Pretraining on the test set is all you need. Rylan Schaeffer, arXiv:2309.086322023arXiv preprint</p>
<p>Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Alexis Chevalier, Julius Berner, arXiv:2301.13867Mathematical capabilities of chatgpt. 2023arXiv preprint</p>
<p>A catalog of complexity classes. Johnson David, Algorithms and complexity. Elsevier1990</p>
<p>Larger language models do in-context learning differently. Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, arXiv:2303.038462023arXiv preprint</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837Rethinking the role of demonstrations: What makes in-context learning work?. 2022arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Jie Huang, Kevin Chen, -Chuan Chang, arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>Datachat: Prototyping a conversational agent for dataset search and visualization. Lizhou Fan, Sara Lafia, Lingyao Li, Fangyuan Yang, Libby Hemphill, arXiv:2305.183582023arXiv preprint</p>
<p>Examining the potential of chatgpt on biomedical information retrieval: Fact-checking drugdisease associations. Zhenxiang Gao, Lingyao Li, Siyuan Ma, Qinyong Wang, Libby Hemphill, Rong Xu, Annals of Biomedical Engineering. 2023</p>
<p>Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill, arXiv:2304.10619hot" chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media. 2023arXiv preprint</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Rylan Schaeffer, Brando Miranda, Sanmi Koyejo, arXiv:2304.15004Are emergent abilities of large language models a mirage?. 2023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Iteratively prompt pre-trained language models for chain of thought. Boshi Wang, Xiang Deng, Huan Sun, arXiv:2203.083832022arXiv preprint</p>
<p>System 1+ system 2= better world: Neural-symbolic chain of logic reasoning. Wenyue Hua, Yongfeng Zhang, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Nils Maciej, Ales Blach, Robert Kubicek, Lukas Gerstenberger, Joanna Gianinazzi, Tomasz Gajda, Michal Lehmann, Hubert Podstawski, Piotr Niewiadomski, Nyczyk, arXiv:2308.096872023arXiv preprint</p>
<p>Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, Recmind, arXiv:2308.14296Large language model powered agent for recommendation. 2023arXiv preprint</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Large language models are reasoners with self-verification. Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, Jun Zhao, arXiv:2212.095612022arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, arXiv:2303.11366202314arXiv preprint</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Xiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, arXiv:2305.12474Liang He, and Xipeng Qiu. Evaluating the performance of large language models on gaokao benchmark. 2023arXiv preprint</p>
<p>Dyval: Graph-informed dynamic evaluation of large language models. Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie, arXiv:2309.171672023arXiv preprint</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, arXiv:2305.14387Alpacafarm: A simulation framework for methods that learn from human feedback. 2023arXiv preprint</p>
<p>Liang Xu, Anqi Li, Lei Zhu, Hang Xue, Changtai Zhu, Kangkang Zhao, Haonan He, Xuanwei Zhang, Qiyue Kang, Zhenzhong Lan, arXiv:2307.15020Superclue: A comprehensive chinese large language model benchmark. 2023arXiv preprint</p>
<p>Challenging bigbench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Ed H Quoc V Le, Denny Chi, Zhou, arXiv:2210.092612022arXiv preprint</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, arXiv:1903.00161Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. 2019arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.078302019arXiv preprint</p>
<p>Sosd: A benchmark for learned indexes. Andreas Kipf, Ryan Marcus, Alexander Van Renen, Mihail Stoian, Alfons Kemper, Tim Kraska, Thomas Neumann, 2019</p>
<p>Exact methods for the traveling salesman problem with drone. Roberto Roberti, Mario Ruthmair, Transportation Science. 5522021</p>
<p>Applications of graph coloring in modern computer science. Shamim Ahmed, International Journal of Computer and Information Technology. 322012</p>
<p>Competitive algorithms for the online multiple knapsack problem with application to electric vehicle charging. Bo Sun, Ali Zeynali, Tongxin Li, Mohammad Hajiesmaili, Adam Wierman, Danny Hk Tsang, Proceedings of the ACM on Measurement and Analysis of Computing Systems. 432020</p>
<p>The knapsack problem and its applications to the cargo loading problem. Michael Cho, Anal. Appl. Math. 132019</p>
<p>Register allocation with graph coloring by ant colony optimization. Carla Negri Lintzmayer, Mauro Henrique Mulati, Anderson Faustino Da Silva, 2011 30th International Conference of the Chilean Computer Science Society. IEEE2011</p>
<p>Constraint solving approaches to the business-to-business meeting scheduling problem. Miquel Bofill, Jordi Coll, Marc Garcia, Jesús Giráldez-Cru, Gilles Pesant, Josep Suy, Mateu Villaret, Journal of Artificial Intelligence Research. 742022</p>
<p>. Google Deepmind, Gemini, </p>
<p>. Mojan Javaheripi, Sébastien Bubeck, Phi-2</p>
<p>T H Cormen, C E Leiserson, R L Rivest, C Stein, Introduction to Algorithms. MIT Press2022fourth edition</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.123972023arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, Yongfeng Zhang, Openagi, arXiv:2304.04370When llm meets domain experts. 2023arXiv preprint</p>
<p>Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, Yongfeng Zhang, arXiv:2312.03815Llm as os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
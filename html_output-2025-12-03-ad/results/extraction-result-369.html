<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-369 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-369</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-369</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-251109261</p>
                <p><strong>Paper Title:</strong> Spatial relation learning in complementary scenarios with deep neural networks</p>
                <p><strong>Paper Abstract:</strong> A cognitive agent performing in the real world needs to learn relevant concepts about its environment (e.g., objects, color, and shapes) and react accordingly. In addition to learning the concepts, it needs to learn relations between the concepts, in particular spatial relations between objects. In this paper, we propose three approaches that allow a cognitive agent to learn spatial relations. First, using an embodied model, the agent learns to reach toward an object based on simple instructions involving left-right relations. Since the level of realism and its complexity does not permit large-scale and diverse experiences in this approach, we devise as a second approach a simple visual dataset for geometric feature learning and show that recent reasoning models can learn directional relations in different frames of reference. Yet, embodied and simple simulation approaches together still do not provide sufficient experiences. To close this gap, we thirdly propose utilizing knowledge bases for disembodied spatial relation reasoning. Since the three approaches (i.e., embodied learning, learning from simple visual data, and use of knowledge bases) are complementary, we conceptualize a cognitive architecture that combines these approaches in the context of spatial relation learning.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e369.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e369.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PVAE (embodied)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired Variational Autoencoders (embodied bidirectional model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bidirectional embodied model that jointly trains a language VAE and an action VAE with a binding loss and visual feature extractor (channel-separated CAE) to map between robot joint trajectories (proprioception + vision) and natural language descriptions, encoding spatial and procedural knowledge in shared latent variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embodied language learning with paired variational autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paired Variational Autoencoders (PVAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two variational autoencoders (language VAE and action VAE) whose latent codes are pulled together by a binding loss; visual features are extracted by a channel-separated convolutional autoencoder (CAE); action VAE decodes joint-angle trajectories conditioned on visual features and latent samples. Trained supervised on paired (action, description, visual) data.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Bidirectional action-language translation in a simulated manipulation scenario</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Robot (NICO) manipulates two cubes on a table; tasks: (1) translate executed joint-angle trajectories + visual features to textual descriptions (action->language) and (2) translate textual descriptions to predicted joint-angle trajectories (language->action). Scenes include left/right positional terms, color, speed; proprioception and egocentric images are available during training/testing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised embodied interaction data (simulated trajectories + egocentric images + paired language descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised paired training of VAEs (SGVB), sampling from latent Gaussian codes; evaluation via decoding in each direction</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>distributed latent variables in VAEs (shared latent space between language and action), visual features from CAE, joint-angle trajectories (procedural sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>description accuracy (for action->language), normalized RMSE (nRMSE) for trajectory reconstruction (language->action)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Action->Language: 100% description accuracy on 144 patterns (108 train / 36 test). Language->Action: nRMSE train=0.53, test=0.55 (average normalized RMSE between original and predicted joint trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Perfect mapping from joint trajectories+vision to short language descriptions in the constrained left/right cube scenario; latent codes captured one-to-many mappings (one action to multiple valid descriptions) due to VAE stochasticity; proprioception alone sufficed to infer left/right in this constrained setup.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited scenario complexity: only simple left/right relations and constrained kinematics; predicted joint trajectories were not executed in simulation due to potential divergences (contact dynamics, kinematic subtleties); not evaluated on more degrees-of-freedom or complex spatial layouts.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>PVAE with channel-separated CAE outperformed prior PRAE (Paired Recurrent Autoencoders) in one-to-many action->description mapping and color recognition; exact numerical baselines for PRAE not provided in this paper but qualitative improvement reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Channel-separated CAE (training CAE on each RGB channel separately and concatenating mid-layer features) improved color recognition and action->description mapping compared to non-separated CAE / PRAE; binding loss (bringing VAEs' latents closer) was critical for crossmodal association (qualitative claim, no numeric ablation table in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Spatial and procedural knowledge for manipulation can be encoded as shared latent representations between language and action VAEs; proprioceptive signals alone can carry egocentric spatial position (left/right) in constrained setups; stochastic latent sampling allows modeling one-to-many language descriptions for the same action.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e369.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Paired Recurrent Autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier bidirectional embodied model that fuses language and action modalities via recurrent autoencoders for translation between simple robotic manipulation actions and language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Paired Recurrent Autoencoders (PRAE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paired recurrent autoencoders that learn joint latent features for language and robot action sequences (recurrent encoder/decoder architecture) to perform bidirectional translation between actions and descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Bidirectional action-language translation (prior work / baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate between simple robot joint-action trajectories and fixed-format language descriptions in toy manipulation tasks (earlier model used as a baseline comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised paired trajectories and descriptions</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised training of paired recurrent autoencoders</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>recurrent latent spaces encoding sequences (implicit knowledge in weights)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>one-to-many translation success (qualitative comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Capable of bidirectional translation in simple setups (reported in prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Less successful than PVAE for one-to-many action->description mapping and color-disambiguation according to this paper's claims.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as prior baseline for PVAE; PVAE reported superior performance (quantitative PVAE gains reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrent paired autoencoders can learn crossmodal mappings but variational formulations (PVAE) and channel-separated visual features improved one-to-many mapping and perceptual disentanglement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e369.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QDRL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qualitative Directional Relation Learning (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthetic dataset for testing directional spatial-relation reasoning under multiple frames of reference (absolute, intrinsic, relative), consisting of (image, question, true/false) triples with controlled entity placements and orientations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiLM and MAC (evaluated on QDRL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>QDRL is a generated dataset (128x128 RGB images with oriented emoji entities) that elicits models' capacity to learn oriented bounding boxes and frame-of-reference-dependent relations; it is used to train/evaluate end-to-end visual reasoning models (FiLM, MAC).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Directional relation verification (QDRL benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a synthetic scene image and a relation triple (head, relation, tail) (and in relative FoR also a source entity), predict whether the relation holds true under the specified frame of reference (absolute/intrinsic/relative); scenes vary number of entities (2,3,5).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual relational reasoning / spatial relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (frame-of-reference: axis-aligned bounding boxes, oriented bounding boxes, relative-direction alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised synthetic image-question-answer pairs (no external KBs required)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised training of end-to-end neural visual reasoning models (CNN+RNN text encoders, FiLM/MAC architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in CNN visual features and RNN question embeddings; decision boundaries correspond to learned (oriented) bounding-box-like filters in internal representations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>classification accuracy on validation and compositional (held-out entity combinations) validation sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>FiLM validation accuracies near 0.98-0.996 across FoRs; FiLM compositional accuracies drop (absolute: ~0.912-0.933; intrinsic: ~0.862-0.882; relative (3 entities): 0.745). MAC validation accuracies ~0.97-0.992; MAC compositional accuracies higher and more stable (absolute: ~0.929-0.958; intrinsic: ~0.927-0.937; relative: 0.975). (Values from Table 2 in the paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Models can learn absolute-frame relations well; both FiLM and MAC learn axis-aligned and oriented cues when trained; MAC shows stronger compositional generalization, especially for relative-frame relations where direction depends on a pairwise source-target vector.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>FiLM struggles more with compositional generalization in intrinsic and relative frames (orientation-dependent relations); intrinsic and relative frames are harder because models must learn orientation cues or compute directions between entity centers.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Comparison between FiLM and MAC; MAC consistently outperformed FiLM on compositional generalization sets, especially for relative frames (e.g., FiLM comp=0.745 vs MAC comp=0.975 on relative 3-entity case).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not explicit per-module ablations in the QDRL experiments beyond hyperparameter tuning (number of FiLM blocks / MAC cells); curriculum pretraining (intrinsic -> relative) improved FiLM compositional accuracy markedly (from 0.745 to ~0.9 reported in narrative).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>End-to-end visual reasoning models can learn different frames of reference, but intrinsic and relative FoRs require representing orientation information; MAC's control-memory recurrent reasoning yields superior compositional generalization for relation tasks requiring relational orientation computations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e369.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FiLM (Feature-wise Linear Modulation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual reasoning architecture that modulates CNN feature maps via affine transformations whose parameters are predicted from the question encoding, allowing conditional visual feature selection for relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FiLM: visual reasoning with a general conditioning layer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FiLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Processes image with a ResNet-like stack; at each block, pre-activation outputs are affine-transformed (scale+shift) by parameters linearly predicted from question RNN embeddings, thereby conditioning visual processing on language.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QDRL directional relation classification (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given image and relation-question, predict true/false for spatial relation under given frame of reference; trained end-to-end on QDRL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual relational reasoning / spatial relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (frame-of-reference dependent)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised training on QDRL synthetic image-question-answer pairs</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised end-to-end training; question-conditioned modulation of visual features</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in CNN feature maps modulated by question-conditioned affine parameters (weights encode relational/positional cues)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on validation and compositional validation sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Validation accuracies near 0.98-0.996 across settings; compositional validation accuracies lower, e.g., absolute 2-entity comp ~0.912, intrinsic comp ~0.862-0.882, relative (3 entities) comp = 0.745 (from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learns axis-aligned spatial relations well in absolute FoR and generalizes within training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Weaker compositional generalization for unseen entity combinations, particularly for relative-frame (direction computed between two entities) scenes and intrinsic-frame orientation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to MAC on QDRL; MAC outperforms FiLM on compositional validation, especially for relative FoR.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Pretraining curricula (intrinsic then relative) improved FiLM's compositional performance substantially (reported narrative improvement from 0.745 to ~0.9), indicating initialization matters.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FiLM's conditional modulation enables strong within-distribution spatial reasoning, but it is more brittle on compositional generalization and orientation-dependent relations than MAC.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e369.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MAC (Memory, Attention, and Control networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A visual reasoning architecture that models stepwise reasoning with a recurrent control unit that selects visual information and a memory unit that accumulates intermediate reasoning results, suitable for compositional relational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Compositional attention networks for machine reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MAC network</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MAC uses a recurrent sequence of reasoning steps (cells). Each step has a control state (decides which parts of question to attend) and a memory state (aggregates retrieved visual information). This supports multi-step compositional reasoning over images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QDRL directional relation classification (evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same QDRL relation verification task; MAC trained end-to-end to answer true/false relation queries under given FoR.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>visual relational reasoning / spatial relation classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (frame-of-reference dependent) and relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>supervised synthetic QDRL training data</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>supervised end-to-end training; iterative control-memory attention steps to extract visual relational evidence</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit in recurrent control and memory states that attend to image regions and accumulate relational facts across steps</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy on validation and compositional validation sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Validation accuracies ~0.967-0.992; compositional validation accuracies higher than FiLM (absolute comp ~0.929-0.958; intrinsic comp ~0.927-0.937; relative 3-entity comp = 0.975) (from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Strong compositional generalization; particularly robust on relative-frame relations where relational direction must be computed between entities, indicating successful encoding of oriented relational computations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Still challenged when training distribution lacks sufficient variety, but overall much smaller validation-to-compositional gap than FiLM.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms FiLM across compositional validation sets, especially for relative FoR (e.g., MAC comp 0.975 vs FiLM comp 0.745 on relative 3-entity).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Paper reports MAC hyperparameter tuning (number of MAC cells) but no detailed per-component ablations; overall MAC's recurrent reasoning mechanism is implicated in its stronger compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative control-memory architectures like MAC encode relational and orientation-requiring spatial knowledge more robustly and generalize better to unseen entity combinations than single-pass modulation architectures (FiLM) on QDRL.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e369.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Visual Distant Supervision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Visual Distant Supervision for scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique aligning commonsense knowledge-base relation triples with unlabeled images to create distantly supervised training data for visual relation detectors, then denoising labels via EM to mitigate noise from KB-image mismatches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual distant supervision for scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Visual distant supervision pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automatically creates candidate relation labels for object pairs in images by looking up relation triples from large commonsense knowledge bases; trains relation classifiers on these noisy labels and applies an EM-based denoising framework to refine probabilistic labels.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Visual relation detection / scene graph generation (pretraining source)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given detected object categories (and optionally bounding boxes), predict relations (triples) between object pairs using distantly labeled data derived from KBs aligned to images.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object-relational learning / scene graph generation</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (commonsense relations; some implicit spatial info)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit knowledge bases (ConceptNet, Visual Genome, other KBs) aligned to images (distant supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>distant supervision (automatic label assignment using KB lookups) and EM-based denoising; used to pretrain relation detectors</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>relation label candidates and learned relation classifier weights; downstream relation embeddings can be transferred to spatial relation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>improvements in relation detection metrics vs semi-supervised baselines (qualitative summary in paper; original ICCV'21 paper reports strong performance but exact numbers not reproduced here)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Provides large-scale prior relation knowledge for relation detectors without human labeling; pretraining on distantly labeled data improves downstream supervised relation detectors when label noise is addressed via denoising.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Distant supervision label noise due to KB relations not matching specific image spatial layouts (e.g., KB relation 'beside' for bowl+plant may be spurious); limited direct usefulness for fine-grained directional spatial relations (left/right) because KB triples often lack directional detail.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to semi-supervised relation detectors that rely on seed human annotations; visual distant supervision yields strong performance improvements when EM denoising is applied (specific baselines and numbers are in Yao et al., 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>EM denoising reduces noise impact; when human-labeled data is available, pretraining on distantly labelled data still helps (qualitative summary in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Commonsense KBs can supply disembodied object-relational priors useful for visual relation learning, but they are noisy for directional spatial specifics; denoising and transfer mechanisms are required to make KB-derived knowledge actionable for spatial tasks, especially when direct sensory input is limited or unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e369.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e369.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN Integration (KB -> model)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Neural Network integration of disembodied knowledge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed mechanism to inject structured commonsense knowledge (knowledge-base triples) into neural embodied/spatial models by mapping activated concepts to graph nodes and running a trainable GNN whose outputs are fused into the central joint representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Neural Network (as KB encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Represent commonsense knowledge as a sparse graph (nodes=concepts, edges=relations); activate nodes corresponding to perceived objects or instruction words; run message-passing updates and read out embeddings that are connected to the main multimodal model (trainable input/readout layers).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Integration of disembodied commonsense knowledge into embodied spatial reasoning (conceptual component)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>At inference/training time, retrieve relevant KB triples for visual/language inputs, instantiate a subgraph, compute node embeddings via GNN, and fuse these embeddings into joint multimodal representation to bias expectations (e.g., expect 'cup on table') even when sensory input is incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge integration for object-relational priors / expectation setting</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational + spatial priors (implicit)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>explicit knowledge bases (ConceptNet, Visual Genome) converted to sparse graph structures and embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>retrieval of relevant triples followed by GNN message-passing; trainable fusion into main network (proposed conceptual integration, not fully implemented/evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit graph structure (nodes/vectors, edges as relation-specific networks), then converted to embeddings read into central model</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Conceptually provides a way to supply disembodied priors (typical object locations/relations) to an embodied agent, which can guide search or expectation when sensory input is missing or partial.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Paper presents a concept only; no empirical results here; known risks include sparsity of KB links, noisy/mismatched relations, and the need for supervised mapping between perception/language tokens and KB nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Structured KB knowledge can be injected via GNNs to enhance spatial reasoning and set priors when sensory input is absent, but practical integration requires retrieval, supervised alignment, and joint training to be effective.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spatial relation learning in complementary scenarios with deep neural networks', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Embodied language learning with paired variational autoencoders <em>(Rating: 2)</em></li>
                <li>Visual distant supervision for scene graph generation <em>(Rating: 2)</em></li>
                <li>FiLM: visual reasoning with a general conditioning layer <em>(Rating: 2)</em></li>
                <li>Compositional attention networks for machine reasoning <em>(Rating: 2)</em></li>
                <li>Learning representations specialized in spatial knowledge: leveraging language and vision <em>(Rating: 2)</em></li>
                <li>Experience grounds language <em>(Rating: 2)</em></li>
                <li>Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions <em>(Rating: 2)</em></li>
                <li>ConceptNet 5.5: an open multilingual graph of general knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-369",
    "paper_id": "paper-251109261",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "PVAE (embodied)",
            "name_full": "Paired Variational Autoencoders (embodied bidirectional model)",
            "brief_description": "A bidirectional embodied model that jointly trains a language VAE and an action VAE with a binding loss and visual feature extractor (channel-separated CAE) to map between robot joint trajectories (proprioception + vision) and natural language descriptions, encoding spatial and procedural knowledge in shared latent variables.",
            "citation_title": "Embodied language learning with paired variational autoencoders",
            "mention_or_use": "use",
            "model_name": "Paired Variational Autoencoders (PVAE)",
            "model_size": null,
            "model_description": "Two variational autoencoders (language VAE and action VAE) whose latent codes are pulled together by a binding loss; visual features are extracted by a channel-separated convolutional autoencoder (CAE); action VAE decodes joint-angle trajectories conditioned on visual features and latent samples. Trained supervised on paired (action, description, visual) data.",
            "task_name": "Bidirectional action-language translation in a simulated manipulation scenario",
            "task_description": "Robot (NICO) manipulates two cubes on a table; tasks: (1) translate executed joint-angle trajectories + visual features to textual descriptions (action-&gt;language) and (2) translate textual descriptions to predicted joint-angle trajectories (language-&gt;action). Scenes include left/right positional terms, color, speed; proprioception and egocentric images are available during training/testing.",
            "task_type": "object manipulation / instruction following",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "supervised embodied interaction data (simulated trajectories + egocentric images + paired language descriptions)",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised paired training of VAEs (SGVB), sampling from latent Gaussian codes; evaluation via decoding in each direction",
            "knowledge_representation": "distributed latent variables in VAEs (shared latent space between language and action), visual features from CAE, joint-angle trajectories (procedural sequences)",
            "performance_metric": "description accuracy (for action-&gt;language), normalized RMSE (nRMSE) for trajectory reconstruction (language-&gt;action)",
            "performance_result": "Action-&gt;Language: 100% description accuracy on 144 patterns (108 train / 36 test). Language-&gt;Action: nRMSE train=0.53, test=0.55 (average normalized RMSE between original and predicted joint trajectories).",
            "success_patterns": "Perfect mapping from joint trajectories+vision to short language descriptions in the constrained left/right cube scenario; latent codes captured one-to-many mappings (one action to multiple valid descriptions) due to VAE stochasticity; proprioception alone sufficed to infer left/right in this constrained setup.",
            "failure_patterns": "Limited scenario complexity: only simple left/right relations and constrained kinematics; predicted joint trajectories were not executed in simulation due to potential divergences (contact dynamics, kinematic subtleties); not evaluated on more degrees-of-freedom or complex spatial layouts.",
            "baseline_comparison": "PVAE with channel-separated CAE outperformed prior PRAE (Paired Recurrent Autoencoders) in one-to-many action-&gt;description mapping and color recognition; exact numerical baselines for PRAE not provided in this paper but qualitative improvement reported.",
            "ablation_results": "Channel-separated CAE (training CAE on each RGB channel separately and concatenating mid-layer features) improved color recognition and action-&gt;description mapping compared to non-separated CAE / PRAE; binding loss (bringing VAEs' latents closer) was critical for crossmodal association (qualitative claim, no numeric ablation table in this paper).",
            "key_findings": "Spatial and procedural knowledge for manipulation can be encoded as shared latent representations between language and action VAEs; proprioceptive signals alone can carry egocentric spatial position (left/right) in constrained setups; stochastic latent sampling allows modeling one-to-many language descriptions for the same action.",
            "uuid": "e369.0",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "PRAE",
            "name_full": "Paired Recurrent Autoencoders",
            "brief_description": "An earlier bidirectional embodied model that fuses language and action modalities via recurrent autoencoders for translation between simple robotic manipulation actions and language descriptions.",
            "citation_title": "Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions",
            "mention_or_use": "mention",
            "model_name": "Paired Recurrent Autoencoders (PRAE)",
            "model_size": null,
            "model_description": "Paired recurrent autoencoders that learn joint latent features for language and robot action sequences (recurrent encoder/decoder architecture) to perform bidirectional translation between actions and descriptions.",
            "task_name": "Bidirectional action-language translation (prior work / baseline)",
            "task_description": "Translate between simple robot joint-action trajectories and fixed-format language descriptions in toy manipulation tasks (earlier model used as a baseline comparison).",
            "task_type": "object manipulation / instruction following",
            "knowledge_type": "spatial+procedural",
            "knowledge_source": "supervised paired trajectories and descriptions",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised training of paired recurrent autoencoders",
            "knowledge_representation": "recurrent latent spaces encoding sequences (implicit knowledge in weights)",
            "performance_metric": "one-to-many translation success (qualitative comparison)",
            "performance_result": null,
            "success_patterns": "Capable of bidirectional translation in simple setups (reported in prior work).",
            "failure_patterns": "Less successful than PVAE for one-to-many action-&gt;description mapping and color-disambiguation according to this paper's claims.",
            "baseline_comparison": "Used as prior baseline for PVAE; PVAE reported superior performance (quantitative PVAE gains reported in this paper).",
            "ablation_results": null,
            "key_findings": "Recurrent paired autoencoders can learn crossmodal mappings but variational formulations (PVAE) and channel-separated visual features improved one-to-many mapping and perceptual disentanglement.",
            "uuid": "e369.1",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "QDRL",
            "name_full": "Qualitative Directional Relation Learning (dataset)",
            "brief_description": "A synthetic dataset for testing directional spatial-relation reasoning under multiple frames of reference (absolute, intrinsic, relative), consisting of (image, question, true/false) triples with controlled entity placements and orientations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FiLM and MAC (evaluated on QDRL)",
            "model_size": null,
            "model_description": "QDRL is a generated dataset (128x128 RGB images with oriented emoji entities) that elicits models' capacity to learn oriented bounding boxes and frame-of-reference-dependent relations; it is used to train/evaluate end-to-end visual reasoning models (FiLM, MAC).",
            "task_name": "Directional relation verification (QDRL benchmark)",
            "task_description": "Given a synthetic scene image and a relation triple (head, relation, tail) (and in relative FoR also a source entity), predict whether the relation holds true under the specified frame of reference (absolute/intrinsic/relative); scenes vary number of entities (2,3,5).",
            "task_type": "visual relational reasoning / spatial relation classification",
            "knowledge_type": "spatial (frame-of-reference: axis-aligned bounding boxes, oriented bounding boxes, relative-direction alignment)",
            "knowledge_source": "supervised synthetic image-question-answer pairs (no external KBs required)",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised training of end-to-end neural visual reasoning models (CNN+RNN text encoders, FiLM/MAC architectures)",
            "knowledge_representation": "implicit in CNN visual features and RNN question embeddings; decision boundaries correspond to learned (oriented) bounding-box-like filters in internal representations",
            "performance_metric": "classification accuracy on validation and compositional (held-out entity combinations) validation sets",
            "performance_result": "FiLM validation accuracies near 0.98-0.996 across FoRs; FiLM compositional accuracies drop (absolute: ~0.912-0.933; intrinsic: ~0.862-0.882; relative (3 entities): 0.745). MAC validation accuracies ~0.97-0.992; MAC compositional accuracies higher and more stable (absolute: ~0.929-0.958; intrinsic: ~0.927-0.937; relative: 0.975). (Values from Table 2 in the paper.)",
            "success_patterns": "Models can learn absolute-frame relations well; both FiLM and MAC learn axis-aligned and oriented cues when trained; MAC shows stronger compositional generalization, especially for relative-frame relations where direction depends on a pairwise source-target vector.",
            "failure_patterns": "FiLM struggles more with compositional generalization in intrinsic and relative frames (orientation-dependent relations); intrinsic and relative frames are harder because models must learn orientation cues or compute directions between entity centers.",
            "baseline_comparison": "Comparison between FiLM and MAC; MAC consistently outperformed FiLM on compositional generalization sets, especially for relative frames (e.g., FiLM comp=0.745 vs MAC comp=0.975 on relative 3-entity case).",
            "ablation_results": "Not explicit per-module ablations in the QDRL experiments beyond hyperparameter tuning (number of FiLM blocks / MAC cells); curriculum pretraining (intrinsic -&gt; relative) improved FiLM compositional accuracy markedly (from 0.745 to ~0.9 reported in narrative).",
            "key_findings": "End-to-end visual reasoning models can learn different frames of reference, but intrinsic and relative FoRs require representing orientation information; MAC's control-memory recurrent reasoning yields superior compositional generalization for relation tasks requiring relational orientation computations.",
            "uuid": "e369.2",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "FiLM",
            "name_full": "FiLM (Feature-wise Linear Modulation)",
            "brief_description": "A visual reasoning architecture that modulates CNN feature maps via affine transformations whose parameters are predicted from the question encoding, allowing conditional visual feature selection for relational reasoning.",
            "citation_title": "FiLM: visual reasoning with a general conditioning layer",
            "mention_or_use": "use",
            "model_name": "FiLM",
            "model_size": null,
            "model_description": "Processes image with a ResNet-like stack; at each block, pre-activation outputs are affine-transformed (scale+shift) by parameters linearly predicted from question RNN embeddings, thereby conditioning visual processing on language.",
            "task_name": "QDRL directional relation classification (evaluated)",
            "task_description": "Given image and relation-question, predict true/false for spatial relation under given frame of reference; trained end-to-end on QDRL.",
            "task_type": "visual relational reasoning / spatial relation classification",
            "knowledge_type": "spatial (frame-of-reference dependent)",
            "knowledge_source": "supervised training on QDRL synthetic image-question-answer pairs",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised end-to-end training; question-conditioned modulation of visual features",
            "knowledge_representation": "implicit in CNN feature maps modulated by question-conditioned affine parameters (weights encode relational/positional cues)",
            "performance_metric": "accuracy on validation and compositional validation sets",
            "performance_result": "Validation accuracies near 0.98-0.996 across settings; compositional validation accuracies lower, e.g., absolute 2-entity comp ~0.912, intrinsic comp ~0.862-0.882, relative (3 entities) comp = 0.745 (from Table 2).",
            "success_patterns": "Learns axis-aligned spatial relations well in absolute FoR and generalizes within training distribution.",
            "failure_patterns": "Weaker compositional generalization for unseen entity combinations, particularly for relative-frame (direction computed between two entities) scenes and intrinsic-frame orientation reasoning.",
            "baseline_comparison": "Compared directly to MAC on QDRL; MAC outperforms FiLM on compositional validation, especially for relative FoR.",
            "ablation_results": "Pretraining curricula (intrinsic then relative) improved FiLM's compositional performance substantially (reported narrative improvement from 0.745 to ~0.9), indicating initialization matters.",
            "key_findings": "FiLM's conditional modulation enables strong within-distribution spatial reasoning, but it is more brittle on compositional generalization and orientation-dependent relations than MAC.",
            "uuid": "e369.3",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "MAC",
            "name_full": "MAC (Memory, Attention, and Control networks)",
            "brief_description": "A visual reasoning architecture that models stepwise reasoning with a recurrent control unit that selects visual information and a memory unit that accumulates intermediate reasoning results, suitable for compositional relational tasks.",
            "citation_title": "Compositional attention networks for machine reasoning",
            "mention_or_use": "use",
            "model_name": "MAC network",
            "model_size": null,
            "model_description": "MAC uses a recurrent sequence of reasoning steps (cells). Each step has a control state (decides which parts of question to attend) and a memory state (aggregates retrieved visual information). This supports multi-step compositional reasoning over images.",
            "task_name": "QDRL directional relation classification (evaluated)",
            "task_description": "Same QDRL relation verification task; MAC trained end-to-end to answer true/false relation queries under given FoR.",
            "task_type": "visual relational reasoning / spatial relation classification",
            "knowledge_type": "spatial (frame-of-reference dependent) and relational",
            "knowledge_source": "supervised synthetic QDRL training data",
            "has_direct_sensory_input": true,
            "elicitation_method": "supervised end-to-end training; iterative control-memory attention steps to extract visual relational evidence",
            "knowledge_representation": "implicit in recurrent control and memory states that attend to image regions and accumulate relational facts across steps",
            "performance_metric": "accuracy on validation and compositional validation sets",
            "performance_result": "Validation accuracies ~0.967-0.992; compositional validation accuracies higher than FiLM (absolute comp ~0.929-0.958; intrinsic comp ~0.927-0.937; relative 3-entity comp = 0.975) (from Table 2).",
            "success_patterns": "Strong compositional generalization; particularly robust on relative-frame relations where relational direction must be computed between entities, indicating successful encoding of oriented relational computations.",
            "failure_patterns": "Still challenged when training distribution lacks sufficient variety, but overall much smaller validation-to-compositional gap than FiLM.",
            "baseline_comparison": "Outperforms FiLM across compositional validation sets, especially for relative FoR (e.g., MAC comp 0.975 vs FiLM comp 0.745 on relative 3-entity).",
            "ablation_results": "Paper reports MAC hyperparameter tuning (number of MAC cells) but no detailed per-component ablations; overall MAC's recurrent reasoning mechanism is implicated in its stronger compositional generalization.",
            "key_findings": "Iterative control-memory architectures like MAC encode relational and orientation-requiring spatial knowledge more robustly and generalize better to unseen entity combinations than single-pass modulation architectures (FiLM) on QDRL.",
            "uuid": "e369.4",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Visual Distant Supervision",
            "name_full": "Visual Distant Supervision for scene graph generation",
            "brief_description": "A technique aligning commonsense knowledge-base relation triples with unlabeled images to create distantly supervised training data for visual relation detectors, then denoising labels via EM to mitigate noise from KB-image mismatches.",
            "citation_title": "Visual distant supervision for scene graph generation",
            "mention_or_use": "use",
            "model_name": "Visual distant supervision pipeline",
            "model_size": null,
            "model_description": "Automatically creates candidate relation labels for object pairs in images by looking up relation triples from large commonsense knowledge bases; trains relation classifiers on these noisy labels and applies an EM-based denoising framework to refine probabilistic labels.",
            "task_name": "Visual relation detection / scene graph generation (pretraining source)",
            "task_description": "Given detected object categories (and optionally bounding boxes), predict relations (triples) between object pairs using distantly labeled data derived from KBs aligned to images.",
            "task_type": "object-relational learning / scene graph generation",
            "knowledge_type": "object-relational (commonsense relations; some implicit spatial info)",
            "knowledge_source": "explicit knowledge bases (ConceptNet, Visual Genome, other KBs) aligned to images (distant supervision)",
            "has_direct_sensory_input": true,
            "elicitation_method": "distant supervision (automatic label assignment using KB lookups) and EM-based denoising; used to pretrain relation detectors",
            "knowledge_representation": "relation label candidates and learned relation classifier weights; downstream relation embeddings can be transferred to spatial relation tasks",
            "performance_metric": "improvements in relation detection metrics vs semi-supervised baselines (qualitative summary in paper; original ICCV'21 paper reports strong performance but exact numbers not reproduced here)",
            "performance_result": null,
            "success_patterns": "Provides large-scale prior relation knowledge for relation detectors without human labeling; pretraining on distantly labeled data improves downstream supervised relation detectors when label noise is addressed via denoising.",
            "failure_patterns": "Distant supervision label noise due to KB relations not matching specific image spatial layouts (e.g., KB relation 'beside' for bowl+plant may be spurious); limited direct usefulness for fine-grained directional spatial relations (left/right) because KB triples often lack directional detail.",
            "baseline_comparison": "Compared to semi-supervised relation detectors that rely on seed human annotations; visual distant supervision yields strong performance improvements when EM denoising is applied (specific baselines and numbers are in Yao et al., 2021).",
            "ablation_results": "EM denoising reduces noise impact; when human-labeled data is available, pretraining on distantly labelled data still helps (qualitative summary in this paper).",
            "key_findings": "Commonsense KBs can supply disembodied object-relational priors useful for visual relation learning, but they are noisy for directional spatial specifics; denoising and transfer mechanisms are required to make KB-derived knowledge actionable for spatial tasks, especially when direct sensory input is limited or unavailable.",
            "uuid": "e369.5",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GNN Integration (KB -&gt; model)",
            "name_full": "Graph Neural Network integration of disembodied knowledge",
            "brief_description": "A proposed mechanism to inject structured commonsense knowledge (knowledge-base triples) into neural embodied/spatial models by mapping activated concepts to graph nodes and running a trainable GNN whose outputs are fused into the central joint representation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Graph Neural Network (as KB encoder)",
            "model_size": null,
            "model_description": "Represent commonsense knowledge as a sparse graph (nodes=concepts, edges=relations); activate nodes corresponding to perceived objects or instruction words; run message-passing updates and read out embeddings that are connected to the main multimodal model (trainable input/readout layers).",
            "task_name": "Integration of disembodied commonsense knowledge into embodied spatial reasoning (conceptual component)",
            "task_description": "At inference/training time, retrieve relevant KB triples for visual/language inputs, instantiate a subgraph, compute node embeddings via GNN, and fuse these embeddings into joint multimodal representation to bias expectations (e.g., expect 'cup on table') even when sensory input is incomplete.",
            "task_type": "knowledge integration for object-relational priors / expectation setting",
            "knowledge_type": "object-relational + spatial priors (implicit)",
            "knowledge_source": "explicit knowledge bases (ConceptNet, Visual Genome) converted to sparse graph structures and embeddings",
            "has_direct_sensory_input": null,
            "elicitation_method": "retrieval of relevant triples followed by GNN message-passing; trainable fusion into main network (proposed conceptual integration, not fully implemented/evaluated in this paper)",
            "knowledge_representation": "explicit graph structure (nodes/vectors, edges as relation-specific networks), then converted to embeddings read into central model",
            "performance_metric": null,
            "performance_result": null,
            "success_patterns": "Conceptually provides a way to supply disembodied priors (typical object locations/relations) to an embodied agent, which can guide search or expectation when sensory input is missing or partial.",
            "failure_patterns": "Paper presents a concept only; no empirical results here; known risks include sparsity of KB links, noisy/mismatched relations, and the need for supervised mapping between perception/language tokens and KB nodes.",
            "baseline_comparison": null,
            "ablation_results": null,
            "key_findings": "Structured KB knowledge can be injected via GNNs to enhance spatial reasoning and set priors when sensory input is absent, but practical integration requires retrieval, supervised alignment, and joint training to be effective.",
            "uuid": "e369.6",
            "source_info": {
                "paper_title": "Spatial relation learning in complementary scenarios with deep neural networks",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Embodied language learning with paired variational autoencoders",
            "rating": 2,
            "sanitized_title": "embodied_language_learning_with_paired_variational_autoencoders"
        },
        {
            "paper_title": "Visual distant supervision for scene graph generation",
            "rating": 2,
            "sanitized_title": "visual_distant_supervision_for_scene_graph_generation"
        },
        {
            "paper_title": "FiLM: visual reasoning with a general conditioning layer",
            "rating": 2,
            "sanitized_title": "film_visual_reasoning_with_a_general_conditioning_layer"
        },
        {
            "paper_title": "Compositional attention networks for machine reasoning",
            "rating": 2,
            "sanitized_title": "compositional_attention_networks_for_machine_reasoning"
        },
        {
            "paper_title": "Learning representations specialized in spatial knowledge: leveraging language and vision",
            "rating": 2,
            "sanitized_title": "learning_representations_specialized_in_spatial_knowledge_leveraging_language_and_vision"
        },
        {
            "paper_title": "Experience grounds language",
            "rating": 2,
            "sanitized_title": "experience_grounds_language"
        },
        {
            "paper_title": "Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions",
            "rating": 2,
            "sanitized_title": "paired_recurrent_autoencoders_for_bidirectional_translation_between_robot_actions_and_linguistic_descriptions"
        },
        {
            "paper_title": "ConceptNet 5.5: an open multilingual graph of general knowledge",
            "rating": 1,
            "sanitized_title": "conceptnet_55_an_open_multilingual_graph_of_general_knowledge"
        }
    ],
    "cost": 0.019903249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>OPEN ACCESS EDITED BY</p>
<p>Akira Taniguchi 
Ritsumeikan University
Japan REVIEWED BY</p>
<p>Nikhil Krishnaswamy 
Colorado State University
United States</p>
<p>Sina Ardabili 
University of Mohaghegh
ArdabiliIran</p>
<p>Jae Hee Lee 
CORRESPONDENCE
Department of Informatics
Natural Language Processing Lab
Department of Computer Science and Technology
Knowledge Technology Group
University of Hamburg
HamburgGermany</p>
<p>Lee Jh 
Tsinghua University
BeijingChina</p>
<p>Yao Y 
Tsinghua University
BeijingChina</p>
<p>zdemir O Li 
Tsinghua University
BeijingChina</p>
<p>M 
Tsinghua University
BeijingChina</p>
<p>Weber C 
Tsinghua University
BeijingChina</p>
<p>Liu Z 
Tsinghua University
BeijingChina</p>
<p>Wermter S 
Tsinghua University
BeijingChina</p>
<p>OPEN ACCESS EDITED BY
TYPE Original Research PUBLISHED</p>
<p>. Introduction</p>
<p>Spatial concepts and relations are essential for agents perceiving and acting in the physical space. Because of the ubiquitous nature of spatial concepts and relations, it is plausible from a developmental point of view to believe that they are "among the first to be formed in natural cognitive agents" (Freksa, 2004). Endowing an artificial cognitive agent with the capability to reliably handle spatial concepts and relations can thus be regarded as an important task in AI and, in particular, in deep learning, which has become a predominant paradigm in AI (LeCun et al., 2015).</p>
<p>In this paper, we present three different but complementary approaches to spatial relation learning with deep neural networks and propose a way to integrate them. In the first approach, a robotic agent collects experiences in its environment, learning about space in an embodied way. This approach allows the agent to ground the embodied . /fnbot. . experience similar to how humans would do and help the agent learn more accurate linguistic concepts suitable for humanrobot interaction (Bisk et al., 2020). In such a learning setup, however, the variety of experiences is limited due to multiple factors such as exploration costs, limited complexity of explored environments, robot limitations in sensory, processing, and physical capabilities, which are not only present in the real physical environments, but also to a lesser extent in simulated environments. To increase the variety of experiences, further approaches are required. The second approach utilizes computer-generated largescale image data for spatial relation learning. An immediate advantage of this approach is that data generation is cheaper than in the first approach, such that training a large model with millions of samples is possible. This allows learning of complex relations, which can depend on different frames of reference. However, concerning detail, the simplified sensory input is insufficient for embodied multimodal learning. Furthermore, concerning variety, the number of automatically generated relations is still not on par with the variety of relations encountered in the real world.</p>
<p>In the third approach, a diverse and large amount of structured data from knowledge bases is used, which can be manually curated, crowd-sourced, or extracted from text resources on the web. This kind of data reflects human knowledge and experiences in unlimited domains beyond any specific scenarios. A limitation of this approach is that it accesses primarily semantic information from which spatial relations need to be inferred and they often do not involve directional relations .</p>
<p>These different, complementary approaches of data access have fostered the development of distinct tasks and of distinct classes of models: typical embodied models process sequences of multimodal data and output actions for robot control; models using disembodied image data are frequently used for classification; models using disembodied data from knowledge bases process symbolic information and are often used for inference and reasoning. We argue that all three approaches, although not directly compatible, are necessary to solve realworld tasks that involve spatial relation learning (cf. Figure 1). In this paper, we provide an example model for each of the three approaches. Moreover, we sketch a concept for their integration into a unified neural architecture for spatial relation learning.</p>
<p>Our contributions in this paper can be summarized as follows:</p>
<p>For example, "the cup holds a drink" has the implicit spatial meaning that the drink is inside the cup (cf. Collell et al., ).</p>
<p>According to Collell and Moens ( ), the spatial relations left and right comprise only &lt; . % of the well-known visual genome dataset (Krishna et al., ).</p>
<p>FIGURE</p>
<p>Spatial relations between objects can be obtained in di erent ways. Consider the instruction to the robot: "Take the cup to the left of the fruit bowl to water the plant." Prior embodied experiences are needed for grounding the instruction in the real world. From its camera image, the robot can infer that there are cups on the table, but it needs to resolve "to the left of the fruit bowl" to use the correct cup. To infer that the plant, which is not in the robot's field of view, is on the windowsill, the robot can use prior knowledge, e.g., retrieved from a knowledge base, since it is a typical location for a plant.</p>
<ol>
<li>We test an embodied language learning model on a realistic scenario with a 3D dataset including spatial relations (Section 3). 2. We present a new image data set and evaluate state-of-the-art models on spatial relation learning (Section 4). 3. We propose a way to apply a relation learning approach that uses data from knowledge bases to learning spatial relations (Section 5). 4. We provide a concept for integrating the three approaches and discuss further extension possibilities (Section 6).</li>
</ol>
<p>. Related work</p>
<p>In this section, we discuss previous work that is relevant for this paper, where we discuss models for spatial relation learning and embodied language learning. We introduce datasets that involve spatial relations and contrast them with the Qualitative Directional Relation Learning (QDRL) dataset that we propose in this paper.</p>
<p>. . Embodied learning models</p>
<p>For embodied learning, an embodied agent (e.g., a robot) needs to act using its whole body or parts thereof (e.g., arms, hands, etc.) in an environment. As we are interested in spatial relation learning within the scope of this paper, and since it is a subset of language learning, in this part, we refer to the models that learn language in an embodied fashion. Specifically, we .</p>
<p>/fnbot. .</p>
<p>focus on embodied language learning with object manipulation. For a detailed and extensive review on language and robots, please refer to Tellex et al. (2020) where NLP-based robotic learning approaches are compared and categorized based on their technicalities and the problems that they address. Early robotic language learning approaches focused on mapping human language input to formal language which could be interpreted by a robot (Dzifcak et al., 2009;Kollar et al., 2010;Matuszek et al., 2012Matuszek et al., , 2013. Dzifcak et al. (2009) introduced an integrated robotic architecture that parsed natural language directions created from a limited vocabulary in order to execute actions and achieve goals in an office environment by using formal logic. Similarly, Kollar et al. (2010) proposed an embodied spatial learning system that learned how to navigate in a building according to given human language input by mapping the natural language directions into formal language clauses and grounding them in the environment to find the most probable paths. Further, Matuszek et al. (2013) introduced an approach that could parse natural language commands to a formal robot control language (RCL) in order to map directions to executable sequences of actions depending on the world state in a navigation setup. Moreover, Matuszek et al. (2012) put forward a joint multimodal approach that flexibly learned novel grounded object attributes in the scene based on the linguistic and visual input using an online probabilistic learning algorithm. These early works were all symbolic learning approaches, while we are interested in neural network-based learning approaches in this paper.</p>
<p>As embodied language learning usually involves executing actions according to language input or describing those actions using language, it generally requires not only language and visual perception but also proprioception. Recently, numerous studies have been reported in which different objects are manipulated by a robot for embodied language learning (Hatori et al., 2017;Shridhar and Hsu, 2018;Yamada et al., 2018;Heinrich et al., 2020;Shao et al., 2020). Hatori et al. (2017) present a multimodal neural architecture which is composed of object recognition and language processing modules intended to learn the mapping between object names and actual objects as well as their attributes such as color, texture, or size for moving them to different boxes, especially in cluttered settings. Shridhar and Hsu (2018) introduce the INGRESS (interactive visual grounding of referring expressions) approach that has two streams, namely self-reference (describing the object with inherent characteristics in isolation) and relation (describing the object according to its spatial relation to other objects), and that can generate language expressions from input images to be compared with input commands to locate objects in question to pick them with the robotic arm. Yamada et al. (2018) propose the paired recurrent autoencoders (PRAE) model, which fuses language and action modalities in the latent feature space via a shared loss, for bidirectional translation between predefined language descriptions and simple robotic manipulation actions on objects. Heinrich et al. (2020) propose a biologically inspired crossmodal neural network approach, the adaptive multiple timescale recurrent neural network (adaptive MTRNN), which enables the robot to acquire language by listening to commands while interacting with objects in a playground environment. Shao et al. (2020) put forward a robot learning framework that combines a neural network with reinforcement learning, which accepts a linguistic instruction and a scene image as input and produces a motion trajectory, trained to obtain concepts of manipulation by watching video demonstrations from humans.</p>
<p>. . Spatial relation learning datasets</p>
<p>Spatial relation learning can be understood as a subproblem of visual relationship detection (VRD) (Lu et al., 2016;Krishna et al., 2017) that has as its task predicting the subject-predicateobject (SPO) triples from images. As the SPO triples are often biased toward frequent scenarios (e.g., a book on a table), datasets such as the UnRel Dataset (Peyre et al., 2017) and the SpatialSense dataset (Yang K. et al., 2019) were proposed to reduce the effect of the dataset bias. A task that is more general than visual relation detection and implicitly requires spatial relation learning is visual question answering (VQA), whose goal is to answer questions on a given image (Antol et al., 2015;Goyal et al., 2017;Wu et al., 2017).</p>
<p>Existing datasets for VRD and VQA do not distinguish between different frames of reference, which aggravates not only the difficulty of spatial relation prediction but also the difficulty of analyzing the performance of the models. To overcome the limitations of the existing datasets, in Section 4 we propose the Qualitative Directional Relation Learning (QDRL) dataset for analyzing the model performance on spatial relation learning in different frames of reference. Similar to the existing visual reasoning datasets CLEVR , ShapeWorld (Kuhnle and Copestake, 2017), and SQOOP (Bahdanau et al., 2019), QDRL is a generated dataset that allows for controlled evaluations of the models. But different from the former three datasets, whose spatial relations are exclusively based on an absolute frame of reference, QDRL also allows us to test model performance concerning intrinsic and relative frames of reference.</p>
<p>. . Spatial relation learning models</p>
<p>One of the early approaches to learning spatial relations is the connectionist model proposed in Regier (1992), which was developed as a part of the L 0 project (Feldman et al., 1996). As an early connectionist model it is characterized by its involvement of several hand-engineered components, e.g., the object boundaries and orientations of the objects are preprocessed and not learned from data. In Collell and Moens . /fnbot. . (2018), the authors propose a model that predicts the location and the size of an object based on another object that is in relation to it. The model uses bounding boxes and does not distinguish between left and right for location and size prediction. For general VRD and VQA problems, most models rely on the cues from the language models they employ and use the bounding box information (Wu et al., 2017;Lu et al., 2019;Tan and Bansal, 2019). Contrary to the VRD and VQA models, models for visual reasoning such as FiLM (Perez et al., 2018) and MAC (Hudson and Manning, 2018) do not rely on bounding boxes or pretrained language models. Furthermore, these two models do not assume any task-specific knowledge, which is for example exploited by neuro-symbolic approaches or neural module networks (Andreas et al., 2016;Yi et al., 2018).</p>
<p>. Embodied spatial relation learning</p>
<p>Having a body and acting in the environment is essential for cognition as human cognition relies upon having embodied context-dependent sensorimotor action capabilities in the environment, i.e., perception and action are inseparable in experienced cognition (Varela et al., 2017), since humans perceive the world through a variety of sensors and act in the world with their motor functions (Arbib et al., 1986). Similarly, machines cannot truly infer true meanings from words without experiencing the real world with vision, touch, and other sensors (Arbib et al., 1986). Therefore, embodiment is also a necessary condition in spatial relation learning: by having an embodied agent situated in the environment we can learn grounded meanings of spatial relations such as left or right.</p>
<p>A simple robotic scenario generally involves a robot manipulating a few objects on a table. The robot may either execute actions according to given commands in textual/audio form or translate actions to commands. This requires a crossmodal architecture that involves multiple modalities like vision, language, and proprioception. Using multiple modalities helps for the case of spatial relation learning since seeing the object to be manipulated (vision), grounding commands that are associated with actions (language) and registering joint angle trajectories (proprioception) are all different interpretations of the world. For example, when executing a command such as "push the left object, " both seeing the objects on the table and moving the arm of the robot in the correct trajectory with learned joint angles support learning the position "left."</p>
<p>. . A bidirectional embodied model</p>
<p>A bidirectional embodied model, such as the PRAE (paired recurrent autoencoders; Yamada et al., 2018), is attractive to approach grounding of language, since it is able to both execute simple robot actions given language descriptions and FIGURE Bidirectional embodied model.</p>
<p>FIGURE</p>
<p>The NICO robot (Kerzel et al., ) in the simulation environment (zdemir et al., ). (Left) NICO is sliding the right cube. (Right) NICO is pulling the left cube. In both segments, NICO's field of view is shown in the top right insets.</p>
<p>to generate language descriptions given executed and visually perceived actions. In our recent extension of the model in a robotic scenario (zdemir et al., 2021), schematically shown in Figure 2, two cubes of different colors are placed on a table at which the NICO robot (Kerzel et al., 2017) is seated to interact with them (see Figure 3). Given proprioceptive and visual input, the approach is capable of translating robot actions to textual descriptions. The proposed Paired Variational Autoencoders (PVAE) extension allows to associate each robot action with eight description alternatives, and provides oneto-many mapping, by using Stochastic Gradient Variational Bayes (SGVB).</p>
<p>The model consists of two autoencoders: a language and an action VAE. The language VAE learns descriptions while the action VAE learns joint angle values conditioned on the visual input. After encoding, the encoded representations are used to extract latent representations by randomly sampling from a Gaussian distribution. A binding loss brings the two VAEs closer by reducing the distance between two latent variables. Additionally, we introduced a channel-separated CAE (convolutional autoencoder), for the PVAE approach, for extracting visual features from the egocentric scene images (zdemir et al., 2021). The channel separation refers to training the same CAE once for each RGB channel and .</p>
<p>/fnbot. .</p>
<p>concatenating the features extracted from the middle layer of the CAE for each color channel to arrive at the combined visual features. The PVAE with channel-separated CAE visual feature extraction outperforms the standard PRAE (Yamada et al., 2018) in the one-to-many translation of actions into language commands. The approach is significantly more successful in the case of three color alternatives per cube and also with six color alternatives compared to PRAE. Our findings suggest that variational autoencoders facilitate better one-to-many actionto-description translation and address the linguistic ambiguity between an action and its probable descriptions in the simple scenario shown in Figure 3. Moreover, channel separation in visual feature extraction leads to a more accurate recognition of object colors.</p>
<p>. . Embodied spatial relation learning dataset</p>
<p>The previously mentioned works on bidirectional autoencoders do not experiment with the models' spatial relation learning capabilities. The instructions that the model processes are composed of three words with the first word indicating the type of action (push, pull, or slide), the second the cube color (six color alternatives) and the last the speed at which the action is performed (slowly or fast). The command "slide yellow slowly" and "pull pink fast" are example descriptions used for the model (cf. Figure 3). Therefore, the corpus includes 36 possible sentences (3 action  6 color  2 speed) without the alternative words and 288 possible sentences are created by replacing each word with an alternative (36  2 3 ). Moreover, the dataset consists of 12 action types (e.g., push left, pull right etc.) and 12 cube arrangements (e.g., pink-yellow, red-green etc.), thus of 144 patterns (12 action type  12 arrangement).</p>
<p>We extend this corpus by adding "left" or "right" as a new term to each description. Therefore, above example descriptions become "slide right yellow slowly" and "pull left pink slowly, " respectively-the descriptions are composed of four words. Color words may also be omitted so that the model needs to rely on the spatial specification. For simplicity, the cubes are placed on two fixed positions and the two cubes on the table are never of the same color. We have trained the model with the modified descriptions using the same hyperparameters as in zdemir et al. (2021) for 15,000 iterations with a learning rate of 10 4 and batch size of 100 .</p>
<p>The use of left and right can involve reference objects that are not explicitly mentioned. For example, "slide right yellow slowly" implies that there is a reference object in the scene (e.g., the pink cube in Figure ) and the yellow object is-as seen by the agent-to the right of the reference object. </p>
<p>Translation type Evaluation measure Train (%) Test (%)</p>
<p>ActionLanguage</p>
<p>Description accuracy 100 100</p>
<p>LanguageAction nRMSE 0.53 0.55</p>
<p>Green background indicates good performance.</p>
<p>. . Results of the PVAE model</p>
<p>To translate actions to descriptions, we use the action encoder and language decoder: given joint angle values and visual features, we expect the model to produce the correct descriptions. For the bidirectional aspect of PVAE, we also test the language-to-action translation capability. For this task, we give as input one of the eight alternative descriptions for each pattern (action-description-arrangement combination) and we expect the model to predict the corresponding joint angle values.</p>
<p>To that end, we use the language encoder and action decoder of PVAE. Both tasks are evaluated using the same trained model.</p>
<p>The results are as follows:</p>
<p> PVAE is able to translate from actions to descriptions with 100% accuracy for all 144 patterns, including 108 training and 36 test patterns (see Table 1). This matches the results reported in zdemir et al. (2021).</p>
<p> The predicted joint angle values are tightly close to the original values, as can be seen in Figure 4 with qualitative results and in Table 1 with average quantitative results in terms of the normalized root-mean-square error (nRMSE) between the original and predicted joint trajectories. Therefore, we expect the robot to execute correct actions according to the given instructions.</p>
<p>It is arbitrary to describe an action with both the relative position and color of the object being manipulated in this scenario due to the cube arrangements. However, when two cubes of the same color are present on the table, adding relative position information into descriptions is necessary to avoid confusion-we do not test this since the dataset (zdemir et al., 2021) does not involve two cubes of the same color simultaneously on the table. Furthermore, we can also be sure that the same action-to-description translation performance could be achieved by removing the color term from the descriptions as the position information of the object can be extracted through proprioception only, i.e., joint angle values, without the need for the vision modality. This is because the position of the cube being handled can be inferred from the For more details on the hyperparameters and dataset details please refer to zdemir et al. ( ).</p>
<p>Frontiers in Neurorobotics frontiersin.org . /fnbot. .</p>
<p>FIGURE</p>
<p>Examples of original and predicted joint angle trajectories for four di erent actions. The predicted values are generated by PVAE, given language descriptions and conditioned on visual input. Solid lines show the ground truth, while the dashed lines, which are often hidden by the solid lines, show the predicted joint angle values. The titles denote the action types, e.g., "PULL-R-SLOW" means pulling the right object slowly. The ground truth action trajectories with joint angle values were generated with an inverse kinematics solver in the simulation environment (zdemir et al., ).</p>
<p>proprioception as action types include the position (left or right) rather than the color of the cube. For practical reasons, we do not simulate the robot with predicted joint angle trajectories. Due to certain subtleties in object manipulation (contact point etc.), there may be divergences in the simulated kinematics where the objects are moved toward compared to original trajectories. Note further that, compared to a human of the same size, the arm movements of our robot (i.e., the NICO robot; Kerzel et al., 2017) are more constrained due to fewer degrees of freedom, short arms, self-obstruction by the limbs, and by its inflexible trunk. We, therefore, set up only a simple scenario with left-right relation in the robot's egocentric frame of reference. In the following section, we tackle more complex spatial problems with multiple frames of reference.</p>
<p>. Spatial relation learning using image data Investigating how well neural networks learn the geometric features underlying different spatial relations is an important step toward building robust deep learning models for learning spatial relations. In this section, we propose a new dataset that we call the Qualitative Directional Relation Learning (QDRL) dataset, which allows for testing the performance of deep learning models on directional relational learning tasks. We evaluate the performance of representative end-toend neural models on the QDRL dataset concerning different frames of reference and their generalizability to unseen entity-relation combinations (also known as compositional generalizability).</p>
<p>. . Directional relation learning</p>
<p>Humans adopt different strategies when giving instructions to robots, where different frames of reference play a role (Tenbrink et al., 2002). There are three kinds of frames of reference according to Levinson (1996). In an absolute frame of reference, the location of an entity is given by a fixed frame of reference shared by all entities (cf. Figure 7). In an intrinsic frame of reference, each object determines the reference frame given by its orientation (cf. Figure 7). In a relative frame of reference, the direction between two entities determines the frame of reference for locating another third entity (cf. Figure 7).</p>
<p>In this section, we evaluate two deep learning models, FiLM (Perez et al., 2018) and MAC (Hudson and Manning, 2018). Schematically, Figure 5 shows that they take as input a raw RGB image and a question as a sequence of strings. These are turned into vectors v and q using a convolutional neural network (CNN) and a recurrent neural network (RNN), .</p>
<p>/fnbot. . respectively. They produce a text answer as output, which here reduces to true or false. The two models differ in how they process v and q. As generic visual reasoning models they are fully differentiable and do not assume any task-specific knowledge (e.g., bounding boxes or the structure of the question).</p>
<p>The FiLM network processes v through a sequence of ResNet (He et al., 2016) blocks where the output values before the final ReLu activation function in each block are affinely transformed. The parameters for the affine transformations are obtained from the question vector q through a linear transformation. This way, FiLM allows the question to modulate what information passes through each ResNet block function, which helps sequential reasoning.</p>
<p>The main idea of the MAC network is to model a reasoning process by keeping a sequence of control operations and a recurrent memory, where the control operations decide what information to retrieve from the image, and the memory retains information relevant for each reasoning step.</p>
<p>. . The qualitative directional relation learning dataset</p>
<p>The Qualitative Directional Relation Learning (QDRL) dataset we propose consists of (image, question, answer) triples . Here, the question is a simple statement about the spatial relation between the objects in the image and is of the form (head, relation, tail), e.g., (rabbit, left_of, cat). The answer can be either true or false and depends on the adopted frame of reference, and the distribution of the truth values of the answers is balanced, such that no bias can be exploited. The image is of size 128  128 with a black background and contains non-overlapping entities of size 24  24. As entities we chose face emojis that have clear front sides and facilitate detecting orientations. The samples are generated as follows. First, a fixed number n of emoji names are randomly chosen from 38 possible emoji names. Then a head entity h, a tail entity t, a relation r</p>
<p>The code for reproducing the results in this section can be downloaded from https://github.com/knowledgetechnologyuhh/QDRL. and an answer a are randomly selected so as to form an (h, r, t) question triple and the ground-truth answer a. To prepare a corresponding image, the n entities are randomly rotated and placed in the image until the constraint [(h, r, t), a] is satisfied. An example with ground truth answers concerning different frames of reference is given in Figure 6.</p>
<p>As directional relations we use {above, below, left_of, right_of} for absolute and intrinsic frames of reference and {in_front_of, behind, left_of, right_of} for a relative frame of reference. Examples of the directional relations, where frames of reference are taken into consideration, are given in Figure 7. The QDRL dataset encourages a neural network model to learn the (oriented) bounding box of the reference entity as it induces the decision boundaries for different relations, where the kind of bounding box a model has to learn depends on the given frame of reference. In an absolute frame of reference, a model has to learn the axis-aligned bounding box of the reference entity.</p>
<p>In an intrinsic frame of reference, a model has to additionally learn the orientation of the reference entity and the bounding box that is aligned to that orientation. In a relative frame bsof reference, a model has to determine the centers of the reference entity and the source entity that "sees" the reference entity and align the bounding box to the direction from the center of the source entity to the center of the reference entity.</p>
<p>. . Experiments on the QDRL dataset</p>
<p>In this section, we evaluate the performance of the FiLM and the MAC networks on the QDRL dataset with respect to different frames of reference and their compositional generalizability, i.e., their generalizability to unseen entityrelation combinations. To this end, we train the two models on 1,000,000 (image, question, answer) triples and validate on 10 10,000 (image, question, answer)-triples, where we vary the following parameters for each experiment: (i) frame of reference and (ii) for absolute and intrinsic frames of reference the number of entities in each scene ( {2, 5}).</p>
<p>In addition to the standard validation set, to test how the models generalize compositionally, we hold out a subset S of 18 entities from the 32 entities appearing in the training set and make sure that every question in the training set involves at least an entity that is not in S. We then create a dataset consisting of 10,000 (image, question, answer) triples exclusively with the entities from S and call it the compositional validation set. This way it is guaranteed that the set of questions in the training set has no overlap with the set of the questions in the compositional validation set. This allows us to test whether a model is able to learn to disentangle entities and relations as well as to learn the syntactic structures, Frontiers in Neurorobotics frontiersin.org . /fnbot. .</p>
<p>FIGURE</p>
<p>An example of a QDRL dataset sample. Given an image, in an absolute and an intrinsic frame of reference a question about the image is a triple (entity , relation, entity ), and in a relative frame of reference, a question about the image is a quadruple (entity , relation, entity , entity ). As can be seen in the ground truth answers to the question, di erent frames of reference (FoR) lead to di erent answers.</p>
<p>FIGURE</p>
<p>The three frames of reference according to Levinson ( )</p>
<p>. (Left)</p>
<p>In an absolute frame of reference, the location of an entity is given by a fixed frame of reference shared by all entities (above is fixed to the north, here, of the cat). (Middle) In an intrinsic frame of reference, an object has its own frame of reference given by its orientation (here, the cat is oriented toward the northeast). (Right) In a relative frame of reference, the direction given by two entities (here, the direction from the rabbit to the cat) determines the frame of reference for locating another third entity (here, dog).</p>
<p>such that they can deal with unseen combinations of entities and relations.</p>
<p>All model hyperparameters, except for the number of FiLM blocks ( {2, 4, 6}) and the MAC cells ( {2, 8}) that we optimize, are taken from Bahdanau et al. (2019). For training, we choose 32 as the batch size and apply early stopping based on the model's performance on the validation set.</p>
<p>. . Results by FiLM and MAC models</p>
<p>In Table 2, we report the accuracy results of the experiments. From the table, we can observe the following.</p>
<p> Learning directional relations in an intrinsic frame of reference and a relative frame of reference is more challenging than in an absolute reference, which intuitively makes sense as the models have the extra burden to learn the orientations.</p>
<p> All models achieve relatively high performance on the validation set, which indicates that both FiLM and MAC have sufficient capacity to learn the training distribution.</p>
<p> Regarding the compositional generalization set, for the FiLM model the difficulty of the tasks increases in the order of absolute, intrinsic, and relative frame of reference, whereas the MAC model is not affected by the frames of reference, and consistently outperforms the FiLM model. The performance gap between MAC and FiLM is significant in the case of relative frames of reference.</p>
<p> The MAC model shows overall a smaller gap between the performances on the validation set and the compositional validation set. Even though MAC does not perform better than FiLM on the validation set, its performances on the compositional validation set are consistently better than those of FiLM.</p>
<p>These results demonstrate the good ability of neural networks to learn spatial relations in diverse frames of reference. However, due to the simplicity of the simulated dataset, it will be necessary to test the models' capabilities on more realistic 3D data (cf. Section 3). Since it is difficult to model the prior knowledge about spatial relations in the real world, in the following section we will consider the possibility of making use of existing knowledge bases.  </p>
<p>. Spatial relation learning using knowledge bases</p>
<p>People have created several large-scale commonsense knowledge bases to store relational knowledge about objects in structured triples (Speer et al., 2017;Ji et al., 2021;Nayak et al., 2021), such as (person, riding, horse) and (plant, on, windowsill). Intuitively, relational triples in commonsense knowledge bases store expected prior relations between objects, which can provide useful disembodied learning signals for relation detectors. Combined with object detectors, the relation detectors can produce structured graph representations of the scene, which can be useful for robots to obtain a deep understanding of the environment and perform subsequent interactions.</p>
<p>To leverage commonsense knowledge bases for visual relation detection, we have proposed the visual distant supervision technique in Yao et al. (2021). Visual distant supervision aligns commonsense knowledge bases with unlabeled images to automatically create distantly labeled relation data, which can be used to train any visual relation detectors. The underlying assumption is that the relations between two objects in an image tend to be the same as their relations in the knowledge bases. As shown in the example in Figure 8, since the object pair (bowl, cup) is labeled with relation beside in knowledge bases, an image with object pair (bowl, cup) will have beside as a candidate relation for the pair. In this way, visual distant supervision can train visual relation detectors without any human-labeled relation data, achieving strong performance compared to semi-supervised relation detectors that utilize several seed human annotations for each relation .</p>
<p>However, the assumption of distant supervision inevitably introduces noise in its automatic label generation, such as the relation label beside for the object pair (bowl, plant) in Figure 8. The reason is that distant supervision only depends on object categories for relation label generation, without considering the complete image content or spatial layout. To alleviate the noise in distant supervision, we have proposed a denoising framework that iteratively refines the probabilistic relation labels based on the EM optimization method . When human-labeled relation data is available, pretraining on distantly labeled data can also bring in improvements over fully supervised relation detectors.</p>
<p>Despite its effectiveness in learning relations, distant supervision is not always useful for spatial relation learning (e.g., there is no prior knowledge about whether a cup with water should be to the left or to the right of the fruit bowl in Figure 8). However, some relations have implicit spatial information, which can potentially be useful for spatial relation learning. For example, the relation riding implies the spatial relation on, where this implication can be obtained from linguistic knowledge bases, such as WordNet (Fellbaum, 1998). Based on the implications, relation representations learned via distant supervision can be transferred to help spatial relation learning. Effectively leveraging distant supervision for spatial relation learning is, therefore, an important research problem.</p>
<p>. Concept of an integrated architecture</p>
<p>The previous sections presented complementary models for spatial reasoning: a model to collect embodied, but costly, experiences; a model for plentiful, but oversimplifying, simulations; and a knowledge base enriched, but disembodied, technique. To achieve the intelligent behavior of an AI agent, the merits of such models must be combined. However, neural models mostly cannot be trivially combined by using a modular setup with well-defined interfaces. Since our models have overlapping functionality, their combination needs to be designed in the architecture and by joint training of the architecture components. In the area of multi-task learning, there have been recent attempts to tackle multiple datasets and tasks, combining multiple inputs and outputs, by a single model (Kaiser et al., 2017;Pramanik et al., 2019;Lu et al., 2020). The conjecture is that while multiple tasks are concurrently learned, learning one task can help the others. To transfer knowledge or skills, parts of the neural architecture are shared between the tasks. Figure 9 shows a concept for our proposition that follows a bidirectional model architecture (cf. Section 3), which enables tasks in two directions: The task to act, given language instructions, is best performed by embodied learning in a realistic 3D simulation (green arrows indicate the direction of the information flow). The task to produce language descriptions, given (visual) sensor input (red pathway), lends itself to using simulated visual data containing geometric relations that can be easily produced in large quantities (cf.</p>
<p>. /fnbot. .</p>
<p>FIGURE</p>
<p>Visual distant supervision (Yao et al., ) retrieves plausible relations between the detected objects (only a selection of bounding boxes and relations is shown). Correct relation labels are highlighted in bold and green thick arrows.</p>
<p>FIGURE</p>
<p>Concept of an integrated architecture for spatial relationship learning. Our presented models cover only input-to-output. The loop closure with the environment depicted here indicates an extension, such as dialogue with a human. Smaller loops on the decoders indicate low-level feedback-driven behaviors such as reaching a target object or producing a sentence.</p>
<p>Section 4). The representations on the central part benefit from joint training by forming a joint abstract representation of entities, which are independent of the input modality. The bidirectionality of the model ensures compatibility with both directions, while a large overlap in the joint central part should ensure that extensive spatial relation learning on large datasets can help the other tasks.</p>
<p>Disembodied knowledge (cf. Section 5), e.g., from a knowledge base, enters the model as another input (blue arrows in Figure 9). The recognition of concepts, given language and sensor input, will activate related disembodied knowledge to help to finish the task, which can be achieved by first retrieving related relational triples from the knowledge base, and then obtaining enhanced representations of the language and sensor Frontiers in Neurorobotics frontiersin.org . /fnbot. .</p>
<p>input with the retrieved knowledge. For example, when a robot is asked to "fetch the cup, " related relational triples will be retrieved, such as (cup, on, table), represented into embeddings, and integrated into the representation of the instruction so that the robot will expect to find the cup on the table first (see also Figure 1). A practical methodology to incorporate disembodied knowledge into an integrated model is using a graph neural network (GNN) (Gori et al., 2005;Liu and Zhou, 2020). The disembodied knowledge is represented in the form of a graph structure where nodes capture the concepts and edges capture the existing relations between the nodes. Nodes hold a vector, while interactions over the edges are represented as neural networks, which share weights in case of same relation types. To compute a target function, vectors on each node are iteratively updated. The structure of the GNN can be derived from knowledge bases such as ConceptNet or Visual Genome, and the structure will be typically sparse, i.e., only a small proportion of node pairs will be connected. The trainable GNN parameters, including the input and readout connections that connect the GNN layer to the main neural model architecture (blue arrows in Figure 9), can be trained as part of the integrated architecture. This requires a dataset, in which the model function can benefit from the GNN, such as commonsense reasoning (Talmor et al., 2019). In our integrated model, the graph neural network would first need relevant nodes activated, which correspond to items from the visual input or to words from the language input. Such a mapping could be established by supervised pretraining. The GNN converts commonsense knowledge regarding object-toobject relations, which is encoded in its structure, to be used by the distributed representations of our neural model. Thereby the external knowledge gets fused with representations obtained from the instruction and visual signals in order to enhance spatial reasoning .</p>
<p>While our models are trained in a supervised way from pairs of input and output vectors, interacting with the environment means that actions are iteratively performed by an embodied learning agent ( Figure 9 shows the environment in the loop). There are many approaches to train the action policy of an agent, including supervised learning (Shah et al., 2021), imitation learning (Chevalier-Boisvert et al., 2019;Chaplot et al., 2020;Shridhar et al., 2021), and reinforcement learning (Hermann et al., 2017;Chaplot et al., 2018;Li et al., 2021). Among these approaches, reinforcement learning is most versatile because it does not require human-labeled data for all situations, but the agent can learn its action policy by interacting with the environment and only occasionally receiving rewards (purple dashed arrow in Figure 9). The reward function is typically designed manually based on the domain knowledge of the target task, or it can be an intrinsic reward function (Pathak et al., 2017).</p>
<p>. Discussion . . Integrating reinforcement learning</p>
<p>Our bidirectional model is trained in a supervised fashion to perform physical actions in a continuous 3D space (Section 3). However, small deviations from a teacher trajectory could lead to failure, for example, in grasping an object. Reinforcement learning (RL), in contrast, is sensitive to the narrow regions in action space that distinguish successful from non-successful actions. In Figure 9, we therefore suggest using RL as a superior method for the physical actions.</p>
<p>Goal-conditioned RL is advisable for cases where the agent's goal not only depends on the state of the environment, but where the goal is also conditioned on further input, such as its internal state (Dickinson and Balleine, 1994), or on language input as in our model. Goal-conditioned RL furthermore underlies hierarchical RL, where a higherlevel module dynamically sets goals for a lower-level module, and hindsight experience replay (HER), where a future state in any trajectory is set as a goal in hindsight. With the availability of abundant high-quality trajectories, Lynch and Sermanet (2021) use an imitation learning approach, where the agent uses HER to learn from crowd-sourced trajectories, where the goal representation is paired with language input, in order to realize a flexible language-toaction mapping.</p>
<p>While RL is established for learning physical actions and suitable for general use (Silver et al., 2021), its use for language learning is yet emergent (Rder et al., 2021;Uc-Cetina et al., 2021). Regarding language as a sequence production problem, our language decoder could benefit from the availability of high-quality forward models, such as the Transformer language model. Such a language model could be used as a forward model in a model-based RL algorithm, as done by the decision transformer (Chen et al., 2021) and the trajectory transformer (Janner et al., 2021). However, such open-domain language models are difficult to use in a visual context to achieve specified goals. In order to define terminal goals in RL for language learning in specific domains, simple visual guessing game scenarios were devised (Das et al., 2017;Zhao et al., 2021). The generated language can be further augmented for high-quality dialogue by rewarding certain properties like informativity, coherence, and ease of answering (Li et al., 2016), which works in open domains, or by other scores (e.g., BLEU or ROUGE) that compare to human-generated text (Keneshloo et al., 2020). The contrast between domain-specific scenarios, which allow to guide RL language learning via rewards, and open-domain sophisticated language models, reflects the contrast between embodied and simulated learning, which allows control over spatial relations, .</p>
<p>/fnbot. .</p>
<p>and the use of knowledge bases with their open-domain information.</p>
<p>A challenge for deep RL is that its many parameters are trained from sparse and often binary reward feedback. Therefore, unsupervised or supervised pretraining of model components, such as for sensory preprocessing, or for end-toend components as described in Sections 3 and 4, can render deep RL efficient.</p>
<p>. . Curriculum learning</p>
<p>For machine learning tasks that span multiple levels of difficulty, curriculum learning has been shown to be efficient for a variety of models (Elman, 1993;Bengio et al., 2009).</p>
<p>In one of our experiments on the QDRL dataset (cf. Section 4) we observed that pretraining the FiLM model first on scenes in an intrinsic frame of reference with two objects and then fine-tuning it on scenes in a relative frame of reference with three objects helped the model to achieve about 0.9 accuracy on the compositional validation set, instead of 0.745 accuracy without the pretraining (cf. Table 2). This large increase in performance could be attributed to the fact that scenes in an intrinsic frame of reference are easier to learn as the relations involve only two objects, while at the same time helping a model to learn the concept of orientation. Fine-tuning on scenes in a relative frame of reference thus requires only modifying the concept of orientation, i.e., orientation is determined by two objects instead of intrinsically (cf. Figure 7).</p>
<p>A specific spatial relation between objects arises when one object occludes another, i.e., when one object is behind another from the observer's point of view. In the task of robotic object existence prediction by occlusion reasoning , a robot needs to reason whether a target object is possibly occluded by a visible object. Curriculum learning has proven essential for the successful training of the proposed model. We found that training the model from scratch on data containing all types of scenes is hard. In the curriculum training strategy, the model is sequentially trained on four types of scenes with increasing difficulty. First, the model is trained on scenes with only one object. Then the model is trained on scenes with two objects but all of them are visible. Next, the model is trained on scenes with two objects with occlusion. In the end, the model is trained jointly on all possible scenes. After the curriculum learning, the obtained model is able to tackle all types of scenes well. Curriculum learning has also been proven useful in other works on embodied learning (Wu et al., 2019;Yang W. et al., 2019).</p>
<p>Models that use knowledge graphs can also benefit from gradually increasing levels of difficulty. For example, to decompose the prediction of a complex scene graph, Mao et al. (2019) propose to first predict easy relations that models are confident with, and then better infer difficult relations based on the easy ones. Zhang et al. (2021) leverage relation hierarchies in knowledge bases, and propose to first learn the coarse-grained relations that are distant in relation hierarchies, and then distinguish the fine-grained relations that are nearby in relation hierarchies.</p>
<p>. . Architecture extension possibilities</p>
<p>The combined model concept considers recurrent networks such as LSTMs to be used as the action and language encoders and decoders, following the bidirectional embodied model architecture presented in this paper. Pretrained Transformerbased language models like BERT (Devlin et al., 2019) do not have language grounded in the environment because they are trained exclusively on textual data-they are unimodal, with no visual or sensorimotor information considered. However, spatial reasoning requires visual and/or sensorimotor perception to make sense of whether an object is to the left or right of another. Therefore, in order to make use of a pretrained language model via transfer learning, we leave adopting a BERT model as a language encoder/decoder and fine-tuning it as part of future work. Integrating a language model in this manner should endow our combined model with commonsense knowledge without having to lose its spatial reasoning capabilities.</p>
<p>Learning spatial relations requires reasoning about the frame of reference. In Section 4, the task was to learn spatial relations when frames of reference are given. A more challenging scenario would be when frames of reference are not given explicitly but need to be inferred. We often encounter this scenario in real-world conversations: some people tend to take the perspective of others, whereas some tend to use the egocentric perspective. This gives rise to ambiguities, which need to be resolved in a dialogue through questions and answers.</p>
<p>Existing works have demonstrated that commonsense knowledge graphs can effectively facilitate visual relation learning. However, knowledge graphs are typically introduced to train a relation predictor to produce scene graphs for downstream tasks. To leverage the symbol-based scene graphs in downstream tasks, graph embedding models are usually needed, which makes the overall procedure expensive and cumbersome. In the future, knowledge graphs can be directly integrated into the representations of pretrained vision-language models during pretraining, helping the models to better learn objects and their relations. The knowledge in pretrained vision-language models can then be readily used to serve downstream tasks through simple fine-tuning.</p>
<p>. /fnbot. .</p>
<p>. Conclusion</p>
<p>In this paper, we have investigated multiple approaches for spatial relation learning. We have shown that an embodied bidirectional model can generate physical actions from language descriptions and vice versa, involving simple left/right relations. We have then shown on a new simple visual dataset that recent visual reasoning models can learn spatial relations in multiple reference frames, with the MAC model outperforming the FiLM model. Since it is unrealistic for a robot to learn exhaustive world knowledge through interaction, or through simple visual datasets, we have considered using the relations from knowledge bases to infer likely spatial relations in a current scene. A practical limitation that has become apparent in our study is that different datasets are needed to learn complementary aspects of spatial reasoning, which hampers the development of a single joint model. This limitation may be overcome by developing more comprehensive datasets, or by devising integrated modular architectures. Finally, we have presented a concept of such an integrated architecture for combining the different models and tasks, which still requires implementation and validation in the future. We furthermore discussed their extension possibilities, which can serve as a basis for intelligent robots solving tasks in the real world that require spatial relation learning and reasoning.</p>
<p>Data availability statement</p>
<p>The code for reproducing the results in Section 4 can be downloaded from https://github.com/knowledgetechnologyuhh /QDRL. . /fnbot. .</p>
<p>FIGURE
Spatial relation learning model.</p>
<p>TABLE Performance of
PerformancePVAE on bidirectional translation.</p>
<p>TABLE Accuracies of
AccuraciesFiLM and MAC networks on the QDRL dataset.FoR a </p>
<h1>Ents b</h1>
<p>FiLM 
MAC </p>
<p>Val c 
Comp d 
Val 
Comp </p>
<p>Absolute 
2 
0.996 
0.912 
0.985 
0.929 </p>
<p>5 
0.996 
0.933 
0.992 
0.958 </p>
<p>Intrinsic 
2 
0.979 
0.882 
0.973 
0.927 </p>
<p>5 
0.978 
0.862 
0.967 
0.937 </p>
<p>Relative 
3 
0.978 
0.745 
0.978 
0.975 </p>
<p>a Frame of reference. b # Entities. c Validation set. d Compositional validation set. Green 
background indicates good performance, red indicates worse performance. </p>
<p>Frontiers in Neuroroboticsfrontiersin.org
This work was jointly funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 62061136001/DFG TRR-169.Conflict of interestThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.Publisher's noteAll claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.Author contributions JL, O, YY, and ML developed, implemented, and evaluated the models. CW, ZL, and SW helped in writing and revising the paper. JL and YY collected the data. All authors contributed to the article and approved the submitted version.Funding
Neural module networks. J Andreas, M Rohrbach, T Darrell, D Klein, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLas Vegas, NVIEEEAndreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016). "Neural module networks, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Las Vegas, NV: IEEE), 39-48.</p>
<p>VQA: visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C Lawrence Zitnick, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionSantiagoIEEEAntol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C., et al. (2015). "VQA: visual question answering, " in Proceedings of the IEEE International Conference on Computer Vision (Santiago: IEEE), 2425-2433.</p>
<p>The Construction of Reality. M A Arbib, M A Arbib, M B Hesse, 10.1017/CBO9780511527234Cambridge University PressArbib, M. A., Arbib, M. A., and Hesse, M. B. (1986). The Construction of Reality. Cambridge University Press. doi: 10.1017/CBO9780511527234</p>
<p>. D Bahdanau, S Murty, M Noukhovitch, T H Nguyen, de Vries, H., andBahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T. H., de Vries, H., and</p>
<p>Systematic generalization: what is required and can it be learned?. A Courville, International Conference on Learning Representations. New Orleans, LACourville, A. (2019). "Systematic generalization: what is required and can it be learned?, " in International Conference on Learning Representations (New Orleans, LA).</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, Weston , J , 10.1145/1553374.1553380Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09. the 26th Annual International Conference on Machine Learning, ICML 09Montreal, QCAssociation for Computing MachineryBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). "Curriculum learning, " in Proceedings of the 26th Annual International Conference on Machine Learning, ICML 09 (Montreal, QC: Association for Computing Machinery), 41-48. doi: 10.1145/1553374.1553380</p>
<p>Y Bisk, A Holtzman, J Thomason, J Andreas, Y Bengio, J Chai, 10.48550/arXiv.2004.10151Experience grounds language. Bisk, Y., Holtzman, A., Thomason, J., Andreas, J., Bengio, Y., Chai, J., et al. (2020). Experience grounds language. arXiv:2004.10151 [cs]. doi: 10.48550/arXiv.2004.10151</p>
<p>Gated-attention architectures for task-oriented language grounding. D S Chaplot, D Gandhi, S Gupta, A Gupta, R ; D S Salakhutdinov, K M Sathyendra, R K Pasumarthi, D Rajagopal, R Salakhutdinov, 10.1609/aaai.v32i1.11832Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence. the Thirty-Second AAAI Conference on Artificial IntelligenceAddis Ababa; New Orleans, LA20208th International Conference on Learning RepresentationsChaplot, D. S., Gandhi, D., Gupta, S., Gupta, A., and Salakhutdinov, R. (2020). "Learning to explore using active neural SLAM, " in 8th International Conference on Learning Representations, ICLR 2020 (Addis Ababa). Available online at: openreview.net/ Chaplot, D. S., Sathyendra, K. M., Pasumarthi, R. K., Rajagopal, D., and Salakhutdinov, R. (2018). "Gated-attention architectures for task-oriented language grounding, " in Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (New Orleans, LA), 2819-2826. doi: 10.1609/aaai.v32i1.11832</p>
<p>Decision transformer: reinforcement learning via sequence modeling. L Chen, K Lu, A Rajeswaran, K Lee, A Grover, M Laskin, arXiv:2106.01345arXiv preprintChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., et al. (2021). Decision transformer: reinforcement learning via sequence modeling. arXiv preprint arXiv:2106.01345.</p>
<p>. V S Chen, P Varma, R Krishna, M Bernstein, C Re, L Fei-Fei, Chen, V. S., Varma, P., Krishna, R., Bernstein, M., Re, C., and Fei-Fei, L. (2019).</p>
<p>Scene graph prediction with limited labels. 10.1109/ICCVW.2019.00220Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionLong Beach, CAIEEE"Scene graph prediction with limited labels, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Long Beach, CA: IEEE), 2580-2590. doi: 10.1109/ICCVW.2019.00220</p>
<p>BabyAI: first steps towards grounded language learning with a human in the loop. M Chevalier-Boisvert, D Bahdanau, S Lahlou, L Willems, C Saharia, T H Nguyen, Y Bengio, International Conference on Learning Representations. New Orleans, LAChevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. (2019). "BabyAI: first steps towards grounded language learning with a human in the loop, " in International Conference on Learning Representations (New Orleans, LA).</p>
<p>Acquiring common sense spatial knowledge through implicit spatial templates. G Collell, L V Gool, M.-F Moens, Thirty-Second AAAI Conference on Artificial Intelligence. New Orleans, LAAAAICollell, G., Gool, L. V., and Moens, M.-F. (2018). "Acquiring common sense spatial knowledge through implicit spatial templates, " in Thirty-Second AAAI Conference on Artificial Intelligence (New Orleans, LA: AAAI)</p>
<p>Learning representations specialized in spatial knowledge: leveraging language and vision. G Collell, M.-F Moens, 10.1162/tacl_a_00010Trans. Assoc. Comput. Linguist. 6Collell, G., and Moens, M.-F. (2018). Learning representations specialized in spatial knowledge: leveraging language and vision. Trans. Assoc. Comput. Linguist. 6, 133-144. doi: 10.1162/tacl_a_00010</p>
<p>Learning cooperative visual dialog agents with deep reinforcement learning. A Das, S Kottur, J M F Moura, S Lee, D Batra, 2017Das, A., Kottur, S., Moura, J. M. F., Lee, S., and Batra, D. (2017). "Learning cooperative visual dialog agents with deep reinforcement learning, " in 2017</p>
<p>10.1109/ICCV.2017.321IEEE International Conference on Computer Vision. VeniceIEEEIEEE International Conference on Computer Vision (Venice: IEEE), 2970-2979. doi: 10.1109/ICCV.2017.321</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MNACM1Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). "BERT: pre-training of deep bidirectional transformers for language understanding, " in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol. 1 (Minneapolis, MN: ACM).</p>
<p>Motivational control of goal-directed action. A Dickinson, B Balleine, 10.3758/BF03199951Anim. Learn. Behav. 22Dickinson, A., and Balleine, B. (1994). Motivational control of goal-directed action. Anim. Learn. Behav. 22, 1-18. doi: 10.3758/BF03199951</p>
<p>What to do and how to do it: translating natural language directives into temporal and dynamic logic representation for goal management and action execution. J Dzifcak, M Scheutz, C Baral, P Schermerhorn, 10.1109/ROBOT.2009.51527762009 IEEE International Conference on Robotics and Automation. Dzifcak, J., Scheutz, M., Baral, C., and Schermerhorn, P. (2009). "What to do and how to do it: translating natural language directives into temporal and dynamic logic representation for goal management and action execution, " in 2009 IEEE International Conference on Robotics and Automation, 4163-4168. doi: 10.1109/ROBOT.2009.5152776</p>
<p>Learning and development in neural networks: the importance of starting small. J L Elman, 10.1016/0010-0277(93)90058-4Elman, J. L. (1993). Learning and development in neural networks: the importance of starting small. Cognition 71-99. doi: 10.1016/0010-0277(93)90058-4</p>
<p>L0-the first five years of an automated language acquisition project. J Feldman, G Lakoff, D Bailey, S Narayanan, T Regier, A Stolcke, 10.1007/978-94-009-1639-515Integration of Natural Language and Vision Processing: Theory and Grounding Representations Volume III. P. Mc KevittDordrecht; NetherlandsSpringerFeldman, J., Lakoff, G., Bailey, D., Narayanan, S., Regier, T., and Stolcke, A. (1996). "L0-the first five years of an automated language acquisition project, " in Integration of Natural Language and Vision Processing: Theory and Grounding Representations Volume III, ed P. Mc Kevitt (Dordrecht: Springer Netherlands), 205-231. doi: 10.1007/978-94-009-1639-515</p>
<p>WordNet: An Electronic Lexical Database. Bradford Books. C Fellbaum, 10.7551/mitpress/7287.001.0001Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Bradford Books. doi: 10.7551/mitpress/7287.001.0001</p>
<p>Spatial cognition an AI perspective. C Freksa, Proceedings of the 16th European Conference on Artificial Intelligence, ECAI 04. the 16th European Conference on Artificial Intelligence, ECAI 04ValenciaIOS PressFreksa, C. (2004). "Spatial cognition an AI perspective, " in Proceedings of the 16th European Conference on Artificial Intelligence, ECAI 04 (Valencia: IOS Press), 1122-1128.</p>
<p>A new model for learning in graph domains. M Gori, G Monfardini, F Scarselli, 10.1109/IJCNN.2005.1555942Proceedings. 2005 IEEE International Joint Conference on Neural Networks. 2005 IEEE International Joint Conference on Neural NetworksMontreal, QCIEEE2Gori, M., Monfardini, G., and Scarselli, F. (2005). "A new model for learning in graph domains, " in Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005, Vol. 2 (Montreal, QC: IEEE), 729-734. doi: 10.1109/IJCNN.2005.1555942</p>
<p>Making the v in VQA matter: elevating the role of image understanding in visual question answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HIIEEEGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. (2017). "Making the v in VQA matter: elevating the role of image understanding in visual question answering, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Honolulu, HI: IEEE), 6904-6913.</p>
<p>Interactively picking real-world objects with unconstrained spoken language instructions. J Hatori, Y Kikuchi, S Kobayashi, K Takahashi, Y Tsuboi, Y Unno, 10.1109/ICRA.2018.8460699CoRRHatori, J., Kikuchi, Y., Kobayashi, S., Takahashi, K., Tsuboi, Y., Unno, Y., et al. (2017). Interactively picking real-world objects with unconstrained spoken language instructions. CoRR, abs/1710.06280. doi: 10.1109/ICRA.2018.84</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 10.1109/CVPR.2016.902016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NVIEEEHe, K., Zhang, X., Ren, S., and Sun, J. (2016). "Deep residual learning for image recognition, " in 2016 IEEE Conference on Computer Vision and Pattern Recognition (Las Vegas, NV: IEEE), 770-778. doi: 10.1109/CVPR.2016.90</p>
<p>. S Heinrich, Y Yao, T Hinz, Z Liu, T Hummel, M Kerzel, Heinrich, S., Yao, Y., Hinz, T., Liu, Z., Hummel, T., Kerzel, M., et al. (2020).</p>
<p>Crossmodal language grounding in an embodied neurocognitive model. 10.3389/fnbot.2020.00052Front. Neurorobot. 1452Crossmodal language grounding in an embodied neurocognitive model. Front. Neurorobot. 14:52. doi: 10.3389/fnbot.2020.00052</p>
<p>. K M Hermann, F Hill, S Green, F Wang, R Faulkner, H Soyer, Hermann, K. M., Hill, F., Green, S., Wang, F., Faulkner, R., Soyer, H., et al.</p>
<p>10.48550/arXiv.1706.06551arXiv:1706.06551Grounded language learning in a simulated 3D world. arXiv preprintGrounded language learning in a simulated 3D world. arXiv preprint arXiv:1706.06551. doi: 10.48550/arXiv.1706.06551</p>
<p>Compositional attention networks for machine reasoning. D A Hudson, C D Manning, 10.48550/arXiv.1803.03067Hudson, D. A., and Manning, C. D. (2018). Compositional attention networks for machine reasoning. arXiv:1803.03067 [cs]. doi: 10.48550/arXiv.1803.03067</p>
<p>Reinforcement learning as one big sequence modeling problem. M Janner, Q Li, S Levine, 10.48550/arXiv.2106.02039arXiv:2106.02039arXiv preprintJanner, M., Li, Q., and Levine, S. (2021). Reinforcement learning as one big sequence modeling problem. arXiv preprint arXiv:2106.02039. doi: 10.48550/arXiv.2106.02039</p>
<p>A survey on knowledge graphs: Representation, acquisition, and applications. S Ji, S Pan, E Cambria, P Marttinen, Yu , P S , 10.1109/TNNLS.2021.3070843IEEE Trans. Neural Netw. Learn. Syst. 33Ji, S., Pan, S., Cambria, E., Marttinen, P., and Yu, P. S. (2021). A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Trans. Neural Netw. Learn. Syst. 33, 1-21. doi: 10.1109/TNNLS.2021.3070843</p>
<p>CLEVR: a diagnostic dataset for compositional language and elementary visual reasoning. J Johnson, B Hariharan, L Van Der Maaten, L Fei-Fei, C Lawrence Zitnick, R Girshick, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHonolulu, HIIEEEJohnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C., and Girshick, R. (2017). "CLEVR: a diagnostic dataset for compositional language and elementary visual reasoning, " in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (Honolulu, HI: IEEE), 2901-2910.</p>
<p>One model to learn them all. L Kaiser, A N Gomez, N Shazeer, A Vaswani, N Parmar, L Jones, 10.48550/arXiv.1706.05137arXiv:1706.05137Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., et al. (2017). One model to learn them all. arXiv:1706.05137. doi: 10.48550/arXiv.1706.05137</p>
<p>Deep reinforcement learning for sequence-to-sequence models. Y Keneshloo, T Shi, N Ramakrishnan, C K Reddy, 10.1109/TNNLS.2019.2929141IEEE Trans. Neural Netw. Learn. Syst. 31Keneshloo, Y., Shi, T., Ramakrishnan, N., and Reddy, C. K. (2020). Deep reinforcement learning for sequence-to-sequence models. IEEE Trans. Neural Netw. Learn. Syst. 31, 2469-2489. doi: 10.1109/TNNLS.2019.2929141</p>
<p>NICO-Neuro-Inspired COmpanion: a developmental humanoid robot platform for multimodal interaction. M Kerzel, E Strahl, S Magg, N Navarro-Guerrero, S Heinrich, S Wermter, 10.1109/ROMAN.2017.817228926th IEEE International Symposium on Robot and Human Interactive Communication. RO-MANKerzel, M., Strahl, E., Magg, S., Navarro-Guerrero, N., Heinrich, S., and Wermter, S. (2017). "NICO-Neuro-Inspired COmpanion: a developmental humanoid robot platform for multimodal interaction, " in 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), 113-120. IEEE. doi: 10.1109/ROMAN.2017.8172289</p>
<p>Toward understanding natural language directions. T Kollar, S Tellex, D Roy, Roy , N , 10.1109/HRI.2010.54531862010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI) (Osaka). Kollar, T., Tellex, S., Roy, D., and Roy, N. (2010). "Toward understanding natural language directions, " in 2010 5th ACM/IEEE International Conference on Human- Robot Interaction (HRI) (Osaka), 259-266. doi: 10.1109/HRI.2010.5453186</p>
<p>. R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., et al. (2017).</p>
<p>Visual genome: connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis. 123Visual genome: connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis. 123, 32-73. 10/f96kc4</p>
<p>A Kuhnle, Copestake , A , 10.48550/arXiv.1704.04517ShapeWorld-a new test methodology for multimodal language understanding. Kuhnle, A., and Copestake, A. (2017). ShapeWorld-a new test methodology for multimodal language understanding. arXiv:1704.04517 [cs]. doi: 10.48550/arXiv.1704.04517</p>
<p>Deep learning. Y Lecun, Y Bengio, G Hinton, 10.1038/nature14539Nature. 521LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning. Nature 521, 436-444. doi: 10.1038/nature14539</p>
<p>Frames of reference and Molyneux's question : cross linguistic evidence. S Levinson, Language and Space. Cambridge, Mass: A Bradford BookLevinson, S. (1996). "Frames of reference and Molyneux's question : cross linguistic evidence, " in Language and Space (Cambridge, Mass: A Bradford Book). 109-170.</p>
<p>Deep reinforcement learning for dialogue generation. J Li, W Monroe, A Ritter, D Jurafsky, M Galley, J Gao, 10.18653/v1/D16-1127Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TXAssociation for Computational LinguisticsLi, J., Monroe, W., Ritter, A., Jurafsky, D., Galley, M., and Gao, J. (2016). "Deep reinforcement learning for dialogue generation, " in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (Austin, TX: Association for Computational Linguistics), 1192-1202. doi: 10.18653/v1/D16-1127</p>
<p>Robotic occlusion reasoning for efficient object existence prediction. M Li, C Weber, M Kerzel, J H Lee, Z Zeng, Z Liu, S Wermter, 10.1109/IROS51168.2021.9635947Proceedings of The International Conference on Intelligent Robots and Systems. The International Conference on Intelligent Robots and SystemsPrague: IROS)Li, M., Weber, C., Kerzel, M., Lee, J. H., Zeng, Z., Liu, Z., and Wermter, S. (2021). "Robotic occlusion reasoning for efficient object existence prediction, " in Proceedings of The International Conference on Intelligent Robots and Systems (Prague: IROS). doi: 10.1109/IROS51168.2021.9635947</p>
<p>Introduction to graph neural networks. Z Liu, J Zhou, 10.2200/S00980ED1V01Y202001AIM045Synthesis Lectures on Artificial Intelligence and Machine Learning 1-127. Liu, Z. and Zhou, J. (2020). "Introduction to graph neural networks, " in Synthesis Lectures on Artificial Intelligence and Machine Learning 1-127. doi: 10.2200/S00980ED1V01Y202001AIM045</p>
<p>Visual relationship detection with language priors. C Lu, R Krishna, M Bernstein, L Fei-Fei, 10.1007/978-3-319-46448-051Computer Vision-ECCV 2016. B. Leibe, J. Matas, N. Sebe, and M. WellingChamSpringer International PublishingLu, C., Krishna, R., Bernstein, M., and Fei-Fei, L. (2016). "Visual relationship detection with language priors, " in Computer Vision-ECCV 2016, Lecture Notes in Computer Science, eds B. Leibe, J. Matas, N. Sebe, and M. Welling (Cham: Springer International Publishing), 852-869. doi: 10.1007/978-3-319-46448-051</p>
<p>ViLBERT: pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in Neural Information Processing Systems. Vancouver, BCCurran Associates, Inc32Lu, J., Batra, D., Parikh, D., and Lee, S. (2019). "ViLBERT: pretraining task- agnostic visiolinguistic representations for vision-and-language tasks, " in Advances in Neural Information Processing Systems 32 (Vancouver, BC: Curran Associates, Inc.), 13-23.</p>
<p>12-in-1: multi-task vision and language representation learning. J Lu, V Goswami, M Rohrbach, D Parikh, S Lee, 10.1109/CVPR42600.2020.010452020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEELu, J., Goswami, V., Rohrbach, M., Parikh, D., and Lee, S. (2020). "12-in- 1: multi-task vision and language representation learning, " in 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (IEEE), 10434-10443. doi: 10.1109/CVPR42600.2020.01045</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, RSS 2021P Sermanet, RSS 202110.15607/RSS.2021.XVII.047Robotics: Science and Systems. Lynch, C., and Sermanet, P. (2021). "Language conditioned imitation learning over unstructured data, " in Robotics: Science and Systems (RSS 2021). doi: 10.15607/RSS.2021.XVII.047</p>
<p>. J Mao, Y Yao, S Heinrich, T Hinz, C Weber, S Wermter, Mao, J., Yao, Y., Heinrich, S., Hinz, T., Weber, C., Wermter, S., et al. (2019).</p>
<p>Bootstrapping knowledge graphs from images and text. 10.3389/fnbot.2019.00093Front. Neurorobot. 1393Bootstrapping knowledge graphs from images and text. Front. Neurorobot. 13:93. doi: 10.3389/fnbot.2019.00093</p>
<p>A joint model of language and perception for grounded attribute learning. C Matuszek, N Fitzgerald, L Zettlemoyer, L Bo, D Fox, 10.48550/arXiv.1206.6423arXiv preprintMatuszek, C., FitzGerald, N., Zettlemoyer, L., Bo, L., and Fox, D. (2012). A joint model of language and perception for grounded attribute learning. arXiv preprint arXiv:1206.6423. doi: 10.48550/arXiv.1206.6423</p>
<p>Learning to parse natural language commands to a robot control system. C Matuszek, E Herbst, L Zettlemoyer, D Fox, 10.1007/978-3-319-00065-7_28J. Desai, G. Dudek, O. Khatib, and V. KumarSpringerHeidelbergin Experimental RoboticsMatuszek, C., Herbst, E., Zettlemoyer, L., and Fox, D. (2013). "Learning to parse natural language commands to a robot control system, " in Experimental Robotics, eds J. Desai, G. Dudek, O. Khatib, and V. Kumar (Heidelberg: Springer), 403-415. doi: 10.1007/978-3-319-00065-7_28</p>
<p>Deep neural approaches to relation triplets extraction: a comprehensive survey. T Nayak, N Majumder, P Goyal, S Poria, 10.1007/s12559-021-09917-7Cogn. Comput. 13Nayak, T., Majumder, N., Goyal, P., and Poria, S. (2021). Deep neural approaches to relation triplets extraction: a comprehensive survey. Cogn. Comput. 13, 1215-1232. doi: 10.1007/s12559-021-09917-7</p>
<p>Embodied language learning with paired variational autoencoders. O zdemir, M Kerzel, S Wermter, 10.1109/ICDL49984.2021.9515668IEEE International Conference on Development and Learning (ICDL). zdemir, O., Kerzel, M., and Wermter, S. (2021). "Embodied language learning with paired variational autoencoders, " in IEEE International Conference on Development and Learning (ICDL). doi: 10.1109/ICDL49984.2021.9515668</p>
<p>Curiositydriven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, Darrell , T , 10.1109/CVPRW.2017.70Proceedings of the 34th International Conference on Machine Learning. D. Precup and Y. W. Tehthe 34th International Conference on Machine LearningSydney, NSWPathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). "Curiosity- driven exploration by self-supervised prediction, " in Proceedings of the 34th International Conference on Machine Learning, ICML 2017, eds D. Precup and Y. W. Teh (Sydney, NSW), 2778-2787. doi: 10.1109/CVPRW.20</p>
<p>FiLM: visual reasoning with a general conditioning layer. E Perez, F Strub, H De Vries, V Dumoulin, A Courville, Thirty-Second AAAI Conference on Artificial Intelligence. New Orleans, LAPerez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville, A. (2018). "FiLM: visual reasoning with a general conditioning layer, " in Thirty-Second AAAI Conference on Artificial Intelligence (New Orleans, LA).</p>
<p>Weakly-supervised learning of visual relations. J Peyre, J Sivic, I Laptev, C Schmid, 10.48550/arXiv.1907.07804Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionVeniceIEEEPeyre, J., Sivic, J., Laptev, I., and Schmid, C. (2017). "Weakly-supervised learning of visual relations, " in Proceedings of the IEEE International Conference on Computer Vision (Venice: IEEE), 5179-5188. doi: 10.48550/arXiv.1907.07804</p>
<p>OmniNet: a unified architecture for multi-modal multi-task learning. S Pramanik, P Agrawal, A Hussain, arXiv:1907.07804Frontiers in Neurorobotics frontiersin.org. Pramanik, S., Agrawal, P., and Hussain, A. (2019). OmniNet: a unified architecture for multi-modal multi-task learning. arXiv:1907.07804. Frontiers in Neurorobotics frontiersin.org</p>
<p>The Acquisition of Lexical Semantics for Spatial Terms: A Connectionist Model of Perceptual Categorization. T P Regier, 62Berkeley, CAUniversity of CaliforniaTechnical ReportRegier, T. P. (1992). The Acquisition of Lexical Semantics for Spatial Terms: A Connectionist Model of Perceptual Categorization. Technical Report 62, University of California, Berkeley, CA.</p>
<p>. F Rder, O -Zdemir, D P Nguyen, S Wermter, M Eppe, Rder, F., -zdemir, O., Nguyen, D. P., Wermter, S., and Eppe, M. (2021).</p>
<p>The embodied crossmodal self-forms language and interaction: a computational cognitive review. 10.3389/fpsyg.2021.716671Front. Psychol. 123374The embodied crossmodal self-forms language and interaction: a computational cognitive review. Front. Psychol. 12:3374. doi: 10.3389/fpsyg.2021.716671</p>
<p>. D Shah, B Eysenbach, G Kahn, N Rhinehart, S Levine, Shah, D., Eysenbach, B., Kahn, G., Rhinehart, N., and Levine, S. (2021).</p>
<p>Ving: learning open-world navigation with visual goals. 10.1109/ICRA48506.2021.9561936IEEE International Conference on Robotics and Automation, ICRA 2021. Xi'an"Ving: learning open-world navigation with visual goals, " in IEEE International Conference on Robotics and Automation, ICRA 2021 (Xi'an), 13215-13222. doi: 10.1109/ICRA48506.2021.9561936</p>
<p>Concept2robot: learning manipulation concepts from instructions and human demonstrations. L Shao, T Migimatsu, Q Zhang, K Yang, J Bohg, 10.15607/RSS.2020.XVI.082Proceedings of Robotics: Science and Systems (RSS) (Virtual). Robotics: Science and Systems (RSS) (Virtual)Shao, L., Migimatsu, T., Zhang, Q., Yang, K., and Bohg, J. (2020). "Concept2robot: learning manipulation concepts from instructions and human demonstrations, " in Proceedings of Robotics: Science and Systems (RSS) (Virtual). doi: 10.15607/RSS.2020.XVI.082</p>
<p>Interactive visual grounding of referring expressions for human-robot interaction. M Shridhar, D Hsu, 10.15607/RSS.2018.XIV.028arXiv preprintShridhar, M., and Hsu, D. (2018). Interactive visual grounding of referring expressions for human-robot interaction. arXiv preprint arXiv:1806.03831. doi: 10.15607/RSS.2018.XIV.028</p>
<p>Cliport: what and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Proceedings of the 5th Conference on Robot Learning. the 5th Conference on Robot LearningLondonShridhar, M., Manuelli, L., and Fox, D. (2021). "Cliport: what and where pathways for robotic manipulation, " in Proceedings of the 5th Conference on Robot Learning (London).</p>
<p>Reward is enough. D Silver, S Singh, D Precup, R S Sutton, 10.1016/j.artint.2021.103535Artif. Intell. 299103535Silver, D., Singh, S., Precup, D., and Sutton, R. S. (2021). Reward is enough. Artif. Intell. 299:103535. doi: 10.1016/j.artint.2021.103535</p>
<p>ConceptNet 5.5: an open multilingual graph of general knowledge. R Speer, J Chin, C Havasi, 10.1609/aaai.v31i1.11164Proceedings of AAAI. AAAISan Francisco, CAAAAISpeer, R., Chin, J., and Havasi, C. (2017). "ConceptNet 5.5: an open multilingual graph of general knowledge, " in Proceedings of AAAI (San Francisco, CA: AAAI). doi: 10.1609/aaai.v31i1.11164</p>
<p>CommonsenseQA: a question answering challenge targeting commonsense knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MNAssociation for Computational Linguistics1Talmor, A., Herzig, J., Lourie, N., and Berant, J. (2019). "CommonsenseQA: a question answering challenge targeting commonsense knowledge, " in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (Minneapolis, MN: Association for Computational Linguistics), 4149-4158.</p>
<p>LXMERT: learning cross-modality encoder representations from transformers. H Tan, M Bansal, Proceedings of the 2019. the 2019Tan, H., and Bansal, M. (2019). "LXMERT: learning cross-modality encoder representations from transformers, " in Proceedings of the 2019</p>
<p>10.18653/v1/D19-1514Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. Hong KongAssociation for Computational LinguisticsConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP- IJCNLP) (Hong Kong: Association for Computational Linguistics), 5100-5111. doi: 10.18653/v1/D19-1514</p>
<p>. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Tellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C. (2020).</p>
<p>Robots that use language. 10.1146/annurev-control-101119-071628Annu. Rev. Control Robot. Auton. Syst. 3Robots that use language. Annu. Rev. Control Robot. Auton. Syst. 3, 25-55. doi: 10.1146/annurev-control-101119-071628</p>
<p>Spatial strategies in humanrobot communication. T Tenbrink, K Fischer, R Moratz, Knstl. Intell. 16Tenbrink, T., Fischer, K., and Moratz, R. (2002). Spatial strategies in human- robot communication. Knstl. Intell. 16, 19-23.</p>
<p>V Uc-Cetina, N Navarro-Guerrero, A Martin-Gonzalez, C Weber, S Wermter, 10.1007/s10462-022-10205-5arXiv:2104.05565Survey on reinforcement learning for language processing. Uc-Cetina, V., Navarro-Guerrero, N., Martin-Gonzalez, A., Weber, C., and Wermter, S. (2021). Survey on reinforcement learning for language processing. arXiv:2104.05565. doi: 10.1007/s10462-022-</p>
<p>The Embodied Mind, Revised Edition: Cognitive Science and Human Experience. F J Varela, E Thompson, E Rosch, 10.7551/mitpress/9780262529365.001.0001MIT PressCambridge, MA; LondonVarela, F. J., Thompson, E., and Rosch, E. (2017). The Embodied Mind, Revised Edition: Cognitive Science and Human Experience. Cambridge, MA; London: MIT Press. doi: 10.7551/mitpress/9780262529365.001.0001</p>
<p>. Q Wu, D Teney, P Wang, C Shen, A Dick, Van Den, A Hengel, Wu, Q., Teney, D., Wang, P., Shen, C., Dick, A., and van den Hengel, A. (2017).</p>
<p>Visual question answering: a survey of methods and datasets. 10.1016/j.cviu.2017.05.001Comput. Vis. Image Understand. 163Visual question answering: a survey of methods and datasets. Comput. Vis. Image Understand. 163, 21-40. doi: 10.1016/j.cviu.2017.05.001</p>
<p>. Y Wu, Y Wu, A Tamar, S Russell, G Gkioxari, Y Tian, Wu, Y., Wu, Y., Tamar, A., Russell, S., Gkioxari, G., and Tian, Y. (2019).</p>
<p>Bayesian relational memory for semantic visual navigation. 10.1109/ICCV.2019.00286Proceedings of the 2019 IEEE International Conference on Computer Vision. the 2019 IEEE International Conference on Computer VisionSeoulIEEE"Bayesian relational memory for semantic visual navigation, " in Proceedings of the 2019 IEEE International Conference on Computer Vision (Seoul: IEEE). doi: 10.1109/ICCV.2019.00286</p>
<p>Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions. T Yamada, H Matsunaga, T Ogata, 10.1109/LRA.2018.2852838IEEE Robot. Autom. Lett. 3Yamada, T., Matsunaga, H., and Ogata, T. (2018). Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions. IEEE Robot. Autom. Lett. 3, 3441-3448. doi: 10.1109/LRA.2018.2852838</p>
<p>Embodied amodal recognition: learning to move to perceive objects. J Yang, Z Ren, M Xu, X Chen, D Crandall, D Parikh, 2019Yang, J., Ren, Z., Xu, M., Chen, X., Crandall, D., Parikh, D., et al. (2019). "Embodied amodal recognition: learning to move to perceive objects, " in 2019</p>
<p>10.1109/ICCV.2019.00213IEEE/CVF International Conference on Computer Vision. SeoulIEEEIEEE/CVF International Conference on Computer Vision (Seoul: IEEE), 2040-2050. doi: 10.1109/ICCV.2019.00213</p>
<p>SpatialSense: an adversarially crowdsourced benchmark for spatial relation recognition. K Yang, O Russakovsky, J Deng, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionYang, K., Russakovsky, O., and Deng, J. (2019). "SpatialSense: an adversarially crowdsourced benchmark for spatial relation recognition, " in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2051-2060.</p>
<p>Visual semantic navigation using scene priors. W Yang, X Wang, A Farhadi, A Gupta, R Mottaghi, Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning RepresentationsICLRYang, W., Wang, X., Farhadi, A., Gupta, A., and Mottaghi, R. (2019). "Visual semantic navigation using scene priors, " in Proceedings of the 7th International Conference on Learning Representations (ICLR).</p>
<p>Visual distant supervision for scene graph generation. Y Yao, A Zhang, X Han, M Li, C Weber, Z Liu, 10.1109/ICCV48922.2021.015522021 IEEE International Conference on Computer Vision. IEEEYao, Y., Zhang, A., Han, X., Li, M., Weber, C., Liu, Z., et al. (2021). "Visual distant supervision for scene graph generation, " in 2021 IEEE International Conference on Computer Vision (IEEE) (Virtual). doi: 10.1109/ICCV48922.2021.01552</p>
<p>Neuralsymbolic VQA: disentangling reasoning from vision and language understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in Neural Information Processing Systems 31. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. GarnettMontreal, QCCurran Associates, IncYi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., and Tenenbaum, J. (2018). Neural- symbolic VQA: disentangling reasoning from vision and language understanding, " in Advances in Neural Information Processing Systems 31, eds S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (Montreal, QC: Curran Associates, Inc.), 1031-1042.</p>
<p>Open hierarchical relation extraction. K Zhang, Y Yao, R Xie, X Han, Z Liu, F Lin, 10.18653/v1/2021.naacl-main.452Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Online). the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Online)Zhang, K., Yao, Y., Xie, R., Han, X., Liu, Z., Lin, F., et al. (2021). "Open hierarchical relation extraction, " in Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Online) 5682-5693. doi: 10.18653/v1/2021.naacl-main.452</p>
<p>L Zhao, X Lyu, J Song, L Gao, 10.1016/j.patcog.2021.107823Guess which? Visual dialog with attentive memory network. Pattern Recogn. 114107823Zhao, L., Lyu, X., Song, J., and Gao, L. (2021). Guess which? Visual dialog with attentive memory network. Pattern Recogn. 114:107823. doi: 10.1016/j.patcog.2021.107823</p>            </div>
        </div>

    </div>
</body>
</html>
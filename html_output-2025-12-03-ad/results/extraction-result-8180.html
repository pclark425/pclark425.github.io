<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8180 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8180</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8180</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-277510246</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.01963v1.pdf" target="_blank">LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems</a></p>
                <p><strong>Paper Abstract:</strong> This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8180.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8180.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VecDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector Databases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>External high-dimensional vector index systems used to store and retrieve embedding representations for LLMs, enabling scalable, retrieval-based access to large knowledge stores and mitigating hallucination and memory limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>When Large Language Models Meet Vector Databases: A Survey</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>VecDB (as memory component)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent per se but an external memory layer: a vector index serving LLM agents by storing embeddings and enabling semantic retrieval of documents or encoded experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge retrieval / knowledge-intensive tasks (general)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides fast semantic access to external documents, facts, and past interactions to support LLM generation in knowledge-intensive applications and long interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge retrieval / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector database (non-parametric)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store high-dimensional embeddings for documents/contexts; retrieval via similarity search to augment prompts or model inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Embeddings of external documents, past interactions, or other knowledge artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic similarity search (vector search) / dense retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports VecDBs reduce hallucination and offload knowledge from parametric weights but notes engineering and integration challenges; no quantified ablation reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>VecDBs are a practical solution to LLMs' memory and knowledge limitations, enabling dynamic, context-aware retrieval and hot-swapping of knowledge without re-training the model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration/engineering challenges with LLM architectures, requirement for retrieval/index optimization, and compatibility concerns across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8180.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid memory design combining parametric model knowledge with a dense non-parametric retrieval index to improve factuality and specificity in generation by dynamically retrieving external passages at generation time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM augmentation technique where a retriever finds relevant passages from an external index and a generator conditions on those retrieved passages to produce answers or text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a parametric seq2seq base model combined with a dense retrieval index; exact backbone model not specified in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain / knowledge-intensive question answering and text generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Retrieve relevant external evidence to ground answers and generate more factual, diverse responses for QA and knowledge-intensive generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / knowledge-intensive generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid: parametric + non-parametric retrieval index</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Dense vector index stores external passages; retriever returns relevant passages which are concatenated or otherwise conditioned into the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External documents/passages stored as dense vectors; also uses parametric memory within the seq2seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Dense retrieval / semantic search; variants include retrieving per-sequence or per-token passages as described by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey states RAG outperforms state-of-the-art parametric seq2seq models and retrieve-and-extract baselines on multiple QA tasks, and highlights variants that retrieve same vs different passages per token; no numeric ablation provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining parametric and non-parametric memory improves factual accuracy, specificity, and allows hot-swapping/updating of knowledge without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Complexity in retrieval index management, need for extensive training data and careful retrieval engineering for best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8180.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments LLMs with a symbolic SQL database as explicit external memory, enabling precise multi-hop reasoning by issuing and executing SQL operations as part of the model's chain-of-memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ChatDB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-memory architecture where the model generates SQL instructions to interact with a symbolic database; SQL query results are used in subsequent reasoning steps and final response generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex multi-hop reasoning tasks mapped to database operations</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that can be represented as sequences of SQL selects/updates/inserts/deletes, enabling precise intermediate-state manipulation and elimination of error accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-hop reasoning / database-backed reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>symbolic external database (SQL) as memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM emits SQL commands which are executed against the external database; results are fed back into the LLM chain-of-memory for further steps and final summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured rows/records in an SQL database representing facts, intermediate results, or knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Explicit SQL queries generated by the LLM (programmatic symbolic retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey claims ChatDB outperforms models like ChatGPT on tasks requiring complex reasoning by preventing error propagation via precise database operations; no numerical ablation reported in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic database memory enables precise intermediate result storage and manipulation, improving performance on structured multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited to tasks that can be mapped to SQL operations; requires careful generation of correct SQL instructions to avoid errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8180.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A long-term memory mechanism for LLMs that stores, retrieves, and updates user/personality memories using a memory updater inspired by the Ebbinghaus Forgetting Curve to mimic selective forgetting and reinforcement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MemoryBank: Enhancing Large Language Models with Long-Term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory subsystem with storage, retrieval, and an updater module that decays or reinforces memories over time based on psychological forgetting curves, enabling more personalized long-term interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Designed to work with closed-source (e.g., ChatGPT) and open-source LLMs (e.g., ChatGLM); specific backbone models are not enumerated in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term conversational companionship (SiliconFriend) and long-term personalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain and recall user preferences, personalities, and past conversation history over extended interactions to produce personalized responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-term dialogue / personalization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term episodic memory with updater (hybrid external store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory storage + retriever + updater that applies Ebbinghaus Forgetting Curve to selectively decay or reinforce stored memories over time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Conversational memories, user preference records, past interactions and events.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Memory retriever selects context-specific memories for use in current dialogue (context-aware retrieval, presumably semantic/recency-informed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports MemoryBank enables improved personalized responses in the SiliconFriend chatbot but does not provide quantitative ablation comparisons in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dynamic memory updating and forgetting-inspired retention improve personalization and enable long-term user modeling across sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance varies with user interaction context; requires careful tuning of memory updating to avoid overload or irrelevant retention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8180.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that equips LLMs with a general read-write memory unit storing knowledge as triplets (first arg, relation, second arg) accessible via a text-based API schema to enable explicit storage and retrieval for tasks like temporal reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RET-LLM: Towards a General Read-Write Memory for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture combining a controller, a fine-tuned LLM, and a triplet-structured memory unit; the LLM issues standardized API calls to read/write triplets and aggregate knowledge for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned LLM is part of the architecture but specific model names/sizes are not provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question answering and temporal reasoning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring explicit, updatable relational knowledge storage and retrieval (e.g., temporally sensitive QA).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / temporal reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>read-write external memory storing structured triplets (interpretable knowledge base)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Text-based API schema: the LLM generates standardized memory API calls that the controller translates to read/write operations on a triplet memory store.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Knowledge triplets (subject, relation, object) capturing relational facts and temporal information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>API-driven read operations (controlled retrieval of triplets) and aggregation of retrieved triplets for reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey notes RET-LLM shows improved performance on QA tasks requiring temporal reasoning compared to baseline approaches, though comprehensive empirical figures are deferred to future work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>An explicit read-write triplet memory enables interpretable and updatable knowledge handling, improving tasks that need temporal or relational reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Preliminary evaluations only; future work needed on real datasets and refining fine-tuning steps to broaden applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8180.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Controlled Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A plug-and-play memory framework that augments instruction-following LLMs with a memory stream and a controller that updates and decides when to use stored memories, aimed at improving performance on ultra-long-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Enhancing Large Language Model with Self-Controlled Memory Framework</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-Controlled Memory (SCM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Consists of an LLM agent, a memory stream for storing memories, and a memory controller that updates memories and decides retrieval timing; integrates with existing LLMs without heavy fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Designed to integrate with instruction-following models such as text-davinci-003 and gpt-3.5-turbo-0301 as noted by the survey, though no single backbone is mandated.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ultra-long text tasks: long-term dialogue, book summarization, meeting summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring retention and retrieval across very long inputs (tens of thousands of tokens and many conversation turns).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>long-context understanding / summarization / long-term dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory stream + controller (working/episodic-style)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Memory stream stores agent memories; controller updates the stream and selects which memories to use when producing responses, operating in a plug-and-play manner.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored agent memories (past dialogue snippets, summarized content, salient events).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Controller-driven selection and retrieval from memory stream (contextual/controlled retrieval); exact retrieval algorithm not detailed in survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey reports SCM significantly improves retrieval recall and generates more informative responses versus competitive baselines in long-term dialogue evaluations; no exact numeric ablations included in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A controller-based memory stream can extend LLMs' effective context window and improve information recall over ultra-long inputs, with easy integration into existing instruction-following models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Evaluations limited to dialogues up to 200 turns and 34,000 tokens; depends on strong instruction-following LLMs and evaluation of truly infinite dialogues remains open.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8180.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8180.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval Augmented Processing (RAP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning framework that integrates retrieval-augmented techniques with contextual memory to dynamically leverage past multimodal experiences to guide LLM agent planning and decision making in both text-only and multimodal/embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework enabling agents to store and retrieve past experiences (text and images) relevant to current context, using retrieval to augment planning and improve decision making in multimodal environments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Textual benchmarks and multimodal embodied tasks (general multimodal planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agents must plan and act in contexts where past experiences across modalities inform present decisions (e.g., robotics, multimodal gaming, API integrations).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / multimodal embodied decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>contextual retrieval-augmented memory (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store past experiences across modalities; dynamically retrieve experiences relevant to current context to guide planning and action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Past experiences (textual passages, images, multimodal context snippets) tailored as memory entries.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Contextual relevance-based retrieval across stored experiences (semantic/context matching across modalities).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Survey states RAP achieves state-of-the-art performance in textual scenarios and substantially improves multimodal agent capabilities compared to baselines, but does not provide numeric ablation details in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Leveraging relevant past experiences via retrieval markedly improves planning and decision-making in both text-only and multimodal agent tasks, enabling more flexible and human-like use of history.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires careful implementation for multimodal retrieval and management; complexity in retrieval processes and integration across modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>When Large Language Models Meet Vector Databases: A Survey <em>(Rating: 2)</em></li>
                <li>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory <em>(Rating: 2)</em></li>
                <li>MemoryBank: Enhancing Large Language Models with Long-Term Memory <em>(Rating: 2)</em></li>
                <li>RET-LLM: Towards a General Read-Write Memory for Large Language Models <em>(Rating: 2)</em></li>
                <li>Enhancing Large Language Model with Self-Controlled Memory Framework <em>(Rating: 2)</em></li>
                <li>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8180",
    "paper_id": "paper-277510246",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "VecDB",
            "name_full": "Vector Databases",
            "brief_description": "External high-dimensional vector index systems used to store and retrieve embedding representations for LLMs, enabling scalable, retrieval-based access to large knowledge stores and mitigating hallucination and memory limits.",
            "citation_title": "When Large Language Models Meet Vector Databases: A Survey",
            "mention_or_use": "mention",
            "agent_name": "VecDB (as memory component)",
            "agent_description": "Not an agent per se but an external memory layer: a vector index serving LLM agents by storing embeddings and enabling semantic retrieval of documents or encoded experiences.",
            "model_name": null,
            "model_description": null,
            "task_name": "Knowledge retrieval / knowledge-intensive tasks (general)",
            "task_description": "Provides fast semantic access to external documents, facts, and past interactions to support LLM generation in knowledge-intensive applications and long interactions.",
            "task_type": "knowledge retrieval / retrieval-augmented generation",
            "memory_used": true,
            "memory_type": "external vector database (non-parametric)",
            "memory_mechanism": "Store high-dimensional embeddings for documents/contexts; retrieval via similarity search to augment prompts or model inputs.",
            "memory_representation": "Embeddings of external documents, past interactions, or other knowledge artifacts.",
            "memory_retrieval_method": "Semantic similarity search (vector search) / dense retrieval",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports VecDBs reduce hallucination and offload knowledge from parametric weights but notes engineering and integration challenges; no quantified ablation reported in the survey.",
            "key_findings": "VecDBs are a practical solution to LLMs' memory and knowledge limitations, enabling dynamic, context-aware retrieval and hot-swapping of knowledge without re-training the model.",
            "limitations_or_challenges": "Integration/engineering challenges with LLM architectures, requirement for retrieval/index optimization, and compatibility concerns across systems.",
            "uuid": "e8180.0",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A hybrid memory design combining parametric model knowledge with a dense non-parametric retrieval index to improve factuality and specificity in generation by dynamically retrieving external passages at generation time.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "mention_or_use": "mention",
            "agent_name": "RAG",
            "agent_description": "LLM augmentation technique where a retriever finds relevant passages from an external index and a generator conditions on those retrieved passages to produce answers or text.",
            "model_name": null,
            "model_description": "Described as a parametric seq2seq base model combined with a dense retrieval index; exact backbone model not specified in the survey summary.",
            "task_name": "Open-domain / knowledge-intensive question answering and text generation",
            "task_description": "Retrieve relevant external evidence to ground answers and generate more factual, diverse responses for QA and knowledge-intensive generation.",
            "task_type": "question answering / knowledge-intensive generation",
            "memory_used": true,
            "memory_type": "hybrid: parametric + non-parametric retrieval index",
            "memory_mechanism": "Dense vector index stores external passages; retriever returns relevant passages which are concatenated or otherwise conditioned into the generator.",
            "memory_representation": "External documents/passages stored as dense vectors; also uses parametric memory within the seq2seq model.",
            "memory_retrieval_method": "Dense retrieval / semantic search; variants include retrieving per-sequence or per-token passages as described by authors.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey states RAG outperforms state-of-the-art parametric seq2seq models and retrieve-and-extract baselines on multiple QA tasks, and highlights variants that retrieve same vs different passages per token; no numeric ablation provided in the survey.",
            "key_findings": "Combining parametric and non-parametric memory improves factual accuracy, specificity, and allows hot-swapping/updating of knowledge without retraining.",
            "limitations_or_challenges": "Complexity in retrieval index management, need for extensive training data and careful retrieval engineering for best performance.",
            "uuid": "e8180.1",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "ChatDB",
            "name_full": "ChatDB",
            "brief_description": "A framework that augments LLMs with a symbolic SQL database as explicit external memory, enabling precise multi-hop reasoning by issuing and executing SQL operations as part of the model's chain-of-memory.",
            "citation_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
            "mention_or_use": "mention",
            "agent_name": "ChatDB",
            "agent_description": "An LLM-memory architecture where the model generates SQL instructions to interact with a symbolic database; SQL query results are used in subsequent reasoning steps and final response generation.",
            "model_name": null,
            "model_description": null,
            "task_name": "Complex multi-hop reasoning tasks mapped to database operations",
            "task_description": "Tasks that can be represented as sequences of SQL selects/updates/inserts/deletes, enabling precise intermediate-state manipulation and elimination of error accumulation.",
            "task_type": "multi-hop reasoning / database-backed reasoning",
            "memory_used": true,
            "memory_type": "symbolic external database (SQL) as memory",
            "memory_mechanism": "LLM emits SQL commands which are executed against the external database; results are fed back into the LLM chain-of-memory for further steps and final summarization.",
            "memory_representation": "Structured rows/records in an SQL database representing facts, intermediate results, or knowledge.",
            "memory_retrieval_method": "Explicit SQL queries generated by the LLM (programmatic symbolic retrieval)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey claims ChatDB outperforms models like ChatGPT on tasks requiring complex reasoning by preventing error propagation via precise database operations; no numerical ablation reported in the survey.",
            "key_findings": "Symbolic database memory enables precise intermediate result storage and manipulation, improving performance on structured multi-step reasoning tasks.",
            "limitations_or_challenges": "Limited to tasks that can be mapped to SQL operations; requires careful generation of correct SQL instructions to avoid errors.",
            "uuid": "e8180.2",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank",
            "brief_description": "A long-term memory mechanism for LLMs that stores, retrieves, and updates user/personality memories using a memory updater inspired by the Ebbinghaus Forgetting Curve to mimic selective forgetting and reinforcement.",
            "citation_title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "A memory subsystem with storage, retrieval, and an updater module that decays or reinforces memories over time based on psychological forgetting curves, enabling more personalized long-term interactions.",
            "model_name": null,
            "model_description": "Designed to work with closed-source (e.g., ChatGPT) and open-source LLMs (e.g., ChatGLM); specific backbone models are not enumerated in the survey.",
            "task_name": "Long-term conversational companionship (SiliconFriend) and long-term personalization",
            "task_description": "Maintain and recall user preferences, personalities, and past conversation history over extended interactions to produce personalized responses.",
            "task_type": "long-term dialogue / personalization",
            "memory_used": true,
            "memory_type": "long-term episodic memory with updater (hybrid external store)",
            "memory_mechanism": "Memory storage + retriever + updater that applies Ebbinghaus Forgetting Curve to selectively decay or reinforce stored memories over time.",
            "memory_representation": "Conversational memories, user preference records, past interactions and events.",
            "memory_retrieval_method": "Memory retriever selects context-specific memories for use in current dialogue (context-aware retrieval, presumably semantic/recency-informed).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports MemoryBank enables improved personalized responses in the SiliconFriend chatbot but does not provide quantitative ablation comparisons in this review.",
            "key_findings": "Dynamic memory updating and forgetting-inspired retention improve personalization and enable long-term user modeling across sessions.",
            "limitations_or_challenges": "Performance varies with user interaction context; requires careful tuning of memory updating to avoid overload or irrelevant retention.",
            "uuid": "e8180.3",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RET-LLM",
            "name_full": "RET-LLM",
            "brief_description": "A framework that equips LLMs with a general read-write memory unit storing knowledge as triplets (first arg, relation, second arg) accessible via a text-based API schema to enable explicit storage and retrieval for tasks like temporal reasoning.",
            "citation_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
            "mention_or_use": "mention",
            "agent_name": "RET-LLM",
            "agent_description": "Architecture combining a controller, a fine-tuned LLM, and a triplet-structured memory unit; the LLM issues standardized API calls to read/write triplets and aggregate knowledge for downstream tasks.",
            "model_name": null,
            "model_description": "Fine-tuned LLM is part of the architecture but specific model names/sizes are not provided in the survey summary.",
            "task_name": "Question answering and temporal reasoning tasks",
            "task_description": "Tasks requiring explicit, updatable relational knowledge storage and retrieval (e.g., temporally sensitive QA).",
            "task_type": "question answering / temporal reasoning",
            "memory_used": true,
            "memory_type": "read-write external memory storing structured triplets (interpretable knowledge base)",
            "memory_mechanism": "Text-based API schema: the LLM generates standardized memory API calls that the controller translates to read/write operations on a triplet memory store.",
            "memory_representation": "Knowledge triplets (subject, relation, object) capturing relational facts and temporal information.",
            "memory_retrieval_method": "API-driven read operations (controlled retrieval of triplets) and aggregation of retrieved triplets for reasoning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey notes RET-LLM shows improved performance on QA tasks requiring temporal reasoning compared to baseline approaches, though comprehensive empirical figures are deferred to future work.",
            "key_findings": "An explicit read-write triplet memory enables interpretable and updatable knowledge handling, improving tasks that need temporal or relational reasoning.",
            "limitations_or_challenges": "Preliminary evaluations only; future work needed on real datasets and refining fine-tuning steps to broaden applicability.",
            "uuid": "e8180.4",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "SCM",
            "name_full": "Self-Controlled Memory",
            "brief_description": "A plug-and-play memory framework that augments instruction-following LLMs with a memory stream and a controller that updates and decides when to use stored memories, aimed at improving performance on ultra-long-text tasks.",
            "citation_title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
            "mention_or_use": "mention",
            "agent_name": "Self-Controlled Memory (SCM)",
            "agent_description": "Consists of an LLM agent, a memory stream for storing memories, and a memory controller that updates memories and decides retrieval timing; integrates with existing LLMs without heavy fine-tuning.",
            "model_name": null,
            "model_description": "Designed to integrate with instruction-following models such as text-davinci-003 and gpt-3.5-turbo-0301 as noted by the survey, though no single backbone is mandated.",
            "task_name": "Ultra-long text tasks: long-term dialogue, book summarization, meeting summarization",
            "task_description": "Tasks requiring retention and retrieval across very long inputs (tens of thousands of tokens and many conversation turns).",
            "task_type": "long-context understanding / summarization / long-term dialogue",
            "memory_used": true,
            "memory_type": "external memory stream + controller (working/episodic-style)",
            "memory_mechanism": "Memory stream stores agent memories; controller updates the stream and selects which memories to use when producing responses, operating in a plug-and-play manner.",
            "memory_representation": "Stored agent memories (past dialogue snippets, summarized content, salient events).",
            "memory_retrieval_method": "Controller-driven selection and retrieval from memory stream (contextual/controlled retrieval); exact retrieval algorithm not detailed in survey.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey reports SCM significantly improves retrieval recall and generates more informative responses versus competitive baselines in long-term dialogue evaluations; no exact numeric ablations included in the survey.",
            "key_findings": "A controller-based memory stream can extend LLMs' effective context window and improve information recall over ultra-long inputs, with easy integration into existing instruction-following models.",
            "limitations_or_challenges": "Evaluations limited to dialogues up to 200 turns and 34,000 tokens; depends on strong instruction-following LLMs and evaluation of truly infinite dialogues remains open.",
            "uuid": "e8180.5",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "RAP",
            "name_full": "Retrieval Augmented Processing (RAP)",
            "brief_description": "A planning framework that integrates retrieval-augmented techniques with contextual memory to dynamically leverage past multimodal experiences to guide LLM agent planning and decision making in both text-only and multimodal/embodied tasks.",
            "citation_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
            "mention_or_use": "mention",
            "agent_name": "RAP",
            "agent_description": "Framework enabling agents to store and retrieve past experiences (text and images) relevant to current context, using retrieval to augment planning and improve decision making in multimodal environments.",
            "model_name": null,
            "model_description": null,
            "task_name": "Textual benchmarks and multimodal embodied tasks (general multimodal planning)",
            "task_description": "Agents must plan and act in contexts where past experiences across modalities inform present decisions (e.g., robotics, multimodal gaming, API integrations).",
            "task_type": "planning / multimodal embodied decision-making",
            "memory_used": true,
            "memory_type": "contextual retrieval-augmented memory (multimodal)",
            "memory_mechanism": "Store past experiences across modalities; dynamically retrieve experiences relevant to current context to guide planning and action generation.",
            "memory_representation": "Past experiences (textual passages, images, multimodal context snippets) tailored as memory entries.",
            "memory_retrieval_method": "Contextual relevance-based retrieval across stored experiences (semantic/context matching across modalities).",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Survey states RAP achieves state-of-the-art performance in textual scenarios and substantially improves multimodal agent capabilities compared to baselines, but does not provide numeric ablation details in the survey.",
            "key_findings": "Leveraging relevant past experiences via retrieval markedly improves planning and decision-making in both text-only and multimodal agent tasks, enabling more flexible and human-like use of history.",
            "limitations_or_challenges": "Requires careful implementation for multimodal retrieval and management; complexity in retrieval processes and integration across modalities.",
            "uuid": "e8180.6",
            "source_info": {
                "paper_title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "When Large Language Models Meet Vector Databases: A Survey",
            "rating": 2,
            "sanitized_title": "when_large_language_models_meet_vector_databases_a_survey"
        },
        {
            "paper_title": "ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory",
            "rating": 2,
            "sanitized_title": "chatdb_augmenting_llms_with_databases_as_their_symbolic_memory"
        },
        {
            "paper_title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "RET-LLM: Towards a General Read-Write Memory for Large Language Models",
            "rating": 2,
            "sanitized_title": "retllm_towards_a_general_readwrite_memory_for_large_language_models"
        },
        {
            "paper_title": "Enhancing Large Language Model with Self-Controlled Memory Framework",
            "rating": 2,
            "sanitized_title": "enhancing_large_language_model_with_selfcontrolled_memory_framework"
        },
        {
            "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
            "rating": 2,
            "sanitized_title": "rap_retrievalaugmented_planning_with_contextual_memory_for_multimodal_llm_agents"
        }
    ],
    "cost": 0.013835749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems
13 Mar 2025</p>
<p>R M Aratchige 
DrWmks Ilmini </p>
<p>Department of Computer Science</p>
<p>Faculty of Computing
General Sir John Kotelawala Defence University
RatmalanaSri Lanka</p>
<p>Department of Computer Science</p>
<p>Faculty of Computing
General Sir John Kotelawala Defence University
RatmalanaSri Lanka</p>
<p>LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems
13 Mar 202597511740050AB9948A144DA8FA25A5CCarXiv:2504.01963v1[cs.MA]Multi-Agent SystemsLarge Language ModelsArtificial IntelligenceTechnology Survey
This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)based multi-agent systems.Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks.By analyzing recent advancements and their limitations-such as scalability, real-time response challenges, and agent coordination constraints-we provide a detailed view of the technological landscape.Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making.This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability.Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.</p>
<p>I. INTRODUCTION</p>
<p>"Individually, we are one drop.Together, we are an ocean."</p>
<p>-Ryunosuke Satoro The advent of Large Language Models (LLMs) has transformed artificial intelligence, with the introduction of the Transformer architecture in the landmark paper "Attention is All You Need" [1] marking a key turning point.The Transformer replaced traditional sequence models like recurrent neural networks with an attention-based mechanism, boosting machine translation performance and reducing training time.Since then, LLMs have evolved further, particularly with models like GPT, which demonstrate unprecedented performance in natural language processing tasks [2].LLMs now handle tasks ranging from text generation to summarization, enabling software that can understand and reason with natural language.This versatility drives the current research: exploring how LLMs can build complex multi-agent systems that collaborate and specialize in tasks, especially in environments demanding more than a single model's capabilities.</p>
<p>Despite their successes, LLM-based applications have limitations.Hallucination remains a major issue, as models can produce inaccurate information without external validation, as noted by Adewumi et al. [3].This limits reliability where precision is essential.LLMs also struggle with complex or abstract concepts, a challenge discussed by Cherkassky et al. [4], who showed that even advanced models like GPT-4 often fall short in imitating human reasoning.Multi-agent systems can mitigate these issues, allowing distinct agents to collaborate on complex tasks, as explored by Han et al. [5].Such collaboration improves decision-making, especially for tasks requiring deeper reasoning or specialized skills.</p>
<p>Research on LLM-based multi-agent systems exists [6], but there remains a gap in identifying the best technological approaches for building these systems.This survey aims to address this gap by answering two research questions: What state-of-the-art technologies and approaches are available for LLM multi-agent systems?And, which of these are most effective in practice?This survey seeks to identify optimal technologies and methodologies, helping researchers and practitioners navigate the evolving landscape of tools for building advanced multi-agent systems.</p>
<p>II. LITERATURE REVIEW</p>
<p>The literature on large language models (LLMs) within multi-agent systems is still emerging, with significant research gaps, particularly in understanding and improving multi-agent paradigms.This review critically examines the current research in multi-agent LLM systems, focusing on architecture, planning, memory, and frameworks.These technological aspects shape the potential and limitations of LLM multi-agent systems, and further investigation can expand their applications and efficiency.</p>
<p>This literature review is structured as given below:</p>
<p> Planning: Discussing frameworks such as AdaPlanner, ChatCoT, KnowAgent, RAP, Tree of Thoughts (ToT), and ReAct. Memory: Covering the role of memory in LLM systems, with insights on Vector Databases, Retrieval Augmented Generation, ChatDB, MemoryBank, RET-LLM, and Self-Controlled Memory. Technologies / Frameworks: Examining the technologies and frameworks that facilitate collaboration and task execution, including AutoGen, CAMEL, CrewAI, MetaGPT, and LangGraph.A summary of the findings of each section is provided in Tables I-IV.</p>
<p>A. Architecture</p>
<p>Research on multi-agent architectures in the LLM space is relatively limited.A significant gap exists in developing frameworks that effectively orchestrate multiple agents to collaborate and solve complex reasoning tasks.While some research has started addressing these challenges, there remains a need for more comprehensive solutions.Below, we analyze some of the key papers exploring architectural designs in multi-agent LLM systems.</p>
<p>1) Conquer-and-Merge Discussion (CMD): In their paper, Wang et al. [7] present the Conquer-and-Merge Discussion (CMD) framework.This architecture leverages multiple LLMpowered agents that engage in open discussions to address reasoning tasks.Inspired by Minsky's Society of Mind (1988), the CMD framework simulates human-like debates, where each agent contributes different perspectives to improve overall reasoning capabilities.</p>
<p>The CMD framework is structured to allow a group of agents to discuss a question, with each agent generating a viewpoint and explanation in several rounds of interaction.The discussion is guided by a shared history of responses, with agents building on each other's inputs.This design outperforms single-agent methods like the Chain-of-Thought (CoT) approach, as demonstrated in their experiments.However, several limitations persist: the framework simplifies LLM sessions as agents, missing the opportunity to integrate more sophisticated reasoning techniques such as the Treeof-Thought method or external knowledge bases; CMD has only been tested on reasoning tasks, leaving its applicability to broader domains such as strategic planning or real-time decision-making unexplored; and their experiments were limited to a few LLMs (Bard, Gemini Pro, and ChatGPT-3.5),so further analysis using other models is needed to assess generalizability.</p>
<p>2) Chain-of-Agents (CoA): Zhang et al. [8], in their paper "Chain of Agents: Large Language Models Collaborating on Long-Context Tasks," propose the Chain-of-Agents (CoA) framework.This architecture is designed for handling longcontext tasks that surpass the token limits of individual LLMs.It consists of worker agents that sequentially process portions of the input and pass their results to a manager agent, which aggregates the final output.</p>
<p>The CoA architecture's key innovation is the interleaved read-process method, allowing agents to process chunks of input before receiving the full context.This approach reduces the complexity of handling long-context tasks and enhances interpretability by splitting the work across multiple agents.However, CoA has some limitations: the communication between agents could be improved by leveraging in-context learning or fine-tuning LLMs to optimize their interaction; and the architecture needs further refinement to reduce computational costs and latency, particularly in tasks requiring multiple rounds of communication between agents.</p>
<p>3) Agent Forest: The Agent Forest method, introduced by Li et al. [9] in their paper "More Agents is All You Need," focuses on scaling LLM performance by simply increasing the number of agents.The method employs a sampling-andvoting approach: multiple agents generate responses, and the final answer is determined through majority voting.</p>
<p>This technique demonstrates that increasing the number of agents improves performance, particularly on complex tasks.However, it also reveals limitations: the performance gains from Agent Forest depend on the inherent difficulty of the task, with diminishing returns for overly complex or overly simple tasks; and the approach increases computational costs due to the need for multiple LLM queries, requiring optimization of the sampling phase to enhance cost efficiency.</p>
<p>4) Mixture-of-Agents: The Mixture-of-Agents (MoA) architecture proposed by Wang et al. [10] offers a layered design where agents collaborate in both proposer and aggregator roles.Proposers generate diverse responses, while aggregators synthesize the responses into high-quality outputs.This collaborative model improves LLM performance across multiple benchmarks, including AlpacaEval 2.0 and MT-Bench.</p>
<p>MoA excels by leveraging the strengths of different LLMs, enabling specialized roles for various agents.Its main limitations include the fact that not all LLMs are equally effective in both proposer and aggregator roles, with some models, like WizardLM, excelling as proposers but struggling as aggregators; and although MoA shows impressive results, expanding the number of agents and aggregators introduces complexity in managing collaboration, which may require more sophisticated orchestration techniques in the future.</p>
<p>B. Planning</p>
<p>In LLM-based multi-agent systems, planning involves the reasoning and action strategies that agents employ to achieve their goals in dynamic environments.These systems must balance reasoning over short and long horizons while responding to environmental feedback.As autonomous decision-makers, LLM agents generate sequences of actions based on initial goals but often require adaptive planning to address the complexity of real-world problems.Effective planning frameworks for LLM multi-agent systems incorporate feedback mechanisms, refining or recalibrating actions based on evolving environmental conditions to avoid issues like hallucination or over-simplification of plans.1) AdaPlanner: In the AdaPlanner framework, Sun et al. [11] introduce a novel approach that allows LLM agents to refine and adapt plans in response to real-time environmental feedback.This marks a departure from traditional static planning systems, which typically follow a fixed sequence of actions.AdaPlanner's closed-loop system enables dynamic adjustments, providing critical flexibility for handling complex, long-horizon tasks where static plans often fail due to unexpected changes.</p>
<p>AdaPlanner incorporates two major refinement strategies: in-plan refinement, where agents modify specific parts of an existing plan to address immediate feedback, and out-of-plan refinement, where they create new actions to tackle unforeseen scenarios.To tackle the issue of LLM hallucinations, the authors implement a code-style prompting mechanism, reducing ambiguity in the generated plans and fostering consistency across various tasks.Additionally, AdaPlanner includes a skill discovery feature, allowing agents to reuse successful plans from past tasks as few-shot examples for future problemsolving, effectively improving their adaptability and efficiency.</p>
<p>Despite its advancements, AdaPlanner is not without limitations.Its reliance on few-shot expert demonstrations for more complex tasks remains a constraint, suggesting the need for future research on reducing or eliminating this dependency.Additionally, while its performance in environments like ALF-World and MiniWoB++ is promising, further testing across diverse domains would better establish the robustness and generalizability of this adaptive planning approach.</p>
<p>2) ChatCoT: Chen et al. [12] introduce ChatCoT, a framework designed to improve large language models (LLMs) in handling complex, multi-step reasoning tasks.Recognizing the limitations of traditional static reasoning models for tasks that require specific knowledge and complex logical steps, ChatCoT employs a tool-augmented chain-of-thought (CoT) reasoning approach tailored to chat-based interactions, such as those in ChatGPT.This framework allows LLMs to alternate between tool manipulation and reasoning actions within a dynamic, multi-turn conversation, enhancing adaptability in complex scenarios.</p>
<p>ChatCoT leverages the strengths of chat-based LLMs by initiating discussions with foundational knowledge about the tools, tasks, and reasoning structure involved.The resulting process is iterative, using step-by-step reasoning that integrates tool manipulation seamlessly with CoT reasoning.Evaluations on datasets such as MATH and HotpotQA show that ChatCoT achieves a 7.9% relative improvement in performance over existing methods, demonstrating its potential in advancing LLM reasoning capabilities for intricate tasks.</p>
<p>However, ChatCoT has several limitations.The framework has yet to be tested with GPT-4 due to access restrictions, which could affect the generalizability of its findings.Moreover, its design is optimized for chat-based LLMs, which may limit its compatibility with other architectures.Additionally, the high computational requirements, particularly in terms of GPU resources, pose challenges for widespread implementation.Future research will aim to extend ChatCoT's applicability across a broader range of tasks and expand its toolset, potentially enhancing its utility in diverse, complex reasoning scenarios.</p>
<p>3) KnowAgent: In the KnowAgent framework, Zhu et al. [13] present a novel approach to enhance large language models (LLMs) in performing complex reasoning tasks.LLMs, while powerful, often struggle with generating coherent action sequences and interacting effectively with environments due to a lack of inherent action knowledge.KnowAgent addresses this gap by integrating an action knowledge base and employing a self-learning strategy, providing LLMs with structured action knowledge that guides the planning process and mitigates issues like planning hallucinations.</p>
<p>KnowAgent utilizes this action knowledge to refine planning paths, ensuring LLMs generate more reasonable and executable action trajectories.This structured approach translates complex action data into an understandable format for LLMs, significantly enhancing planning accuracy.Experimental results on datasets like HotpotQA and ALFWorld show that KnowAgent not only matches but often exceeds current stateof-the-art performance while effectively reducing planning hallucinations, demonstrating its potential for improving LLM task execution.</p>
<p>Despite its advancements, KnowAgent has limitations.The framework has primarily been evaluated on commonsense question-answering and household tasks, with future potential in domains like medical reasoning, arithmetic, and web browsing.Additionally, KnowAgent currently supports only single-agent applications; exploring multi-agent systems could further enhance its utility through collaborative task execution.Lastly, the manual design of action knowledge bases presents a labor-intensive challenge, suggesting the need for automated solutions to improve adaptability and broaden the framework's application across diverse environments.</p>
<p>4) RAP: Retrieval Augmented Processing: RAP [14] introduces a groundbreaking framework that enhances the planning capabilities of large language models (LLMs) by integrating retrieval-augmented techniques with contextual memory.As LLMs are increasingly employed as agents for complex decision-making tasks in fields such as robotics, gaming, and API integration, the challenge of incorporating past experiences into current decision-making processes remains significant.To address this, RAP dynamically leverages relevant past experiences tailored to the current context, thereby improving the agents' ability to plan effectively.</p>
<p>What sets RAP apart is its versatility, as it is designed to function seamlessly in both text-only and multimodal environments.This adaptability allows it to tackle a broad spectrum of tasks.Empirical evaluations demonstrate RAP's effectiveness, achieving state-of-the-art performance in textual scenarios and significantly enhancing the capabilities of multimodal LLM agents in embodied tasks.These findings underscore RAP's potential to advance the functionality and applicability of LLM agents in real-world applications that demand sophisticated decision-making.</p>
<p>The RAP framework enables agents to store and retrieve past experiences, guiding subsequent actions based on contextual information extracted from various modalities, including text and images.The results from evaluations across multiple benchmarks reveal that RAP outperforms baseline methods, allowing language agents to flexibly utilize historical experiences in alignment with current situations.This capability mirrors a fundamental human ability, thereby enhancing decision-making capabilities and paving the way for more effective and intelligent LLM-based agents in complex, realworld scenarios.</p>
<p>5) Tree of Thoughts (ToT):</p>
<p>The "Tree of Thoughts" (ToT) framework [15] introduces a novel approach to enhance the problem-solving capabilities of language models (LMs), addressing their limitations in tasks that require exploration, strategic foresight, and the importance of initial decisions.Traditional LMs typically operate within a token-level, left-toright decision-making paradigm during inference, which can hinder performance in more complex scenarios.ToT expands upon the popular "Chain of Thought" prompting technique, allowing LMs to explore coherent units of text, referred to as "thoughts," that act as intermediate steps in problem-solving.</p>
<p>ToT enables LMs to engage in deliberate decision-making by evaluating multiple reasoning paths and assessing choices, facilitating the ability to look ahead or backtrack as necessary for making more informed global decisions.Experiments demonstrate that ToT significantly improves LMs' performance on three novel tasks that necessitate intricate planning and search: Game of 24, Creative Writing, and Mini Crosswords.For example, in the Game of 24, while GPT-4 with chain-of-thought prompting achieved only a 4% success rate, the ToT framework elevated this figure to 74%.</p>
<p>In the limitations and future directions section, the authors note that deliberate search methods like ToT may not be essential for many tasks where GPT-4 already performs well.The initial exploration is limited to three relatively simple tasks, indicating a need for further research into more complex decision-making applications, such as coding, data analysis, and robotics.Additionally, the resource-intensive nature of search methods compared to sampling methods may pose challenges, though the modular flexibility of ToT allows users to tailor performance-cost tradeoffs.Ongoing open-source initiatives could further reduce associated costs.The potential for fine-tuning LMs using ToT-style high-level counterfactual decision-making is also highlighted as an avenue for improving LMs' problem-solving abilities.</p>
<p>6) ReAct:</p>
<p>The "ReAct" framework [16] represents a significant advancement in the integration of reasoning and acting within large language models (LLMs).While LLMs have showcased remarkable capabilities in language understanding and interactive decision-making, the traditional separation of reasoning (e.g., chain-of-thought prompting) and acting (e.g., action plan generation) has limited their effectiveness in complex tasks.ReAct addresses this limitation by facilitating an interleaved generation of reasoning traces and task-specific actions, enhancing synergy between the two processes.This approach allows reasoning traces to aid the model in inducing, tracking, and updating action plans while also managing exceptions, whereas actions enable interaction with external sources, such as knowledge bases or environments.</p>
<p>The application of ReAct across various language and decision-making tasks demonstrates its superiority over stateof-the-art baselines, providing enhanced human interpretability and trustworthiness.In experiments involving question answering (HotpotQA) and fact verification (Fever), ReAct effectively mitigates common issues of hallucination and error propagation found in chain-of-thought reasoning by leveraging a simple Wikipedia API.This interaction leads to more humanlike task-solving trajectories that are more interpretable than those produced by methods lacking reasoning traces.</p>
<p>ReAct introduces a straightforward yet powerful method for synergizing reasoning and acting within LLMs, yielding superior performance and interpretable decision traces across diverse tasks, including multi-hop question answering, factchecking, and interactive decision-making.Given the positive results achieved, this approach is highly recommended for enhancing LLMs' action planning capabilities.Although the simplicity of ReAct presents advantages, it also reveals the need for more demonstrations to effectively learn complex tasks with large action spaces.Initial experiments indicate that fine-tuning on specific tasks, such as HotpotQA, may further improve performance, particularly through the incorporation of high-quality human annotations.Exploring multi-task training and integrating ReAct with complementary paradigms like reinforcement learning could lead to the development of more robust agents, further unlocking the potential of LLMs for diverse applications.</p>
<p>C. Memory</p>
<p>Memory plays a critical role in enhancing the capabilities of agents to retain and retrieve information relevant to their tasks.Effective memory systems enable agents to recall past interactions and experiences, facilitating informed decisionmaking and improving task performance in dynamic environments.The integration of advanced memory mechanisms, such as vector databases and retrieval-augmented generation, allows agents to store vast amounts of information while maintaining quick access to pertinent data.Additionally, these memory systems can implement self-controlled mechanisms to ensure that agents prioritize the most relevant memories, enabling them to adapt to new challenges while avoiding the pitfalls of irrelevant or outdated information.By enhancing memory functionality, LLM agents can better navigate complex tasks and interactions, leading to more robust and intelligent behavior.</p>
<p>1) Vector Databases: Jing et al. [17] provide a comprehensive survey of the intersection between large language models (LLMs) and vector databases (VecDBs), a rapidly evolving area of research aimed at addressing critical limitations in LLM-based systems.Despite the impressive capabilities of LLMs, they struggle with issues such as hallucinations, memory constraints, outdated knowledge, and the high costs associated with commercial deployment.VecDBs offer a promising solution to these challenges by efficiently storing and retrieving the high-dimensional vector representations that are fundamental to LLM operations.</p>
<p>The integration of LLMs and VecDBs enhances the ability of LLM systems to manage and retrieve vast amounts of information, reducing reliance on static memory and enabling more dynamic, context-aware interactions.By leveraging VecDBs, LLMs can access external knowledge bases, mitigating hallucinations and outdated information while improving response accuracy.Additionally, this synergy helps overcome memory limitations by enabling the offloading of knowledge, allowing for scalable, long-term storage solutions.This framework has paved the way for the development of memory systems specifically designed for LLMs, advancing their ability to manage complex tasks over extended interactions.</p>
<p>The paper highlights both the opportunities and challenges in combining LLMs and VecDBs, categorizing existing research into distinct prototypes and interdisciplinary approaches.It also addresses the engineering challenges related to optimizing this integration, such as designing efficient data retrieval mechanisms and ensuring compatibility with LLM architectures.Looking ahead, the authors call for further research into expanding the utility of VecDBs in diverse LLM applications, driving advancements in data handling, knowledge extraction, and the development of robust memory solutions for LLM-based systems.</p>
<p>2) Retrieval Augmented Generation: Lewis et al. [18] introduce the Retrieval-Augmented Generation (RAG) framework, a novel approach aimed at enhancing the performance of large pre-trained language models on knowledge-intensive tasks.Traditional LLMs, while capable of storing vast amounts of factual knowledge within their parameters, often struggle with accessing and manipulating this knowledge in a precise and scalable way.Furthermore, updating their knowledge or providing provenance for decisions remains a significant challenge.RAG addresses these limitations by combining parametric memory, based on a pre-trained sequence-to-sequence (seq2seq) model, with non-parametric memory, represented as a dense vector index of external sources like Wikipedia, accessed via a neural retriever.</p>
<p>By integrating these two memory systems, RAG allows models to retrieve relevant information dynamically from external sources during language generation, rather than solely relying on the knowledge embedded in their parameters.This dual-memory design significantly improves the specificity, diversity, and factual accuracy of generated responses, particularly in tasks such as open-domain question answering (QA) and knowledge-intensive text generation.The authors present two variants of RAG: one that retrieves the same passage for the entire sequence and another that retrieves different passages for each token, further refining the generation process.Through extensive experiments, RAG outperforms stateof-the-art parametric seq2seq models and retrieve-and-extract architectures, setting new benchmarks in multiple QA tasks.</p>
<p>RAG's impact goes beyond immediate performance gains; it laid the groundwork for future developments in LLM memory systems.The ability to hot-swap the retrieval index without retraining the model provides a scalable solution for updating LLMs with new knowledge, addressing a critical issue in longterm LLM use.This hybrid memory structure has inspired subsequent memory-augmented solutions, paving the way for more sophisticated LLMs capable of seamlessly integrating parametric and non-parametric knowledge sources to enhance reasoning, generation, and decision-making across a wide range of tasks.</p>
<p>3) ChatDB: In their paper, Hu et al. [19] introduce a novel framework that enhances large language models (LLMs) by integrating symbolic memory, represented by SQL databases.The motivation stems from the limitations of neural memory mechanisms, which are prone to error accumulation and struggle with complex reasoning tasks.By incorporating symbolic memory, ChatDB enables more precise and reliable memory manipulation, allowing LLMs to perform multi-hop reasoning through interaction with an external database.This is inspired by modern computer architectures rather than biological models, providing a more robust solution for advanced reasoning tasks.</p>
<p>The ChatDB framework operates in three main stages.First, in the input processing stage, the system generates SQL instructions to interact with the database if memory is required; otherwise, the LLM responds directly.In the chain-of-memory stage, a series of SQL operations such as select, update, insert, and delete are executed, with each step influencing the next based on the results of previous operations.Finally, in the response summary stage, ChatDB generates a coherent final output based on the results obtained from manipulating the symbolic memory, ensuring accurate and logical responses.</p>
<p>The experimental results show that ChatDB outperforms models like ChatGPT, especially in tasks requiring complex reasoning, by eliminating error propagation through precise memory operations.By using SQL as a symbolic memory language, ChatDB introduces a reliable method for LLMs to handle intermediate results, enhancing both the accuracy and capability of the model in various management and reasoning scenarios.This symbolic memory approach sets the stage for further advancements in memory-augmented LLMs, offering a more scalable and efficient framework for handling knowledge-intensive tasks.4) MemoryBank: Zhong et al. [20] introduce MemoryBank, an innovative memory mechanism designed to address a significant limitation in large language models (LLMs): the lack of long-term memory.While LLMs have made remarkable strides in performing various tasks, their inability to maintain and recall information from past interactions has hindered their performance in applications requiring sustained context, such as personal assistance or therapy.MemoryBank enables LLMs to store, retrieve, and update memories dynamically, allowing models to evolve in understanding users' personalities over time.By integrating a memory updater based on the Ebbinghaus Forgetting Curve theory, the system can selectively forget or reinforce memories depending on their relevance and the time elapsed, mirroring human memory retention patterns.</p>
<p>MemoryBank operates around three key components: a memory storage system for data retention, a memory retriever to summon context-specific memories, and a memory updater inspired by psychological principles.This updater ensures that the system adapts over time, retaining essential information while allowing less significant memories to fade.This anthropomorphic memory mechanism enhances user interactions, providing more personalized responses and a deeper understanding of user behavior.The framework is versatile, functioning across both closed-source models, such as ChatGPT, and open-source models, including ChatGLM.MemoryBank demonstrates its capabilities through the chatbot SiliconFriend, designed for long-term companionship, which uses MemoryBank to recall past conversations and adjust its responses based on user preferences and emotional state.</p>
<p>MemoryBank significantly improves the ability of LLMs to handle long-term interactions by offering a scalable solution for memory retention and recall.SiliconFriend, equipped with this mechanism, demonstrates the potential for AI systems to deliver more empathetic and personalized experiences.Mem-oryBank's flexible structure and memory updating mechanism allow LLMs to provide relevant and accurate information across extended dialogues, setting the stage for further advancements in memory-augmented LLMs.This framework not only enhances LLM performance in personal companion systems but also lays the groundwork for future developments in AI-human interaction, where long-term memory plays a critical role in delivering meaningful and sustained engagements.</p>
<p>5) RET-LLM:</p>
<p>Modarressi et al. [21] present RET-LLM, a groundbreaking framework designed to enhance large language models (LLMs) by integrating a general read-write memory unit.Despite the remarkable advancements LLMs have made in natural language processing (NLP), their lack of a dedicated memory system restricts their ability to store and retrieve knowledge explicitly for diverse tasks.RET-LLM addresses this gap by allowing LLMs to extract, store, and recall information as needed, improving their task performance.Drawing inspiration from Davidsonian semantics theory, the memory unit captures knowledge in the form of triplets, facilitating a scalable and interpretable memory structure that can be easily updated and aggregated.</p>
<p>The RET-LLM architecture comprises three key components: a Controller, a Fine-tuned LLM, and a Memory unit.The Controller regulates the information flow among the user, the LLM, and the Memory unit, ensuring efficient communication.The Fine-tuned LLM processes incoming text and determines when to invoke memory.To facilitate memory interaction, the framework implements a text-based API schema, allowing the LLM to generate standardized memory API calls.Knowledge is stored in a triplet format, structured as first argument, relation, second argument, reflecting the theoretical principles of Davidsonian semantics.This organization enables effective management of relational knowledge, allowing the model to perform better in various NLP tasks, particularly in question answering.</p>
<p>RET-LLM significantly enhances LLM capabilities by enabling the explicit storage and retrieval of information, thereby addressing of the critical limitations of traditional LLMs.The framework's triplet-based memory structure allows for nuanced relationships to be stored and accessed, showcasing superior performance in question answering tasks, especially those requiring temporal reasoning.Preliminary qualitative evaluations indicate that RET-LLM outperforms baseline approaches, demonstrating its potential in effectively managing time-dependent information.Although still under development, future iterations of RET-LLM will focus on comprehensive empirical evaluations using real datasets and refining the fine-tuning process to broaden its applicability across various types of informative relations.The ongoing research highlights the transformative potential of incorporating a robust memory unit into LLMs, paving the way for more intelligent and adaptable AI systems.6) Self-Controlled Memory: Wang et al. [22] introduce the Self-Controlled Memory (SCM) framework, a novel approach aimed at enhancing large language models (LLMs) by addressing their limitations in processing lengthy inputs.Traditional LLMs often struggle to retain critical historical information, which hinders their performance in tasks requiring longterm memory.The SCM framework consists of three main components: an LLM-based agent that serves as the backbone of the system, a memory stream that stores agent memories, and a memory controller that updates these memories and determines when and how to utilize them.Importantly, SCM operates in a plug-and-play manner, enabling seamless integration with any instruction-following LLMs without the need for extensive modifications or fine-tuning.</p>
<p>To validate the effectiveness of the SCM framework, the authors annotated a dataset designed for evaluating its capabilities in handling ultra-long texts across three tasks: long-term dialogues, book summarization, and meeting summarization.</p>
<p>Experimental results reveal that the SCM framework significantly improves retrieval recall and generates more informative responses compared to competitive baselines in long-term dialogue scenarios.These findings demonstrate SCM's potential to enhance the performance of LLMs, allowing them to better manage extensive conversations and detailed summarizations, ultimately addressing a key challenge in the field of natural language processing.Despite its advantages, the SCM framework has some limitations, particularly regarding the evaluation of its performance in infinite dialogue settings, which were tested only up to 200 dialogue turns and a maximum token count of 34,000.This constraint arises from the challenges associated with qualitatively and quantitatively evaluating very long texts.Additionally, the effectiveness of the SCM framework relies on powerful instruction-following LLMs like text-davinci-003 and gpt-3.5-turbo-0301.However, the authors anticipate that future advancements in smaller, more powerful LLMs could mitigate this limitation.Overall, the SCM framework offers a promising direction for extending the input length of LLMs and improving their ability to capture and recall useful information from historical data.</p>
<p>D. Technologies / Frameworks</p>
<p>The development of LLM-based multi-agent systems relies heavily on a variety of technologies and frameworks that facilitate efficient agent collaboration and task execution.These frameworks provide essential tools for building, deploying, and managing multi-agent environments, enabling seamless communication and coordination among agents.Technologies such as AutoGen and MetaGPT empower agents to generate dynamic responses and solutions based on real-time data and interactions.Additionally, frameworks like CAMEL and CrewAI offer integrated environments that streamline the design and orchestration of multi-agent systems, allowing for enhanced scalability and flexibility.By leveraging these technologies, researchers and practitioners can create sophisticated LLM-based systems that adapt to changing circumstances and optimize their performance across a range of applications, from robotics to intelligent assistance.</p>
<p>1) AutoGen: Wu et al. [23] present AutoGen, an opensource framework designed to facilitate the development of large language model (LLM) applications through multi-agent conversations.This innovative framework allows multiple agents to interact, collaborate, and accomplish tasks by leveraging customizable and conversable agents that can operate in diverse modes.AutoGen supports a combination of LLMs, human inputs, and various tools, enabling developers to flexibly define agent interaction behaviors.By employing both natural language and computer code for programming conversation patterns, AutoGen serves as a generic framework that caters to a wide range of applications, including mathematics, coding, question answering, operations research, and online decisionmaking.</p>
<p>To streamline the creation of complex LLM applications, AutoGen is built upon the principles of conversable agents Addresses LLM limitations in storing and retrieving knowledge; Superior performance in question answering tasks.</p>
<p>Still under development; Requires comprehensive evaluations and real dataset testing.</p>
<p>Self-Controlled Memory (SCM)</p>
<p>Consists of an agent, memory stream, and memory controller; Enhances handling of lengthy inputs.</p>
<p>Significantly improves retrieval recall; Generates informative responses; Easy integration with existing LLMs.</p>
<p>Limited evaluation in infinite dialogue settings; Dependent on powerful instruction-following LLMs for effectiveness.and conversation programming.A conversable agent can send and receive messages to engage with other agents while maintaining its internal context.This modular approach allows for a variety of capabilities, powered by LLMs, tools, or human input.Conversation programming encompasses two key concepts: computation, which pertains to the actions agents undertake in a multi-agent conversation, and control flow, which dictates the sequence or conditions under which these actions occur.This conversation-centric paradigm simplifies the reasoning behind complex workflows, allowing agents to pass messages dynamically and adaptively as they collaborate.</p>
<p>The authors emphasize that AutoGen enhances multi-agent cooperation through its unified conversation interface and auto-reply mechanisms, effectively harnessing the strengths of chat-optimized LLMs.The framework enables developers to create and experiment with multi-agent systems that can be reused, customized, and extended, all while significantly reducing development effort.Experimental results indicate that AutoGen outperforms state-of-the-art approaches, streamlining the development process and enabling flexible, dynamic interactions among agents.While still in the early stages of development, AutoGen lays the groundwork for future research into the integration of existing agent implementations, optimal agent topologies, and the balance between automation and human control in multi-agent workflows, addressing potential safety challenges as the complexity of applications increases.</p>
<p>2) CAMEL: Li et al. [24] present CAMEL, a novel framework aimed at enhancing the autonomous cooperation of communicative agents in chat-based language models.As these models continue to evolve, their effectiveness often hinges on human input to guide conversations, which can be a daunting and time-consuming task.The authors propose a roleplaying approach that employs inception prompting to enable agents to work collaboratively toward task completion while staying aligned with human intentions.This framework not only facilitates the generation of conversational data but also serves as a valuable resource for exploring the behaviors and capabilities of a society of agents, particularly in multi-agent settings focused on instruction-following cooperation.</p>
<p>The paper highlights the significance of autonomous cooperation among communicative agents and delineates the challenges that accompany it, such as conversation deviation, role flipping, and defining termination conditions.The roleplaying framework offers a scalable solution to these challenges, allowing agents to engage in effective collaboration with minimal human intervention.The authors conducted comprehensive evaluations to assess the framework's effectiveness, demonstrating that it leads to better outcomes in task completion.Additionally, their open-sourced library includes implementations of various agents, data generation pipelines, and analytical tools, thus fostering research on communicative agents and advancing the understanding of cooperative behaviors in multi-agent systems.</p>
<p>By providing insights into the complexities of agent interactions and the dynamics of cooperative AI systems, this work significantly contributes to the growing field of large language models.The framework not only emphasizes the potential for autonomous agent collaboration but also sets the stage for future research endeavors aimed at improving the scalability and efficacy of communicative agents in diverse applications.With CAMEL, Li et al. pave the way for more sophisticated interactions among agents, enhancing the capabilities of language models and their applications in real-world scenarios.</p>
<p>3) CrewAI: In the paper by Berti et al. [25], the CrewAI framework is introduced as a crucial component for implementing the AI-Based Agents Workflow (AgWf) paradigm, aimed at enhancing process mining (PM) tasks through the integration of Large Language Models (LLMs).CrewAI serves as a Python framework that facilitates the design and execution of AgWf, enabling developers to harness the capabilities of LLMs in a structured manner.The framework is built upon several key concepts: AI-based agents, AI-based tasks, and tools.AI-based agents combine LLMs with tailored system prompts, effectively aligning the model's behavior with specific roles.This role prompting is essential for ensuring that the agents perform their designated tasks accurately.</p>
<p>Within the CrewAI framework, AI-based tasks are defined textual instructions linked to these AI-based agents, allowing for a clear delineation of responsibilities.Furthermore, tools are implemented as Python classes or functions, which can be integrated into tasks based on their documentation strings, including input parameters and output types.The framework supports both traditional sequential execution of tasks and more complex concurrent execution through hierarchical processes, although further development is needed in this area.By decomposing complex PM tasks into simpler, manageable workflows, CrewAI aims to enhance the reasoning capabilities of LLMs, thus addressing the limitations that arise when these models are faced with intricate scenarios.</p>
<p>The CrewAI framework exemplifies a modern approach to leveraging AI for process mining by combining the strengths of LLMs with deterministic tools to produce high-quality outputs.The paper details various AI-based tasks that can be employed within CrewAI for PM applications, including prompt optimizers, ensembles, routers, evaluations, and output improvers.Through practical examples such as root cause analysis and bias detection in process mining event logs, Berti et al. demonstrate the potential of CrewAI to revolutionize how process mining tasks are approached in the era of AIbased agents.The framework not only provides a pathway for implementing effective workflows but also encourages further research into automating workflow definitions and enhancing agent evaluation frameworks.</p>
<p>4) MetaGPT: The paper by Hong et al. [26] introduces MetaGPT, an innovative meta-programming framework designed to enhance collaboration among multi-agent systems built on large language models (LLMs).Existing LLMbased multi-agent systems excel at simple dialogue tasks but struggle with complex scenarios due to logic inconsistencies and cascading hallucinations that arise from naively chaining LLMs together.MetaGPT addresses these challenges by incorporating efficient human workflows through the encoding of Standardized Operating Procedures (SOPs) into structured prompt sequences.This approach allows agents with humanlike domain expertise to verify intermediate results, thus reducing errors and improving overall performance.</p>
<p>A key feature of MetaGPT is its assembly line paradigm, which efficiently assigns diverse roles to various agents, breaking down complex tasks into manageable subtasks that promote effective collaboration.The framework emphasizes role specialization and structured communication, enhancing the agents' ability to interact and share information.By implementing a communication protocol that includes structured interfaces and a publish-subscribe mechanism, agents can access relevant information from other roles and the environment, thereby streamlining the workflow and facilitating a more coherent solution generation process.</p>
<p>MetaGPT represents a significant advancement in the development of LLM-based multi-agent systems, combining flexibility and convenience with robust functionality.The integration of human-like SOPs within the framework minimizes unproductive collaboration, while the novel executable feedback mechanism allows for real-time debugging and code execution during runtime, leading to notable improvements in code generation quality.MetaGPT's impressive performance on benchmarks like HumanEval and MBPP underscores its potential as a valuable tool for future research and application in multi-agent collaborations, paving the way for more effective and coherent solutions in complex problem-solving scenarios.</p>
<p>5) LangGraph: The LangGraph framework [27] emerges as a powerful tool for developing advanced Retrieval-Augmented Generation (RAG) systems, particularly for knowledge-based question-answering (QA) applications.Unlike traditional RAG models that often suffer from accuracy degradation due to their reliance on static pre-loaded knowledge, LangGraph leverages graph technology to enhance the information retrieval process.By enabling efficient searches and evaluations of the reliability of retrieved data, LangGraph significantly improves the contextual understanding and accuracy of generated responses.This innovative approach not only mitigates the limitations of existing RAG models but also facilitates the integration of real-time data, allowing for a more dynamic and accurate information synthesis process.</p>
<p>LangGraph stands out among other frameworks by providing a stateful, multi-actor application environment specifically designed for LLMs.Its capability to create agent workflows as cyclic graph structures allows developers to define intricate flows and control the state of the application, which is essential for building reliable agents.The LangGraph Conversational Retrieval Agent further enhances this by incorporating language processing, AI model integration, and graph-based data management, making it an ideal option for crafting sophisticated language-based AI applications.Its architecture encourages collaborative interactions among agents, ensuring that complex tasks are handled with precision and reliability.</p>
<p>Overall, the implementation of the LangGraph framework within the context of advanced RAG systems offers a compelling advantage over previously mentioned frameworks.Its focus on creating cyclic workflows not only allows for a more robust and efficient handling of multi-agent tasks but also significantly elevates the quality of responses through improved data processing and reliability assessment.The framework's ability to enhance real-time data accessibility and support diverse question types positions it as an invaluable resource for developing high-quality generative AI services, particularly in customer support and information retrieval applications.As demonstrated in Jeong's study, LangGraph provides a crucial foundation for advancing the capabilities of RAGbased systems, making it a preferred choice for researchers and developers in the field.</p>
<p>III. METHODOLOGY</p>
<p>This review aimed to systematically evaluate and synthesize existing research on large language model (LLM) multiagent systems, specifically addressing the aspects that directly support their application and scalability, while ensuring that a methodology synonymous with the standard practice of following the scientific method was utilized.</p>
<p>To focus the scope effectively, research questions were first defined to limit the exploration to technologies explicitly designed for multi-agent systems rather than those pertaining to LLMs and multi-agent systems independently.This choice allowed for a thorough examination of the unique intersections between LLMs and multi-agent interactions, avoiding the dilution of findings across broader, less targeted studies.</p>
<p>To cover the breadth of critical topics in the field, four primary aspects were identified: Architecture, Memory, Planning, and Technologies/Frameworks.Each aspect addresses an essential component in the design and operation of LLM-based multi-agent systems, reflecting the distinctive requirements and challenges of these systems.</p>
<p>The literature search was conducted across multiple wellregarded academic sources, including Google Scholar, IEEE Xplore, and arXiv, using targeted keywords associated with each of the four topics.Among these, arXiv proved to be the most valuable repository, providing a high concentration of relevant papers that detailed recent developments and experimental applications of LLM-based multi-agent systems.Each paper was evaluated for its relevance to the identified topics, with priority given to publications from credible authors and reputable conferences or journals.This selection process ensured that the reviewed literature included the most influential and innovative work within the field.</p>
<p>For each shortlisted paper, detailed content analysis was performed, with particular attention to descriptions of methods, architectures, and experimental setups.The merits and limitations of each approach were recorded, allowing for a comprehensive understanding of current capabilities, typical challenges, and areas with potential for improvement.This analysis informed recommendations for researchers and engineers in the field, offering guidance on optimal practices and common pitfalls in developing LLM-based multi-agent systems.</p>
<p>The findings from this review are synthesized to highlight prominent trends and identify future research opportunities, such as advancements in scalability and robustness.The focus on these emerging needs aims to guide ongoing research efforts in building systems that can effectively manage complex multi-agent interactions.An overview of this methodology is depicted in the following diagram.</p>
<p>IV. DISCUSSION</p>
<p>A. Key Findings</p>
<p>Upon analyzing various architectural approaches for LLMbased multi-agent systems, the "Mixture of Agents" (MoA) Regarding memory in LLM multi-agent systems, the analysis found that various memory approaches could be equally applicable, depending on the specific use case.Short-term memory models, for example, excel in scenarios where agents need rapid access to recent information but do not necessarily require extensive historical context.Conversely, long-term memory models are valuable for applications that demand more in-depth information retention over extended interactions.The choice of memory architecture should align with the intended function of the system, as this can have a significant impact on performance and scalability, especially in cases that require high responsiveness or nuanced historical recall.</p>
<p>For planning, the ReAct framework stands out as a preferred approach for integrating reasoning and action planning within LLM-based multi-agent systems.By enabling an interleaved generation of reasoning traces and task-specific actions, ReAct effectively addresses traditional limitations in complex task handling.This framework synergizes reasoning and action, allowing the model to update and manage plans while interacting with external sources like knowledge bases.ReAct has demonstrated its strengths across diverse tasks, including multi-hop question answering and fact verification, by mitigating common issues such as hallucination and error propagation.While ReAct provides an elegant solution for task planning, it could benefit from enhancements in action space handling, with possibilities for multi-task training or integration with reinforcement learning to expand its robustness and applicability.</p>
<p>In terms of technologies and frameworks for developing LLM-based multi-agent systems, this review found that the choice of framework often depends on the requirements of the application rather than any inherent superiority of one framework over another.Factors such as ease of integration, support for specific programming languages, scalability, and compatibility with external data sources play a significant role in determining which framework is optimal.As the field continues to evolve, the adaptability of frameworks to new advances and compatibility with emerging tools for LLMs will be crucial for their sustained utility.</p>
<p>B. Future Directions</p>
<p>In summary, this review has identified the Mixture of Agents architecture and ReAct planning framework as highly effective strategies for designing and managing LLM-based multi-agent systems.Memory and technology choices remain largely application-specific, underscoring the importance of aligning system components with the desired outcomes.Together, these findings present a roadmap for future developments in LLM multi-agent systems, where ongoing research and refined frameworks will likely contribute to increasingly robust, versatile applications in this domain.</p>
<p>V. CONCLUSION</p>
<p>The development of LLM-based multi-agent systems marks a significant step forward in enabling complex, collaborative AI applications.This review has identified core frameworks and methodologies that enhance system effectiveness, such as the Mixture of Agents (MoA) for structured agent collaboration and the ReAct framework for integrating reasoning and action.Memory architectures within multi-agent systems remain diverse, with application-specific requirements determining the most suitable approach for short-term or longterm data retention.Technology choices hinge on application demands, favoring adaptable frameworks capable of integrating external data and supporting scalability.Although these systems exhibit great potential, ongoing challenges-including computational costs, communication optimization, and role specialization-must be addressed for broader applicability.Future research and practical advancements are essential to evolving these frameworks, promoting resilient and flexible LLM-based multi-agent systems poised to support sophisticated AI-driven workflows in varied domains.</p>
<p>Fig. 1 :
1
Fig. 1: Overview of Survey Methodology</p>
<p>TABLE I :
I
Comparison of Architectures in LLM Multi-Agent Systems
ArchitectureKey FeaturesStrengthsLimitationsConquer-and-MergeMultiple LLM-powered agents en-Outperforms single-agent methods;Lacks integration of sophisticatedDiscussion (CMD)gage in open discussions; Simu-Allows agents to build on eachreasoning techniques; Limited test-lates human-like debates to im-other's inputs.ing on broader domains; Analysisprove reasoning.required with other models for gen-eralizability.Chain-of-AgentsSequential processing of input por-Reduces complexity of long-Communicationimprovement(CoA)tions by worker agents; Man-contexttasks;Enhancesneeded;Furtherrefinementager agent aggregates results; Inter-interpretability by splitting workrequired to reduce computationalleaved read-process method.across agents.costs and latency.Agent ForestEmploys a sampling-and-votingPerformance improves with theGains depend on task difficulty;approach with multiple agents; Per-number of agents; Effective forIncreased computational costs dueformance determined by majoritycomplex tasks.to multiple LLM queries.voting.Mixture-of-AgentsLayered design with proposers andLeverages strengths of differentRole effectiveness varies among(MoA)aggregators; Collaborates to gen-LLMs; Shows impressive resultsLLMs; Complexity in managingerate and synthesize high-qualityacross benchmarks.collaboration increases with moreoutputs.agents.</p>
<p>TABLE II :
II
Comparison of Planning Frameworks in LLM Multi-Agent Systems
FrameworkKey FeaturesStrengthsLimitationsAdaPlannerDynamic adjustments to plansFlexibility in complex, long-Relies on few-shot expert demon-based on real-time feedback; In-horizontasks;Improvedstrations; Performance needs fur-plan and out-of-plan refinementadaptability and efficiency throughther testing across diverse domains.strategies; Code-style promptingskill discovery.for reducing ambiguity.ChatCoTTool-augmented chain-of-thoughtAchieves 7.9% relative improve-Limited testing with GPT-4; Highreasoning for chat-based interac-ment in performance on complexcomputational requirements; Opti-tions; Iterative, multi-turn conver-tasks; Enhances adaptability in rea-mized for chat-based models.sations.soning scenarios.KnowAgentIntegration of an action knowledgeMatches or exceeds state-of-the-Primarily evaluated on specificbase with self-learning strategies;art performance; Reduces planningtasks; Currently onlyRefines planning paths for coherenthallucinations effectively.single-agent applications; Manualaction sequences.design of action knowledge basesis labor-intensive.RAPRetrieval-augmented techniques in-Achieves state-of-the-art perfor-Adapting for multimodal taskstegrated with contextual memory;mance in various benchmarks; En-requires careful implementation;Functions in both text-only andhances decision-making by utiliz-Complexity in managing retrievalmultimodal environments.ing past experiences.processes.ReActInterleaves reasoning and acting;Superior performance across di-Simplicity may limit complexityFacilitates decision traces and task-verse tasks; Effective in mitigat-handling; Further demonstrationsspecific actions; Enhances inter-ing hallucination and error prop-needed for large action spaces.pretability and trustworthiness.agation; Highly recommended foraction planning capabilities.</p>
<p>TABLE III :
III
Comparison of Memory Frameworks in LLM Multi-Agent Systems
FrameworkKey FeaturesStrengthsLimitationsVectorDatabasesEfficient storage and retrieval ofReduces reliance on static mem-Integration challenges; Requires(VecDB)high-dimensional vector represen-ory; Mitigates hallucinations andoptimization for compatibility withtations; Enhances LLMs' ability tooutdated information; Enables dy-LLM architectures.access external knowledge bases.namic, context-aware interactions.Retrieval-AugmentedCombines parametric and non-Improves specificity, diversity, andComplexity in retrieval index man-Generation (RAG)memory; Retrieves rel-factual accuracy; Scalable solu-agement; Requires extensive train-evant information during languagetion for updating LLMs with newing data for optimal performance.generation.knowledge.ChatDBIntegrates symbolic memory viaEnables precise memory manipu-Limited to tasks that can beSQL databases; Supports multi-lation; Reduces error propagation;mapped to SQL operations; Re-hop reasoning through externalEnhances performance in complexquires careful SQL instruction gen-database interaction.reasoning tasks.eration.MemoryBankOffers long-term memory retentionEnhances personalized responses;Performance may vary based onand dynamic updating; Uses theAdapts over time to user interac-user interaction context; RequiresEbbinghaus Forgetting Curve fortions; Versatile for various LLMcareful memory updating to avoidselective memory management.architectures.overload.RET-LLMIntegrates a read-write memoryunit; Stores knowledge as tripletsfor scalable, interpretable memorymanagement.</p>
<p>TABLE IV :
IV
Comparison of Technologies and Frameworks in LLM Multi-Agent Systems
FrameworkKey FeaturesStrengthsLimitationsAutoGenOpen-source framework for multi-Enhances multi-agent cooperation;Still in early development; Integra-agent conversations; Supports di-Flexible definition of agent behav-tion of existing agent implementa-verse modes of agent interaction;iors; Outperforms state-of-the-arttions requires further research.Combines LLMs, human inputs,approaches.and tools.CAMELRole-playing approach with in-Autonomous cooperation reducesChallenges with conversation de-ception prompting; Facilitates taskhuman input; Scalable solution toviation and role flipping; Requirescompletion among communicativeconversation challenges; Includesevaluation in diverse contexts.agents; Generates conversationalopen-sourced agent implementa-data.tions.CrewAIFramework for AI-Based AgentsEnhances reasoning capabilities ofFurther development needed forWorkflow (AgWf); IntegratesLLMs; Breaks down complex tasksconcurrent execution; Manual de-LLMs with AI-based tasks andinto manageable workflows; En-sign of agents can be complex.tools; Supports sequential andcourages high-quality outputs.concurrent task execution.MetaGPTMeta-programming framework thatReduces errors through human-Complexity in implementation; Re-integrates Standardized Operatinglike verification; Improves collabo-liance on structured prompts mayProcedures (SOPs); Assigns rolesration and solution generation; No-limit flexibility.to agents and enhances structuredtable performance improvements incommunication.benchmarks.LangGraphEnhancesRetrieval-AugmentedImproves accuracy and contextualComplexity in managing cyclicGeneration (RAG) systems withunderstanding; Enables real-timeworkflows; Reliance on graphgraph technology; Allows cyclicdata integration; Supports collabo-structures may pose a learningworkflows for agent applications.rative interactions among agents.curve.</p>
<p>Attention Is All You Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, arXiv:1706.037622017</p>
<p>Large Language Models: A Survey. S Minaee, T Mikolov, N Nikzad, M Chenaghlu, R Socher, X Amatriain, J Gao, arXiv:2402.061962024</p>
<p>On the Limitations of Large Language Models (LLMs): False Attribution. T Adewumi, N Habib, L Alkhaled, E Barney, arXiv:2404.046312024</p>
<p>A Perspective on Large Language Models, Intelligent Machines, and Knowledge Acquisition. V Cherkassky, E Hock Lee, arXiv:2408.065982024</p>
<p>LLM Multi-Agent Systems: Challenges and Open Problems. S Han, Q Zhang, Y Yao, W Jin, Z Xu, C He, arXiv:2402.035782024</p>
<p>Large Language Model based Multi-Agents: A Survey of Progress and Challenges. T Guo, X Chen, Y Wang, R Chang, S Pei, N V Chawla, O Wiest, X Zhang, arXiv:2402.016802024</p>
<p>Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?. Q Wang, Z Wang, Y Su, H Tong, Y Song, arXiv:2402.182722024</p>
<p>Chain of Agents: Large Language Models Collaborating on Long-Context Tasks. Y Zhang, R Sun, Y Chen, T Pfister, R Zhang, S  Arik, arXiv:2406.028182024</p>
<p>More Agents Is All You Need. J Li, Q Zhang, Y Yu, Q Fu, D Ye, arXiv:2402.051202024</p>
<p>Mixture-of-Agents Enhances Large Language Model Capabilities. J Wang, J Wang, B Athiwaratkun, C Zhang, J Zou, arXiv:2406.046922024</p>
<p>AdaPlanner: Adaptive Planning from Feedback with Language Models. H Sun, Y Zhuang, L Kong, B Dai, C Zhang, arXiv:2305.166532023</p>
<p>ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models. Z Chen, K Zhou, B Zhang, Z Gong, W X Zhao, J.-R Wen, arXiv:2305.143232023</p>
<p>KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents. Y Zhu, S Qiao, Y Ou, S Deng, N Zhang, S Lyu, Y Shen, L Liang, J Gu, H Chen, arXiv:2403.031012024</p>
<p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents. T Kagaya, T J Yuan, Y Lou, J Karlekar, S Pranata, A Kinose, K Oguri, F Wick, Y You, arXiv:2402.036102024</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022</p>
<p>When Large Language Models Meet Vector Databases: A Survey. Z Jing, Y Su, Y Han, B Yuan, H Xu, C Liu, K Chen, M Zhang, arXiv:2402.017632024</p>
<p>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Kttler, M Lewis, W -T. Yih, T Rocktschel, S Riedel, D Kiela, arXiv:2005.114012020</p>
<p>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory. C Hu, J Fu, C Du, S Luo, J Zhao, H Zhao, arXiv:2306.039012023</p>
<p>MemoryBank: Enhancing Large Language Models with Long-Term Memory. W Zhong, L Guo, Q Gao, H Ye, Y Wang, arXiv:2305.102502023</p>
<p>RET-LLM: Towards a General Read-Write Memory for Large Language Models. A Modarressi, A Imani, M Fayyaz, H Schtze, arXiv:2305.143222023</p>
<p>Enhancing Large Language Model with Self-Controlled Memory Framework. B Wang, X Liang, J Yang, H Huang, S Wu, P Wu, L Lu, Z Ma, Z Li, arXiv:2304.133432023</p>
<p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation. Q Wu, G Bansal, J Zhang, Y Wu, B Li, E Zhu, L Jiang, X Zhang, S Zhang, J Liu, A H Awadallah, R W White, D Burger, C Wang, arXiv:2308.081552023</p>
<p>CAMEL: Communicative Agents for 'Mind' Exploration of Large Language Model Society. G Li, H A A Kader Hammoud, H Itani, D Khizbullin, B Ghanem, arXiv:2303.177602023</p>
<p>Re-Thinking Process Mining in the AI-Based Agents Era. A Berti, M Maatallah, U Jessen, M Sroka, S A Ghannouchi, arXiv:2408.077202024</p>
<p>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. S Hong, M Zhuge, J Chen, X Zheng, Y Cheng, C Zhang, J Wang, Z Wang, S K Shing Yau, Z Lin, L Zhou, C Ran, L Xiao, C Wu, J Schmidhuber, arXiv:2308.003522023</p>
<p>A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph. C Jeong, arXiv:2407.199942024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-654 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-654</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-654</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-9b8327b04667269fdae78cd34064eb2ee05ddee8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9b8327b04667269fdae78cd34064eb2ee05ddee8" target="_blank">Multi-Object Representation Learning with Iterative Variational Inference</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work argues for the importance of learning to segment and represent objects jointly, and demonstrates that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations.</p>
                <p><strong>Paper Abstract:</strong> Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns -- without supervision -- to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e654.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e654.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IODINE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Object Decomposition Inference NEtwork (IODINE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid neural–probabilistic system that models a scene as a spatial Gaussian mixture (explicit probabilistic generative model) whose components are parameterized and inferred by neural networks using amortized iterative variational inference and a learned refinement network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IODINE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>IODINE is built around an explicit probabilistic generative model: a spatial Gaussian mixture in which each of K slots corresponds to an object and has a latent vector z_k. Each z_k is decoded (via a shared broadcast convolutional decoder) into pixel-wise means μ_k and mask-logits which are normalized across slots to produce per-pixel mixture weights. Inference uses amortized iterative variational inference: an initial posterior parameterization λ^(1) is refined over T iterations by a learned refinement network f_φ (convolutional layers followed by an LSTM) which consumes the image-sized auxiliary inputs (image, current means, masks, mask-logits, pixelwise likelihoods, leave-one-out likelihoods, and important gradient signals such as ∇_{μ} L, ∇_{m} L and ∇_{λ} L) plus samples z ~ q_λ to produce additive updates to posterior parameters. Training is end-to-end by backprop through the unrolled refinement iterations (with gradients stopped for certain gradient-inputs to avoid numerical instabilities). The stochastic sampling in each refinement step acts as an auxiliary inference variable and, together with the iterative slot-wise message-passing, enables multimodal posterior behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>An explicit probabilistic generative model (spatial Gaussian mixture model / mixture-of-components per pixel with global fixed variance) together with a variational Bayesian objective (ELBO) and a statistical notion of slots/components; masks correspond to component responsibilities in the mixture — i.e., a declarative spatial mixture model.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks implementing the decoder (spatial broadcast convolutional decoder producing per-slot RGB means and mask logits) and the iterative refinement network f_φ (convolutional trunk + LSTM) that amortizes posterior updates; sampling via reparameterization trick; trained with gradient-based optimization (Adam).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight end-to-end integration: the declarative mixture likelihood is parameterized by neural decoders; inference is implemented as amortized iterative variational inference where the neural refinement network ingests both image-level signals and explicit gradient signals (∇_{λ}L, ∇_{μ}L, ∇_{m}L) computed from the probabilistic model and returns additive updates to posterior parameters. Masks are combined with a softmax across slots to instantiate the mixture. Training uses backprop through unrolled iterations (with stop-gradient for double-derivative inputs). Stochastic sampling inside the inference loop couples the probabilistic and neural parts and enables multimodal inference.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Unsupervised discovery of object-centric decomposition (per-slot masks and reconstructions); disentangled per-object latent representations whose object attributes (position, color, shape, size) are linearly decodable; inpainting of occluded object parts; systematic compositional generalization to scenes with more objects and to novel feature combinations (e.g., unseen green sphere); capacity to represent and resolve multimodal/multi-stable decompositions (different valid segmentations) due to sampling in iterative inference; slot-exchangeability and ability to keep excess slots empty; slot–to–object stability across sequential iterations enabling unsupervised tracking when applied to sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Unsupervised multi-object scene decomposition and representation; evaluated on CLEVR (and CLEVR6 subset), Multi-dSprites, Tetris, Shapes, and preliminary sequential Objects Room sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Segmentation ARI scores (foreground-only, mean ± std across seeds): CLEVR6: 0.988 ± 0.000; Multi-dSprites: 0.797 ± 0.056; Tetris: 0.992 ± 0.004. (Additional evaluations: linear factor regression/R^2 for attribute decoding reported in paper figures.)</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>MONet (neural baseline) ARI reported in paper: CLEVR6: 0.962 ± 0.006; Multi-dSprites: 0.904 ± 0.008 (as reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Good compositional generalization: can be run with different K at test time (e.g., trained with K=7 on up to 6 objects, evaluated with K=11 on 9 objects with only modest degradation); generalizes to unseen feature combinations (e.g., green sphere) and to more inference iterations than trained for (converges rapidly in 3–5 iterations and is stable for more iterations up to a point). Failure modes include models trained with too few slots which learn entangled multi-object slot encodings and do not benefit from increasing K at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability: per-slot masked reconstructions and mask logits provide explicit, human-interpretable object assignments; per-object latents are disentangled and linearly decodable into object attributes (verified with linear probes); the mask- and reconstruction-based outputs serve as direct explanations for model outputs; multi-stable interpretations are inspectable by inspecting different inference runs and corresponding slot assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Memory and compute scale linearly with number of slots K and number of iterations T (training requires keeping activations for every slot and refinement step); relies on handcrafted auxiliary inputs (gradient signals) which incur extra backward passes (∇_{λ}L is ~20% of compute); numerical instabilities from double derivatives require stopping gradients on some inputs; struggles on complex real-world data (on ImageNet it segments largely by color not objectness; fails to capture foreground digits on Textured MNIST); must choose a maximum slot count K (though K can be varied at test time); when trained with too few slots, it learns entangled representations and cannot be rescued by increasing K at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Iterative amortized variational inference within a variational Bayes framework applied to a structured spatial mixture generative model; the refinement network can be interpreted as amortizing (an approximation to) gradient-based posterior updates, and the alternating updates resemble message-passing/EM; stochastic sampling in the iterative procedure provides an implicit auxiliary inference variable enabling multimodal posterior representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Object Representation Learning with Iterative Variational Inference', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e654.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e654.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probabilistic Programming Approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Probabilistic programming based generative models (e.g., stroke-based character generation; Bayesian indoor scene models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Declarative probabilistic models that explicitly encode compositional generative processes (programs) for data, enabling sample-efficient and interpretable generation and inference but typically requiring hand-engineered model structure and specialized inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Human-level concept learning through probabilistic program induction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Probabilistic programming generative models</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models expressed as probabilistic programs which specify hierarchical, symbolic generative processes (for example, program primitives for strokes in character generation or geometric scene priors for indoor scenes). Inference is performed by the chosen probabilistic programming inference engines (MCMC, specialized inference, or sometimes learned amortized inference). These models emphasize compositional symbolic structure and often produce highly interpretable latent programs or symbolic descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit probabilistic program or Bayesian generative model specifying symbolic/compositional primitives and rules (e.g., stroke primitives, scene layout rules), typically represented in a probabilistic programming language or as structured Bayesian models.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Inference and perception front-ends: sampling/MCMC/variational inference algorithms (imperative procedures); in some work neural networks are used as perceptual pipelines or learned proposal distributions (amortized inference), but the core model remains declarative.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular combination in practice: hand-specified declarative generative model combined with separate imperative inference algorithms (MCMC, variational inference) and sometimes neural amortized proposal/inference networks; typically not a single end-to-end differentiable neural parameterization of the entire generative process.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Strong compositional generalization and sample-efficient learning in domains well-captured by the programmatic prior; explicit symbolic explanations and latent programmatic structure enabling interpretable generalization and one-shot learning (as in stroke-based character experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>One-shot character concept learning (stroke-based characters) and 3D indoor scene modeling/rendering tasks (as cited examples).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Excellent in-domain compositional generalization and sample efficiency when the program captures the domain structure; poor scalability to raw complex real-world images unless much domain engineering is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability: latent programs or symbolic scene descriptions provide human-readable explanations for generated/computed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Require careful engineering of the generative model; limited scalability to high-resolution, richly varying real-world data; often not fully learned from data (hand-designed primitives/prior needed).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Bayesian generative modeling via probabilistic programming: specifying priors and likelihoods as a programmatic generative process and performing Bayesian inference over programmatic latent structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Object Representation Learning with Iterative Variational Inference', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e654.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e654.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural EM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Expectation Maximization (NEM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that amortizes the EM algorithm using recurrent neural networks to perform iterative inference for spatial mixture models, intended to discover object components in an unsupervised manner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural expectation maximization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Expectation Maximization (NEM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NEM frames scene decomposition as inference in a (spatial) mixture model and uses recurrent neural networks to amortize the E and M steps: RNNs predict cluster assignments and update component parameters iteratively. The architecture couples a (declarative) EM algorithmic structure with learned (imperative) RNN-based update rules.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Expectation-Maximization algorithm for mixture models (statistical mixture model with responsibilities and component parameter updates).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Recurrent neural networks (RNNs) used to produce amortized update steps that mimic EM updates and to represent component parameters/encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Amortization of EM by neural networks: the algorithmic EM structure provides the declarative skeleton while learned RNNs supply the update operators executed iteratively; interactions follow EM-like expectation and maximization updates but with learned neural parameterizations.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables unsupervised grouping and object-discovery via learned iterative updates; can capture spatial clustering structure more flexibly than handcrafted EM, but in the experiments reported here it failed to handle colored images robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Unsupervised image segmentation / object discovery on synthetic datasets (paper notes NEM variants failed on colored CLEVR and could only be compared on binarized datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Demonstrates object discovery on simpler/binarized data; did not scale to colored scenes in the comparisons described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Produces per-pixel assignments akin to clustering responsibilities, interpretable as segmentation masks and component explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported inability (in authors' experiments) to cope with colored images such as CLEVR; scalability and robustness limitations relative to IODINE/MONet on those datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Connection to classical Expectation-Maximization interpreted as an iterative inference algorithm, with RNNs amortizing the iterative update rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Object Representation Learning with Iterative Variational Inference', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e654.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e654.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MONet+IODINE hybrid (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid combining MONet's sequential attention with IODINE's iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed hybrid (discussed but not implemented) combining MONet's sequential mask-attention mechanism (which recurrently attends to one object at a time) with IODINE's iterative per-slot refinement, to potentially get benefits of both dynamic object counts and iterative refinement of masks/representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MONet–IODINE Hybrid (proposed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The proposed hybrid would use MONet's masking network to sequentially select yet-unexplained image regions (one object at a time) while replacing or augmenting MONet's per-step VAE with IODINE's iterative refinement per attended object (or alternatively, applying iterative refinement after sequential attention). This would combine a sequential attention/scheduling mechanism with iterative message-passing-like refinement across slots.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Algorithmic sequential attention / mask-prior (a procedural masking schedule that determines which portion of the image is considered 'unexplained') and an underlying spatial-mixture view for composing the scene (softmaxed masks correspond to component assignment rules).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural masking network from MONet, neural decoder, and IODINE's refinement network (convolutional + LSTM) — all trained with gradient-based learning; possible use of iterative stochastic sampling during refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular integration: sequential masking module (MONet) provides per-object inputs to a refinement/inference module (IODINE) that iteratively refines the object's latent parameters and masks; envisioned end-to-end training of masking and iterative refinement, but the paper only proposes this idea and does not implement it.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Hypothesized advantages include combining dynamic varying object count (MONet) with the ability to iteratively refine difficult boundaries and resolve occlusion/ambiguity (IODINE), potentially improved segmentation fidelity, better handling of complex mask shapes, and retention of iterative multimodal inference capability.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Suggested for complex unsupervised scene decomposition tasks (CLEVR-like and more complex scenes); not evaluated in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Speculative: expected to inherit MONet's dynamic object count generalization and IODINE's compositional and multi-modal inference generalization, but untested.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Would retain per-object masks and reconstructions as interpretable outputs; sequential ordering might provide salience priors that are interpretable (background first, then large/front objects, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Not implemented or evaluated in the paper; potential increased training complexity and compute cost, and potential challenges in jointly training sequential attention and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Proposed complementary strengths: sequential attention provides scheduling/dynamic object handling while iterative refinement supplies iterative message-passing and multimodal posterior refinement; idea motivated by complementarity rather than formal proof.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Object Representation Learning with Iterative Variational Inference', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e654.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e654.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tagger</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tagger: Deep unsupervised perceptual grouping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative unsupervised grouping model that uses a Ladder network / denoising-based objective to produce perceptual grouping; noted to lack an explicit object bottleneck and to scale poorly to large images without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tagger: Deep unsupervised perceptual grouping</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Tagger</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tagger performs iterative denoising-based grouping using Ladder networks and a denoising objective to partition inputs into groups; it is an iterative neural grouping system rather than a probabilistic generative model with an explicit object slot bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Implicit denoising/grouping algorithmic objective (procedural), but no explicit declarative probabilistic generative model with slot-style components.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural Ladder architectures trained with denoising objectives executed iteratively to produce grouping assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Iterative neural grouping driven by a denoising loss; grouping emerges from learned neural dynamics rather than explicit probabilistic mixture structure.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Can perform unsupervised perceptual grouping on simpler tasks; lacks compact per-object latent representations because of the Ladder architecture design.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Unsupervised perceptual grouping on small/binarized images; not scaled to CLEVR in this work due to computational/architecture constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Limited scaling to larger images; would require major adaptation to scale to CLEVR-sized images.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Produces grouping outputs but no explicit per-slot object bottleneck; interpretability of outputs is lower compared to slot-based methods that produce per-object latents and masks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No bottleneck (so no compact object latents), and extremely large parameter count if naively applied to large images (would require ~600M parameters for CLEVR in original form), making scaling impractical without architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Denoising-based iterative grouping approach implemented with Ladder networks; grouping emerges via learned denoising dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-Object Representation Learning with Iterative Variational Inference', 'publication_date_yy_mm': '2019-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural expectation maximization <em>(Rating: 2)</em></li>
                <li>MONet: Unsupervised scene decomposition and representation <em>(Rating: 2)</em></li>
                <li>Tagger: Deep unsupervised perceptual grouping <em>(Rating: 2)</em></li>
                <li>Human-level concept learning through probabilistic program induction <em>(Rating: 2)</em></li>
                <li>Bayesian geometric modeling of indoor scenes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-654",
    "paper_id": "paper-9b8327b04667269fdae78cd34064eb2ee05ddee8",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "IODINE",
            "name_full": "Iterative Object Decomposition Inference NEtwork (IODINE)",
            "brief_description": "A hybrid neural–probabilistic system that models a scene as a spatial Gaussian mixture (explicit probabilistic generative model) whose components are parameterized and inferred by neural networks using amortized iterative variational inference and a learned refinement network.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "IODINE",
            "system_description": "IODINE is built around an explicit probabilistic generative model: a spatial Gaussian mixture in which each of K slots corresponds to an object and has a latent vector z_k. Each z_k is decoded (via a shared broadcast convolutional decoder) into pixel-wise means μ_k and mask-logits which are normalized across slots to produce per-pixel mixture weights. Inference uses amortized iterative variational inference: an initial posterior parameterization λ^(1) is refined over T iterations by a learned refinement network f_φ (convolutional layers followed by an LSTM) which consumes the image-sized auxiliary inputs (image, current means, masks, mask-logits, pixelwise likelihoods, leave-one-out likelihoods, and important gradient signals such as ∇_{μ} L, ∇_{m} L and ∇_{λ} L) plus samples z ~ q_λ to produce additive updates to posterior parameters. Training is end-to-end by backprop through the unrolled refinement iterations (with gradients stopped for certain gradient-inputs to avoid numerical instabilities). The stochastic sampling in each refinement step acts as an auxiliary inference variable and, together with the iterative slot-wise message-passing, enables multimodal posterior behavior.",
            "declarative_component": "An explicit probabilistic generative model (spatial Gaussian mixture model / mixture-of-components per pixel with global fixed variance) together with a variational Bayesian objective (ELBO) and a statistical notion of slots/components; masks correspond to component responsibilities in the mixture — i.e., a declarative spatial mixture model.",
            "imperative_component": "Neural networks implementing the decoder (spatial broadcast convolutional decoder producing per-slot RGB means and mask logits) and the iterative refinement network f_φ (convolutional trunk + LSTM) that amortizes posterior updates; sampling via reparameterization trick; trained with gradient-based optimization (Adam).",
            "integration_method": "Tight end-to-end integration: the declarative mixture likelihood is parameterized by neural decoders; inference is implemented as amortized iterative variational inference where the neural refinement network ingests both image-level signals and explicit gradient signals (∇_{λ}L, ∇_{μ}L, ∇_{m}L) computed from the probabilistic model and returns additive updates to posterior parameters. Masks are combined with a softmax across slots to instantiate the mixture. Training uses backprop through unrolled iterations (with stop-gradient for double-derivative inputs). Stochastic sampling inside the inference loop couples the probabilistic and neural parts and enables multimodal inference.",
            "emergent_properties": "Unsupervised discovery of object-centric decomposition (per-slot masks and reconstructions); disentangled per-object latent representations whose object attributes (position, color, shape, size) are linearly decodable; inpainting of occluded object parts; systematic compositional generalization to scenes with more objects and to novel feature combinations (e.g., unseen green sphere); capacity to represent and resolve multimodal/multi-stable decompositions (different valid segmentations) due to sampling in iterative inference; slot-exchangeability and ability to keep excess slots empty; slot–to–object stability across sequential iterations enabling unsupervised tracking when applied to sequences.",
            "task_or_benchmark": "Unsupervised multi-object scene decomposition and representation; evaluated on CLEVR (and CLEVR6 subset), Multi-dSprites, Tetris, Shapes, and preliminary sequential Objects Room sequences.",
            "hybrid_performance": "Segmentation ARI scores (foreground-only, mean ± std across seeds): CLEVR6: 0.988 ± 0.000; Multi-dSprites: 0.797 ± 0.056; Tetris: 0.992 ± 0.004. (Additional evaluations: linear factor regression/R^2 for attribute decoding reported in paper figures.)",
            "declarative_only_performance": null,
            "imperative_only_performance": "MONet (neural baseline) ARI reported in paper: CLEVR6: 0.962 ± 0.006; Multi-dSprites: 0.904 ± 0.008 (as reported in Table 1).",
            "has_comparative_results": true,
            "generalization_properties": "Good compositional generalization: can be run with different K at test time (e.g., trained with K=7 on up to 6 objects, evaluated with K=11 on 9 objects with only modest degradation); generalizes to unseen feature combinations (e.g., green sphere) and to more inference iterations than trained for (converges rapidly in 3–5 iterations and is stable for more iterations up to a point). Failure modes include models trained with too few slots which learn entangled multi-object slot encodings and do not benefit from increasing K at test time.",
            "interpretability_properties": "High interpretability: per-slot masked reconstructions and mask logits provide explicit, human-interpretable object assignments; per-object latents are disentangled and linearly decodable into object attributes (verified with linear probes); the mask- and reconstruction-based outputs serve as direct explanations for model outputs; multi-stable interpretations are inspectable by inspecting different inference runs and corresponding slot assignments.",
            "limitations_or_failures": "Memory and compute scale linearly with number of slots K and number of iterations T (training requires keeping activations for every slot and refinement step); relies on handcrafted auxiliary inputs (gradient signals) which incur extra backward passes (∇_{λ}L is ~20% of compute); numerical instabilities from double derivatives require stopping gradients on some inputs; struggles on complex real-world data (on ImageNet it segments largely by color not objectness; fails to capture foreground digits on Textured MNIST); must choose a maximum slot count K (though K can be varied at test time); when trained with too few slots, it learns entangled representations and cannot be rescued by increasing K at test time.",
            "theoretical_framework": "Iterative amortized variational inference within a variational Bayes framework applied to a structured spatial mixture generative model; the refinement network can be interpreted as amortizing (an approximation to) gradient-based posterior updates, and the alternating updates resemble message-passing/EM; stochastic sampling in the iterative procedure provides an implicit auxiliary inference variable enabling multimodal posterior representations.",
            "uuid": "e654.0",
            "source_info": {
                "paper_title": "Multi-Object Representation Learning with Iterative Variational Inference",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Probabilistic Programming Approaches",
            "name_full": "Probabilistic programming based generative models (e.g., stroke-based character generation; Bayesian indoor scene models)",
            "brief_description": "Declarative probabilistic models that explicitly encode compositional generative processes (programs) for data, enabling sample-efficient and interpretable generation and inference but typically requiring hand-engineered model structure and specialized inference.",
            "citation_title": "Human-level concept learning through probabilistic program induction",
            "mention_or_use": "mention",
            "system_name": "Probabilistic programming generative models",
            "system_description": "Models expressed as probabilistic programs which specify hierarchical, symbolic generative processes (for example, program primitives for strokes in character generation or geometric scene priors for indoor scenes). Inference is performed by the chosen probabilistic programming inference engines (MCMC, specialized inference, or sometimes learned amortized inference). These models emphasize compositional symbolic structure and often produce highly interpretable latent programs or symbolic descriptions.",
            "declarative_component": "Explicit probabilistic program or Bayesian generative model specifying symbolic/compositional primitives and rules (e.g., stroke primitives, scene layout rules), typically represented in a probabilistic programming language or as structured Bayesian models.",
            "imperative_component": "Inference and perception front-ends: sampling/MCMC/variational inference algorithms (imperative procedures); in some work neural networks are used as perceptual pipelines or learned proposal distributions (amortized inference), but the core model remains declarative.",
            "integration_method": "Modular combination in practice: hand-specified declarative generative model combined with separate imperative inference algorithms (MCMC, variational inference) and sometimes neural amortized proposal/inference networks; typically not a single end-to-end differentiable neural parameterization of the entire generative process.",
            "emergent_properties": "Strong compositional generalization and sample-efficient learning in domains well-captured by the programmatic prior; explicit symbolic explanations and latent programmatic structure enabling interpretable generalization and one-shot learning (as in stroke-based character experiments).",
            "task_or_benchmark": "One-shot character concept learning (stroke-based characters) and 3D indoor scene modeling/rendering tasks (as cited examples).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Excellent in-domain compositional generalization and sample efficiency when the program captures the domain structure; poor scalability to raw complex real-world images unless much domain engineering is applied.",
            "interpretability_properties": "High interpretability: latent programs or symbolic scene descriptions provide human-readable explanations for generated/computed outputs.",
            "limitations_or_failures": "Require careful engineering of the generative model; limited scalability to high-resolution, richly varying real-world data; often not fully learned from data (hand-designed primitives/prior needed).",
            "theoretical_framework": "Bayesian generative modeling via probabilistic programming: specifying priors and likelihoods as a programmatic generative process and performing Bayesian inference over programmatic latent structure.",
            "uuid": "e654.1",
            "source_info": {
                "paper_title": "Multi-Object Representation Learning with Iterative Variational Inference",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Neural EM",
            "name_full": "Neural Expectation Maximization (NEM)",
            "brief_description": "An approach that amortizes the EM algorithm using recurrent neural networks to perform iterative inference for spatial mixture models, intended to discover object components in an unsupervised manner.",
            "citation_title": "Neural expectation maximization",
            "mention_or_use": "mention",
            "system_name": "Neural Expectation Maximization (NEM)",
            "system_description": "NEM frames scene decomposition as inference in a (spatial) mixture model and uses recurrent neural networks to amortize the E and M steps: RNNs predict cluster assignments and update component parameters iteratively. The architecture couples a (declarative) EM algorithmic structure with learned (imperative) RNN-based update rules.",
            "declarative_component": "Expectation-Maximization algorithm for mixture models (statistical mixture model with responsibilities and component parameter updates).",
            "imperative_component": "Recurrent neural networks (RNNs) used to produce amortized update steps that mimic EM updates and to represent component parameters/encodings.",
            "integration_method": "Amortization of EM by neural networks: the algorithmic EM structure provides the declarative skeleton while learned RNNs supply the update operators executed iteratively; interactions follow EM-like expectation and maximization updates but with learned neural parameterizations.",
            "emergent_properties": "Enables unsupervised grouping and object-discovery via learned iterative updates; can capture spatial clustering structure more flexibly than handcrafted EM, but in the experiments reported here it failed to handle colored images robustly.",
            "task_or_benchmark": "Unsupervised image segmentation / object discovery on synthetic datasets (paper notes NEM variants failed on colored CLEVR and could only be compared on binarized datasets).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Demonstrates object discovery on simpler/binarized data; did not scale to colored scenes in the comparisons described in this paper.",
            "interpretability_properties": "Produces per-pixel assignments akin to clustering responsibilities, interpretable as segmentation masks and component explanations.",
            "limitations_or_failures": "Reported inability (in authors' experiments) to cope with colored images such as CLEVR; scalability and robustness limitations relative to IODINE/MONet on those datasets.",
            "theoretical_framework": "Connection to classical Expectation-Maximization interpreted as an iterative inference algorithm, with RNNs amortizing the iterative update rules.",
            "uuid": "e654.2",
            "source_info": {
                "paper_title": "Multi-Object Representation Learning with Iterative Variational Inference",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "MONet+IODINE hybrid (proposed)",
            "name_full": "Hybrid combining MONet's sequential attention with IODINE's iterative refinement",
            "brief_description": "A proposed hybrid (discussed but not implemented) combining MONet's sequential mask-attention mechanism (which recurrently attends to one object at a time) with IODINE's iterative per-slot refinement, to potentially get benefits of both dynamic object counts and iterative refinement of masks/representations.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "MONet–IODINE Hybrid (proposed)",
            "system_description": "The proposed hybrid would use MONet's masking network to sequentially select yet-unexplained image regions (one object at a time) while replacing or augmenting MONet's per-step VAE with IODINE's iterative refinement per attended object (or alternatively, applying iterative refinement after sequential attention). This would combine a sequential attention/scheduling mechanism with iterative message-passing-like refinement across slots.",
            "declarative_component": "Algorithmic sequential attention / mask-prior (a procedural masking schedule that determines which portion of the image is considered 'unexplained') and an underlying spatial-mixture view for composing the scene (softmaxed masks correspond to component assignment rules).",
            "imperative_component": "Neural masking network from MONet, neural decoder, and IODINE's refinement network (convolutional + LSTM) — all trained with gradient-based learning; possible use of iterative stochastic sampling during refinement.",
            "integration_method": "Modular integration: sequential masking module (MONet) provides per-object inputs to a refinement/inference module (IODINE) that iteratively refines the object's latent parameters and masks; envisioned end-to-end training of masking and iterative refinement, but the paper only proposes this idea and does not implement it.",
            "emergent_properties": "Hypothesized advantages include combining dynamic varying object count (MONet) with the ability to iteratively refine difficult boundaries and resolve occlusion/ambiguity (IODINE), potentially improved segmentation fidelity, better handling of complex mask shapes, and retention of iterative multimodal inference capability.",
            "task_or_benchmark": "Suggested for complex unsupervised scene decomposition tasks (CLEVR-like and more complex scenes); not evaluated in the paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Speculative: expected to inherit MONet's dynamic object count generalization and IODINE's compositional and multi-modal inference generalization, but untested.",
            "interpretability_properties": "Would retain per-object masks and reconstructions as interpretable outputs; sequential ordering might provide salience priors that are interpretable (background first, then large/front objects, etc.).",
            "limitations_or_failures": "Not implemented or evaluated in the paper; potential increased training complexity and compute cost, and potential challenges in jointly training sequential attention and iterative refinement.",
            "theoretical_framework": "Proposed complementary strengths: sequential attention provides scheduling/dynamic object handling while iterative refinement supplies iterative message-passing and multimodal posterior refinement; idea motivated by complementarity rather than formal proof.",
            "uuid": "e654.3",
            "source_info": {
                "paper_title": "Multi-Object Representation Learning with Iterative Variational Inference",
                "publication_date_yy_mm": "2019-03"
            }
        },
        {
            "name_short": "Tagger",
            "name_full": "Tagger: Deep unsupervised perceptual grouping",
            "brief_description": "An iterative unsupervised grouping model that uses a Ladder network / denoising-based objective to produce perceptual grouping; noted to lack an explicit object bottleneck and to scale poorly to large images without adaptation.",
            "citation_title": "Tagger: Deep unsupervised perceptual grouping",
            "mention_or_use": "mention",
            "system_name": "Tagger",
            "system_description": "Tagger performs iterative denoising-based grouping using Ladder networks and a denoising objective to partition inputs into groups; it is an iterative neural grouping system rather than a probabilistic generative model with an explicit object slot bottleneck.",
            "declarative_component": "Implicit denoising/grouping algorithmic objective (procedural), but no explicit declarative probabilistic generative model with slot-style components.",
            "imperative_component": "Neural Ladder architectures trained with denoising objectives executed iteratively to produce grouping assignments.",
            "integration_method": "Iterative neural grouping driven by a denoising loss; grouping emerges from learned neural dynamics rather than explicit probabilistic mixture structure.",
            "emergent_properties": "Can perform unsupervised perceptual grouping on simpler tasks; lacks compact per-object latent representations because of the Ladder architecture design.",
            "task_or_benchmark": "Unsupervised perceptual grouping on small/binarized images; not scaled to CLEVR in this work due to computational/architecture constraints.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Limited scaling to larger images; would require major adaptation to scale to CLEVR-sized images.",
            "interpretability_properties": "Produces grouping outputs but no explicit per-slot object bottleneck; interpretability of outputs is lower compared to slot-based methods that produce per-object latents and masks.",
            "limitations_or_failures": "No bottleneck (so no compact object latents), and extremely large parameter count if naively applied to large images (would require ~600M parameters for CLEVR in original form), making scaling impractical without architectural changes.",
            "theoretical_framework": "Denoising-based iterative grouping approach implemented with Ladder networks; grouping emerges via learned denoising dynamics.",
            "uuid": "e654.4",
            "source_info": {
                "paper_title": "Multi-Object Representation Learning with Iterative Variational Inference",
                "publication_date_yy_mm": "2019-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural expectation maximization",
            "rating": 2
        },
        {
            "paper_title": "MONet: Unsupervised scene decomposition and representation",
            "rating": 2
        },
        {
            "paper_title": "Tagger: Deep unsupervised perceptual grouping",
            "rating": 2
        },
        {
            "paper_title": "Human-level concept learning through probabilistic program induction",
            "rating": 2
        },
        {
            "paper_title": "Bayesian geometric modeling of indoor scenes",
            "rating": 1
        }
    ],
    "cost": 0.02287025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Multi-Object Representation Learning with Iterative Variational Inference</h1>
<p>Klaus Greff ${ }^{12}$ Raphaël Lopez Kaufman ${ }^{3}$ Rishabh Kabra ${ }^{3}$ Nick Watters ${ }^{3}$ Chris Burgess ${ }^{3}$ Daniel Zoran ${ }^{3}$<br>Loic Matthey ${ }^{3}$ Matthew Botvinick ${ }^{3}$ Alexander Lerchner ${ }^{3}$</p>
<h4>Abstract</h4>
<p>Human perception is structured around objects which form the basis for our higher-level cognition and impressive systematic generalization abilities. Yet most work on representation learning focuses on feature learning without even considering multiple objects, or treats segmentation as an (often supervised) preprocessing step. Instead, we argue for the importance of learning to segment and represent objects jointly. We demonstrate that, starting from the simple assumption that a scene is composed of multiple entities, it is possible to learn to segment images into interpretable objects with disentangled representations. Our method learns - without supervision - to inpaint occluded parts, and extrapolates to scenes with more objects and to unseen objects with novel feature combinations. We also show that, due to the use of iterative variational inference, our system is able to learn multi-modal posteriors for ambiguous inputs and extends naturally to sequences.</p>
<h2>1. Introduction</h2>
<p>Learning good representations of complex visual scenes is a challenging problem for artificial intelligence that is far from solved. Recent breakthroughs in unsupervised representation learning (Higgins et al., 2017a; Makhzani et al., 2015; Chen et al., 2016) tend to focus on data where a single object of interest is placed in front of some background (e.g. dSprites, 3D Chairs, CelebA). Yet in general, visual scenes contain a variable number of objects arranged in various spatial configurations, and often with partial occlusions (e.g., CLEVR, Johnson et al. 2017; see Figure 1). This motivates the question: what forms a good representation of a scene with multiple objects? In line with recent advances (Burgess et al., 2019; van Steenkiste et al., 2018; Eslami et al., 2016),</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Object decomposition of an image from the CLEVR dataset by IODINE. The model is able to decompose the image into separate objects in an unsupervised manner, inpainting occluded objects in the process (see slots (d), (e) and (h)).
we maintain that discovery of objects in a scene should be considered a crucial aspect of representation learning, rather than treated as a separate problem.</p>
<p>We approach the problem from a spatial mixture model perspective (Greff et al., 2017) and use amortized iterative refinement (Marino et al., 2018b) of latent object representations within a variational framework (Rezende et al., 2014; Kingma \&amp; Welling, 2013). We encode our basic intuition about the existence of objects into the structure of our model, which simultaneously facilitates their discovery and efficient representation in a fully data-driven, unsupervised manner. We name the resulting architecture IODINE (short for Iterative Object Decomposition Inference NEtwork).</p>
<p>IODINE can segment complex scenes and learn disentangled object features without supervision on datasets like CLEVR, Objects Room (Burgess et al., 2019), and Tetris (see Appendix B). We show systematic generalization to more objects than included in the training regime, as well as objects formed with unseen feature combinations. This highlights the benefits of multi-object representation learning by comparison to a VAE's single-slot representations. We also justify how the sampling used in iterative refinement lends to resolving multi-modal and multi-stable decomposition.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Generative model illustrations. (a) A regular VAE decoder. (b) A hypothetical multi-object VAE decoder that recomposes the scene from three objects. (c) IODINE's multi-object decoder showing latent vectors (denoted z) corresponding to $K$ objects refined over $N$ iterations from images of dimension $D$. The deterministic pixel-wise means and masks are denoted $\boldsymbol{\mu}$ and $\mathbf{m}$ respectively. (d) The neural architecture of the IODINE's multiobject spatial mixture decoder.</p>
<h2>2. Method</h2>
<p>We first express multi-object representation learning within the framework of generative modelling (Section 2.1). Then, building upon the successful Variational AutoEncoder framework (VAEs; Rezende et al. 2014; Kingma &amp; Welling 2013), we leverage variational inference to jointly learn both the generative and inference model (Section 2.2). There we also discuss the particular challenges that arise for inference in a multi-object context and show how they can be solved using iterative amortization. Finally, in Section 2.3 we bring all elements together and show how the complete system can be trained end-to-end.</p>
<h3>2.1. Multi-Object Representations</h3>
<p>Flat vector representations as used by standard VAEs are inadequate for capturing the combinatorial object structure that many datasets exhibit. To achieve the kind of systematic generalization that is so natural for humans, we propose employing a multi-slot representation where each slot shares the underlying representation format, and each would ideally describe an independent part of the input. Consider the example in Figure 1: by construction, the scene consists of 8 objects, each with its own properties such as shape, size, position, color and material. To split objects, a flat representation would have to represent each object using separate feature dimensions. But this neglects the simple and (to us) trivial fact that they are interchangeable objects with common properties.</p>
<p>Generative Model We represent each scene with $K$ latent object representations $\mathbf{z}<em k="k">{k} \in \mathbb{R}^{M}$ that collaborate to generate the input image $\mathbf{x} \in \mathbb{R}^{D}$ (c.f. Figure 2b). The $\mathbf{z}</em>$ are assumed to be independent and their generative mechanism is shared such that any ordering of them produces the same image (i.e. entailing permutation invariance). Objects distinguished in this way can easily be compared, reused and recombined, thus facilitating combinatorial generalization.</p>
<p>The image $\mathbf{x}$ is modeled with a spatial Gaussian mixture model where each mixing component (slot) corresponds to a single object. That means each object vector $\mathbf{z}<em i="i" k="k">{k}$ is decoded into a pixel-wise mean $\mu</em>$, the likelihood thus becomes:}$ (the appearance of the object) and a pixel-wise assignment $m_{i k}=p\left(C=k \mid \mathbf{z}_{k}\right)$ (the segmentation mask; c.f. Figure 2c). Assuming that the pixels $i$ are independent conditioned on $\mathbf{z</p>
<p>$$
p(\mathbf{x} \mid \mathbf{z})=\prod_{i=1}^{D} \sum_{k=1}^{K} m_{i k} \mathcal{N}\left(x_{i} ; \mu_{i k}, \sigma^{2}\right)
$$</p>
<p>where we use a global fixed variance $\sigma^{2}$ for all pixels.
Decoder Structure Our decoder network structure directly reflects the structure of the generative model. See Figure 2d for an illustration. Each object latent $\mathbf{z}<em k="k">{k}$ is decoded separately into pixel-wise means $\boldsymbol{\mu}</em>}$ and mask-logits $\tilde{\mathbf{m}<em k="k">{k}$, which we then normalize using a softmax operation applied across slots such that the masks $\mathbf{m}</em>$ parameterize the spatial mixture distribution as defined in Equation (1). For the network architecture we use a broadcast decoder }$ for each pixel sum to 1 . Together, $\boldsymbol{\mu}$ and $\mathbf{m<em>(Watters et al., 2019)</em>, which spatially replicates the latent vector $\mathbf{z}_{k}$, appends two coordinate channels (ranging from -1 to 1 horizontally and vertically), and applies a series of size-preserving convolutional layers. This structure encourages disentangling the position across the image from other features such as color or texture, and generally supports disentangling. All slots $k$ share weights to ensure a common format, and are independently decoded, up until the mask normalization.</p>
<h3>2.2. Inference</h3>
<p>Similar to VAEs, we use amortized variational inference to get an approximate posterior $q_{\lambda}(\mathbf{z} \mid \mathbf{x})$ parameterized as a Gaussian with parameters $\boldsymbol{\lambda}=\left{\boldsymbol{\mu}<em _mathbf_z="\mathbf{z">{\mathbf{z}}, \boldsymbol{\sigma}</em>\right}$. However, our object-oriented generative model poses a few specific challenges for the inference process: Firstly, being a (spatial) mixture model, we need to infer both the components (i.e. object appearance) and the mixing (i.e. object segmentation). This type of problem is well known, for example in clustering and image segmentation, and is traditionally tackled as an iterative procedure, because there are no efficient direct solutions. A related second problem is that any slot can, in principle, explain any pixel. Once a pixel is explained}</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Illustration of the iterative inference procedure.
by one of the slots, the others don't need to account for it anymore. This explaining-away property complicates the inference by strongly coupling it across the individual slots. Finally, slot permutation invariance induces a multimodal posterior with at least one mode per slot permutation. This is problematic, since our approximate posterior $q_{\lambda}(\mathbf{z} \mid \mathbf{x})$ is parameterized as a unimodal distribution. For all the above reasons, the standard feed-forward VAE inference model is inadequate for our case, so we consider a more powerful method for inference.</p>
<p>Iterative Inference The basic idea of iterative inference is to start with an arbitrary guess for the posterior parameters $\boldsymbol{\lambda}$, and then iteratively refine them using the input and samples from the current posterior estimate. We build on the framework of iterative amortized inference (Marino et al., 2018b), which uses a trained refinement network $f_{\phi}$. Unlike Marino et al., we consider only additive updates to the posterior and use several salient auxiliary inputs $\mathbf{a}$ to the refinement network (instead of just $\nabla_{\lambda} \mathcal{L}$ ). We update the posterior of the $K$ slots independently and in parallel (indicated by $\stackrel{\wedge}{\leftarrow}$ and $\stackrel{\wedge}{\sim}$ ), as follows:</p>
<p>$$
\begin{aligned}
\mathbf{z}<em _lambda="\lambda">{k}^{(t)} &amp; \stackrel{\wedge}{\sim} q</em>}\left(\mathbf{z<em k="k">{k}^{(t)} \mid \mathbf{x}\right) \
\boldsymbol{\lambda}</em>}^{(t+1)} &amp; \stackrel{\wedge}{\leftarrow} \boldsymbol{\lambda<em _phi="\phi">{k}^{(t)}+f</em>}\left(\mathbf{z<em k="k">{k}^{(t)}, \mathbf{x}, \mathbf{a}</em>\right)
\end{aligned}
$$</p>
<p>Thus the only place where the slots interact are at the input level. As refinement network $f_{\phi}$ we use a convolutional network followed by an LSTM (see Appendix C for details). Instead of amortizing the posterior directly (as in a regular VAE encoder), the refinement network can be thought of as amortizing the gradient of the posterior (Marino et al., 2018a). The alternating updates to $q_{\lambda}(\mathbf{z} \mid \mathbf{x})$ and $p(\mathbf{x} \mid \mathbf{z})$ are also akin to message passing.</p>
<p>Inputs For each slot $k$ we feed a set of auxiliary inputs $\boldsymbol{a}<em _phi="\phi">{k}$ to the refinement network $f</em>}$ which then computes an update for the posterior $\boldsymbol{\lambda<em k="k">{k}$. Crucially, we include gradient information about the ELBO in the inputs, as it conveys information about what is not yet explained by other slots. Omitting the superscript $(t)$ for clarity, the auxiliary inputs $\mathbf{a}</em>$ are (see Appendix C for details): image</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="w"> </span><span class="nt">IODINE</span><span class="w"> </span><span class="nt">Pseudocode</span><span class="o">.</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">image</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">hyperparamters</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">K</span><span class="o">,</span><span class="w"> </span><span class="nt">T</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">trainable</span><span class="w"> </span><span class="nt">parameters</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(1)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">Initialize</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">h</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(1)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">stackrel</span><span class="p">{</span><span class="err">\wedge</span><span class="p">}{</span><span class="err">\leftarrow</span><span class="p">}</span><span class="w"> </span><span class="nt">0</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">for</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">t</span><span class="o">=</span><span class="nt">1</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">to</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">T</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">do</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">stackrel</span><span class="p">{</span><span class="err">\wedge</span><span class="p">}{</span><span class="err">\sim</span><span class="p">}</span><span class="w"> </span><span class="nt">q_</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Sample</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mu</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{m</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">stackrel</span><span class="p">{</span><span class="err">\wedge</span><span class="p">}{</span><span class="err">\leftarrow</span><span class="p">}</span><span class="w"> </span><span class="nt">g_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Decode</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">softmax</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">hat</span><span class="p">{</span><span class="err">\boldsymbol{m</span><span class="p">}</span><span class="err">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Masks</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nt">sum_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">m</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">N</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\mu</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">sigma</span><span class="o">^</span><span class="p">{</span><span class="err">2</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Likelihood</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathcal</span><span class="p">{</span><span class="err">L</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="w"> </span><span class="nt">D_</span><span class="p">{</span><span class="err">K</span><span class="w"> </span><span class="err">L</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">q_</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="nt">p</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="o">))</span><span class="nt">-</span><span class="err">\</span><span class="nt">log</span><span class="w"> </span><span class="nt">p</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">mid</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">a</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">\mathbf{k</span><span class="p">}</span><span class="err">}</span><span class="w"> </span><span class="err">\</span><span class="nt">stackrel</span><span class="p">{</span><span class="err">\wedge</span><span class="p">}{</span><span class="err">\leftarrow</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">operatorname</span><span class="p">{</span><span class="err">inputs</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">x</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">z</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Inputs</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\lambda</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t+1)</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">h</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t+1)</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">stackrel</span><span class="p">{</span><span class="err">\wedge</span><span class="p">}{</span><span class="err">\leftarrow</span><span class="p">}</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">\phi</span><span class="p">}</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">a</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="nt">mathbf</span><span class="p">{</span><span class="err">h</span><span class="p">}</span><span class="nt">_</span><span class="p">{</span><span class="err">k</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="err">(t)</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="w"> </span><span class="err">\</span><span class="nt">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Refinement</span>
<span class="w">    </span><span class="nt">end</span><span class="w"> </span><span class="nt">for</span>
</code></pre></div>

<p>$\mathbf{x}$, means $\boldsymbol{\mu}<em k="k">{k}$, masks $\mathbf{m}</em>}$, mask-logits $\hat{\boldsymbol{m}<em _boldsymbol_mu="\boldsymbol{\mu">{k}$, mean gradient $\nabla</em><em _mathbf_m="\mathbf{m">{k}} \mathcal{L}$, mask gradient $\nabla</em><em _boldsymbol_lambda="\boldsymbol{\lambda">{k}} \mathcal{L}$, posterior gradient $\nabla</em><em k="k">{k}} \mathcal{L}$, posterior mask $p\left(\mathbf{m}</em>\right)$, and two coordinate channels like in the decoder.} \mid \mathbf{x}, \boldsymbol{\mu}\right)=\frac{p\left(x \mid \mu_{k}\right)}{\sum_{j} p\left(x \mid \mu_{j}\right)}$, pixelwise likelihood $p(\mathbf{x} \mid \mathbf{z})$, leave-one-out likelihood $p\left(\mathbf{x} \mid \mathbf{z}_{i \neq k</p>
<p>With the exception of $\nabla_{\boldsymbol{\lambda}<em _boldsymbol_lambda="\boldsymbol{\lambda">{k}} \mathcal{L}$, these are all image-sized and cheap to compute, so we feed them as additional inputchannels into the refinement network. The approximate gradient $\nabla</em><em k="k">{k}} \mathcal{L}$ is computed using the reparameterization trick by a backward pass through the generator network. This is computationally quite expensive, but we found that this information helps to significantly improve training of the refinement network. This input is the same size as the posterior $\boldsymbol{\lambda}</em>$ and is fed to the LSTM part of the refinement network. Like Marino et al. (2018b) we found it beneficial to normalize the gradient-based inputs with LayerNorm (Ba et al., 2016). See Section 4.3 for an ablation study.</p>
<h3>2.3. Training</h3>
<p>We train the parameters of the decoder $(\boldsymbol{\theta})$, of the refinement network $(\boldsymbol{\phi})$, and of the initial posterior $\left(\boldsymbol{\lambda}^{(1)}\right)$ by gradient descent through the unrolled iterations. In principle, it is enough to minimize the final negative ELBO $\mathcal{L}^{T}$, but we found it beneficial to use a weighted sum which also includes earlier terms:</p>
<p>$$
\mathcal{L}<em t="1">{\text {total }}=\sum</em>
$$}^{T} \frac{t}{T} \mathcal{L}^{(t)</p>
<p>Each refinement step of IODINE uses gradient information to optimize the posterior $\boldsymbol{\lambda}$. Unfortunately, backpropagating through this process leads to numerical instabilities connected to double derivatives like $\nabla_{\Theta} \nabla_{z} \mathcal{L}$. We found that this problem can be mitigated by dropping the double derivative terms, i.e. stopping the gradients from backpropagating through the gradient-inputs $\nabla_{\boldsymbol{\mu}<em _mathbf_m="\mathbf{m">{k}} \mathcal{L}, \nabla</em><em _boldsymbol_lambda="\boldsymbol{\lambda">{k}} \mathcal{L}$, and $\nabla</em>$ (see Appendix C for details).}_{k}} \mathcal{L</p>
<h2>3. Related Work</h2>
<p>Representation learning <em>(Bengio et al., 2013)</em> has received much attention and has seen several recent breakthroughs. This includes disentangled representations through the use of $\beta$-VAEs <em>(Higgins et al., 2017a)</em>, adversarial autoencoders <em>(Makhzani et al., 2015)</em>, Factor VAEs <em>(Kim &amp; Mnih, 2018)</em>, and improved generalization through non-euclidean embeddings <em>(Nickel &amp; Kiela, 2017)</em>. However, most advances have focused on the feature-level structure of representations, and do not address the issue of representing multiple, potentially repeating objects, which we tackle here.</p>
<p>Another line of work is concerned with obtaining segmentations of images, usually without considering representation learning. This has led to impressive results on real-world images, however, many approaches (such as “semantic segmentation” or object detection) rely on supervised signals <em>(Girshick, 2015; He et al., 2017; Redmon &amp; Farhadi, 2018)</em>, while others require hand-engineered features <em>(Shi &amp; Malik, 2000; Felzenszwalb &amp; Huttenlocher, 2004)</em>. In contrast, as we learn to both segment and represent, our method can perform inpainting (Figure 1) and deal with ambiguity (Figure 10), going beyond what most methods relying on feature engineering are currently able to do.</p>
<p>Works tackling the full problem of scene representation are rarer. Probabilistic programming based approaches, like stroke-based character generation <em>(Lake et al., 2015)</em> or 3D indoor scene rendering <em>(Pero et al., 2012)</em>, have produced appealing results, but require carefully engineered generative models, which are typically not fully learned from data. Work on end-to-end models has shown promise in using autoregressive inference or generative approaches <em>(Eslami et al., 2016; Gregor et al., 2015)</em>, including the recent MONet <em>(Burgess et al., 2019)</em>. Few methods can achieve similar comparable with the complexity of the scenes we consider here, apart from MONet. Section 4.1 shows a preliminary comparison between MONet and IODINE, and we discuss their relationship further in Appendix A.3.</p>
<p>Two other methods related to ours are Neural Expectation Maximization <em>(Greff et al., 2017)</em> (along with its sequential and relational extensions <em>(van Steenkiste et al., 2018)</em>) and Tagger <em>(Greff et al., 2016)</em>. NEM uses recurrent neural networks to amortize expectation maximization for a spatial mixture model. However, NEM variants fail to cope with colored scenes, as we note in our comparison in Section 4.1. Tagger also uses iterative inference to segment and represent images based on a denoising training objective. We disregard Tagger for our comparison, because (1) its use of a Ladder network means that there is no bottleneck and thus no explicit object representations, and (2) without adapting it to a convolutional architecture, it does not scale to larger images (Tagger would require $\approx 600$ M weights for CLEVR).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. IODINE segmentations and object reconstructions on CLEVR6 (top), Multi-dSprites (middle), and Tetris (bottom). The individual masked reconstruction slots represent objects separately (along with their shadow on CLEVR). Border colours are matched to the segmentation mask on the left.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Prediction accuracy / $R^{2}$ score for the factor regression on CLEVR6. Position is continuous; the rest are categorical with 8 colors, 3 shapes, and 2 sizes. IODINE (deconv) does not use spatial broadcasting in the decoder (see Section 4.3).</p>
<h2>4. Results</h2>
<p>We evaluate our model on three main datasets: 1) CLEVR <em>(Johnson et al., 2017)</em> and a variant CLEVR6 which uses only scenes with up to 6 objects, 2) a multi-object version of the dSprites dataset <em>(Matthey et al., 2017)</em>, and 3) a dataset of multiple "Tetris"-like pieces that we created. In all cases we train the system using the Adam optimizer <em>(Kingma &amp; Ba, 2015)</em> to minimize the negative ELBO for $10^{6}$ updates. We varied several hyperparameters, including: number of slots, dimensionality of $\mathbf{z}_{k}$, number of inference iterations, number of convolutional layers and their filter sizes, batch size, and learning rate. For details of the models and hyperparameters refer to Appendix C and the code for all experiments can be found on GitHub.</p>
<h3>4.1. Decomposition</h3>
<p>IODINE can provide a readily interpretable segmentation of the data, as seen in Figure 4. These examples clearly demonstrate the models ability to segmenting out the same objects which were used to generate the dataset, despite never having received supervision to do so. To quantify segmentation quality, we measure the similarity between ground-truth (instance) segmentations and our predicted object masks using the Adjusted Rand Index (ARI; [Rand 1971</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Disentanglement in regular VAEs vs IODINE. Rows indicate traversals of single latents, annotated by our interpretation of their effects. (Left) When a VAE is trained on single-object scenes it can disentangle meaningful factors of variation. (Center) When the same VAE is trained on multi-object scenes, the latents entangle across both factors and objects. (Right) In contrast, traversals of individual latents in IODINE vary individual factors of single objects, here the orange cylinder. Thus, the architectural bias for discovering multiple entities in a common format enables not only the discovery of objects, but also facilitates disentangling of their features.</p>
<p>Hubert &amp; Arabie 1985). ARI is a measure of clustering similarity that ranges from 0 (chance) to 1 (perfect clustering) and can handle arbitrary permutations of the clusters. We apply it as a measure of instance segmentation quality by treating each foreground pixel (ignoring the background) as one point and its segmentation as cluster assignment. As shown in Table 1, IODINE achieves almost perfect ARI scores of around 0.99 for CLEVR6, and Tetris as well as a relatively good score of 0.77 for Multi-dSprites. The lower scores on Multi-dSprites are largely because IODINE struggles to produce sharp boundaries for the sprites, and we are uncertain as to the reasons for this behaviour.</p>
<p>We compare with MONet (Burgess et al., 2019), following the CLEVR model implementation described in the paper except using fewer (7) slots and different standard deviations for the decoder distribution (0.06 and 0.1 for σbg and σfg, respectively), which gave better scores. With this, MONet obtained a similar ARI score (0.96) as IODINE on CLEVR6, and on Multi-dSprites it performed significantly better with a score of 0.90 (using the unmodified model). We also attempted to compare ARI scores to Neural Expectation Maximization, but neither Relational-NEM nor the simpler RNN-NEM variant could cope well with colored images. As a result, we could only compare with those methods on a binarized version of Multi-dSprites and the Shapes dataset. These scores are summarized in Table 1.</p>
<h3>4.2. Representation Quality</h3>
<p>Information Content The object-reconstructions in Figure 4 show that their representations contain all the information about the object. But in what format, and how usable is it? To answer this question we associate each ground-truth object with its corresponding zk based on the segmentation masks. We then train a single-layer network to predict ground-truth factors for each object. Note that this predictor is trained <em>after</em> IODINE has finished training (i.e. no</p>
<table>
<thead>
<tr>
<th></th>
<th>IODINE</th>
<th>R-NEM</th>
<th>MONet</th>
</tr>
</thead>
<tbody>
<tr>
<td>CLEVR6</td>
<td>0.988 ± 0.000</td>
<td>+</td>
<td>0.962 ± 0.006</td>
</tr>
<tr>
<td>M-dSprites</td>
<td>0.797 ± 0.056</td>
<td>+</td>
<td>0.904 ± 0.008</td>
</tr>
<tr>
<td>M-dSprites bin.</td>
<td>0.648 ± 0.172</td>
<td>0.685 ± 0.017</td>
<td></td>
</tr>
<tr>
<td>Shapes</td>
<td>0.910 ± 0.119</td>
<td>0.776 ± 0.019</td>
<td></td>
</tr>
<tr>
<td>Tetris</td>
<td>0.992 ± 0.004</td>
<td>+</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 1. Summary of IODINE's segmentation performance in terms of ARI (mean ± stddev across five seeds) versus baseline models. For each independent run, we computed the ARI score over 320 images, using only foreground pixels. We then picked the best hyperparameter combination for each model according to the mean ARI score over five random seeds.</p>
<p>supervised fine-tuning). It tells us if a linear mapping is sufficient to extract information like color, position, shape or size of an object from its latent representation, and gives an important indication about the usefulness of the representation. Results in Figure 5 clearly show that a linear mapping is sufficient to extract relevant information about these object attributes from the latent representation to high accuracy. This result is in contrast with the scene representations learned by a standard VAE. Here even training the factor-predictor is difficult, as there is no obvious way to align objects with features. To make this comparison, we chose a canonical ordering of the objects based on their size, material, shape, and position (with decreasing precedence). The precedence of features was intended as a heuristic to maximize the predictability of the ordering. We then trained a linear network to predict the concatenated features of the canonically ordered objects from the latent scene representation. As the results in Figure 5 indicate, the information is present, but in a much less explicit/usable state.</p>
<p>Disentanglement Disentanglement is another important desirable property of representations (Bengio et al., 2013) that captures how well learned features separate and correspond to individual, interpretable factors of variation in the data. While its precise definition is still highly debated (Hig-</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. IODINE's iterative inference process and generalization capabilities. Rows indicate steps of iterative inference, refining reconstructions and segmentations when moving down the figure. Of particular interest is the <em>explaining away</em> effect visible between slots 2 and 3, where they settle on different objects despite both starting with the large cylinder. The model was only trained with K = 7 slots on 3-6 objects (excluding green spheres), and yet is able to generalize to K = 11 slots (only 4 are shown, see Figure 19 in the appendix for a full version) on a scene with 9 objects, including the never seen before green sphere (last column).</p>
<p>gins et al., 2018; Eastwood &amp; Williams, 2018; Ridgeway &amp; Mozer, 2018; Locatello et al., 2018), the concept of disentanglement has generated a lot of interest recently. Good disentanglement is believed to lead to both better generalization and more interpretable features (Lake et al., 2016; Higgins et al., 2017b). Interestingly, for these desirable advantages to bear out, disentangled features seem to be most useful for properties of single objects, such as color, position, shape, etc. It is much less clear how to operationalize this in order to create disentangled representations of entire scenes with variable numbers of objects. And indeed, if we train a VAE that can successfully disentangle features of a single-object dataset, we find that that its representation becomes highly entangled on a multi-object dataset, (see Figure 6 left vs middle). IODINE, on the other hand, successfully learns disentangled representations, because it is able to first decompose the scene and then represent individual objects (Figure 6 right). In Figure 6 we show traversals of the most important features (selected by KL) of a standard VAE vs IODINE. While the standard VAE clearly entangles many properties even across multiple objects, IODINE is able to neatly separate them.</p>
<p><strong>Generalization</strong> Finally, we can ask directly: Does the system generalize to novel scenes in a systematic way? Specifically, does it generalize to scenes with more or fewer objects than ever encountered during training? Slots are exchangeable by design, so we can freely vary the number of slots during test-time (more on this in Section 4.3).</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. The effect of varying the number of iterations, for both training and at test time. (a) Median ARI score, (b) MSE and (c) KL over test-iterations, for models trained with different numbers of iterations on CLEVR6. The region beyond the filled dots thus shows test-time generalization behavior. Shaded region from 25th to 75th percentile.</p>
<p>So in Figure 7 we qualitatively show the performance of a system that was trained with K = 7 on up to 6 objects, but evaluated with K = 11 on 9 objects. In Figure 9a the orange boxes show, that, even quantitatively, the segmentation performance decreases little when generalizing to more objects.</p>
<p>A more extreme form of generalization involves handling unseen feature combinations. To test this we trained our system on a subset of CLEVR that does not contain green spheres (though it does contain spheres and other green objects). And then we tested what the system does when confronted with a green sphere. In Figure 7 it can be seen that IODINE is still able to represent green spheres, despite never having seen this combination during training.</p>
<h3>4.3. Robustness &amp; Ablation</h3>
<p><strong>Iterations</strong> The number of iterations is one of the central hyperparameters to our approach. To investigate its impact, we trained four models with 1, 2, 4 and 6 iterations on CLEVR6, and evaluated them all using 15 iterations (c.f. Figure 8). The first thing to note is that the inference converges very quickly within the first 3-5 iterations after which neither the segmentation nor reconstruction change much. The second important finding is that the system is very stable for much longer than the number of iterations it was trained with. The model even further improves the segmentation and reconstruction when it is run for more iterations, though it eventually starts to diverge after about two to three times the number of training iterations as can be seen with the blue and orange curves in Figure 8.</p>
<p><strong>Slots</strong> The other central parameter of IODINE is the number of slots K, as it controls the maximum number of objects the system can separate. It is important to distinguish varying K for training vs varying it at test-time. As can be seen in Figure 9, if the model was trained with sufficiently many slots to fit all objects (K = 7, and K = 9), then test-time behavior generalizes very well. Typical behavior (not</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. IODINE trained on CLEVR6 with varying numbers of slots (columns). Evaluation of (a) ARI, (b) MSE, and (c) KL with 7 slots on 3-6 Objects (blue) and 11 slots on 3-9 objects (orange).
shown) is to leave excess slots empty, and when confronted with too many objects it will often completely ignore some of them, leaving the other object-representations mostly intact. Given enough slots at test time, such a model can even segment and represent scenes of higher complexity (more objects) than any scene encountered during training (see Figure 7 and the orange boxes in Figure 9). If on the other hand, the model was trained with too few slots ( $K=3$ and $K=5$ ), its performance suffers substantially. This happens because, here the only way to reconstruct the entire scene during training is to consistently represent multiple objects per slot. And that leads to the model learning inefficient and entangled representations akin to the VAE in Figure 6 (also apparent from their much higher KL in Figure 9c). Once learned, this sub-optimal strategy cannot be mitigated by increasing the number of slots at test-time as can be seen by their decreased performance in Figure 9a.</p>
<p>Input Ablations We ablated each of the different inputs to the refinement network described in Section 2.2. Broadly, we found that individually removing an input did not noticeably affect the results (with two exceptions noted below). See Figures 33-40 in the Appendix demonstrating this lack of effect on different terms of the model's loss and the ARI segmentation score on both CLEVR6 and Tetris. A more comprehensive analysis could ablate combinations of inputs and identify synergistic or redundant groups, and thus potentially simplify the model. We didn't pursue this direction since none of the inputs incurs any noticeable computational overhead and at some point during our experimentation each of them contributed towards stable training behavior.</p>
<p>The main exceptions to the above are $\nabla_{\lambda} \mathcal{L}$ and $\mathbf{x}$. Computing the former requires an entire backward pass through the decoder, and contributes about $20 \%$ of the computational cost of the entire model. But we found that it often substantially improves performance and training convergence, which justifies its inclusion. A somewhat surprising finding was that for the Tetris dataset, removing $\mathbf{x}$ from the list of inputs had a pronounced detrimental effect, while for CLEVR it was negligible.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Multi-stability of segmentation when presented with an ambiguous stimulus. Left: Depending on the random sampling during iterative refinement, IODINE can produce different permutations of groups (row 2 vs 3), a different decomposition (row 1) or sometimes an invalid segmentation and reconstruction (row 4). Right: PCA of the latent space, coloured by which slot corresponds to the background. Paths show the trajectory of the iterative refinement for the four examples on the left.</p>
<p>Broadcast Decoder Ablation We use the spatial broadcast decoder (Watters et al., 2019) primarily for its significant impact on the disentanglement of the representations, but its continuous spatial representation bias also seems to help decomposition. When replacing it with a deconvolution-based decoder the factor regression scores on CLEVR6 are significantly worse as can be seen in Figure 5. Especially for shape and size it now performs no better than the VAE which uses spatial broadcasting. The foreground-ARI scores also drop significantly ( $0.67 \pm 0.06$ down from 0.99 ) and the model seems less able to specialize slots to single objects (see Figure 23). Note though, that these discrepancies might easily be reduced, since we haven't invested much effort in tuning the architecture of the deconv-based decoder.</p>
<h3>4.4. Multi-Modality and Multi-Stability</h3>
<p>Standard VAEs are unable to represent multi-modal posteriors, because $q_{\lambda}(\mathbf{z} \mid \mathbf{x})$ is parameterized using a unimodal Gaussian distribution. However, as demonstrated in Figure 10, IODINE can actually handle this problem quite well. How is that possible? It turns out that this is an important side-effect of iterative variational inference, that to the best of our knowledge has not been noticed before: The stochasticity at each iteration, which results from sampling $\mathbf{z}$ to approximate the likelihood, implicitly acts as an auxilliary (inference) random variable. This effect compounds over iterations, and is amplified by the slot-structure and the effective message-passing between slots over the course of iterations. In effect the model can implicitly represent multiple modes (if integrated over all ways of sampling $\mathbf{z}$ ) and thus converge to different modes (see Figure 10 left) depending on these samples. This does not happen in a regular</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Segmentation challenges a) IODINE did not succeed in capturing the foreground digits in the Textured MNIST dataset. b) IODINE groups ImageNet not into meaningful objects but mostly into regions of similar color. c) On a grayscale version of CLEVR, IODINE still produces the desired groupings.</p>
<p>VAE, where no stochasticity enters the inference process. If we had an exact and deterministic way to compute the likelihood and its gradient, this effect would vanish.</p>
<p>A neat side-effect of this is the ability to elegantly capture ambiguous (aka multi-stable) segmentations such as the ones shown in Figure 10. We presented IODINE with an ambiguous arrangement of Tetris blocks, which has three different yet equally valid "explanations" (given the data distribution). When we evaluate a trained model on this image, we get different segmentations on different evaluations. Some of these correspond to different slot-orderings (1st vs 3rd row). But we also find qualitatively different segmentations (i.e. 3rd vs 4th row) that correspond to different interpretations of the scene. This is an impressive result given that multi-stability is a well-studied, pervasive feature of human perception that is important for handling ambiguity, and that is not modelled by any standard image recognition networks.</p>
<h2>5. Discussion and Future Work</h2>
<p>We have introduced IODINE, a novel approach for unsupervised representation learning of multi-object scenes, based on amortized iterative refinement of the inferred latent representation. We analyzed IODINE's performance on various datasets, including realistic images containing variable numbers of partially occluded 3D objects, and demonstrated that our method can successfully decompose the scenes into objects and represent each of them in terms of their individual properties such as color, size, and material. IODINE can robustly deal with occlusions by inpainting covered sections, and generalises beyond the training distribution in terms of numerosity and object-property combinations. Furthermore, when applied to scenes with ambiguity in terms of their object decomposition, IODINE can represent - and converge to - multiple valid solutions given the same input image.</p>
<p>We also probed the limits of our current setup by applying IODINE to the Textured MNIST dataset (Greff et al., 2016) and to ImageNet, testing how it would deal with texturesegmentation and more complex real-world data (Figure 11). Trained on ImageNet data, IODINE segmented mostly by color rather than by objects. This behavior is not unexpected: ImageNet was never designed as a dataset for unsupervised learning, and likely lacks the richness in poses, lighting, sizes, positions and distance variations required to learn object segmentations from scratch. Trained on Textured MNIST, IODINE was able to model the background, but mostly failed to capture the foreground digits. Together these results point to the importance of color as a strong cue for segmentation, especially early in the iterative refinement process. As demonstrated by our results on grayscale CLEVR (Figure 11c) though, color is not a requirement.</p>
<p>Beyond more diverse training data, we want to highlight three other promising directions to scale IODINE to richer real-world data. First, an extension to sequential data is attractive, because temporal data naturally contains rich statistics about objectness both in the movement itself, and in the smooth variations of object factors. IODINE can readily be applied to sequences feeding a new frame at every iteration, and we have done some preliminary experiments described in Appendix A.1. As a nice side-effect, the model automatically maintains the object to slot association, turning it into an unsupervised object tracker.</p>
<p>Physical interaction between objects is another common occurrence in sequential data. IODINE in its current form has limited abilities for modelling dynamics. Even statically placed objects commonly adhere to certain relations between each other, such as cars on streets. IODINE currently assumes objects to be placed independently of each other; relaxing this assumption will be important for modelling physical interactions. Yet there is also a need to balance this with the independence assumption required to split objects, since the system should still be able to segment out a car floating in space. Thus we believe integration with some form of graph network to support relations while preserving slot symmetry is another promising direction.</p>
<p>Finally, object representations have to be useful, such as for supervised tasks, or for agents in reinforcement learning setups. Whatever the task, it should provide important feedback about which objects matter and which are irrelevant. Complex visual scenes can contain an extremely large number of potential objects (think of sand grains on a beach), which can make it unfeasible to represent them all simultaneously. Allowing task-related signals to bias selection, for what and how to decompose, may enable scaling up unsupervised scene representation learning approaches like IODINE to arbitrarily complex scenes.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Danilo Rezende, Sjoerd van Steenkiste, and Malcolm Reynolds for helpful suggestions and generous support.</p>
<h2>References</h2>
<p>Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv:1607. 06450 [cs, stat], July 2016.</p>
<p>Bengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798-1828, 2013.</p>
<p>Burgess, C. P., Matthey, L., Watters, N., Kabra, R., Higgins, I., Botvinick, M., and Lerchner, A. MONet: Unsupervised scene decomposition and representation. arXiv preprint, January 2019.</p>
<p>Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets. arXiv:1606.03657 [cs, stat], June 2016.</p>
<p>Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast and accurate deep network learning by exponential linear units (ELUs). November 2015.</p>
<p>Eastwood, C. and Williams, C. K. I. A framework for the quantitative evaluation of disentangled representations. $I C L R, 2018$.</p>
<p>Eslami, S. M. A., Heess, N., Weber, T., Tassa, Y., Szepesvari, D., Kavukcuoglu, K., and Hinton, G. E. Attend, infer, repeat: Fast scene understanding with generative models. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 3225-3233. Curran Associates, Inc., 2016.</p>
<p>Felzenszwalb, P. F. and Huttenlocher, D. P. Efficient GraphBased image segmentation. Int. J. Comput. Vis., 59(2): 167-181, September 2004.</p>
<p>Girshick, R. B. Fast R-CNN. CoRR, abs/1504.08083, 2015. URL http://arxiv.org/abs/1504.08083.</p>
<p>Greff, K., Rasmus, A., Berglund, M., Hao, T. H., Valpola, H., and Schmidhuber, J. Tagger: Deep unsupervised perceptual grouping. In Lee, D. D., Sugiyama, M., Luxburg, U. V., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 29, pp. 4484-4492, 2016.</p>
<p>Greff, K., van Steenkiste, S., and Schmidhuber, J. Neural expectation maximization. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S.,
and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 6694-6704. Curran Associates, Inc., 2017.</p>
<p>Gregor, K., Danihelka, I., Graves, A., Rezende, D. J., and Wierstra, D. DRAW: A recurrent neural network for image generation. In Proceedings of the 32Nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, pp. 1462-1471, Lille, France, 2015. JMLR.org.</p>
<p>He, K., Gkioxari, G., Dollár, P., and Girshick, R. B. Mask R-CNN. CoRR, abs/1703.06870, 2017. URL http: //arxiv.org/abs/1703.06870.</p>
<p>Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta-VAE: Learning basic visual concepts with a constrained variational framework. In In Proceedings of the International Conference on Learning Representations (ICLR), 2017a.</p>
<p>Higgins, I., Pal, A., Rusu, A., Matthey, L., Burgess, C., Pritzel, A., Botvinick, M., Blundell, C., and Lerchner, A. DARLA: Improving zero-shot transfer in reinforcement learning. ICML, 2017b.</p>
<p>Higgins, I., Amos, D., Pfau, D., Racanière, S., Matthey, L., Rezende, D. J., and Lerchner, A. Towards a definition of disentangled representations. CoRR, abs/1812.02230, 2018. URL http://arxiv.org/ abs/1812.02230.</p>
<p>Hubert, L. and Arabie, P. Comparing partitions. J. Classification, 2(1):193-218, December 1985.</p>
<p>Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Zitnick, C. L., and Girshick, R. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 1988-1997. openaccess.thecvf.com, 2017.</p>
<p>Kim, H. and Mnih, A. Disentangling by factorising. February 2018.</p>
<p>Kingma, D. and Ba, J. Adam: A method for stochastic optimization. CBLS, 2015.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. Human-level concept learning through probabilistic program induction. Science, 350(6266):1332-1338, December 2015.</p>
<p>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and Brain Sciences, pp. 1-101, 2016.</p>
<p>Locatello, F., Bauer, S., Lucic, M., Gelly, S., Schölkopf, B., and Bachem, O. Challenging common assumptions in the unsupervised learning of disentangled representations. arXiv preprint arXiv:1811.12359, 2018.</p>
<p>Maaten, L. v. d. and Hinton, G. Visualizing data using t-sne. Journal of machine learning research, 9(Nov): 2579-2605, 2008.</p>
<p>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B. Adversarial autoencoders. November 2015.</p>
<p>Marino, J., Cvitkovic, M., and Yue, Y. A general method for amortizing variational filtering. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31, pp. 7868-7879. Curran Associates, Inc., 2018a.</p>
<p>Marino, J., Yue, Y., and Mandt, S. Iterative amortized inference. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3403-3412, Stockholmsmässan, Stockholm Sweden, 2018b. PMLR.</p>
<p>Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/ dsprites-dataset/, 2017.</p>
<p>Nickel, M. and Kiela, D. Poincaré embeddings for learning hierarchical representations. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 30, pp. 6338-6347. Curran Associates, Inc., 2017.</p>
<p>Pascanu, R., Mikolov, T., and Bengio, Y. Understanding the exploding gradient problem. CoRR, abs/1211. 5063, 2012.</p>
<p>Pero, L. D., Bowdish, J., Fried, D., Kermgard, B., Hartley, E., and Barnard, K. Bayesian geometric modeling of indoor scenes. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 2719-2726. ieeexplore.ieee.org, June 2012.</p>
<p>Rand, W. M. Objective criteria for the evaluation of clustering methods. J. Am. Stat. Assoc., 66(336):846-850, December 1971.</p>
<p>Redmon, J. and Farhadi, A. Yolov3: An incremental improvement. CoRR, abs/1804.02767, 2018. URL http://arxiv.org/abs/1804.02767.</p>
<p>Reichert, D. P. and Serre, T. Neuronal Synchrony in Complex-Valued Deep Networks. arXiv:1312. 6115 [cs, q-bio, stat], December 2013.</p>
<p>Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In Xing, E. P. and Jebara, T. (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1278-1286, Bejing, China, 2014. PMLR.</p>
<p>Ridgeway, K. and Mozer, M. C. Learning deep disentangled embeddings with the f-statistic loss. NIPS, 2018.</p>
<p>Shi, J. and Malik, J. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 22(8): 888-905, August 2000.
van Steenkiste, S., Chang, M., Greff, K., and Schmidhuber, J. Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. In Proceedings of the International Conference on Learning Representations (ICLR), January 2018.</p>
<p>Watters, N., Matthey, L., Burgess, C. P., and Lerchner, A. Spatial broadcast decoder: A simple architecture for learning disentangled representations in vaes. arXiv, 1901.07017, 2019.</p>
<h2>A. Further Discussion</h2>
<h2>A.1. Sequences</h2>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. IODINE applied to Objects Room sequences by setting $N$, the number of refinement iterations, equal to the number of timesteps in the data.</p>
<p>The iterative nature of IODINE lends itself readily to sequential data, by, e.g., feeding a new frame at every iteration, instead of the same input image $\mathbf{x}$. This setup corresponds to one iteration per timestep, and using next-step-prediction instead of reconstruction as part of the training objective. An example of this can be seen in Figure 12 where we show a 16 timestep sequence along with reconstructions and masks. When using the model in this way, it automatically maintains the association of object to slot over time (i.e, displaying robust slot stability). Thus, object tracking comes almost for free as a by-product in IODINE. Notice though, that IODINE has to rely on the LSTM that is part of the inference network to model any dynamics. That means none of the dynamics of tracked objects (e.g. velocity) will be part of the object representation.</p>
<h2>A.2. Memory Limitations</h2>
<p>It is worth pointing out that memory consumption presents an important limiting factor to scaling IODINE. To allow training by backpropagation, each slot and each refinement step require the storage of activations for an entire decoder and refinement network. Memory consumption during training thus scales linearly with both $K$ and $T$. This is particularly restrictive for sequential data, where the number of steps can grow very large. In our experiments from Appendix A.1, we found that 16 timesteps with a batch-size of 4 was the upper limit on V100 GPUs with 16 GB of RAM. Of course this also depends on the size of the input and the size of the network. Note also that at inference time there is no need to keep the activations of previous timesteps, so the dependence on $T$ can be eliminated there.</p>
<h2>A.3. Comparison with MONet</h2>
<p>The Multi-Object NETwork (MONet; Burgess et al. 2019) is a complementary method for unsupervised object representation learning also developed recently. It learns to
sequentially attend to individual objects using a masking network and a VAE. In each step the masking network segments out a yet unexplained part of the image (the next object) which is then fed to the VAE which has to reconstruct that object and the mask. Thus, in contrast to IODINE, MONet uses one iteration per object and doesn't adjust an object once it has been covered.</p>
<p>Both methods focus on the representation learning aspect and both ensure that all objects are encoded in the same format by sharing weights across objects. In our preliminary experiments MONet produced results very similar to IODINE on CLEVR both in terms of segmentation and regarding the quality of object representations, and also learns to inpaint occluded parts of objects.</p>
<p>Since MONet only visits each object once, it is a more lightweight method that requires less computation and memory to train and run. Recurrently iterating over objects also has the benefit that the model can dynamically vary the number of objects, whereas in IODINE the maximum number of objects is a hyperparameter that has to be fixed manually (though it can be changed at test time). The usage of a separate masking network which isn't directly subject to a representational bottleneck likely leads to less regularization for the segmentation mask. This could potentially allow MONet to better deal with complex segmentation shapes. But it also has to use that ability to directly produce masks that respect occlusion, whereas IODINE tends to produce masks for full unoccluded objects and leverages the softmax to resolve overlap. For more complex scenes, we also expect iterative refinement to be advantageous for resolving difficult cases. There, IODINE could start with a rough segmentation and then use the progressively better understanding of the constituent objects for refining the boundaries.</p>
<p>The segmentation process of MONet is deterministic which induces an order on the objects, which might be useful because it naturally prioritizes salient objects. We observed that it typically starts with the background, then processes large frontal objects, and finally smaller or farther away objects. But this approach does break symmetry between objects, and we prefer keeping such a bias out of the object segmentation learning as much as possible.</p>
<p>Another disadvantage of a deterministic segmentation is that it cannot directly deal with ambiguous cases like the one shown in Section 4.4 and Figure 10. The iterative message-passing-like approach of IODINE might also lend itself well for incorporating top-down feedback to bias the segmentation towards one that is useful for a given task. It is less clear how to do that in MONet, though adding a way for conditioning the masking network could potentially serve a similar purpose. Finally the iterative refinement of IODINE naturally extends to sequential data (see Appendix A.1)</p>
<p>which would be less straightforward for MONet.</p>
<p>In summary, it is not at all clear yet which approach will work better and under which circumstances. If the data is sequential or contains ambiguity, IODINE presents a better choice. For other data that is not visually more complex than CLEVR, both methods will likely produce similar results making MONet the simpler and less computationally intensive choice. For more complex data it is unclear yet which approach would be the better choice, and in fact a hybrid approach might be the most promising. Sequentially attending to objects and iterative refinement are not mutually exclusive and might support each other. We consider this a very attractive research direction and are excited to explore its possibilities.</p>
<h2>B. Dataset Details</h2>
<h3>B.1. CLEVR</h3>
<p>We regenerated the CLEVR dataset <em>Johnson et al. (2017)</em> using the authors’ open-source code, because we needed ground-truth segmentation masks for evaluation purposes. The dataset contains 70 000 images with a resolution of $240 \times 320$ pixels, from which we extract a square center crop of $192 \times 192$ and scale it to $128 \times 128$ pixels. Each scene contains between three and ten objects, characterized in terms of shape (cube, cylinder, or sphere), size (small or large), material (rubber or metal), color (8 different colors), position (continuous), and rotation (continuous).</p>
<p>The subset of images which contain 3-6 objects (inclusive) served as the training set for our experiments; we refer to it as CLEVR6. Unless noted otherwise, we evaluate models on the full CLEVR distribution, containing 3-10 objects. All references to CLEVR refer to the full distribution.</p>
<p>We do not make use of the question answering task. Figure 13 shows a few samples from the dataset.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Samples from CLEVR6. The first column is the scene, the second column is the background mask and the following columns are the ground-truth object masks.</p>
<h3>B.2. Multi-dSprites</h3>
<p>This dataset, based on the dSprites dataset <em>Matthey et al. (2017)</em>, consists of 60 000 images with a resolution of $64 \times 64$.</p>
<p>Each image contains two to five random sprites, which vary in terms of shape (square, ellipse, or heart), color (uniform saturated colors), scale (continuous), position (continuous), and rotation (continuous). Furthermore the background color is varied in brightness but always remains grayscale. Figure 14 shows a few samples from the dataset.</p>
<p>We also used a binarized version of Multi-dSprites, where the sprites are always white, the background is always black, and each image contains two to three random sprites.</p>
<h3>B.3. Tetris</h3>
<p>We generated this dataset of 60 000 images by placing three random Tetrominoes without overlap in an image of $35 \times 35$ pixels. Each Tetromino is composed of four blocks that are each $5 \times 5$ pixels. There are a total of 17 different Tetrominoes (counting rotations). We randomly color each Tetromino with one of 6 colors (red, green, blue, cyan, magenta, or yellow). Figure 15 shows a few samples from the dataset.</p>
<h3>B.4. Shapes</h3>
<p>We use the same shapes dataset as in <em>Reichert &amp; Serre (2013)</em>. It contains 60 000 binary images of size $28 \times 28$ each with three random shapes from the set ${\triangle, \nabla, \square}$.</p>
<h3>B.5. Objects Room</h3>
<p>For the preliminary sequential experiments we used a sequential version of the Objects Room dataset <em>Burgess et al. (2019)</em>. This dataset consists of 64x64 RGB images of a cubic room, with randomly colored walls, floors and objects randomly scattered around the room. The camera is always positioned on a ring inside the room, always facing towards the centre and oriented vertically in the range $\left(-25^{\circ}, 22^{\circ}\right)$. There are 3 randomly shaped objects in the room with 1-3 objects visible in any given frame. This version contains sequences of camera-flights for 16 time steps, with the camera</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. Samples from the Multi-dSprites dataset. The first column is the full image, the second column is the background mask and the following columns are the ground-truth object masks.</p>
<p><img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. Samples from the Tetris dataset. The first column is the full image, the second column is the background mask and the following columns are the ground-truth object masks.
position and angle (within the above constraints) changing according to a fixed velocity for the entire sequence (with a random velocity sampled for each sequence).</p>
<h2>C. Model and Hyperparameter Details</h2>
<p>Training Unless otherwise specified all the models are trained with the ADAM optimizer (Kingma \&amp; Ba, 2015), with default parameters and a learning rate of 0.0003 . We used gradient clipping as recommended by (Pascanu et al., 2012): if the norm of global gradient exceeds 5.0 then the gradient is scaled down to that norm. Note that this is virtually always the case as the gradient norm is typically on the order of $10^{5}$, but we nonetheless found it useful to apply this strategy. We always use $\sigma=0.1$ for the global scale of the output distribution $p\left(\mathbf{x} \mid \mathbf{z}^{(t)}\right)=\mathcal{N}\left(\mathbf{x} ; \boldsymbol{\mu}_{k}^{(t)}, \sigma^{2}\right)$. Finally, batch size was $32(4 \times 8 \mathrm{GPUs})$.</p>
<p>Initialization of Posterior IODINE iteratively refines an initial posterior $\boldsymbol{\lambda}^{(1)}$ which is independent of the input data. Initially we set this initial value to match the prior (i.e. $q_{\boldsymbol{\lambda}}\left(\mathbf{z}_{k}^{(1)}\right)=\mathcal{N}(\mathbf{0}, \mathbf{1}))$. But we found that this poses problems for the model, because of the competing requirements it poses for structuring the latent space w.r.t. the prior: On the one hand, samples from the prior need to be good starting values for iterative refinement. On the other hand, the prior should correspond to the accumulated posterior (KL term). For this reason we decided to simply make the parameters $\boldsymbol{\lambda}^{(1)}$ of the initialization distribution trainable parameters which are optimized alongside the weights of the decoder $(\boldsymbol{\theta})$ and of the refinement network $(\boldsymbol{\phi})$. This lead to faster training, and improved the visual quality of reconstructions from prior samples.</p>
<p>Inputs For all models, we use the following inputs to the refinement network, where LN means Layernorm and SG means stop gradients, and we omit the iteration index . ${ }^{(t)}$ for brevity. The following image-sized inputs are concatenated and fed to the corresponding convolutional network:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: left;">LN</th>
<th style="text-align: left;">SG</th>
<th style="text-align: right;">Ch.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">image</td>
<td style="text-align: left;">$\mathbf{x}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">means</td>
<td style="text-align: left;">$\boldsymbol{\mu}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">mask</td>
<td style="text-align: left;">$\boldsymbol{m}_{k}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">mask-logits</td>
<td style="text-align: left;">$\hat{\boldsymbol{m}}_{k}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">mask posterior</td>
<td style="text-align: left;">$p\left(\boldsymbol{m}_{k} \mid \mathbf{x}, \boldsymbol{\mu}\right)$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">gradient of means</td>
<td style="text-align: left;">$\nabla_{\boldsymbol{\mu}_{k}} \mathcal{L}$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: right;">3</td>
</tr>
<tr>
<td style="text-align: left;">gradient of mask</td>
<td style="text-align: left;">$\nabla_{\boldsymbol{m}_{k}} \mathcal{L}$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">pixelwise likelihood</td>
<td style="text-align: left;">$p(\mathbf{x} \mid \mathbf{z})$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">leave-one-out likelih.</td>
<td style="text-align: left;">$p\left(\mathbf{x} \mid \mathbf{z}_{i \neq k}\right)$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">coordinate channels</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">total:</td>
<td style="text-align: left;">17</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>The posterior parameters $\boldsymbol{\lambda}$ and their gradients are flat vectors, and as such we concatenate them with the output of the convolutional part of the refinement network and use the result as input to the refinement LSTM:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Formula</th>
<th style="text-align: left;">LN</th>
<th style="text-align: left;">SG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gradient of posterior <br> posterior</td>
<td style="text-align: left;">$\nabla_{\boldsymbol{\lambda}_{k}} \mathcal{L}$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\boldsymbol{\lambda}_{k}$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Architecture All layers use the ELU (Clevert et al., 2015) activation function and the Convolutional layers use a stride equal to 1, unless mentioned otherwise. Architecture details for the individual datasets are summarized in the following subsections.</p>
<h2>C.1. CLEVR</h2>
<p>All models were trained on scenes with 3-6 objects (CLEVR6) with $K=7$ slots and $T=5$ iterations, and a latent object dimension of size $\operatorname{dim}\left(\mathbf{z}_{k}\right)=64$. Training was done on eight V100 GPUs for 1M updates (approx. 1 week wall time). When evaluating on the full CLEVR dataset, we increased the number of slots to $K=11$. For some of the analysis, we varied $T$ and $K$ as mentioned in the text.</p>
<p>The rest of the architecture and hyperparameters are described in the following.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Decoder</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Type</td>
<td style="text-align: center;">Size/Ch.</td>
<td style="text-align: center;">Act. Func.</td>
<td style="text-align: center;">Comment</td>
</tr>
<tr>
<td style="text-align: left;">Input: z</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ coordinates</td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;">RGB + Mask</td>
</tr>
<tr>
<td style="text-align: left;">{f5002200-c055-47a4-a79-865221e2188}Refinement Network</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Type</td>
<td style="text-align: center;">Size/Ch.</td>
<td style="text-align: center;">Act. Func.</td>
<td style="text-align: center;">Comment</td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">Tanh</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Concat $\left[\boldsymbol{\lambda}, \nabla_{\boldsymbol{\lambda}} \mathcal{L}\right]$</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Avg. Pool</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $3 \times 3$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: center;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Inputs</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Deconv Decoder used in Section 4.3</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Size/Ch.</th>
<th style="text-align: center;">Act. Func.</th>
<th style="text-align: left;">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input: z</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Reshape</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">$8 \times 8 \times 8$</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;">stride 2</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">Linear</td>
<td style="text-align: left;">RGB + Mask</td>
</tr>
</tbody>
</table>
<h2>C.2. Multi-dSprites</h2>
<p>Models were trained with $K=6$ slots, and used $T=5$ iterations.</p>
<h2>Decoder</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Size/Ch.</th>
<th style="text-align: left;">Act. Func.</th>
<th style="text-align: left;">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input: $\boldsymbol{\lambda}$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: center;">34</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">+ coordinates</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">RGB + Mask</td>
</tr>
</tbody>
</table>
<p>Refinement Network</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Size/Ch.</th>
<th style="text-align: left;">Act. Func.</th>
<th style="text-align: left;">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: center;">128</td>
<td style="text-align: left;">Tanh</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Concat $\left[\boldsymbol{\lambda}, \nabla_{\boldsymbol{\lambda}} \mathcal{L}\right]$</td>
<td style="text-align: center;">192</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">128</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Avg. Pool</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Inputs</td>
<td style="text-align: center;">17</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<h2>C.3. Tetris</h2>
<p>Models were trained with $K=4$ slots, and used $T=5$ iterations. For Tetris, in contrast to the other models, we did not use an LSTM in the refinement network.</p>
<h2>Decoder</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Size/Ch.</th>
<th style="text-align: left;">Act. Func.</th>
<th style="text-align: left;">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Input: $\boldsymbol{\lambda}$</td>
<td style="text-align: center;">64</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Broadcast</td>
<td style="text-align: center;">66</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">+ coordinates</td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">4</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;">RGB + Mask</td>
</tr>
</tbody>
</table>
<h2>Refinement Network</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Size/Ch.</th>
<th style="text-align: left;">Act. Func.</th>
<th style="text-align: left;">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">64</td>
<td style="text-align: left;">Linear</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Concat $\left[\boldsymbol{\lambda}, \nabla_{\boldsymbol{\lambda}} \mathcal{L}\right]$</td>
<td style="text-align: center;">256</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MLP</td>
<td style="text-align: center;">128</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Avg. Pool</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Conv $5 \times 5$</td>
<td style="text-align: center;">32</td>
<td style="text-align: left;">ELU</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Inputs</td>
<td style="text-align: center;">17</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 16. Additional segmentation and object reconstruction results on CLEVR6. Border colors are matched to the segmentation mask on the left.</p>
<h1>D. Additional Plots</h1>
<p>Decompositions Figures 16-18 show additional decomposition samples on our datasets. Figure 19 shows a complete version of Figure 7, showing all individual masked reconstruction slots. Figures 20-22 show a comparison between the object reconstructions and the mask logits used for assigning decoded latents to pixels.</p>
<p>Projections of Object Latents Figures 24-26 demonstrate how object latents are clustered when projected onto the first two principal components of the latent distribution. Figures 27-29 show how object latents are clustered when projected onto a t-SNE (Maaten \&amp; Hinton, 2008) of the latent distribution.</p>
<p>Traversals Figures 30-32 show additional (randomly chosen) latent traversals for IODINE on CLEVR like on the right side of Figure 6.</p>
<p>Input Ablations Figures 33-40 give an overview of the impact of each of the inputs to the refinement network on the total loss, mean squared reconstruction error, KL divergence loss term, and the ARI segmentation performance (excluding the background pixels) on the CLEVR and Tetris datasets.</p>
<p><img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 17. Additional segmentation and object reconstruction results on Multi-dSprites. Border colors are matched to the segmentation mask on the left.</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 18. Additional segmentation and object reconstruction results on Tetris. Border colors are matched to the segmentation mask on the left.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 19. Full version of Figure 7, showcasing all slots.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 20. CLEVR6 dataset. Odd rows: image and object masks as determined by the model. Even rows: first column is the input image, second one is the ground-truth masks and the following ones are mask logits produced by the model.
<img alt="img-20.jpeg" src="img-20.jpeg" /></p>
<p>Figure 21. Multi-dSprites dataset. Odd rows: image and object masks as determined by the model. Even rows: first column is the input image, second one is the ground-truth masks and the following ones are mask logits produced by the model.</p>
<p><img alt="img-21.jpeg" src="img-21.jpeg" /></p>
<p>Figure 22. Tetris dataset. Odd rows: image and object masks as determined by the model. Even rows: first column is the input image, second one is the ground-truth masks and the following ones are mask logits produced by the model.</p>
<p><img alt="img-22.jpeg" src="img-22.jpeg" /></p>
<p>Figure 23. Segmentation and object reconstruction results on CLEVR6 using a deconvolution based decoder instead of the spatial broadcast decoder. Note that IODINE still cleanly segments objects from the background (now ignoring shadows), but specialization of the individual slots is much worse. Both, slots holding multiple objects, and objects replicated across multiple slots are much more frequent now. Slot reconstructions are also much less clean, containing much more noise and residue of other objects. (Note though, that in this figure we didn't mask the reconstructions as we have for Figure 16.)
<img alt="img-23.jpeg" src="img-23.jpeg" /></p>
<p>Figure 24. Projection on the first two principal components of the latent distribution for the CLEVR6 dataset. Each dot represents one object latent and is colored according to the corresponding ground truth factor.</p>
<p><img alt="img-24.jpeg" src="img-24.jpeg" /></p>
<p>Figure 25. Projection on the first two principal components of the latent distribution for the Multi-dSprites dataset. Each dot represents one object latent and is colored according to the corresponding ground truth factor.
<img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>Figure 26. Projection on the first two principal components of the latent distribution for the Tetris dataset. Each dot represents one object latent and is colored according to the corresponding ground truth factor.
<img alt="img-26.jpeg" src="img-26.jpeg" /></p>
<p>Figure 27. t-SNE of the latent distribution for the CLEVR6 dataset. Each dot represents one object latent and is colored according to the corresponding ground truth factor
<img alt="img-27.jpeg" src="img-27.jpeg" /></p>
<p>Figure 28. t-SNE of the latent distribution for the Multi-dSprites dataset. Each dot represents one object latent and is colored according to the corresponding ground truth factor</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The Swiss AI lab IDSIA, Lugano, Switzerland ${ }^{2}$ Work done at DeepMind ${ }^{3}$ DeepMind, London, UK. Correspondence to: Klaus Greff <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#108;&#97;&#117;&#115;&#46;&#103;&#114;&#101;&#102;&#102;&#64;&#115;&#116;&#97;&#114;&#116;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;">&#107;&#108;&#97;&#117;&#115;&#46;&#103;&#114;&#101;&#102;&#102;&#64;&#115;&#116;&#97;&#114;&#116;&#109;&#97;&#105;&#108;&#46;&#99;&#111;&#109;</a>.</p>
<p>Proceedings of the $36^{\text {th }}$ International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
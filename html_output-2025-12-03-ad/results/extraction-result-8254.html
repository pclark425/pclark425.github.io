<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8254 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8254</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8254</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-267938465</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.07088v2.pdf" target="_blank">Diversity of Thought Improves Reasoning Abilities of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance. However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming prior work, DIV-SE(in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks. Our results improve the Pareto frontier of the accuracy-cost trade-off.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8254.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8254.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-4 evaluated in this paper as the primary high-performance LLM; used to generate diverse approaches, augment few-shot examples, and run DIV-SE and IDIV-SE evaluations across multiple reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source multimodal large language model from OpenAI. Used with instruction prompting in zero-shot and few-shot CoT settings; inference configurations in the paper include temperature T=0 for DIV-SE/IDIV-SE runs and T=0.7 for Self-Consistency runs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) prompting (zero-shot and few-shot)', 'Self-Consistency (SC) ensemble decoding', 'DIV-SE (DIVerse reasoning path Self-Ensemble)', 'IDIV-SE (In-call DIVerse reasoning path Self-Ensemble)', 'Majority-vote aggregation', 'Meta-reasoning aggregation (as alternative aggregation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: add step-by-step reasoning instructions or demonstrations (zero-shot 'Think step-by-step' or few-shot CoT examples). SC: sample multiple stochastic decodings (temperature 0.7) and ensemble answers. DIV-SE: automatically solicit multiple high-level reasoning approaches and personas from the LLM, augment few-shot examples per approach+persona, then run separate inference calls for each approach/persona and aggregate outputs via majority vote. IDIV-SE: compose multiple approach+persona-augmented demonstrations into a single prompt so the model produces multiple reasoning paths sequentially in one inference call and then aggregate the different answers (majority vote). Meta-reasoning aggregation: using an LLM-based aggregator to combine chains (evaluated as an alternative).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct experimental comparisons of diverse-vs-similar reasoning: DIV-SE and IDIV-SE (explicitly diverse, approach/persona-level diversity) are evaluated against standard CoT (single reasoning conditioning) and Self-Consistency (token-level stochastic diversity). Ensembles of size 3 or 5 (depending on dataset) are used; SC sampled up to s in [1,10]. Ablations/analyses include: (i) ensemble size sweeps (Figures 6,7) showing improvements even at size=3; (ii) error-propagation analysis for IDIV-SE where cases with all approaches producing the same wrong answer are re-run to estimate propagation (GPT-4 error-propagation rate 6.2%); (iii) comparison of cost-vs-accuracy (Pareto frontier) between DIV-SE, IDIV-SE, SC, and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multiple reasoning benchmarks: AQUA‑RAT (algebra word problems), GSM8K (grade-school math), MATH (counting & probability subset), BLOCKSWORLD planning (BLOCKSWORLD 3 and BLOCKSWORLD 4/5), GRAPH COLORING (constraint satisfaction), COMMONSENSEQA (multiple-choice common-sense reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reports that DIV-SE and IDIV-SE outperform CoT and SC on most benchmarks with concrete gains: AQUA-RAT: DIV-SE yields +9.84 p.p (FS-CoT) and +14.6 p.p (ZS-CoT) improvements over baselines (absolute accuracies for some configurations reported in Table 2; e.g., GPT-4 few-shot DIV-SE ~84.25% in table excerpt). GSM8K: baseline ZS-CoT/FS-CoT ~94%/95%; DIV-SE modestly improves to 96.3% (both ZS and FS reported). MATH (counting & probability): DIV-SE yielded +14.3 and +16.87 p.p increases in FS-CoT and ZS-CoT respectively (baseline numbers in text). BLOCKSWORLD planning: DIV-SE yields state-of-the-art 94% on BLOCKSWORLD 3 (GPT-4, ZS setting) and 69.6% on BLOCKSWORLD 4/5; IDIV-SE gave large absolute gains over ZS-CoT as well (e.g., IDIV-SE +12 p.p over ZS-CoT on BLOCKSWORLD 3; +17 p.p on BLOCKSWORLD 4/5). GRAPH COLORING: ZS-CoT 15%, SC-10 23%; IDIV-SE improves +59 p.p above ZS-CoT (text), and DIV-SE achieves 97% (state-of-the-art). COMMONSENSEQA: modest improvements. Cost/ensemble settings: ensembles of size 5 for AQUA-RAT/GSM8K, size 3 for planning/graph-coloring/commonsenseQA; DIV-SE uses multiple inference calls (higher cost) while IDIV-SE puts multiple approaches in-call (lower cost) with some error-propagation risk. Self-Consistency baselines used temperature 0.7 and up to 10 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Key qualitative observations: (1) Approach/persona-level diversity (thought-level) yields more independent and complementary solution methods than token-level stochasticity; (2) DIV-SE (separate calls per approach) attains the highest accuracies but at higher inference cost; (3) IDIV-SE (in-call composition) achieves comparable accuracy at reduced cost but exhibits measurable error propagation (GPT-4 6.2%, GPT-3.5 5.5% on AQUA-RAT in the specific test described); (4) even small ensembles (size 3) of diverse approaches substantially improve accuracy, indicating high diversity/independence of reasoning paths; (5) GPT-3.5 struggled to follow complex instruction constraints in planning, limiting gains; (6) approach/persona augmentation of few-shot examples is reusable across related tasks and helps performance.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Explicit claim: promoting diversity of thought (generating and conditioning on multiple distinct high-level reasoning approaches and personas) yields substantial improvements in LLM reasoning across tasks, improving the accuracy-vs-cost Pareto frontier, and achieving state-of-the-art on hard planning and constraint-satisfaction benchmarks. DIV-SE (multi-call diverse prompts) yields the strongest accuracy, while IDIV-SE reduces cost with small degradation attributable partly to error propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Improves Reasoning Abilities of LLMs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8254.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8254.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI GPT-3.5 Turbo evaluated as a comparative model to test DIV-SE/IDIV-SE and baselines; shows larger relative gains from diversity in some arithmetic tasks but limitations in instruction-following for planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned generative language model (OpenAI). Evaluated with the same prompting regimes (CoT, SC, DIV-SE, IDIV-SE); experimental runs used temperature T=0 for DIV/IDIV and T=0.7 for SC.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) prompting (zero-shot & few-shot)', 'Self-Consistency (SC)', 'DIV-SE', 'IDIV-SE', 'Majority-vote aggregation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same implementations as for GPT-4: CoT few-shot or zero-shot; SC via stochastic sampling and ensembling; DIV-SE by creating multiple prompts each with different approach/persona augmented few-shot examples and running separate inference calls; IDIV-SE composing multiple approaches into single prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct comparisons against CoT and SC baselines; ensembles sizes and temperature same as GPT-4 experiments. Additional analysis: error-propagation for IDIV-SE measured on AQUA-RAT (5.5% for GPT-3.5), ensemble-size sweeps (Figures 6,7) show gains even at small ensemble sizes. Observed failure modes (planning) inform qualitative interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT, GSM8K, MATH (counting & probability), BLOCKSWORLD planning, GRAPH COLORING, COMMONSENSEQA (same suite as GPT-4 experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>GPT-3.5 exhibits larger relative gains from DIV-SE/IDIV-SE in several arithmetic tasks: GSM8K baseline ZS-CoT 76.11% -> IDIV-SE +5.31 p.p and DIV-SE +10.39 p.p (absolute accuracies in text); AQUA-RAT: DIV-SE yields +14.23 and +16.52 p.p in FS-CoT and ZS-CoT respectively (baseline numbers given in text); MATH: DIV-SE and IDIV-SE yield large p.p improvements (e.g., DIV-SE +21.84 p.p FS-CoT). In planning, GPT-3.5 baseline accuracy low (BLOCKSWORLD 3 baseline 6%, 4/5 baseline 0.6%) and both DIV-SE and IDIV-SE did not yield improvements due to instruction-following failures and infeasible plans.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Findings: (1) GPT-3.5 benefits substantially from diverse approach conditioning on arithmetic tasks, indicating that diversity helps weaker/less-instruction-following models more in certain domains; (2) GPT-3.5's weak instruction-following caused failures in planning tasks, limiting DIV-SE/IDIV-SE usefulness for those benchmarks; (3) IDIV-SE showed modest error-propagation (~5.5%) similar to GPT-4 magnitude on AQUA-RAT; (4) DIV-SE (multi-call) often outperformed IDIV-SE, suggesting that isolating approaches reduces intra-call contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>The paper concludes that conditioning on multiple distinct reasoning approaches substantially helps GPT-3.5 on arithmetic and math reasoning benchmarks, but gains depend on the model's ability to follow instructions and constraints (which limited planning improvements). DIV-SE yields larger accuracy gains than IDIV-SE at higher cost; IDIV-SE reduces cost but carries error-propagation risks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Improves Reasoning Abilities of LLMs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8254.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8254.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2 70B (meta-llama/Llama-2-70b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA-2 70B model evaluated in a limited set of experiments (AQUA-RAT) to assess applicability of DIV-SE/IDIV-SE to non-OpenAI models; runs used 8-bit quantization and greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source 70-billion-parameter Llama-2 chat model. In experiments quantized to 8-bit (bitsandbytes) run with greedy decoding, temperature T=0.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT) prompting (ZS & FS)', 'DIV-SE', 'IDIV-SE']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same prompting pipeline (DIVERSEPROMPTING) used to extract approaches and personas, augment few-shot examples, and compose DIV-SE/IDIV-SE prompts. For LLaMA-2-70B experiments they used greedy decoding (T=0) and smaller-scale evaluation due to compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Limited experiments run on AQUA-RAT; compared ZS-CoT, FS-CoT, IDIV-SE, DIV-SE. Reported that DIV-SE and IDIV-SE improve over baseline in FS-CoT by >10 p.p while ZS-CoT gains are negligible in this model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>AQUA-RAT (limited evaluation due to compute constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 3 in the paper reports that for LLaMA-2-70B DIV-SE and IDIV-SE demonstrate >10 percentage-point improvements over the FS-CoT baseline; gains in ZS-CoT were negligible. Exact numeric baselines and gains are provided in the paper's Table 3 excerpt (e.g., DIV-SE and IDIV-SE improved over CoT by ~10+ p.p in FS-CoT for AQUA-RAT).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Observations: (1) Open-source LLMs (here LLaMA-2-70B) can benefit from DIVERSEPROMPTING in few-shot settings, but zero-shot efficacy is limited, possibly due to weaker instruction-following; (2) quantized 8-bit inference and greedy decoding were used successfully; (3) limited compute prevented broader evaluation across all benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>DIV-SE and IDIV-SE are applicable beyond closed-source models and provide improvements for LLaMA-2-70B in the few-shot CoT setting, but zero-shot gains are small; the method's effectiveness depends on the model's instruction-following capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Diversity of Thought Improves Reasoning Abilities of LLMs', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Answering questions by meta-reasoning over multiple chains of thought <em>(Rating: 2)</em></li>
                <li>Making language models better reasoners with step-aware verifier <em>(Rating: 2)</em></li>
                <li>On the planning abilities of large language models - a critical investigation <em>(Rating: 1)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8254",
    "paper_id": "paper-267938465",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "OpenAI GPT-4 evaluated in this paper as the primary high-performance LLM; used to generate diverse approaches, augment few-shot examples, and run DIV-SE and IDIV-SE evaluations across multiple reasoning benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source multimodal large language model from OpenAI. Used with instruction prompting in zero-shot and few-shot CoT settings; inference configurations in the paper include temperature T=0 for DIV-SE/IDIV-SE runs and T=0.7 for Self-Consistency runs.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) prompting (zero-shot and few-shot)",
                "Self-Consistency (SC) ensemble decoding",
                "DIV-SE (DIVerse reasoning path Self-Ensemble)",
                "IDIV-SE (In-call DIVerse reasoning path Self-Ensemble)",
                "Majority-vote aggregation",
                "Meta-reasoning aggregation (as alternative aggregation)"
            ],
            "reasoning_methods_description": "CoT: add step-by-step reasoning instructions or demonstrations (zero-shot 'Think step-by-step' or few-shot CoT examples). SC: sample multiple stochastic decodings (temperature 0.7) and ensemble answers. DIV-SE: automatically solicit multiple high-level reasoning approaches and personas from the LLM, augment few-shot examples per approach+persona, then run separate inference calls for each approach/persona and aggregate outputs via majority vote. IDIV-SE: compose multiple approach+persona-augmented demonstrations into a single prompt so the model produces multiple reasoning paths sequentially in one inference call and then aggregate the different answers (majority vote). Meta-reasoning aggregation: using an LLM-based aggregator to combine chains (evaluated as an alternative).",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct experimental comparisons of diverse-vs-similar reasoning: DIV-SE and IDIV-SE (explicitly diverse, approach/persona-level diversity) are evaluated against standard CoT (single reasoning conditioning) and Self-Consistency (token-level stochastic diversity). Ensembles of size 3 or 5 (depending on dataset) are used; SC sampled up to s in [1,10]. Ablations/analyses include: (i) ensemble size sweeps (Figures 6,7) showing improvements even at size=3; (ii) error-propagation analysis for IDIV-SE where cases with all approaches producing the same wrong answer are re-run to estimate propagation (GPT-4 error-propagation rate 6.2%); (iii) comparison of cost-vs-accuracy (Pareto frontier) between DIV-SE, IDIV-SE, SC, and CoT.",
            "task_or_benchmark": "Multiple reasoning benchmarks: AQUA‑RAT (algebra word problems), GSM8K (grade-school math), MATH (counting & probability subset), BLOCKSWORLD planning (BLOCKSWORLD 3 and BLOCKSWORLD 4/5), GRAPH COLORING (constraint satisfaction), COMMONSENSEQA (multiple-choice common-sense reasoning).",
            "performance_results": "Paper reports that DIV-SE and IDIV-SE outperform CoT and SC on most benchmarks with concrete gains: AQUA-RAT: DIV-SE yields +9.84 p.p (FS-CoT) and +14.6 p.p (ZS-CoT) improvements over baselines (absolute accuracies for some configurations reported in Table 2; e.g., GPT-4 few-shot DIV-SE ~84.25% in table excerpt). GSM8K: baseline ZS-CoT/FS-CoT ~94%/95%; DIV-SE modestly improves to 96.3% (both ZS and FS reported). MATH (counting & probability): DIV-SE yielded +14.3 and +16.87 p.p increases in FS-CoT and ZS-CoT respectively (baseline numbers in text). BLOCKSWORLD planning: DIV-SE yields state-of-the-art 94% on BLOCKSWORLD 3 (GPT-4, ZS setting) and 69.6% on BLOCKSWORLD 4/5; IDIV-SE gave large absolute gains over ZS-CoT as well (e.g., IDIV-SE +12 p.p over ZS-CoT on BLOCKSWORLD 3; +17 p.p on BLOCKSWORLD 4/5). GRAPH COLORING: ZS-CoT 15%, SC-10 23%; IDIV-SE improves +59 p.p above ZS-CoT (text), and DIV-SE achieves 97% (state-of-the-art). COMMONSENSEQA: modest improvements. Cost/ensemble settings: ensembles of size 5 for AQUA-RAT/GSM8K, size 3 for planning/graph-coloring/commonsenseQA; DIV-SE uses multiple inference calls (higher cost) while IDIV-SE puts multiple approaches in-call (lower cost) with some error-propagation risk. Self-Consistency baselines used temperature 0.7 and up to 10 samples.",
            "qualitative_findings": "Key qualitative observations: (1) Approach/persona-level diversity (thought-level) yields more independent and complementary solution methods than token-level stochasticity; (2) DIV-SE (separate calls per approach) attains the highest accuracies but at higher inference cost; (3) IDIV-SE (in-call composition) achieves comparable accuracy at reduced cost but exhibits measurable error propagation (GPT-4 6.2%, GPT-3.5 5.5% on AQUA-RAT in the specific test described); (4) even small ensembles (size 3) of diverse approaches substantially improve accuracy, indicating high diversity/independence of reasoning paths; (5) GPT-3.5 struggled to follow complex instruction constraints in planning, limiting gains; (6) approach/persona augmentation of few-shot examples is reusable across related tasks and helps performance.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Explicit claim: promoting diversity of thought (generating and conditioning on multiple distinct high-level reasoning approaches and personas) yields substantial improvements in LLM reasoning across tasks, improving the accuracy-vs-cost Pareto frontier, and achieving state-of-the-art on hard planning and constraint-satisfaction benchmarks. DIV-SE (multi-call diverse prompts) yields the strongest accuracy, while IDIV-SE reduces cost with small degradation attributable partly to error propagation.",
            "uuid": "e8254.0",
            "source_info": {
                "paper_title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo",
            "name_full": "OpenAI GPT-3.5 Turbo",
            "brief_description": "OpenAI GPT-3.5 Turbo evaluated as a comparative model to test DIV-SE/IDIV-SE and baselines; shows larger relative gains from diversity in some arithmetic tasks but limitations in instruction-following for planning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 Turbo",
            "model_description": "Instruction-tuned generative language model (OpenAI). Evaluated with the same prompting regimes (CoT, SC, DIV-SE, IDIV-SE); experimental runs used temperature T=0 for DIV/IDIV and T=0.7 for SC.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) prompting (zero-shot & few-shot)",
                "Self-Consistency (SC)",
                "DIV-SE",
                "IDIV-SE",
                "Majority-vote aggregation"
            ],
            "reasoning_methods_description": "Same implementations as for GPT-4: CoT few-shot or zero-shot; SC via stochastic sampling and ensembling; DIV-SE by creating multiple prompts each with different approach/persona augmented few-shot examples and running separate inference calls; IDIV-SE composing multiple approaches into single prompt.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Direct comparisons against CoT and SC baselines; ensembles sizes and temperature same as GPT-4 experiments. Additional analysis: error-propagation for IDIV-SE measured on AQUA-RAT (5.5% for GPT-3.5), ensemble-size sweeps (Figures 6,7) show gains even at small ensemble sizes. Observed failure modes (planning) inform qualitative interpretations.",
            "task_or_benchmark": "AQUA-RAT, GSM8K, MATH (counting & probability), BLOCKSWORLD planning, GRAPH COLORING, COMMONSENSEQA (same suite as GPT-4 experiments).",
            "performance_results": "GPT-3.5 exhibits larger relative gains from DIV-SE/IDIV-SE in several arithmetic tasks: GSM8K baseline ZS-CoT 76.11% -&gt; IDIV-SE +5.31 p.p and DIV-SE +10.39 p.p (absolute accuracies in text); AQUA-RAT: DIV-SE yields +14.23 and +16.52 p.p in FS-CoT and ZS-CoT respectively (baseline numbers given in text); MATH: DIV-SE and IDIV-SE yield large p.p improvements (e.g., DIV-SE +21.84 p.p FS-CoT). In planning, GPT-3.5 baseline accuracy low (BLOCKSWORLD 3 baseline 6%, 4/5 baseline 0.6%) and both DIV-SE and IDIV-SE did not yield improvements due to instruction-following failures and infeasible plans.",
            "qualitative_findings": "Findings: (1) GPT-3.5 benefits substantially from diverse approach conditioning on arithmetic tasks, indicating that diversity helps weaker/less-instruction-following models more in certain domains; (2) GPT-3.5's weak instruction-following caused failures in planning tasks, limiting DIV-SE/IDIV-SE usefulness for those benchmarks; (3) IDIV-SE showed modest error-propagation (~5.5%) similar to GPT-4 magnitude on AQUA-RAT; (4) DIV-SE (multi-call) often outperformed IDIV-SE, suggesting that isolating approaches reduces intra-call contamination.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "The paper concludes that conditioning on multiple distinct reasoning approaches substantially helps GPT-3.5 on arithmetic and math reasoning benchmarks, but gains depend on the model's ability to follow instructions and constraints (which limited planning improvements). DIV-SE yields larger accuracy gains than IDIV-SE at higher cost; IDIV-SE reduces cost but carries error-propagation risks.",
            "uuid": "e8254.1",
            "source_info": {
                "paper_title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-2-70B",
            "name_full": "LLaMA-2 70B (meta-llama/Llama-2-70b-chat-hf)",
            "brief_description": "Open-source LLaMA-2 70B model evaluated in a limited set of experiments (AQUA-RAT) to assess applicability of DIV-SE/IDIV-SE to non-OpenAI models; runs used 8-bit quantization and greedy decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-70B",
            "model_description": "Open-source 70-billion-parameter Llama-2 chat model. In experiments quantized to 8-bit (bitsandbytes) run with greedy decoding, temperature T=0.",
            "reasoning_methods": [
                "Chain-of-Thought (CoT) prompting (ZS & FS)",
                "DIV-SE",
                "IDIV-SE"
            ],
            "reasoning_methods_description": "Same prompting pipeline (DIVERSEPROMPTING) used to extract approaches and personas, augment few-shot examples, and compose DIV-SE/IDIV-SE prompts. For LLaMA-2-70B experiments they used greedy decoding (T=0) and smaller-scale evaluation due to compute budget.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Limited experiments run on AQUA-RAT; compared ZS-CoT, FS-CoT, IDIV-SE, DIV-SE. Reported that DIV-SE and IDIV-SE improve over baseline in FS-CoT by &gt;10 p.p while ZS-CoT gains are negligible in this model.",
            "task_or_benchmark": "AQUA-RAT (limited evaluation due to compute constraints).",
            "performance_results": "Table 3 in the paper reports that for LLaMA-2-70B DIV-SE and IDIV-SE demonstrate &gt;10 percentage-point improvements over the FS-CoT baseline; gains in ZS-CoT were negligible. Exact numeric baselines and gains are provided in the paper's Table 3 excerpt (e.g., DIV-SE and IDIV-SE improved over CoT by ~10+ p.p in FS-CoT for AQUA-RAT).",
            "qualitative_findings": "Observations: (1) Open-source LLMs (here LLaMA-2-70B) can benefit from DIVERSEPROMPTING in few-shot settings, but zero-shot efficacy is limited, possibly due to weaker instruction-following; (2) quantized 8-bit inference and greedy decoding were used successfully; (3) limited compute prevented broader evaluation across all benchmarks.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "DIV-SE and IDIV-SE are applicable beyond closed-source models and provide improvements for LLaMA-2-70B in the few-shot CoT setting, but zero-shot gains are small; the method's effectiveness depends on the model's instruction-following capabilities.",
            "uuid": "e8254.2",
            "source_info": {
                "paper_title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Answering questions by meta-reasoning over multiple chains of thought",
            "rating": 2,
            "sanitized_title": "answering_questions_by_metareasoning_over_multiple_chains_of_thought"
        },
        {
            "paper_title": "Making language models better reasoners with step-aware verifier",
            "rating": 2,
            "sanitized_title": "making_language_models_better_reasoners_with_stepaware_verifier"
        },
        {
            "paper_title": "On the planning abilities of large language models - a critical investigation",
            "rating": 1,
            "sanitized_title": "on_the_planning_abilities_of_large_language_models_a_critical_investigation"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 1,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        }
    ],
    "cost": 0.01388625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Diversity of Thought Improves Reasoning Abilities of LLMs</p>
<p>Ranjita Naik 
Microsoft Varun Chandrasekaran 
Mert Yuksekgonul 
Hamid Palangi 
Besmira Nushi </p>
<p>University of Illinois Urbana-Champaign</p>
<p>Stanford University</p>
<p>Microsoft Research</p>
<p>Microsoft Research</p>
<p>Diversity of Thought Improves Reasoning Abilities of LLMs
16B9358A4220165E5604C6398B16DB15
Large language models (LLMs) are documented to struggle in settings that require complex reasoning.Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance.However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling.In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought.We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem.We then ensemble the diverse prompts in our method DIV-SE (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDIV-SE (In-call DIVerse reasoning path Self-Ensemble).Apart from our approaches outperforming prior work, DIV-SE (in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks.Our results improve the Pareto frontier of the accuracy-cost trade-off.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael started with 58 golf balls.After losing 23 on tuesday, he had 58 -23 = 35.After losing 2 more, he had 35 -2 = 33 golf balls.The answer is 33.Q: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM? Input Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Approach 1 <Using direct calculation>: Subtract the total number of golf balls lost from (…) Therefore, he had 33 golf balls at the end of Wednesday.Approach 2 <Using algebra>: Let x be the number of golf balls Michael had (…) we find that Michael had 33 golf balls at the end of Wednesday.Approach 3 <Using visualization>: Represent the problem using a diagram with three stages: (…) leaving him with 33 golf balls at the end of Wednesday.</p>
<p>Introduction</p>
<p>Large language models (LLMs) exhibit state-ofthe-art performance across a myriad of tasks, but their effectiveness is strongly influenced by prompt design (Anil et al., 2023;OpenAI, 2023a;Nori et al., 2023).For complex reasoning tasks, the right prompt can enable LLMs to capitalize on task structure (Guidance, 2024), such as by facilitating memory (by externalizing thought processes), or through tractable problem decomposition (Zhou et al., 2024).However, existing prompt design either relies on iterative trial-and-error (White et al., 2023), or is expensive (Lester et al., 2021).</p>
<p>Previous works identified two simple, yet general prompting principles to enable complex reasoning: (i) Chain-of-Thought (CoT) prompting, and (ii) ensembling multiple solutions from diverse decoding paths.CoT prompting (Wei et al., 2022) improves performance by guiding the LLM to follow step-by-step reasoning.Selfconsistency (SC) (Wang et al., 2023) instead increases the stochasticity by modifying the decoding process and obtaining multiple completions, which are then ensembled.</p>
<p>However, combining the two principles raises limitations.First, inference is significantly more expensive due to numerous runs, each generating long completions with many reasoning steps.Next, it may be impermissible to modify the decoding process in some settings, such as commercial deployments.Finally, stochasticity-based methods do not directly guide the diversity at the level of thought or method, but rather at the token level.This poses limitations because linguistic token diversity does not always ensure diverse and independent solution approaches.</p>
<p>In this paper, we explore how to explicitly promote the diversity of thought while mitigating the aforementioned issues.Prior work by Li et al. (2023) highlights the importance of prompt diversity, but their notion of diversity is captured through variety in the few-shot examples provided with the prompt; ours focuses on the reasoning approach.We first solicit the LLM to produce multiple-highlevel reasoning approaches for problem-solving (e.g., method of elimination, visualization techniques etc. for math reasoning problems).We then leverage GPT-4 to augment few-shot examples used in prior work (Wei et al., 2022) into the corresponding approaches, whenever applicable.</p>
<p>We propose DIV-SE (DIVerse reasoning path Self-Ensemble) to extract and aggregate responses Graph Coloring, GPT-4</p>
<p>Figure 1: Diversity of Thought enhances the inference cost vs.accuracy trade-off.We compare DIV-SE and IDIV-SE with SC (Wang et al., 2023) and CoT (Wei et al., 2022) across three benchmarks.The x-axis indicates the total inference cost (as defined in § 3) on the benchmark using the given method, while the y-axis represents the LLM's performance.The few-shot-CoT setting is represented by filled gray dots, while the zero-shot-CoT setting is indicated by unfilled dots.Notice that for a fixed cost, our approaches always give better performance.</p>
<p>(via majority vote) across multiple inference calls ( § 2.2).Since distinct approaches introduce diversity at the "thought" level, our methodology results in improved ensemble accuracy.In Fig. 1, we show that it yields more accurate results across multiple reasoning benchmarks at a fixed inference cost, without modifying the decoding procedure.For instance, in the BLOCKSWORLD 4/5 task (Valmeekam et al., 2022), DIV-SE improves the performance by 29.6 percentage points (p.p).However, this method still leverages multiple inference calls, which could be costly.</p>
<p>To reduce inference costs, we build on the observation that the approaches are often mutually independent, and can be combined in a single prompt to solicit multiple solutions (Cheng et al., 2023).Based on this premise, we propose IDIV-SE (Incall DIVerse reasoning path Self-Ensemble; § 2.2), which combines all approaches within the same prompt and aggregates all resulting outputs to leverage diversity with a reduced cost.Fig. 1 demonstrates that this method obtains comparable accuracy to DIV-SE and better performance than prior work with lower inference costs.</p>
<p>We push the pareto frontier of the cost-accuracy trade-off of prompting strategies across multiple reasoning tasks ( § 4), outperforming both CoT and SC prompting on both GPT-3.5 and GPT-4.This is evident from Fig. 1 for the AQUA-RAT (Ling et al., 2017), planning (Valmeekam et al., 2023), and graph coloring (Stechly et al., 2023) benchmarks, where there is a performance improvement of 16.52, 29.6, and 82.5 p.p respectively.These improvements, some of which are state-of-the-art, show the potential of thought diversity to extract complex reasoning abilities from LLMs that were impossible to leverage otherwise.We will open source our code upon publication to encourage further research.</p>
<p>Diversity through LLM Interactions</p>
<p>First, we introduce terms and notations that we use throughout the paper.We use upper case for sets, lower case for variables, and [n] = {1, • • • , n}.Approach: These are reasoning strategies for problem solving, denoted with the variable a.For example, for the GSM8K (Cobbe et al., 2021), a benchmark of grade-school math problems , some of the (generated) approaches can include a 1 ="using visualizations", a 2 ="working backwards", a 3 ="using direct calculation", and a 4 ="method of elimination".Persona: In addition to specifying "how" to solve a reasoning problem, specifying a persona can also influence how the LLM behaves (Salewski et al., 2023).We denote this with the variable p e.g., p 1 ="Thinking like Alan Turing", p 2 ="Thinking like Math Professor" for the GSM8K task.Note that both approaches and personas are dependent on the reasoning problem.</p>
<p>Using the LLM as a guide</p>
<p>Proposed method for creating prompts, which we term DIVERSEPROMPTING is presented in Algorithm 1. Below, we will describe each step in more detail.At a high-level, we solicit feedback from the LLM on how to solve tasks.</p>
<p>Step 1+2.Extracting Approaches &amp; Personas: Note that LLMs trained on internet-scale data encode a significant knowledge from multiple do-Q: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM?</p>
<p>Approach 1 <using hypothetical numbers> : Assume a starting length for the candle, say 100 centimeters.(…) The candle will be 8 centimeters shorter.The answer is 8 centimeters.</p>
<p>Approach 2 <using stepwise reduction>: Let x be the original length of the candle.After 4 hours(…) the candle will be 2 centimeters shorter at the end of 4 hours.The answer is 2 centimeters.</p>
<p>Approach 3 <using rate and time>: The rate of the candle melting is 2 centimeters per hour.So, if we multiply the rate by the time it burns, we (…)At 5:00 PM, the candle is 8 centimeters shorter than its original length.The answer is 8 centimeters.</p>
<p>Input Output</p>
<p>A: If a candle melts by 2 centimeters every hour and it burns from 1:00 PM to 5:00 PM, we can assume it burns for a total of 4 hours.4 divided by 2 is 2.the candle would be 2 centimeters shorter after burning from 1:00 PM to 5:00 PM.The answer is 2 centimeters.</p>
<p>CoT Prompting</p>
<p>IDIV-SE Prompting</p>
<p>Output</p>
<p>Figure 2: Diversity of Thought.This illustration depicts CoT and IDIV-SE prompting strategies.Notice that both have a single example.However, IDIV-SE presents more diversity in terms of reasoning paths.This enables it to generate diverse completions, yielding more accurate responses.</p>
<p>mains (Liang et al., 2022;Bubeck et al., 2023).While LLMs may not be perfect at solving reasoning tasks, we hypothesize that they are helpful in providing high-quality intermediate feedback.</p>
<p>To extract approaches, we utilize the following methodology: (i) Randomly picking a question from the reasoning dataset D we want to evaluate; and (ii) Creating an instruction prompt where we ask the LLM to generate the names of b ∈ [1, 5] approaches to solve the aforementioned question conforming to a predefined template (for easier post-processing).Refer to Figure 5 for an example of the prompt used.</p>
<p>We extract the part of the response that is compliant with the template and store it.We repeat this process c times (obtaining of c • b candidate approaches), and pick the n most frequent approaches to store in set A1 .This process is abstracted as method det_approaches(.).</p>
<p>One can repeat the above process used to extract relevant personas for a given reasoning task.However, we followed a simpler route and asked the model directly for relevant personas for a given task and then included them in the set of m candidate personas P used.This is abstracted as method det_personas(.).Note that no persona (ϕ) is also part of the persona set.</p>
<p>Step 3. Choosing the Best Persona, Approach Pairs: The choice of persona and approaches introduces a principled way to promote diversity.</p>
<p>If the set of personas is P , and the set of approaches is A, the Cartesian product of P and A yields the total number of prompts.In practice, for each combination (denoted by s i ) of persona and approach, we evaluate the prompt formed using the composition on a small validation set V 1 and choose the best performing "size" elements on the given task2 .</p>
<p>Step 4. Augmenting few-shot examples: Once the (subset of) approach and persona pairs are fixed, we ask the LLM to augment existing few-shot examples (denoted F = {f 1 , • • • }) with the given set of approaches.Specifically, we take the few-shot examples provided by Wei et al. (2022), and ask the LLM to solve them in the style of a chosen approach and persona pair (Fig. 8); we term the DIVERSEPROMPTING: Prompt creation.output augmented few-shot examples.This is abstracted in method augment(.),where Ti,j is the set of augmented few-shot examples corresponding to the approach and persona pair from s i and example f j .An example is visualized in the bottom left of Fig. 2, where the prompt contains different approaches for solving a math problem.</p>
<p>Designing the Prompts</p>
<p>Step 5. Prompt Composition: We create prompts for our approach using the best approach and persona pairs identified in step 3, and augmented fewshot examples from step 4 as shown in Fig. 2 and 4.</p>
<p>We now describe two techniques to generate prompts with the augmented demonstrations (T ) that have been accumulated.Candidate 1. DIV-SE: We first propose DIV-SE (DIVerse reasoning path Self-Ensemble), a method to execute the diverse set of approaches in different inference calls and aggregate their solutions.Apart from the question to be solved and the augmented few-shot examples, the final prompt contains a persona, approach, and additional instructions.One example is visualized in Fig. 4 (please refer to appendix for more examples of prompts: Fig. 9 through 16).Diversity is ensured through running inference with multiple prompts, each with a different approach and persona pairs and augmented few-shot examples.However, since the approaches are executed separately, generating a solution (via aggregation of multiple responses) requires multiple inference calls, which can be costly.</p>
<p>Candidate 2. IDIV-SE:</p>
<p>To further reduce the inference costs while promoting diversity, we propose IDIV-SE (In-call DIVerse reasoning path Self-Ensemble).In IDIV-SE, the final prompt is a composition of all approach and persona pairs and corresponding augmented few-shot examples, and the question to be solved.An example is presented in Fig. 2 (bottom left).More examples of prompts are presented in the appendix in Fig. 9 through 16.This noticeably decreases the number of calls to be made, since all few-shot examples are presented within the same prompt.We note that there might be error propagation due to the autoregressive nature of models.We evaluate this in detail in § 4.3.Practicality.Crucially, DIVERSEPROMPTING finds approaches that are general and reusable across similar reasoning problems.We reused the strategies identified for solving AQUA-RAT and Planning benchmark respectively in the MATH (counting and probability) and Graph Coloring benchmarks.This also reduces the cost of repeated evaluation on a separate evaluation set.Aggregation.We aggregate the responses via majority vote for both prompting strategies.Other aggregation strategies can also be leveraged, such as utilizing the LLM itself to aggregate responses or weighted aggregation.In § 4.4, we consider an aggregation strategy proposed by Yoran et al. (2023) and describe how compatible it is with our prompting approaches.</p>
<p>Experiments</p>
<p>We consider the following reasoning benchmarks.Arithmetic Reasoning: We use: (i) AQUA-RAT (Ling et al., 2017), a suite of algebraic word problems, (ii) GSM8K (Cobbe et al., 2021), a benchmark of grade-school math problems described in natural language (involving elementary arithmetic operations), and (iii) MATH (Counting and Probability) (Hendrycks et al., 2021), a collection of math problems from which we choose only counting and probability as these are not covered by GSM8K and AQUA-RAT.For all datasets, we use the test split for evaluation, containing 254, 1319, and 474 questions respectively.Planning Capabilities: We use the Blocksworld Planning benchmark proposed in Valmeekam et al. (2022Valmeekam et al. ( , 2023)).The benchmark has two datasets: one involves 3 blocks (BLOCKSWORLD 3, 100 We compare DIV-SE and IDIV-SE with SC (Wang et al., 2023) and CoT (Wei et al., 2022) across three benchmarks.The x-axis indicates the total cost (as defined in § 3) of running inference with the LLM on the benchmark using the given method, while the y-axis represents the LLM's performance.The FS-CoT setting is represented by filled gray dots, while the ZS-CoT setting is indicated by unfilled dots.Notice that for BLOCKSWORLD 3, despite being in the ZS-CoT setting, our approaches are more performant than the SC-s (FS-CoT) baseline.</p>
<p>instances), while the other dataset involves 4 or 5 blocks (BLOCKSWORLD 4/5, 500 instances).</p>
<p>Constraint Satisfaction Optimization: We use the GRAPH COLORING benchmark (Stechly et al., 2023) containing 100 examples to test reasoning for constraint satisfaction.Commonsense Reasoning: We use COMMONSENSEQA (Talmor et al., 2019) which consists of generic multiple-choice questions elicited for testing common sense reasoning.We use the validation split containing 1,221 questions.</p>
<p>Language Models.We evaluate our proposed methods on both GPT-3.5 Turbo (OpenAI, 2022) and GPT-4 (OpenAI, 2023b).We also conduct an additional evaluation on LLaMA-2 70B (Touvron et al., 2023) to explore the performance of our technique on open-source LLMs.For the latter, we use meta-llama/Llama-2-70b-chat-hf through the Transformers library (Wolf et al., 2019).</p>
<p>Baselines.</p>
<p>We consider Chain-of-Thought (CoT) (Wei et al., 2022) and Self-Consistency (SC ) (Wang et al., 2023) as our baselines.For CoT, we consider two settings: zero-shot (ZS) CoT (Kojima et al., 2022) (i.e., "Think step by step" is added to the prompt), and few-shot (FS) CoT (i.e., CoT with demonstrations).In our SC runs, we set the temperature T = 0.7 without top-k truncation and sample up to s ∈ [1, 10] outputs (denoted SC-s).For all other approaches, we set T = 0. We use ensembles of size 5 in IDIV-SE and DIV-SE for GSM8K and AQUA-RAT.For the planning, GRAPH COLORING, and COMMON-SENSEQA benchmarks, we use a size of 3. Performance Metrics.We measure the accuracy on the task, and the generation inference cost.To measure the cost, we assume 1000 tokens are about 750 words3 .For GPT-4 (8K) the input and output prices used to estimate inference cost are $0.03/1ktokens and $0.06/1k tokens, respectively.For GPT 3.5 Turbo (16K), the input and output prices used in the cost estimation are $0.003/1k(tokens) and $0.004/1k (tokens) respectively.Results Summary.include: Across most benchmarks we consider, our techniques provide substantial performance gains (e.g., 16.52, 82.5, and 14.3 p.p improvements for AQUA-RAT, GRAPH COLORING, and MATH respectively).They are also Pareto optimal (in terms of the utility vs. cost trade-off).For the challenging planning benchmark (BLOCKSWORLD 4/5), our techniques improve accuracy by 29.6 p.p achieving state-of-the-art performance.Using GPT-4 for BLOCKSWORLD 3, our approach (in the ZS-CoT setting) is substantially more effective than SC-10 (in the FS-CoT setting) at 4× lower cost (Figure 3 (center figure)).</p>
<p>Since prompts are chained together in IDIV-SE, error propagation is possible.Our evaluation on AQUA-RAT in § 4.3 suggests that even though error propagation is estimated as less than 6.5% for both models, these rates are comparable to differences in performance between DIV-SE and IDIV-SE.When combined with aggregation approaches that are capable of reasoning across the diverse generations (Yoran et al., 2023), we observe additional performance gains as shown in § 4.4.For the AQUA-RAT benchmark for instance, we see an accuracy of 67.7% for GPT-3.5 (3.23 p.p improvement to majority voting).</p>
<p>Results</p>
<p>Main Results</p>
<p>We present the summary of results in  , 1998).We use natural language prompts in all the experiments.For the baseline runs, we introduce minor alterations to the prompt originally proposed by Valmeekam et al. (2023).These changes involve incorporating an explicit directive to prevent under-block movement and resolving minor language ambiguities we observed to be problematic during initial investigation.Furthermore, we reposition the initial condition and goal state information to the beginning of the prompt.The modified improved prompt is presented in Fig. 9.We aggregate the plans through majority voting and utilize string matching for comparing the plans.As a result, we optimize the plan by eliminating the redundant "no-op" steps.GPT-4 Results: We note that GPT-4 performs slightly better in a ZS setting, and use this to run all experiments.From Fig. 1, we observe that for BLOCKSWORLD 3, ZS-CoT records an accuracy of 70%, while SC-10 reaches an accuracy level of 73%.IDIV-SE enhances the absolute accuracy by 12 p.p above the ZS-CoT baseline, while DIV-SE produces an impressive state-of-the-art accuracy of 94%.An analysis of the six unsuccessful instances suggests the capacity for further performance improvement by increasing the size of the ensemble, as already two out of five current approaches generate accurate plans.For the BLOCKSWORLD 4/5 case, the ZS-CoT accuracy is 40%, while SC-10 has an accuracy of 41.2%.Here, IDIV-SE results in an absolute gain of 17 p.p above the ZS-CoT baseline, and DIV-SE too enhances performance, leading to 69.6%.As outlined in Fig. 1 and 3, both IDIV-SE and DIV-SE achieve Pareto optimality.GPT-3.5 Results: The baseline performance on BLOCKSWORLD 3 is 6%, and on BLOCKSWORLD 4/5 is 0.6%.We do not see any additional improvement using both IDIV-SE and DIV-SE.Qualitatively, we observe that during plan generation, GPT-3.5 fails to follow the restrictions provided as part of the problem instructions too often, leading to either infeasible or incorrect plans.This shows instruction following capabilities are crucial to the success of the methods proposed here.</p>
<p>Constraint Satisfaction via GRAPH COLORING</p>
<p>There may exist numerous non-optimal yet valid colorings for a given graph.Since exact string matching is not usable for identifying the majority solution from the ensembles of IDIV-SE and DIV-SE, we employ the external, sound verifier (Stechly et al., 2023) to pick the correct solution.</p>
<p>GPT-4 Results: From Fig. 1, it is observed that ZS-CoT achieves an accuracy of 15%, whereas SC-10 attains an accuracy level of 23%.IDIV-SE improves the absolute accuracy by 59 p.p above the ZS-CoT baseline.Remarkably, DIV-SE delivers a state-of-the-art accuracy of 97%.Given that GPT-4's performance plateaus in the ZS setting, we chose to omit conducting the few-shot experiments.Summary: Methods in this work often demonstrate state-of-the-art performance on reasoning tasks.This is most significant in the planning and constraint satisfaction benchmarks, where the corresponding authors claimed immense difficulty for existing LLMs.Our work shows that statusquo prompt design approaches including chain of thought are too generic for these problems, and prompt customization (via DIVERSEPROMPTING) can yield substantial gains by guiding the chain of thought to the general nature of the problem.</p>
<p>Open Source Models</p>
<p>Due to the limited computational budget, we only performed experiments with the AQUA-RAT benchmark.Please refer to Appendix B for further details.</p>
<p>Errors &amp; Prompt Utility</p>
<p>Error Propagation: Due to the autoregressive nature of LLM decoding, early incorrect answers in IDIV-SE may get propagated to the latter ones.</p>
<p>To quantify this, we select examples where the solution is incorrect and all five approaches produce the same erroneous answer.We focus only on these cases to see if e.g., a wrong conclusion in the initial approaches leaks into the following ones.Next, we attempt the last two approaches again in a separate session: if the LLM generates the same outcomes as in the original session (i.e., IDIV-SE setup) within 3 attempts, we consider it as no error propagation.However, if it does not produce the same answer within the 3 attempts, we interpret this as a case of error propagation since the change in answer could be attributed to the initial approaches with wrong answers in the chain.</p>
<p>We measure this phenomenon on AQUA-RAT (FS-CoT) on both GPT-4 and GPT-3.5.We find that GPT-4 and GPT-3.5 have error propagation rates of 6.2% and 5.5% respectively, which are comparable to performance differences between DIV-SE and IDIV-SE, making error propagation one of the main explanatory hypotheses for the differences between the two methods.Reducing these error rates remains a challenging problem given the autoregressive nature of current LLMs.</p>
<p>Beyond Thinking Step by</p>
<p>Step: The diverse approaches and personas we utilize not only enhance the performance in IDIV-SE and IDIV-SE, but are also independently superior to ZS-CoT.Table 4 highlights this effect, which showcases the importance of conditioning the model for solutions via DIVERSEPROMPTING.</p>
<p>Alternative Aggregation Strategies</p>
<p>Our aggregation thus far relies on majority voting.Li et al. (2023), which shows the importance of prompt diversity.However, they rely on selecting few-shot demonstrations from a holdout set (which defines diversity in their method), without explicitly stating reasoning pathways.</p>
<p>Conclusions</p>
<p>In this work, we promoted diversity of thought as a principled prompting strategy and proposed methodologies that leverage the LLM as a guide to design a diverse set of approaches to solve complex reasoning tasks.Extracting solution approaches from LLMs themselves becomes a discovery mechanism that seeds and conditions generative solutions.Reported results on a variety of tasks confirm that there is a large space for improvement in com-plex reasoning by uncovering the necessary skills and knowledge from LLMs through targeted and diverse prompting methods.These results demonstrated how promoting diversity can improve the Pareto frontier of accuracy-cost trade-off for current LLMs and yield state-of-the-art solutions for planning and mathematical reasoning tasks.We hope that future work will expand these results to complex tasks from other real-world applications.</p>
<p>Limitations</p>
<p>Our study mainly experimented with GPT-3.5 and GPT-4 models because of their instructionfollowing capabilities.While current open-source models have shown remarkable improvements to this end, they are still not able to reliably follow instructions relevant to complex reasoning tasks (e.g.state tracking, plan validity, constraint satisfaction).We hope that progress in the field will enable further experimentation in this direction.</p>
<p>In addition, we also observe that error propagation during autoregressive generation may sometimes negatively impact the performance of IDIV-SE, where all approaches are executed in order within the same prompt.Some of this could be addressed by explicitly instructing the model to forget about the previous solution but ultimately as long as previous generation history remains in context and short-term memory, error propagation risks may still need to be tracked and measured.</p>
<p>B Model Details</p>
<p>B.1 Open-Source Models</p>
<p>We perform the Llama-2 70B experiments with a single 80GB A100 GPU.To fit the 70B model to a single A100, we use 8-bit precision through bitsandbytes (Dettmers et al., 2022a,b).Further, Dettmers et al. (2022a) reports no performance drop with this quantization method.</p>
<p>As the system prompt, we use You are a helpful, respectful and honest assistant.We perform inference with greedy decoding, having temperature T = 0.</p>
<p>C Additional Results</p>
<p>In this section, we provide additional results on COMMONSENSEQA and GSM8K benchmarks.</p>
<p>C.1 Common sense via COMMONSENSEQA</p>
<p>Table 2 presents the results of the experiments.Overall, the improvements in accuracy are relatively modest.This is likely because answering questions in COMMONSENSEQA does not demand as much reasoning and thought diversity as is required in some other benchmarks.In addition, the dataset also contains a number of ambiguous questions, which if read verbatim may have many plausible answers but the ground truth contains only one answer.</p>
<p>C.2 Arithmetic reseasoning via GSM8K</p>
<p>GPT-4 Results: As shown in Fig. 6, accuracy on GSM8K have nearly plateaued, with the ZS-CoT and FS-CoT baselines achieving accuracies of 94% and 95% respectively.IDIV-SE does not produce any significant gains in either setting.On the other hand, DIV-SE reaches accuracy of 96.3% in both FS-CoT and ZS-CoT settings, providing a modest improvement.GPT-3.5 Results: Here, the gains are more substantial.Compared to the ZS-CoT baseline of 76.11%, IDIV-SE provides an improvement of 5.31 p.p. DIV-SE goes a step further, enhancing the accuracy by 10.39 p.p.In the FS-CoT setting, DIV-SE posts an accuracy improvement of 7.68 p.p (with a baseline accuracy of 81.4%).</p>
<p>Fig. 3 (rightmost) presents the cost vs.accuracy trade-offs between IDIV-SE, DIV-SE, and SC.While the performance of SC does improve with the expansion of reasoning paths, both IDIV-SE and DIV-SE offer better trade-offs.</p>
<p>D Evaluating Ensemble Sizes</p>
<p>Figure 6 depicts the average accuracy of different ensemble sizes on GSM8K for both ZS-CoT and FS-CoT settings, utilizing GPT-4 and GPT-3.5.Similarly, Figure 7 demonstrates the average accuracy of various ensemble sizes on AquA for both ZS-CoT and FS-CoT settings, using GPT-4 and GPT-3.5.It is noteworthy that in both AQuA and GSM8K, even an ensemble of size three yields significant performance improvements over the baseline, which we attribute to the high diversity and independence of reasoning paths.</p>
<p>E Prompt Templates</p>
<p>The following section provides a comprehensive visual representation of the prompts used in our study.These prompts, depicted in Figures 9 through 16, were used in different settings and for the planning, AQuA, and graph coloring benchmarks, and incorporate various personas and approaches.</p>
<p>Figure 9 illustrates the prompt used in the baseline run, Figure 10 demonstrates the prompt employed when applying a Finite State Machine approach, Figure 11 depicts the prompt used when incorporating the persona of Alan Turing and an Action Rationale approach, and lastly, Figure 12 shows the prompt used when applying the persona of Alan Turing and the Progressive Block Placement approach in the zero-shot setting for the planning benchmark.</p>
<p>Figure 13 illustrates the algebraic approach, while Figure 14 and Figure 15 demonstrate the prompts incorporating the personas of Alan Turing and Dr. Patel, a renowned mathematician, respectively, in the few-shot-CoT setting for the AQuA benchmark.</p>
<p>In Figure 16 we present the IDIV-SE prompt used in the zero-shot setting for the graph coloring benchmark.</p>
<p>Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?<Using direct calculation>: Subtract the total number of golf balls lost from (…) Therefore, he had 33 golf balls at the end of Wednesday.</p>
<p>Q: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM? <using direct calculation>: The candle burns for 4 hours from 1:00 PM to 5:00 PM. (…) The candle will be 8 centimeters shorter.The answer is 8 centimeters.</p>
<p>Input 1</p>
<p>Output 1</p>
<p>DIV-SE Prompting</p>
<p>Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?</p>
<p><Using visualization>: Represent the problem using a diagram with three stages: (….) leaving him with 33 golf balls at the end of Wednesday.</p>
<p>Q: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM Input 2 <using algebra>: Let x be the original length of the candle.After 4 hours(…) the candle will be 2 centimeters shorter at the end of 4 hours.</p>
<p>The answer is 2 centimeters.</p>
<p>Output 2 Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wed,lost 2 more.How many golf balls did he have at the end of wednesday?</p>
<p><Using algebra>: Let x be the number of golf balls Michael had …we find that Michael had 33 golf balls at the end of Wednesday.</p>
<p>Q: A candle melts by 2 centimeters every hour that it burns.How many centimeters shorter will a candle be after burning from 1:00 PM to 5:00 PM?</p>
<p><using visualization>: Here's the visual representation: ---| (2 hours later) (…)At 5:00 PM, the candle is 8 centimeters shorter than its original length.The answer is 8 centimeters.
|-----------------| (original length) |--------| (1 hour later) |-</p>
<p>Output 3 Input 3</p>
<p>Figure 4: DIV-SE prompting.</p>
<p>Use five distinct approaches to solve the given problem accurately.there is no exact match choose the closest option.Q: {Question} Use the following output format: Approach 1 &lt; name of the approach &gt; : &lt; Details of Approach 1 &gt; Approach 2 &lt; name of the approach &gt; : &lt; Details of Approach 2 &gt; Approach 3 &lt; name of the approach &gt; : &lt; Details of Approach 3 &gt; Approach 4 &lt; name of the approach &gt; : &lt; Details of Approach 4 &gt; Approach 5 &lt; name of the approach &gt; : &lt; Details of Approach 5 &gt;</p>
<p>Input Prompt</p>
<p>Solve the problem accurately using {approach name}.</p>
<p>Question: John found that the average of 15 numbers is 40.If 10 is added to each number then the mean of the numbers Answer Choices: (A) 50 (B) 45 (C) 65 (D) 78 (E) 64 A: If 10 is added to each number, then the mean of the numbers also increases by 10.So the new mean would be 50.The answer is (A).</p>
<p>Approach <using {approach name} >:
Augmented Few-shot Examples
Approach <using visualization>:</p>
<p>To solve this problem using visualization, let's consider the given information and the effect of adding 10 to each number on the average.1.The average of 15 numbers is 40.This means that the sum of these 15 numbers is 15 * 40 = 600.</p>
<ol>
<li>
<p>Now, we add 10 to each number.Since there are 15 numbers, the total added is 15 * 10 = 150.</p>
</li>
<li>
<p>The new sum of the 15 numbers is 600 + 150 = 750.4. To find the new average, we divide the new sum by the number of values, which is 750 / 15 = 50.So, the mean of the numbers after adding 10 to each number is 50.The answer is: (A) 50</p>
</li>
</ol>
<p>Approach <using logical reasoning>:</p>
<p>We know that adding the same constant to each number in a set will increase the average by that constant.In this case, we are adding 10 to each number, so the average will increase by 10.New average = 40 + 10 = 50 The answer is: (A) 50</p>
<p>Approach <using elimination strategy>:</p>
<p>We know that adding 10 to each number will increase the average by 10.So, we're looking for an answer choice that is 10 more than the original average of 40.I am playing with a set of blocks where I need to arrange the blocks into stacks.</p>
<p>[STATEMENT] As initial conditions I have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the What is the plan to achieve my goal?Just give the actions in the plan.</p>
<p>[PLAN]  [PLAN]</p>
<p>procedure DIVERSEPROMPT(size, type, F, D, V ) ▷ Step 1: Identify different approaches to be used.A = {a1, . . ., an} ← det_approaches(D) ▷ where A is the set of approaches ▷ Step 2: Identify different personas to be used.P = {ϕ, p1, . . ., pm} ← det_personas(D) ▷ where P is the set of personas ▷ Step 3: Find the best combination.S = {s1, . . .ssize} ← combine(A, P, size, V ) ▷ where S is the set of combined approaches and personas, and si = (p, ai ∈ A) ▷ Step 4: Augment the few-shot examples.T = { Ti,j, . ..} ← augment(S, F ) ▷ where T is the set of augmented examples, and Ti,j is formed using si ∈ S and fj ∈ F ; |T | =size ▷ Step 5: Compose the final prompt.O ← compose(T, S, type) return O ▷ Return the final output.end procedure</p>
<p>Figure 3 :
3
Figure3: Diversity of Thought enhances the inference cost and accuracy trade-off.We compare DIV-SE and IDIV-SE with SC(Wang et al., 2023) and CoT(Wei et al., 2022) across three benchmarks.The x-axis indicates the total cost (as defined in § 3) of running inference with the LLM on the benchmark using the given method, while the y-axis represents the LLM's performance.The FS-CoT setting is represented by filled gray dots, while the ZS-CoT setting is indicated by unfilled dots.Notice that for BLOCKSWORLD 3, despite being in the ZS-CoT setting, our approaches are more performant than the SC-s (FS-CoT) baseline.</p>
<p>Figure 5 :
5
Figure 5: Prompt template for extracting diverse approaches for problem solving.</p>
<p>Figure 6 :
6
Figure 6: Average accuracy for different ensemble sizes on GSM8K for ZS-CoT and FS-CoT settings on GPT-4 and GPT-3.5.Note that all graphs are zoomed in.</p>
<p>Figure 7 :
7
Figure 7: Average accuracy for different ensemble sizes on AQUA-RAT for ZS-CoT and FS-CoT settings on GPT-4 and GPT-3.5.Note that all graphs are zoomed in.</p>
<p>is exactly 10 more than the original average of 40, so that's the correct answer.The other choices can be eliminated because they are not 10 more than the original average.Approach <using algebra>:Let's denote the sum of the 15 numbers as S. Since the average of the 15 numbers is 40, we can write the equation: S / 15 = 40 S = 40 * 15 = 600 Now, we add 10 to each number, so the new sum is S + 15 * 10 = 600 + 150 = 750.The new average is: New average = 750 / 15 = 50 The answer is: (A) 50</p>
<p>Figure 8 :
8
Figure 8: Illustration of Augmentation of few-shot examples -Algorithm 1, Step 4</p>
<p>Figure 9 :
9
Figure 9: Zero-shot prompt used in the baseline run of the Planning -Blocksworld Domain</p>
<p>Figure 12 :
12
Figure 12: The Zero-shot prompt used in the Alan Turing + Progressive Block Placement Approach for solving the Planning -Blocksworld Domain Problem.</p>
<p>Table 1 and 2. Detailed results are available in Appendix C.These also cover results on the impact of ensemble size in Appendix D.
accuracy while maintaining low costs.4.1.2 Counting and probabilistic reasoningvia MATHGPT-4 Results: From Table 2, we see that DIV-SEachieves an accuracy increase of 14.3 and 16.87 p.pin the FS-CoT (baseline of 66.46%) and ZS-CoT(baseline of 62.24%) settings, respectively. Onthe other hand, IDIV-SE achieves a boost of 5.54and 9.76 p.p in the FS-CoT and ZS-CoT settings,respectively, over the baseline.GPT-3.5 Results: From Table 2, we see that DIV-SE yields a gain of 21.84 and 13.04 p.p in the FS-CoT (baseline of 30.38%) and ZS-CoT (baselineof 31.90%) settings, respectively. Likewise IDIV-SE achieves a boost of 13.72 and 10.60 p.p in theFS-CoT and ZS-CoT settings, respectively.4.1.3 Planning via BLOCKSWORLDSettingMethod AQuA MATH CQACoT59.0031.9071.40SC-361.4032.0772.00GPT-3.5 ZSSC-563.3738.1972.80IDIV-SE62.6042.5074.00DIV-SE72.8344.9474.50CoT57.4830.3879.4GPT-3.5 FSIDIV-SE64.5744.1080.00DIV-SE72.8452.2280.40CoT70.4762.2481.60GPT-4 ZSIDIV-SE71.6572.0082.50DIV-SE80.3179.1181.70CoT71.9066.4687.70GPT-4 FSIDIV-SE79.9072.0089.00DIV-SE84.2580.7688.00Table 2: Performance on AQUA-RAT, MATH (Count-ing and Probability), and COMMONSENSEQA for GPT-3.5 Turbo and GPT-4 in the ZS-CoT and few-shot-CoTsettings respectively.4.1.1 Arithmetic reasoning via AQUA-RATGPT-4 Results: In Table 2, we observe that DIV-SEachieves an accuracy increase of 9.84 and 14.6 p.pin the FS-CoT (baseline accuracy of 71.9%) andZS-CoT (baseline of 70.47%) settings, respectively.While the gains from IDIV-SE are nominal in ZS-CoT, it achieves a boost of 7.7 p.p for FS-CoT.GPT-3.5 Results: In Table 2, we see that DIV-SEyields a gain of 14.23 and 16.52 p.p in the FS-CoT (baseline of 57.48%) and ZS-CoT (baselineof 59%) settings, respectively. Within the FS-CoTsetting, IDIV-SE gets an absolute increase of 7 p.p.Note that Fig. 1 also displays the total inferencecost. Both IDIV-SE and DIV-SE are Pareto opti-mal, indicating their capacity to achieve a higher
Setup:The benchmark provides both natural language and Planning Definition and Domain Language prompts(McDermott et al.</p>
<p>Table 3 :
3
We hypothesize that this is partly due to model's lack of capabilities to both follow instructions and the mentioned approach in the absence of examples.Results on AQUA-RAT and LLaMA-2 70B.
Prompting Strategy ZS-CoT (%) FS-CoT (%)CoT31.3229.1IDIV-SE27.0039.7DIV-SE32.0039.9
Table 3 demonstrates the results for LLaMA-2 70B with 8-bit quantization.DIV-SE and IDIV-SE demonstrate an improvement of over 10 p.p over the baseline in the FS-CoT settings.However, the gain in the ZS-CoT setting has been negligible.</p>
<p>Table 4 :
4
Prompts, derived from approaches and personas, boost performance.Blue rows denote ZS-CoT prompts, while black lines denote FS-CoT prompts.∅ denotes absence (of persona or approach respectively).
MethodGPT-4 (%) GPT-3.5 (%)Majority Voting79.9064.47Meta Reasoning79.2467.70</p>
<p>Table 5 :
5
Alternative aggregation strategies.
Observe</p>
<p>table.My goal is to have that the red block on top of the blue block and the orange block on top of the red block.Here are the actions I can do: Pick up a block from the table Unstack a block from on top of another block Put down a block on the table Stack a block on top of another block I have the following restrictions on my actions: I can only pick up or unstack one block at a time.I can only pick up or unstack a block if my hand is empty.I can only pick up a block if the block is on the table and the block is clear.A block is clear if the block has no other blocks on top of it and if the block is not picked up.
I can only unstack a block from on top of another block if the block I am unstacking wasreally on top of the other block.I can only unstack a block from on top of another block if the block I am unstacking isclear.Once I pick up or unstack a block, I am holding the block.I can only put down a block that I am holding.I can only stack a block on top and not under of another block if I am holding the blockbeing stacked.I can only stack a block on top and not under of another block if the block onto which Iam stacking the block is clear.Once I put down or stack a block, my hand becomes empty.Once you stack a block on top of a second block, the second block is no longer clear.</p>
<p>You are playing with a set of blocks where you need to arrange the blocks into stacks.What is the plan to achieve the goal?<Initial State> : As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.<GoalState>:Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.You can only pick up or unstack one block at a time.2.You can only pick up or unstack a block if your hand is empty.3.You can only pick up a block if the block is on the table and the block is clear.A block is clear if the block has no other blocks on top of it and if the block is not picked up.4.You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.5.You can only unstack a block from on top of another block if the block you are unstacking is clear.6.Once you pick up or unstack a block, you are holding the block.7.You can only put down a block that you are holding.8.You can only stack a block on top and not under of another block if you are holding the block being stacked.9.You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.10.Once you put down or stack a block, your hand becomes empty.11.Once you stack a block on top of a second block, the second block is no longer clear.Using a finite state machine and a search algorithm what is the plan to achieve the goal?You can model each state of the blocks configuration on the table and the hand as a state.For each action step check that the step follows the rules and that the step brings you closer to the goal.After each action describe the state of the table and hand.Always check whether the final state satisfies the goal mentioned.<GoalState>:Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.You are playing with a set of blocks where you need to arrange the blocks into stacks.<InitialState>:As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.<GoalState>:Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.You can only pick up or unstack one block at a time.2.You can only pick up or unstack a block if your hand is empty.3.You can only pick up a block if the block is on the table and the block is clear.A block is clear if the block has no other blocks on top of it and if the block is not picked up.4.You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.5.You can only unstack a block from on top of another block if the block you are unstacking is clear.6.Once you pick up or unstack a block, you are holding the block.7.You can only put down a block that you are holding.8.You can only stack a block on top and not under of another block if you are holding the block being stacked.9.You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.10.Once you put down or stack a block, your hand becomes empty.11.Once you stack a block on top of a second block, the second block is no longer clear.Thinking like Alan Turing starting from the <Initial State> build a plan to get to the <Goal State>.For each action step carefully check that the step follows the rules.<GoalState>: Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.You are playing with a set of blocks where you need to arrange the blocks into stacks.<InitialState> : As initial conditions you have that, the orange block is clear, the hand is empty, the blue block is on top of the red block, the orange block is on top of the blue block and the red block is on the table.<GoalState> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.You can only pick up or unstack one block at a time.2.You can only pick up or unstack a block if your hand is empty.3.You can only pick up a block if the block is on the table and the block is clear.A block is clear if the block has no other blocks on top of it and if the block is not picked up.4.You can only unstack a block from on top of another block if the block you are unstacking was really on top of the other block.5.You can only unstack a block from on top of another block if the block you are unstacking is clear.6.Once you pick up or unstack a block, you are holding the block.7.You can only put down a block that you are holding.8.You can only stack a block on top and not under of another block if you are holding the block being stacked.9.You can only stack a block on top and not under of another block if the block onto which you are stacking the block is clear.10.Once you put down or stack a block, your hand becomes empty.11.Once you stack a block on top of a second block, the second block is no longer clear.Thinking like Alan Turing, starting from the <Initial State> build a plan to get to the <Goal State> .For each action step carefully check that the step follows the rules.Divide the task into smaller steps, starting with placing the bottom block first, followed by the middle blocks, and finally the top block.<Goal State> : Your goal is to have that the red block on top of the blue block and the orange block on top of the red block.
Here are the actions you can do: -Pick up a block from the table -Unstack a block from on top of another block -Put down a block on the table -Stack a block on top of another block Rules: <assess the action> : &lt; are we building the stack bottom up, check carefully&gt; 1. [PLAN] Here are the actions you can do: Here are the actions you can do: -Pick up a block from the table -Pick up a block from the table -Unstack a block from on top of another block -Unstack a block from on top of another block -Put down a block on the table -Stack a block on top of another block -Put down a block on the table -Stack a block on top of another block Rules: <action> : &lt; action to be performed in this step &gt; <state> : <state> 1. output format for each step until you reach the goal state: Rules: 1.Figure 10: The Zero-shot prompt using Finite State Machine Approach for solving the Planning -Blocksworld Figure 11: The Zero-shot prompt used with the persona of Alan Turing and Action Rationale approach for solving Domain Problem. the Planning -Blocksworld Domain Problem.
Correspondence to ranjitan@microsoft.com and besmira.nushi@microsoft.com.
In practice, we set c = 100, b = 5, n ∈ {3, 5}, and |V | &lt;
.
For a given reasoning task, we perform this process once (for GPT-3.5 Turbo), and re-use our selection across all LLMs we evaluate.
https://openai.com/pricing
AppendixA Prompt used for DIVERSEPROMPTING Our diverse prompting strategy for IDIV-SE and DIV-SE is showcased in Fig.2and Fig.4respectively.The instrumental prompt template that determines our approaches is presented in Fig.5.Think like Dr. Patel, the mathematician who specializes in algebra and solve the problem.If there is no exact match choose the closest option.To determine the number of keystrokes needed to type the numbers from 1 to 500, let's break down the problem by analyzing the number of digits in each range: For numbers 1-9, there are 9 numbers with 1 digit each, requiring 9 keystrokes.For numbers 10-99, there are 90 numbers with 2 digits each, requiring 90 * 2 = 180 keystrokes.For numbers 100-499, there are 400 numbers with 3 digits each, requiring 400 * 3 = 1200 keystrokes.For the number 500, it has 3 digits, requiring 3 keystrokes.Now, we can sum up the keystrokes for each range: 9 (for 1-9) + 180 (for 10-99) + 1200 (for 100-499) + 3 (for 500) = 1392 keystrokes.The answer is: (b) 1392 As a math professor, use 3 distinct approaches and without using built-in algorithms, write python programs to color the following graph, described as a set of edges, such that no two vertices on the same edge share a color.You may use at most 3 colors.Vertex 0 is connected to vertex 7. Vertex 0 is connected to vertex 8. Vertex 0 is connected to vertex 9. Vertex 0 is connected to vertex 11.Vertex 1 is connected to vertex 13.Vertex 2 is connected to vertex 9. Vertex 3 is connected to vertex 8. Vertex 3 is connected to vertex 11.Vertex 3 is connected to vertex 12. Vertex 4 is connected to vertex 12. Vertex 5 is connected to vertex 11.Vertex 6 is connected to vertex 9. Vertex 7 is connected to vertex 10.Vertex 7 is connected to vertex 13.Vertex 9 is connected to vertex 11.Vertex 10 is connected to vertex 13.Vertex 11 is connected to vertex 13.There are a total of 14 vertices.Please label every vertex, even if it is disconnected from the rest of the graph.Please provide each vertex's color.Do not skip any vertices.Each color must be provided on a new line in the response and should be formatted as "VERTEX NUMBER: VERTEX COLOR ASSIGNMENT (Color n)".Output format: Approach 1 <name of the approach> : &lt; python program from scratch to color the given graph accurately &gt; Approach 2 <name of the approach> : &lt; python program from scratch to color the given graph accurately&gt; Approach 3 <name of the approach> : &lt; python program from scratch to color the given graph accurately&gt;
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates Inc202033Language models are few-shot learners</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, 2022</p>
<p>Batch prompting: Efficient inference with large language model apis. Zhoujun Cheng, Jungo Kasai, Tao Yu, 2023</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, 2021Training verifiers to solve math word problems</p>
<p>Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.073398-bit matrix multiplication for transformers at scale. 2022aarXiv preprintint8 (</p>
<p>8-bit optimizers via block-wise quantization. Tim Dettmers, Mike Lewis, Sam Shleifer, Luke Zettlemoyer, 9th International Conference on Learning Representations. ICLR2022b</p>
<p>A guidance language for controlling large language models. 2024</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021</p>
<p>Erik Jones, Anca Dragan, arXiv:2303.04381Aditi Raghunathan, and Jacob Steinhardt. 2023. Automatically auditing large language models via discrete optimization. arXiv preprint</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Advances in Neural Information Processing Systems. 2022</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, arXiv:2104.086912021arXiv preprint</p>
<p>Making language models better reasoners with step-aware verifier. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, Weizhu Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Pddl-the planning domain definition language. Drew Mcdermott, Malik Ghallab, Adele E Howe, Craig A Knoblock, Ashwin Ram, Manuela M Veloso, Daniel S Weld, David E Wilkins, 1998</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, arXiv:2311.164522023arXiv preprint</p>
<p>Michael Nye, Anders J Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Marten Bosma, Daan Luan, arXiv:2112.00114Show your work: Scratchpads for intermediate computation with language models. 2021arXiv preprint</p>
<p>Introducing chatgpt. OpenAI. 2023a. Gpt-4 technical report. OpenAI. 2023b. Gpt-4 technical report. 2022OpenAI</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, Zeynep Akata, arXiv:2305.14930-context impersonation reveals large language models' strengths and biases. 2023arXiv preprint</p>
<p>Gpt-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, arXiv:2310.123972023arXiv preprint</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>On the planning abilities of large language models -a critical investigation. Karthik Valmeekam, Matthew Marquez, 2023Sarath Sreedharan, and Subbarao Kambhampati</p>
<p>Large language models still can't plan (a benchmark for llms on planning and reasoning about change). Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati, arXiv:2206.104982022arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, Conference on Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, Douglas C Schmidt, arXiv:2302.11382A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023arXiv preprint</p>
<p>Huggingface's transformers: State-ofthe-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Answering questions by meta-reasoning over multiple chains of thought. Tomer Ori Yoran, Ben Wolfson, Uri Bogin, Daniel Katz, Jonathan Deutch, Berant, arXiv:2304.130072023arXiv preprint</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. Quoc Le, Ed Chi, 2022arXiv preprint</p>
<p>Selfdiscover: Large language models self-compose reasoning structures. Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V Le, Ed H Chi, Denny Zhou, Swaroop Mishra, Huaixiu Steven, Zheng , arXiv:2402.036202024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
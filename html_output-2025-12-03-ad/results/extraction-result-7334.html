<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7334 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7334</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7334</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-264172681</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10632v1.pdf" target="_blank">BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</a></p>
                <p><strong>Paper Abstract:</strong> The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7334.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7334.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned, generalist large language model from OpenAI used in this work both to generate protocol-specific pseudofunctions (teacher) and to generate/plan protocols (student); also used as a component in a retrieval-augmented, chain-of-thought agent for end-to-end protocol creation and lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned generalist LLM (OpenAI API); used as both teacher and student; also used with tool-use / retrieval and chain-of-thought agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (laboratory protocol planning; molecular/microbiology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Text-based simulation of laboratory protocol planning: (a) convert free-text protocols into protocol-specific pseudofunctions and step-by-step pseudocode; (b) next-step prediction (choose the next pseudofunction and its arguments); (c) full protocol generation from title/description plus admissible pseudofunctions; (d) function retrieval (select which pseudofunctions from a pool are required).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>One-shot example prompts for pseudocode/pseudofunction generation (teacher); student prompts that provide admissible pseudofunctions plus title/description to reconstruct pseudocode; settings included shuffled vs ordered function lists; automatic feedback/error loop to detect Python/pseudocode errors and missing argument units; retrieval-augmented Toolformer-like chain-of-thought agent (LangChain) to search BIOPROT, extract functions, and assemble new protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Function-level accuracy (exact match %), precision & recall of function calls (counts include repeats), normalized Levenshtein distance (L_dn) for sequence ordering, BLEU for argument value accuracy, SciBERTscore (average cosine similarity on SciBERT embeddings) for argument values, precision/recall for function retrieval, GPT-4 preference rate when used as an evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Protocol generation (admissible functions provided, random shuffling): precision 48.8%, recall 49.4% (GPT-4, Table 5). Protocol generation (nearest-neighbour distractors): precision 32.5%, recall 39.2% (Table 5). GPT-4 used as an evaluator preferred model outputs over ground truth at rates ~40.9–43.9% depending on shuffle/feedback (Table 8). Real-world validation: GPT-4 generated two novel protocols (reviewed by a scientist); one (E. coli overnight culture + glycerol stock) was executed successfully in the lab.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared primarily to GPT-3.5 (see separate entry): GPT-3.5 protocol generation (random) precision 36.7%, recall 45.2%; nearest precision 24.2%, recall 35.7% (Table 5). Human baseline (function selection): precision 87.5%, recall 84% (n=20). Human next-step accuracy 54.8% (n=32).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model (GPT-4 > GPT-3.5) — GPT-4 better at ordering and next-step selection', 'Shuffling of provided pseudofunctions reduces performance (models exploit original ordering when present)', 'Feedback/error-loop (automatic debugging) can improve performance for GPT models', 'Quality/detail of protocol description: GPT-4–generated descriptions improved downstream performance over original descriptions', 'Distractor functions from nearest-neighbour protocols (vs random) reduce retrieval precision (semantic similarity increases ambiguity)', 'Ambiguity/synonymy of functions (e.g., Mix vs MixSubstance) negatively affects retrieval scoring', 'Evaluator bias: GPT-4 as an evaluator can prefer longer/more coherent outputs irrespective of correctness']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Models accessed via OpenAI API; nearest-neighbour retrieval used text-embedding-ada-002 embeddings; reported scores are mean ± std over 5 runs (where applicable); contexts experimented with: functions provided ordered or shuffled; feedback loop toggled (on/off); use of machine-generated descriptions vs original descriptions; exact generation hyperparameters (temperature, top-k) not reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Imperfect ordering/planning (Levenshtein distance gaps remain); function retrieval performance is poor and sensitive to near-neighbour distractors and naming differences; GPT-4 used as an evaluator performed only slightly above chance in preferring ground truth; models sometimes omit units or protocol-critical details in generated pseudocode (manual editing required for some protocols); real-world risk and biosafety concerns limit dataset scope.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7334.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7334.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3.5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI instruction-tuned LLM (GPT-3.5 family) evaluated as a student model for protocol planning tasks (next-step prediction, full protocol generation, function retrieval) and compared against GPT-4 and Llama-2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>instruction-tuned generalist LLM (OpenAI API)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (laboratory protocol planning; molecular/microbiology)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Same suite as GPT-4: convert descriptions to pseudocode, next-step prediction, full protocol generation from admissible pseudofunctions, and function retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Student prompts given admissible pseudofunction lists and title/description; one-shot pseudocode examples used for teacher generation; experiments included shuffled vs ordered functions and feedback/error-loop toggles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Function-level accuracy (exact match %), precision & recall of function calls, normalized Levenshtein distance for order, BLEU for argument values, SciBERTscore for argument similarity, precision/recall for function retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Protocol generation (admissible functions provided, random shuffling): precision 36.7%, recall 45.2% (GPT-3.5, Table 5). Protocol generation (nearest-neighbour distractors): precision 24.2%, recall 35.7% (Table 5). The paper reports GPT-3.5 performs worse than GPT-4 on next-step prediction and ordering but somewhat better at predicting function argument values (arguments BLEU/SciBERTscore).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared to GPT-4 which outperformed GPT-3.5 in next-step and ordering; human baselines: function selection precision 87.5%, recall 84%; human next-step accuracy 54.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Model capacity/variant (GPT-3.5 performs worse than GPT-4 on ordering and step prediction)', 'Shuffling input functions reduces performance (models exploit ordering cues)', 'Feedback/error-loop helps GPT models but GPT-3.5 shows different strengths (better at argument prediction)', 'Nearest-neighbour distractor functions harm retrieval performance']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>OpenAI API used; same experimental settings as GPT-4 (ordered vs shuffled functions, feedback on/off, 5-run means reported); exact sampling hyperparameters not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Worse at ordering and next-step selection than GPT-4; still produces missing units or occasional omissions requiring manual edits; susceptible to distractor functions; overall lower precision/recall than GPT-4 on protocol-level tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7334.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7334.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2 (7B parameter model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source foundation language model (Llama-2 7B) evaluated in the paper as a baseline open-weight model on protocol generation and function retrieval tasks; found to significantly underperform GPT family models on function selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>open-source foundation model (base/foundation LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology (laboratory protocol planning)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Full protocol generation and function retrieval from admissible pseudofunctions, under the same BIOPROT tasks (next-step prediction attempted but model often failed to produce pseudocode lines).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same student-style prompts with admissible pseudofunctions and title/description; experiments included feedback/error-loop toggles and shuffled functions; one-shot example prompts used for teacher-ground truth generation (teacher was GPT-4), but Llama-2 served as the student.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Function-level accuracy metrics: precision & recall on protocol generation, normalized Levenshtein distance for order, BLEU/SciBERTscore for arguments, precision/recall for function retrieval. Results reported in Tables 9 and 10.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Protocol generation (nearest): precision 26.1%, recall 57.5%; (random): precision 28.1%, recall 56.3% (Table 9). On next-step prediction the model was typically unable to complete the task (produced intent statements rather than pseudocode lines).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Compared against GPT-3.5 and GPT-4; Llama-2-7B substantially underperformed GPT models on function selection and failed next-step-generation behaviorally.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Iterative feedback that helps GPT models was distracting or unhelpful for Llama-2 (feedback loop ineffective)', 'Differences in training regime produce behavioral differences (Llama-2 often outputs intent rather than code)', 'Lower capacity or differing alignment/fine-tuning leads to poorer performance on these code-like/planning tasks']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Evaluated using the same BIOPROT prompts and feedback loop infrastructure; when feedback was enabled Llama-2 tended to be distracted and not re-write code; next-step prediction task generally failed for this model; exact sampling/temperature/hyperparameters not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Could not reliably produce next-step pseudocode (produces descriptive intent instead); iterative feedback loops that improve GPT models do not translate to Llama-2 behavior; overall lower function-selection precision and different failure modes linked to training/fine-tuning differences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemcrow: Augmenting large-language models with chemistry tools. <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>LLM+P: Empowering large language models with optimal planning proficiency <em>(Rating: 1)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7334",
    "paper_id": "paper-264172681",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "An instruction-tuned, generalist large language model from OpenAI used in this work both to generate protocol-specific pseudofunctions (teacher) and to generate/plan protocols (student); also used as a component in a retrieval-augmented, chain-of-thought agent for end-to-end protocol creation and lab validation.",
            "citation_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_type": "instruction-tuned generalist LLM (OpenAI API); used as both teacher and student; also used with tool-use / retrieval and chain-of-thought agent",
            "scientific_domain": "biology (laboratory protocol planning; molecular/microbiology)",
            "simulation_task_description": "Text-based simulation of laboratory protocol planning: (a) convert free-text protocols into protocol-specific pseudofunctions and step-by-step pseudocode; (b) next-step prediction (choose the next pseudofunction and its arguments); (c) full protocol generation from title/description plus admissible pseudofunctions; (d) function retrieval (select which pseudofunctions from a pool are required).",
            "prompting_strategy": "One-shot example prompts for pseudocode/pseudofunction generation (teacher); student prompts that provide admissible pseudofunctions plus title/description to reconstruct pseudocode; settings included shuffled vs ordered function lists; automatic feedback/error loop to detect Python/pseudocode errors and missing argument units; retrieval-augmented Toolformer-like chain-of-thought agent (LangChain) to search BIOPROT, extract functions, and assemble new protocols.",
            "evaluation_metric": "Function-level accuracy (exact match %), precision & recall of function calls (counts include repeats), normalized Levenshtein distance (L_dn) for sequence ordering, BLEU for argument value accuracy, SciBERTscore (average cosine similarity on SciBERT embeddings) for argument values, precision/recall for function retrieval, GPT-4 preference rate when used as an evaluator.",
            "reported_accuracy": "Protocol generation (admissible functions provided, random shuffling): precision 48.8%, recall 49.4% (GPT-4, Table 5). Protocol generation (nearest-neighbour distractors): precision 32.5%, recall 39.2% (Table 5). GPT-4 used as an evaluator preferred model outputs over ground truth at rates ~40.9–43.9% depending on shuffle/feedback (Table 8). Real-world validation: GPT-4 generated two novel protocols (reviewed by a scientist); one (E. coli overnight culture + glycerol stock) was executed successfully in the lab.",
            "baseline_accuracy": "Compared primarily to GPT-3.5 (see separate entry): GPT-3.5 protocol generation (random) precision 36.7%, recall 45.2%; nearest precision 24.2%, recall 35.7% (Table 5). Human baseline (function selection): precision 87.5%, recall 84% (n=20). Human next-step accuracy 54.8% (n=32).",
            "factors_reported": [
                "Model (GPT-4 &gt; GPT-3.5) — GPT-4 better at ordering and next-step selection",
                "Shuffling of provided pseudofunctions reduces performance (models exploit original ordering when present)",
                "Feedback/error-loop (automatic debugging) can improve performance for GPT models",
                "Quality/detail of protocol description: GPT-4–generated descriptions improved downstream performance over original descriptions",
                "Distractor functions from nearest-neighbour protocols (vs random) reduce retrieval precision (semantic similarity increases ambiguity)",
                "Ambiguity/synonymy of functions (e.g., Mix vs MixSubstance) negatively affects retrieval scoring",
                "Evaluator bias: GPT-4 as an evaluator can prefer longer/more coherent outputs irrespective of correctness"
            ],
            "experimental_conditions": "Models accessed via OpenAI API; nearest-neighbour retrieval used text-embedding-ada-002 embeddings; reported scores are mean ± std over 5 runs (where applicable); contexts experimented with: functions provided ordered or shuffled; feedback loop toggled (on/off); use of machine-generated descriptions vs original descriptions; exact generation hyperparameters (temperature, top-k) not reported in paper.",
            "limitations_or_failure_modes": "Imperfect ordering/planning (Levenshtein distance gaps remain); function retrieval performance is poor and sensitive to near-neighbour distractors and naming differences; GPT-4 used as an evaluator performed only slightly above chance in preferring ground truth; models sometimes omit units or protocol-critical details in generated pseudocode (manual editing required for some protocols); real-world risk and biosafety concerns limit dataset scope.",
            "uuid": "e7334.0",
            "source_info": {
                "paper_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "Generative Pre-trained Transformer 3.5",
            "brief_description": "An OpenAI instruction-tuned LLM (GPT-3.5 family) evaluated as a student model for protocol planning tasks (next-step prediction, full protocol generation, function retrieval) and compared against GPT-4 and Llama-2.",
            "citation_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_size": null,
            "model_type": "instruction-tuned generalist LLM (OpenAI API)",
            "scientific_domain": "biology (laboratory protocol planning; molecular/microbiology)",
            "simulation_task_description": "Same suite as GPT-4: convert descriptions to pseudocode, next-step prediction, full protocol generation from admissible pseudofunctions, and function retrieval.",
            "prompting_strategy": "Student prompts given admissible pseudofunction lists and title/description; one-shot pseudocode examples used for teacher generation; experiments included shuffled vs ordered functions and feedback/error-loop toggles.",
            "evaluation_metric": "Function-level accuracy (exact match %), precision & recall of function calls, normalized Levenshtein distance for order, BLEU for argument values, SciBERTscore for argument similarity, precision/recall for function retrieval.",
            "reported_accuracy": "Protocol generation (admissible functions provided, random shuffling): precision 36.7%, recall 45.2% (GPT-3.5, Table 5). Protocol generation (nearest-neighbour distractors): precision 24.2%, recall 35.7% (Table 5). The paper reports GPT-3.5 performs worse than GPT-4 on next-step prediction and ordering but somewhat better at predicting function argument values (arguments BLEU/SciBERTscore).",
            "baseline_accuracy": "Compared to GPT-4 which outperformed GPT-3.5 in next-step and ordering; human baselines: function selection precision 87.5%, recall 84%; human next-step accuracy 54.8%.",
            "factors_reported": [
                "Model capacity/variant (GPT-3.5 performs worse than GPT-4 on ordering and step prediction)",
                "Shuffling input functions reduces performance (models exploit ordering cues)",
                "Feedback/error-loop helps GPT models but GPT-3.5 shows different strengths (better at argument prediction)",
                "Nearest-neighbour distractor functions harm retrieval performance"
            ],
            "experimental_conditions": "OpenAI API used; same experimental settings as GPT-4 (ordered vs shuffled functions, feedback on/off, 5-run means reported); exact sampling hyperparameters not specified in paper.",
            "limitations_or_failure_modes": "Worse at ordering and next-step selection than GPT-4; still produces missing units or occasional omissions requiring manual edits; susceptible to distractor functions; overall lower precision/recall than GPT-4 on protocol-level tasks.",
            "uuid": "e7334.1",
            "source_info": {
                "paper_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama 2 (7B parameter model)",
            "brief_description": "An open-source foundation language model (Llama-2 7B) evaluated in the paper as a baseline open-weight model on protocol generation and function retrieval tasks; found to significantly underperform GPT family models on function selection.",
            "citation_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B",
            "model_size": "7B",
            "model_type": "open-source foundation model (base/foundation LLM)",
            "scientific_domain": "biology (laboratory protocol planning)",
            "simulation_task_description": "Full protocol generation and function retrieval from admissible pseudofunctions, under the same BIOPROT tasks (next-step prediction attempted but model often failed to produce pseudocode lines).",
            "prompting_strategy": "Same student-style prompts with admissible pseudofunctions and title/description; experiments included feedback/error-loop toggles and shuffled functions; one-shot example prompts used for teacher-ground truth generation (teacher was GPT-4), but Llama-2 served as the student.",
            "evaluation_metric": "Function-level accuracy metrics: precision & recall on protocol generation, normalized Levenshtein distance for order, BLEU/SciBERTscore for arguments, precision/recall for function retrieval. Results reported in Tables 9 and 10.",
            "reported_accuracy": "Protocol generation (nearest): precision 26.1%, recall 57.5%; (random): precision 28.1%, recall 56.3% (Table 9). On next-step prediction the model was typically unable to complete the task (produced intent statements rather than pseudocode lines).",
            "baseline_accuracy": "Compared against GPT-3.5 and GPT-4; Llama-2-7B substantially underperformed GPT models on function selection and failed next-step-generation behaviorally.",
            "factors_reported": [
                "Iterative feedback that helps GPT models was distracting or unhelpful for Llama-2 (feedback loop ineffective)",
                "Differences in training regime produce behavioral differences (Llama-2 often outputs intent rather than code)",
                "Lower capacity or differing alignment/fine-tuning leads to poorer performance on these code-like/planning tasks"
            ],
            "experimental_conditions": "Evaluated using the same BIOPROT prompts and feedback loop infrastructure; when feedback was enabled Llama-2 tended to be distracted and not re-write code; next-step prediction task generally failed for this model; exact sampling/temperature/hyperparameters not specified.",
            "limitations_or_failure_modes": "Could not reliably produce next-step pseudocode (produces descriptive intent instead); iterative feedback loops that improve GPT models do not translate to Llama-2 behavior; overall lower function-selection precision and different failure modes linked to training/fine-tuning differences.",
            "uuid": "e7334.2",
            "source_info": {
                "paper_title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_autonomous_scientific_research_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2,
            "sanitized_title": "do_as_i_can_not_as_i_say_grounding_language_in_robotic_affordances"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "LLM+P: Empowering large language models with optimal planning proficiency",
            "rating": 1,
            "sanitized_title": "llmp_empowering_large_language_models_with_optimal_planning_proficiency"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        }
    ],
    "cost": 0.015056249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</p>
<p>Odhran O&apos;donoghue 
Align to Innovate</p>
<p>Francis Crick Institute</p>
<p>University of Oxford</p>
<p>Aleksandar Shtedritski 
Align to Innovate</p>
<p>Francis Crick Institute</p>
<p>University of Oxford</p>
<p>John Ginger 
Align to Innovate</p>
<p>Francis Crick Institute</p>
<p>Ralph Abboud 
Align to Innovate</p>
<p>Francis Crick Institute</p>
<p>Ali Essa Ghareeb 
Francis Crick Institute</p>
<p>Justin Booth 
Align to Innovate</p>
<p>Francis Crick Institute</p>
<p>Samuel G Rodriques 
Francis Crick Institute</p>
<p>Future House</p>
<p>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology
This paper is accepted for publication at EMNLP 2023. The official version of record is in the ACL Anthology.
The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and longterm planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BIOPROT 1 : a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM's ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model planning abilities in other areas of science or other areas that lack automatic evaluation. . 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):1-23.</p>
<p>Introduction</p>
<p>Traditional manual methods for research in biology are time-consuming, labour-intensive, and highly prone to human error. Robotic laboratory automation has the potential to increase accuracy, repro-1 The dataset and code for evaluation are available at https://github.com/bioplanner/bioplanner evaluate Title Description</p>
<p>Step-by-step Instructions</p>
<p>Pseudofunctions</p>
<p>Step-by-step pseudocode Title Description</p>
<p>Pseudofunctions</p>
<p>Step-by-step pseudocode copy Figure 1: Automatic evaluation of protocol generation. The teacher model is given full information about a scientific experiment protocol -title, description, and step-by-step instructions. It is prompted to generate pseudo functions that allow the execution of the protocol. The student model is given the admissible pesudofunctions and is evaluated on its ability to generate the step-by-step pseudocode.</p>
<p>ducibility, and scalability, contributing to more scientific breakthroughs and a faster transition from research to real-world applications. One important step towards automation of biology research is the automated generation of a laboratory protocol (Accurate step-by-step instructions on how to complete an experiment to accomplish a specific goal) which can subsequently be converted into robot code. LLMs have significant latent scientific knowledge and thus may be able to formulate accurate scientific protocols, which has been demonstrated for the field of chemistry (Bran et al., 2023;Boiko et al., 2023). However, to-date there has not been any clear way to evaluate the accuracy of a generated scientific protocol, except by manual evaluation. Without established evaluation metrics, progress in the field of automating science remains challenging.</p>
<p>Evaluating laboratory protocols is difficult for two reasons. Firstly, protocols are very sensitive to tiny details, and slight variations in instructions can lead to significantly different outcomes. When comparing generated protocols against ground truths, metrics that rely on n-gram overlaps such as BLEU (Papineni et al., 2002) or contextual embeddings such as BERTScore (Zhang et al., 2019) might not capture small differences, such as the order of actions, or relation between substances (Bhandari et al., 2020). Secondly, the same protocol can be described correctly at various levels of granularity. The same technique (e.g. sequencing library preparation) can be described by a single line or multiple paragraphs. This variability in granularity makes it difficult to evaluate the accuracy of LLM-generated protocols.</p>
<p>We here present an automated approach to evaluating the ability of a language model to write biological protocols. Our approach is inspired by robotic planning, in which a closed set of admissible actions is provided to a controller agent (Jiménez et al., 2019;Ahn et al., 2022;Huang et al., 2022). We use GPT-4 to automatically convert a written protocol into pseudocode using a protocolspecific set of pseudofunctions that is generated by the model (see Figure 1). Here, a "teacher" model generates the admissible action set and correct answer in terms of step-by-step pseudocode. Having access to this privileged information, we can then evaluate the performance of a "student", that has to solve the task from scratch. In this way, we then evaluate the ability of language models to generate a protocol when presented only with the appropriate pseudocode functions and a short description of the protocol. In effect, our approach allows us to automatically convert the process of writing a scientific protocol into a series of multiple-choice questions (i.e., pick a pseudofunction from a provided set), which can be evaluated much more robustly than natural language generation. This paradigm allows us to rapidly measure the protocol knowledge of GPT-3.5 and GPT-4 with minimal human intervention, and can serve as a general approach for evaluating and improving long-horizon planning in open-ended tasks in the future.</p>
<p>To this end, we also introduce a novel dataset, BIOPROT, of publicly available biology laboratory protocols, containing instructions in both free text and protocol-specific pseudocode. The dataset has been reviewed by domain experts and allows evaluation of model performance several different tasks, such as next-step prediction, or full protocol generation. We further show the utility of this dataset by automatically designing and successfully executing a lab experiment using GPT-4 and the action space defined using BIOPROT.</p>
<p>In summary, we make the following contributions: (i) We propose evaluating protocol generation on pseudocode rather than free text instructions; (ii) We introduce the BIOPROT dataset, a manually audited dataset of open-access biology protocols; (iii) We evaluate the ability of GPT-4 to accurately convert natural language protocols into pseudocode; (iv) We define a suite of tasks and metrics for evaluating protocol generation; (v) We evaluate several LLMs on our tasks to provide objective measures of these models' ability to generate biological experiments; (vi) We automatically generate a biology experiment and successfully execute it in a lab.</p>
<p>Related Works</p>
<p>LLMs for Natural Sciences Using LLM for scientific tasks such as entity extraction in biological documents (Tamari et al., 2021) or retrieval of chemical reaction procedures (Bai et al., 2022) is a natural use case of such models. Work such as SciBERT, BioGPT, Galactica and others have also shown the utility of pretraining an LLM on a corpus of biomedical (Gu et al., 2021;Lewis et al., 2020;Luo et al., 2022;Lee et al., 2020;Shin et al., 2020) or general scientific text (Beltagy et al., 2019;Taylor et al., 2022). More recently, pre-trained generalist LLMs such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023) have shown to be capable of tasks such as searching for chemical compounds similar to a given one (OpenAI, 2023) or drug editing (Liu et al., 2023c). Furthermore, a GPT-4 agent augmented with tools has been shown to be capable of synthesis planning and drug discovery (Bran et al., 2023) or planning reactions and executing them on a robotic platform (Boiko et al., 2023).</p>
<p>Task Decomposition LLMs trained on nexttoken prediction can struggle with more complex logical reasoning in naïve setups (Liu et al., 2023b). However, decomposing complex tasks into subtasks in frameworks such as Chain-of-Thought reasoning (Wei et al., 2022;, and its variants such as Least-to-Most (Zhou et al., 2022) and Tree of Thought reasoning  improves performance in multi-step reasoning problems. In addition to test-time improve-ments, LLMs also improve in performance when trained on step-by-step reasoning data generated by larger LLMs (Mukherjee et al., 2023;. Task decomposition has also been combined with self-verification through deductive reasoning to improve step-by-step reasoning accuracy (Ling et al., 2023). Here, we approach task decomposition from another angle -we first ask the model to define a discrete set of actions needed to complete a task, and then how to compose them.</p>
<p>Planning Closely related to task decomposition is planning. LLMs have been successful at planning in simulated and real embodied space, both through the use of restricted action space (Ahn et al., 2022;Driess et al., 2023), function/tool search (Wang et al., 2023a;Schick et al., 2023;Shen et al., 2023;Bran et al., 2023;Boiko et al., 2023) and translation of plans into admissible action space (Huang et al., 2022). Planning models have been explicitly combined with Chain-of-Thought reasoning for performance improvement Shen et al., 2023). LLM planners can also learn to create their own training curriculum and refine their function use (Wang et al., 2023a). LLM-based planning and reasoning can benefit from writing problems in a machinereadable language such as Planning Domain Definition Language (PDDL) and symbolic logic (Pan et al., 2023;Silver et al., 2023). Furthermore, interactions with simulators and debuggers can be used to improve both plans (Liu et al., 2023a) and value functions that determine the appropriateness of action calls (Ahn et al., 2022;Driess et al., 2023;. Our work extends recent work in planning through the automated generation of admissible action spaces and consequent evaluation without the need for a simulation environment.</p>
<p>Evaluating LLM Scientists Evaluating LLMs on scientific tasks is limited to QA benchmarks for measuring general science knowledge (Hendrycks et al., 2020), or specialist knowledge such as chemistry (Guo et al., 2023;Wu et al., 2017), biomedical science (Sung et al., 2021) or medicine (Jin et al., 2019(Jin et al., , 2021. However, evaluating an LLM's performance on more open-ended tasks, such as healthcare support (Dash et al., 2023) or chemical synthesis planning (Bran et al., 2023) is done manually. To the best of our knowledge, we are the first to approach automatic evaluation of LLMs on open-ended problems in science.</p>
<p>Automatic Evaluation of LLMs While evaluation of the performance of an LLM in games (Wang et al., 2023a) or planning in PDDL domains (Silver et al., 2023) can be done automatically, many works rely on self-evaluation, where GPT-4 is used as an evaluator (Bubeck et al., 2023;Bran et al., 2023;Chiang et al., 2023;Peng et al., 2023;Zhou et al., 2023). However, these have been found to contradict human evaluation (Bran et al., 2023) or be systematically biased (Wang et al., 2023b), where the order of the provided responses affects the predicted ranking. In comparison to these works, we use an LLM to generate pseudo-ground truth data on an easy task, in which the model consistently performs well at, which we use to evaluate on a more difficult task with real-world implications.</p>
<p>The BIOPROT dataset</p>
<p>Here we describe the BIOPROT dataset -a collection of publicly available protocols that are used to evaluate the performance of LLMs on protocol generation on a large range of topics in biology. We discuss the contents of the dataset (Section 3.1), creating a set of admissible actions and translating the protocol steps (Section 3.2), manual verification of the data (Section 3.3), and the tasks that can be approached with it (Section 4). The dataset can be found in the Supplementary Materials. This approach can be used to generate pseudocode datasets in any domain that has step-by-step instructional data.</p>
<p>A Dataset of Protocols for Biology</p>
<p>We collect publicly available protocols from Protocols.io (Teytelman et al., 2016), a platform for developing and sharing reproducible methods. This database contains over 9,000 public protocols of different scientific areas and complexity. Each protocol consists of (i) a title, (ii) a description, and (iii) step-by-step instructions. We automatically and manually filter the protocols, in order to obtain a set of protocols that are related to biology, can be reproduced, and are of sufficient difficulty. For further details about the filtering, refer to the Supplementary Materials. In Table 1 we present a summary of the collected protocols.</p>
<p>Translating Protocols to Pseudocode</p>
<p>As discussed in Section 1, evaluation of planning problems is difficult in natural text, and prior works opt for manual evaluation (Bran et al., 2023; Boiko Title: Hornwort DNA extraction Description: The gametophytic tissue of hornworts is rich in polysaccharides (Renzaglia et al., 2009) and it also seems to be rich in polyphenolics. Both compounds pose a problem for DNA ...</p>
<p>Protocol:</p>
<ol>
<li>Grind 0.5-2 g of tissue using mortar and pestle in the presence of liquid nitrogen... 2. Add 10 ml of 60 oC extraction buffer and 100 mg PVP-40/g tissue (5μl of RNAse A (100mg/ml))... 3. ... grind_tissue(tissue_weight="0.5-2 g", grinding_method="mortar and pestle with liquid nitrogen") transfer_tissue(tube_volume="30 ml") add_extraction_buffer(buffer_volume="10 ml", buffer_temperature="60 °C", pvp_weight="100 mg/g tissue", rnase_volume="5μl") add_and_mix(solvent="12 ml of chloroform:IAA (24:1)", mixing_method="inversion") ...</li>
</ol>
<p>Protocol</p>
<p>Generated Pseudocode</p>
<p>Your goal is to convert molecular biology protocols into python pseudocode.</p>
<p>Here is an example of ...</p>
<p>Prompt grind_tissue(tissue_weight="0.5-2 g", grinding_method="mortar and pestle with liquid nitrogen") transfer_tissue(tube_volume="30 ml") add_extraction_buffer(buffer_volume="10 ml", buffer_temperature="60 °C", pvp_weight="100 mg/g tissue", rnase_volume="5μl") add_and_mix(solvent="12 ml of chloroform:IAA (24:1)", mixing_method="inversion") ...</p>
<p>Pseudocode instructions</p>
<p>Figure 2: Creation of pseudofunction and pseudocode data The model is prompted to generate pseudofunctions and pseudocode based on a target protocol. This generated code is automatically debugged using a feedback error loop, and then manually reviewed. Generated pseudofunctions are used to define the admissible action space in downstream evaluation tasks, and pseudocode instructions using the pseudofunction calls are used as ground truth to measure the accuracy of generated code in downstream tasks, enabling automatic evaluation.</p>
<p>Statistic Value</p>
<p>Protocols 100 Average number of steps 12.5 Average total protocol length in tokens 641.0 Average tokens per step 52.6 Average tokens per original description 83.8 Average tokens per generated description 66.3 Table 1: Dataset Statistics We present aggregate statistics for the BIOPROT dataset. The "generated descriptions" are generated using GPT-4 from the step-by-step instructions, as discussed in Section 5.4. et al., 2023). To this end, we "translate" the free text protocols into pseudocode using GPT-4 (see Figure 2). We task GPT-4 to (i) define a set of pseudofunctions that suffice to execute the protocol, and (ii) convert the protocol steps into pseudocode using only the provided pseudofunctions.</p>
<p>We make use of a one-shot example prompt, and an automatic feedback loop (Liu et al., 2023a) that provides error signals if: the generated code is not valid Python pseudocode; no pseudofunctions are defined; the pseudocode or pseudofunctions do not have arguments; any numerical parameters in the pseudocode do not have units. Finally, GPT-4 is prompted to check for errors or omissions in the pseudofunctions and pseudocode. Information about our generated pseudocode is summarized in Table 2.  </p>
<p>Manual Verification</p>
<p>We manually reviewed the generated pseudofunctions and pseudocode for accuracy. Original protocols and generated ground-truth pseudocode were assessed line-by-line by a competent laboratory scientist. They confirmed (i) whether the original natural language protocol made sense, (ii) whether the title and description sufficiently described the protocol so that a competent scientist could attempt to complete it without the protocol, and (iii) whether the pseudocode was accurate. Finally, edits were made to the generated pseudocode as necessary.</p>
<p>We show a breakdown of the edits made in Table 3.</p>
<p>Statistic Value % generated protocols requiring no edits 59 % generated protocols with 1 ≤ 3 edited lines 24 % generated protocols with &gt; 3 edited lines 17 average number of line edits in edited files 11.8 Table 3: Manual Verification We provide a breakdown of the protocols that required manual edits.</p>
<p>Overall, 59 of the 100 protocols were found to be completely accurate requiring no edits. However, many protocols that did require edits only required minor edits. The most common errors found were missing units for numbers, which in most cases would not prevent a competent scientist from completing a protocol. The more impactful errors found were most commonly (1) missing details which would allow one to successfully complete a step of the protocol (these were usually highly verbose steps which explained a detailed technical method for manipulating a sample) and (2) not explaining the composition of a material used in the protocol (e.g. a buffer).</p>
<p>The corrected protocols are made available as the BIOPROT dataset. Even without human editing, LLMs with error-checking loops can be used to create a largely accurate dataset for biology protocol pseudocode, thus enabling self-evaluation.</p>
<p>Machine-generated Descriptions</p>
<p>For some of our downstream tasks, it is necessary to have high-quality descriptions of protocols that give a sense of what the protocol steps should include. However, protocol descriptions in Protocols.io are not always suitable for this purpose. To this end, we also generated descriptions of protocols that provided a high-level overview of the protocols' objective (the prompt for this is seen in the Supplementary Materials). We include both our machine-generated descriptions and the original descriptions in our dataset.</p>
<p>Metrics and evaluation</p>
<p>Using the BIOPROT dataset, we evaluate an LLM's capabilities to reason about and generate scientific protocols on several different tasks.</p>
<p>Next</p>
<p>Step Prediction Given a protocol title, description, an admissible set of pseudofunctions, and partially completed pseudocode, we evaluate the model's ability to correctly identify the pseudofunction corresponding to the next step in the protocol. We evaluate the correctness of both the predicted function and the function arguments.</p>
<p>For function-level accuracy, we report the percentage of the number of correct function assignments
accuracy = 1 N N n=1 1[f pred i = f GT i ],
where f pred and f GT are the predicted and groundtruth functions, respectively, and N is the number of steps in the protocol.</p>
<p>During generation, the model is prompted to name each function argument and provide the argument parameters. To evaluate accuracy of the arguments, we first check whether the function argument names is correct. For that purpose, we compute precision and recall of the arguments' names. For correct function arguments, we consider the accuracy of the argument value using the BLEU metric (Papineni et al., 2002). Additionally, we encode the predicted and ground truth argument values, a pred i and a GT i , respectively, with SciBERT (Beltagy et al., 2019) sentence encoder E to get the SciBERTscore:
SciBERTscore = 1 N N i=0 ⟨E(a pred i ), E(a GT i )⟩ ∥E(a pred i )∥∥E(a GT i )∥ ,
which is the average cosine similarity between predicted and ground truth argument values for all N steps. This metric is inspired by BERTScore (Zhang et al., 2019), but we use a SciB-ERT encoder as it is better suited to the scientific domain. We only compute argument-level metrics for correctly predicted functions, as not to penalize the model twice for wrong function predictions.</p>
<p>Protocol Generation Given a protocol title, description, and an admissible set of pseudofunctions, the model is tasked to generate corresponding pseudocode. We again evaluate the correctness of predicted functions and their corresponding arguments. This is a more difficult task than the previous one, as the model needs to plan the entire execution of the protocol. For function-level evaluation, we need to measure (i) if the correct functions were called, and (ii) if they were used in the correct order.</p>
<p>For the former, we report precision and recall of function calls, where we take into account repeated calls of the same function. For evaluating whether the functions are used in the correct order, we use the Levenshtein distance L d between the predicted and ground-truth sequence of functions. The Levenshtein distance is originally a string edit distance that measures the number of insertions, deletions, or substitutions to make one word into another. We consider each function call as a separate symbol, which incurs a cost of 1 for being added, deleted, or substituted. We report a normalized Levenshtein  Step Prediction Evaluation Given a protocol title and description, the admissible pseudofunctions and partially completed pseudocode, we evaluate the model's ability to correctly predict the next step. For all metrics, higher is better. We report mean and standard deviation over 5 runs.</p>
<p>distance L dn
L dn = L d N ,
where N is the number of functions in the groundtruth pseudocode.</p>
<p>In addition, we evaluate the predicted function arguments. We use the same metrics as described under "Next Step Prediction".</p>
<p>Function Retrieval Our approach has the potential to allow novel protocols to be assembled from steps provided in existing protocols in the dataset, if the model is able to correctly identify which steps are needed for any given protocol. Thus, given a protocol title and description, and a set of pseudofunctions, we evaluate the models' ability to correctly identify which of the provided functions are needed to execute the protocol. In this task, we provide the model with a set of pseudofunctions consisting of the ground-truth pseudofunctions for the given protocol, and pseudofunctions drawn from several (i) random or (ii) nearest neighbour protocols. Providing functions from nearest neighbour protocols is more difficult, as they are likely to be more similar to the correct functions. We measure the precision and recall of retrieved functions.</p>
<p>Experiments</p>
<p>Implementation details</p>
<p>We explore the performance of GPT-3.5 and GPT-4 from the OpenAI API. Where we find nearest neighbors, we use an embedding index of all protocols' descriptions using text-embedding-ada-002 embeddings, unless stated otherwise. We show the prompts we use in the Supplementary Material.</p>
<p>For each of the tasks listed in Section 4, we evaluate the models in several settings:</p>
<p>• Shuffled: the model can be provided either with functions in the order in which they are generated, or randomly shuffled. The functions tend to be defined in the order they appear in the original protocol, and that serves as a signal to the model we evaluate. By randomly shuffling the input functions, we make the task more difficult.</p>
<p>• Feedback: The model has access to an error loop that can detect undefined functions and Python syntax errors. Such feedback loops have been found to be beneficial in PDDL planning (Silver et al., 2023) and reasoning (Madaan et al., 2023).</p>
<p>Results</p>
<p>Next step prediction We show results on next step prediction in Table 4. We see that GPT-4 consistently outperforms GPT-3.5 in both the prediction of the correct next step, whereas GPT-3.5 performs better at predicting function arguments. We note there is a drop in performance when the input functions are shuffled, likely because if not shuffled, the functions appear roughly in the order as they should be called as they were sequentially generated by the LLM.</p>
<p>Protocol generation We show results on full protocol generation in Table 5. We observe the biggest gap in the Levenshtein distance score metric, where GPT-4 significantly outperforms GPT-3.5. Meanwhile, GPT-4 and GPT-3.5 show similar precision and recall of used functions. This suggests that while both have a similar ability to use the correct functions, GPT-4 performs better at using the right order. We also observe that shuffling the input functions consistently leads to a drop in performance.</p>
<p>Function retrieval We show retrieval results in Table 6. We see that GPT-4 outperforms GPT-3.5 on this task. However, the results on this task appear generally poor. One possible reason for the   poor performance is that the correct answer may sometimes be ambiguous. For example, Mix and MixSubstance are semantically identical, but have different syntax, and the model would be penalized for selecting a function not from the query protocol. This effect would explain why performance using the""nearest" neighbours is worse than performance when using "random" protocols.</p>
<p>Using GPT-4 as an evaluator</p>
<p>We use GPT-4 as an evaluator, where given (i) a protocol description, (ii) admissible pseudofunctions, (iii) ground-truth pseudocode (generated as described in Section 3.2), and (iv) predicted pseudocode, the model is prompted to predict which one of (iii) or (iv) better matches the protocol description (i). We report the rate at which the predicted pseudocode was preferred in Table 8. In general, GPT-4 only performs slightly above chance in identifying the ground truth protocol, versus LLM generations, although it is unclear whether this is because the machine-generated protocols are largely correct, or because GPT-4 is unable to distinguish correct from incorrect protocols. Note that prior works (Bran et al., 2023) found that a GPT evaluator tends to prefer longer and more coherent, but not necessarily more correct generations.</p>
<p>Using GPT-4-Generated Descriptions</p>
<p>For some protocols, we observe that the detail present in the protocol description does not suffice to enable protocol reconstruction. To this end, we use GPT-4 to generate a short pseudo description given the protocol steps in natural text. We present results on next step generation and full protocol generation in Figure 8. We see a small increase in performance, which is expected, as the summarygenerating model can include more detail (however, the pseudo descriptions are shorter -see Table 2).</p>
<p>Real-World Validation</p>
<p>Finally, to validate that BIOPROT can be used to generate accurate novel protocols, we devised a setup for end-to-end protocol creation. To do this we opted to build an LLM agent with access to tools, such that it can retrieve protocols that contain relevant pseudofunctions, and use their pseudofunctions to generate new pseudocode. Note that for good performance in this real-world validation task, the LLM needs to be able to (1) find relevant psueodofunctions from other protocols, and (2) generate correct pseudocode, both of which are tasks we build metrics for. Details are as follows: we created a Toolformer-like (Schick et al., 2023) chain-of-thought LLM agent (Wei et al., 2022) with access to a tool for searching for protocols in the BIOPROT database. This agent used the GPT-4 LLM. We prompted the agent to retrieve protocols relevant to generating a new target protocol. We extracted the pseudofunctions from the retrieved protocols and then prompted the agent to generate a new protocol using only the retrieved pseudofunctions. We used this setup to create two experiments using GPT-4: (1) culturing a single colony of E.coli bacteria overnight and making a glycerol stock with  Table 7: Using GPT-4 -generated description We compare performance on next step prediction (top) and protocol generation (bottom) when using a protocol description generated by (i) scientists, or (ii) GPT-4. We see that using a GPT-4 generated description consistently outperforms the original one. The input pseudofunctions to the model are shuffled and we use a feedback loop.</p>
<p>Model</p>
<p>Shuffle Feedback GPT-4 score ↑ Table 8: GPT-4 as an evaluator. The GPT-4 score shows the rate at which GPT-4 predicted the model's output to be better than the ground truth.
GPT-3.5 ✗ ✗ 35.6 GPT-3.5 ✗ ✓ 40.2 GPT-3.5 ✓ ✗ 40.9 GPT-3.5 ✓ ✓ 39.3 GPT-4 ✗ ✗ 43.9 GPT-4 ✗ ✓ 42.4 GPT-4 ✓ ✗ 40.9 GPT-4 ✓ ✓ 42.4
the suspension (a form of cryopreservation for longterm storage), and (2) culturing Symbiodinum (a large genus of dinoflagellates endosymbiontic to cnidarians that may help corals survive in warming oceans), extracting its DNA, and then running the DNA on an agarose gel.</p>
<p>Real-World Validation Results</p>
<p>The model generated two new protocols using pseudofunctions from our database. Both of these protocols were reviewed by a scientist and were determined to be accurate and sufficient for a competent lab scientist to follow. We opted to complete the first protocol using E.coli as we did not have Symbiodinium available in the laboratory. We validated the first protocol by implementing it in the lab with the instructions and parameter values provided by the model. The protocol ran successfully: the cells remained viable after storage at -80°C, as evidenced by subsequent culture on nutrient agar (see Figure 3). The methods and prompts used to generate these experiments, as well as the agent chain-of-thought reasoning, can be found in the Appendix. Figure 3: E.coli growing on nutrient agar plates. We carried out a protocol for overnight culture and cryopreservation of E.coli in glycerol for long-term storage. One hour after completion of the protocol, the cells were thawed and spread onto the surface of nutrient agar. After 10 hours they can be seen growing on the surface of the agar plate (top) plate, while there is no growth on the control (no E.coli) plate (bottom). This shows the LLM-generated protocol was correct.</p>
<p>Conclusion</p>
<p>We have introduced a method for automatic evaluation of LLMs on open-ended planning problems, such as those found in experimental sciences, and a dataset of such planning problems in biology laboratory protocols. We then defined a suite of tasks and evaluation metrics that can be used to measure performance and help drive progress in the field. We evaluate GPT-3.5 and GPT-4 on these tasks and find that there is more to be desired in terms of performance. Finally, we show an application of our dataset and framework, where an LLM generates a protocol that is successfully executed in a laboratory. Additional scientific fields Our work is focused on biology, but could be extended to other fields such as chemistry and materials science. Future works should explore extending the dataset and framework.</p>
<p>Use of paid API</p>
<p>Misuse There is a risk of misuse, where adversaries could use our framework or dataset to inform the synthesis of harmful compounds. We have taken care to ensure the protocols in BIOPROT contain no protocols that can easily be used for such purposes. Continued research on aligning LLMs and restriction of dangerous outputs is important to minimize risk. We hope that our approach of using pseudofunctions may in the future allow for easier programmatic evaluation of outputs, and easier detection of the generation of hazardous substances.</p>
<p>BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology</p>
<p>Supplementary Material</p>
<p>A Dataset Filtering</p>
<p>We filter Protocols.io protocols such that (i) they can be parsed and provided to an LLM, and (ii) they are sufficiently challenging to serve as an evaluation set. We performed the following automatic and manual filtering:</p>
<p>Automatic filtering Protocols were automatically removed if they:</p>
<p>• Do not contain a description </p>
<p>• Consist of fewer than three steps (such protocols were insufficiently complex to demonstrate multi-step planning)</p>
<p>Manual filtering Following automatic filtering, protocols were manually removed if they:</p>
<p>• Were not relevant to biology</p>
<p>• Were considered poorly written to the extent that a human could not accurately replicate the protocol</p>
<p>B Prompts</p>
<p>B.1 Main experiments</p>
<p>We show the prompts we use for generating pseudofunctions and pseudocode in Figure 6, predicting pseudocode in Figure 7, summarising protocol steps in Figure 8. We show the error messages we use in the feedback loops in Figure 9. Figure 12, and the resulting generated protocol used in our real-world experiment is found in Figure 13 </p>
<p>B.2 Lab experiments</p>
<p>Here we show the prompts we use in Section 5 of the paper. Figure 10 is the prompt provided to the CoT agent. Figure 11 shows the Langchain output form the agent. Figure 12 shows the prompt that contains the retrieved pseudofunctions. Finally, Figure 13 shows the pseudocode that was given to a biologist to execute in a laboratory.</p>
<p>C Qualitative Evaluation</p>
<p>We show qualitative results for protocol id 145 from BIOPROT. For further qualitative examples, please refer to the BIOPROT dataset.</p>
<p>Title Ethanol precipitation of nucleic acids (Eppendorf tubes)</p>
<p>Description Nucleic acid precipitation is used to concentrate and/or purify nucleic acids. The below protocol is based on the fact that nucleic acids are less soluble in alcohol than in more polar water. Addition of salt further decreases solubility by competing for water dipoles; as does low temperature. Please see the OpenWetWare website for more details.</p>
<p>Steps   7. Add your desired quantity of water. Vortex and spin down to resuspend. NOTES Beware of using water unless you are sure of what you are getting in to. The "pH" of water can vary widely (I've seen from pH 5 to pH 8.5), and depurination of DNA at low pH or degradation of RNA at high pH are possibilities. Water also typically contains trace metals, which can accelerate these reactions. I typically recommend resuspension in TE (10 mM Tris-HCl, pH 7.5, 1 mM EDTA). This makes sure your nucleic acid is at a neutral pH and the EDTA will chelate any trace metals. Since they are in such small amounts, neither the buffer nor the EDTA will affect most downstream reactions.</p>
<p>Generated Pseudocode and Pseudofunctions</p>
<p>We show the generated pseudocode and pseudofunctions, which we use as ground truth, in Figure 4 Predicted Protocol We show the predicted protocol in Figure 5 </p>
<p>D LLama evaluation</p>
<p>To benchmark performance on open-source models, we also conducted a run of our experimental evaluation tasks on Llama-2 (Touvron et al., 2023).</p>
<p>We evaluate the 7B model and report performance on protocol generation and function retrieval in Table 9 and Table 10, respectively. We found that Llama-2 significantly underperforms GPT-3.5 and GPT-4 models in function selection. As part of our evaluation on Llama-2 we observe that, when using feedback, the model is distracted and does not attempt to re-write code. Iterative feedback appears to be a process that is effective for GPT models and not Llama models, and this observation is consistent with prior work (Madaan et al., 2023). We also ran Llama-2 on the next step prediction task, but we found that the model was unable to complete this task. The model would typically produce text that states an intent to complete the pseudocode rather than writing the actual next pseudocode line. This difference in behaviour is likely due to a difference in training regimes between GPT models and Lllama-2, but given the lack of documentation around the training of GPT models, the precise nature of this difference is unknown.</p>
<p>E Dataset and Evaluation</p>
<p>The BIOPROT dataset and evaluation metrics from this paper can be found at https://github.com/bioplanner/bioplanner</p>
<p>F Human Benchmarking</p>
<p>While we believe our metric is internally useful for comparing the performance of LLM models and approaches, we wanted to assess how our tasks used relate to human performance. To this end, we performed a human evaluation of next-step prediction tasks and function selection tasks. We worked with an undergraduate biomedical sciences student and asked them to complete the next step prediction task and the function selection task. The student had access to internet search and an unlimited amount of time to answer questions. With the next step prediction task we provided shuffled functions, and with the function selection task, we used random distractor functions. For the function selection task, Human Precision was 87.5%, and human Recall was 0.84%, (n=20), indicating a significant increase in performance over GPT-4. GPT-4 performance is potentially weaker than human performance in the function selection task due to the large number of nearest-neighbour functions in the context window acting as distractors from the task instructions. For the next step prediction task human accuracy was 54.8% (n=32), with Precision and Recall or arguments being 97% and 95% respectively. This performance is roughly comparable to GPT-4 in the shuffled function setting.          </p>
<p>Figure 4 :
4Generated pseudofunctions and pseudocode. Given the protocol title, description, and free text step-by-step instructions, we generate pseudocode and pseudofunctions.</p>
<p>Figure 5 :
5Predicted pseudocode. Given protocol title, description, and an admissible set of pseudofunctions, a model predicts pseudocode. This coresponds to the pseudofunctions inFigure 4.</p>
<p>Figure 6 :
6Prompt for generating pseudofunctions and pseudocode.</p>
<p>Figure 7 :
7Prompt for predicting pseudocode.</p>
<p>Figure 8 :
8Prompt for summarizing a protocol.</p>
<p>Figure 9 :
9Error messages for feedback loops.</p>
<p>Figure 10 :
10LLM query for protocol retrieval.</p>
<p>Figure 11 :
11Langchain output for protocol retrieval.</p>
<p>Figure 12 :
12Prompt to generate protocol from retrieved functions.</p>
<p>Figure 13 :
13The LLM generated protocol used in our lab experiment.</p>
<p>Statistic Value Avg. number of pseudofunctions per protocol 10.3 Avg. number of pseudofunctions per step 0.82 Avg. number of lines of pseudocode 17.2</p>
<p>Table 2 :
2Pseudocode Statistics We present aggregate statistics about the automatically generated pseudofunctions and pseudocode.</p>
<p>Table 4 :
4Next</p>
<p>Table 5 :
5Protocol Generation Evaluation Given a protocol title and description, and a set of admissible pseudofunctions, we evaluate the model performance on full protocol generation. For all metrics higher values are better, except for the normalized Levenshtein distance L d n, where lower values are better. Best performance is bolded and second best is underlined. We report mean and standard deviation over 5 runs.Model 
Neighbourhood Precision Recall </p>
<p>GPT-3.5 
Nearest 
24.2 
35.7 
GPT-3.5 
Random 
36.7 
45.2 </p>
<p>GPT-4 
Nearest 
32.5 
39.2 
GPT-4 
Random 
48.8 
49.4 </p>
<p>Table 6 :
6Function retrieval. Performance on function retrieval of pseudofunctions from the query protocol, as well as (i) random or (ii) nearest neighbors protocols.</p>
<ol>
<li>Add 1/10 volume of 3M sodium acetate, pH 5.2 or 1/2 volume of 5M ammonium acetate. reagents 2. Add 2-3 volumes of 100% Ethanol. 3. Mix and freeze overnight in -20. NOTES In general, the time you need to incubate in the freezer depends on how much nucleic acid you have, how big it is and the volume it is in. My general protocol is to freeze for 20 min to 1 hr at -80C. This seems to work well for most things, but you may want to freeze longer if you have only a small concentration of nucleic acid or if it is small in size(&lt;15 nucleotides). (Kathleen) NOTES If you are in a hurry, you can also dip you epi shortly into liquid nitrogen. If you added enough ethanol, the mix won't freeze. Careful with isopropanol -it freezes more quickly. This works well for me and saves me a lengthy incubation in the fridge. (Jasu) 4. Spin at full speed in a standard microcentrifuge at 4 degrees for 30 minutes. 1800s 5. Decant (or carefully pipet off) the supernatant. 6. Dry the pellet. NOTES For this you can air dry (tubes open, 15 min) or dry in a speedvac. DNA and RNA (if you don't have RNases in your sample) are typically hearty Recall L dn ↓ Precision Recall SciBERTScore BLEUModel </li>
</ol>
<p>Shuffle Feedback 
Functions 
Arguments 
Precision Llama2-7B 
✗ 
✗ 
83.6 
49.8 
0.74 
76.2 
41.4 
79.8 
0.048 
Llama2-7B 
✗ 
✓ 
81.0 
45.9 
0.82 
70.4 
42.9 
80.4 
0.050 
Llama2-7B 
✓ 
✗ 
82.2 
45.1 
0.63 
70.7 
43.8 
81.3 
0.051 
Llama2-7B 
✓ 
✓ 
78.5 
30.4 
0.56 
73.0 
51.4 
81.1 
0.047 </p>
<p>Table 9 :
9Evaluating Llama on Protocol Generation Evaluation Given a protocol title and description, and a set of admissible pseudofunctions, we evaluate the model performance on full protocol generation. For all metrics higher values are better, except for the normalized Levenshtein distance L d n, where lower values are better.Model 
Neighbourhood Precision Recall </p>
<p>LLama2-7B 
Nearest 
26.1 
57.5 
LLama2-7B 
Random 
28.1 
56.3 </p>
<p>Table 10 :
10Evaluating Llama on Function retrieval. Performance on function retrieval of pseudofunctions from the query protocol, as well as (i) random or (ii) nearest neighbors protocols. enough for you to air dry at 37C, if desired. NOTES Overdrying can make DNA hard to redissolve. Especially for longer DNA, I avoid vacuum drying and airdry only briefly before re-dissolving. (Jasu)</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, arXiv:2204.01691arXiv preprintMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. 2022. Do as i can, not as i say: Ground- ing language in robotic affordances. arXiv preprint arXiv:2204.01691.</p>
<p>SynKB: Semantic search for synthetic procedures. Fan Bai, Alan Ritter, Peter Madrid, Dayne Freitag, John Niekrasz, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2022 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAbu Dhabi, UAEAssociation for Computational LinguisticsFan Bai, Alan Ritter, Peter Madrid, Dayne Freitag, and John Niekrasz. 2022. SynKB: Semantic search for synthetic procedures. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan- guage Processing: System Demonstrations, pages 311-318, Abu Dhabi, UAE. Association for Compu- tational Linguistics.</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China. Association for Computational Linguistics.</p>
<p>Reevaluating evaluation in text summarization. Manik Bhandari, Atabak Pranav Narayan Gour, Pengfei Ashfaq, Graham Liu, Neubig, 10.18653/v1/2020.emnlp-main.751Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Manik Bhandari, Pranav Narayan Gour, Atabak Ash- faq, Pengfei Liu, and Graham Neubig. 2020. Re- evaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computa- tional Linguistics.</p>
<p>Emergent autonomous scientific research capabilities of large language models. A Daniil, Robert Boiko, Gabe Macknight, Gomes, arXiv:2304.05332arXiv preprintDaniil A Boiko, Robert MacKnight, and Gabe Gomes. 2023. Emergent autonomous scientific research ca- pabilities of large language models. arXiv preprint arXiv:2304.05332.</p>
<p>Sam Andres M Bran, Cox, D Andrew, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprintAndres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. 2023. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376.</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprintSébastien Bubeck, Varun Chandrasekaran, Ronen El- dan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund- berg, et al. 2023. Sparks of artificial general intelli- gence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%<em> chatgpt quality. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open- source chatbot impressing gpt-4 with 90%</em> chatgpt quality.</p>
<p>Evaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare delivery. Debadutta Dash, Rahul Thapa, M Juan, Akshay Banda, Morgan Swaminathan, Mehr Cheatham, Nikesh Kashyap, Jonathan H Kotecha, Saurabh Chen, Lance Gombar, Downing, arXiv:2304.13714arXiv preprintDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham, Mehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gom- bar, Lance Downing, et al. 2023. Evaluation of gpt-3.5 and gpt-4 for supporting real-world infor- mation needs in healthcare delivery. arXiv preprint arXiv:2304.13714.</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.03872arXiv preprintZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. 2023. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872.</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, arXiv:2304.11477Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprintBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. Llm+ p: Empowering large language models with optimal planning proficiency. arXiv preprint arXiv:2304.11477.</p>
<p>. Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023b. Evaluating the logical reasoning ability of chatgpt and gpt-4Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023b. Evaluating the logical reasoning ability of chatgpt and gpt-4.</p>
<p>Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, arXiv:2305.18090Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprintShengchao Liu, Jiongxiao Wang, Yijin Yang, Cheng- peng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. 2023c. Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprint arXiv:2305.18090.</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in Bioinformatics. 623Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. Biogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6).</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, arXiv:2303.17651Self-refine: Iterative refinement with self-feedback. arXiv preprintAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651.</p>
<p>Embodiedgpt: Visionlanguage pre-training via embodied chain of thought. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, Ping Luo, arXiv:2305.15021arXiv preprintYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision- language pre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah, arXiv:2306.02707Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprintSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa- har, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.</p>
<p>. OpenAI. 2023. Gpt-4 technical report. OpenAI. 2023. Gpt-4 technical report.</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang, Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. 2023. Logic-lm: Empower- ing large language models with symbolic solvers for faithful logical reasoning.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalu- ation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Compu- tational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprintBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal- ley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, arXiv:2302.04761Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprintTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, arXiv:2303.17580arXiv preprintYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugging- gpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Mujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sungdong Kim, Jaewoo Kang, arXiv:2109.07154Can language models be biomedical knowledge bases. arXiv preprintMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung- dong Kim, and Jaewoo Kang. 2021. Can language models be biomedical knowledge bases? arXiv preprint arXiv:2109.07154.</p>
<p>Process-level representation of scientific protocols with interactive annotation. Ronen Tamari, Fan Bai, Alan Ritter, Gabriel Stanovsky, 10.18653/v1/2021.eacl-main.187Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsRonen Tamari, Fan Bai, Alan Ritter, and Gabriel Stanovsky. 2021. Process-level representation of scientific protocols with interactive annotation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Lin- guistics: Main Volume, pages 2190-2202, Online. Association for Computational Linguistics.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, arXiv:2211.09085Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprintRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Protocols. io: virtual communities for protocol development and discussion. Leonid Teytelman, Alexei Stoliartchouk, Lori Kindler, Bonnie L Hurwitz, PLoS Biology. 1481002538Leonid Teytelman, Alexei Stoliartchouk, Lori Kindler, and Bonnie L Hurwitz. 2016. Protocols. io: virtual communities for protocol development and discus- sion. PLoS Biology, 14(8):e1002538.</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288arXiv preprintHugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda- tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, arXiv:2305.16291arXiv preprintGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man- dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An- ima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, arXiv:2305.17926Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprintPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903.</p>
<p>Moleculenet: A benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, Vijay Pande, Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay Pande. 2017. Moleculenet: A benchmark for molecular machine learning.</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, L Thomas, Yuan Griffiths, Karthik Cao, Narasimhan, arXiv:2305.10601arXiv preprintShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger, Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. arXiv preprintTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675.</p>
<p>Multimodal chain-of-thought reasoning in language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, Alex Smola, arXiv:2302.00923arXiv preprintZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. 2023. Multi- modal chain-of-thought reasoning in language mod- els. arXiv preprint arXiv:2302.00923.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, arXiv:2305.11206Lima: Less is more for alignment. arXiv preprintChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, arXiv:2205.10625Quoc Le, and Ed Chi. 2022arXiv preprintDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-most prompting enables complex reason- ing in large language models. arXiv preprint arXiv:2205.10625.</p>            </div>
        </div>

    </div>
</body>
</html>
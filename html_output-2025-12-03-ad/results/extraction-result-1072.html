<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1072 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1072</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1072</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-9862caed8ee93321c78b0196e0b7eef516b545ba</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9862caed8ee93321c78b0196e0b7eef516b545ba" target="_blank">Reverse Curriculum Generation for Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a method to learn goal-oriented tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved, and generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal- oriented tasks.</p>
                <p><strong>Paper Abstract:</strong> Many relevant tasks require an agent to reach a certain state, or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in reverse, gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1072.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1072.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Point-mass maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point-mass maze navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated point-mass agent trained with policy-gradient RL (TRPO) using a reverse-curriculum of start states generated by short Brownian rollouts from known-goal states; evaluated on a G-shaped maze navigation task with sparse binary reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Point-mass agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated point-mass embodied agent trained with reinforcement learning (Trust Region Policy Optimization) and the paper's reverse-curriculum start-state generation (Brownian motion from 'good starts').</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Point-mass G-shaped maze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D maze (G-shaped) in which the agent must navigate its (x,y) position to within 0.3 m of a fixed goal; sparse binary reward (1 on reaching goal, episode terminates). Complexity arises from maze geometry (constraining feasible paths) and sparse reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Geometry of maze (G-shaped), feasible (x,y) state space size; task horizon T=500; difficulty characterized qualitatively by how far start-state distribution S^0 is from the goal and path constraints imposed by the maze.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (constrained navigation with sparse rewards and long horizon)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Target start-state distribution defined as uniform over all feasible (x,y) positions in the maze (explicitly procedurally varied initial positions); curriculum generates start states via Brownian rollouts (M aggregated samples, subsampled to N_new).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (uniform over the full feasible maze area)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Probability of reaching goal (success rate) / expected return R(π,s0) under uniform S^0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline (Uniform Sampling + TRPO) shows high variance across seeds; Brownian-on-Good-Starts substantially improves mean success and reduces variance (no single numeric final success rate given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper discusses that larger variation in start states (uniform S^0 across maze) increases difficulty for on-policy RL due to rare successful trajectories in batches; concentrating start-samples near the goal (growing curriculum) reduces effective exploration burden and improves learning. Also notes that exhaustive (oracle) sampling of 'good starts' gives better learning at the expense of much higher sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Reverse curriculum learning (automatic curriculum via adapted start-state distributions using Brownian rollouts) combined with on-policy RL (TRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Performance evaluated on original uniform start distribution S^0; reverse-curriculum training generalized to a wider set of start states and improved success across S^0 compared to uniform-start training (qualitative improvement shown in learning curves).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used TRPO with batch size 50,000 timesteps; authors note that Oracle rejection-sampling to identify S_i^0 is orders of magnitude more sample-intensive, while the Brownian approximation trades some performance for far lower sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting the start-state distribution to concentrate on 'good starts' (states with intermediate success probability) dramatically speeds up learning and reduces variance for maze navigation; Brownian-generated nearby states around current 'good starts' is an effective, sample-efficient approximation of the ideal curriculum; exhaustive oracle selection is better but infeasible due to much higher sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse Curriculum Generation for Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1072.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1072.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant-maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant (quadruped) maze navigation agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated quadruped 'Ant' agent trained with TRPO plus reverse-curriculum start-state generation; task is to move the robot's center-of-mass to within 0.5 m of a goal at the end of a U-shaped maze under sparse binary reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Ant agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated quadruped robot (MuJoCo Ant) trained with reinforcement learning (TRPO) and the reverse-curriculum method that samples start states by Brownian motion rollouts from 'good starts'.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (robotic morphology)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ant U-shaped maze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>High-dimensional locomotion environment with contact dynamics and constrained corridors (U-shaped maze); objective is to get the ant's center-of-mass within 0.5 m of goal. Complexity stems from high-dimensional state/action spaces of the ant and maze-induced path constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>State/action dimensionality (ant morphology), maze geometry constraints, episode horizon T=2000, sparse binary reward (only when reaching goal).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (continuous high-dim locomotion with constrained navigation and long horizons)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Start-state variation defined as uniform over all feasible ant positions inside the maze (variation in initial positions and orientations); curriculum expands start-state set from the goal outwards using Brownian rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (uniform across feasible positions inside maze)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Probability of reaching goal (success rate) / expected return under uniform S^0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Uniform Sampling + TRPO shows slower learning; reverse-curriculum methods show considerably faster learning and higher success (no numeric success rates provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that when the start-state distribution is over a smaller space (ant-maze vs. point-mass), the difficulty from variation is reduced but still benefits from curriculum; adapting start-state distribution improves learning speed even for smaller S^0.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Reverse curriculum learning (start-state adaptation via Brownian rollouts) with on-policy TRPO</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on uniform S^0; reverse-curriculum training achieved faster learning and better performance across start states than uniform-start baseline (qualitative improvement in learning curves).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Batch size 50,000 timesteps per TRPO update; episode horizon T=2000; curriculum approach reduces sample complexity compared to naive exploration but exact interaction counts to reach final performance are not numerically specified.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Even in high-dimensional locomotion tasks, adapting the start distribution and focusing on 'good starts' accelerates learning; variation in start states still imposes difficulty but curriculum expansion from the goal mitigates exploration burden.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse Curriculum Generation for Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1072.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1072.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ring-on-peg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>7-DOF robot ring-on-peg manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7 degree-of-freedom simulated robotic arm trained with TRPO and reverse curriculum start-state generation to place a ring onto a tight-fitting peg using sparse binary reward; environment is contact-rich and requires precise, multi-stage maneuvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-DOF robotic arm (ring task)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated 7-DOF robot arm trained with reinforcement learning (TRPO) using the reverse curriculum algorithm which grows a set of feasible start states outward from a provided goal state via Brownian action-space perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (manipulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ring-on-peg manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Contact-rich fine-grained manipulation: place a ring (square disk with hole) onto a tight peg and have it reach within 3 cm of peg bottom; start-state distribution S^0 is uniform over feasible joint positions where the ring center is within 40 cm of the peg bottom. Complexity arises from tight clearances, multi-stage maneuvering (lift, align, insert), contacts, and high-dimensional joint space.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>High-dimensional joint-space (7 DOF), contact dynamics, required sequence of precise sub-actions (lifting over peg then insertion), sparse binary reward, goal-ball radius 0.03 m; dataset for evaluation contains 39,530 feasible start states (5,660 already with ring in peg).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (contact-rich, precise manipulation with multi-stage requirements)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Start-state variation measured by number of feasible joint configurations in S^0; dataset size reported: 39,530 sampled states used to approximate S^0 for evaluation; start-state variation operationalized as uniform sampling over that feasible set.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (tens of thousands of distinct feasible initial joint configurations sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Probability of reaching the goal (success rate) / expected return under uniform S^0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Uniform Sampling (baseline) success ~10% (probability of reaching goal); reverse-curriculum (Brownian-on-Good-Starts) achieves substantially higher success across a wider set of start states (qualitative improvement; exact numeric final success rate not stated).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors emphasize that high environment complexity (contact-rich manipulation) combined with high start-state variation makes sparse-reward RL effectively intractable without curriculum; reverse curriculum reduces effective complexity by starting near goal and expanding, and focusing training on 'good starts'. They also show that exhaustive selection of S_i^0 (oracle) yields better learning but is much more sample-intensive, demonstrating a trade-off between performance and sample complexity when approximating ideal curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Reverse curriculum learning (automatic start-state generation via Brownian motion) with TRPO; replay buffer of previous 'good starts' used to avoid forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Policies trained with the reverse curriculum generalized to succeed from a wide range of far-away start states sampled from the uniform S^0 proxy dataset, whereas policies trained with uniform start sampling largely only succeeded from starts already near the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Hyperparameters: aggregated sampled start states M=10,000, subsampled N_new=200 per iteration, N_old=100 replayed old starts; Brownian rollouts horizon T_B=50; TRPO batch size 50,000 timesteps per update; authors report that the Brownian approximation reduces sample complexity compared to exhaustive (oracle) selection, which is orders of magnitude more expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For contact-rich, high-precision manipulation tasks with large start-state variation, reverse curriculum generation (growing start-state set from a known goal) is essential to enable learning under sparse rewards; focusing on 'good starts' speeds training and avoids wasting effort on starts that are currently too hard; Brownian action-space perturbations generate feasible nearby starts respecting constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse Curriculum Generation for Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1072.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1072.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Key-insertion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>7-DOF robot key insertion manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated 7-DOF arm trained via TRPO combined with reverse-curriculum start-state sampling to learn the multi-stage key insertion task (precise orientation and rotations) from sparse binary reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>7-DOF robotic arm (key task)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Simulated 7-DOF robotic manipulator trained with reinforcement learning (TRPO) using the paper's automatic reverse-curriculum start-state generation and Brownian rollouts to expand feasible start states from a provided goal state.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (manipulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Key insertion manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Contact-rich, multi-stage manipulation requiring insertion of a key with specific orientation and sequential rotations (insert, rotate 90deg CW, push, rotate 90deg CCW) until three reference points are within 3 cm of targets; start states are uniform over feasible joint positions with the key tip within 40 cm of the key-hole. Complexity arises from precise orientation requirements, sequential sub-tasks, and contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>7 DOF joint space, contact interactions, sequence of precise rotations and translations, sparse binary reward, goal-ball radius 0.03 m; evaluation dataset contains 544,575 sampled feasible start states (120,784 have the key already inside key-hole).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>very high (fine-grained multi-step contact manipulation in high-dimensional joint space)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Measured by number of feasible joint configurations in S^0 used for evaluation: 544,575 sampled start states (with ~120,784 states already having the key inside the key-hole); start-state variation operationalized as uniform sampling over this large feasible set.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>very high (hundreds of thousands of distinct feasible initial joint configurations sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Probability of reaching goal (success rate) / expected return under uniform S^0</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Uniform Sampling (baseline) success ~2% (very low); reverse-curriculum methods achieved successful policies that reach the goal from a wide range of far-away start states (qualitative substantial improvement; numeric final success rate not reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper demonstrates that for very high-complexity, high-variation manipulation tasks, naive uniform training fails (very low success rates); reverse-curriculum expansion from the goal and focusing on 'good starts' enables learning. They note an explicit trade-off: exhaustive (oracle) selection of 'good starts' improves learning but is prohibitively sample-intensive; the Brownian approximation is more sample-efficient but less optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Reverse curriculum learning via automatic start-state generation (Brownian action-space rollouts) plus TRPO; replay buffer used to prevent forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents trained with reverse curriculum generalized to a large fraction of the evaluated start-state dataset S^0, whereas agents trained with uniform sampling only reliably reached the goal from starts already very near the goal.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Large evaluation dataset (544,575 states). Training hyperparameters: aggregated M=10,000 sampled starts, N_new=200 per iteration, N_old=100, T_B=50 Brownian horizon, TRPO batch size 50,000 timesteps; baseline uniform training has low sample efficiency (fails), Oracle rejection-sampling is orders of magnitude more sample-intensive than the Brownian approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In extremely challenging, high-variation manipulation tasks, curriculum generation that expands feasible start states outward from a provided goal is critical to learn under sparse rewards; focusing training on 'good starts' is an important mechanism, and Brownian action-space perturbations provide a practical way to generate feasible varied starts while controlling sample complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Reverse Curriculum Generation for Reinforcement Learning', 'publication_date_yy_mm': '2017-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play <em>(Rating: 2)</em></li>
                <li>Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World <em>(Rating: 2)</em></li>
                <li>Approximately Optimal Approximate Reinforcement Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1072",
    "paper_id": "paper-9862caed8ee93321c78b0196e0b7eef516b545ba",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "Point-mass maze",
            "name_full": "Point-mass maze navigation agent",
            "brief_description": "A simulated point-mass agent trained with policy-gradient RL (TRPO) using a reverse-curriculum of start states generated by short Brownian rollouts from known-goal states; evaluated on a G-shaped maze navigation task with sparse binary reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Point-mass agent",
            "agent_description": "Simulated point-mass embodied agent trained with reinforcement learning (Trust Region Policy Optimization) and the paper's reverse-curriculum start-state generation (Brownian motion from 'good starts').",
            "agent_type": "simulated agent",
            "environment_name": "Point-mass G-shaped maze",
            "environment_description": "2D maze (G-shaped) in which the agent must navigate its (x,y) position to within 0.3 m of a fixed goal; sparse binary reward (1 on reaching goal, episode terminates). Complexity arises from maze geometry (constraining feasible paths) and sparse reward signals.",
            "complexity_measure": "Geometry of maze (G-shaped), feasible (x,y) state space size; task horizon T=500; difficulty characterized qualitatively by how far start-state distribution S^0 is from the goal and path constraints imposed by the maze.",
            "complexity_level": "medium (constrained navigation with sparse rewards and long horizon)",
            "variation_measure": "Target start-state distribution defined as uniform over all feasible (x,y) positions in the maze (explicitly procedurally varied initial positions); curriculum generates start states via Brownian rollouts (M aggregated samples, subsampled to N_new).",
            "variation_level": "high (uniform over the full feasible maze area)",
            "performance_metric": "Probability of reaching goal (success rate) / expected return R(π,s0) under uniform S^0",
            "performance_value": "Baseline (Uniform Sampling + TRPO) shows high variance across seeds; Brownian-on-Good-Starts substantially improves mean success and reduces variance (no single numeric final success rate given in text).",
            "complexity_variation_relationship": "Paper discusses that larger variation in start states (uniform S^0 across maze) increases difficulty for on-policy RL due to rare successful trajectories in batches; concentrating start-samples near the goal (growing curriculum) reduces effective exploration burden and improves learning. Also notes that exhaustive (oracle) sampling of 'good starts' gives better learning at the expense of much higher sample complexity.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Reverse curriculum learning (automatic curriculum via adapted start-state distributions using Brownian rollouts) combined with on-policy RL (TRPO)",
            "generalization_tested": true,
            "generalization_results": "Performance evaluated on original uniform start distribution S^0; reverse-curriculum training generalized to a wider set of start states and improved success across S^0 compared to uniform-start training (qualitative improvement shown in learning curves).",
            "sample_efficiency": "Training used TRPO with batch size 50,000 timesteps; authors note that Oracle rejection-sampling to identify S_i^0 is orders of magnitude more sample-intensive, while the Brownian approximation trades some performance for far lower sample complexity.",
            "key_findings": "Adapting the start-state distribution to concentrate on 'good starts' (states with intermediate success probability) dramatically speeds up learning and reduces variance for maze navigation; Brownian-generated nearby states around current 'good starts' is an effective, sample-efficient approximation of the ideal curriculum; exhaustive oracle selection is better but infeasible due to much higher sample complexity.",
            "uuid": "e1072.0",
            "source_info": {
                "paper_title": "Reverse Curriculum Generation for Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Ant-maze",
            "name_full": "Ant (quadruped) maze navigation agent",
            "brief_description": "A simulated quadruped 'Ant' agent trained with TRPO plus reverse-curriculum start-state generation; task is to move the robot's center-of-mass to within 0.5 m of a goal at the end of a U-shaped maze under sparse binary reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Ant agent",
            "agent_description": "Simulated quadruped robot (MuJoCo Ant) trained with reinforcement learning (TRPO) and the reverse-curriculum method that samples start states by Brownian motion rollouts from 'good starts'.",
            "agent_type": "simulated agent (robotic morphology)",
            "environment_name": "Ant U-shaped maze",
            "environment_description": "High-dimensional locomotion environment with contact dynamics and constrained corridors (U-shaped maze); objective is to get the ant's center-of-mass within 0.5 m of goal. Complexity stems from high-dimensional state/action spaces of the ant and maze-induced path constraints.",
            "complexity_measure": "State/action dimensionality (ant morphology), maze geometry constraints, episode horizon T=2000, sparse binary reward (only when reaching goal).",
            "complexity_level": "high (continuous high-dim locomotion with constrained navigation and long horizons)",
            "variation_measure": "Start-state variation defined as uniform over all feasible ant positions inside the maze (variation in initial positions and orientations); curriculum expands start-state set from the goal outwards using Brownian rollouts.",
            "variation_level": "high (uniform across feasible positions inside maze)",
            "performance_metric": "Probability of reaching goal (success rate) / expected return under uniform S^0",
            "performance_value": "Uniform Sampling + TRPO shows slower learning; reverse-curriculum methods show considerably faster learning and higher success (no numeric success rates provided in text).",
            "complexity_variation_relationship": "Paper reports that when the start-state distribution is over a smaller space (ant-maze vs. point-mass), the difficulty from variation is reduced but still benefits from curriculum; adapting start-state distribution improves learning speed even for smaller S^0.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Reverse curriculum learning (start-state adaptation via Brownian rollouts) with on-policy TRPO",
            "generalization_tested": true,
            "generalization_results": "Evaluated on uniform S^0; reverse-curriculum training achieved faster learning and better performance across start states than uniform-start baseline (qualitative improvement in learning curves).",
            "sample_efficiency": "Batch size 50,000 timesteps per TRPO update; episode horizon T=2000; curriculum approach reduces sample complexity compared to naive exploration but exact interaction counts to reach final performance are not numerically specified.",
            "key_findings": "Even in high-dimensional locomotion tasks, adapting the start distribution and focusing on 'good starts' accelerates learning; variation in start states still imposes difficulty but curriculum expansion from the goal mitigates exploration burden.",
            "uuid": "e1072.1",
            "source_info": {
                "paper_title": "Reverse Curriculum Generation for Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Ring-on-peg",
            "name_full": "7-DOF robot ring-on-peg manipulation",
            "brief_description": "A 7 degree-of-freedom simulated robotic arm trained with TRPO and reverse curriculum start-state generation to place a ring onto a tight-fitting peg using sparse binary reward; environment is contact-rich and requires precise, multi-stage maneuvers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-DOF robotic arm (ring task)",
            "agent_description": "Simulated 7-DOF robot arm trained with reinforcement learning (TRPO) using the reverse curriculum algorithm which grows a set of feasible start states outward from a provided goal state via Brownian action-space perturbations.",
            "agent_type": "simulated robotic agent (manipulator)",
            "environment_name": "Ring-on-peg manipulation",
            "environment_description": "Contact-rich fine-grained manipulation: place a ring (square disk with hole) onto a tight peg and have it reach within 3 cm of peg bottom; start-state distribution S^0 is uniform over feasible joint positions where the ring center is within 40 cm of the peg bottom. Complexity arises from tight clearances, multi-stage maneuvering (lift, align, insert), contacts, and high-dimensional joint space.",
            "complexity_measure": "High-dimensional joint-space (7 DOF), contact dynamics, required sequence of precise sub-actions (lifting over peg then insertion), sparse binary reward, goal-ball radius 0.03 m; dataset for evaluation contains 39,530 feasible start states (5,660 already with ring in peg).",
            "complexity_level": "high (contact-rich, precise manipulation with multi-stage requirements)",
            "variation_measure": "Start-state variation measured by number of feasible joint configurations in S^0; dataset size reported: 39,530 sampled states used to approximate S^0 for evaluation; start-state variation operationalized as uniform sampling over that feasible set.",
            "variation_level": "high (tens of thousands of distinct feasible initial joint configurations sampled)",
            "performance_metric": "Probability of reaching the goal (success rate) / expected return under uniform S^0",
            "performance_value": "Uniform Sampling (baseline) success ~10% (probability of reaching goal); reverse-curriculum (Brownian-on-Good-Starts) achieves substantially higher success across a wider set of start states (qualitative improvement; exact numeric final success rate not stated).",
            "complexity_variation_relationship": "Authors emphasize that high environment complexity (contact-rich manipulation) combined with high start-state variation makes sparse-reward RL effectively intractable without curriculum; reverse curriculum reduces effective complexity by starting near goal and expanding, and focusing training on 'good starts'. They also show that exhaustive selection of S_i^0 (oracle) yields better learning but is much more sample-intensive, demonstrating a trade-off between performance and sample complexity when approximating ideal curricula.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Reverse curriculum learning (automatic start-state generation via Brownian motion) with TRPO; replay buffer of previous 'good starts' used to avoid forgetting.",
            "generalization_tested": true,
            "generalization_results": "Policies trained with the reverse curriculum generalized to succeed from a wide range of far-away start states sampled from the uniform S^0 proxy dataset, whereas policies trained with uniform start sampling largely only succeeded from starts already near the goal.",
            "sample_efficiency": "Hyperparameters: aggregated sampled start states M=10,000, subsampled N_new=200 per iteration, N_old=100 replayed old starts; Brownian rollouts horizon T_B=50; TRPO batch size 50,000 timesteps per update; authors report that the Brownian approximation reduces sample complexity compared to exhaustive (oracle) selection, which is orders of magnitude more expensive.",
            "key_findings": "For contact-rich, high-precision manipulation tasks with large start-state variation, reverse curriculum generation (growing start-state set from a known goal) is essential to enable learning under sparse rewards; focusing on 'good starts' speeds training and avoids wasting effort on starts that are currently too hard; Brownian action-space perturbations generate feasible nearby starts respecting constraints.",
            "uuid": "e1072.2",
            "source_info": {
                "paper_title": "Reverse Curriculum Generation for Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        },
        {
            "name_short": "Key-insertion",
            "name_full": "7-DOF robot key insertion manipulation",
            "brief_description": "A simulated 7-DOF arm trained via TRPO combined with reverse-curriculum start-state sampling to learn the multi-stage key insertion task (precise orientation and rotations) from sparse binary reward.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "7-DOF robotic arm (key task)",
            "agent_description": "Simulated 7-DOF robotic manipulator trained with reinforcement learning (TRPO) using the paper's automatic reverse-curriculum start-state generation and Brownian rollouts to expand feasible start states from a provided goal state.",
            "agent_type": "simulated robotic agent (manipulator)",
            "environment_name": "Key insertion manipulation",
            "environment_description": "Contact-rich, multi-stage manipulation requiring insertion of a key with specific orientation and sequential rotations (insert, rotate 90deg CW, push, rotate 90deg CCW) until three reference points are within 3 cm of targets; start states are uniform over feasible joint positions with the key tip within 40 cm of the key-hole. Complexity arises from precise orientation requirements, sequential sub-tasks, and contact dynamics.",
            "complexity_measure": "7 DOF joint space, contact interactions, sequence of precise rotations and translations, sparse binary reward, goal-ball radius 0.03 m; evaluation dataset contains 544,575 sampled feasible start states (120,784 have the key already inside key-hole).",
            "complexity_level": "very high (fine-grained multi-step contact manipulation in high-dimensional joint space)",
            "variation_measure": "Measured by number of feasible joint configurations in S^0 used for evaluation: 544,575 sampled start states (with ~120,784 states already having the key inside the key-hole); start-state variation operationalized as uniform sampling over this large feasible set.",
            "variation_level": "very high (hundreds of thousands of distinct feasible initial joint configurations sampled)",
            "performance_metric": "Probability of reaching goal (success rate) / expected return under uniform S^0",
            "performance_value": "Uniform Sampling (baseline) success ~2% (very low); reverse-curriculum methods achieved successful policies that reach the goal from a wide range of far-away start states (qualitative substantial improvement; numeric final success rate not reported in text).",
            "complexity_variation_relationship": "Paper demonstrates that for very high-complexity, high-variation manipulation tasks, naive uniform training fails (very low success rates); reverse-curriculum expansion from the goal and focusing on 'good starts' enables learning. They note an explicit trade-off: exhaustive (oracle) selection of 'good starts' improves learning but is prohibitively sample-intensive; the Brownian approximation is more sample-efficient but less optimal.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Reverse curriculum learning via automatic start-state generation (Brownian action-space rollouts) plus TRPO; replay buffer used to prevent forgetting.",
            "generalization_tested": true,
            "generalization_results": "Agents trained with reverse curriculum generalized to a large fraction of the evaluated start-state dataset S^0, whereas agents trained with uniform sampling only reliably reached the goal from starts already very near the goal.",
            "sample_efficiency": "Large evaluation dataset (544,575 states). Training hyperparameters: aggregated M=10,000 sampled starts, N_new=200 per iteration, N_old=100, T_B=50 Brownian horizon, TRPO batch size 50,000 timesteps; baseline uniform training has low sample efficiency (fails), Oracle rejection-sampling is orders of magnitude more sample-intensive than the Brownian approximation.",
            "key_findings": "In extremely challenging, high-variation manipulation tasks, curriculum generation that expands feasible start states outward from a provided goal is critical to learn under sparse rewards; focusing training on 'good starts' is an important mechanism, and Brownian action-space perturbations provide a practical way to generate feasible varied starts while controlling sample complexity.",
            "uuid": "e1072.3",
            "source_info": {
                "paper_title": "Reverse Curriculum Generation for Reinforcement Learning",
                "publication_date_yy_mm": "2017-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play",
            "rating": 2
        },
        {
            "paper_title": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World",
            "rating": 2
        },
        {
            "paper_title": "Approximately Optimal Approximate Reinforcement Learning",
            "rating": 1
        }
    ],
    "cost": 0.01411825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Reverse Curriculum Generation for Reinforcement Learning</h1>
<p>Carlos Florensa<br>UC Berkeley<br>florensa@berkeley.edu<br>David Held<br>UC Berkeley<br>davheld@berkeley.edu<br>Markus Wulfmeier<br>Oxford Robotics Institute<br>markus@robots.ox.ac.uk<br>Michael R. Zhang<br>UC Berkeley<br>michaelrzhang@berkeley.edu</p>
<p>Pieter Abbeel<br>OpenAI<br>UC Berkeley<br>ICSI<br>pabbeel@berkeley.edu</p>
<h4>Abstract</h4>
<p>Many relevant tasks require an agent to reach a certain state or to manipulate objects into a desired configuration. For example, we might want a robot to align and assemble a gear onto an axle or insert and turn a key in a lock. These goal-oriented tasks present a considerable challenge for reinforcement learning, since their natural reward function is sparse and prohibitive amounts of exploration are required to reach the goal and receive some learning signal. Past approaches tackle these problems by exploiting expert demonstrations or by manually designing a task-specific reward shaping function to guide the learning agent. Instead, we propose a method to learn these tasks without requiring any prior knowledge other than obtaining a single state in which the task is achieved. The robot is trained in "reverse," gradually learning to reach the goal from a set of start states increasingly far from the goal. Our method automatically generates a curriculum of start states that adapts to the agent's performance, leading to efficient training on goal-oriented tasks. We demonstrate our approach on difficult simulated navigation and fine-grained manipulation problems, not solvable by state-of-the-art reinforcement learning methods.</p>
<p>Keywords: Reinforcement Learning, Robotic Manipulation, Automatic Curriculum Generation</p>
<h2>1 Introduction</h2>
<p>Reinforcement Learning (RL) is a powerful learning technique for training an agent to optimize a reward function. Reinforcement learning has been demonstrated on complex tasks such as locomotion [1], Atari games [2], racing games [3], and robotic manipulation tasks [4]. However, there are many tasks for which it is hard to design a reward function such that it is both easy to maximize and yields the desired behavior once optimized. An ubiquitous example is a goal-oriented task; for such tasks, the natural reward function is usually sparse, giving a binary reward only when the task is completed [5]. This sparse reward can create difficulties for learning-based approaches [6]; on the other hand, non-sparse reward functions for such tasks might lead to undesired behaviors [7].</p>
<p>For example, suppose we want a seven DOF robotic arm to learn how to align and assemble a gear onto an axle or place a ring onto a peg, as shown in Fig. 1c. The complex and precise motion required to align the ring at the top of the peg and then slide it to the bottom of the peg makes learning highly impractical if a binary reward is used. On the other hand, using a reward function based on the distance between the center of the ring and the bottom of the peg leads to learning a policy that places the ring next to the peg, and the agent never learns that it needs to first lift the ring over the top of the peg and carefully insert it. Shaping the reward function [8] to efficiently guide the policy towards the desired solution often requires considerable human expert effort and experimentation to find the correct shaping function for each task. Another source of prior knowledge is the use of demonstrations, but it requires an expert intervention.</p>
<p>In our work, we avoid all reward engineering or use of demonstrations by exploiting two key insights. First, it is easier to reach the goal from states nearby the goal, or from states nearby where the agent already knows how to reach the goal. Second, applying random actions from one such state leads the agent to new feasible nearby states, from where it is not too much harder to reach the goal. This can be understood as requiring a minimum degree of reversibility, which is usually satisfied in many robotic manipulation tasks like assembly and manufacturing.</p>
<p>We take advantage of these insights to develop a "reverse learning" approach for solving such difficult manipulation tasks. The robot is first trained to reach the goal from start states nearby a given goal state. Then, leveraging that knowledge, the robot is trained to solve the task from increasingly distant start states. All start states are automatically generated by executing a short random walk from the previous start states that got some reward but still require more training. This method of learning in reverse, or growing outwards from the goal, is inspired by dynamic programming methods like value iteration, where the solutions to easier sub-problems are used to compute the solution to harder problems.</p>
<p>In this paper, we present an efficient and principled framework for performing such "reverse learning." Our method automatically generates a curriculum of initial positions from which to learn to achieve the task. This curriculum constantly adapts to the learning agent by observing its performance at each step of the training process. Our method requires no prior knowledge of the task other than providing a single state that achieves the task (i.e. is at the goal). The contributions of this paper include:</p>
<ul>
<li>Formalizing a novel problem definition of finding the optimal start-state distribution at every training step to maximize the overall learning speed.</li>
<li>A novel and practical approach for sampling a start state distribution that varies over the course of training, leading to an automatic curriculum of start state distributions.</li>
<li>Empirical experiments showing that our approach solves difficult tasks like navigation or fine-grained robotic manipulation, not solvable by state-of-the-art learning methods.</li>
</ul>
<h1>2 Related Work</h1>
<p>Curriculum-based approaches with manually designed schedules have been explored in supervised learning $[9,10,11,12]$ to split particularly complex tasks into smaller, easier-to-solve sub-problems. One particular type of curriculum learning explicitly enables the learner to reject examples which it currently considers too hard [13, 14]. This type of adaptive curriculum has mainly been applied to supervised tasks, and most practical curriculum approaches in RL rely on pre-specified task sequences [15, 16]. Some very general frameworks have been proposed to generate increasingly hard problems [17, 18], although challenges remain to apply the idea to difficult robotics tasks. A similar line of work uses intrinsic motivation based on learning progress to obtain "developmental trajectories" that focus on increasingly difficult tasks [19]. Nevertheless, their method requires iteratively partitioning the full task space, which strongly limits the application to fine-grain manipulation tasks like the ones presented in our work (see detailed analysis on easier tasks in [5]).
More recent work in using a curriculum for RL assumes that baseline performances for several tasks are given, and it uses them to gauge which tasks are the hardest (furthest behind the baseline) and require more training [20]. However, this framework can only handle finite sets of tasks and requires each task to be learnable on its own. On the other hand, our method trains a policy that generalizes to a set of continuously parameterized tasks, and it is shown to perform well even under sparse rewards by not allocating training effort to tasks that are too hard for the current performance of the agent.
Closer to our method of adaptively generating the tasks to train on, an interesting asymmetric selfplay strategy has recently been proposed [21]. Contrary to our approach, which aims to generate and train on all tasks that are at the appropriate level of difficulty, the asymmetric component of their method can lead to biased exploration concentrating on only a subset of the tasks that are at the appropriate level of difficulty, as the authors and our own experiments suggests. This problem and their time-oriented metric of hardness may lead to poor performance in continuous state-action spaces, which are typical in robotics. Furthermore, their approach is designed as an exploration bonus for a single target task; in contrast, we define a new problem of efficiently optimizing a policy across a range of start states, which is considered relevant to improve generalization [22].</p>
<p>Our approach can be understood as sequentially composing locally stabilizing controllers by growing a tree of stabilized trajectories backwards from the goal state, similar to work done by Tedrake et al. [23]. This can be viewed as a "funnel" which takes start states to the goal state via a series of locally valid policies [24]. Unlike these methods, our approach does not require any dynamic model of the system. An RL counterpart, closer to our approach, is the work by Bagnell et al. [25], where a policy search algorithm in the spirit of traditional dynamic programming methods is proposed to learn a non-stationary policy: they learn what should be done in the last time-step and then "back it up" to learn the previous time-step and so on. Nevertheless, they require the stronger assumption of having access to baseline distributions that approximate the optimal state-distribution at every time-step.</p>
<p>The idea of directly influencing the start state distribution to accelerate learning in a Markov Decision Process (MDP) has drawn attention in the past. Kakade and Langford [26] studied the idea of exploiting the access to a 'generative model' [27] that allows training the policy on a fixed 'restart distribution' different from the one originally specified by the MDP. If properly chosen, this is proven to improve the policy training and final performance on the original start state distribution. Nevertheless, no practical procedure is given to choose this new distribution (only suggesting to use a more uniform distribution over states, which is what our baseline does), and they don't consider adapting the start state distribution during training, as we do. Other researchers have proposed to use expert demonstrations to improve learning of model-free RL algorithms, either by modifying the start state distribution to be uniform among states visited by the provided trajectories [7], or biasing the exploration towards relevant regions [28]. Our method works without any expert demonstrations, so we do not compare against these lines of research.</p>
<h1>3 Problem Definition</h1>
<p>We consider the general problem of learning a policy that leads a system into a specified goal-space, from any start state sampled from a given distribution. In this section we first briefly introduce the general reinforcement learning framework and then we formally define our problem statement.</p>
<h3>3.1 Preliminaries</h3>
<p>We define a discrete-time finite-horizon Markov decision process (MDP) by a tuple $M=$ $\left(\mathcal{S}, \mathcal{A}, \mathcal{P}, r, \rho_{0}, T\right)$, in which $\mathcal{S}$ is a state set, $\mathcal{A}$ an action set, $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}<em 0="0">{+}$is a transition probability distribution, $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ is a bounded reward function, $\rho</em>}: \mathcal{S} \rightarrow \mathbb{R<em _theta="\theta">{+}$is a start state distribution, and $T$ is the horizon. Our aim is to learn a stochastic policy $\pi</em>}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R<em _rho__0="\rho_{0">{+}$ parametrized by $\theta$ that maximizes the expected return, $\eta</em>}}\left(\pi_{\theta}\right)=\mathbb{E<em 0="0">{s</em>} \sim \rho_{0}} R\left(\pi, s_{0}\right)$. We denote by $R\left(\pi, s_{0}\right):=\mathbb{E<em 0="0">{\tau \mid s</em>$ ) and use them to improve the current policy [29, 30, 31].}}\left[\sum_{t=0}^{T} r\left(s_{t}, a_{t}\right)\right]$ the expected cumulative reward starting when starting from a $s_{0} \sim \rho_{0}$, where $\tau=\left(s_{0}, a_{0},, \ldots, a_{T-1}, s_{T}\right)$ denotes a whole trajectory, with $a_{t} \sim \pi_{\theta}\left(a_{t} \mid s_{t}\right)$, and $s_{t+1} \sim \mathcal{P}\left(s_{t+1} \mid s_{t}, a_{t}\right)$. Policy search methods iteratively collect trajectories on-policy (i.e. sampling from the above distributions $\rho_{0}, \pi_{\theta}, \mathcal{P</p>
<p>In our work we propose to instead use a different start-state distribution $\rho_{i}$ at every training iteration $i$ to maximize the learning rate. Learning progress is still evaluated based on the original distribution $\rho_{0}$. Convergence of $\rho_{i}$ to $\rho_{0}$ is desirable but not required as an optimal policy $\pi_{i}^{*}$ under a start distribution $\rho_{i}$ is also optimal under any other $\rho_{0}$, as long as their support coincide. In the case of approximately optimal policies under $\rho_{i}$, bounds on the performance under $\rho_{0}$ can be derived [26].</p>
<h3>3.2 Goal-oriented tasks</h3>
<p>We consider the general problem of reaching a certain goal space $S^{g} \subset \mathcal{S}$ from any start state in $S^{0} \subset \mathcal{S}$. This simple, high-level description can be translated into an MDP without further domain knowledge by using a binary reward function $r\left(s_{t}\right)=\mathbb{1}\left{s_{t} \in S^{g}\right}$ and a uniform distribution over the start states $\rho_{0}=\operatorname{Unif}\left(S^{0}\right)$. We terminate the episode when the goal is reached. This implies that the return $R\left(\pi, s_{0}\right)$ associated with every start state $s_{0}$ is the probability of reaching the goal at some time-step $t \in{0 \ldots T}$.</p>
<p>$$
R\left(\pi, s_{0}\right)=\mathbb{E}<em t="t">{\pi\left(\cdot \mid s</em>\right)
$$}\right)} \mathbb{1}\left{\bigcup_{t=0}^{T} s_{t} \in S^{g} \mid s_{0}\right}=\mathbb{P}\left(\bigcup_{t=0}^{T} s_{t} \in S^{g} \mid \pi, s_{0</p>
<p>As advocated by Rajeswaran et al. [22], it is important to be able to train an agent to achieve the goal from a large set of start states $S^{0}$. An agent trained in this way would be much more robust than an agent that is trained from just a single start state, as it could recover from undesired deviations from the intended trajectory. Therefore, we choose the set of start states $S^{0}$ to be all the feasible points in a wide area around the goal. On the other hand, the goal space $S^{g}$ for our robotics fine-grained manipulation tasks is defined to be a small set of states around the desired configuration (e.g. key in the key-hole, or ring at the bottom of the peg, as described in Sec. 5).</p>
<p>As discussed above, the sparsity of this reward function makes learning extremely difficult for RL algorithms [6, 32, 33], and approaches like reward shaping [8] are difficult and time-consuming to engineer for each task. In the following subsection we introduce three assumptions, and the rest of the paper describes how we can leverage these assumptions to efficiently learn to achieve complex goal-oriented tasks directly from sparse reward functions.</p>
<h1>3.3 Assumptions for reverse curriculum generation</h1>
<p>In this work we study how to exploit three assumptions that hold true in a wide range of practical learning problems (especially if learned in simulation):</p>
<p>Assumption 1 We can arbitrarily reset the agent into any start state $s_{0} \in \mathcal{S}$ at the beginning of all trajectories.
Assumption 2 At least one state $s^{g}$ is provided such that $s^{g} \in S^{g}$.
Assumption 3 The Markov Chain induced by taking uniformly sampled random actions has a communicating class ${ }^{1}$ including all start states $S^{0}$ and the given goal state $s^{g}$.</p>
<p>The first assumption has been considered previously (e.g. access to a generative model in Kearns et al. [27]) and is deemed to be a considerably weaker assumption than having access to the full transition model of the MDP. Kakade and Langford [26] proved that Assumption 1 can be used to improve the learning in MDPs that require large exploration. Nevertheless, they do not propose a concrete procedure to choose a distribution $\rho$ from which to sample the start states in order to maximally improve on the objective in Eq. (1). In our case, combining Assumption 1 with Assumption 2, we are able to reset the state to $s^{g}$, which is critical in our method to initialize the start state distribution to concentrate around the goal space at the beginning of learning. For Assumption 2, note that we only assume access to one state $s^{g}$ in the goal region; we do not require a description of the full region nor trajectories leading to it. Finally, Assumption 3 ensures that the goal can be reached from any of the relevant start states, and that those start states can also be reached from the goal; this assumption is satisfied by many robotic problems of interest, as long as there are no major irreversibilities in the system. In the next sections we detail our automatic curriculum generation method based on continuously adapting the start state distribution to the current performance of the policy. We demonstrate the value of this method for challenging robotic manipulation tasks.</p>
<h2>4 Methodology</h2>
<p>In a wide range of goal-oriented RL problems, reaching the goal from an overwhelming majority of start states in $S^{0}$ requires a prohibitive amount of on-policy or undirected exploration. On the other hand, it is usually easy for the learning agent (i.e. our current policy $\pi_{i}$ ) to reach the goal $S^{g}$ from states nearby a goal state $s^{g} \in S^{g}$. Therefore, learning from these states will be fast because the agent will perceive a strong signal, even under the indicator reward introduced in Section 3.2. Once the agent knows how to reach the goal from these nearby states, it can train from even further states and bootstrap its already acquired knowledge. This reverse expansion is inspired by classical RL methods like Value Iteration or Policy Iteration [34], although in our case we do not assume knowledge of the transition model and our environments have high-dimensional continuous action and state spaces. In the following subsections we propose a method that leverages the assumptions from the previous section and the idea of reverse expansion to automatically adapt the start state distribution, generating a curriculum of start state distributions that can be used to tackle problems unsolvable by standard RL methods.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Algorithm 1: Policy Training</th>
<th>Procedure 2: SampleNearby</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input : $\pi_{0}, s^{g}, \rho_{0}, N_{\text {new }}, N_{\text {old }}, R_{\min }, R_{\max }$, Iter</td>
<td>Input : starts, $N_{\text {new }}, \Sigma, T_{B}, M$</td>
</tr>
<tr>
<td>Output: Policy $\pi_{N}$ starts, old $\leftarrow\left[s^{g}\right]$; starts, rews $\leftarrow\left[s^{g}\right],[1]$;</td>
<td>Output: starts ${ }<em 0="0">{\text {new }}$ while len(starts) $&lt;M$ do $s</em>($ starts $) ;$} \sim \operatorname{Unif</td>
</tr>
<tr>
<td>for $i \leftarrow 1$ to Iter do starts $\leftarrow$ SampleNearby $\left(\right.$ starts, $\left.N_{\text {new }}\right)$ starts.append $[$ sample $\left(\right.$ starts $\left., N_{\text {old }}\right)];$ $\rho_{i} \leftarrow \operatorname{Unif}($ starts $)$; $\pi_{i}$, rews $\leftarrow$ train_pol $\left(\rho_{i}, \pi_{i-1}\right)$; starts $\leftarrow$ select $($ starts, rews, $R_{\min }, R_{\max }$ ); starts,old.append[starts];</td>
<td>for $t \leftarrow 1$ to $T_{B}$ do $a_{t}=\epsilon_{t}, \epsilon_{t} \sim \mathcal{N}(0, \Sigma)$; $s_{t} \sim \mathcal{P}\left(s_{t} \mid s_{t-1}, a_{t}\right) ;$ starts.append $\left(s_{t}\right)$;</td>
</tr>
<tr>
<td>end</td>
<td>end</td>
</tr>
<tr>
<td>end starts ${ }<em _new="{new" _text="\text">{\text {new }} \leftarrow$ sample $\left(s t a r t s, N</em>\right)$}</td>
<td></td>
</tr>
</tbody>
</table>
<h1>4.1 Policy Optimization with modified start state distribution</h1>
<p>Policy gradient strategies are well suited for robotic tasks with continuous and high dimensional action-spaces [35]. Nevertheless, applying them directly on the original MDP does poorly in tasks with sparse rewards and long horizons like our challenging manipulation tasks. If the goal is not reached from the start states in $S^{0}$, no reward is received, and the policy cannot improve. Therefore, we propose to adapt the distribution $\rho_{i}$ from where start states $s_{0}$ are sampled to train policy $\pi_{i}$.
Analogously to Held et al. [5], we postulate that in goal-oriented environments, a strong learning signal is obtained when training on start states $s_{0} \sim \rho_{i}$ from where the agent reaches the goal sometimes, but not always. We call these start states "good starts". More formally, at training iteration $i$, we would like to sample from $\rho_{i}=\operatorname{Unif}\left(S_{i}^{0}\right)$ where $S_{i}^{0}=\left{s_{0}: R_{\min }&lt;R\left(\pi_{i}, s_{0}\right)&lt;\right.$ $\left.R_{\max }\right}$. The hyper-parameters $R_{\min }$ and $R_{\max }$ are easy to tune due to their interpretation as bounds on the probability of success, derived from Eq. (1). Unfortunately, sampling uniformly from $S_{i}^{0}$ is intractable. Nevertheless, at least at the beginning of training, states nearby a goal state $s^{g}$ are more likely to be in $S_{i}^{0}$. Then, after some iterations of training on these start states, some will be completely mastered (i.e. $R\left(\pi_{i+1}, s_{0}\right)&gt;R_{\max }$ and $s_{0}$ is no longer in $S_{i+1}^{0}$ ), but others will still need more training. To find more "good starts", we follow the same reasoning: the states nearby these remaining $s \in S_{i+1}^{0}$ are likely to also be in $S_{i+1}^{0}$. In the rest of the section we describe an effective way of sampling feasible nearby states and we layout the full algorithm.</p>
<h3>4.2 Sampling "nearby" feasible states</h3>
<p>For robotic manipulation tasks with complex contacts and constraints, applying noise in state-space $s^{\prime}=s+\epsilon, \epsilon \sim \mathcal{N}$ may yield many infeasible states $s^{\prime}$. For example, even small random perturbations of the joint angles of a seven degree-of-freedom arm generate large modifications to the end-effector position, potentially placing it in an infeasible state that intersects with surrounding objects. For this reason, the concept of "nearby" states might be unrelated to the Euclidean distance $\left|s^{\prime}-s\right|^{2}$ between these states. Instead, we have to understand proximity in terms of how likely it is to reach one state from the other by taking actions in the MDP.
Therefore, we choose to generate new states $s^{\prime}$ from a certain seed state $s$ by applying noise in action space. This means we exploit Assumption 1 to reset the system to state $s$, and from there we execute short "Brownian motion" rollouts of horizon $T_{B}$ taking actions $a_{t+1}=\epsilon_{t}$ with $\epsilon_{t} \sim \mathcal{N}(0, \Sigma)$. This method of generating "nearby" states is detailed in Procedure 2. The total sampled states $M$ should be large enough such that the $N_{\text {new }}$ desired states starts $_{\text {new }}$, obtained by subsampling, extend in all directions around the input states starts. All states visited during the rollouts are guaranteed to be feasible and can then be used as start states to keep training the policy.</p>
<h3>4.3 Detailed Algorithm</h3>
<p>Our generic algorithm is detailed in Algorithm 1. We first initialize the policy with $\pi_{0}$ and the "good start" states list starts with the given goal state $s^{g}$. Then we perform Iter training iterations of our RL algorithm of choice train_pol. In our case we perform 5 iterations of Trust Region Policy Optimization (TRPO) [36] but any on-policy method could be used. At every iteration, we set the start state distribution $\rho_{i}$ to be uniform over a list of start states obtained by sampling $N_{\text {new }}$ start states from nearby the ones in our "good starts" list starts (see SampleNearby in previous</p>
<p>section), and $N_{\text {old }}$ start states from our replay buffer of previous "good starts" starts $<em 0="0">{\text {old }}$. As already shown by Held et al. [5], the replay buffer is an important feature to avoid catastrophic forgetting. Technically, to check which of the states $s</em>$. We found this heuristic to give a good enough estimate and not drastically decrease learning performance of the overall algorithm.} \in$ starts are in $S_{i}^{0}$ (i.e. the "good starts") we should execute some trajectories from each of those states to estimate the expected returns $R\left(s_{0}, \pi_{i-1}\right)$, but this considerably increases the sample complexity. Instead, we use the trajectories collected by train_pol to estimate $R\left(\pi_{i-1}, s_{0}\right)$ and save it in the list rews. These are used to select the "good" start states for the next iteration - picking the ones with $R_{\min } \leq R\left(\pi_{i-1}, s_{0}\right) \leq R_{\max </p>
<p>Our method keeps expanding the region of the state-space from which the policy can reach the goal reliably. It samples more heavily nearby the start states that need more training to be mastered and avoiding start states that are yet too far to receive any reward under the current policy. Then, thanks to Assumption 3, the Brownian motion that is used to generate further and further start states will eventually reach all start states in $S^{0}$, and therefore our method improves the metric $\eta_{\rho_{0}}$ defined in Sec. 3.1 (see also Sec. A. 2 for details on how we evaluate our progress on this metric).</p>
<h1>5 Experimental Results</h1>
<p>We investigate the following questions in our experiments:</p>
<ul>
<li>Does the performance of the policy on the target start state distribution $\rho_{0}$ improve by training on distributions $\rho_{i}$ growing from the goal?</li>
<li>Does focusing the training on "good starts" speed up learning?</li>
<li>Is Brownian motion a good way to generate "good starts" from previous "good starts"?</li>
</ul>
<p>We use the below task settings to explore these questions. All are implemented in MuJoCo [37] and the hyperparameters used in our experiments are described in Appendix A.1.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Task images. Source code and videos of the performance obtained by our algorithm are available here: http://bit.ly/reversecurriculum</p>
<p>Point-mass maze: (Fig. 1a) A point-mass agent (orange) must navigate within 30 cm of the goal position $(4 m, 4 m)$ at the end of a G-shaped maze (red). The target start state distribution from which we seek to reach the goal is uniform over all feasible $(x, y)$ positions in the maze.</p>
<p>Ant maze: (Fig. 1b) A quadruped robot (orange) must navigate its Center of Mass to within 50cm of the goal position $(0 m, 4 m)$ at the end of a U-shaped maze (red). The target start state distribution from which we seek to reach the goal is uniform over all feasible ant positions inside the maze.</p>
<p>Ring on Peg: (Fig. 1c) A 7 DOF robot must learn to place a "ring" (actually a square disk with a hole in the middle) on top of a tight-fitting round peg. The task is complete when the ring is within 3 cm of the bottom of the 15 cm tall peg. The target start state distribution from which we seek to reach the goal is uniform over all feasible joint positions for which the center of the ring is within 40 cm of the bottom of the peg.</p>
<p>Key insertion: (Fig. 1d) A 7 DOF robot must learn to insert a key into a key-hole. The task is completed when the distance between three reference points at the extremities of the key and its corresponding targets is below 3 cm . In order to reach the target, the robot must first insert the key at a specific orientation, then rotate it 90 degrees clockwise, push forward, then rotate 90 degrees counterclockwise. The target start state distribution from which we seek to reach the goal is uniform over all feasible joint positions such that the tip of the key is within 40 cm of key-hole.</p>
<p>5.1 Effect of start state distribution</p>
<p>In Figure 2, the Uniform Sampling (baseline) red curves show the average return of policies learned with TRPO without modifying the start state distribution. The green and blue curves correspond to our method and an ablation, both exploiting the idea of modifying the start state distribution at every learning iteration. These approaches perform consistently better across the board. In the case of the point-mass maze navigation task in Fig. 2a, we observe that Uniform Sampling has a very high variance because some policies only learn how to perform well from one side of the goal (see Appendix B.2 for a thorough analysis). The Ant-maze experiments in Fig. 2b also show a considerable slow-down of the learning speed when using plain TRPO, although the effect is less drastic as the start state distribution $\rho_{0}$ is over a smaller space.</p>
<p>In the more complex manipulation tasks shown in Fig. 2c-2d, we see that the probability of reaching the goal with Uniform Sampling is around 10% for the ring task and 2% for the key task. These success probabilities correspond to reliably reaching the goal only from very nearby positions: when the ring is already on the peg or when the key is initialized very close to the final position. None of the learned policies trained on the original $\rho_{0}$ learn to reach the goal from more distant start states. On the other hand, our methods do succeed at reaching the goal from a wide range of far away start states. The underlying RL training algorithm and the evaluation metric are the same. We conclude that training on a different start state distribution $\rho_{i}$ can improve training or even allow learning at all.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Learning curves for goal-oriented tasks (mean and variance over 5 random seeds).</p>
<h3>5.2 Effect of "good starts"</h3>
<p>In Figure 2 we see how applying our Algorithm 1 to modify the start state distribution considerably improves learning (Brownian on Good Starts, in green) and final performance on the original MDP. Two elements are involved in this improvement: first, the backwards expansion from the goal, and second, the concentration of training efforts on "good starts". To test the relevance of this second element, we ablate our method by running our SampleNearby Procedure 2 on all states from which the policy was trained in the previous iteration. In other words, the select function in Algorithm 1 is replaced by the identity, returning all starts independently of the rewards rews they obtained during the last training iteration. The resulting algorithm performance is shown as the Brownian from All Starts blue curve in Figures 2. As expected, this method is still better than not modifying the start state distribution but has a slower learning rate than running SampleNearby around the estimated good starts.</p>
<p>Now we evaluate an upper bound of the benefit provided by our idea of sampling "good starts". As mentioned in Sec. 4.1, we would ideally like to sample start states from $\rho_{i}=\operatorname{Unif}\left(S_{i}^{0}\right)$, but it is intractable. Instead, we evaluate states in $S_{i-1}^{0}$, and we use Brownian motion to find nearby states, to approximate $S_{i}^{0}$. We can evaluate how much this approximation hinders learning by exhaustively sampling states in the lower dimensional point-mass maze task. To do so, at every iteration we can sample states $s_{0}$ uniformly from the state-space $\mathcal{S}$, empirically estimate their return $R\left(s_{0}, \pi_{i}\right)$, and reject the ones that are not in the set $S_{i}^{0}=\left{s_{0}: R_{\min }&lt;R\left(\pi_{i}, s_{0}\right)&lt;R_{\max }\right}$. This exhaustive sampling method is orders of magnitude more expensive in terms of sample complexity, so it would not be of practical use. In particular, we can only run it in the easier point-mass maze task. Its performance is shown in the brown curve of Fig. 2a, called "Oracle (rejection sampling)"; training on states sampled in such a manner further improves the learning rate and final performance. Thus we can see that our approximation of using states in $S_{i-1}^{0}$ to find states in $S_{i}^{0}$ leads to some loss in performance, at the benefit of a greatly reduced computation time.</p>
<p>Finally, we compare to another way of generating start states based on the asymmetric self-play method of Sukhbaatar et al. [38]. The basic idea is to train another policy, "Alice", that proposes start states to the learning policy, "Bob". As can be seen, this method performs very poorly in the pointmass maze task, and our investigation shows that "Alice" often gets stuck in a local optimum, leading to poor start states suggestions for "Bob". In the original paper, the method was demonstrated only on discrete action spaces, in which a multi-modal distribution for Alice can be maintained; even in such settings, the authors observed that Alice can easily get stuck in local optima. This problem is exacerbated when moving to continuous action spaces defined by a unimodal Gaussian distribution. See a detailed analysis of these failure modes in Appendix B.3.</p>
<h1>5.3 Brownian motion to generate good starts "nearby" good starts</h1>
<p>Here we evaluate if running our Procedure 2 SampleNearby with the start states estimated as "good" from the previous iteration yields more good starts than running SampleNearby from all start states used in the previous iteration. This can clearly be seen in Figs. 3b-3a for the robotic manipulation tasks.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Fraction of "good starts" generated during training for the robotic manipulation tasks</p>
<h2>6 Conclusions and Future Directions</h2>
<p>We propose a method to automatically adapt the start state distribution on which an agent is trained, such that the performance on the original problem is efficiently optimized. We leverage three assumptions commonly satisfied in simulated tasks to tackle hard goal-oriented problems that state of the art RL methods cannot solve.</p>
<p>A limitation of the current approach is that it generates start states that grow from a single goal uniformly outwards, until they cover the original start state distribution $\operatorname{Unif}\left(S^{0}\right)$. Nevertheless, if the target set of start states $S^{0}$ is far from the goal and we have some prior knowledge, it would be interesting to bias the generated start distributions $\rho_{i}$ towards the desired start distribution. A promising future line of work is to combine the present automatic curriculum based on start state generation with goal generation [5], similar to classical results in planning [39].
It can be observed in the videos of our final policy for the manipulation tasks that the agent has learned to exploit the contacts instead of avoiding them. Therefore, the learning based aspect of the presented method has a huge potential to tackle problems that classical motion planning algorithms could struggle with, such as environments with non-rigid objects or with uncertainties in the task geometric parameters. We also leave as future work to combine our curriculum-generation approach with domain randomization methods [40] to obtain policies that are transferable to the real world.</p>
<h1>Acknowledgments</h1>
<p>This work was supported in part by Berkeley Deep Drive, ONR PECASE N000141612723, Darpa FunLoL. Carlos was also supported by a La Caixa Fellowship. Markus was supported by UKs EPSRC Doctoral Training Award (DTA), and the Hans-Lenze-Foundation.</p>
<h2>References</h2>
<p>[1] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-Dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representation, 2015.
[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[3] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.
[4] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016.
[5] D. Held, X. Geng, C. Florensa, and P. Abbeel. Automatic goal generation for reinforcement learning agents. arXiv preprint arXiv:1705.06366, abs/1705.06366, 2017.
[6] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement learning for continuous control. International Conference on Machine Learning, 2016.
[7] I. Popov, N. Heess, T. Lillicrap, R. Hafner, G. Barth-Maron, M. Vecerik, T. Lampe, Y. Tassa, T. Erez, and M. Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. arXiv preprint arXiv: 1704.03073, 2017.
[8] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference in Machine Learning, volume 99, pages 278-287, 1999.
[9] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International Conference on Machine Learning, pages 41-48. ACM, 2009.
[10] W. Zaremba and I. Sutskever. Learning to execute. CoRR, abs/1410.4615, 2014.
[11] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, 2015.
[12] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu. Automated Curriculum Learning for Neural Networks. arXiv preprint, arXiv:1704.03003, 2017.
[13] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In Advances in Neural Information Processing Systems, pages 1189-1197, 2010.
[14] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann. Self-paced curriculum learning. In AAAI, volume 2, page 6, 2015.
[15] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. Purposive behavior acquisition for a real robot by Vision-Based reinforcement learning. Machine Learning, 1996.
[16] A. Karpathy and M. Van De Panne. Curriculum learning for motor skills. In Canadian Conference on Artificial Intelligence, pages 325-330. Springer, 2012.
[17] J. Schmidhuber. POWER P LAY : Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem. Frontiers in Psychology, 2013.
[18] R. K. Srivastava, B. R. Steunebrink, M. Stollenga, and J. Schmidhuber. Continually adding self-invented problems to the repertoire: First experiments with POWERPLAY. In IEEE International Conference on Development and Learning and Epigenetic Robotics, 2012.</p>
<p>[19] A. Baranes and P.-Y. Oudeyer. Active learning of inverse models with intrinsically motivated goal exploration in robots. Robotics and Autonomous Systems, 61(1), 2013.
[20] S. Sharma and B. Ravindran. Online Multi-Task Learning Using Biased Sampling. arXiv preprint arXiv: 1702.06053, 2017.
[21] S. Sukhbaatar, I. Kostrikov, A. Szlam, and R. Fergus. Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play. arXiv preprint, arXiv: 1703.05407, 2017.
[22] A. Rajeswaran, K. Lowrey, E. Todorov, and S. Kakade. Towards generalization and simplicity in continuous control. arXiv preprint, arXiv:1703.02660, 2017.
[23] R. Tedrake, I. R. Manchester, M. Tobenkin, and J. W. Roberts. Lqr-trees: Feedback motion planning via sums-of-squares verification. The International Journal of Robotics Research, 29 (8):1038-1052, 2010.
[24] R. R. Burridge, A. A. Rizzi, and D. E. Koditschek. Sequential composition of dynamically dexterous robot behaviors. The International Journal of Robotics Research, 1999.
[25] J. A. Bagnell, S. Kakade, A. Y. Ng, and J. Schneider. Policy search by dynamic programming. Advances in Neural Information Processing Systems, 16:79, 2003.
[26] S. Kakade and J. Langford. Approximately Optimal Approximate Reinforcement Learning. International Conference in Machine Learning, 2002.
[27] M. Kearns, Y. Mansour, and A. Y. Ng. A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Machine Learning, 49(2/3):193-208, 2002.
[28] K. Subramanian, C. L. Isbell, Jr., and A. L. Thomaz. Exploration from demonstration for interactive reinforcement learning. In Proceedings of the International Conference on Autonomous Agents \&amp; Multiagent Systems, 2016.
[29] D. P. Bertsekas, D. P. Bertsekas, D. P. Bertsekas, and D. P. Bertsekas. Dynamic programming and optimal control, volume 1. Athena scientific Belmont, MA, 1995.
[30] J. Peters and S. Schaal. Reinforcement learning of motor skills with policy gradients. Neural networks, 21(4):682-697, 2008.
[31] I. Szita and A. Lörincz. Learning tetris using the noisy cross-entropy method. Learning, 18 (12), 2006.
[32] I. Osband, B. Van Roy, and Z. Wen. Generalization and exploration via randomized value functions. In International Conference on Machine Learning, 2016.
[33] S. D. Whitehead. Complexity and Cooperation in Q-Learning. Machine Learning Proceedings 1991, pages 363-367, 1991.
[34] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT press Cambridge, 1998.
[35] M. P. Deisenroth, G. Neumann, J. Peters, et al. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(1-2):1-142, 2013.
[36] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In International Conference on Machine Learning, pages 1889-1897, 2015.
[37] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2012.
[38] S. Sukhbaatar, I. Kostrikov, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. arXiv preprint arXiv:1703.05407, 2017.
[39] J. Kuffner and S. LaValle. RRT-connect: An efficient approach to single-query path planning. In IEEE International Conference on Robotics and Automation, volume 2, 2000.
[40] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World. arXiv preprint, arXiv:1703.06907, 2017.</p>
<h1>A Experiment Implementation Details</h1>
<h2>A. 1 Hyperparameters</h2>
<p>Here we describe the hyperparemeters used for our method. Each iteration, we generate new start states (as described in Section 4.2 and Procedure 2), which we append to the seed states until we have a total of $M=10000$ start states. We then subsample these down to $N_{\text {new }}=200$ new start states. These are appended with $N_{\text {old }}=100$ sampled old start states (as described in Section 4.3 and Procedure 1), and these states are used to initialize our agent when we train our policy. The "Brownian motion" rollouts have a horizon of $T_{B}=50$ timesteps, and the actions taken are random, sampled from a standard normal distribution (e.g. a 0 -mean Gaussian with a covariance $\Sigma=I$ ).</p>
<p>For our method as well as the baselines, we train a $(64,64)$ multi-layer perceptron (MLP) Gaussian policy with TRPO [36], implemented with rllab [6]. We use a TRPO step-size of 0.01 and a $(32,32)$ MLP baseline. For all tasks, we train with a batch size of 50,000 timesteps. All experiments use a maximum horizon of $T=500$ time steps except for the Ant maze experiments that use a maximum horizon of $T=2000$. The episode ends as soon as the agent reaches a goal state. We define the goal set $S^{g}$ to be a ball around the goal state, in which the ball has a radius of 0.03 m for the ring and key tasks, 0.3 m for the point-mass maze task and 0.5 m for the ant-maze task. In our definition of $S_{i}^{0}$, we use $R_{\min }=0.1$ and $R_{\max }=0.9$. We use a discount factor $\gamma=0.998$ for the optimization, in order to encourage the policy to reach the goal as fast as possible.</p>
<h2>A. 2 Performance metric</h2>
<p>The aim of our tasks is to reach a specified goal region $S^{g}$ from all start states $s_{0} \in S^{0}$ that are feasible and within a certain distance of that goal region. Therefore, to evaluate the progress on $\eta_{p_{0}}\left(\pi_{i}\right)$ we need to collect trajectories starting at states uniformly sampled from $S^{0}$. For the pointmass maze navigation task this is straight forward as the designer can give a concrete description of the feasible $(x, y)$ space, so we can uniformly sample from it. Nevertheless, it is not trivial to uniformly sample from all feasible start states for the robotics tasks. In particular, the state space is in joint angles and angular velocities of the 7 DOF arm, but the physical constraints of these contact-rich environments are given by the geometries of the task. Therefore, uniformly sampling from the angular bounds mostly yields infeasible states, with some part of the arm or the endeffector intersecting with other objects in the scene. In order to approximate uniformly sampling from $S^{0}$, we make use of our assumptions (Section 3.3). We simply run our SampleNearby procedure initialized with starts $=\left[s^{g}\right]$ with a very large $M$ and long time horizons $T_{B}$. This large aggregated state data-set is saved and samples from it are used as proxy to $S^{0}$ to evaluate the performance of our algorithm. Figures 4 and 4 show six sampled start states from the data sets used to evaluate the ring task and the key task. These data sets are available at the project website ${ }^{2}$ for future reproducibility and benchmarking.</p>
<p>Given the quasi-static nature of the tasks considered, we generate only initial joint positions, and we set all initial velocities to zero. Generating initial velocities is a fairly simple extension of our approach that we leave for future work.</p>
<h2>B Other methods</h2>
<h2>B. 1 Distance reward shaping</h2>
<p>Although our method is able to train policies with sparse rewards, the policy optimization steps train_pol can use any kind of reward shaping available. To an extent, we already do that by using a discount factor $\gamma$, which motivates the policies to reach the goal as soon as possible. Similar reward modulations could be included to take into account energy penalties or reward shaping from prior knowledge. For example, in the robotics tasks considered in this paper, the goal is defined in terms of a reference state, and hence it seems natural to try to use the distance to this state as an additional penalty to guide learning. However, we have found that this modification does not actually improve training. For the start states near to the goal, the policy can learn to reach the goal simply from the indicator reward introduced in Section 3.2. For the states that are further away, the distance</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" />
(a) Uniformly sampled start states for ring task. There are 39,530 states in the data-set, of which 5,660 have the ring with its hole already in the peg
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Uniformly sampled start states for key task. There are 544,575 states in the data-set, of which 120,784 have the key somewhere inside the key-hole</p>
<p>Figure 4: Samples from the test distribution for the manipulation tasks
to the goal is actually not a useful metric to guide the policy; hence, the distance reward actually guides the policy updates towards a suboptimal local optimum, leading to poor performance. In Fig. 5 we see that the ring task is not much affected by the additional reward, whereas the key task suffers considerably if this reward is added.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Learning curves for the robotics manipulation tasks</p>
<h1>B. 2 Failure cases of Uniform Sampling for maze navigation</h1>
<p>In the case of the maze navigation task, we observe that applying TRPO directly on the original MDP incurs a very high variance across learning curves. We have observed that some policies only learned how to perform well from a certain side of the goal. The reason for this is that our learning algorithm (TRPO) is a batch on-policy method; therefore, at the beginning of learning, uniformly sampling from the state-space might give a batch with very few trajectories that reach the goal and hence it is likely that the successful trajectories all come from one side of the goal. In this case, the algorithm will update the policy to go in the same direction from everywhere, wrongly extrapolating from these very few successful trajectories it received. This is less likely to happen if the trajectories for the batch are collected with a different start state distribution that concentrates more uniformly around the goal, as the better learning progress of the other curves show.</p>
<h2>B. 3 Failure cases of Asymmetric Self-play</h2>
<p>In Section 5.2, we compare the performance of our method to the asymmetric self-play approach of Sukhbaatar et al. [38]. Although such an approach learns faster than the uniform sampling baseline, it gets stuck in a local optimum and fails to learn to reach the goal from more than $40 \%$ of start-states in the point-mass maze task.</p>
<p>As explained above, part of the reason that this method gets stuck in a local optimum is that "Alice" (the policy that is proposing start-states) is represented with a unimodal Gaussian distribution, which is a common representation for policies in continuous action spaces. Thus Alice's policy tends to converge to moving in a single direction. In the original paper, this problem is somewhat mitigated by using a discrete action space, in which a multi-modal distribution for Alice can be maintained. However, even in such a case, the authors of the original paper also observed that Alice tends to converge to a local optimum [38].
A further difficulty for Alice is that her reward function can be sparse, which can be inherently difficult to optimize. Alice's reward is defined as $r_{A}=\max \left(0, t_{B}-t_{A}\right)$, where $t_{A}$ is the time that it takes Alice to reach a given start state from the goal (at which point Alice executes the "stop" action), and $t_{B}$ is the time that it takes Bob to return to the goal from the start state. Based on this reward, the optimal policy for Alice is to find the nearest state for which Bob does not know how to return to the goal; this will lead to a large value for $t_{B}$ with a small value for $t_{A}$. In theory, this should lead to an automatic curriculum of start-states for Bob.
However, in practice, we find that sometimes, Bob's policy might improve faster than Alice's. In such a case, Bob will have learned how to return to the goal from many start states much faster than Alice can reach those start states from the goal. In such cases, we would have that $t_{B}&lt;t_{A}$, and hence $r_{A}=0$. Thus, Alice's rewards are sparse (many actions that Alice takes result in 0 reward) and hence it will be difficult for Alice's policy to improve, leading to a locally optimal policy for Alice. For these reasons, we have observed Alice's policy often getting "stuck," in which Alice is unable to find new start-states to propose for Bob that Bob does not already know how to reach the goal from.
We have implemented a simple environment that illustrates these issues. In this environment, we use a synthetic "Bob" that can reach the goal from any state within a radius $r_{B}$ from the goal. For states within $r_{B}$, Bob can reach the goal in a time proportional to the distance between the state and the goal; in other words, for such states $s_{0} \in\left{s:\left|s-s^{g}\right|&lt;r_{B}, s \in S^{0}\right}$, we have that $t_{B}=\left|s_{0}-s^{g}\right| / v_{B}$, where $\left|s_{0}-s^{g}\right|$ is the distance between state $s_{0}$ and the goal $s^{g}$, and $v_{B}$ is Bob's speed. For states further than $r_{B}$ from the goal, Bob does not know how to reach the goal, and thus $t_{B}$ for such states takes the maximum possible value.
This setup is illustrated in Figure 6. The region shown in red designates the area within $r_{B}$ from the goal, e.g. the set of states from which Bob knows how to reach the goal. On the first iteration, Alice has a random policy (Figure 6a). After 10 iterations of training, Alice has converged to a policy that reaches the location just outside of the set of states from which Bob knows how to reach the goal (Figure 6b). From these states, Alice receives a maximum reward, because $t_{B}$ is very large while $t_{A}$ is low. Note that we also observe the unimodal nature of Alice's policy; Alice has converged to a policy which proposes just one small set of states among all possible states for which she would receive a similar reward.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Simple environment to illustrate asymmetric self-play [38]. The red areas indicate the states from which Bob knows how to reach the goal. The blue points are the start-states proposed by Alice at each iteration (i.e. the states from which Alice performed the stop action)</p>
<p>At this point we synthetically increase $r_{B}$, corresponding to the situation in which Bob learns how to reach the goal from a larger set of states. However, Alice's policy has already converged to reaching a small set of states which were optimal for Bob's previous policy. From these states Alice now receives a reward of 0 , as described above: Bob can return from these states quickly to the goal, so</p>
<p>we have that $t_{B}&lt;t_{A}$ and $r_{A}=0$. Thus, Alice does not receive any reward signal and is not able to improve her policy. Hence, Alice's policy remains stuck at this point and she is not able to find new states to propose to Bob (Figure 6c).</p>
<p>In this simple case, one could attempt to perform various hacks to try to fix the situation, e.g. by artificially increasing Alice's variance, or by resetting Alice to a random policy. However, note that, in a real example, Bob is learning an increasingly complex policy, and so Alice would need to learn an equally complex policy to find a set of states that Bob cannot succeed from; hence, these simple fixes would not suffice to overcome this problem. Fundamentally, the asymmetric nature of the selfplay between Alice and Bob creates a situation in which Alice has a difficult time learning and often gets stuck in a local optimum from which she is unable to improve.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Videos, data sets and code available at: bit.ly/reversecurriculum&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
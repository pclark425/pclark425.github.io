<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8506 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8506</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8506</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-b115c1e1e9e51f8ad7d47b745bc04e29a654b84d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b115c1e1e9e51f8ad7d47b745bc04e29a654b84d" target="_blank">Faithful Chain-of-Thought Reasoning</a></p>
                <p><strong>Paper Venue:</strong> International Joint Conference on Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> Faithful CoT, a reasoning framework involving two stages: Translation and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively, guarantees that the reasoning chain provides a faithful explanation of the final answer.</p>
                <p><strong>Paper Abstract:</strong> While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8506.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8506.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Faithful CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Faithful Chain-of-Thought Reasoning (Faithful CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage reasoning framework that (1) translates a natural language query into an interleaved natural-language (NL) decomposition and executable symbolic-language (SL) program, and (2) deterministically executes the SL program with an external solver, guaranteeing that the produced chain of thought is a faithful explanation of the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex); also evaluated with GPT-4 and other OpenAI models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experiments primarily use OpenAI Codex (code-davinci-002) as the Translator LM; authors also evaluate with text-davinci variants and GPT-4. The LM generates interleaved NL+SL chains; execution is offloaded to deterministic solvers (Python interpreter, Datalog executor, PDDL planner).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought style decomposition (NL subquestions / rationales / dependency graph)', 'Symbolic program generation (SL: Python, Datalog, PDDL)', 'Deterministic symbolic execution (external solver executes SL)', 'Optional ensemble decoding (self-consistency sampling over translations)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Translation stage: LM generates a reasoning chain C that interleaves NL decompositions (subquestions, dependency graphs, rationales) and SL code that implements the subanswers. Problem Solving stage: an external deterministic solver (Python interpreter, Datalog engine, or PDDL planner) executes the SL portion to produce the final answer; self-consistency can be applied by sampling multiple C and majority-voting on the executed answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Compared vs: (a) standard CoT (NL-only, all-at-once); (b) Least-to-Most (modular NL decomposition only); (c) ensemble-based self-consistency. Ablations: removed NL components (No NL, No NL but nudge), removed rationales, removed external solver (No solver), enforced structural constraints on dependency graphs and code (graph validity, no over-dependency, no under-dependency). Also compared to concurrent SL-only methods (PoT, PAL) by using the 'No NL' prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>10 reasoning datasets across 4 domains: Math Word Problems (GSM8K, SVAMP, MultiArith, ASDiv, AQuA), Multi-hop QA (StrategyQA, Date Understanding, Sports Understanding), Planning (SayCan), Relational Inference (CLUTRR). Few-shot settings (6-10 exemplars depending on dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>With code-davinci-002 (greedy): Faithful CoT outperforms baselines on 8/10 datasets (e.g., GSM8K 72.3% vs CoT 63.3%; SVAMP 83.4% vs CoT 77.3%; MultiArith 98.8% vs CoT 96.5%; CLUTRR 58.9% vs CoT 48.5%). With self-consistency (40 samples, T=0.4): Faithful CoT often improves further (GSM8K 80.0% vs CoT 78.0%; CLUTRR 71.9% vs CoT 45.7%). Paper reports relative accuracy gains (Faithful CoT vs vanilla CoT) aggregated: +6.3% on Math Word Problems, +3.4% on Planning, +5.5% on Multi-hop QA, +21.4% on Relational Inference. With GPT-4 as the Translator, Faithful CoT reached few-shot SOTA on 7/10 datasets and achieved >=95.0% on six datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Faithful CoT provides guaranteed faithfulness of the final answer to the generated SL program; interpretability improves. The external solver is pivotal: "No solver" ablation causes large accuracy drops on numeric and relational tasks (e.g., GSM8K -50.8 points). NL comments sometimes do not improve accuracy on certain datasets but are crucial for interpretability and for tasks like CLUTRR; a single 'nudge' line can substantially help on CLUTRR. Constraints enforcing graph validity had little effect; constraints preventing over/under-dependency tended to reduce accuracy. Error analysis: frequent errors include wrong NL subquestions, wrong code generation, semantic understanding errors, generation cutoffs, and dataset/gold-label issues. For knowledge-heavy tasks (StrategyQA), use of less-frequent SL (Datalog) led to syntax and generation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Guaranteeing faithfulness by separating translation (LM) and deterministic execution (solver) yields more interpretable reasoning chains and empirically improves accuracy across diverse reasoning tasks; faithfulness need not trade off with performance and can synergize with accuracy. The solver is critical, and NL decomposition aids interpretability (and sometimes robustness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8506.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8506.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An all-at-once prompting technique that elicits natural-language chain-of-thought explanations from a language model as it answers complex reasoning questions, typically via few-shot exemplars including (Q, chain, A) triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General pre-trained large language models (e.g., GPT-3 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs prompted with NL reasoning exemplars produce an NL sequence of reasoning steps and a final answer in one continuous generation. No deterministic external execution is enforced; the chain and answer are produced jointly by the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['NL Chain-of-Thought (all-at-once generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The LM is prompted with exemplar (Q, NL chain, A) triples and asked to generate an NL reasoning chain and final answer. The chain is not executed by an external solver; there is no faithfulness guarantee between chain and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as baseline across all datasets; compared with Faithful CoT, LtM, and self-consistency ensemble variants in experiments (greedy and self-consistency decoding).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same 10 datasets as Faithful CoT: GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sports, SayCan, CLUTRR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 1 (greedy): GSM8K 63.3%, SVAMP 77.3%, MultiArith 96.5%, ASDiv 80.0%, AQuA 42.1%, SayCan 86.4%, StrategyQA 72.5%, Date 59.9%, Sport 98.6%, CLUTRR 48.5%. With self-consistency: improved on many datasets (e.g., GSM8K 78.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT often generates plausible NL explanations but these explanations are not guaranteed to be faithful; the final answer may not causally follow the chain (unfaithful examples shown). CoT benefits from self-consistency (sampling multiple chains and voting), but still lacks a faithfulness guarantee.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT improves LM reasoning performance versus standard prompting, but generated NL chains are not necessarily faithful explanations of how the model arrived at the answer; Faithful CoT proposes to address this by deterministic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8506.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8506.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency decoding (Ensemble sampling of CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble decoding method that samples multiple chain-of-thought generations from an LM and chooses the final answer by majority vote over executed or predicted answers, intended to mitigate local optimum mistakes of single-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applied with LM Translators (code-davinci-002; text-davinci variants; GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sampling-based ensemble: temperature 0.4 and n=40 generations in the experiments; each generated chain is executed (Faithful CoT: execute SL) or interpreted (CoT: evaluate answer) and majority-vote selects final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Sampling-based ensemble of chains', 'Majority voting on final answers']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple chains-of-thought (or translations to SL) are sampled from the LM; for Faithful CoT the SL parts are executed for each sample and answers are voted; for CoT the final answers from the NL generation are voted. Parameters used: temperature 0.4, 40 samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Comparative experiments: greedy vs self-consistency decoding across Faithful CoT, CoT, and LtM. Reported changes in relative gains when using self-consistency (e.g., Faithful CoT vs CoT: some relative gains increase or decrease across datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Applied across the same 10 datasets. Results reported separately under 'Self-Consistency Decoding' in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Self-consistency improves many methods: e.g., CoT on GSM8K increases from 63.3% (greedy) to 78.0%; Faithful CoT increases from 72.3% to 80.0%. On some datasets self-consistency increased Faithful CoT relative gains (e.g., CLUTRR Faithful CoT: 71.9% vs CoT 45.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency reduces variance from single-sample local optima and can amplify Faithful CoT gains in some datasets; however, on some datasets it changes the relative advantage (e.g., Faithful CoT gains shrink on some MWP datasets under self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Ensemble-based decoding (self-consistency) often improves absolute performance for both CoT and Faithful CoT; Faithful CoT retains advantages under self-consistency on most datasets and sometimes sees even larger relative gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8506.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8506.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most prompting (LtM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modularized prompting method that decomposes a complex question into a sequence of subproblems in natural language and solves them iteratively, but without generating executable symbolic code nor enforcing faithfulness by deterministic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>General LMs (evaluated with code-davinci-002 and others in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Modularized NL decomposition: LM produces successive subquestions and answers in NL which are fed into the next step; no symbolic execution and no explicit solver.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Modular NL decomposition and iterative solving']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Break a problem into smaller NL subquestions solved sequentially by the LM; the method structures reasoning but remains NL-only and thus lacks faithfulness guarantees of Faithful CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Used as baseline across datasets; compared to Faithful CoT (which adds SL + deterministic solver) and to CoT and self-consistency. Also used to create CLUTRR baseline in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same 10 datasets; LtM results reported in Table 1 (greedy and self-consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 1 (greedy): GSM8K 38.3%, SVAMP 80.3%, MultiArith 74.0%, ASDiv 76.5%, AQuA 40.6%, SayCan 77.7%, StrategyQA 72.2%, Date 76.6%, Sport 99.5%, CLUTRR 47.2%. Self-consistency yields small or no improvements for LtM on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LtM's NL decomposition is similar to Faithful CoT's NL component, but without SL and solver it lacks faithfulness and in many datasets underperforms Faithful CoT. On some easy tasks (Sport) LtM performs well (saturated).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Modular NL decomposition alone (as in LtM) improves reasoning but is not sufficient to guarantee faithfulness; adding executable SL + solver (Faithful CoT) provides faithfulness and additional empirical gains on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8506.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8506.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT / PAL (concurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program of Thoughts (PoT) and PAL (Program-aided Language Models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Concurrent methods that generate executable Python programs (SL-only reasoning chains) from NL questions to compute answers; primarily performance-driven approaches that separate computation by program generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks; PAL: Program-aided Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-generating LMs (e.g., Codex variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LMs prompted to generate Python programs that implement the reasoning and compute numeric answers. These methods typically output SL-only chains (no structured NL decomposition) and rely on program execution to produce answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['SL-only program generation (Python)', 'Deterministic execution of generated programs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LMs produce only executable code (no interleaved NL rationale). The program is executed to produce the final answer, separating computation from reasoning similar to Faithful CoT but without the structured NL decomposition for interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>The paper reimplemented PoT/PAL by using the 'No NL' prompt ablation of Faithful CoT to compare empirically on the same datasets with code-davinci-002 (greedy). Figure 8 compares Faithful CoT to PoT/PAL across 10 datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Primarily math and symbolic reasoning datasets; in this paper re-evaluated on the same 10 datasets (MWP, Planning, Multi-hop QA, CLUTRR) via the 'No NL' ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Empirical comparison (Figure 8): On 6/10 datasets (most MWP, SayCan, Date) PoT/PAL and Faithful CoT have very close accuracy (<2% difference). On AQuA PoT/PAL outperforms Faithful CoT; on StrategyQA, Sports Understanding, and CLUTRR Faithful CoT outperforms PoT/PAL. Overall performance is dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PoT/PAL (SL-only) are strong for numeric/symbolic tasks where Python is well-supported in pretraining. Faithful CoT tends to have advantages when tasks require external knowledge (StrategyQA) or when SL (e.g., Datalog or custom relational expressions) is less represented in LM pretraining; Faithful CoT's NL decomposition increases interpretability and may help debugging by non-programmers.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>SL-only program-generation approaches achieve competitive performance; adding structured NL comments (as in Faithful CoT) improves interpretability and can give empirical advantages on some benchmarks, especially those needing domain-specific SL or external knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Faithful Chain-of-Thought Reasoning', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided Language Models <em>(Rating: 2)</em></li>
                <li>Solving Quantitative Reasoning Problems with Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8506",
    "paper_id": "paper-b115c1e1e9e51f8ad7d47b745bc04e29a654b84d",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "Faithful CoT",
            "name_full": "Faithful Chain-of-Thought Reasoning (Faithful CoT)",
            "brief_description": "A two-stage reasoning framework that (1) translates a natural language query into an interleaved natural-language (NL) decomposition and executable symbolic-language (SL) program, and (2) deterministically executes the SL program with an external solver, guaranteeing that the produced chain of thought is a faithful explanation of the final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex); also evaluated with GPT-4 and other OpenAI models",
            "model_description": "Experiments primarily use OpenAI Codex (code-davinci-002) as the Translator LM; authors also evaluate with text-davinci variants and GPT-4. The LM generates interleaved NL+SL chains; execution is offloaded to deterministic solvers (Python interpreter, Datalog executor, PDDL planner).",
            "reasoning_methods": [
                "Chain-of-Thought style decomposition (NL subquestions / rationales / dependency graph)",
                "Symbolic program generation (SL: Python, Datalog, PDDL)",
                "Deterministic symbolic execution (external solver executes SL)",
                "Optional ensemble decoding (self-consistency sampling over translations)"
            ],
            "reasoning_methods_description": "Translation stage: LM generates a reasoning chain C that interleaves NL decompositions (subquestions, dependency graphs, rationales) and SL code that implements the subanswers. Problem Solving stage: an external deterministic solver (Python interpreter, Datalog engine, or PDDL planner) executes the SL portion to produce the final answer; self-consistency can be applied by sampling multiple C and majority-voting on the executed answers.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Compared vs: (a) standard CoT (NL-only, all-at-once); (b) Least-to-Most (modular NL decomposition only); (c) ensemble-based self-consistency. Ablations: removed NL components (No NL, No NL but nudge), removed rationales, removed external solver (No solver), enforced structural constraints on dependency graphs and code (graph validity, no over-dependency, no under-dependency). Also compared to concurrent SL-only methods (PoT, PAL) by using the 'No NL' prompt variant.",
            "task_or_benchmark": "10 reasoning datasets across 4 domains: Math Word Problems (GSM8K, SVAMP, MultiArith, ASDiv, AQuA), Multi-hop QA (StrategyQA, Date Understanding, Sports Understanding), Planning (SayCan), Relational Inference (CLUTRR). Few-shot settings (6-10 exemplars depending on dataset).",
            "performance_results": "With code-davinci-002 (greedy): Faithful CoT outperforms baselines on 8/10 datasets (e.g., GSM8K 72.3% vs CoT 63.3%; SVAMP 83.4% vs CoT 77.3%; MultiArith 98.8% vs CoT 96.5%; CLUTRR 58.9% vs CoT 48.5%). With self-consistency (40 samples, T=0.4): Faithful CoT often improves further (GSM8K 80.0% vs CoT 78.0%; CLUTRR 71.9% vs CoT 45.7%). Paper reports relative accuracy gains (Faithful CoT vs vanilla CoT) aggregated: +6.3% on Math Word Problems, +3.4% on Planning, +5.5% on Multi-hop QA, +21.4% on Relational Inference. With GPT-4 as the Translator, Faithful CoT reached few-shot SOTA on 7/10 datasets and achieved &gt;=95.0% on six datasets.",
            "qualitative_findings": "Faithful CoT provides guaranteed faithfulness of the final answer to the generated SL program; interpretability improves. The external solver is pivotal: \"No solver\" ablation causes large accuracy drops on numeric and relational tasks (e.g., GSM8K -50.8 points). NL comments sometimes do not improve accuracy on certain datasets but are crucial for interpretability and for tasks like CLUTRR; a single 'nudge' line can substantially help on CLUTRR. Constraints enforcing graph validity had little effect; constraints preventing over/under-dependency tended to reduce accuracy. Error analysis: frequent errors include wrong NL subquestions, wrong code generation, semantic understanding errors, generation cutoffs, and dataset/gold-label issues. For knowledge-heavy tasks (StrategyQA), use of less-frequent SL (Datalog) led to syntax and generation errors.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Guaranteeing faithfulness by separating translation (LM) and deterministic execution (solver) yields more interpretable reasoning chains and empirically improves accuracy across diverse reasoning tasks; faithfulness need not trade off with performance and can synergize with accuracy. The solver is critical, and NL decomposition aids interpretability (and sometimes robustness).",
            "uuid": "e8506.0",
            "source_info": {
                "paper_title": "Faithful Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "An all-at-once prompting technique that elicits natural-language chain-of-thought explanations from a language model as it answers complex reasoning questions, typically via few-shot exemplars including (Q, chain, A) triples.",
            "citation_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "General pre-trained large language models (e.g., GPT-3 family)",
            "model_description": "LMs prompted with NL reasoning exemplars produce an NL sequence of reasoning steps and a final answer in one continuous generation. No deterministic external execution is enforced; the chain and answer are produced jointly by the LM.",
            "reasoning_methods": [
                "NL Chain-of-Thought (all-at-once generation)"
            ],
            "reasoning_methods_description": "The LM is prompted with exemplar (Q, NL chain, A) triples and asked to generate an NL reasoning chain and final answer. The chain is not executed by an external solver; there is no faithfulness guarantee between chain and final answer.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as baseline across all datasets; compared with Faithful CoT, LtM, and self-consistency ensemble variants in experiments (greedy and self-consistency decoding).",
            "task_or_benchmark": "Same 10 datasets as Faithful CoT: GSM8K, SVAMP, MultiArith, ASDiv, AQuA, StrategyQA, Date, Sports, SayCan, CLUTRR.",
            "performance_results": "Table 1 (greedy): GSM8K 63.3%, SVAMP 77.3%, MultiArith 96.5%, ASDiv 80.0%, AQuA 42.1%, SayCan 86.4%, StrategyQA 72.5%, Date 59.9%, Sport 98.6%, CLUTRR 48.5%. With self-consistency: improved on many datasets (e.g., GSM8K 78.0%).",
            "qualitative_findings": "CoT often generates plausible NL explanations but these explanations are not guaranteed to be faithful; the final answer may not causally follow the chain (unfaithful examples shown). CoT benefits from self-consistency (sampling multiple chains and voting), but still lacks a faithfulness guarantee.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT improves LM reasoning performance versus standard prompting, but generated NL chains are not necessarily faithful explanations of how the model arrived at the answer; Faithful CoT proposes to address this by deterministic execution.",
            "uuid": "e8506.1",
            "source_info": {
                "paper_title": "Faithful Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-Consistency decoding (Ensemble sampling of CoT)",
            "brief_description": "An ensemble decoding method that samples multiple chain-of-thought generations from an LM and chooses the final answer by majority vote over executed or predicted answers, intended to mitigate local optimum mistakes of single-shot generation.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "use",
            "model_name": "applied with LM Translators (code-davinci-002; text-davinci variants; GPT-4)",
            "model_description": "Sampling-based ensemble: temperature 0.4 and n=40 generations in the experiments; each generated chain is executed (Faithful CoT: execute SL) or interpreted (CoT: evaluate answer) and majority-vote selects final answer.",
            "reasoning_methods": [
                "Sampling-based ensemble of chains",
                "Majority voting on final answers"
            ],
            "reasoning_methods_description": "Multiple chains-of-thought (or translations to SL) are sampled from the LM; for Faithful CoT the SL parts are executed for each sample and answers are voted; for CoT the final answers from the NL generation are voted. Parameters used: temperature 0.4, 40 samples.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Comparative experiments: greedy vs self-consistency decoding across Faithful CoT, CoT, and LtM. Reported changes in relative gains when using self-consistency (e.g., Faithful CoT vs CoT: some relative gains increase or decrease across datasets).",
            "task_or_benchmark": "Applied across the same 10 datasets. Results reported separately under 'Self-Consistency Decoding' in Table 1.",
            "performance_results": "Self-consistency improves many methods: e.g., CoT on GSM8K increases from 63.3% (greedy) to 78.0%; Faithful CoT increases from 72.3% to 80.0%. On some datasets self-consistency increased Faithful CoT relative gains (e.g., CLUTRR Faithful CoT: 71.9% vs CoT 45.7%).",
            "qualitative_findings": "Self-consistency reduces variance from single-sample local optima and can amplify Faithful CoT gains in some datasets; however, on some datasets it changes the relative advantage (e.g., Faithful CoT gains shrink on some MWP datasets under self-consistency).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Ensemble-based decoding (self-consistency) often improves absolute performance for both CoT and Faithful CoT; Faithful CoT retains advantages under self-consistency on most datasets and sometimes sees even larger relative gains.",
            "uuid": "e8506.2",
            "source_info": {
                "paper_title": "Faithful Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most prompting (LtM)",
            "brief_description": "A modularized prompting method that decomposes a complex question into a sequence of subproblems in natural language and solves them iteratively, but without generating executable symbolic code nor enforcing faithfulness by deterministic execution.",
            "citation_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "General LMs (evaluated with code-davinci-002 and others in this paper)",
            "model_description": "Modularized NL decomposition: LM produces successive subquestions and answers in NL which are fed into the next step; no symbolic execution and no explicit solver.",
            "reasoning_methods": [
                "Modular NL decomposition and iterative solving"
            ],
            "reasoning_methods_description": "Break a problem into smaller NL subquestions solved sequentially by the LM; the method structures reasoning but remains NL-only and thus lacks faithfulness guarantees of Faithful CoT.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Used as baseline across datasets; compared to Faithful CoT (which adds SL + deterministic solver) and to CoT and self-consistency. Also used to create CLUTRR baseline in this study.",
            "task_or_benchmark": "Same 10 datasets; LtM results reported in Table 1 (greedy and self-consistency).",
            "performance_results": "Table 1 (greedy): GSM8K 38.3%, SVAMP 80.3%, MultiArith 74.0%, ASDiv 76.5%, AQuA 40.6%, SayCan 77.7%, StrategyQA 72.2%, Date 76.6%, Sport 99.5%, CLUTRR 47.2%. Self-consistency yields small or no improvements for LtM on many datasets.",
            "qualitative_findings": "LtM's NL decomposition is similar to Faithful CoT's NL component, but without SL and solver it lacks faithfulness and in many datasets underperforms Faithful CoT. On some easy tasks (Sport) LtM performs well (saturated).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Modular NL decomposition alone (as in LtM) improves reasoning but is not sufficient to guarantee faithfulness; adding executable SL + solver (Faithful CoT) provides faithfulness and additional empirical gains on many tasks.",
            "uuid": "e8506.3",
            "source_info": {
                "paper_title": "Faithful Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "PoT / PAL (concurrent)",
            "name_full": "Program of Thoughts (PoT) and PAL (Program-aided Language Models)",
            "brief_description": "Concurrent methods that generate executable Python programs (SL-only reasoning chains) from NL questions to compute answers; primarily performance-driven approaches that separate computation by program generation.",
            "citation_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks; PAL: Program-aided Language Models",
            "mention_or_use": "mention",
            "model_name": "code-generating LMs (e.g., Codex variants)",
            "model_description": "LMs prompted to generate Python programs that implement the reasoning and compute numeric answers. These methods typically output SL-only chains (no structured NL decomposition) and rely on program execution to produce answers.",
            "reasoning_methods": [
                "SL-only program generation (Python)",
                "Deterministic execution of generated programs"
            ],
            "reasoning_methods_description": "LMs produce only executable code (no interleaved NL rationale). The program is executed to produce the final answer, separating computation from reasoning similar to Faithful CoT but without the structured NL decomposition for interpretability.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "The paper reimplemented PoT/PAL by using the 'No NL' prompt ablation of Faithful CoT to compare empirically on the same datasets with code-davinci-002 (greedy). Figure 8 compares Faithful CoT to PoT/PAL across 10 datasets.",
            "task_or_benchmark": "Primarily math and symbolic reasoning datasets; in this paper re-evaluated on the same 10 datasets (MWP, Planning, Multi-hop QA, CLUTRR) via the 'No NL' ablation.",
            "performance_results": "Empirical comparison (Figure 8): On 6/10 datasets (most MWP, SayCan, Date) PoT/PAL and Faithful CoT have very close accuracy (&lt;2% difference). On AQuA PoT/PAL outperforms Faithful CoT; on StrategyQA, Sports Understanding, and CLUTRR Faithful CoT outperforms PoT/PAL. Overall performance is dataset-dependent.",
            "qualitative_findings": "PoT/PAL (SL-only) are strong for numeric/symbolic tasks where Python is well-supported in pretraining. Faithful CoT tends to have advantages when tasks require external knowledge (StrategyQA) or when SL (e.g., Datalog or custom relational expressions) is less represented in LM pretraining; Faithful CoT's NL decomposition increases interpretability and may help debugging by non-programmers.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "SL-only program-generation approaches achieve competitive performance; adding structured NL comments (as in Faithful CoT) improves interpretability and can give empirical advantages on some benchmarks, especially those needing domain-specific SL or external knowledge.",
            "uuid": "e8506.4",
            "source_info": {
                "paper_title": "Faithful Chain-of-Thought Reasoning",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "rating": 2
        },
        {
            "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "rating": 2
        },
        {
            "paper_title": "PAL: Program-aided Language Models",
            "rating": 2
        },
        {
            "paper_title": "Solving Quantitative Reasoning Problems with Language Models",
            "rating": 1
        }
    ],
    "cost": 0.01839875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Faithful Chain-of-Thought Reasoning</h1>
<p>Qing Lyu * Shreya Havaldar<em> Adam Stein</em> Li Zhang<br>Delip Rao Eric Wong Marianna Apidianaki Chris Callison-Burch<br>University of Pennsylvania<br>{lyuqing, shreyah, steinad, zharry, exwong, marapi, ccb}@seas.upenn.edu, deliprao@gmail.com</p>
<h4>Abstract</h4>
<p>While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of $6.3 \%$ on Math Word Problems (MWP), 3.4\% on Planning, 5.5\% on Multi-hop Question Answering (QA), and 21.4\% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 datasets (with $95.0+$ accuracy on 6 of them), showing a strong synergy between faithfulness and accuracy. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Complex reasoning tasks, such as commonsense reasoning and math reasoning, have long been the Achilles heel of LMs (Bengio, 2019), until a recent line of work on Chain-of-Thought (CoT) reasoning (Wei et al., 2022; Wang et al., 2022, i.a.) brought striking performance gains. CoT prompts an LM to generate a reasoning chain along with the answer, given only a few in-context exemplars.</p>
<p>Besides performance improvement, CoT is also claimed to "provide an interpretable window into the behavior of the model" (Wei et al., 2022). However, it is unclear how accurately these reason-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of unfaithful output from CoT prompting (Wei et al., 2022) on GSM8K. The answer (green) does not follow from the reasoning chain (blue).
ing chains reflect the underlying reasoning process behind the model's prediction, namely, how faithful they are as explanations (Jacovi and Goldberg, 2020, i.a.). In standard CoT, faithfulness is not guaranteed and even systematically violated (Turpin et al., 2023), as the final answer does not necessarily follow from the generated reasoning chain. In other words, CoT can "lie" about the model's true reasoning process. Figure 1 exemplifies such an unfaithful CoT generation from Wei et al. (2022) on GSM8K: the answer " 0 " is not even mentioned in the reasoning chain. This, along with more examples in Appendix B.1, illustrates that standard CoT does not provide interpretability of how the model predicts the answer.</p>
<p>The lack of faithfulness in CoT can be dangerous in high-stake applications because it may mislead people into believing that the model is selfinterpretable, while there is no actual causal relationship between the reasoning chain and the answer. Even worse, when an unfaithful explanation looks plausible (i.e., convincing to humans) (Jacovi and Goldberg, 2020), this makes it easier for people (e.g., legal practitioners) to over-trust the model (e.g., a recidivism predictor) even if it has implicit biases (e.g., against racial minorities) (Pruthi et al., 2020; Slack et al., 2020).</p>
<p>To address this concern, we propose Faithful CoT, a reasoning framework where the answer is the result of deterministically executing the reasoning chain. Specifically, we break down a com-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An overview of our Faithful CoT framework, consisting of Translation, where an LM translates a query (in NL/Natural Language) into a reasoning chain (which interleaves NL and SL/Symbolic Language), and Problem Solving, where an external solver executes the reasoning chain to derive the answer.
plex reasoning task into two stages: Translation and Problem Solving (Figure 2). During Translation, an LM translates a query into a reasoning chain, which interleaves NL and Symbolic Language (SL). The NL component decomposes the original query into multiple simpler, interdependent subproblems. Then, each subproblem is tackled in a task-dependent SL, such as Python, Datalog, or Planning Domain Definition Language (PDDL). In the Problem Solving stage, the reasoning chain is executed by a deterministic solver, e.g., a Python/Datalog interpreter, or a PDDL planner, to derive the answer.</p>
<p>Our reasoning chain (outcome of Translation) is guaranteed to provide a faithful explanation of how the final answer is produced (outcome of Problem Solving), therefore making our method more interpretable than standard CoT methods. ${ }^{2}$ While interpretability is not the same as correctness (i.e. our method can reveal the reasoning process behind both correct and wrong answers), we find that it does empirically improve correctness: when evaluated on 10 reasoning datasets from 4 diverse domains (MWP, Planning, Multi-hop QA, and Relational Inference), Faithful CoT brings consistent performance gains over three existing baselines, across different LMs and decoding strategies. With Codex, our approach outperforms vanilla CoT on 9 of the 10 datasets, with a relative accuracy gain of $6.3 \%$ on MWP, $3.4 \%$ on Planning, $5.5 \%$ on Multihop QA, and $21.4 \%$ on Relational Inference. With GPT-4, our method sets the new SOTA few-shot</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>performance on 7 datasets, with $95.0+$ accuracy on 6 of them. This suggests that interpretability does not have to come at the cost of performance; instead, there exists a strong synergy in between.</p>
<p>Our key contributions are as follows:
(a) We propose Faithful CoT, a framework that decomposes reasoning into Translation and Problem Solving. The reasoning chain interleaves userunderstandable natural language comments and executable symbolic language programs, thus providing faithful interpretability of how the model arrives at the answer.
(b) Our approach is generalizable to multiple domains beyond arithmetic reasoning and simple symbolic reasoning, thanks to its flexible integration with any choice of SL and external solver. We set the new SOTA performance on 7 out of the 10 reasoning datasets, showing a strong synergy between faithfulness and accuracy.
(c) We provide an extensive analysis of the strengths and weaknesses of our method, showing its generalizability across LMs, robustness to the choice of exemplars and prompt phrasing, the pivotal role of the solver, the plausibility of generated reasoning chains, as well as frequent error patterns where it still struggles.</p>
<h2>2 Related Work</h2>
<p>Faithfulness. In interpretability, faithfulness (also called fidelity or reliability) means that an explanation should "accurately represent the reasoning process behind the model's prediction", which is a fundamental requirement of an explanation (Harrington et al., 1985; Ribeiro et al., 2016; Gilpin et al., 2018; Jacovi and Goldberg, 2020). ${ }^{3}$ It should be contrasted with plausibility (a.k.a. persuasiveness or understandability), which refers to "how convincing an explanation is to humans" (Herman, 2019; Jacovi and Goldberg, 2020). In the context of CoT prompting, a faithful reasoning chain needs to accurately reflect how the model arrives at the final answer, whereas a plausible reasoning chain is one that looks reasonable and coherent to humans. Standard CoT (Wei et al., 2022) generates the rea-</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>soning chain in pure NL, which may often look plausible; nevertheless, the final answer does not need to causally follow from the reasoning chain, thus not guaranteeing faithfulness.
Chain-of-Thought-style prompting. In CoTstyle prompting, given a complex question $Q$, an LM is prompted to generate a reasoning chain $C$ along with the final answer $A$. Specifically, the prompt consists of a few examples of $(Q, C, A)$ triples, called in-context exemplars. This allows pre-trained LMs (e.g., GPT-3 (Brown et al., 2020)) to solve unseen questions with much higher accuracy than standard prompting, where the exemplars do not contain the reasoning chain $C$.</p>
<p>We create a taxonomy of existing CoT-style prompting methods into three types: all-at-once, ensemble-based, and modularized. All-at-once prompting means that the LM produces $C$ and $A$ as one continuous string, without any dependencies or constraints in between. Scratchpad (Nye et al., 2021), standard CoT (Wei et al., 2022), and "Let's think step by step" (Kojima et al., 2022), are all examples of this kind. Ensemble-based prompting is designed to overcome the local optimality issue of the one-shot generation in previous methods by sampling multiple $(C, A)$ pairs and choosing the best answer via strategies like majority voting. Examples include Self-Consistent CoT (Wang et al., 2022), Minerva (Lewkowycz et al., 2022), and DIVERSE (Li et al., 2022), which differ mainly in the voting granularity and the underlying LM. Modularized methods break down $Q$ into subproblems and then conquer them individually (Jung et al., 2022; Qian et al., 2022, i.a.). In particular, Least-to-Most prompting (Zhou et al., 2022) has a similar question decomposition process to ours, but there is still no faithfulness guarantee since the reasoning chain is entirely in NL.</p>
<p>Concurrent with our work, Chen et al. (2022) and Gao et al. (2022) both generate Python programs (i.e., SL-only reasoning chains) to derive the answer. We want to highlight the following qualitative differences: ${ }^{4}$ (a) In terms of motivation, our approach is interpretability-driven, whereas theirs are performance-driven. (b) Our reasoning chain involves a structured decomposition of the problem in NL, allowing users without a programming background to better understand and potentially interact with the system. (c) They only use Python as the SL and only tackle math and simple symbolic</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>reasoning tasks, whereas we demonstrate the generalizability of our approach to multiple symbolic languages and various other domains. In particular, we innovatively recast a diverse set of realistic tasks (Planning, Multi-hop QA, and Relational Inference) into a symbolic representation, which allows us to tackle them with a single framework. (d) We perform a more comprehensive analysis compared to previous work, especially a human evaluation of the reasoning chain correctness.</p>
<h2>3 Method</h2>
<p>Our method, Faithful CoT, is a 2-stage pipeline, as seen in Figure 2. Like previous CoT-style work, our prompt consists of $(Q, C, A)$ triples. Notable differences lie in our unique interleaving of NL (natural language) and SL (symbolic language) in $C$, as well as the way we derive the final answer $A$.</p>
<p>In the Translation stage, given a complex query $Q$ in NL, we prompt an LM to translate it into a reasoning chain $C$, which interleaves NL comments and SL programs. The NL component decomposes the original query into multiple simpler, interdependent subproblems. Then, each subproblem is tackled in a task-dependent SL, such as Python, Datalog, or PDDL. In the Problem Solving stage, we call a deterministic external solver, e.g., a Python interpreter, a Datalog executor, or PDDL planner, depending on the task, to obtain the answer $A$ from the reasoning chain $C$. As shown in Figure 3, we define $C_{N L}$ to be the NL component (black) and $C_{S L}$ to be the SL component (blue) in $C$. Though we separate the two components notationally, they are interleaved in the generation. Using this approach, $C$ is guaranteed to be a faithful model explanation, since our final $A$ is the result of deterministically executing $C_{S L}$. Moreover, $C_{N L}$ allows the user to better understand the reasoning process. ${ }^{5}$</p>
<p>We apply this method to 4 types of complex reasoning tasks: MWP, Multi-hop QA, Planning, and Relational Inference. Next, we will illustrate how our method works for each of them, with examples from Figure 3.</p>
<h3>3.1 Math Word Problems (MWP)</h3>
<p>Given a grade-school math question $Q$ written in NL ("If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?", shown in green in Figure 3), we want to</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Examples from each task (Math Word Problems, Multi-hop QA, Relational Inference, and Planning) showing our 2-stage Translation and Problem Solving pipeline.
obtain $A$ as a real-valued number (5). In the Translation stage, we prompt the LM to take in $Q$ and generate a reasoning chain $C$, which interleaves $C_{N L}$ and $C_{S L}$. Specifically, the $C_{N L}$ component consists of three types of information:
(a) Subquestions: $Q$ is broken down into multiple smaller-scale subquestions, e.g., "1. how many cars are there in the beginning?", "2. how many cars arrive?", and "3. how many cars are in the parking lot?".
(b) Dependency Graph: Each subquestion can either be answered directly via context (subquestions 1 and 2 are "independent") or rely on answers to previous subquestions (subquestion 3 "depends on 1 and 2 ").
(c) Rationales: Each subquestion is accompanied with rationale(s) to support the answer (the "support" field). The rationales can be either a subset of the original context (" 2 more cars arrive") or any external knowledge ("there are 7 days in a week") relevant to the subquestion.</p>
<p>Each subquestion and its corresponding dependencies and rationales inform the subsequent generation of $C_{S L}$. In our example in Figure 3, $C_{S L}$ consists of Python code generated to answer each subquestion in $C_{N L}$. During the Problem Solving stage, we execute $C_{S L}$ using our solver, a Python interpreter, to derive $A$ ( 5 cars in the end).</p>
<h3>3.2 Multi-hop QA</h3>
<p>Given a complex question $Q$ that involves multiple steps of reasoning (e.g., "Would a pear sink in water?", shown in red in Figure 3), we want to obtain the answer $A$ as a Boolean value or string value variable. Similar to our MWP task formulation, $C$ interleaves $C_{N L}$ (NL comments), and $C_{S L}$ (symbolic program). Depending on the nature of the task, the format of the reasoning chain $C$ is slightly different: for some datasets, the LM first generates all subquestions and their answers in NL, and then represents these answers as SL to derive $A$ (see Figure 3); for others, the LM interleaves the NL subquestions and the SL program, similar to the case of MWP (see Table 14 and Table 15 for examples). In terms of SL, we use both Python and Datalog, also depending on the dataset. As Multi-hop QA problems involve multi-step reasoning to solve, $C_{S L}$ often utilizes Boolean algebra and string comparisons (in Python) along with relation definitions and logic programming (in Datalog). We use their corresponding interpreter as our deterministic solver to execute $C_{S L}$ and obtain $A$.</p>
<p>In the example from Figure 3, the LM first generates the subquestions, "1. What is the density of a pear?" and "2. What is the density of water?", which are individually answered</p>
<p>in NL. The answers ("Water has a density of $1 \mathrm{~g} / \mathrm{cm}^{3}$ ") are converted to Datalog statements (Has_density("water", 1)), which are then combined to formalize the truth condition of the final answer. Finally, we execute the Datalog program to determine that a pear would not sink in water.</p>
<h3>3.3 Planning</h3>
<p>In a user-robot interaction scenario, given a household task query $Q$ from a user, we want to come up with a plan of actions $A$ that the robot should take in order to accomplish the task. For example, in Figure 3, given user query "I spilled my coke on the table, could you throw it away and bring something to clean with?", a possible plan can be "find(coke), pick(coke), find(trash), put(coke) ...". In the Translation stage, an LM translates $Q$ into $C$, consisting of $C_{N L}$ (which breaks down $Q$ into subtasks) and $C_{S L}$ (which represents the subtasks as a symbolic goal in PDDL ${ }^{6}$ - a language to define and solve classical planning problems). Figure 3 shows this translation, with $C_{S L}$ in blue and $C_{N L}$ in black. Finally, we call a PDDL Planner as the deterministic solver to obtain $A$, a plan to accomplish the goal $C_{S L}$ under the predefined scenario.</p>
<h3>3.4 Relational Inference</h3>
<p>Given a relational inference problem $Q$ written in NL, we want to obtain $A$ as a string-valued variable. For example, the CLUTRR (Sinha et al., 2019) dataset involves inferring the family relationship (e.g., "grandson") between two people from a short story (e.g., "[Gabrielle] drove her daughter [Dorothy] to the hospital. [Dorothy]'s son [Vincent] showed up shortly after. How is [Vincent] related to [Gabrielle]?", shown in yellow in Figure 3). During the Translation stage, we prompt the LM to generate $C$, consisting of $C_{N L}$ and $C_{S L}$. Similar to previous tasks, $C_{N L}$ breaks down $Q$ into subquestions ("How is [Vincent] related to [Dorothy]" and "How is [Dorothy] related to [Gabrielle]"), as well as provide input extracts as rationales to support the answer ("[Dorothy]'s son [Vincent] showed up shortly after", etc.). Each subquestion in $C_{N L}$ is answered in $C_{S L}$ via a relational expression representing the relation between the mentioned entities, for example, relation(Vincent, Dorothy $)=$ son denotes that Vincent is Dorothy's</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>son. In the Problem Solving stage, our solver is a simple relational inference engine that relies on a set of transitivity rules provided by Zhang et al. (2022) among possible family relationships, e.g., son@daughter=grandson (the son of one's daughter is one's grandson). Our solver recursively applies these rules on $C_{S L}$ to derive $A$, and determine that Vincent is Gabrielle's grandson.</p>
<h2>4 Experimental setup</h2>
<h3>4.1 Datasets</h3>
<p>Here, we summarize the evaluation datasets used for each domain. We select the same number ( 6 to 10, depending on the task) of exemplars as in Wei et al. (2022) to form our few-shot prompt, which can be found in our repository. Unless otherwise stated, we use the official splits: training set for exemplar selection, validation set for prompt tuning, and test set for evaluation. ${ }^{7}$
Math Word Problems (MWP). We follow Wei et al. (2022) and consider the same five MWP benchmarks: GSM8K (Cobbe et al., 2021), SVAMP (Patel et al., 2021), MultiArith (Roy and Roth, 2015), ASDiv (Miao et al., 2020), and AQuA (Ling et al., 2017). For all datasets, the input question is phrased in NL. The answer is a string-valued mathematical expression for AQuA, and one or more integer(s) for all other datasets. We use the same 8 -shot prompt for all datasets except AQuA.
Multi-hop QA. We consider the three datasets: StrategyQA (Geva et al., 2021), a dataset of opendomain questions that require an implicit multistep strategy to answer, e.g., "Did Aristotle use a laptop?" involves answering "1. When did Aristotle live?", "2. When was the laptop invented?", and "3. Is #2 before #1?"; Date Understanding from BIG-bench (BIG-Bench collaboration, 2021), which asks the model to infer a date from a context, by performing computation on relative periods of time; and finally, Sports Understanding from BIG-bench, which asks the model to decide whether an artificially constructed statement related to sports is plausible or implausible. Since the latter two datasets do not have a training set, we follow Wei et al. (2022) and select 10 examples from the test set to form the prompt and use the rest for evaluation.
Planning. We use the SayCan dataset (Ahn et al., 2022), which assumes a scenario of a robot operat-</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: A sample output for a math question from three baselines and Faithful CoT (our method). The ground-truth answer is 280 , and only our method correctly computes the answer.
ing in a kitchen, helping the user with household tasks, e.g., "bring a coke to the table". There are a number of locations and objects that the robot can interact with. The robot can only perform a fixed set of actions, including find, pick, and put. The task is to map a user query in NL to a plan of predefined actions. Following Wei et al. (2022), we manually write 7 exemplars, since no training set is provided.
Relational inference. We use the CLUTRR (Sinha et al., 2019) benchmark described in Section 3.4. The dataset has multiple splits based on the number of intermediate steps $K$ required to reach the answer. We construct the prompt using 8 exemplars with $K \in{2,3}$, and test the models on the remaining examples with $K$ up to 10 .</p>
<h3>4.2 Evaluation Metrics</h3>
<p>We evaluate the model performance with final answer accuracy as the main metric. Following previous work (Wei et al., 2022; Wang et al., 2022; Chen et al., 2022), for all MWP datasets (except AQuA) where the answer contains integer(s), a correct answer is defined as the exact match between the prediction and the ground truth both converted to the nearest integer; for StrategyQA and Sports Understanding where the answer is a Boolean value, it is defined as the exact match between the prediction and the ground truth both evaluated as a Boolean variable; for SayCan, the generated plan is considered correct if it is among the ground truth plans; for all other datasets, we rely on the exact match between the prediction string and the ground truth string. Additionally, we evaluate the human-rated plausibility of the reasoning chain in Appendix D.</p>
<h3>4.3 Baselines</h3>
<p>We compare our method to three other few-shot prompting baselines, shown in Figure 4: standard prompting, popularized by Brown et al. (2020), with demonstrations of only the question and the answer; CoT (Wei et al., 2022), which additionally includes an NL reasoning chain; and Least-to-Most (LtM) (Zhou et al., 2022), which decomposes the question in NL but does not involve SL. All prompting methods are compared under two decoding strategies: greedy decoding, where the LM samples the most probable next token from the vocabulary (i.e., temperature $=0.0$ ); and selfconsistency decoding (Wang et al., 2022), where the LM generates multiple reasoning chains and chooses the final chain based on majority voting on the evaluated answer (we use a temperature of 0.4 and 40 generations for all datasets). ${ }^{8}$ We reproduce the baseline results ourselves in cases when they are not reported on certain tasks or when we clean the test set.</p>
<h3>4.4 LMs</h3>
<p>We use OpenAI Codex (Chen et al., 2021) (code-davinci-002) in Section 5 and experiment with four other code-generation LMs in Appendix C.3.</p>
<h2>5 Results</h2>
<p>Our results on all datasets are shown in Table 1. With code-davinci-002 as the Translator, Faithful CoT outperforms all baselines across the vast</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Math Word Problems</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Planning <br> SayCan</th>
<th style="text-align: center;">Multi-hop QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relation CLUTRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Sport</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Greedy Decoding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">72.1</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">42.0</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">59.9</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">76.5</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">47.2</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">63.0</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">58.9</td>
</tr>
<tr>
<td style="text-align: center;">Self-Consistency Decoding</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">86.8</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">52.0</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">45.7</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">71.9</td>
</tr>
</tbody>
</table>
<p>Table 1: Accuracy of different prompting methods on 10 reasoning datasets from 4 domains. We compare our method, Faithful CoT, with standard (Brown et al., 2020), CoT (Wei et al., 2022), and Least-to-Most prompting (Zhou et al., 2022), with code-davinci-002 as the LM. The best results within each decoding strategy are bolded.
majority of datasets and domains under both decoding strategies. With greedy decoding, Faithful CoT outperforms all baselines on 8 of the 10 datasets, by a relative improvement of up to $14.2 \%$ on MWP, $3.4 \%$ on Planning, $6.5 \%$ on Date Understanding from Multi-hop QA, and a surprising $21.4 \%$ on Relational Inference. Generally, we see larger gains on harder datasets. Take MWP as an example: on simpler datasets where CoT already performs decently (e.g., MultiArith and AsDiv, where most questions require only 1-2 steps to solve), the gains are smaller ( $0.3 \%$ to $2.4 \%$ ); however, we see the largest gain ( $14 \%$ ) on the most difficult GSM8K, which requires up to 8 steps to solve. With selfconsistency decoding, Faithful CoT still performs the best on 7 out of the 10 datasets. Compared to greedy decoding, the relative gain increases on 4 datasets (AQUA: $12.1 \% \rightarrow 18.1 \%$, SayCan: $3.4 \%$ $\rightarrow 5.5 \%$, Date Understanding: $6.5 \% \rightarrow 10.8 \%$, and CLUTRR: $21.4 \% \rightarrow 41.3 \%$ ), but decreases or remains unchanged for the remaining three MWP datasets (GSM8K: $9.0 \% \rightarrow 2.6 \%$, SVAMP: $3.9 \%$ $\rightarrow 2.3 \%$, ASDiV $0.2 \% \rightarrow 0.2 \%$ ).</p>
<p>On the other hand, we do not see clear empirical gains on two multi-hop QA datasets, Sports Understanding on StrategyQA. On Sports Understanding, Faithful CoT and LtM both have near perfect accuracy (99+), suggesting that the dataset is almost saturated. On StrategyQA, however, the performance of our method is still far from the baselines. To understand why, we specifically compare the examples where CoT makes a correct prediction but our method fails. As shown in Figure 11 in Appendix F, we find that the likely primary cause is the sparsity of Datalog in the pretraining data for Codex, as an overwhelming $29 \%$ of errors are syntax-related. Moreover, including Datalog in the prompt also interferes with NL generation, making
it harder for Codex to produce relevant subquestions (17\%), retrieve knowledge correctly (10\%), and come up with valid reasoning from the knowledge to the answer ( $10 \%$ ). Another potential cause is the nature of the task, as the difficulty for many StrategyQA questions does not lie in reasoning but rather in knowledge retrieval, which makes the advantages of our deterministic solver less obvious. Still, with further pretraining on Datalog, we believe that there is room for improvement.</p>
<p>To see how generalizable our method is, we also experiment with four alternative LMs and observe consistent gains brought by Faithful CoT over the baselines, as shown in Appendix C.3. In particular, with GPT-4, we set the new few-shot SOTA results on 7 datasets, achieving $95.0+$ accuracy in four out of five MWP and two out of three Multi-hop QA datasets. Overall, these results suggest that faithfulness does empirically improve performance.</p>
<h2>6 Analysis</h2>
<p>In this section, we perform an extensive analysis of the strengths and weaknesses of our method, to better understand the role of different components, the robustness to design choices, the plausibility of generated reasoning chains, as well as frequent error patterns where it still struggles. Here, we only show the first two aspects; see the rest in Appendix C. Unless otherwise stated, we choose one dataset from each domain (GSM8K, Date Understanding, SayCan, and CLUTRR) and use code-davinci-002 outputs with greedy decoding.</p>
<h3>6.1 Ablation Study</h3>
<p>Given the strong performance of Faithful CoT, we now address a natural question: how much does each part of the prompt contribute to the accuracy? We perform an ablation study where we re-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Ablation study results: accuracy when we remove different parts of the prompt. See Section 6.1 for details.
move different parts of the prompt and see how the performance changes. In addition to the original prompt ("Full"), we test four variations, illustrated with the example from Figure 4:
No rationale. We remove the rationales, i.e., everything in the brackets from the NL comments, e.g., "independent, support: ['There are 15 trees']".
No NL but nudge. We remove all NL comments except the "nudge" line: e.g., "# To answer this question, we write a Python program to answer the following subquestions".
No NL. We remove all NL comments.
No solver. Instead of calling the external solver, we add "Answer: {answer}" to the end of every exemplar and let the LM predict the answer itself.</p>
<p>Figure 5 shows the results of all prompt variations. On GSM8K, Date Understanding, and SayCan, NL comments contribute little to the performance, and sometimes even slightly hurt it. On CLUTRR, however, their role is crucial, since the exclusion of each component (rationale, nudge, subquestions) results in a clear accuracy drop. In particular, comparing No NL but nudge and No NL, the nudge line itself brings a striking improvement by 31.3 points.</p>
<p>The external solver relieves the burden of problem solving from the LM. Without it, the accuracy suffers a huge decline on GSM8K, Date Understanding, and CLUTRR ( $-50.8,-22.9$, and -19.4 respectively), while on SayCan it improves by 2.9 nonetheless. One potential influencing factor is that SayCan might be too homogeneous, as it contains a set of only 3 predefined actions. This can make the task relatively easy, which allows all model variants to achieve around $90 \%$ accuracy and renders the solver unnecessary. Another potential reason is the level of correspondence between the final answer and the reasoning chain for different datasets: as shown in Figure 3, the answer in SayCan is a sequence of actions (e.g., find(redbull)), each directly corresponding to one step in the reasoning</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Exemplars</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">Date</th>
<th style="text-align: center;">SayCan</th>
<th style="text-align: center;">CLUTRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Set 0 (Table 1)</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">58.9</td>
</tr>
<tr>
<td style="text-align: center;">Set 1</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">59.0</td>
</tr>
<tr>
<td style="text-align: center;">Set 2</td>
<td style="text-align: center;">71.1</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: center;">Set 3</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: center;">Set 4</td>
<td style="text-align: center;">71.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">55.5</td>
</tr>
<tr>
<td style="text-align: center;">Set 5</td>
<td style="text-align: center;">71.5</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">56.0</td>
</tr>
<tr>
<td style="text-align: center;">Mean</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">57.4</td>
</tr>
<tr>
<td style="text-align: center;">Std</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">1.9</td>
<td style="text-align: center;">1.5</td>
</tr>
</tbody>
</table>
<p>Table 2: Robustness to the choice of exemplars.
chain (e.g., at redbull trash). However, the answer in the other three datasets is only a single number or string, which can only be derived after executing all the steps in the reasoning chain. Therefore, the latter type of tasks further necessitates the presence of an external solver.</p>
<h3>6.2 Robustness to Exemplars</h3>
<p>We now answer the next question: how much does the choice of exemplars matter? To do this, we annotate 20 examples in total, randomly sample k (7-10, depending on the dataset) to construct the prompt, and repeat the process five times. Table 2 shows the performance of all six runs, including the original (from Table 1). The mean accuracy is close to the original ( -1.5 to +1.2 ), still above the baselines by a large margin ( 7 to 17 ) on all datasets except the arguably easiest SayCan, considering the standard deviation (1.3 to 2.9). This strongly suggests that the benefits of Faithful CoT are minimally influenced by the choice of exemplars.</p>
<h3>6.3 Human Evaluation of Plausibility</h3>
<p>Our main experiments use final answer accuracy as the performance measure, but this does not necessarily correspond to the validity of the reasoning chain. Technically, a model can sometimes accidentally arrive at the correct answer with an invalid reasoning chain. We then ask: when the answer is correct, how often is the reasoning chain truly correct? In other words, we want to evaluate the plausibility of the reasoning chains.</p>
<p>We conduct a human evaluation study on Prolific: given a generated reasoning chain that results in a correct answer, a crowd-worker selects whether it is A) completely correct, or, if incorrect, specify why with B) incorrect NL and/or C) incorrect SL. Alternatively, they can select D) flawed question and E) I am confused. ${ }^{9}$</p>
<p>The results of our study are shown in Figure 6 (see Table 8 in the Appendix for numerical results).</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Human evaluation results of reasoning chain plausibility. Each column represents the percent of different answer choices selected by human evaluators in each domain/dataset.</p>
<p>For most domains, we see that annotators often find the reasoning chain fully correct - Sports, SayCan, and MWP have a $90 \%+$ correctness rate. To gain more insight into when the reasoning chain can be "incorrect", we perform an in-depth analysis of user annotations for the three worst-performing datasets - StrategyQA ( $66.7 \%$ correctness), Date ( $87.9 \%$ correctness), and CLUTRR ( $88.0 \%$ correctness). From our inspection, we find that annotators mistakenly mark a correct reasoning chain as incorrect at different rates based on the task ( $8.3 \%$ of the time for StrategyQA, $41.7 \%$ for Date Understanding, and $100 \%$ for CLUTRR). We find annotators are inaccurate for Date because they incorrectly believe the generated code misuses a Python library (relativedelta), or they complain that there is a better way to answer the question. For CLUTRR, annotators mark chains as incorrect due to known ambiguity in the dataset. For example, the grandmother of one's child may not necessarily be their parent, but also a parent-in-law.</p>
<p>As for the remaining truly incorrect reasoning chains, we find the LM can sometimes add unnecessary steps or arrive at the correct answer by chance. The latter is especially an issue in StrategyQA - given that all questions have a True/False answer, it is common for an incorrect reasoning chain to result in a correct answer. For example, the LM correctly answers the question "Was Karachi a part of Alexander the Great's success?" as "True." However, the reasoning chain contains the flawed subquestion "Which countries are in Pakistan? Pakistan includes Pakistan, Afghanistan, and India." Though the final answer is correct and faithful to the LM generation, the reasoning chain contains wrong knowledge.</p>
<p>Overall, Faithful CoT does generate valid reasoning chains for the vast majority of the time when the answer is correct. However, we still see exceptions where the model arrives at the right answer via an incorrect reasoning chain. Though this happens infrequently, it raises concerns about when people should trust LMs. To our knowledge, we are the first to conduct a systematic human study on the plausibility of CoT-style reasoning chains, and we hope to see future work further investigate and improve on the flaws that our study brings to light.</p>
<h2>7 Conclusion</h2>
<p>We propose Faithful CoT, a framework that decomposes complex reasoning into Translation and Problem Solving. It guarantees that the reasoning chain is a faithful explanation of how the model arrives at the answer. We demonstrate the efficacy of our approach on 4 types of complex reasoning problems: Math Word Problems, Multi-hop QA, Planning, and Relational Inference. Our method sets new SOTA performance on 7 of the 10 datasets, while additionally providing a faithful explanation for the final answer. These results give empirical evidence that improving model interpretability, by guaranteeing the faithfulness of an explanation, does not come at the expense of overall performance; in fact, we see a strong synergy in between. Through a comprehensive analysis of the strengths and weaknesses of our method, we show its robustness to the choice of exemplars, the pivotal role of the solver, as well as frequent error patterns where it still struggles.</p>
<h2>Limitations</h2>
<p>One crucial limitation of our study is that on March 23rd, 2023, OpenAI discontinued the use of codedavinci-002. This has rendered part of our results unreplicable for any teams or researchers who have not been granted continued access to the model. This discontinuation was unexpected during our study. It raises important questions about using closed-source models for academic research.</p>
<p>Meanwhile, one methodological limitation of our approach lies in the scope of faithfulness. Currently, we guarantee that Problem Solving stage is faithful. However, the Translation stage is still opaque, meaning it is not self-interpretable how the LM generates the reasoning chain from the question. It is still an under-explored question whether</p>
<p>it is possible to improve the interpretability of the LM generation process in general, and a few recent studies have made promising early progress (Yin and Neubig, 2022; Sarti et al., 2023) that might be used to improve the faithfulness of the Translation stage.</p>
<p>Finally, it still needs further exploration of the role NL comments in the reasoning chain. From our ablation study, in terms of performance, whether to include the NL comments in the reasoning chain does not make a big difference on many of the datasets, especially those where the task is not knowledge-intensive. Nevertheless, speaking of interpretability, NL comments can make the reasoning chain more structured and understandable to the end user. Further, NL comments can be an interface that allows users without a programming background to interact with and debug the model, which we leave for future work.</p>
<h2>Ethics Statement</h2>
<p>With the recent success of generative large LMs, they are now being used to solve complex reasoning problems. When using the output of an LM for reasoning, there is a danger that if the reasoning $a p$ pears realistic, then the final answer or conclusion will also be considered reliable. As we highlighted in Figure 1 and 7, this is often not true, since an LM may produce a reasoning chain that looks plausible, but the final answer is still wrong. This work is a step in the direction of making the use of LMs more trustworthy by using the LM for just expressing its reasoning in a symbolic program and executing the program independently. In this work, we have ensured the faithfulness of the reasoning chain w.r.t how the final answer is produced in a variety of domains, but admittedly the Translation phase is still opaque. Therefore, our pipeline is still not entirely interpretable. Furthermore, as we have stressed in Section 1, faithfulness does not guarantee correctness, so our method can still sometimes produce erroneous answers, which may pose a risk for users that rely on it for decision making.</p>
<h2>Acknowledgements</h2>
<p>This research is based upon work supported in part by the DARPA KAIROS Program (contract FA8750-19-2-1004), the DARPA LwLL Program (contract FA8750-19-2-0201), the IARPA HIATUS Program (contract 2022-22072200005), and the NSF (Award 1928631). Approved for Public Re-
lease, Distribution Unlimited. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, IARPA, NSF, or the U.S. Government.</p>
<p>We appreciate the support from OpenAI on increasing the rate limit for the Codex API. We also thank Jiani Huang, Ziyang Li, Litao Yan, Andrew Head, Mayur Naik, and Lyle Ungar for their valuable feedback.</p>
<h2>References</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J. Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. 2022. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. ArXiv:2204.01691 [cs].</p>
<p>Yoshua Bengio. 2019. FROM SYSTEM 1 DEEP LEARNING TO SYSTEM 2 DEEP LEARNING.</p>
<p>BIG-Bench collaboration. 2021. Beyond the Imitation Game: Measuring and extrapolating the capabilities of language models.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,</p>
<p>Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. ArXiv:2107.03374 [cs].</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. ArXiv:2211.12588 [cs].</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. ArXiv:2110.14168 [cs].</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. PAL: Program-aided Language Models. ArXiv:2211.10435 [cs].</p>
<p>Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for Computational Linguistics, 9:346361 .</p>
<p>Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael Specter, and Lalana Kagal. 2018. Explaining Explanations: An Overview of Interpretability of Machine Learning. In 2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA), pages 80-89.
L. A. Harrington, M. D. Morley, A. cedrov, and S. G. Simpson. 1985. Harvey Friedman's Research on the Foundations of Mathematics. Elsevier. Google-Books-ID: 2plPRR4LDxIC.</p>
<p>Bernease Herman. 2019. The Promise and Peril of Human Evaluation for Model Interpretability. arXiv:1711.07414 [cs, stat]. ArXiv: 1711.07414.</p>
<p>Alon Jacovi and Yoav Goldberg. 2020. Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4198-4205. Association for Computational Linguistics.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. 2022. Maieutic Prompting: Logically</p>
<p>Consistent Reasoning with Recursive Explanations. ArXiv:2205.11822 [cs].</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large Language Models are Zero-Shot Reasoners.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yahuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving Quantitative Reasoning Problems with Language Models. ArXiv:2206.14858 [cs].</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022. On the Advance of Making Language Models Better Reasoners. ArXiv:2206.02336 [cs].</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167. Association for Computational Linguistics. ArXiv:1705.04146 [cs].</p>
<p>Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. 2020. A diverse corpus for evaluating and developing English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975-984, Online. Association for Computational Linguistics.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your Work: Scratchpads for Intermediate Computation with Language Models. ArXiv:2112.00114 [cs].</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online. Association for Computational Linguistics.</p>
<p>Silviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba. 2023. Boosted prompt ensembles for large language models. arXiv preprint arXiv:2304.05970.</p>
<p>Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. 2020. Learning to deceive with attention-based explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 47824793, Online. Association for Computational Linguistics.</p>
<p>Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. 2022. Limitations of Language Models in Arithmetic and Symbolic Induction. ArXiv:2208.05051 [cs].</p>
<p>Marco Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "why should I trust you?": Explaining the predictions of any classifier. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, KDD '16, pages 97-101, New York, NY, USA. Association for Computational Linguistics.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Gabriele Sarti, Nils Feldhus, Ludwig Sickert, and Oskar van der Wal. 2023. Inseq: An interpretability toolkit for sequence generation models. arXiv preprint arXiv:2302.13942.</p>
<p>Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR: A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4506-4515, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 180-186. Association for Computing Machinery, New York, NY, USA.</p>
<p>Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023. Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-Consistency Improves Chain of Thought Reasoning in Language Models. ArXiv:2203.11171 [cs].</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models.</p>
<p>Kayo Yin and Graham Neubig. 2022. Interpreting language models with contrastive explanations. arXiv preprint arXiv:2202.10419.</p>
<p>Hanlin Zhang, Ziyang Li, Jiani Huang, Mayur Naik, and Eric Xing. 2022. Improved logical reasoning of language models via differentiable symbolic programming. In First Workshop on Pre-training: Perspectives, Pitfalls, and Paths Forward at ICML 2022.</p>
<p>Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022. Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. ArXiv:2205.10625 [cs].</p>
<h2>A Implementation Details</h2>
<p>In all our experiments, we use OpenAI GPT-3 (text-davinci-001 and text-davinci-002) and Codex (code-davinci-001 and code-davinci-002) models through the Python API available at beta.openai.com, from Sept, 2022 to Jan, 2023. The inference cost per example is $\$ 0$ for all Codex models since they are in limited beta period, and $\$ 0.01$ - $\$ 0.03$ for GPT-3 models depending on the dataset. It takes 2-15 seconds to run inference on one example with Codex models under a rate limit of 150,000 tokens/minute, and 1-8 seconds with GPT-3 models under 250,000 tokens/minute, also depending on the dataset. For example, on the GSM8K test set of 1,319 examples, it takes 3.5 h to finish the inference with Codex and 2.3 h with GPT-3.</p>
<p>We use the following hyper-parameters throughout all experiments:
temperature: 0.0 for greedy decoding, 0.4 for self-consistent decoding;
max_tokens: 1000;
n: 1 for greedy decoding, 40 for self-consistent decoding;
frequency_penalty: 0 ;
presence_penalty: 0 .
Any unspecified hyper-parameters are set to the default value on https://beta.openai.com/ docs/api-reference/completions/create.</p>
<h2>B Extended Results</h2>
<p>In this section, we present more results that do not fit into the main text.</p>
<h2>B. 1 Examples of Unfaithful CoT Output</h2>
<p>Figure 7 shows examples of unfaithful output from the CoT method (Wei et al., 2022) on three datasets: GSM8K (MWP), StrategyQA (Multi-hop QA), and SayCan (Planning). Note that here we only show model outputs that are apparently unfaithful. In other cases, there is no known inconsistency between the reasoning chain and the answer, but this still does not imply that the reasoning chain is a faithful explanation, since the ground truth (how the model predicts the answer) is unknown.</p>
<p>The GSM8K example is explained in Section 1. In the StrategyQA example, though the reasoning chain correctly identifies that a hummingbird weighs much more than a pea, the answer is still "yes"; in the SayCan example, the reasoning chain only mentions "visit the table and the counter",
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Examples of unfaithful output from CoT prompting (Wei et al., 2022) on three datasets. The answer (green) does not follow from the reasoning chain (blue).
but the plan contains unnecessary "pick" and "put" operations.</p>
<p>Wei et al. (2022) claim that CoT "provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong". As we have pointed out in Section 1, since CoT does not guarantee faithfulness, how the model arrives at the answer could differ drastically from what is shown in the reasoning chain. Furthermore, it is still hard for the user to debug the model: even if they manually correct the reasoning chain and let the model regenerate the answer, it might still be wrong, since there is no causality between the reasoning chain and the answer.</p>
<h2>B. 2 Comparison with Few-shot SOTA</h2>
<p>We compare the results of Faithful CoT with the published few-shot SOTA in Table 3. The Faithful CoT results are obtained with the best-performing LM among all five LMs we experiment with (see Section C. 3 on each dataset. The SOTA results are obtained from the following sources:</p>
<p>GSM8K: OpenAI (2023);
MultiArith, ASDiv, StrategyQA: Wang et al. (2022);</p>
<p>SVAMP: Chen et al. (2022);
AQuA: Pitis et al. (2023);</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Math Word Problems</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Planning</th>
<th style="text-align: center;">Multi-hop QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Relation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Sport</td>
<td style="text-align: center;">CLUTRR</td>
</tr>
<tr>
<td style="text-align: center;">Few-shot SOTA</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">76.2</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">71.9</td>
</tr>
<tr>
<td style="text-align: center;">$d_{\text {max }}$</td>
<td style="text-align: center;">+5.0</td>
<td style="text-align: center;">+6.3</td>
<td style="text-align: center;">-0.8</td>
<td style="text-align: center;">+7.8</td>
<td style="text-align: center;">-2.8</td>
<td style="text-align: center;">+4.9</td>
<td style="text-align: center;">-16.4</td>
<td style="text-align: center;">+19.6</td>
<td style="text-align: center;">+0.8</td>
<td style="text-align: center;">+21.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison between the existing few-shot SOTA results and the optimal Faithful CoT results (with the best-performing LM (code-davinci-002 for SayCan and CLUTRR, and gpt-4 for the rest of the datasets). See Appendix B. 2 for sources of SOTA results.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Accuracy of our method and two concurrent methods, Program of Thoughts (POT) (Chen et al., 2022) and Program-Aided Language Models (PAL) (Gao et al., 2022), on 10 reasoning datasets.</p>
<p>SayCan, Sports Understanding: (Wei et al., 2022);</p>
<p>Date Understanding: (Gao et al., 2022);
CLUTRR: Our implementation of LtM prompting (Zhou et al., 2022) (no existing work reports few-shot performance on CLUTRR with $K$ up to 10).</p>
<p>With Codex and GPT-4, Faithful CoT sets new SOTA performance on 7 out of the 10 datasets across four domains, achieving 95.0+ accuracy on 6 of them.</p>
<h2>B. 3 Empirical Comparison with Concurrent Work</h2>
<p>Two pieces of concurrent work, Program of Thoughts (PoT) (Chen et al., 2022) and ProgramAided Language Models (PAL) (Gao et al., 2022), were announced on arXiv within three months of our work. Essentially, they both generate Python programs, or SL-only reasoning chains, to derive the answer. Our approach differs from them mainly in the additional component of structured NL comments, which decomposes the original problem into simpler, inter-dependent subproblems.</p>
<p>Aside from the qualitative differences highlighted in Section 2, we perform an empirical performance comparison with them on the same set of 10 datasets used in our main evaluation. Since
both papers have only tackled math reasoning and symbolic reasoning tasks, we reimplement their methods by using the "noNL" prompt in our ablation study from Section 6.1. The comparison is done with code-davinci-002 as the underlying LM and greedy decoding.</p>
<p>As shown in Figure 8, on 6 of the 10 datasets (including most MWP datasets, SayCan, and Date Understanding), PAL/PoT and Faithful CoT have very close accuracy ( $&lt;2.0$ difference). On AQuA, PAL/PoT is visibly better. On the remaining three datasets (StrategyQA, Sports Understanding, and CLUTRR), Faithful CoT reasonably outperforms PAL/PoT. This may suggest that our method has an advantage when the task requires extensive external knowledge (e.g., StrategyQA and Sports Understanding) or when the SL is not frequent in the LM's pretraining data (e.g., Datalog, or our selfdefined relational expressions).</p>
<p>Finally, note that the key contribution of our method lies in interpretability. Though the addition of structured NL comments sometimes does not make a difference in performance, it does make the reasoning chain more understandable to the user. Furthermore, it may even enable users without a programming background to debug the model, by only interacting with the NL subproblems (e.g., adding/removing/editing a subproblem), which is worth further exploration in the future.</p>
<h2>C Extended Analysis</h2>
<h2>C. 1 Ablation Study</h2>
<p>Table 4 shows the full results of the ablation study from Section 6.1.</p>
<h2>C. 2 Robustness to Prompt Phrasing</h2>
<p>We study the sensitivity of our method to subtle differences in the prompt design. We experiment with three prompt variations: 1. randomly permuting the order of independent subquestions/reasoning steps; 2: Changing the variable names; 3. changing the nudge line (e.g. from "# To answer this</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">Date</th>
<th style="text-align: right;">SayCan</th>
<th style="text-align: right;">CLUTRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Full</td>
<td style="text-align: right;">72.3</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">89.3</td>
<td style="text-align: right;">$\mathbf{5 8 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">No rationale</td>
<td style="text-align: right;">$\mathbf{7 5 . 4}$</td>
<td style="text-align: right;">$\mathbf{8 3 . 0}$</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">51.8</td>
</tr>
<tr>
<td style="text-align: left;">No NL but nudge</td>
<td style="text-align: right;">73.5</td>
<td style="text-align: right;">80.2</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">39.6</td>
</tr>
<tr>
<td style="text-align: left;">No NL.</td>
<td style="text-align: right;">72.8</td>
<td style="text-align: right;">79.7</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">9.3</td>
</tr>
<tr>
<td style="text-align: left;">No solver</td>
<td style="text-align: right;">21.5</td>
<td style="text-align: right;">57.9</td>
<td style="text-align: right;">$\mathbf{9 0 . 3}$</td>
<td style="text-align: right;">40.9</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study results that accompany Figure 5. We report accuracy when we remove different parts of the prompt.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Prompt</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">Date</th>
<th style="text-align: right;">SayCan</th>
<th style="text-align: right;">CLUTRR</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: right;">72.3</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">89.3</td>
<td style="text-align: right;">58.9</td>
</tr>
<tr>
<td style="text-align: left;">Variation 1</td>
<td style="text-align: right;">69.1</td>
<td style="text-align: right;">84.4</td>
<td style="text-align: right;">88.3</td>
<td style="text-align: right;">-</td>
</tr>
<tr>
<td style="text-align: left;">Variation 2</td>
<td style="text-align: right;">70.3</td>
<td style="text-align: right;">81.6</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">56.2</td>
</tr>
<tr>
<td style="text-align: left;">Variation 3</td>
<td style="text-align: right;">70.2</td>
<td style="text-align: right;">80.5</td>
<td style="text-align: right;">87.4</td>
<td style="text-align: right;">55.9</td>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: right;">70.5</td>
<td style="text-align: right;">82.0</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: right;">57.0</td>
</tr>
<tr>
<td style="text-align: left;">Std</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">1.3</td>
<td style="text-align: right;">1.7</td>
</tr>
</tbody>
</table>
<p>Table 5: Robustness to prompt phrasing.
question, write a Python program to answer the following subquestions" to "# To solve this question, we answer each of the following subquestions with a Python program").</p>
<p>We rerun the evaluation of all three variations on 4 datasets (when applicable) used in the Section 6, under greedy decoding. Table 5 shows the results. Overall, the performance is quite stable, always above each baseline on all four datasets.</p>
<h2>C. 3 Model Sensitivity</h2>
<p>In this section, we want to answer the question: how much does the choice of LM matter? All results in Section 5 are obtained using code-davinci-002. Here, we examine the effect of using four alternative code-generation models as the Translator: code-davinci-001, text-davinci-002, text-davinci-003, gpt-4. We compare our method with the three baselines using each of the above LM on five MWP datasets, using the greedy decoding strategy.</p>
<p>As shown in Table 6, regardless of the underlying LM, Faithful CoT consistently outperforms all baselines on the vast majority of the datasets, and performs very closely with the best method ( $&lt;2.0$ difference) on the remaining ones. On average, it has a relative accuracy gain of $16.1 \%, 11.0 \%, 9.4 \%$, and $4.6 \%$ over the best-performing method among the baselines, for each LM respectively. This indicates that even though the absolute performance varies depending on the LM, Faithful CoT brings a relatively consistent accuracy gain.</p>
<p>Notably, with GPT-4 as the underlying LM, Faithful CoT results in $95.0+$ accuracy in 4 of the 5 MWP datasets, far outperforming the previous fewshot SOTA on three of them (GSM8K, SVAMP, and ASDiv).</p>
<h2>C. 4 Enforcing Constraints</h2>
<p>Since our generated reasoning chain contains structured components (e.g., dependency graphs), another natural question to ask is: will it be helpful to enforce certain constraints on the generation? Using MWP datasets as a case study, we examine the effect of three such constraints:</p>
<p>Graph validity. The dependency graph must be a Directed Cyclic Graph (DAG), e.g., it is not allowed for a subquestion to depend on itself.</p>
<p>No over-dependency. The code cannot depend on any variable that its corresponding subquestion has not mentioned, e.g. in Figure 4, since Q5 says "depend on 4", then the corresponding code should not use the variable eggs_in_dozen, since it is not the output of Q4.</p>
<p>No under-dependency. The code must depend on all variables that its corresponding subquestion has mentioned, e.g. in the same example, since Q5 says "depend on 4", then the corresponding code must use the variable eggs_in_dozen.</p>
<p>We investigate the effect of adding constraints on the generations under self-consistent decoding. Starting with our original results (without any constraint), we add a different set of constraints at each time and report the accuracy change in Table 7. Individually, the graph validity constraint results in little to no change in the performance, but the other two constraints lead to a more unstable changemostly a decrease-across datasets. Adding two or more constraints further lowers the performance in almost all cases except on MultiArith (the easiest dataset), revealing the tradeoff between accuracy and satisfying the constraints. It also indicates that a proportion of generations ( $1.0 \%$ to $8.9 \%$ ) in our existing results do not satisfy all constraints. However, it may still be worth enforcing some of these constraints (e.g., graph validity) at the cost of performance, in order for users to better control and interact with the model.</p>
<h2>D Human Evaluation Details</h2>
<p>We hire crowd workers on Prolific to evaluate the correctness of model-generated reasoning chains that result in a correct answer. We sample</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method / Dataset</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">MultiArith</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LM: code-davinci-001</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">5.2</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">20.8</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">57.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">37.2</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">44.9</td>
<td style="text-align: center;">18.5</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">20.5</td>
<td style="text-align: center;">28.1</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">53.7</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;">LM: text-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">15.4</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">34.1</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">40.7</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">91.1</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">63.4</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">92.8</td>
<td style="text-align: center;">75.4</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">70.4</td>
</tr>
<tr>
<td style="text-align: center;">LM: text-davinci-003</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">16.9</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">42.7</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">79.5</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">60.5</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">50.8</td>
<td style="text-align: center;">76.6</td>
</tr>
<tr>
<td style="text-align: center;">LM: gpt-4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Standard</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">50.4</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;">CoT</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">77.1</td>
</tr>
<tr>
<td style="text-align: center;">LtM</td>
<td style="text-align: center;">91.8</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;">Faithful CoT (ours)</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">91.6</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracy of different prompting methods with each underlying LM on 5 MWP reasoning datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Constraint</th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">SVAMP</th>
<th style="text-align: right;">MultiArith</th>
<th style="text-align: right;">ASDiv</th>
<th style="text-align: right;">AQuA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">None</td>
<td style="text-align: right;">80.0</td>
<td style="text-align: right;">88.8</td>
<td style="text-align: right;">99.2</td>
<td style="text-align: right;">84.4</td>
<td style="text-align: right;">61.4</td>
</tr>
<tr>
<td style="text-align: left;">+ G</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-0.8</td>
</tr>
<tr>
<td style="text-align: left;">+ O</td>
<td style="text-align: right;">-0.9</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">+0.4</td>
<td style="text-align: right;">-3.9</td>
</tr>
<tr>
<td style="text-align: left;">+ U</td>
<td style="text-align: right;">-1.0</td>
<td style="text-align: right;">-3.6</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-1.2</td>
<td style="text-align: right;">+1.2</td>
</tr>
<tr>
<td style="text-align: left;">+ GO</td>
<td style="text-align: right;">-1.7</td>
<td style="text-align: right;">-0.4</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">+0.2</td>
<td style="text-align: right;">-3.9</td>
</tr>
<tr>
<td style="text-align: left;">+ GU</td>
<td style="text-align: right;">-1.0</td>
<td style="text-align: right;">-3.7</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">-1.2</td>
<td style="text-align: right;">+0.8</td>
</tr>
<tr>
<td style="text-align: left;">+ OU</td>
<td style="text-align: right;">-4.0</td>
<td style="text-align: right;">-5.4</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-2.6</td>
<td style="text-align: right;">-4.3</td>
</tr>
<tr>
<td style="text-align: left;">+ GOU</td>
<td style="text-align: right;">-5.0</td>
<td style="text-align: right;">-5.9</td>
<td style="text-align: right;">-0.1</td>
<td style="text-align: right;">-3.2</td>
<td style="text-align: right;">-5.5</td>
</tr>
</tbody>
</table>
<p>Table 7: Accuracy change after enforcing different constraints on the generation. The "None" row shows the original performance without any constraint (from Table 1). Each row below adds a different set of constraints: G stands for "graph validity", O for "no overdependency", and U for "no under-dependency". Results are on all MWP datasets under self-consistent decoding.</p>
<p>100 reasoning chains for each domain generated by code-davinci-002 with the greedy decoding strategy, where each set of 100 contains an equal number of samples from all datasets within the domain. We further require annotators to have experience coding in the programming language of the dataset they annotate (Python for MWP/CLUTRR/Sports/Date, and Scala for StrategyQA, as Datalog was not an option in Prolific). We have a single survey for each domain, with the exception of Multi-hop QA (in this case, we have separate surveys for StrategyQA, Date, and Sports, given the different nature of each dataset). Additionally, there was no way to ensure annotators knew PPDL on Prolific. In order to ensure high-quality annotations for SayCan, the authors</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Example of our annotation interface for the CLUTRR survey
annotate this dataset themselves.</p>
<p>Annotation Process Each of the 100 questions for each domain is annotated by at least three annotators. Each annotator is given 10 questions to annotate. A screenshot of our survey interface is shown in Figure 9. At the start of the survey, annotators are given examples of a correct reasoning chain, a reasoning chain with incorrect NL, and a reasoning chain with incorrect SL. The examples match the domain of reasoning chains in each survey. As an attention and understanding check, we ask annotators to label an example they had just seen. If the annotator fails this question, they are sent to the end of the survey and their responses are filtered out. We also manually filter out all spammers (annotators who answer with the same</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Domain</th>
<th style="text-align: center;">Correct</th>
<th style="text-align: center;">Incorrect NL</th>
<th style="text-align: center;">Incorrect SL</th>
<th style="text-align: center;">Flawed Question</th>
<th style="text-align: center;">Confused</th>
<th style="text-align: center;">Agreement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MWP</td>
<td style="text-align: center;">92.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.790</td>
</tr>
<tr>
<td style="text-align: left;">Planning</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.947</td>
</tr>
<tr>
<td style="text-align: left;">StrategyQA</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">30.3</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.455</td>
</tr>
<tr>
<td style="text-align: left;">Date</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.788</td>
</tr>
<tr>
<td style="text-align: left;">Sports</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.899</td>
</tr>
<tr>
<td style="text-align: left;">Relation</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.683</td>
</tr>
</tbody>
</table>
<p>Table 8: Numerical results for human evaluation of reasoning chain correctness, accompanying Figure 6. Each row represents the percent $(0-100)$ of different answer choices selected by human evaluators in each domain/dataset, as well as the inter-annotator agreement.
response repeatedly or complete the survey in under 3 minutes). After the surveys are complete, we compute annotator agreement and then take the majority label for each reasoning chain as the final label in our analysis. Our annotator population consists of 100 annotators with an average age of 25 years and an average income of 40 k per year. $87.9 \%$ of the annotators are males and $56 \%$ have a four year college degree. Annotators are compensated at $\$ 16 / \mathrm{hr}$, and average 2 minutes per question. Our full study cost $\$ 280$. Sample instructions for the CLUTRR survey can be found in the Supplementary Materials.</p>
<h2>E Dataset Details</h2>
<h2>E. 1 Statistics</h2>
<p>We show the dataset details in Table 9, including the statistics, the number of few-shot exemplars used in the prompt, and example inputs and outputs.</p>
<p>In particular, we notice that in one of our baselines Wei et al. (2022), the reported number of exemplars used in the prompt is inconsistent between the main text (10) and the appendix (6). To ensure fair comparison, we rerun the baseline with 10 exemplars for our results in Table 1, which is what we use for our method.</p>
<h2>E. 2 URLs and Licenses</h2>
<p>We use the same distribution of datasets following Wei et al. (2022):</p>
<h2>Math Word Problems</h2>
<ul>
<li>GSM8K (Cobbe et al., 2021): https:// github.com/openai/grade-school-math, MIT license: https://github.com/ openai/grade-school-math/blob/ master/LICENSE.</li>
<li>SVAMP (Patel et al., 2021): https:// github.com/arkilpatel/SVAMP, MIT license: https://github.com/arkilpatel/ SVAMP/blob/main/LICENSE.</li>
<li>MultiArith (Roy and Roth, 2015), license: CC BY 4.0.</li>
<li>ASDiv (Miao et al., 2020): https://github. com/chaochun/nlu-asdiv-dataset.</li>
<li>AQuA (Ling et al., 2017): https: //github.com/deepmind/AQuA, license: https://github.com/deepmind/AQuA/ blob/master/LICENSE.</li>
</ul>
<h2>Multi-hop QA</h2>
<ul>
<li>StrategyQA (Geva et al., 2021): we use the open-domain setting (question-only set) from (BIG-Bench collaboration, 2021): https://github.com/google/BIG-bench/ tree/main/bigbench/benchmark_tasks/ strategyqa.</li>
<li>Date Understanding and Sports Understanding from BIG-Bench (BIG-Bench collaboration, 2021): Apache License v.2: https://github.com/google/BIG-bench/ blob/main/LICENSE.</li>
</ul>
<h2>Planning</h2>
<ul>
<li>SayCan (Ahn et al., 2022): SayCan dataset can be accessed at https://say-can. github.io/ under CC BY 4.0 license.</li>
</ul>
<h2>Relational Reasoning</h2>
<ul>
<li>CLUTRR (Sinha et al., 2019): https:// github.com/facebookresearch/clutrr, license: https://github.com/ facebookresearch/clutrr/blob/main/</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Shot</th>
<th style="text-align: center;"># Test</th>
<th style="text-align: center;">Example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Math <br> Word <br> Problems</td>
<td style="text-align: center;">GSM8K</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,319</td>
<td style="text-align: center;">Q: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? A: 72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SVAMP</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;">Q: Each pack of dvds costs 76 dollars. If there is a discount of 25 dollars on each pack. How much do you have to pay to buy each pack? <br> A: 51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MultiArith</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">Q: For Halloween Debby and her sister combined the candy they received. Debby had 32 pieces of candy while her sister had 42. If they ate 35 pieces the first night, how many pieces do they have left? <br> A: 39</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2,096</td>
<td style="text-align: center;">Q: Seven red apples and two green apples are in the basket. How many apples are in the basket? <br> A: 9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AQuA</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">Q: A car finishes a journey in 20 hours at the speed of $60 \mathrm{~km} / \mathrm{hr}$. If the same distance is to be covered in 10 hours, how much speed does the car gain? <br> A: "120 kmph"</td>
</tr>
<tr>
<td style="text-align: center;">Multi- <br> hop <br> QA</td>
<td style="text-align: center;">StrategyQA</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2,290</td>
<td style="text-align: center;">Q: Did Aristotle use a laptop? <br> A: False</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Date <br> Understanding <br> Sports <br> Understanding</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">359</td>
<td style="text-align: center;">Q: Yesterday was April 30, 2021. What is the date tomorrow in MM/DD/YYYY? <br> A: "05/02/2021"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">977</td>
<td style="text-align: center;">Q: Is the following sentence plausible: "Lebron James hit the turnaround jumper"? A: True</td>
</tr>
<tr>
<td style="text-align: center;">Planning</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">Q: Could you get me a drink with caffeine? <br> A: "1.find(redbull) 2.pick(redbull) 3.find(user) 4.put(redbull) 5. done()."</td>
</tr>
<tr>
<td style="text-align: center;">Relational <br> Inference</td>
<td style="text-align: center;">CLUTRR</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1,042</td>
<td style="text-align: center;">Q: [Carlos] is [Clarence]'s brother. [Carlos] and his sister, [Annie], went shopping. [Annie] asked her mom [Valerie] if she wanted anything, but [Valerie] said no. How is [Valerie] related to [Clarence]? <br> A: "mother"</td>
</tr>
</tbody>
</table>
<p>Table 9: Datasets used for evaluation. "# Shot" stands for the number of few-shot examples in the prompt (following Wei et al. (2022)) and "# Test" stands for the number of test examples.</p>
<p>LICENSE. We obtain the publicly distributed version available at https: //drive.google.com/file/d/1SEq_ e1IVCDDzsBIBhoUQ5pOVH5kxRoZF/view, specifically the data_089907f8 split.</p>
<p>We use all the above datasets for research purposes only, consistent with their intended use.</p>
<h2>E. 3 Data Cleaning</h2>
<p>We perform manual cleaning on ASDiv, Date Understanding, Sports Understanding, and SayCan as we discover a number of annotation issues. In our experiment, we rerun all baselines on our cleaned version of the test sets. They are provided in our repository to assist future research.</p>
<p>Specifically, we clean each of the datasets as follows:</p>
<p>ASDiv: We start with the test set used by Wei et al. (2022), which removes all questions with float-valued and string-valued answers. However, in their released version, we notice an error in the answer extraction step for questions with more than one value in the answer (e.g., "what is the width and
length of X?", where the answer consists of two values). In their implementation, only the first value is extracted as the ground truth answer, which is then compared against model outputs. This might artificially inflate the final accuracy. To fix this, we extract all values in the answer as a set and compare model outputs against it.</p>
<p>Date Understanding: We find a number of wrong answers in the test set. For example, for the question "Jane and John married on Jan 2, 1958. It is their 5-year anniversary today. What is the date today in MM/DD/YYYY?", the provided answer is "01/02/1961", whereas the correct answer should be "01/02/1963". We manually correct these answers, and the resulting test set has the same number of examples as the original one.</p>
<p>Sports Understanding: We notice a few ambiguities with the Sports Understanding dataset. For instance, running out of bounds is illegal in many sports. The phrase "Domantas Sabonis ran out of bounds" is labeled as implausible, however, Domantas Sabonis is a basketball player, and basketball players can indeed run out of bounds on</p>
<p>the court. We remove 8 questions with such actionbased ambiguities. Additionally, since the release of this dataset, a few new athletes have risen to fame with identical names to those mentioned in the dataset. For example, the question "Chris Paul struck out the side" is implausible, as the referenced "Chris Paul" is a famous basketball player. However, "Chris Paul" is also the name of a new MLB baseball player, in which case this statement is plausible. We remove 5 questions with such name-based ambiguities.</p>
<p>SayCan: We discover a few issues in the test set: (1) the environment setup (e.g., the list of objects, the list of locations, and the initial location of each object) is not the same for all examples; (2) the annotation of the ground truth answer is often incomplete (i.e., for a given task like "visit all locations", there exist many possible plans in terms of the order of locations visited, but not all of them are included in the annotation); (3) there are ambiguous descriptions in certain queries, for example, in "Could you get me something refreshing?", it is unclear what drinks are considered "refreshing". For these questions, we complete the annotation whenever possible, and filter out the rest. The resulting test set contains 103 examples out of the original 120 .</p>
<h2>E. 4 Dataset Splits</h2>
<p>As stated in Section 4.1, we use the official splits whenever possible: training set for exemplar selection, validation set for prompt tuning, and test set for evaluation. In cases where they are available, we adopt the following strategies for each dataset:</p>
<p>GSM8K: it only has training and test sets. We form the validation set by randomly sampling 1,000 examples from the training set.</p>
<p>Other MWP datasets: for AQuA, we use the official training/validation/test split. For the other datasets, only the test sets are used, since we have the same prompt for GSM8K and them.</p>
<p>Date Understanding and Sports Understanding: they only have test sets. We follow Wei et al. (2022) to select the same number of examples from the test set to form the few-shot prompt and use the remaining examples as a new test set.</p>
<p>SayCan: Following Wei et al. (2022), we manually write 7 few-shot exemplars, since no training set is provided. We evaluate the models on our cleaned version of the test set, described in the previous subsection.</p>
<p>CLUTRR: this dataset is split into multiple folds. There is a training fold with $K \in{2,3}$ (where $K$ is the number of intermediate steps required to reach the answer), and one test fold for each $K$ from 2 to 10 . We construct the few-shot prompt using exemplars from the training fold, and test our method on the concatenation of all test folds.</p>
<h2>F Error Analysis</h2>
<p>To further investigate where our method still fails, we inspect 100 errors ${ }^{10}$ from model predictions on each of the four datasets and manually annotate the error categories.</p>
<h2>F. 1 GSM8K</h2>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Error analysis for GSM8K. For a detailed description of the error categories, see Appendix F.1.</p>
<p>As shown in Figure 10, we categorize the errors on GSM8K into 6 types, inversely sorted with frequency:
Wrong Subquestion (49\%): The LM produces a wrong NL subquestion, which eventually leads to the incorrect answer. While this is the majority error type in our sample, it is worth noting that in a typical human-in-the-loop collaboration, these errors are easily fixable. Even if the user is unfamiliar with programming, they can inspect the NL subquestions and potentially correct the model error by simply deleting or editing a wrong subquestion.
Wrong Code (24\%): The NL subquestion is correct, but the code fails to answer the subquestion correctly. For example, the code uses a variable that has not been previously defined.
Semantic Understanding Error (12\%): The LM incorrectly interprets certain semantic subtleties in the query. This is the most complex and most interesting error category. For example, consider the</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p># 1. How many pounds will Martin lose per week if he eats Cheerios every day for breakfast? (independent, support: ["he'll lose 1.25 pounds/week"])
pounds_lost_cheerios $=1.25$
# 2. How many pounds will Martin gain per week if he eats donuts every day for breakfast? (independent, support: ["he'll gain 1.75 pounds/week"]) pounds_gained_donuts $=1.75$
# 3. How many weeks are there in 5 weeks? (independent, support: ["External knowledge: there are 7 days in a week"])
weeks_in_5_weeks $=5$
# 4. How many pounds will Martin lose in 5 weeks if he eats Cheerios every day for breakfast? (depends on 1 and 3, support: [])
pounds_lost_cheerios_5_weeks = pounds_lost_cheerios * weeks_in_5_weeks
# 5. How many pounds will Martin gain in 5 weeks if he eats donuts every day for breakfast? (depends on 2 and 3, support: [])
pounds_gained_donuts_5_weeks = pounds_gained_donuts * weeks_in_5_weeks
# 6. What will be the difference in his weight at the end of 5 weeks between the two breakfast options? (depends on 4 and 5 , support: [])
difference_5_weeks = pounds_gained_donuts_5_weeks - pounds_lost_cheerios_5_weeks
# 7. Final Answer: What will be the difference in his weight at the end of 5 weeks between the two breakfast options? (depends on 6 , support: [])
answer $=$ difference_5_weeks
Table 10: Generated code for the question in Appendix F.1, as an example of "semantic understanding error".
following problem:
If Martin eats Cheerios every day for
breakfast, he'll lose 1.25 pounds/week.
If he eats donuts every day for breakfast, he'll gain 1.75 pounds/week. What will be the difference in his weight at the end of 5 weeks between the two breakfast options?</p>
<p>The generated code, as shown in Table 10, does not assign opposite polarities (signs) for "pounds lost" vs. "pounds gained". For other examples in this category, we notice errors like missing that a pair of something has 2 items in it, missing to subtract 2 for "two years ago" when it occurs as a subjunctive, and so on. Fixing these errors, in general, will require more than providing additional examples in the prompt.
Generation Cutoff (7\%): The generation stops midway, mainly due to the LM producing the same steps over and over again. These errors could be easily detected in postprocessing and possibly fixed by re-prompting the LM.
Wrong Gold Label (5\%): We find 5 (out of our 100) examples that are genuine annotation errors in the gold labels.
Missing Subquestion (3\%): The LM misses a relevant subquestion needed for the rest of the reasoning chain to work. These errors are also potentially fixable via human-in-the-loop interaction, where the user can insert a subquestion into the reasoning chain.</p>
<h2>F. 2 StrategyQA</h2>
<p>As mentioned in Section 5, Figure 11 shows the error type distribution on a sample of 70 instances from StrategyQA, where we specifically compare the cases where the prediction of CoT is correct whereas ours is wrong.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Error analysis for StrategyQA. For a detailed description of the error categories, see Section F.2.</p>
<p>There are 8 error types, from most to least frequent:</p>
<p>Syntax (29\%): these are syntax-related errors that result in an invalid Datalog program. For example, the LM redefines a relation that is previously declared, uses an invalid string comparison operation, generates an ungrounded variable in the answer definition, and so on. With further pretraining on Datalog, we believe Codex is able to overcome these errors to a reasonable extent.</p>
<p>Infinite Loop (24\%): the LM generates an in-</p>
<p>finite loop of subquestions, which are oftentimes rather philosophical. For example, for the question "Are potatoes native to the European continent?", the LM generates a list of 40 subquestions (potentially more, if not truncated by the token limit):</p>
<ol>
<li>Where are potatoes native to?</li>
</ol>
<p>Potatoes are native to the Andes.
2. Which continent is the Andes in?</p>
<p>The Andes is in South America.
3. Which continent is South America in?</p>
<p>South America is in the Americas.
4. Which continent is the Americas in?</p>
<p>The Americas is in the Western Hemisphere.
5. Which hemisphere is the Western Hemisphere in?
The Western Hemisphere is in the Earth.
6. Which continent is the Earth in?</p>
<p>The Earth is in the Solar System.
11. Which continent is the Universe in?</p>
<p>The Universe is in the Multiverse.
12. Which continent is the Multiverse in?
The Multiverse is in the Omniverse.
13. Which continent is the Omniverse in?
The Omniverse is in the Metaverse.
14. Which continent is the Metaverse in?</p>
<p>The Metaverse is in the Hyperverse.
39. Which continent is the Quattuordecaverse in?
The Quattuordecaverse is in the Quindecaverse.
40. Which continent is the Quindecaverse in?</p>
<p>We notice that this type of errors also happen if the prompt does not include Datalog code but just the NL subquestions, but much less often.</p>
<p>Wrong Subquestions (17\%): The LM generates an irrelevant subquestion to the original question. For instance, given the question, "Would toast for a vegan have margarine instead of butter?", one subquestion that is generated is "What is the difference between a vegan and a vegetarian?", which does not help with reaching the final answer.</p>
<p>Wrong Reasoning (10\%): The subquestions are correct, but the reasoning from the retrieved facts to the final answer is problematic. For example,
to answer the question, "Do solo pianists require a conductor?", the model correctly generates the relevant subquestions and their respective answers:</p>
<ol>
<li>What is the definition of a solo pianist? A solo pianist is a pianist who performs alone.</li>
<li>What is the definition of a conductor? A conductor is a person who leads an orchestra.</li>
<li>What is the definition of an orchestra? An orchestra is a large instrumental ensemble that contains string, brass, woodwind, and percussion instruments.</li>
</ol>
<p>But in deriving the final answer, the reasoning is wrong: "Now, we derive the final answer: Do solo pianists require a conductor? The answer is Yes only if a solo pianist is not a conductor."</p>
<p>Wrong Knowledge (10\%): the LM fails to retrieve the correct knowledge to answer the subquestions. For example, given the original question "Is the largest city in New Mexico also known as Yoot?", the model correctly generates the subquestions "What is the largest city in New Mexico?" (answer: Albuquerque) and "Is Albuquerque also known as Yoot?". However, when answering the second subquestion, it retrieves a wrong piece of knowledge ("Albuquerque is also known as Yoot.", whereas in reality, it should be "Santa Fe" that is known as Yoot).</p>
<p>Answer Definition (6\%): In our prompt, we always derive the answer in the format of "The answer is Yes only if ...", which is followed by a Datalog rule containing conditions that should be satisfied for the answer to be true. However, the LM sometimes generates this as "The answer is No only if ...", which outputs the reversed answer.</p>
<p>Knowledge Representation (3\%): The retrieved knowledge in NL is correct, but the representation of it in Datalog is wrong. For example, for the piece of knowledge "The Lucy Show is not the same TV series as JAG (TV series)", the model represents it as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="na">.decl</span><span class="w"> </span><span class="no">Same_TV_series</span><span class="p">(</span><span class="no">TV_series1</span><span class="p">:</span><span class="no">symbol</span><span class="p">,</span>
<span class="nl">TV_series2:</span><span class="nf">symbol</span><span class="p">]</span>
<span class="nf">Same_TV_series</span><span class="p">(</span><span class="err">&quot;</span><span class="no">The</span><span class="w"> </span><span class="no">Lucy</span><span class="w"> </span><span class="no">Show</span><span class="err">&quot;</span><span class="p">,</span><span class="w"> </span><span class="err">&quot;</span><span class="no">JAG</span><span class="w"> </span><span class="p">(</span><span class="no">TV</span>
<span class="nf">series</span><span class="p">)</span><span class="err">&quot;</span><span class="p">).</span><span class="err">&quot;</span>
</code></pre></div>

<p>which actually means the reverse (they are the same).</p>
<p>Unknown (1\%): There is a very small proportion of errors (1 out of 70) where we are unsure</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ To encourage sample diversity, we embed all the errors using text-davinci-002 and cluster the embeddings using spectral clustering. This produces around 70 clusters of different sizes, from which we gather 100 samples using importance sampling.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ See Appendix E for dataset statistics, examples, data cleaning method, splits, prompt construction strategy, etc.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
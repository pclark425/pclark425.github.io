<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7562 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7562</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7562</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-3e04193d5110288b776edd5724aeca2229d2d182</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3e04193d5110288b776edd5724aeca2229d2d182" target="_blank">How Good Are LLMs at Out-of-Distribution Detection?</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Language Resources and Evaluation</p>
                <p><strong>Paper TL;DR:</strong> A pioneering empirical investigation into the OOD detection capabilities of LLMs is initiated, focusing on the LLaMA series ranging from 7B to 65B in size, and it is unveiled that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors.</p>
                <p><strong>Paper Abstract:</strong> Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at https://github.com/Awenbocc/LLM-OOD for other researchers to reproduce our results.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7562",
    "paper_id": "paper-3e04193d5110288b776edd5724aeca2229d2d182",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004664499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Good Are LLMs at Out-of-Distribution Detection?</h1>
<p>Bo Liu ${ }^{1 <em>}$, Li-Ming Zhan ${ }^{1 </em>}$, Zexin Lu ${ }^{1 *}$, Yujie Feng ${ }^{1}$, Lei Xue ${ }^{2}$, Xiao-Ming Wu ${ }^{1 \dagger}$<br>Department of Computing, The Hong Kong Polytechnic University, Hong Kong S.A.R. ${ }^{1}$<br>School of Cyber Science and Technology, Sun Yat-Sen University, Shenzhen, China. ${ }^{2}$<br>{bokelvin.liu, lmzhan.zhan, zexin.lu, yujie.feng}@connect.polyu.edu.hk<br>xuelei3@mail.sysu.edu.cn<br>xiao-ming.wu@polyu.edu.hk</p>
<h4>Abstract</h4>
<p>Out-of-distribution (OOD) detection plays a vital role in enhancing the reliability of machine learning models. As large language models (LLMs) become more prevalent, the applicability of prior research on OOD detection that utilized smaller-scale Transformers such as BERT, RoBERTa, and GPT-2 may be challenged, due to the significant differences in the scale of these models, their pre-training objectives, and the paradigms used for inference. This paper initiates a pioneering empirical investigation into the OOD detection capabilities of LLMs, focusing on the LLaMA series ranging from 7B to 65B in size. We thoroughly evaluate commonly used OOD detectors, examining their performance in both zero-grad and fine-tuning scenarios. Notably, we alter previous discriminative in-distribution fine-tuning into generative fine-tuning, aligning the pre-training objective of LLMs with downstream tasks. Our findings unveil that a simple cosine distance OOD detector demonstrates superior efficacy, outperforming other OOD detectors. We provide an intriguing explanation for this phenomenon by highlighting the isotropic nature of the embedding spaces of LLMs, which distinctly contrasts with the anisotropic property observed in smaller BERT family models. The new insight enhances our understanding of how LLMs detect OOD data, thereby enhancing their adaptability and reliability in dynamic environments. We have released the source code at https://github.com/Awenbocc/LLM-OOD for other researchers to reproduce our results.</p>
<p>Keywords: Out-of-distribution detection, large language models, performance evaluation</p>
<h2>1. Introduction</h2>
<p>Out-of-distribution (OOD) detection has attracted significant attention due to its crucial role in ensuring Al safety (Salehi et al., 2022). The objective is to identify and raise an alarm for inputs that exhibit distributional shifts compared to the in-distribution (ID) training data. Given that the test distribution can dynamically change over time, OOD detection has become indispensable in high-stakes applications, such as healthcare and self-driving cars. Its ability to detect anomalous inputs and adapt to evolving scenarios makes it a vital component in ensuring the reliability and robustness of Al systems in realworld, dynamic environments.</p>
<p>Utilizing sentence representations yielded by pretrained language models (PLMs) to derive OOD confidence scores has been the de facto method for textual OOD detection. Specifically, PLMs are first fine-tuned on the ID data and then OOD detectors are applied on the sentence representations generated by PLMs. Compared to ID data, there are two types of OOD instances: far-OOD where ID and OOD data come from different domains and near-OOD where ID and OOD data come from the same domain but with different classes, as shown in Figure 1. Typically, near-OOD samples are harder to recognize. A body of works (Hendrycks et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of two types of OOD instances compared to ID samples: far-OOD where ID and OOD data come from different domains and nearOOD where ID and OOD data come from the same domain but with different classes.</p>
<p>2020; Podolskiy et al., 2021; Uppaal et al., 2023) have shown that Transformer-based models can produce better sentence representations for OOD detection. However, these studies have mainly focused on evaluating the OOD detection performance of small-scale encoder-based Transformers, such as RoBERTa and BERT.</p>
<p>Recently, large language models (LLMs) have made significant strides in various cognitive tasks, yet their capabilities on OOD detection remain largely unexplored. Unlike relatively small-scale PLMs used by prior studies, LLMs often display</p>
<p>notable differences. In particular, the majority of previously prominent PLMs utilized for OOD detection adopt the encoder-based architecture, such as BERT and RoBERTa. These models are predominantly designed with a pre-training objective that focuses on sentence classification. However, recent LLMs (Touvron et al., 2023a; Zeng et al., 2022; Du et al., 2022; Chowdhery et al., 2022; Chung et al., 2022) exclusively adopt an autoregressive training objective during pre-training. Consequently, the hidden states of LLMs are specialized for next token prediction, which could influence their performance in OOD detection. Moreover, previous works test changes in OOD detection when adapting PLMs to downstream tasks through discriminative finetuning, even for decoder-based models (Cho et al., 2023). However, a more intuitive approach is to probe the pre-training knowledge of LLMs through generative fine-tuning, which better aligns LLMs' pre-training objective with downstream tasks. Thus, it is imperative to extensively investigate the OOD detection capabilities of LLMs to gain deeper insights into their potential and limitations.</p>
<p>This paper aims to fill this gap by offering a comprehensive and structured assessment of OOD detection with LLMs across varying scales (ranging from 7B to 65B). Notably, our evaluation process is specifically designed to consider the scaling laws of LLMs with commonly utilized OOD detection detectors, ensuring broader and more generalized findings. In summary, our analysis has revealed the following new insights:</p>
<ol>
<li>Discriminative vs. generative fine-tuning. We have observed that generative fine-tuning demonstrates greater resilience to the issue of ID overfitting when compared to discriminative finetuning. As highlighted by Uppaal et al. (2023), there exists a trade-off between achieving higher accuracy on ID tasks and ensuring effective OOD detection. It has been shown that OOD detectors progressively lose efficacy as the training of ID tasks continues. However, our findings indicate that adopting a generative approach to fine-tuning LLMs can effectively mitigate this issue, potentially resulting in stable OOD performance even as training progresses and ID accuracy improves.</li>
<li>LLM-based far- vs. near-OOD detection. Our results consistently demonstrate that LLMs are natural far-OOD detectors. Remarkably, LLMs of all scales achieve near-perfect OOD performance in far-OOD scenarios without requiring any fine-tuning. However, when it comes to near-OOD detection, only the 65B model is able to achieve satisfactory performance without any fine-tuning. Despite that, we discover that fine-tuning significantly improves the near-OOD detection capability of LLMs.</li>
<li>Anisotropy vs. isotropy. Our experimental results suggest that the cosine distance function, when used as a straightforward OOD detector, performs exceptionally well. This observation leads to an intriguing discovery: the embedding spaces of LLMs exhibit a desirable isotropic property, which is not possessed by the BERT family models. The sentence embeddings produced by the BERT family models have been noted to possess an undesirable characteristic of being concentrated within a narrow cone, a phenomenon referred to as anisotropic representations (Ethayarajh, 2019), which negatively affects tasks involving semantic relationships and is commonly known as representation degeneration (Gao et al., 2019). The issue is resolved through the isotropic representations generated by LLMs, which allow the cosine distance to excel in OOD detection and may potentially benefit a broad spectrum of tasks.</li>
</ol>
<h2>2. Related Work</h2>
<h3>2.1. Out-of-Distribution Detection</h3>
<p>Out-of-Distribution (OOD) detection has a long history in machine learning and is highly related to research topics like outlier detection, anomaly detection and novelty detection (Hendrycks and Mazeika, 2022). In the task setting of OOD detection, the $i n$ distribution is characterized by the labeled training dataset and the out-of-distribution refers to anything else that possesses distributional shifts. OOD detection distinguishes from other related topics in the case that it requires both of the ID and OOD classification accuracy (Yang et al., 2021).</p>
<h3>2.2. Textual OOD Detection with PLMs</h3>
<p>The significance of textual OOD detection in ensuring the robustness of NLP applications, such as dialogue systems, has led to a surge in research interest. Pre-trained Transformers have shown intrinsic superiority in handing OOD detection (Hendrycks et al., 2020; Zhan et al., 2021).</p>
<p>Several works have further evaluated the OOD performance of PLMs with respect to commonly used OOD detectors including MSP (Hendrycks and Gimpel, 2017), Mahalanobis distance (MD) (Lee et al., 2018), and Energy score (Liu et al., 2020). For example, Podolskiy et al. (2021) show that the Gaussian distribution assumption of MD better matches the representation space of BERT and can yield the best OOD performance in intent OOD detection benchmarks. Zhou et al. (2021) show that a contrastive regularizer can further improve the sentence representation of Transformers for OOD detection.</p>
<p>More recently, Uppaal et al. (2023) present a thorough analysis on the fine-tuning strategies</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Our proposed evaluation framework for LLMs at OOD detection, taking three aspects into consideration: (1) distribution of OOD samples (near or far), (2) impact of model tuning on OOD detection, and (3) diverse OOD score functions.
for OOD detection with RoBERTa and show that RoBERTa (Liu et al., 2019) without fine-tuning can achieve near-perfect far-OOD detection performance. Similarly, we find that LLMs can also achieve perfect far-OOD detection performance without fine-tuning. Cho et al. (2023) explore the OOD detection capability of medium-sized PLMs (such as GPT-2 (Radford et al., 2019)), as well as the impact of various ID fine-tuning techniques. While they also assess decoder-based models, the models they evaluated are not extensive as this work and they neglect to undertake generative ID tuning, a crucial step to fine-tune decoder-based models for downstream ID tasks. Furthermore, the models they examine remain at relatively moderate scales, and an exploration of the possible dataefficient characteristics of the model is lacking.</p>
<p>Recently, large language models (LLMs) have been leading a paradigm shift in the field of natural language processing (NLP) (Touvron et al., 2023a; Wei et al., 2022; Zeng et al., 2022; Du et al., 2022; Chowdhery et al., 2022; Chung et al., 2022; Feng et al., 2023). The use of LLMs to solve NLP tasks in a generative way has become widespread. These LLMs commonly adopt the decoder-based architecture and are trained with the autoregressive objective. In this paper, we focus on the OOD performance of open-source LLMs and anticipate our work can provide useful insights for OOD detection under this paradigm.</p>
<h2>3. Method</h2>
<p>Problem statement. The objective of OOD detection is to effectively differentiate between instances
that belong to a specific distribution (in-distribution $\mathcal{D}<em _OOD="{OOD" _text="\text">{\text {ID }}$ ) and those falling outside of that distribution (out-of-distribution $\mathcal{D}</em>$ can be used to reject whether outputting results for such inputs or not.}}$ ). To better and fairly evaluate the capabilities of LLMs for OOD detection compared to prior smaller models (e.g., RoBERTa (Liu et al., 2019)) (Uppaal et al., 2023), we utilize the same sentence classification task as the ID training task. In practical application scenarios, undesired inputs (e.g., a severe distribution shift towards ID data) may occur, and an OOD confidence scoring function $f_{\text {OOD }</p>
<h3>3.1. ID Generative Fine-tuning with LLMs</h3>
<p>For the ID sentence classification task, we align with the nature of LLMs and adopt a generative approach (referred to as open-ended classification) (Radford et al., 2018). Concretely, given an input sentence $\mathbf{X}<em _mathrm_x="\mathrm{x">{\mathbf{x}}$, we first expand it with a simple template: "### Input:\n $\mathrm{X}</em>$ with $L$ tokens by:}}$ ### Output: \n", to facilitate the extraction of outputs by identifying the section following the "Output" symbol. Subsequently, we maximize the probability of generating the target label $\mathbf{X}_{\mathbf{a}</p>
<p>$$
\max p\left(\mathbf{X}<em _mathbf_s="\mathbf{s">{\mathbf{a}} \mid \mathbf{X}</em>}}\right)=\prod_{i=1}^{L} p_{\theta}\left(x_{i} \mid \mathbf{X<em _mathbf_a="\mathbf{a">{\mathbf{s}}, \mathbf{X}</em>\right)
$$},&lt;i</p>
<p>where $\theta$ represents the model parameters and $\mathbf{X}<em i="i">{\mathbf{a},&lt;i}$ are partial label tokens that come before the current prediction token $x</em>$.</p>
<p>Parameter-efficient fine-tuning. To improve the performance of LLMs in the in-distribution sentence</p>
<p>classification task, we employ a parameter-efficient fine-tuning (PEFT) approach, to minimize the usage of additional parameters. Specifically, we utilize the low-rank adaptation (LoRA) (Hu et al., 2021) technique which freezes the pre-trained LLMs' weights and inserts trainable rank decomposition matrices into each Transformer layer. We perform PEFT with answer predictions, i.e., only class label tokens are utilized to compute the auto-regressive loss. During the test stage, we use strict matching to determine whether the generated labels are identical to the ground truth.</p>
<h3>3.2. OOD Detection with LLMs</h3>
<p>The overview of our OOD detection framework is illustrated in Figure 2. Our primary focus is on decoder-like LLMs, such as LLaMA (Touvron et al., 2023a), as they have demonstrated excellent performance when their model size scales up (OpenAI, 2023; Brown et al., 2020). To obtain a comprehensive observation, we conduct OOD detection experiments on two different semantic distribution settings (Ming et al., 2022; Lin et al., 2021): farOOD and near-OOD (cf. Figure 1). Regarding OOD detection methods, we focus on the prevailing post-hoc paradigm (Yang et al., 2021). In the following, we elaborate on how to integrate the posthoc OOD detectors into decoder-like LLMs, which has never been addressed in existing literature.</p>
<p>Customized post-hoc methods. According to prior studies (Hendrycks et al., 2020; Zhou et al., 2021), there mainly exist two categories of post-hoc methods: logits-based OOD score functions and distance-based ones. Since previous works used these methods for language models accompanied by a classifier, we here customize them for decodertype LLMs with only a language model head (as shown in Figure 2) in the following:</p>
<p>Logits-based OOD score functions operate on the final class-related logits. In generative classification, the generated class name is usually composed of several tokens, e.g., "positive" consists of "posi" and "tive". Instead of calculating the probability (logits) for the complete ID class name, we simplify the process by considering the probability assigned to the first token of its class name. For instance, in a sentiment analysis task with classes like "positive" and "negative" as depicted in Figure 2, we only need to identify the probability corresponding to the tokens "posi" and "negative" respectively. Considering that different class names may have common prefixes, such as "positive" and "position", we will rephrase the conflicting class names at the beginning of ID training, such as replacing "position" with "location". In practice, we observe this re-translation has no impact on the ID task. Overall,
there are mainly two logits-based functions:</p>
<ul>
<li>Maximum softmax probability (MSP) (Lee et al., 2018) utilizes the maximum softmax probability corresponding to each class, i.e, score $\mathcal{S}(x)=\max \left{p\left(y_{i} \mid x\right)\right}_{i}^{K}$, where $K$ is the number of classes, and ID samples always exhibit higher probability scores while OOD ones correspond to lower scores.</li>
<li>Energy score (Energy) (Liu et al., 2020; LeCun et al., 2006) computes confidence score $\mathcal{S}(x)=\log \sum_{i}^{K} e^{\left(w^{T} \cdot z\right)_{i}}$ where $w^{T}$ is the weight of the language model head and $z$ is all word embeddings. Note that for both MSP and Energy, we only select the probability and logits corresponding to the first token of each class name, as mentioned before.</li>
</ul>
<p>Distance-based OOD score functions apply to sentence representations. Prior studies using encoder-based PLMs treated the embeddings of special token <cls> as sentence representations. For LLMs, we employ the embeddings of the last token as the representation. There are mainly two functions considered for evaluation: Mahalanobis distance (Maha) (Lee et al., 2018) and Cosine distance (Cosine) (Zhou et al., 2021) ${ }^{1}$.</p>
<h2>4. Experimental Setup</h2>
<h3>4.1. Datasets</h3>
<p>To draw universal conclusions, we conduct a comprehensive evaluation of two kinds of dataset distribution settings (Arora et al., 2021) as illustrated in Figure 1 and Figure 2.</p>
<p>Far-OOD. In this paradigm, ID and OOD samples come from different distributions (datasets), exhibiting significant semantic differences. Following Hendrycks et al. (2020) and Zhou et al. (2021), we evaluate 8 datasets, including 20 Newsgroups (20NG) (Lang, 1995) for topic classification, RTE (Wang et al., 2018) and MNLI (Williams et al., 2017) for nature language inference, TREC10 (Li and Roth, 2002) for question classification, SST-2 (Socher et al., 2013) and IMDB (Maas et al., 2011) for sentiment analysis, and the English side of Multi30K (Elliott et al., 2016) and WMT16 (Bojar et al., 2016) for machine translation. Among them, we choose 20NG and SST-2 as two separate in-distribution tasks and the remaining ones are recognized as out-distribution. Note that when SST-2 is used as the ID, we do not consider IMDB as an OOD dataset since both of them are sentiment analysis tasks.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Maha</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cosine</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Energy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ID Dataset</td>
<td style="text-align: center;">LLM</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
<td style="text-align: center;">AUPR $\uparrow$</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
<td style="text-align: center;">AUPR $\uparrow$</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
<td style="text-align: center;">AUPR $\uparrow$</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
<td style="text-align: center;">AUPR $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">$\begin{aligned} &amp; \text { O } \ &amp; \text { O } \ &amp; \text { O } \end{aligned}$</td>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.811</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.380</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-13B</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.213</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.478</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-30B</td>
<td style="text-align: center;">0.994</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.881</td>
<td style="text-align: center;">0.361</td>
<td style="text-align: center;">0.742</td>
<td style="text-align: center;">0.651</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.540</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-65B</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.776</td>
<td style="text-align: center;">0.621</td>
<td style="text-align: center;">0.646</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.485</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20NG</td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.784</td>
<td style="text-align: center;">0.417</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-13B</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.622</td>
<td style="text-align: center;">0.754</td>
<td style="text-align: center;">0.482</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.362</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-30B</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.847</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.491</td>
<td style="text-align: center;">0.906</td>
<td style="text-align: center;">0.362</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-65B</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.999</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.616</td>
<td style="text-align: center;">0.764</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.369</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLINC-Banking</td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.568</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.891</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.916</td>
<td style="text-align: center;">0.720</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.763</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.818</td>
<td style="text-align: center;">0.758</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-13B</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.769</td>
<td style="text-align: center;">0.760</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.831</td>
<td style="text-align: center;">0.743</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-30B</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.472</td>
<td style="text-align: center;">0.913</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.813</td>
<td style="text-align: center;">0.746</td>
<td style="text-align: center;">0.724</td>
<td style="text-align: center;">0.795</td>
<td style="text-align: center;">0.735</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-65B</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.200</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.604</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.826</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.834</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLINC-Travel</td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.680</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.887</td>
<td style="text-align: center;">0.738</td>
<td style="text-align: center;">0.927</td>
<td style="text-align: center;">0.584</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.640</td>
<td style="text-align: center;">0.637</td>
<td style="text-align: center;">0.912</td>
<td style="text-align: center;">0.674</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-13B</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.485</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.922</td>
<td style="text-align: center;">0.730</td>
<td style="text-align: center;">0.955</td>
<td style="text-align: center;">0.639</td>
<td style="text-align: center;">0.834</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.633</td>
<td style="text-align: center;">0.909</td>
<td style="text-align: center;">0.695</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-30B</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.928</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.650</td>
<td style="text-align: center;">0.911</td>
<td style="text-align: center;">0.697</td>
<td style="text-align: center;">0.653</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-65B</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.182</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.076</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.739</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.753</td>
<td style="text-align: center;">0.755</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.768</td>
</tr>
</tbody>
</table>
<p>Table 1: OOD detection performance of zero-grad LLaMA models. We use the full validation set to calculate each OOD score. The results are averaged over five seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Full-shot</th>
<th style="text-align: center;">10-shot</th>
<th style="text-align: center;">5-shot</th>
<th style="text-align: center;">1-shot</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">20NG</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">CLINC150</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">(Banking or Travel)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 3: UMAP (McInnes et al., 2018) visualization of representations generated from the penultimate layer of the zero-grad ( $\odot$ ) and fine-tuned ( $\triangle$ ) LLaMA-7B models. (a) Far-OOD: 20NG is treated as ID while SST-2, RTE, and TREC are treated as OOD. (b) Near-OOD: the banking domain of CLINC150 is selected, of which $50 \%$ of the classes are treated as ID, and the rest are treated as OOD.</p>
<p>Near-OOD. We also test on a more challenging scenario, where ID and OOD samples come from the same domain but with disjoint label sets. A wellresearched domain is OOD intent detection (Larson et al., 2019). Specifically, we use CLINC150 dataset and choose Banking and Travel domains. Within each domain, $50 \%$ of the classes are chosen as ID, and the remaining classes as OOD.</p>
<h3>4.2. Evaluation Metrics</h3>
<p>We employ three commonly used metrics for OOD detection: (1) AUROC (area under the receiver operating characteristic curve). (2) FAR@95 (false alarm rate at $95 \%$ recall). It represents the probability of incorrectly classifying a negative sample as positive when the Recall or True Positive Rate (TPR) is $95 \%$. We treat the OOD class as negative. (3) AUPR (area under the precision-recall curve). Additionally, we use accuracy as a metric for ID classification task.</p>
<h3>4.3. Implementation Details</h3>
<p>All experiments are conducted on a workstation with 4 NVIDIA A100 80G GPUs. For zero-grad</p>
<p>OOD detection, LLaMA-7B, -13B, -30B, and -65B are deployed on 1, 1, 2, and 4 A100 GPUs, respectively. When further fine-tuning LLMs on ID tasks, the LoRA configurations (Section 3.1) are that rank $r$ is 16 , scaling $\alpha$ is 16 , and query/key/value/output projection matrices $\left{W_{q}, W_{k}, W_{v}, W_{o}\right}$ in each selfattention module need to be updated. We train the network for 50 epochs with early stop criteria that if the model's performance on the validation set continuously drops for 6 consecutive epochs and the current epoch number exceeds 15, training will be terminated. We use AdamW optimizer with learning rate $1 \times 10^{-4}$, further decayed by linear schedule. Due to the varying lengths of sentences in different ID datasets, we configure different batch sizes shown in Table 2. All experiments are conducted over five seeds $(1,2,3,4,5)$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Maha</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Cosine</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Energy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ID Dataset</td>
<td style="text-align: center;">Shot</td>
<td style="text-align: center;">ID ACC:</td>
<td style="text-align: center;">AUROC ：</td>
<td style="text-align: center;">FAR@95；</td>
<td style="text-align: center;">AUPR ：</td>
<td style="text-align: center;">AUROC ：</td>
<td style="text-align: center;">FAR@95；</td>
<td style="text-align: center;">AUPR ：</td>
<td style="text-align: center;">AUROC ：</td>
<td style="text-align: center;">FAR@95；</td>
<td style="text-align: center;">AUPR ：</td>
<td style="text-align: center;">AUROC :</td>
<td style="text-align: center;">FAR@95；</td>
<td style="text-align: center;">AUPR :</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.934</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.587</td>
<td style="text-align: center;">0.716</td>
<td style="text-align: center;">0.589</td>
<td style="text-align: center;">0.637</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.664</td>
<td style="text-align: center;">0.678</td>
<td style="text-align: center;">0.625</td>
<td style="text-align: center;">0.843</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.768</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.674</td>
<td style="text-align: center;">0.885</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.794</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.857</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.009</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.514</td>
<td style="text-align: center;">0.693</td>
<td style="text-align: center;">0.896</td>
<td style="text-align: center;">0.379</td>
<td style="text-align: center;">0.803</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">0.976</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.296</td>
<td style="text-align: center;">0.888</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.199</td>
<td style="text-align: center;">0.907</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.380</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.047</td>
<td style="text-align: center;">0.985</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.779</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.850</td>
<td style="text-align: center;">0.681</td>
<td style="text-align: center;">0.824</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20NG</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.713</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.074</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.023</td>
<td style="text-align: center;">0.989</td>
<td style="text-align: center;">0.868</td>
<td style="text-align: center;">0.503</td>
<td style="text-align: center;">0.799</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.283</td>
<td style="text-align: center;">0.916</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.796</td>
<td style="text-align: center;">0.992</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.996</td>
<td style="text-align: center;">0.013</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.438</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.215</td>
<td style="text-align: center;">0.924</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.939</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.114</td>
<td style="text-align: center;">0.945</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.569</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.510</td>
<td style="text-align: center;">0.926</td>
<td style="text-align: center;">0.846</td>
<td style="text-align: center;">0.696</td>
<td style="text-align: center;">0.860</td>
<td style="text-align: center;">0.870</td>
<td style="text-align: center;">0.656</td>
<td style="text-align: center;">0.897</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLINC-Banking</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.882</td>
<td style="text-align: center;">0.863</td>
<td style="text-align: center;">0.614</td>
<td style="text-align: center;">0.879</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.873</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.878</td>
<td style="text-align: center;">0.903</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.916</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.937</td>
<td style="text-align: center;">0.424</td>
<td style="text-align: center;">0.956</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.902</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.046</td>
<td style="text-align: center;">0.928</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.147</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.269</td>
<td style="text-align: center;">0.945</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.225</td>
<td style="text-align: center;">0.931</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.533</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">0.481</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.767</td>
<td style="text-align: center;">0.756</td>
<td style="text-align: center;">0.771</td>
<td style="text-align: center;">0.780</td>
<td style="text-align: center;">0.733</td>
<td style="text-align: center;">0.793</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLINC-Travel</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">0.644</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.148</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.886</td>
<td style="text-align: center;">0.875</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.872</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.137</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.078</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.930</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.931</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.231</td>
<td style="text-align: center;">0.933</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Full</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.980</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.049</td>
<td style="text-align: center;">0.987</td>
<td style="text-align: center;">0.942</td>
<td style="text-align: center;">0.121</td>
<td style="text-align: center;">0.933</td>
<td style="text-align: center;">0.948</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">0.953</td>
</tr>
</tbody>
</table>
<p>Table 3: The performance of the fine-tuned LLaMA-7B model for OOD detection and ID classification. "Shot" denotes the number of examples in the ID training or validation set. We report the average results of five seeds.</p>
<h2>5. Findings</h2>
<h3>5.1. Zero-grad OOD Detection with LLMs</h3>
<p>In this section, we evaluate the zero-grad OOD performance of LLMs. The objective is to examine how well OOD detection performs when utilizing the knowledge acquired by LLMs during pre-training. The results are summarized in Table 1 and all LLMs are frozen in this setting. Note that we use full-shot validation set to calculate each OOD score.</p>
<p>LLMs are natural far-OOD detectors. As shown in Table 1, when applying distance-based OOD detection methods, such as Maha and Cosine, all LLMs can achieve near-perfect results (e.g., AUROC and AUPR approach 1 while FAR@95 approaches 0 ). To better understand why distancebased OOD detectors are so effective, we visualize the corresponding sentence representations yielded by the penultimate layer (before the top head layer), as shown in Figure 3 (a ○). It can be found that representations from the same dataset are tighter, while ID and OOD sentences have clear boundaries, indicating the profound semantic discrimination prowess exhibited by LLMs. However, both MSP and Energy generate poor results. This is foreseeable, as both of them condition on the first token generated from the input sentence. When the model has not been fine-tuned, it often struggles to accurately output class names, leading to inferior OOD performance. Moreover, from the probability density of Figure $4(\circ)$, it can be found that there is a significant overlap between ID and OOD, leading to a decrease in OOD detection performance.</p>
<p>The capability of LLMs for near-OOD detection improves with their scale. We present the zerograd near-OOD results in Table 1 (CLINC-Banking and CLINC-Travel). For the near-OOD setting, as the number of model parameters increases, the OOD detection performance will also be improved.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Impact of fine-tuning on logits-based OOD scores (MSP at the top row and Energy at the bottom row). We plot SST-2 (ID) vs. TREC-10 (OOD) for visualization.</p>
<p>Remarkably, when the model has an exceedingly large number of parameters (i.e., LLaMA-65B), we can observe a dramatic performance surge (Wei et al., 2022) to detect OOD inputs, especially with distance-based OOD methods. In particular, the AUROC values for Maha and Cosine both surpass $95 \%$, and FAR95 is enhanced by at least $30 \%$ in comparison to the 7B model.</p>
<p>Furthermore, it is evident that the near-OOD performance of LLMs is notably inferior compared to their performance on far-OOD instances. To understand this, we provide a visualization for this setting as illustrated in Figure 3 (b ○). The embeddings of ID and OOD samples are mixed up since their labels come from the same domain (i.e., travel or banking). Consequently, detecting near-OOD instances becomes notably more challenging than far-OOD instances.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Performance changes for ID classification and OOD detection as training progresses with the different number of training shots. Top row: 20NG is ID training task; Bottom row: banking domain of CLINC150 is selected where $50 \%$ classes are used as ID training task and the rest are OOD samples. The star $(*)$ on each line indicates the selected results whose epoch corresponds to the best ID performance on the validation set.</p>
<h3>5.2. OOD Detection with Generatively Fine-tuned LLMs</h3>
<p>In this subsection, we study the influence of finetuning LLMs on OOD detection. Specifically, we conduct an in-depth examination of how the OOD detection performance evolves with the progression of ID task training.</p>
<p>ID fine-tuning can boost OOD detection. We fine-tune LLMs in a generative manner in both fewshot and full-shot scenarios. The results are summarized in Table 3. Likewise, we present both far- and near-OOD results comparable to the zerograd configuration. Clearly, fine-tuning LLMs on in-distribution tasks can notably augment the models' capacity to detect OOD instances, surpassing the performance of the zero-grad setting by a significant margin in most cases like in near-OOD setting and with logits-based functions (in both full-shot scenarios with LLaMA-7B model).</p>
<p>In Figure 5, we present the fine-tuning curves. It can be observed that as the ID accuracy increases, almost all OOD detectors are improved accordingly. To study how fine-tuning impacts the ID vs. OOD separability, we plot their density distributions in Figure 4. Clearly, fine-tuning can improve the separability between ID and OOD instances. A similar effect can be cross-validated in Figure 3 (b 0 ) in which the embedding of different classes within the ID becomes more compact, while the separation between ID and OOD becomes clearer. However, it is important to highlight that as the training continues, there is a possibility of encountering overfitting, which could result in inferior OOD performance, es-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Impact of different ID training objectives, discriminative vs. generative. SST-2 dataset with full data is used as the ID training task.
pecially for logits-based methods as illustrated in Figure 5 for both full-shot and 1-shot far-OOD scenarios. This observation is similar to the findings in (Uppaal et al., 2023).</p>
<p>Generative fine-tuning generalizes better. In addition to generative fine-tuning, we also explore discriminative fine-tuning by appending a classifier after LLMs (replacing the language model head ${ }^{2}$ ) to conduct ID task. The comparison of the trend charts presented in Figure 6 reveals that generative fine-tuning tends to be less overfit on the ID task and all OOD detectors consistently perform better than discriminative fine-tuning, especially for distance-based OOD detectors. To better understand this effect, based on the transformations of embeddings illustrated in Figure 3 (a), it becomes evident that throughout the generative training pro-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID Dataset</th>
<th style="text-align: center;">PTM</th>
<th style="text-align: center;">$\begin{gathered} \text { Mahe } \ \text { AUROC } \end{gathered}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{gathered} \text { Cosine } \ \text { FAR@95 } \end{gathered}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MSP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Energy</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
<td style="text-align: center;">AUROC $\uparrow$</td>
<td style="text-align: center;">FAR@95 $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">RoBERTa-Li (Uppaal et al., 2023)</td>
<td style="text-align: center;">0.971</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.919</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.006</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.318</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Fine-tuned</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa-Li (Zhou et al., 2021)</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">0.183</td>
<td style="text-align: center;">0.962</td>
<td style="text-align: center;">0.236</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.613</td>
<td style="text-align: center;">0.877</td>
<td style="text-align: center;">0.632</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.004</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.005</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">0.961</td>
<td style="text-align: center;">0.189</td>
</tr>
<tr>
<td style="text-align: center;">20NG</td>
<td style="text-align: center;">RoBERTa-Li (Uppaal et al., 2023)</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0.002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.998</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.441</td>
<td style="text-align: center;">0.929</td>
<td style="text-align: center;">0.571</td>
<td style="text-align: center;">0.784</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">RoBERTa-Li (Zhou et al., 2021)</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.073</td>
<td style="text-align: center;">0.978</td>
<td style="text-align: center;">0.107</td>
<td style="text-align: center;">0.946</td>
<td style="text-align: center;">0.305</td>
<td style="text-align: center;">0.965</td>
<td style="text-align: center;">0.158</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.003</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.959</td>
<td style="text-align: center;">0.207</td>
<td style="text-align: center;">0.968</td>
<td style="text-align: center;">0.114</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of large and small PTMs under zero-grad and fine-tuned settings for OOD detection. $\dagger$ denotes the results we reproduce due to different calculating methods, while $\ddagger$ indicates results cited from the original paper.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">PTMs</th>
<th style="text-align: center;">Data Corpus</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CLINC150</td>
<td style="text-align: center;">CLINC150-Banking</td>
<td style="text-align: center;">CLINC150-Travel</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.4731</td>
<td style="text-align: center;">0.5529</td>
<td style="text-align: center;">0.5312</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">0.9991</td>
<td style="text-align: center;">0.9992</td>
<td style="text-align: center;">0.9989</td>
</tr>
</tbody>
</table>
<p>Table 5: Average sentence anisotropy of model's last layer.
cess, while the ID's distribution shifts into classspecific clusters, a distinct separation continues to exist between these clusters and the OOD samples. This preserves the effectiveness of distance-based OOD detection methods. Prior study (Uppaal et al., 2023) pointed out that discriminative tuning the small models (e.g., RoBERTa (Liu et al., 2019)) negatively impacts the performance of distancebased OOD detection methods. This issue also exists in discriminative tuning LLMs but has been solved in the generative tuning.</p>
<p>Besides, in Table 4, we compare encoder-based and decoder-based Transformers and observe impressive improvement on decoder-based LLMs.</p>
<p>Cosine distance is a data-efficient OOD detector. To further investigate whether LLMs possess data-efficient OOD detection capabilities, we configure the training samples of the ID as few-shot instances (e.g., 1, 5, and 10). Please note that we also set the number of validation sets to be the same shot, since all OOD detection methods rely on the validation set. Results presented in Table 3 convey that as the number of shots increases, the OOD detection capability of the LLMs also improves. Moreover, distance-based OOD detection methods are superior to logits-based ones, and they can achieve good performance even with only 10-shot samples. Particularly, cosine distance is a data-efficient OOD detector that can provide effective detection by requiring only 1-shot instance. For example, it achieves AUROC of $\mathbf{9 9 . 1 \%}$ (neargerfect) on 20NG (ID) and over $90 \%$ on others. Besides, in the 1-shot setting, the Mahalanobis distance loses its efficacy since it's unfeasible to
model the necessary Gaussian distribution when there's only a single sample for each class.</p>
<p>Isotropy vs. anisotropy. By examining Table 1 and Table 3, it becomes evident that Cosine distance, as a simple OOD detector, consistently delivers superior performance and ranks among the top performers in both the zero-grad and generative fine-tuning settings. We provide an explanation of this phenomenon from the perspective of representation learning. In the past few years, the anisotropic issue, also known as the representation degeneration problem, of BERT family models has garnered considerable attention (Ethayarajh, 2019; Gao et al., 2019). Researchers have highlighted that BERT's sentence representations are concentrated within a narrow cone, resulting in substantial challenges for tasks involving semantic matching. Nevertheless, we discover that this concern does not apply to LLMs. The representations generated by off-the-shelf LLMs inherently exhibit isotropy, enabling Cosine distance to excel in OOD detection. To quantify anisotropy, we adopt the methodology introduced by Ethayarajh (2019) to measure sentence-level anisotropy. Let $\mathbf{X}_{\mathbf{i}}$ be a sentence that appears in the corpus. The anisotropy value can be calculated by:</p>
<p>$$
\text { anisotropy }=\frac{1}{n^{2}-n}\left|\sum_{i} \sum_{j \neq i} \cos \left(z\left(\mathbf{X}<em _mathbf_j="\mathbf{j">{\mathbf{i}}\right), z\left(\mathbf{X}</em>\right)\right)\right|
$$}</p>
<p>where $\cos$ is the cosine similarity and $z(\cdot)$ is the sentence embedding from the last layer. A higher anisotropy value suggests that the sentence embeddings are less distinguishable by Cosine distance. The quantitative results presented in Table 5 show that the anisotropy values of LLMs are considerably lower in comparison to those of RoBERTa.</p>
<h2>6. Analysis</h2>
<h3>6.1. Performance of More LLMs</h3>
<table>
<thead>
<tr>
<th style="text-align: center;">ID Dataset</th>
<th style="text-align: center;">PTMs</th>
<th style="text-align: center;">Maha</th>
<th style="text-align: center;">Cosine</th>
<th style="text-align: center;">MSP</th>
<th style="text-align: center;">Energy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.571</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.905</td>
<td style="text-align: center;">0.368</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA2-7B</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.997</td>
<td style="text-align: center;">0.917</td>
<td style="text-align: center;">0.516</td>
</tr>
<tr>
<td style="text-align: center;">CLINC-Banking (Full)</td>
<td style="text-align: center;">OPT-6.7B</td>
<td style="text-align: center;">0.921</td>
<td style="text-align: center;">0.932</td>
<td style="text-align: center;">0.915</td>
<td style="text-align: center;">0.922</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA2-7B</td>
<td style="text-align: center;">0.967</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.944</td>
<td style="text-align: center;">0.937</td>
</tr>
</tbody>
</table>
<p>Table 6: OOD detection performance of various LLMs. AUROC result is reported.</p>
<p>Additionally, we test OPT-6.7B (Zhang et al., 2022) and LLaMA2-7B (Touvron et al., 2023b) here with settings that zero-grad OOD performance for SST-2 (ID) and fine-tuned OOD performance for CLINC-Banking, as shown in Table 6. For simplicity, we report the AUROC result of each OOD score function. Overall, the OOD detection performance of LLaMA2-7B surpasses that of LLaMA-7B, and LLaMA-7B outperforms OPT-6.7B, which is consistent with the general performance trends observed in these models.</p>
<h3>6.2. Impact of Quantization</h3>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">SST-2</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Quantized</td>
<td style="text-align: left;">Maha</td>
<td style="text-align: left;">Cosine</td>
<td style="text-align: left;">MSP</td>
<td style="text-align: left;">Energy</td>
</tr>
<tr>
<td style="text-align: left;">float32</td>
<td style="text-align: left;">0.991</td>
<td style="text-align: left;">0.990</td>
<td style="text-align: left;">0.905</td>
<td style="text-align: left;">0.368</td>
</tr>
<tr>
<td style="text-align: left;">float16</td>
<td style="text-align: left;">0.990</td>
<td style="text-align: left;">0.987</td>
<td style="text-align: left;">0.893</td>
<td style="text-align: left;">0.331</td>
</tr>
<tr>
<td style="text-align: left;">Int8</td>
<td style="text-align: left;">0.955</td>
<td style="text-align: left;">0.960</td>
<td style="text-align: left;">0.874</td>
<td style="text-align: left;">0.228</td>
</tr>
</tbody>
</table>
<p>Table 7: Impact of quantization for OOD detection.
We test the zero-grad OOD detection performance of LLaMA-7B with different quantized levels (float32, float16, and Int8) for SST-2 as the ID task, shown in Table 7. It was observed that the float16 model can largely preserve the OOD detection capability of the model. However, 8-bit quantization leads to a degradation in its capability.</p>
<h3>6.3. Error Analysis</h3>
<p>We mainly analyze the fine-tuning OOD detection performance in the near-OOD setting since both RoBERTa (Liu et al., 2019) and LLaMA (Touvron et al., 2023a) can achieve near-perfect performance in the far-OOD setting. Here, we provide the OOD detection performance with fine-tuned RoBERTa in the following Table 8, as a complement to Table 3.</p>
<p>Overall, LLaMA's detection capabilities are much stronger than RoBERTa's. Through the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">PTMs</th>
<th style="text-align: center;">CLINC-Banking (Full)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Maha</td>
<td style="text-align: center;">Cosine</td>
<td style="text-align: center;">MSP</td>
<td style="text-align: center;">Energy</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.936</td>
<td style="text-align: center;">0.930</td>
</tr>
<tr>
<td style="text-align: center;">RoBERTa-L</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">0.793</td>
<td style="text-align: center;">0.670</td>
<td style="text-align: center;">0.717</td>
</tr>
</tbody>
</table>
<p>Table 8: Performance comparison between LLaMA and RoBERTa-L in the near-OOD setting. AUROC result is reported for each score function.
analysis of specific error cases, we found that RoBERTa struggles to distinguish semantically similar OOD samples. For instance, when choosing the "bank_balance" class (e.g., the sentence "what's my account balance") as the ID distribution, RoBERTa tends to incorrectly classify the majority of "bill_balance" class inputs (such as "what are my bills this month") as "bank_balance". In contrast, LLaMA generally makes correct judgments in most cases. We attribute this phenomenon to the anisotropy characteristic which is explained in Section 5.2), i.e., sentence embeddings produced by the BERT family models have been noted to possess an undesirable characteristic of being concentrated within a narrow cone, causing representation degeneration. Despite this, in the cases involving extremely similar semantics, LLaMA also makes errors in judgment, such as misclassifying "bill_balance: what is the balance on my bills" as "bank_balance".</p>
<h2>7. Conclusion</h2>
<p>This paper has delved into the critical realm of OOD detection within the context of LLMs. The growing utilization of LLMs across various natural language processing tasks has underscored the need to understand their capabilities and limitations, especially in scenarios involving distribution shifts. Our work deepens the comprehension of OOD detection capabilities of LLMs. Through meticulous analysis, we have showcased the effectiveness of LLMs for OOD detection under various settings, including zero-grad and generative fine-tuning scenarios. Our findings reveal that a simple OOD detector utilizing the cosine similarity function outperforms other sophisticated OOD detectors, especially in the few-shot setting. Our work may serve as a foundational stepping stone for future advancements in effectively and responsibly harnessing the potential of LLMs in diverse environments.</p>
<h2>Acknowledgments</h2>
<p>We thank the anonymous reviewers for their valuable feedback. This research was partially supported by the grant of HK ITF ITS/359/21FP.</p>
<h2>8. Bibliographical References</h2>
<p>Udit Arora, William Huang, and He He. 2021. Types of out-of-distribution texts and how to detect them. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10687-10701, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Matt Post, Raphael Rubino, Carolina Scarton, Lucia Specia, Marco Turchi, Karin Verspoor, and Marcos Zampieri. 2016. Findings of the 2016 conference on machine translation. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 131-198, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Hyunsoo Cho, Choonghyun Park, Junyeop Kim, Hyuhng Joon Kim, Kang Min Yoo, and Sanggoo Lee. 2023. Probing out-of-distribution robustness of language models with parameterefficient transfer learning methods. arXiv preprint arXiv:2301.11660.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320-335.</p>
<p>Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. 2016. Multi30k: Multilingual english-german image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70-74.</p>
<p>Kawin Ethayarajh. 2019. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. arXiv preprint arXiv:1909.00512.</p>
<p>Yujie Feng, Zexin Lu, Bo Liu, Liming Zhan, and Xiao-Ming Wu. 2023. Towards llmdriven dialogue state tracking. arXiv preprint arXiv:2310.14970.</p>
<p>Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019. Representation degeneration problem in training natural language generation models. arXiv preprint arXiv:1907.12009.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2017. A baseline for detecting misclassified and out-ofdistribution examples in neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. 2020. Pretrained transformers improve out-ofdistribution robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2744-2751.</p>
<p>Dan Hendrycks and Mantas Mazeika. 2022. Xrisk analysis for ai research. arXiv preprint arXiv:2206.05862.</p>
<p>Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.</p>
<p>Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Machine learning proceedings 1995, pages 331-339. Elsevier.</p>
<p>Stefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. 2019. An evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027.</p>
<p>Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. 2006. A tutorial on energybased learning. Predicting structured data, 1(0).</p>
<p>Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. 2018. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31.</p>
<p>Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.</p>
<p>Ziqian Lin, Sreya Dutta Roy, and Yixuan Li. 2021. Mood: Multi-level out-of-distribution detection. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 15313-15323.</p>
<p>Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. 2020. Energy-based out-of-distribution detection. Advances in neural information processing systems, 33:21464-21475.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142-150.</p>
<p>Leland McInnes, John Healy, and James Melville. 2018. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426.</p>
<p>Yifei Ming, Hang Yin, and Yixuan Li. 2022. On the impact of spurious correlation for out-ofdistribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10051-10059.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13675-13682.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, and Mohammad Sabokrou. 2022. A unified survey on anomaly, novelty, open-set, and out of-distribution detection: Solutions and future challenges. Trans. Mach. Learn. Res., 2022.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Rheeya Uppaal, Junjie Hu, and Yixuan Li. 2023. Is fine-tuning needed? pre-trained language models are near perfect for out-of-domain detection. arXiv preprint arXiv:2305.13282.</p>
<p>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification.</p>
<p>Adina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick</p>
<p>von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. 2021. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.</p>
<p>Li-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, XiaoMing Wu, and Albert Lam. 2021. Out-of-scope intent detection with self-supervision and discriminative training. arXiv preprint arXiv:2106.08616.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<p>Wenxuan Zhou, Fangyu Liu, and Muhao Chen. 2021. Contrastive out-of-distribution detection for pretrained transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1100-1111.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ We use LlamaForSequenceClassification provided by Huggingface (Wolf et al., 2020)&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
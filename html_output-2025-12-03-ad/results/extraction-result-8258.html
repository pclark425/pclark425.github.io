<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8258 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8258</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8258</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273532235</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.17529v1.pdf" target="_blank">Navigate Complex Physical Worlds via Geometrically Constrained LLM</a></p>
                <p><strong>Paper Abstract:</strong> This study investigates the potential of Large Language Models (LLMs) for reconstructing and understanding the physical world based solely on textual knowledge. It explores the impact of model performance on spatial understanding abilities by introducing a set of geometric conventions and developing a workflow based on multi-layer graphs and multi-agent systems. The study examines how LLMs achieve multi-step and multi-objective geometric inference in a spatial environment, using unified geometric conventions and a graph-driven framework. A genetic algorithm, inspired by large-scale model knowledge, is employed to solve geometric constraint problems, enhancing the spatial reasoning capabilities of LLMs. This work innovatively explores the feasibility of using text-based LLMs as builders of the physical world and designs a workflow to enhance their spatial comprehension and construction capabilities.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8258",
    "paper_id": "paper-273532235",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0027044999999999994,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Navigate Complex Physical Worlds via Geometrically Constrained LLM</p>
<p>Yongqiang Huang 
College of Energy Engineering
Zhejiang University</p>
<p>Wentao Ye 
College of Computer Science and Technology
Zhejiang University</p>
<p>Liyao Li liliyao@zju.edu.cn 
College of Computer Science and Technology
Zhejiang University</p>
<p>Junbo Zhao j.zhao@zju.edu.cn 
College of Computer Science and Technology
Zhejiang University</p>
<p>Navigate Complex Physical Worlds via Geometrically Constrained LLM
153DDE7533C4EDF0DBC9CDB72C1BB3CD
This study investigates the potential of Large Language Models (LLMs) for reconstructing and constructing the physical world solely based on textual knowledge.It explores the impact of model performance on spatial understanding abilities.To enhance the comprehension of geometric and spatial relationships in the complex physical world, the study introduces a set of geometric conventions and develops a workflow based on multi-layer graphs and multi-agent system frameworks.It examines how LLMs achieve multi-step and multiobjective geometric inference in a spatial environment using multi-layer graphs under unified geometric conventions.Additionally, the study employs a genetic algorithm, inspired by large-scale model knowledge, to solve geometric constraint problems.In summary, this work innovatively explores the feasibility of using text-based LLMs as physical world builders and designs a workflow to enhance their capabilities.</p>
<p>Introduction</p>
<p>LLMs acquire extensive world knowledge embedded in textual data through pre-training.This raises an intriguing question: can LLMs reconstruct and simulate the physical world using this textual knowledge?The physical world, characterized by complex geometric and physical constraints, can be abstracted into fundamental geometric shapes.Utilizing a custom-designed engine, we simplify the 3D world's geometric content into basic cube combinations.This work pioneers the exploration of text-only LLMs as potential builders of the physical world, leveraging their pre-trained knowledge to understand and generate 3D spatial representations purely from textual descriptions.</p>
<p>Some preliminary work on world-building has explored constructing 3D worlds at the image level.Techniques like 3D-VAE-GAN (Wu et al., 2016) and Pix2Vox (Xie et al., 2019) combine Variational Autoencoders (VAEs) (Kingma and Welling, 2013) and Generative Adversarial Networks (GANs) (Goodfellow et al., 2020) to generate high-quality 3D models with precise shape and pose control.AtlasNet (Groueix et al., 2018) approximates 3D surfaces by learning a set of 2D textures, effectively handling irregular topologies.Despite their impressive quality, these models struggle with simulating complex physical interactions and maintaining spatial consistency due to intricate and dynamic geometric constraints (Li et al., 2024).</p>
<p>Some methods rely on high-precision geometric libraries or external knowledge bases for humanlevel prior knowledge.For instance, Sun et al. (2023) and Zhou et al. (2024) use LLMs to generate 3D scene images by calling Blender APIs based on user requirements.Wu et al. (2024) proposes combining external knowledge bases to generate 3D scenes from sketches.However, these methods heavily depend on external libraries and interfaces, which lack flexibility and face challenges like resource maintenance, copyright disputes, and network security issues (Gao et al., 2014).</p>
<p>We explored how to leverage LLM pre-training knowledge to autonomously guide complex geometric constraints.Our evaluation compared the spatial construction and geometric relationship understanding abilities of GPT-3.5-turbo and GPT-4, revealing that GPT-4 excels in spatial construction tasks due to its superior performance.we also introduced an innovative multi-agent approach for 3D scene construction, establishing geometric conventions at three levels (center, axis, and surface) to standardize the spatial relationships of 3D objects as understood by LLMs.This multi-level graphdriven approach enhances the spatial understanding and reasoning capabilities of LLMs.The workflow ensures information consistency and uniformity, mitigating data silos and redundancy issues, while enabling LLMs to explore their ability to understand geometric relationships of physical world.The application of GANs and VAEs in 3D scene generation has made notable progress in recent years.Chan et al. (2022) provides a method which can synthesize high-resolution, multi-view consistent images in real-time and also generate highquality 3D geometry.Xie et al. (2019) proposes a context-aware convolutional neural network to reconstruct 3D voxel models from single and multiview images.This method uses GANs to enhance the detail and structural accuracy of the generated 3D models.Wu et al. (2016) combines GAN for generating and controlling 3D objects, producing high-quality 3D models with shape control.Groueix et al. (2018) introduces a 3D surface generation method by learning a collection of 2D maps to approximate 3D surfaces, handling irregular topologies.Besides, Tang et al. (2024) find a method to use 2D diffusion model which can further control the generated content and inject reference-view information for unseen views.</p>
<p>These works typically offer high quality and realism, creativity, and diversity in generated content.However, they also face challenges such as high data dependency, complexity, and computational intensity.Moreover, such work often overlooks the complex geometric relationships between objects in the physical world.</p>
<p>Generation Based On External Libraries</p>
<p>The quality and availability of numerous 3D models have significantly improved.Tang et al. (2024) provide a large amount of 3D materials.And Zhou et al. (2018) provide an open-source library that supports rapid development of software for processing 3D data.It benefits research that utilizes LLMs to invoke open-source models and achieve scene graph construction.Sun et al. (2023), based on a multi-agent system, call the Blender interface to generate 3D scene images according to user requirements.SceneX (Zhou et al., 2024) employs LLMs to drive procedural modeling, utilizing Blender APIs and a vast array of procedural assets.Wu et al. (2024) offer an approach that combines user sketches with external knowledge, progressively generating 3D scenes through a scene diffusion model.Their work demonstrates how these agents can leverage external tools and model libraries to automate the construction and understanding of scene graphs.</p>
<p>Utilizing existing model libraries offers significant advantages in terms of efficiency, scalability, and flexibility in scene generation.However, due to the heavy reliance on external libraries and external materials, the work in question exhibits inconsistent material quality, poses high maintenance complexity, demonstrates insufficient flexibility, and involves copyright challenges.</p>
<p>Method</p>
<p>Graph Runs Through the Entire Workflow</p>
<p>Multi-agent systems have demonstrated effective performance in segmenting complex problems into numerous sub-problems and resolving them (Grossi et al., 2023), aligning with the step-bystep decomposition of three-dimensional scene concepts and the meticulous refinement of generated content at each stage in this work.And implementing information alignment between proxy groups is a huge challenge (Han et al., 2024).Inspired by Qi et al. ( 2023) and Ranasinghe et al. (2024), we choose graph database as the medium.</p>
<p>In our work, we use GPT-4 (OpenAI, 2023b) as the basis for the agent and Neo4j (Neo4j, 2023) database to store our graph.By employing a graph database to capture spatial information and representing shapes and their geometric relationships with nodes and edges, complex geometric relationships can be managed flexibly.The graph database records scene information, providing a comprehensive overview of user objectives and scene graphs throughout the workflow.This ensures that generated scenes align with predefined spatial constraints and design specifications by integrating relational processing with large model generation capabilities, offering a flexible and efficient solution for managing complex spatial data and scene generation.</p>
<p>Scenery Designer</p>
<p>Graph databases can stably and comprehensively record object information in existing scenes, thereby reducing scene graph generation errors caused by illusions or memory problems in LLMs, such as reconstructing existing objects or using nonexistent objects as reference points.By providing detailed scene information to LLMs, the graphics database helps to develop plans that are consistent</p>
<p>Figure 1: The entire workflow is based on geometric conventions and relies on multiple agents to carry out 3D scene construction work around the graph.The user's demand information will be refined layer by layer by designers and used to generate object instances.Finally, the arranger will use the mapping from geometric constraints to deviations and a genetic algorithm solver to determine the correct placement position of the object.</p>
<p>with the given semantics and do not conflict with the current scene graph.Based on this, the scene designer will mobilize their internal world knowledge to design a scene that is semantically consistent with the input, including the main objects in the scene and the spatial geometric relationships between objects.</p>
<p>Object Designer</p>
<p>After the scene planning is completed, the object designer needs to design objects with appropriate structure and size based on the existing reference objects in the scene.On the one hand, image databases are needed to provide background information, and on the other hand, LLMs themselves require a certain level of common sense knowledge and reasoning ability to lay a more detailed foundation for the next step of object creation.</p>
<p>Object Manufacturer</p>
<p>After completing the object design phase, we proceed to the construction phase.At this stage, LLMs require a thorough understanding of the descriptive statements used by object designers, particularly those describing the interrelationships between internal modules of the object.This ensures alignment between the generated objects and their descriptive statements.We have observed that models with weaker performance, such as the GPT-3.5 turbo(OpenAI, 2023a), often have poor performance in this step, regardless of the level of detail provided by the designer.Additionally, to minimize the risk of spatial divergence when using genetic algorithms in later permutation calculations, the initial position of the object should be proximate to its main reference object, typically adhering to their relative spatial relationships.Here, a graphical database becomes crucial, as it offers detailed information about the size and position of reference objects, as well as their approximate relative relationships.This information is essential to guide LLMs in utilizing their internal knowledge effectively.</p>
<p>Arranger</p>
<p>Following the construction of the object, further optimization of its spatial position is required to meet specific spatial requirements, such as those related to smaller particle sizes.Initially, the relationship information between the newly constructed object and the reference object must be extracted from the graph database.This information is then used to perform further inference and to supplement any missing spatial constraints.Based on these completed spatial constraints, the appropriate constraint equations can be selected for positional optimization.</p>
<p>The graph database provides a comprehensive understanding of global scene information at each layer of the workflow and provides necessary in-formation for each layer to complete tasks.It can efficiently manage complex relationships and dependencies, enabling each level to accurately locate and process relevant information in complex scenarios.</p>
<p>Geometric Conventions</p>
<p>Inspired by the work of Hedau (2011) and Klein (1998), we recognize that clearly and systematically representing the relative positions of objects in space is beneficial for enhancing the spatial reasoning capabilities of LLMs.Consequently, we have devised a spatial convention that encompasses three levels of constraint relationships: geometric center, axis, and surface, with varying degrees of constraint strength.By integrating different spatial conventions, we can flexibly and accurately determine the positions of objects within a reasonable range.This set of spatial conventions is integral to our entire workflow.Through the implementation of a unified spatial convention system, we ensure consistency and standardization throughout the workflow.</p>
<p>An example of the spatial convention we designed is as follows:</p>
<p>Geometric Center Relationship Constrain</p>
<p>• Concentric relationship:
x c m = x c r , y c m = y c r and z c m = z c r (1)</p>
<p>Axle Relationship Constraint</p>
<p>• x align:
x c m = x c r(2)
• front half:
x c r &gt; x c m (3)</p>
<p>Surface Relationship Constraint</p>
<p>• front:
x b r − x f m = d(4)
• coplanar front:
x t r = x t m (5)
To avoid misunderstandings, we briefly declare the following symbols:</p>
<p>• x, y and z represent the projections of the corresponding parts of the object on that axis</p>
<p>• In superscripts, f, b and t, etc. respectively represent the corresponding surfaces of the object, such as the front, back/bottom, and top surfaces.And c represents the geometric center.</p>
<p>• In the subscript, r and m represent the reference object and the object to be moved, respectively.And d stands for distance.</p>
<p>Graph Driven LLM Spatial Inference</p>
<p>The final layer of the workflow is called the arranger, responsible for the spatial arrangement of generated objects in the scene.Wei et al. (2024)discussed Detailed introduction on how to construct a knowledge graph of geographic spatial data, as well as how to express and infer spatial relationships.Inspired by this, this work maps the relative positional relationships of objects to a graphics database.By setting strong and weak reference objects, we provide different levels of constraints for the object to be moved.With the continuous enrichment of graphic information, our framework will provide increasingly accurate spatial constraints.</p>
<p>After determining the spatial constraints, the LLM inspired genetic algorithm is used to solve the spatial constraints, which is used to update the spatial position of the object to be moved and dynamically update the graphic data.This layer utilizes a graphical database to store entities and their spatial relationships, establishing and updating spatial constraints at the granularity of objects.The process specifically includes several steps:</p>
<p>Graph Database Interaction</p>
<p>Arranger interacts with graphical databases to generate more detailed relationship information and select the correct constraint equation according to it.Based on the provided rough relationship pairs, the arranger select the strong reference object which will provides 1 to 3 constrains from the graph database and return the weak reference objects which provides 0 to 2 constrains and be associated with the strong reference object.In this way, the computational complexity of constraints can be reduced.The LLM agent will obtain various types of information about the reference object, including its dimensions and spatial positions.It will then infer and add new spatial constraints within the basic spatial constraint framework and select the correct constraint equation for genetic algorithm calculation of accurate spatial positioning.</p>
<p>Genetic Algorithm for Solving Geometric Relationships</p>
<p>Given the global optimization capabilities of the genetic algorithm and its effective use with heuristic initialization, we ultimately opted for the genetic algorithm to address the spatial constraints.When LLM completes spatial constraints and selects the correct geometric equation, the permutator pass the parameters to the genetic algorithm (Shapiro, 1999) solver to optimize the geometric relationships and further adjust and update the spatial position of the objects initialized by LLM.</p>
<p>Each object is composed of multiple blocks, with each block represented by its centroid coordinates and three-dimensional dimensions.The specific representation is as follows:</p>
<p>Single block representation:
b i = {c i , d i1 , d i2 , d i3 }
where c i = (x i , y i , z i ) is the centroid coordinates, and d i1 , d i2 , d i3 represent the length, width, and height, respectively.</p>
<p>Object representation:
O i = {b i1 , b i2 , . . . , b in }
where O i represents an object composed of multiple blocks b ij .In addition, the spatial information of objects can also be represented as follows:
O i = {C i , D i1 , D i2 , D i3 }
where C i is the centroid coordinates, and D i1 , D i2 , D i3 represent the length, width, and height of O i respectively.</p>
<p>We define various types of spatial constraints to describe the relative spatial relationships between objects.Below are examples of above, and upper half:</p>
<dl>
<dt>above</dt>
<dd>z b m ≥ z t r + d upper half : z c m ≥ z c r
To generate appropriate constraint equations, we abstract the reference object as a block and generate movable object pairs with reference part relationships for each object.Then, based on the generated relationship pairs, we generate appropriate constraint equations and pass them to the genetic algorithm for solution.</dd>
</dl>
<p>Assume we have multiple reference objects R k and a movable object M , each pair (R k , relation, M ) can be represented as a set of constraint formations:
e i = max (0, z c m − z c r + d) , if above max (0, z c r − z c m ) , if upper half (6)
The optimization goal is to minimize the total error:
min E = min N i=1 e 2 i
To determine effective motion vectors, we employed a genetic algorithm inspired by LLM initialization.Objects are generated at specific positions based on global and reference content, partially fulfilling constraint requirements.The algorithm's initialization is then refined based on the size of both the reference object and the object to be moved, enhancing the optimization process.Each genome consists of three XYZ coordinates representing motion vectors.The total error E of each individual is calculated to assess fitness, with top-performing individuals selected for crossover and mutation.</p>
<p>During crossover, parent DNA combines to produce new offspring, and mutations make fine adjustments to coordinates.This process iterates until a set number of generations or error convergence is achieved, gradually approaching the optimal solution.</p>
<p>Experiment</p>
<p>In this section, we will discuss the factors affecting the quality of the 3D scene graph generated by the LLM from two aspects.The first influencing factor is the model's ability.We test the generation performance of the base models GPT-3.5-Turbo and GPT-4 without using the framework.The second influencing factor is the degree of integration with the work framework.We set up three sets of experiments to explore the complete use of the work framework, including ablation experiments to analyze the impact of removing certain components.</p>
<p>Model Performance Impact</p>
<p>Our experiment found a strong correlation between LLM performance and spatial understanding.Evaluating GPT-3.5-Turbo and GPT-4-0125 on object and scene generation tasks, we observed that GPT-3.5 had poor spatial comprehension and simplistic outputs.In contrast, GPT-4 showed improved spatial concepts and multi-object scene generation but still used simple blocks with limited detail.Figure 2: GPT-4 produces complex structures and details and achieves better semantic alignment than GPT-3.5.</p>
<p>Object Generation</p>
<p>Scenery Generation</p>
<p>Analysis And Comparison</p>
<p>Metric:We choose CLIP (Radford et al., 2021)to calculate the similarity between the generated object and scene images and text, in order to evaluate the alignment between the text and the generated content.In addition, during the experimental process, there is often a large amount of overlap or object isolation in the generated failed scene images.Therefore, for the scene, we additionally introduced overlap score and isolation score, corresponding to the proportion of overlapping volume to the total volume of all objects and the proportion of isolated blocks to the total block, respectively.Figure 5: In the scenario level generation task, the clip index of GPT-4 group is 10.1% higher than that of GPT-3.5 group, and its isolation rate is much better than that of GPT-3.5 group.</p>
<p>Framework Impact</p>
<p>Baseline Methods:The baseline we have chosen is a single agent without designed agents or graph driven methods, which showed in Figure 3.The base model of each agent is gpt-4-0125 preview with default temperature.In the ablation group experiment, we eliminated the interaction process between the graphical database and the workflow, while retaining the workflow of multi-agent collaboration.The non ablated group completely retained the graph reasoning framework.</p>
<p>Ablation Study</p>
<p>Analysis And Comparison</p>
<p>The schematic diagram illustrates the performance of LLM scene graph generation in three modes.</p>
<p>Images produced by the baseline method neglect object details but exhibit some overall spatial planning capability.The ablation group attempts to emphasize object details but lacks spatial planning, leading to overcrowded scenes.The non-ablated group excels in both object details and proper object placement.According to the above Figure 7, we found that in terms of clip similarity, the graph driven group performed better than both the baseline and ablation groups, and was generally better than both in a single task, with mean values 6.3% and 8.7% higher than the baseline and ablation groups, respectively.In terms of object overlap rate, it is lower than both, but in terms of isolation rate, it is higher than both.</p>
<p>Conclusion And Limitation</p>
<p>Our research provides an intuitive demonstration of the spatial understanding capabilities of LLMs and quantitatively evaluates the spatial comprehension of two distinct models.Additionally, we enhance the geometric understanding and spatial reasoning abilities of LLMs in complex physical environments by implementing well-defined geometric conventions and a graph-driven framework.</p>
<p>This study is conducted using a customdeveloped sandbox platform, designed to present the spatial concepts understood by LLMs in a more intuitive and flexible manner.However, due to resource constraints, we are unable to test higherperforming models, which limits our ability to fully showcase the framework's potential in improving the spatial understanding of LLMs.</p>
<p>• Determine if the bottom edges of the two objects are coplanar.</p>
<ol>
<li>coplanar left: Coplanar on the left.</li>
</ol>
<p>• Determine if the left edges of the two objects are coplanar.</p>
<ol>
<li>coplanar right: Coplanar on the right.</li>
</ol>
<p>• Determine if the right edges of the two objects are coplanar.</p>
<ol>
<li>coplanar front: Coplanar in front.</li>
</ol>
<p>• Determine if the front edges of the two objects are coplanar.</p>
<ol>
<li>coplanar back: Coplanar in the back.</li>
</ol>
<p>• Determine if the back edges of the two objects are coplanar.</p>
<p>B Objects Generated With Workflow</p>
<p>Figure 3: GPT-4 shows better spatial comprehension and multi-object scene generation than GPT-3.5, but still uses simple blocks with limited detail.</p>
<p>Figure 4 :
4
Figure 4: In object level generation tasks, the clip index of agents based on GPT-4 is generally better than ones based on GPT-3.5.</p>
<p>Figure 6 :
6
Figure 6: The ablation group showed detailed structures, but lacked reasonable spatial planning The non-ablated group can not only represent details of objects but also have a reasonable plan for the placement of objects.</p>
<p>Figure 7 :
7
Figure 7: Comparison of metrics across different work modes indicates the following information: using graph driven workflows improves the similarity between images and text, with a decrease in spatial overlap rate but an increase in isolation rate</p>
<p>Figure</p>
<p>Figure 8: Bench</p>
<p>Figure</p>
<p>Figure 10: Counter</p>
<p>Figure</p>
<p>Figure 13: Dumbbell Rack</p>
<p>Figure</p>
<p>Figure 16: Table</p>
<p>Figure</p>
<p>Figure 17: TV</p>
<p>. coplanar bottom: Coplanar on the bottom.
Qian-YiZhou, Jaesik Park, and Vladlen Koltun. 2018.Open3d: A modern library for 3d data processing.A Geometric conventionsA.1 Geometric Center Relationship Constrain 1. concentric: Concentric.• Calculation: The Euclidean distance between the centers of the two objects.A.2 Axle Relationship ConstraintA.2.1 Align Relationship 1.x aligned: X-aligned.• Calculation: The alignment error in the x direction between the two objects.2. y aligned: Y-aligned.• Calculation: The alignment error in the y direction between the two objects.3. z aligned: Z-aligned.• Calculation: The alignment error in the z direction between the two objects.A.2.2 Half Side Relationship1. left half: Left half.• Determine if the ref center object is in the left half of the mov center object.2. right half: Right half.• Determine if the ref center object is in the right half of the mov center object.3. upper half: Upper half.• Determine the ref center object is in the upper half of the mov center object.4. lower half: Lower half.• Determine if the ref center object is in the lower half of the mov center object.5. front half: Front half.• Determine if the ref center object is in the front half of the mov center object.6. back half: Back half.• Determine if the ref center object is in the back half of the mov center object.A.3 Surface Relationship ConstraintA.3.1 Relative Positioning Relationship 1. left: mov center object is to the left of the ref center object.• Calculation: The distance between the left edge of the ref center object and the right edge of the mov center object minus the given distance.2. right: mov center object is to the right of the ref center object.• Calculation: The distance between the left edge of the mov center object and the right edge of the ref center object minus the given distance.3. above: mov center object is above the ref center object.• Calculation: The distance between the bottom edge of the mov center object and the top edge of the ref center object minus the given distance.4. below: mov center object is below the ref center object.• Calculation: The distance between the bottom edge of the ref center object and the top edge of the mov center object minus the given distance.5. front: mov center object is in front of the ref center object.• Calculation: The distance between the back edge of the mov center object and the front edge of the ref center object minus the given distance.6. back: mov center object is behind the ref center object.• Calculation: The distance between the back edge of the ref center object and the front edge of the mov center object minus the given distance.A.3.2 Coplanar Relationship Constrain1. coplanar top: Coplanar on top.• Determine if the top edges of the two objects are coplanar.
Efficient geometry-aware 3d generative adversarial networks. Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Active exploration of large 3d model repositories. Lin Gao, Yan-Pei Cao, Yu-Kun Lai, Hao-Zhi Huang, Leif Kobbelt, Shi-Min Hu, IEEE transactions on visualization and computer graphics. 21122014</p>
<p>Generative adversarial networks. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Communications of the ACM. 63112020</p>
<p>Advances in multi-agent systems research: Eumas 2021 extended selected papers. Davide Grossi, Ariel Rosenfeld, Nimrod Talmon, SN Computer Science. 455872023</p>
<p>A papier-mâché approach to learning 3d surface generation. Thibault Groueix, Matthew Fisher, Vladimir G Kim, Bryan C Russell, Mathieu Aubry, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Llm multi-agent systems: Challenges and open problems. Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, Zhaozhuo Xu, Chaoyang He, 2024</p>
<p>3D spatial layout and geometric constraints for scene understanding. Chandrashekhar Varsha, Hedau, 2011University of Illinois at Urbana-Champaign</p>
<p>P Diederik, Max Kingma, Welling, arXiv:1312.6114Autoencoding variational bayes. 2013arXiv preprint</p>
<p>The role of constraints in geometric modelling. Rüdiger Klein, Geometric Constraint Solving and Applications. Springer1998</p>
<p>Advances in 3d generation: A survey. Xiaoyu Li, Qi Zhang, Di Kang, Weihao Cheng, Yiming Gao, Jingbo Zhang, Zhihao Liang, Jing Liao, Yan-Pei Cao, Ying Shan, 10.1145/3589132.3625597arXiv:2401.17807Inc. Neo4j. 2023. Neo4j Documentation. OpenAI. 2023a. Gpt-3.5 turbo. OpenAI. 2023b. Gpt-4 technical report. OpenAI. Jianzhong Qi, Zuqing Li, and Egemen Tanin. 2023. Maasdb: Spatial databases in the era of large language models. ACM202423arXiv preprintProceedings of the 31st ACM International Conference on Advances in Geographic Information Systems</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International Conference on Machine Learning. PMLR2021</p>
<p>Learning to localize objects improves spatial reasoning in visual-llms. Kanchana Ranasinghe, Omid Satya Narayan Shukla, Michael S Poursaeed, Tsung-Yu Ryoo, Lin, 2024</p>
<p>Genetic algorithms in machine learning. Jonathan Shapiro, Advanced Course on Artificial Intelligence. Springer1999</p>
<p>Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould, arXiv:2310.129453d-gpt: Procedural 3d modeling with large language models. 2023arXiv preprint</p>
<p>Cycle3d: High-quality and consistent imageto-3d generation via generation-reconstruction cycle. Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin, Li Yuan, 2024</p>
<p>Construct and query a finegrained geospatial knowledge graph. Bo Wei, Xi Guo, Xiaodi Li, Ziyan Wu, Jing Zhao, Qiping Zou, Data Science and Engineering. 922024</p>
<p>Learning a probabilistic latent space of object shapes via 3d generativeadversarial modeling. Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, Josh Tenenbaum, Advances in Neural Information Processing Systems. 201629</p>
<p>External knowledge enhanced 3d scene generation from sketch. Zijie Wu, Mingtao Feng, Yaonan Wang, He Xie, Weisheng Dong, Bo Miao, Ajmal Mian, arXiv:2403.141212024arXiv preprint</p>
<p>Pix2vox: Context-aware 3d reconstruction from single and multi-view images. Haozhe Xie, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Shengping Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Mengqi Zhou, Jun Hou, Chuanchen Luo, Yuxi Wang, Zhaoxiang Zhang, Junran Peng, arXiv:2403.15698Scenex: Procedural controllable large-scale scene generation via large-language models. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
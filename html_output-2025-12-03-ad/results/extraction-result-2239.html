<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2239 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2239</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2239</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-277468219</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.00707v2.pdf" target="_blank">Interleaved Multitask Learning with Energy Modulated Learning Progress</a></p>
                <p><strong>Paper Abstract:</strong> As humans learn new skills and apply their existing knowledge while maintaining previously learned information,"continual learning"in machine learning aims to incorporate new data while retaining and utilizing past knowledge. However, existing machine learning methods often does not mimic human learning where tasks are intermixed due to individual preferences and environmental conditions. Humans typically switch between tasks instead of completely mastering one task before proceeding to the next. To explore how human-like task switching can enhance learning efficiency, we propose a multi task learning architecture that alternates tasks based on task-agnostic measures such as"learning progress"and"neural computational energy expenditure". To evaluate the efficacy of our method, we run several systematic experiments by using a set of effect-prediction tasks executed by a simulated manipulator robot. The experiments show that our approach surpasses random interleaved and sequential task learning in terms of average learning accuracy. Moreover, by including energy expenditure in the task switching logic, our approach can still perform favorably while reducing neural energy expenditure.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2239.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2239.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMTL-LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaved Multi-Task Learning with Learning Progress (LP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-task encoder-decoder architecture that interleaves training on tasks based on an intrinsic Learning Progress signal; uses task-specific input/action projections and encoders, a shared encoder, and a shared multi-head attention module with task flag bits to enable dynamic, task-focused information sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IMTL-LP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared encoder + task-specific encoders and decoders; task-aligned state/action projection layers; shared multi-head attention (MHA) over stacked task-specific latent representations with a training-flag bit; task arbitration chooses one task per iteration based on learning progress (slope of recent error). Purpose: improve multi-task effect-prediction learning by dynamically selecting and focusing on tasks making positive progress while enabling bidirectional skill transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-specific projection & encoder modules producing per-task latent representations combined with a shared multi-head attention module and an explicit task flag bit to focus queries on the active task; shared encoder provides common features while task-specific modules allocate task-aligned capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction (supervised multi-task learning across Push, Hit, Stack tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Qualitative: converges faster and attains lower prediction error than SINGLE (separate single-task networks) and IMTL-RAND (same architecture with random task selection); particularly large gains on Stack and Push tasks (figures show faster error reduction across 10 seeds). No absolute numeric error values provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Qualitative: SINGLE and IMTL-RAND show higher prediction error and slower convergence; IMTL-RAND improves over SINGLE but does not match IMTL-LP. No absolute numeric error values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reported qualitatively: IMTL-LP is the most energy-intensive among interleaved variants (highest cumulative neuron activations) compared to energy-modulated variants; exact FLOPs / time / memory numbers are not reported — energy measured as cumulative neuron activations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>SINGLE baseline has lower cumulative activations (similar to low-energy IMTL-EMLP variants); exact numeric costs not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Yes — IMTL-LP reduces prediction error faster (i.e., fewer epochs to reach lower error) than SINGLE and IMTL-RAND, indicating higher sample/learning efficiency; no exact sample counts or epoch-to-threshold numbers are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Object- and task-wise skill transfer reported via ablations (∆L metric): positive transfer observed (e.g., Stack→Hit ∆L=0.86 for sphere; various object-specific ∆L values provided). Stack benefits strongly from Hit (∆L ≈ 3.7–5.2) indicating emergent transfer generalization; quantitative ∆L values given per object/task in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Ablations (zeroing key/value rows in shared attention) produce interpretable ∆L effects used to quantify which source tasks the target task relies on; attention+flag improved distinguishability and transfer patterns, aiding interpretability of inter-task dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Positive: simultaneous learning with LP-based interleaving improves overall multi-task performance and benefits mutual transfer, especially for Stack and Push; IMTL-LP allocated less total training iterations to Stack yet achieved higher performance, showing scheduling matters more than raw training time.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Evaluated three network complexities (low: 800 params, medium: 2000, high: 5200). IMTL-LP shows the largest advantage at low capacity (800 params), maintains advantage at medium, and differences shrink at high capacity — indicating task-aligned strategy helps under resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Task-aligned, dynamically scheduled multi-task training (IMTL-LP) yields faster convergence, better final task accuracy and positive task-to-task knowledge transfer versus uniform/random scheduling and single-task baselines, with the advantage most pronounced under low-capacity constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Provides empirical evidence that allocating task-aligned representations and dynamically focusing computation yields superior learning and transfer compared to uniform or random allocation, supporting the Task-Aligned Abstraction Principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2239.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMTL-EMLP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaved Multi-Task Learning with Energy-Modulated Learning Progress (EMLP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Extension of IMTL-LP that discounts Learning Progress by a measure of recent computational energy (cumulative neuron activations), producing a tunable trade-off between predictive performance and energy consumption via a sensitivity coefficient k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IMTL-EMLP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architecture as IMTL-LP but task selection score s_i = exp(k * LP_i) / normalized_EC_i, where EC_i is cumulative neuron activations over recent steps; k controls sensitivity to LP vs energy. Purpose: reduce neural computation while retaining learning gains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-specific encoders and attention-based sharing as in IMTL-LP; dynamic task selection additionally modulated by an energy proxy (cumulative activations) to prioritize lower-cost tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction (same Push, Hit, Stack tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Qualitative: With moderate k (e.g., k=1, k=1.2) IMTL-EMLP closely matches IMTL-LP predictive performance (small sacrifice). As k decreases (prioritizing energy), prediction error increases progressively. No absolute numeric error rates reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Compared to IMTL-LP: IMTL-EMLP variants with low k perform worse in accuracy but use less energy; IMTL-EMLP-K=0.4 consumes the least energy and approaches SINGLE's energy use while having lower predictive performance than IMTL-LP.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Reported as cumulative neuron activations: energy usage decreases as k decreases; IMTL-EMLP-K=0.4 consumes the least energy (nearly same as SINGLE). Exact FLOPs/time not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>IMTL-LP: highest cumulative activations; SINGLE and low-k IMTL-EMLP: lowest. Exact numeric costs absent.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Implicit: moderate-energy IMTL-EMLP variants preserve much of IMTL-LP's speed of learning; low-k variants slow learning and increase final error. No quantitative sample-efficiency numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Not separately reported beyond IMTL-LP; energy-modulated scheduling maintains transfer trends when k high, but strong energy bias (low k) harms learning and thus likely reduces transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>No new interpretability analyses beyond IMTL-LP; mechanism is interpretable as a tunable trade-off between LP and EC.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>IMTL-EMLP with moderate k provides a favorable trade-off: retains multi-task performance close to IMTL-LP while cutting energy; low-k variants degrade multi-task predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>IMTL-EMLP explicitly designed for energy-constrained settings; low-k settings reduce energy to near SINGLE but at cost to accuracy; no latency/memory benchmarks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adding an energy proxy to dynamic task allocation enables a tunable trade-off: you can substantially reduce neural energy (cumulative activations) with modest loss in predictive performance if k is chosen appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Shows that adaptive allocation based on both expected learning benefit and computational cost can preserve much of the benefit of task-aligned representations while improving energy-efficiency, supporting refined task-alignment principles that include cost-awareness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2239.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SINGLE baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Task Learning Baseline (SINGLE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where each task is trained with its own dedicated network (same architecture as multi-task model but without shared parameters), representing uniform, non-shared representations per task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SINGLE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Independent single-task instances of the same encoder-decoder architecture (state/action projection, task-specific encoder/decoder, 'attention' and other modules exist but are per-task only), isolating the effect of shared representations and interleaving.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-dedicated separate models (uniform per-task allocation; no shared task-aligned representation across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Qualitative: SINGLE is outperformed by IMTL-LP and also by IMTL-RAND in experiments; SINGLE consumes relatively low cumulative activations (energy). No numeric error values provided.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Reported qualitatively: SINGLE energy use similar to energy-aware low-k IMTL-EMLP; exact metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Inferior sample efficiency relative to IMTL-LP (slower reduction in prediction error). No numeric values.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Not applicable (no transfer across tasks since networks are independent).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>As separate models, no multi-task sharing; worse overall multi-task performance when compared to shared/interleaved approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Independent per-task models (uniform allocation) perform worse overall and learn slower than dynamically interleaved, shared representations, though they may use less neural activation.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Demonstrates that purely uniform, separate allocation is inferior to adaptive shared/task-aligned schemes for multi-task learning efficiency and transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2239.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMTL-RAND</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interleaved Multi-Task Learning with Random Task Selection (IMTL-RAND)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same multi-task architecture as IMTL-LP but tasks are chosen uniformly at random each training iteration, representing non-adaptive interleaving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IMTL-RAND</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared encoder + task-specific modules and shared attention identical to IMTL-LP; task selection is random (uniform), not guided by Learning Progress or energy.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Task-specific encoders with shared attention (same as IMTL-LP), but static/random scheduling rather than LP-driven dynamic scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Qualitative: IMTL-RAND outperforms SINGLE (shared parameters help) but does not match IMTL-LP; it shows some positive transfer but lacks efficient scheduling, so converges slower than IMTL-LP.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Better than SINGLE but worse than IMTL-LP (slower reduction in error).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Some positive transfer observed (better than SINGLE) but less effective than LP-based scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Shared architecture yields multi-task benefits vs SINGLE, but random scheduling reduces the efficiency of transfer relative to LP-based adaptive scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Sharing representations helps (IMTL-RAND > SINGLE), but adaptive, progress-driven scheduling (IMTL-LP) is required to fully realize multi-task gains.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Shows that shared, task-specific modules are beneficial, but that dynamic/adaptive task selection further improves outcomes — supporting adaptive allocation over naive shared/uniform strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2239.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLOCK</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Blocked Sequential Training Baseline (BLOCK)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline where tasks are trained in uninterrupted blocks (sequentially) in fixed permutations, representing a commonly used non-interleaved training schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BLOCK</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shared multi-task architecture trained on one task for a block of epochs before switching to another task (six possible task-order permutations were evaluated) to assess sensitivity to order and forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Sequential blocked scheduling of tasks; although architecture is multi-task, training schedule is blocked which leads to periods of no adaptation for certain tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Qualitative: BLOCK performs worse than IMTL-LP across all task orders; exhibits catastrophic forgetting and error spikes at task switches; dependent on task order (sensitive).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Less sample efficient than interleaved methods; blocked training leads to forgetting and slower overall convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Negative interference and catastrophic forgetting reduce transfer and generalization compared to interleaved approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Overall inferior multi-task performance and high sensitivity to task order; interleaved LP scheduling uniformly outperforms all blocked permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Blocked/sequential training is sensitive to task order and suffers catastrophic forgetting; dynamic interleaving based on LP outperforms blocked schedules consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Findings support that dynamic, task-aware allocation (interleaving) is superior to static blocked training, reinforcing the benefit of adaptive task-aligned strategies.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2239.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IMTL ablations (no-attn / no-flag / no-attn&flag)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation Variants of IMTL (without attention, without task-flag, without both)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of ablation models removing the shared attention module and/or the task-identifying flag bit to test the contribution of adaptive attention-guided sharing versus more uniform representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>IMTL ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants: (1) full model; (2) without task flag (attention still present); (3) without attention (flags present but attention removed); (4) without both attention and flag (stacked representations forwarded directly). These probe the role of explicit task cues and adaptive attention in enabling task-aligned sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>When attention removed and flag removed the model effectively uses more uniform concatenated representations; attention+flag provides dynamic, task-focused adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic manipulation effect-prediction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Qualitative results: the model without both attention and flag had the worst performance; adding flag or attention individually improved performance; full model (attention+flag) performed best — demonstrating that adaptive attention+task-flag is critical for task-aligned gains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Ablated models converge slower and to higher error than full IMTL-LP; exact numbers not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Ablations reduce positive task transfer (shown indirectly via worse performance and ablation ∆L analyses), indicating attention+flag facilitate beneficial inter-task transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Results support the interpretability of dependencies via attention ablations (∆L) — removing keys/values reveals source-to-target reliance patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Removing adaptive components harms multi-task performance; the combination of attention and flag yields the best multi-task outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Adaptive attention plus an explicit task-flag is necessary to realize the benefits of task-aligned representations; removing these mechanisms yields behavior closer to uniform representations and worse performance.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Ablations show that adaptive, task-focused computation (attention+flag) is instrumental for gains, supporting task-aligned adaptive representation theory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2239.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-Stitch Networks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Stitch Networks for Multi-Task Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously published adaptive feature-sharing architecture that learns linear combinations of activation maps between task-specific networks to adaptively determine feature sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cross-Stitch Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Cross-Stitch Networks</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parallel task-specific networks whose intermediate activations are linearly combined by learned cross-stitch units, enabling input-dependent, learned sharing of features across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Learned linear combination (cross-stitch units) of task-specific activation maps to allocate shared vs task-specific feature usage.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general multi-task learning (vision examples in original paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Mentioned as a prior method for adaptive feature sharing across tasks; contrasts with current work which trains tasks from scratch and interleaves one task per iteration using LP-based arbitration.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2239.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2239.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-gate Mixture-of-Experts (MG-MoE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cited approach that dynamically gates and mixes expert subnetworks per task, enabling task-specific routing of computation (adaptive allocation) in multi-task settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-gate Mixture-of-Experts</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A MoE-style approach where different gates determine which experts are used per task, achieving task-specific allocation of capacity and routing of information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Gating over expert modules (mixture-of-experts) to allocate computation and representation to tasks dynamically.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-task learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Mentioned in related work as an example of an architecture that dynamically allocates capacity per task (mixture-of-experts gating), relevant to the paper's framing of adaptive resource allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cross-Stitch Networks for Multi-Task Learning <em>(Rating: 2)</em></li>
                <li>Distral: Robust multitask reinforcement learning <em>(Rating: 2)</em></li>
                <li>Modeling task relationships in multi-task learning with multi-gate mixture-of-experts <em>(Rating: 2)</em></li>
                <li>GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks <em>(Rating: 2)</em></li>
                <li>Multi-task learning using uncertainty to weigh losses for scene geometry and semantics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2239",
    "paper_id": "paper-277468219",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "IMTL-LP",
            "name_full": "Interleaved Multi-Task Learning with Learning Progress (LP)",
            "brief_description": "A multi-task encoder-decoder architecture that interleaves training on tasks based on an intrinsic Learning Progress signal; uses task-specific input/action projections and encoders, a shared encoder, and a shared multi-head attention module with task flag bits to enable dynamic, task-focused information sharing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IMTL-LP",
            "model_description": "Shared encoder + task-specific encoders and decoders; task-aligned state/action projection layers; shared multi-head attention (MHA) over stacked task-specific latent representations with a training-flag bit; task arbitration chooses one task per iteration based on learning progress (slope of recent error). Purpose: improve multi-task effect-prediction learning by dynamically selecting and focusing on tasks making positive progress while enabling bidirectional skill transfer.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-specific projection & encoder modules producing per-task latent representations combined with a shared multi-head attention module and an explicit task flag bit to focus queries on the active task; shared encoder provides common features while task-specific modules allocate task-aligned capacity.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "robotic manipulation effect-prediction (supervised multi-task learning across Push, Hit, Stack tasks)",
            "performance_task_aligned": "Qualitative: converges faster and attains lower prediction error than SINGLE (separate single-task networks) and IMTL-RAND (same architecture with random task selection); particularly large gains on Stack and Push tasks (figures show faster error reduction across 10 seeds). No absolute numeric error values provided in paper.",
            "performance_uniform_baseline": "Qualitative: SINGLE and IMTL-RAND show higher prediction error and slower convergence; IMTL-RAND improves over SINGLE but does not match IMTL-LP. No absolute numeric error values reported.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Reported qualitatively: IMTL-LP is the most energy-intensive among interleaved variants (highest cumulative neuron activations) compared to energy-modulated variants; exact FLOPs / time / memory numbers are not reported — energy measured as cumulative neuron activations.",
            "computational_efficiency_baseline": "SINGLE baseline has lower cumulative activations (similar to low-energy IMTL-EMLP variants); exact numeric costs not reported.",
            "sample_efficiency_results": "Yes — IMTL-LP reduces prediction error faster (i.e., fewer epochs to reach lower error) than SINGLE and IMTL-RAND, indicating higher sample/learning efficiency; no exact sample counts or epoch-to-threshold numbers are provided.",
            "transfer_generalization_results": "Object- and task-wise skill transfer reported via ablations (∆L metric): positive transfer observed (e.g., Stack→Hit ∆L=0.86 for sphere; various object-specific ∆L values provided). Stack benefits strongly from Hit (∆L ≈ 3.7–5.2) indicating emergent transfer generalization; quantitative ∆L values given per object/task in paper.",
            "interpretability_results": "Ablations (zeroing key/value rows in shared attention) produce interpretable ∆L effects used to quantify which source tasks the target task relies on; attention+flag improved distinguishability and transfer patterns, aiding interpretability of inter-task dependencies.",
            "multi_task_performance": "Positive: simultaneous learning with LP-based interleaving improves overall multi-task performance and benefits mutual transfer, especially for Stack and Push; IMTL-LP allocated less total training iterations to Stack yet achieved higher performance, showing scheduling matters more than raw training time.",
            "resource_constrained_results": "Evaluated three network complexities (low: 800 params, medium: 2000, high: 5200). IMTL-LP shows the largest advantage at low capacity (800 params), maintains advantage at medium, and differences shrink at high capacity — indicating task-aligned strategy helps under resource constraints.",
            "key_finding_summary": "Task-aligned, dynamically scheduled multi-task training (IMTL-LP) yields faster convergence, better final task accuracy and positive task-to-task knowledge transfer versus uniform/random scheduling and single-task baselines, with the advantage most pronounced under low-capacity constraints.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Provides empirical evidence that allocating task-aligned representations and dynamically focusing computation yields superior learning and transfer compared to uniform or random allocation, supporting the Task-Aligned Abstraction Principle.",
            "uuid": "e2239.0"
        },
        {
            "name_short": "IMTL-EMLP",
            "name_full": "Interleaved Multi-Task Learning with Energy-Modulated Learning Progress (EMLP)",
            "brief_description": "Extension of IMTL-LP that discounts Learning Progress by a measure of recent computational energy (cumulative neuron activations), producing a tunable trade-off between predictive performance and energy consumption via a sensitivity coefficient k.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IMTL-EMLP",
            "model_description": "Same architecture as IMTL-LP but task selection score s_i = exp(k * LP_i) / normalized_EC_i, where EC_i is cumulative neuron activations over recent steps; k controls sensitivity to LP vs energy. Purpose: reduce neural computation while retaining learning gains.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-specific encoders and attention-based sharing as in IMTL-LP; dynamic task selection additionally modulated by an energy proxy (cumulative activations) to prioritize lower-cost tasks.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "robotic manipulation effect-prediction (same Push, Hit, Stack tasks)",
            "performance_task_aligned": "Qualitative: With moderate k (e.g., k=1, k=1.2) IMTL-EMLP closely matches IMTL-LP predictive performance (small sacrifice). As k decreases (prioritizing energy), prediction error increases progressively. No absolute numeric error rates reported.",
            "performance_uniform_baseline": "Compared to IMTL-LP: IMTL-EMLP variants with low k perform worse in accuracy but use less energy; IMTL-EMLP-K=0.4 consumes the least energy and approaches SINGLE's energy use while having lower predictive performance than IMTL-LP.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Reported as cumulative neuron activations: energy usage decreases as k decreases; IMTL-EMLP-K=0.4 consumes the least energy (nearly same as SINGLE). Exact FLOPs/time not reported.",
            "computational_efficiency_baseline": "IMTL-LP: highest cumulative activations; SINGLE and low-k IMTL-EMLP: lowest. Exact numeric costs absent.",
            "sample_efficiency_results": "Implicit: moderate-energy IMTL-EMLP variants preserve much of IMTL-LP's speed of learning; low-k variants slow learning and increase final error. No quantitative sample-efficiency numbers.",
            "transfer_generalization_results": "Not separately reported beyond IMTL-LP; energy-modulated scheduling maintains transfer trends when k high, but strong energy bias (low k) harms learning and thus likely reduces transfer.",
            "interpretability_results": "No new interpretability analyses beyond IMTL-LP; mechanism is interpretable as a tunable trade-off between LP and EC.",
            "multi_task_performance": "IMTL-EMLP with moderate k provides a favorable trade-off: retains multi-task performance close to IMTL-LP while cutting energy; low-k variants degrade multi-task predictive performance.",
            "resource_constrained_results": "IMTL-EMLP explicitly designed for energy-constrained settings; low-k settings reduce energy to near SINGLE but at cost to accuracy; no latency/memory benchmarks reported.",
            "key_finding_summary": "Adding an energy proxy to dynamic task allocation enables a tunable trade-off: you can substantially reduce neural energy (cumulative activations) with modest loss in predictive performance if k is chosen appropriately.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Shows that adaptive allocation based on both expected learning benefit and computational cost can preserve much of the benefit of task-aligned representations while improving energy-efficiency, supporting refined task-alignment principles that include cost-awareness.",
            "uuid": "e2239.1"
        },
        {
            "name_short": "SINGLE baseline",
            "name_full": "Single-Task Learning Baseline (SINGLE)",
            "brief_description": "Baseline where each task is trained with its own dedicated network (same architecture as multi-task model but without shared parameters), representing uniform, non-shared representations per task.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SINGLE",
            "model_description": "Independent single-task instances of the same encoder-decoder architecture (state/action projection, task-specific encoder/decoder, 'attention' and other modules exist but are per-task only), isolating the effect of shared representations and interleaving.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Task-dedicated separate models (uniform per-task allocation; no shared task-aligned representation across tasks).",
            "is_dynamic_or_adaptive": false,
            "task_domain": "robotic manipulation effect-prediction",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Qualitative: SINGLE is outperformed by IMTL-LP and also by IMTL-RAND in experiments; SINGLE consumes relatively low cumulative activations (energy). No numeric error values provided.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": "Reported qualitatively: SINGLE energy use similar to energy-aware low-k IMTL-EMLP; exact metrics not provided.",
            "sample_efficiency_results": "Inferior sample efficiency relative to IMTL-LP (slower reduction in prediction error). No numeric values.",
            "transfer_generalization_results": "Not applicable (no transfer across tasks since networks are independent).",
            "interpretability_results": null,
            "multi_task_performance": "As separate models, no multi-task sharing; worse overall multi-task performance when compared to shared/interleaved approaches.",
            "resource_constrained_results": null,
            "key_finding_summary": "Independent per-task models (uniform allocation) perform worse overall and learn slower than dynamically interleaved, shared representations, though they may use less neural activation.",
            "supports_or_challenges_theory": "challenges",
            "supports_or_challenges_theory_explanation": "Demonstrates that purely uniform, separate allocation is inferior to adaptive shared/task-aligned schemes for multi-task learning efficiency and transfer.",
            "uuid": "e2239.2"
        },
        {
            "name_short": "IMTL-RAND",
            "name_full": "Interleaved Multi-Task Learning with Random Task Selection (IMTL-RAND)",
            "brief_description": "Same multi-task architecture as IMTL-LP but tasks are chosen uniformly at random each training iteration, representing non-adaptive interleaving.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IMTL-RAND",
            "model_description": "Shared encoder + task-specific modules and shared attention identical to IMTL-LP; task selection is random (uniform), not guided by Learning Progress or energy.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Task-specific encoders with shared attention (same as IMTL-LP), but static/random scheduling rather than LP-driven dynamic scheduling.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "robotic manipulation effect-prediction",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Qualitative: IMTL-RAND outperforms SINGLE (shared parameters help) but does not match IMTL-LP; it shows some positive transfer but lacks efficient scheduling, so converges slower than IMTL-LP.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Better than SINGLE but worse than IMTL-LP (slower reduction in error).",
            "transfer_generalization_results": "Some positive transfer observed (better than SINGLE) but less effective than LP-based scheduling.",
            "interpretability_results": null,
            "multi_task_performance": "Shared architecture yields multi-task benefits vs SINGLE, but random scheduling reduces the efficiency of transfer relative to LP-based adaptive scheduling.",
            "resource_constrained_results": null,
            "key_finding_summary": "Sharing representations helps (IMTL-RAND &gt; SINGLE), but adaptive, progress-driven scheduling (IMTL-LP) is required to fully realize multi-task gains.",
            "supports_or_challenges_theory": "mixed",
            "supports_or_challenges_theory_explanation": "Shows that shared, task-specific modules are beneficial, but that dynamic/adaptive task selection further improves outcomes — supporting adaptive allocation over naive shared/uniform strategies.",
            "uuid": "e2239.3"
        },
        {
            "name_short": "BLOCK",
            "name_full": "Blocked Sequential Training Baseline (BLOCK)",
            "brief_description": "Baseline where tasks are trained in uninterrupted blocks (sequentially) in fixed permutations, representing a commonly used non-interleaved training schedule.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BLOCK",
            "model_description": "Shared multi-task architecture trained on one task for a block of epochs before switching to another task (six possible task-order permutations were evaluated) to assess sensitivity to order and forgetting.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "Sequential blocked scheduling of tasks; although architecture is multi-task, training schedule is blocked which leads to periods of no adaptation for certain tasks.",
            "is_dynamic_or_adaptive": false,
            "task_domain": "robotic manipulation effect-prediction",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Qualitative: BLOCK performs worse than IMTL-LP across all task orders; exhibits catastrophic forgetting and error spikes at task switches; dependent on task order (sensitive).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Less sample efficient than interleaved methods; blocked training leads to forgetting and slower overall convergence.",
            "transfer_generalization_results": "Negative interference and catastrophic forgetting reduce transfer and generalization compared to interleaved approaches.",
            "interpretability_results": null,
            "multi_task_performance": "Overall inferior multi-task performance and high sensitivity to task order; interleaved LP scheduling uniformly outperforms all blocked permutations.",
            "resource_constrained_results": null,
            "key_finding_summary": "Blocked/sequential training is sensitive to task order and suffers catastrophic forgetting; dynamic interleaving based on LP outperforms blocked schedules consistently.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Findings support that dynamic, task-aware allocation (interleaving) is superior to static blocked training, reinforcing the benefit of adaptive task-aligned strategies.",
            "uuid": "e2239.4"
        },
        {
            "name_short": "IMTL ablations (no-attn / no-flag / no-attn&flag)",
            "name_full": "Ablation Variants of IMTL (without attention, without task-flag, without both)",
            "brief_description": "A set of ablation models removing the shared attention module and/or the task-identifying flag bit to test the contribution of adaptive attention-guided sharing versus more uniform representations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "IMTL ablations",
            "model_description": "Variants: (1) full model; (2) without task flag (attention still present); (3) without attention (flags present but attention removed); (4) without both attention and flag (stacked representations forwarded directly). These probe the role of explicit task cues and adaptive attention in enabling task-aligned sharing.",
            "model_size": null,
            "uses_task_aligned_abstraction": null,
            "abstraction_mechanism": "When attention removed and flag removed the model effectively uses more uniform concatenated representations; attention+flag provides dynamic, task-focused adaptation.",
            "is_dynamic_or_adaptive": null,
            "task_domain": "robotic manipulation effect-prediction",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Qualitative results: the model without both attention and flag had the worst performance; adding flag or attention individually improved performance; full model (attention+flag) performed best — demonstrating that adaptive attention+task-flag is critical for task-aligned gains.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Ablated models converge slower and to higher error than full IMTL-LP; exact numbers not reported.",
            "transfer_generalization_results": "Ablations reduce positive task transfer (shown indirectly via worse performance and ablation ∆L analyses), indicating attention+flag facilitate beneficial inter-task transfer.",
            "interpretability_results": "Results support the interpretability of dependencies via attention ablations (∆L) — removing keys/values reveals source-to-target reliance patterns.",
            "multi_task_performance": "Removing adaptive components harms multi-task performance; the combination of attention and flag yields the best multi-task outcomes.",
            "resource_constrained_results": null,
            "key_finding_summary": "Adaptive attention plus an explicit task-flag is necessary to realize the benefits of task-aligned representations; removing these mechanisms yields behavior closer to uniform representations and worse performance.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "Ablations show that adaptive, task-focused computation (attention+flag) is instrumental for gains, supporting task-aligned adaptive representation theory.",
            "uuid": "e2239.5"
        },
        {
            "name_short": "Cross-Stitch Networks",
            "name_full": "Cross-Stitch Networks for Multi-Task Learning",
            "brief_description": "A previously published adaptive feature-sharing architecture that learns linear combinations of activation maps between task-specific networks to adaptively determine feature sharing.",
            "citation_title": "Cross-Stitch Networks",
            "mention_or_use": "mention",
            "model_name": "Cross-Stitch Networks",
            "model_description": "Parallel task-specific networks whose intermediate activations are linearly combined by learned cross-stitch units, enabling input-dependent, learned sharing of features across tasks.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Learned linear combination (cross-stitch units) of task-specific activation maps to allocate shared vs task-specific feature usage.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "general multi-task learning (vision examples in original paper)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Mentioned as a prior method for adaptive feature sharing across tasks; contrasts with current work which trains tasks from scratch and interleaves one task per iteration using LP-based arbitration.",
            "supports_or_challenges_theory": "neutral",
            "uuid": "e2239.6"
        },
        {
            "name_short": "Multi-gate Mixture-of-Experts (MG-MoE)",
            "name_full": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
            "brief_description": "A cited approach that dynamically gates and mixes expert subnetworks per task, enabling task-specific routing of computation (adaptive allocation) in multi-task settings.",
            "citation_title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
            "mention_or_use": "mention",
            "model_name": "Multi-gate Mixture-of-Experts",
            "model_description": "A MoE-style approach where different gates determine which experts are used per task, achieving task-specific allocation of capacity and routing of information.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Gating over expert modules (mixture-of-experts) to allocate computation and representation to tasks dynamically.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "multi-task learning",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": false,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": null,
            "transfer_generalization_results": null,
            "interpretability_results": null,
            "multi_task_performance": null,
            "resource_constrained_results": null,
            "key_finding_summary": "Mentioned in related work as an example of an architecture that dynamically allocates capacity per task (mixture-of-experts gating), relevant to the paper's framing of adaptive resource allocation.",
            "supports_or_challenges_theory": "neutral",
            "uuid": "e2239.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cross-Stitch Networks for Multi-Task Learning",
            "rating": 2
        },
        {
            "paper_title": "Distral: Robust multitask reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
            "rating": 2
        },
        {
            "paper_title": "GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
            "rating": 2
        },
        {
            "paper_title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
            "rating": 2
        }
    ],
    "cost": 0.0186405,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interleaved Multitask Learning with Energy Modulated Learning Progress
26 Jul 2025</p>
<p>Hanne Say hanne.say@ozu.edu.tr 
Artificial Intelligence and Data Engineering Department
Ozyegin University
34794IstanbulTurkey</p>
<p>Suzan Ece 
Department of Computer Engineering
Bogazici University
34342IstanbulTurkey</p>
<p>Emre Ugur emre.ugur@bogazici.edu.tr 
Department of Computer Engineering
Bogazici University
34342IstanbulTurkey</p>
<p>Minoru Asada asada@otri.osaka-u.ac.jp 
SISREC
OTRI
University of Osaka
565-0871OsakaJapan</p>
<p>International Professional University of Technology
530-0001OsakaJapan</p>
<p>Erhan Oztop 
Artificial Intelligence and Data Engineering Department
Ozyegin University
34794IstanbulTurkey</p>
<p>SISREC
OTRI
University of Osaka
565-0871OsakaJapan</p>
<p>Interleaved Multitask Learning with Energy Modulated Learning Progress
26 Jul 20252FAB544D9C1D52B18463DAAD8C82E0E3arXiv:2504.00707v2[cs.RO]Preprint submitted to Neural Networksinterleaved learningmulti-task learningtask switchingcomputational neural energylearning progressintrinsic motivation
As humans learn new skills and apply their existing knowledge while maintaining previously learned information, 'continual learning' in machine learning aims to incorporate new data while retaining and utilizing past knowledge.However, existing machine learning methods often does not mimic human learning where tasks are intermixed due to individual preferences and environmental conditions.Humans typically switch between tasks instead of completely mastering one task before proceeding to the next.To explore how human-like task switching can enhance learning efficiency, we propose a multi task learning architecture that alternates tasks based on task-agnostic measures such as 'learning progress' and 'neural computational energy expenditure'.To evaluate the efficacy of our method, we run several systematic experiments by using a set of effect-prediction tasks executed by a simulated manipulator robot.The experiments show that our approach surpasses random interleaved and sequential task learning in terms of average learning accuracy.Moreover, by including energy expenditure in the task switching logic, our approach can still perform favorably while reducing neural energy expenditure.</p>
<p>Introduction</p>
<p>Humans has the ability to learn continuously by integrating new information while exploiting and retaining previously acquired knowledge.This lifelong learning process is characterized by constant interleaving of tasks and goals.We switch tasks in response to environmental and cognitive constraints, adjusting our learning behavior in real time -for example, deciding to disengage from a task when it becomes less efficient or more mentally demanding.[1].This dynamic task switching not only enhances memory consolidation but also adapts us to dynamic environments by shifting attention toward differences between tasks and forcing us to be more mentally engaged [2].</p>
<p>Traditional machine learning approaches often rely on isolated task learning, which optimizes performance for a specific task without considering knowledge transfer from related tasks, limiting cross-domain generalization and adaptability [3].Multi-task learning, on the other hand, aims to improve generalization by training models on multiple tasks simultaneously [4].In continual learning, sequential task acquisition is addressed by learning tasks one after another without forgetting the earlier ones and making use of the previous tasks where possible [5,6], which is called condensed learning in [7] as opposed to dispersed or interleaved learning.The order of the tasks however, affect the learning and transfer performance in these paradigms [7,8].Instead of a static task scheduling, humans exhibit online dynamic task selection leading to an emergent interleaved task learning schedule.</p>
<p>The human brain has evolved to optimize energy efficiency, favoring strategies that maximize information transmission per unit energy [9].With mechanisms such as synaptic pruning and neural plasticity [10], it optimizes its neural pathways, ensuring efficient cognitive function while reducing its metabolic costs [11,12].Along with the brain's drive toward efficient information processing, we also get inspiration from learning progress (LP), which has been proposed as an intrinsic motivation mechanism that guides exploration based on the rate of improvement in performance.Schmidhuber [13,14] introduced LP as a measure of how quickly an agent reduces its prediction error, encouraging the agent to focus on experiences that yield the highest learning gains.This aligns well with biological principles, as the brain appears to allocate information encoding resources preferentially to stimuli that are neither too simple nor too complex, maximizing informational value relative to effort [15,16].</p>
<p>In this paper, we propose a novel approach that integrates interleaved multi-task learning with energy-modulated learning progress (IMTL-EMLP) approach.By designing a system that not only has capacity to switch between tasks in an interleaved fashion but also adjusts its learning dynamics based on energy efficiency.Our method is evaluated in a simulated robotic environment where the robot is tasked with learning the effects of its action in different environmental and task settings.</p>
<p>The experimental results demonstrate that suitable task switching leads to an interleaved learning regime, which improves learning performance across different tasks.Additionally, introduction of energy-modulation to the learning progress based task selection results in reduced energy consumption without a significant drop in learning efficiency.Overall the contributions of our work is as follows:</p>
<p>• First time multi-task learning using neural energy and learning progress for learning arbitration is realized leading to interleaved learning similar to that of humans</p>
<p>• A novel multi-task learning architecture is introduced that uses shared attention layers to enable bi-directional skill transfer in interleaved learning</p>
<p>• Realization of the proposed interleaved multi-task learning framework on effect prediction tasks in a simulated robotic environment.</p>
<p>• Empirical demonstration that integrating energy-aware learning progress criteria reduces neural energy expenditure without degrading the agent's learning performance</p>
<p>The remainder of the paper is organized as follows: Section 2 reviews related work on multi-task learning, interleaved learning, intrinsic motivation, and energy-efficient computation; Section 3 formulates the problem of dynamic task arbitration formally; Section 4 introduces our proposed interleaved multi-task learning framework with energy-modulated learning progress; Section 5 presents the experimental setup and evaluates the performance of our approach across various metrics and baselines; and Section 6 concludes the paper with a summary of findings and directions for future research.</p>
<p>Related Work</p>
<p>Multi-task Learning</p>
<p>Multi-task learning (MTL) [17] is a machine learning paradigm in which several tasks are learned jointly so that knowledge contained in one task can help improve the performance of the others.Unlike transfer learning-where the emphasis is on improving one target task, MTL treats all tasks as equally important and seeks to boost them collectively.MTL is also distinct from multi-label learning and multi-output regression because in MTL, the tasks typically do not share the same data records (i.e., each task has its own dataset).The key questions pertaining to MTL are (1) when to use MTL to share knowledge, (2) what to share (features, instances, or parameters), and (3) how to share, e.g., by feature learning, clustering, or parameter sharing.The most commonly studied question in the literature is "how" [4], as we discuss next.</p>
<p>In feature learning, it is common to map the original features [18,19,20,21] to a common space or choose a subset of the original task features [22,23].One of the notable architectures in the feature learning category is the Cross-Stitch Networks [24].It dynamically combines feature maps from parallel task-specific networks, by a cross-stitch unit that learns a linear combination of the activation maps from the previous layer, allowing the model to adaptively determine which features to share based on the input data.Cross-Stitch Networks has inspired our earlier skill transfer model [25] and shares conceptual elements with the proposed model of the current study.Yet, our model differs in several critical aspects.Cross-Stitch Networks are typically initialized with task-specific single-task networks, finetuned on the respective task, and then combined using cross-stitch units to perform joint training.In contrast, in our model, all tasks are trained from scratch in a unified architecture without any pretraining or fine-tuning.In addition, after introducing the cross-stitch units, in their approach, all the tasks are trained jointly, using the same input data, where we only train one task per training iteration, which is chosen by our task arbitration mechanism, paralleling human learning.Other than the feature-based models, parameter-based models are also developed for MTL problems such as low-rank methods [26,27] suggesting that if tasks are related, the matrix of model parameters often has low rank, task clustering methods [28,29,30] that hypothesize tasks form separate clusters of similarity, with each group sharing parameters.Other parameter-based approaches, such as task relation [31,32] and parameter matrix decomposition [33,34], are likewise covered in the literature.</p>
<p>Multi-task learning is also addressed in reinforcement learning (RL), where the RL agent seeks to improve performance across various decision-making tasks by leveraging shared experiences and representations.A significant challenge in Multi-task RL (MTRL) is negative interference, where learning in one task can degrade performance in others [35].To address negative transfer, Hallak et al. (2015) introduced Contextual Markov Decision Process (CMDP), wherein a context defines the parameters of the transition and reward functions, with each task associated with a context vector [36].Subsequently, Sodhani et al. [37] extended this formalism to Block Contextual Markov Decision Process (BC-MDP), incorporating the state space into the context-dependent framework.Similarly, Hendawy et al. [38] promote diversity among representations by learning orthogonal representations through the Gram-Schmidt process.On the other hand, Teh et al. [35] uses a distilled policy to learn a shared representation that guides the task-specific policies, where the task-specific structure aims to mitigate updates that can lead to negative interference.These methods jointly demonstrate the significance of structuring shared representations and alleviating negative interference in MTRL.In our model, we do this by using an attention layer to facilitate positive skill transfer and suppress negative interference.</p>
<p>An open question in MTL, other than learning and skill sharing, is how to balance the contributions of individual tasks that may vary widely in complexity and data distribution.Recent research has addressed this through adaptive loss weighting and gradient balancing.Kendall et al. [39] introduced an uncertainty-based framework that scales each task's loss according to its inherent noise, thereby directing the learning process toward more reliable signals.Complementing this, Chen et al. [40] proposed GradNorm, a method that normalizes the gradients of different tasks to ensure that no single task dominates the optimization process.</p>
<p>In addition, Sener and Koltun [41] reframed MTL as a multi-objective optimization problem, seeking Pareto-optimal solutions that effectively navigate trade-offs between conflicting task objectives.MTL approaches adopting mixture-of-expert architectures are also proposed, such as in [42], where task contributions are dynamically weighted and shared representations are exploited.However, despite the effectiveness of these approaches in balanc-ing learning objectives and modeling task relations, they typically assume simultaneous or sequential task training schedules.In contrast, our work departs from this by introducing a dynamic task arbitration mechanism that selects a single task at each training step based on learning progress and energy consumption.This enables an interleaved training regime that more closely resembles human learning patterns, which we describe in the following section.</p>
<p>Interleaved Learning in Humans</p>
<p>Interleaved learning is a cognitive strategy in which learners alternate among diverse topics or problem types rather than concentrating on one subject exclusively.Empirical research in cognitive psychology provides compelling evidence for the benefits of interleaved practice.For example, studies have demonstrated that interleaving mathematics problems enhance learners' ability to distinguish between problem types, leading to improved problemsolving skills [43].Similarly, research by Kornell and Bjork [44] indicates that interleaved learning facilitates the formation of more flexible and integrated representations of concepts, enabling learners to better apply acquired knowledge in new contexts.Enhanced retention and transfer are also observed while learning motor skills [45,46].</p>
<p>Neuroimaging studies further show that interleaving practice drives increased frontal-parietal activity and heightened motor cortex excitability along with the reduced retrieval time of information, thus leading to better long-term retention and efficient retrieval compared to the blocked practice [47].The positive effects of interleaved practice are usually associated with a phenomenon called contextual interference (CI) [48], and considered as one of the 'desirable difficulties' for learning [49].This view suggests that introducing a challenge during learning can lead to an improvement in long-term retention [50].In [51], math problems are shuffled so that the problems belonging to the same kind are not consecutively solved by the students, requiring students to come up with a proper strategy while solving the problem, similar to real-world situations.The experiments showed that interleaved practice produced higher scores compared to blocked practice, in a final test given on both day 1 and day 30 (delayed test), showcasing protection against forgetting.In addition, the study suggests that interleaved learning benefits do not lessen over time; quite the opposite, they may increase over time.Similarly, [52] showed that undergraduate physics students who practiced interleaved problem sets demonstrated improved memory and problem-solving skills compared to those who used blocked practice.These findings highlight the potential of interleaved learning as a powerful strategy for optimizing human learning in both educational and real-world settings.</p>
<p>In machine learning, however, interleaved learning received very little attention.Recently, Mayo et al. [53] investigated the interleaving on a multi-task learning problem and discussed that rather than designing mechanisms to prevent forgetting, such as external memory [54] or regularization of weights [55,56], we should focus on designing learning systems and schedules that embrace the natural and resilient mechanisms of human learning.In this view, forgetting is not a failure but a feature that coexists with the capacity for efficient recovery i.e., relearning savings [57] and long-term knowledge retention.Even without mechanisms to prevent forgetting, standard neural networks show memory retention effects [53], similar to humans, when tasks are interleaved.</p>
<p>Intrinsic Motivation and Learning Progress</p>
<p>Intrinsic motivation (IM) refers to the internal drive of an agent to engage in activities for their inherent satisfaction, rather than for external rewards [58].In the context of AI and robotics, intrinsic motivation enables agents to exhibit behaviors such as curiosity [59,60], novelty [61,62], and surprise [63], similar to the motivations observed in humans [64,65,66].As an example, in [67], cognitive dissonance, the mismatch between mental expectation and observation is used an IM signal to enable efficient online adaptation for robotic manipulation.Another IM that is often used in robotics and computational approaches is the prediction progress, also known as learning progress (LP) [68].It is usually calculated by comparing the predictor's error before and after it is updated, using the same sensorimotor context [13].There are numerous applications of LP in machine learning problems such as exploration guiding [69], region selection [70,71], and curriculum learning [72,73] in RL.In a recent work, Ada et al. proposed a form in LP, namely, episodic return progress, for bidirectional progressive neural networks to facilitate skill transfer among morphologically different agents [74].Similarly, Colas et al. used LP in their multi-goal RL model (CURIOUS) as an IM signal to select which goal module to practice and replay, prioritizing those with the highest absolute LP, producing an adaptive curriculum for learning [73].In the same vein, in our previous work [25], we used LP as a signal to autonomously arbitrate task selection.Unlike prior works that use LP solely for guiding exploration, goal or region selection, this study applies LP at the task level and integrates it with energy consumption to enable efficient, interleaved multi-task learning.</p>
<p>Energy Conservation in the Brain</p>
<p>Energy management in the human brain is not only about powering neural activity-it also reflects a sophisticated system for conserving energy that influences our behavior [75].Despite the brain's relatively small size, it consumes a disproportionate share of the body's energy, mainly to support neural signaling through the maintenance of ion gradients and synaptic transmission [76].Over time, the brain has evolved mechanisms such as synaptic pruning and circuit rewiring, where frequently used pathways are strengthened, and rarely used connections are eliminated.This dynamic reorganization minimizes unnecessary activity, leading to what is often described as neural efficiency [77,78].In addition to the brain's physiological mechanisms for preserving energy, research in cognitive neuroscience and psychology has also shown that people systematically avoid tasks perceived as highly demanding, requiring more effort or energy, making the associated rewards seem less valuable, which is often explained by the term effort discounting [79].For example, Kool et al. [80] demonstrated that individuals tend to choose less cognitively demanding tasks when given the option, reflecting an inherent preference for minimizing mental effort.Similarly, Westbrook and Braver [81] provided evidence that cognitive effort carries a subjective cost, influencing decision-making processes.This concept is further elaborated in the opportunity cost model proposed by Kurzban et al. [82], which suggests that the perceived cost of expending cognitive energy plays a central role in our choices, leading us to opt for tasks that require minimal resource expenditure.In this study, we are inspired by the findings in neural and cognitive sciences and propose an energy-efficient task selection mechanism in addition to learning-based task arbitration.</p>
<p>Effect Prediction and its Applications</p>
<p>Effect prediction tasks in robotics involve endowing agents with the capability to anticipate the outcomes of their actions, similar to humans [83], forming an internal model of the environment that can be used for planning and control [84].This predictive capacity-often referred to as a forward model [85] is fundamental to intelligent behavior.By simulating the effects of different actions, a robot can evaluate potential strategies [86] before committing to a course of action, thereby enhancing safety, efficiency, and adaptability.</p>
<p>At the core of effect prediction is the idea that the robot learns to map its actions to subsequent states.Early approaches focused on building explicit physical models [87,88]; however, recent advances have shifted towards learning these models directly from high-dimensional sensory data using deep learning techniques.For example, Watter et al. [89] introduced a latent dynamics model that transforms raw sensory inputs, such as images, into a compact, low-dimensional latent space where the dynamics of the environment are more predictable.In this latent space, the model approximates the effect of actions as locally linear transformations, which can be leveraged for short horizon planning.Another work by Agrawal et al. [90] focuses on self-supervised learning of intuitive physics.Their system trains a robot to predict the outcome of simple interactions, like poking objects, without explicit supervision.Over time, the model learns to infer the underlying physical properties of objects, which is crucial for manipulating unfamiliar items.</p>
<p>Furthermore, effect prediction is central to model-based reinforcement learning (RL).In model-based RL, the agent uses its learned internal model to simulate future trajectories.This allows the agent to plan over longer time horizons and generate synthetic experiences, thereby reducing the need for extensive trial-and-error interactions with the real environment.PlaNet [91] and Dreamer [92] use state space models to learn latent dynamics from images.PlaNet employs a latent overshooting objective and performs planning in the learned latent space [91], whereas Dreamer uses an actor-critic approach, backpropagating value gradients through its latent world model [92].The "World Models" approach by Ha and Schmidhuber [93] learns a compact representation of the world that supports internal simulation and policy training [94].This internal simulation capability enables agents to achieve higher sample efficiency and improved performance, particularly in complex or resource-constrained settings.</p>
<p>In robotic applications, effect prediction tasks play a pivotal role in different problems, such as navigation, manipulation, and autonomous planning.In manipulation, [95] emphasizes the role of object-and relation-centric representations in improving push effect prediction, showing how modeling inter-object dynamics allows robots to infer and control the consequences of their actions in cluttered scenes.This low-level understanding of physical interactions feeds directly into goal-directed planning, as seen in [96].</p>
<p>Ahmetoglu et al. [97] demonstrate how effect prediction supports symbolic manipulation planning by learning object and relational predicates, providing a bridge between physical action and abstract reasoning [98].While these works primarily focus on manipulation and planning, the principles extend to navigation as well; Aktas et al. [99] show that predicting the effects of partial action executions enables more flexible, multi-step planning, which is essential for navigating dynamic or partially observable environments.However, these studies neither addressed multi-task learning nor used an intrinsic motivation signal to guide the action selection.</p>
<p>Problem Statement</p>
<p>The general problem addressed is how a task arbitration and skill transfer mechanism can lead to improved overall learning and skill transfer performance in multitask learning settings.The framework sought to mimic human learning, where the learner has to decide which task to engage in and when to disengage, effectively allowing the emergence of arbitrary interleaved learning regimes.</p>
<p>We focus on supervised learning problems in robotics, and seek an online multi-task learning model equipped with a task arbitration mechanism that enables the robot to decide which task to engage in for data sampling and learning with the goal of minimizing average prediction error over the tasks.Formally, we are given m tasks {T i } m i=1 and an allocated interaction period N T i for each task.When task T i is engaged, it is allowed to execute a total of N T i interactions with the environment.After this period, the task arbitration mechanism chooses a new task to be engaged.The selected task is used in execution, data collection, and learning.Denoting the selected task with T * , the execution experience for task T * can be represented in the form of input, output pairs
D T * = {(x T * j , y T * j )} N T *
j=1 , where x T * j represents input while executing task T * , and y T i j is the corresponding output at sampling step j.Note that by setting N T i values differently, one can encode the expected relative difficulty of the tasks, as in the case for tasks with high-dimensional versus low-dimensional input spaces.Once the agent gathers a mini-batch of experience, D T * , it updates the task specific parameters, θ T * as well as the shared task parameters, θ s .This online sampling and learning process continues to minimize the prediction error for each task.Denoting the prediction for task T i and loss associated with it by f T i x T i ; θ s , θ T i , L T i respectively, the objective for the multi-task learning agent can be stated as finding θ s , {θ T i } m i=1 to minimize the total loss, T L across tasks:
T L = min θ s ,{θ T i } m i=1 L T i f T i (x T i j ; θ s , θ T i ), y T i j N T i j=1 .
The problem statement projected to the experiments conducted in this paper can be given as follows: The learning agent is required to learn a total of m distinct effect prediction tasks by establishing a mapping from the state-action space to the effect space.For each task T i , the state is represented as
x T i ∈ R d T i s
and the action as
a T i ∈ R d T i a
, where d T i s and d T i a denote the dimensions of the state and action spaces of task T i , respectively.The corresponding effect, which is the outcome of executing action a T i in state x T i , is denoted by e T i ∈ R d T i e .Thus, the experience collected and used in learning is
D T i = {((x T i j , a T i j ), e T i j )} N T i j=1 .
The goal is to dynamically arbitrate data sampling and learning among the tasks so that for each task T i actioneffect prediction functions,
f T i : R d T i s × R d T i a → R d T i
e , are learned.After learning, it is expected that for any state-action pair (x T i , a T i ) the predicted effect ẽT i = f T i (x T i , a T i ) closely approximates the actual effect e T i .</p>
<p>Methodology</p>
<p>In this section, we give the details of the learning architecture and task switching mechanisms developed and used in the experiments.</p>
<p>Neural Network Architecture</p>
<p>Our model is based on an encoder-decoder network architecture designed to handle multiple-task learning while allowing for inter-task information sharing.The key components of the architecture are task-specific input (state) and action projection layers, task-specific encoders as well as a shared encoder and a shared attention module, and finally task-specific output (effect) decoders (see Figure 1).We detail these components next.Symbols and notations used are defined in Table 1 for clarity (see Table A.1 in the Appendix for the actual sizes).1.State Projection Layer.For each task T i , the state input
x T i ∈ R d T i s
is first passed through a task-specific projection function P T i state : R d T i s → R ds leading to the projected input vector given by:
xT i = P T i state (x T i ),(1)
where xT i ∈ R ds .This step ensures that inputs from different tasks have the same dimensionality for subsequent shared processing.The projection function may be chosen to be linear or nonlinear.As the main objective of these layers is to have the same dimension at the output, a linear projection is used in the current work, as often done in the literature [100,101].</p>
<ol>
<li>Action Projection Layer.Since the action vector a T i ∈ R d T i a for each task t can vary in dimensionality, similar to the state projection layers, we project them to a fixed-dimensional space to maintain consistency across tasks.Each action vector is passed through a task-specific projection function P T i action : R d T i a → R da :
âT i = P T i action (a T i ),(2)
where âT i ∈ R da is the projected action vector for task T i .This projection ensures that actions from different tasks are compatible for further Symbol Description m Total number of tasks i Task index, {T i } m i=1 T * Selected task to be trained
x T i State of task T i , x T i ∈ R d T i s a T i Action of task T i , a T i ∈ R d T i a e T i Effect of task T i , e T i ∈ R d T i e P T i state
Task-specific state projection function,
P T i state : R d T i s → R ds F Shared state encoder, F : R ds → R d h h Output of the shared encoder, h ∈ R d h r T i Task-specific latent representation, r T i ∈ R dr z T i
Task-specific latent representation with flag bit, z
T i ∈ R dr+1 Z Stacked matrix of all {z T i } m i=1 , Z ∈ R m×(dr+1) P T i action Task-specific action projection function, P T i action : R d T i a → R da f T i Task-specific encoder for task T i , f T i : R d h → R dr g T i Task-specific decoder for task T i , g (t) : R dr+da+1 → R d T i e
Table 1: Overview of symbols and notations.</li>
</ol>
<p>processing.Similar to the (1), we choose to have a linear projection layer for the action vectors.3. Shared State Encoder.We incorporate a shared encoder in our neural network to encourage the extraction of general features that may be beneficial across multiple tasks, promoting knowledge sharing while reducing redundancy.Hence the projected input xT i is fed into the a shared encoder
F : R ds → R d h , to extract a d h -dimensional shared feature vector, h ∈ R d h with h = F (x T i ),(3)
4. Task State Encoders.Following the shared encoder, we further integrate task-specific encoders, aiming to capture state knowledge unique to each task that the shared encoder cannot extract.We hypothesize that each task benefits from specialized features, enhancing its performance during training.Hence, h is forwarded through all task-specific encoders in order to generate task-specific latent representations:
r T i = f T i (h), {T i } m i=1(4)
Note that when training task T * , only the parameters of f T * are updated; all other f T i modules remain frozen, with no gradient propagation.5. Shared Attention Module.To facilitate inter-task communication and let the network focus on the current training task, we utilize a multi-head attention (MHA) mechanism [102].Instead of having one attention function, where Q, K, V ∈ R d h , MHA layer computes attention across H parallel heads that each head projects Q, K, V into a lower-dimensional space and performs scaled dot-product attention.Let d k be the dimensionality per head, and let
W Q i , W K i , W V i ∈ R d h
×d k be the projection matrices for head i ∈ {1, . . ., H}.Then, Q, K, V are projected as follows:
Q i = Q W Q i , K i = K W K i , V i = V W V i , head i = softmax Q i K ⊤ i √ d k V i ∈ R d k .
The outputs from all heads are concatenated and passed through an output projection matrix to form the final attention output, denoted here as A, as follows:
A = [head 1 : head 2 : . . . : head H ] W O ∈ d h ,
where W O ∈ R (H•d k )×d h is the output projection matrix and [• : •] denotes vector concatenation.In [102], all Q, K, V values have the same source in the self-attention layers.In our work, we constrain the attention mechanism such that the current task is focused, or in other words 'queried'.Concretely, when training task T * , we perform the following:</p>
<p>• Beforehand the attention function, we concatenate an additional flag bit to each task-specific latent representation, r T i .Specifically, for each task T i , we concatenate a training flag δ T i indicating whether task T i is currently being trained (δ T i = 1) or not (δ T i = 0).The concatenated vector for each task is:
z T i = [r T i : δ T i ] ∈ R dr+1(5)
From now on, we call z T i the final representation of task T i .</p>
<p>• After gathering z T i for each task, they are stacked to form a matrix Z:
Z =      z T 1 z T 2 . . . z Tm      ∈ R m×(dr+1) .(6)
• Once we obtain the matrix Z; Q, K and V values are gathered as below:</p>
<p>Query (Q).We choose the final representation of the current training task as the query since we want the attention mechanism to be driven by the perspective of the currently active task:
Q = z T * ∈ R (dr+1) .
Key (K) and Value (V ).We use Z so that the active training task can attend to representations from all tasks (including itself):
K = V = Z ∈ R m×(dr+1) .
Using these Q, K, V values, we feed them into our shared MHA module and get the attention output A for the task T * :
A T * = SharedM HA(Q, K, V ),(7)
where A T * ∈ R (dr+1) .Because the query is restricted to the representation of the active task, z T * , the network focuses on how the other tasks' representations (contained in Z) can inform task T * .Moreover, the flags within each row of Z indicate whether a given input originates from the actively trained task (δ T i= * = 1) or from another task (δ T i̸ = * = 0), further guiding the attention toward balancing shared and task-specific information.6. Task Effect Decoders.Same as the task-specific sub-encoder f T i , for each task T i ; we utilize a decoder module g T i designed for predicting the resulting effect (output) e T i , after the executed action a T i while the environment is in the state x T i .For task T * , the output from the shared MHA mechanism A T * along with the projected action vector âT * Algorithm 1 Interleaved Multi-Task Learning Require: Number of tasks m, number of epochs R, exploration rate ϵ = 0.1
for epoch = 1 to R do Compute score s T i for tasks {T i } m i=1 i ′ ← arg max i ({s T i } m i=1 ) Generate a random number r uniformly from [0, 1] if r &lt; ϵ then T * ← Random task from {T 1 , T 2 , . . . , T m } \ {T i ′ } else T * ← T i ′ end if
Perform training on the selected task T * end for from ( 2) is given to the task-specific decoder of the selected task only, contrary to the (4), as follows:
ẽT * = g T * ([A T * : âT * ]). (8)
where ẽT * ∈ R d T * e .By integrating contextual information from the shared attention mechanism with the projected action vector, the decoder maps the state information to the corresponding effect.</p>
<p>Task Arbitration</p>
<p>To investigate the effects of allowing interleaved learning in multi-task settings, we propose a task selection strategy based on the assessment of each task's learning progress or its discounted version by the neural cost of learning.Unlike traditional multi-task learning approaches that train tasks in a fixed schedule or isolate them entirely, our method continuously evaluates the tasks' performance trends and selects which task to train next based on these evaluations.By doing so, we aim to emulate the human tendency to switch among tasks.An overview of the proposed interleaved task engagement mechanism is presented in Algorithm 1, where a non-greedy selection strategy is adopted by giving other tasks a chance to be selected, with an exploration rate ϵ, thereby reducing over-reliance on the highest-scoring task.</p>
<ol>
<li>Learning Progress (LP) Based Task Selection LP of a task is computed based on the recent history of the error signal pertaining to it: the error in the last L (L = 5 in the current study) steps linearly regressed against time and negative of its slope is taken as the LP value.</li>
</ol>
<p>To be concrete, let E T i t be the error at time step t for task T i .Consider the set of errors
{E T i t−L+1 , E T i t−L+2 , . . . , E T i t }.
Then, a linear model is fit to these points to estimate the slope β T i .If β T i &lt; 0, the error is decreasing, and the task is making positive learning progress.If β T i ≥ 0, it suggests that the error has plateaued or is increasing, implying no recent improvement.Under the LP-based interleaving scheme, the proposed task switching mechanism computes the learning progress LP T i for each task {T i } m i=1 .Then it selects the task with the highest LP to be trained next.In other words, tasks that are currently showing rapid improvement receive more training time, while tasks that have stalled or regressed receive less immediate attention.Formally, if β T i is the slope for task T i , then T * is chosen in two steps: (a)
LP T i = |β T i |, if β T i &lt; 0 0, otherwise. (b) Select the task T * = arg max i (LP T i ).
This ensures that at any given moment, the model focuses on tasks where additional practice can yield meaningful gains, rather than spending time on tasks that have stagnated.</p>
<p>Energy Consumption Modulated Learning Progress (EMLP)</p>
<p>Based Task Selection.While learning progress alone can guide the model toward tasks showing improvement, it does not consider the computational or energetic cost associated with training each task.In scenarios where energy efficiency is a concern, we incorporate energy consumption into the interleaving strategy.We define energy consumption EC T i for task T i as the total output of all neurons belonging to it A T i , cumulated over the last L training steps:
EC T i = t j=(t−L) A T i j .
This could be measured in terms of computational operations, memory usage, or any proxy for energy expenditure.Lower EC T i values mean the task has recently been trained efficiently, while higher values indicate that the task has been relatively costly in terms of energy.However, using energy consumption only as the interleaving guidance, the network will only choose the tasks that produce low activation energy without taking the learning performance of the tasks into consideration, which can lead to worsened overall multi-task learning performance (see Figure 8a).To consider the cost of learning, LP is discounted by a positive constant k that controls the sensitivity of the combined criteria to energy consumption:
s T i = e (k• LP T i ) / ẼC T i .(9)
Note that, to combine LP and EC properly, they are first scaled between [0, 1], before the combined score is computed, simply by using a minmax scaler.The s T i can be interpreted as follows: When k approaches to zero, e (k• LP T i ) ≈ 1, thus the score is primarily influenced by ẼC T i , even if the task has strong LP.This discourages spending too many 'energy resources' on a single, costly task, even if it has high learning gains.On the other hand, when k is high, e (k• LP T i ) becomes dominant, thus EC becomes negligible, approximating the previous LP-only selection.Under the EC-modulated LP-based interleaving strategy, we compute combined scores for each task T i as given above.The next task to train is the one with the highest combined score:
T * = arg max i (s T i ).(10)
This ensures a balance between quick gains in task performance and maintaining overall energy efficiency.Tasks with high learning progress but also low recent energy costs are favored, while tasks that have become too energy-intensive or are not showing improvement receive less immediate focus.By integrating these interleaving strategies, we encourage a more dynamic, bio-inspired learning schedule.</p>
<p>Experiments and Results</p>
<p>To evaluate the effectiveness of our proposed Interleaved Multitask Learning with Energy Modulated Learning Progress (IMTL-EMLP), a series of experiments are conducted with a simulated robot tasked with learning the effect of its actions in different task settings.In the experiments, we focus on learning efficiency and final task accuracy as performance measures and asses the knowledge transfer among tasks.</p>
<p>The proposed IMTL framework introduces Computational Energy modulated LP (EMLP) for dynamic task selection, which includes LP-based selection as a special case.We begin by evaluating the LP-based approach (IMTL-LP), followed by its energy-aware extension (IMTL-EMLP), which incorporates energy consumption into the task arbitration process.</p>
<p>To isolate the effects of task arbitration strategies, we compare IMTL-LP against several baselines: single-task learning (SINGLE), a multi-task learning model with random task selection (IMTL-RAND), and blocked learning setup where tasks are trained sequentially in fixed permutations (BLOCK).Additionally, we examine IMTL-EMLP, which extends IMTL-LP with energy sensitivity, analyzing how varying the energy coefficient k affects the balance between prediction accuracy and computational cost.</p>
<p>All baselines use a consistent model architecture1 to ensure that performance differences stem solely from task arbitration strategies.In addition to learning efficiency analysis, we assess the robustness to model size by comparing performance across low, medium, and high-resourced networks.Finally, two ablation studies dissect the individual and combined contributions of model components and explore object-wise skill transfer between tasks.</p>
<p>Simulation Environment</p>
<p>In this study, UR10 robotic arm equipped with a Franka Panda end effector is simulated via the PyBullet engine [103].We use a tabletop environment to evaluate multi-task learning through effect prediction tasks.A set of basic objects are presented on the table for the robot to interact with during the tasks.Namely, a sphere (r = 3 cm), a cube (l = 6 cm), a square prism (w = 6 cm, h = 12 cm), and a cylinder (r = 3 cm, h = 18 cm) are used.The latter two objects can be presented in either vertical or horizontal orientation, thus making it six total object configurations for interaction.In each interaction, the object is randomly placed on the table.</p>
<p>Effect Prediction Tasks</p>
<p>For testing the proposed model, we have defined three tasks for the robotic agent as to predict the effect of its actions in three different settings as described next.Push task.A single object is randomly placed on the table.The robot applies a "pushing" action with its end effector to the object's center of mass (CoM), with an angle θ chosen between [0, 180] degrees.The state of the object is taken as its pose defined by the Cartesian coordinates (x, y, z) and orientation (with Euler angles ϕ x , ϕ y , ϕ z ).The orientation angles are represented with sine cosine pairs to avoid discontinuous input for the neural networks that process the object state.Thus, the object is represented as a 9-dimensional vector:
s = [x, y, z, sin(ϕ x ), cos(ϕ x ), sin(ϕ y ), cos(ϕ y ), sin(ϕ z ), cos(ϕ z )].
The action is encoded using the sine and cosine of the push angle, combined with a one-hot vector indicating the object's id:
a = [sin(θ), cos(θ), onehot(o)].
The effect is defined as the change between the object's pre-action and post-action states, captured using the simulation environment: e = [∆(x), ∆(y), ∆(z), ∆(sin(ϕ x )), ∆(cos(ϕ x )), ∆(sin(ϕ y )), ∆(cos(ϕ y )), ∆(sin(ϕ z )), ∆(cos(ϕ z ))].</p>
<p>Hit Task.The second task is a dynamic variation of the Push task.The setup is the same, but the pushing action is applied with twice the velocity, resulting in less predictable object behavior due to possible multiple bouncing off the walls.As we can see in Figures 2b, after the vertical cylinder is hit by the robotic arm, it falls over and bounces off from the walls of the table, rolling in the opposite direction of the executed hit action.Here, the tabletop environment and the definitions of s, a, and e remain identical to the Push task.Stack Task.In this task, the goal is to place one object on top of another, forming a possible stable stack.This task involves two objects: a 'picked' object and a 'target' object.As a result, the state includes the positional and orientation information for both objects, doubling the state information compared to the Push and Hit tasks:
s = [s p , s t ],
where s p and s t are the 9-dimensional states of the picked and target objects, denoted as o p and o t , respectively.Given the set of six object configurations in the study, this task results in 6 × 6 = 36 possible object pair combinations.The action is represented by concatenating the one-hot encoding of the picked and the target object:
a = [onehot(o p ), onehot(o t )].
Similarly, the effect is computed as the change in the objects pre-action and post-action states: e = [e p , e t ].</p>
<p>Compared Models and Baselines</p>
<p>Before evaluating the proposed IMTL-EMLP model, we first examine the properties of IMTL-LP, a special case of IMTL-EMLP.We then shift focus to IMTL-EMLP, comparing its performance against both the IMTL-LP model and various baselines.Additionally, we assess different IMTL-EMLP variants with varying energy sensitivity coefficient k, to highlight the tradeoff between prediction performance and energy consumption.The baselines used for assessing advantages of our model with LP-based task selection, i.e., IMTL-LP are as follows:</p>
<p>• SINGLE: To isolate the impact of shared representation (multi-task learning), our first baseline is a single-task learning model where each task has its own dedicated network for training.The proposed network in Figure 1 is adapted for single-task learning where each task has its own state projection, encoder, action projection, attention, and decoder modules without any shared parameters between tasks.While it is possible to implement single-task learning using a simpler vanilla MLP for each task, we intentionally adopt the same multi-task learning architecture used for our proposed method.This ensures that the comparison is fair, with both multitask and single-task setups having identical model structures, allowing the differences in performance to be attributed solely to the learning strategy rather than architectural discrepancies.</p>
<p>• IMTL-RAND: The second baseline uses the shared architecture used in our proposed method, without any modification but removes the LPbased task selection mechanism.Instead, tasks are chosen randomly at each training iteration, with each task having an equal chance of being selected.This baseline serves to highlight the impact of our task selection (interleaving) strategy by comparing it against a scenario where the interleaving is uniformly random.This baseline helps isolate the effectiveness of the scheduling algorithm from the benefits of shared parametrization.</p>
<p>• BLOCK: The third and final baseline is implemented using the same shared network architecture but trains tasks in dedicated, uninterrupted blocks rather than interleaving them.To account for any potential effects caused by the order in which tasks are learned, we evaluate all six possible permutations of the three tasks.For each possible order, each task gets a 'blocked' learning period before moving to the next one.If there are R training epochs in total, each task gets R/m number of trainings.This baseline enables us to assess whether the interleaved approach offers improved learning over all possible blocked learning schedules.</p>
<p>For the experiments with the proposed model IMTL-EMLP, we compare it with different variants of it as follows, in addition to the baselines explained above and proposed IMTL-LP model:</p>
<p>• IMTL-EMLP-K: To explore how sensitivity coefficient k affects the learning performance and energy usage, we compare IMTL-EMLP mod-els trained with varying k values: IMTL-EMLP-K=0.4,IMTL-EMLP-K=0.7,IMTL-EMLP-K=1, IMTL-EMLP-K=1.2.</p>
<p>Network Architecture and Initialization</p>
<p>All models are implemented using the PyTorch [104] framework, using the same set of hyper-parameters across all baseline methods and our proposed model: a learning rate of 0.0001, a batch size of 100, a hidden dimension of 4, and training for 3000 epochs.Each encoder and decoder module, whether shared or task-specific, consists of two fully connected layers, with ReLU used as the activation function.Optimization is performed using the AdamW optimizer [105], with the AMSGrad variant enabled and default weight decay settings.To accelerate neural network training, the interactions of the robot with the environment is cached prior to learning so that for each robot interaction the dynamic simulator need not be invoked.An experience cash size of ≈ 10K suffice as the tasks used in the experiments have low action and state dimensions.</p>
<p>LP-based Interleaved Learning (IMTL-LP) Results</p>
<p>The initial evaluation compares the proposed model with no energy modulation, i.e., IMTL-LP to SINGLE and IMTL-RAND baselines.Experiments were conducted using 10 random seeds, and the average task performance results are presented here.As illustrated in Figure 3, the proposed model IMTL-LP reduces prediction error faster than the other two baselines.This indicates that the LP-based task selection with inter-task skill transfer produces an interleaved task schedule that leads to improved overall performance compared to random selection and independent learning with no skill transfer.Furthermore, it is important to note that not only does interleaving enhance performance, learning multiple tasks is also shown to be beneficial, as tasks can mutually support each other and facilitate positive information transfer.This is evidenced by the IMTL-RAND baseline showing improvement compared to the single-task baseline.</p>
<p>Figure 4: Task-specific model performance is shown.It can be seen that LP-based method surpasses the other baselines on both push and stack tasks, with a higher significance in the latter.However, for the hit task, the improvement was constrained to the learning period around ≈ 400-1000.</p>
<p>Task-wise Analysis.To better understand the learning dynamics, we examine the performance of individual tasks throughout the learning process.As illustrated in Figure 4, the performance on both the Push and Stack tasks improves when all tasks are learned simultaneously, with a marked enhancement observed when using the proposed model.Notably, the Stack task exhibits a significant performance gap between our proposed model, IMTL-LP, and the other two baselines.For the Hit task, as in the other task we see an early reduction in prediction error with IMTL-LP but IMTL-RAND also starts to perform competitive in the second half of training.Table 2 shows the total iterations allocated for each task due to autonomous task selection.A higher performance can be obtained by IMTL-LP compared to IMTL-RAND in stack task even though it is allocated a shorter training time in total, which indicates that the amount of total training time allocated is not a direct determinant of task learning performance.We analyzed the impact of network complexity on model performance by evaluating SINGLE, IMTL-RAND, and our proposed IMTL-LP (no energy modulation) at three levels of complexity: low (800 parameters), medium (2000 parameters), and high (5200 parameters), as shown in Figure 5.At low complexity, IMTL-LP significantly outperformed both baselines, highlighting its efficiency in resource-constrained conditions due to our learning progress-based interleaving strategy.At medium complexity, IMTL-LP still outperformed the baselines, although the gap narrowed, suggesting that while SINGLE and IMTL-RAND benefited from additional parameters, IMTL-LP maintained its advantage through better task management.At high complexity, the differences among the three models further decreased, with similar performance across models, indicating that increased capacity partially compensates for limitations in baseline strategies.However, IMTL-LP consistently remained competitive or best performing across all complexity levels.</p>
<p>Model</p>
<p>Effect of Network Complexity on Learning Performance</p>
<p>Interleaved vs. Blocked Learning</p>
<p>Blocked learning is a training paradigm in which tasks are presented sequentially in distinct, uninterrupted intervals.It can be questioned that the blocked learning scheme may match or surpass the performance observed with interleaved learning when a certain task order is used.To test for this, we conducted a detailed comparison of interleaved learning and blocked learning with a neural network suitable for multi-task learning across the tasks of Push, Hit, and Stack introduced in Section 4.</p>
<p>Given the three tasks, there are six possible training orders (e.g., Push → Hit → Stack or Push → Stack → Hit, etc.).To see whether any order can perform similar or bettered that interleaved learning, we trained the BLOCK model across all six task orders (task IDs are denoted in the order as -XXX, e.g., BLOCK-123), while the IMTL-LP model remained unchanged across these configurations, as it does not depend on the task order (see Table 2 for per task training count of IMTL-LP model).</p>
<p>Figure 6 shows the performance of the IMTL-LP model and the BLOCK model for each of the six task orders.It can be seen that the proposed model, IMTL-LP consistently outperforms the BLOCK model across all task orders.No matter the order in which the tasks are trained, our IMTL-LP model always surpasses the blocked learning method, which shows that interleaved learning prevents catastrophic forgetting of learned information by switching the context in periods, thus being able to recall faster.On the other hand, blocked learning as expected suffers from catastrophic forgetting as evidenced by the error spikes at the task switch boundaries.Additionally, the variability in performance across different task orders for the BLOCK model indicates its sensitivity to the order of training.</p>
<p>Modulation of LP with Computational Energy (IMTL-EMLP)</p>
<p>In this section, inspired form human brain's tendency to reduce computational cost, we explore discounting the learning progress of the tasks by the neural cost they incur during learning.For this, we first test IMTL-EMLP that uses task selection according to e k• LP T i / ẼC T i where the relative importance of LP set as k = 1 (see Equation 9) and compare it to IMTL-LP and other baselines.Then we run experiments with different values of k and compare the resulting learning performances in terms of prediction error and energy expenditure.Figure 7 presents the overall learning performance of our proposed interleaved learning (IMTL-LP and IMTL-EMLP)  IMTL-EMLP-K=1.2 closely follows, demonstrating that modest energy modulation can preserve much of the learning performance.As k decreases, the prediction performance gradually degrades, indicating that overly prioritizing energy conservation (i.e., smaller k) can hinder learning effectiveness.On the other hand, Figure 8b shows total energy consumption measured in terms of cumulative neuron activations.Here, we observe the opposite trend: energy usage decreases steadily with smaller k, with IMTL-EMLP-K=0.4 consuming the least energy overall, almost the same as the SINGLE baseline.While IMTL-LP achieves the best predictive performance, it is also the most energy-intensive.Notably, IMTL-EMLP-K=1.2 and K=1 provide a favorable trade-off, offering reductions in energy consumption with only a small sacrifice in prediction accuracy.This tunable behavior highlights the flexibility of the IMTL-EMLP framework, enabling agents to balance performance and energy efficiency according to task demands and environmental constraints.</p>
<p>Ablation Experiments</p>
<p>In order to reveal the individual affects of the self-attention and the task flag introduced in the proposed multitask learning architecture, a series of ablation experiments are conducted.In addition, task skill transfer ablations are conducted to investigate the skill transfer relations formed due to interleaved learning.</p>
<p>Self-Attention and the Task Flag Ablations</p>
<p>The individual contributions of the attention layer and the flag bit in our proposed model are evaluated using four versions of the model: full model, without task flag, without attention layer, without flag and attention layer.Each model was trained using 10 random seeds.Below, we describe the four ablation settings and provide a detailed analysis of their performance: Full model.The proposed model including a shared attention layer that process the task representations with the flag bit.Model w/o task flag.This variation retains the shared attention layer but removes the flag bits.Task representations are combined and passed to the shared attention layer without explicit differentiation using flags.Model w/o attention layer.In this model, the flag bits are concatenated to the task representations, same as the original matrix Z from Equation ( 6), but instead of the shared attention layer, Z is forwarded directly to task decoder of the current training task.Model w/o attention layer and flag.This configuration combines the two ablations thus only concatenated task representations are relayed to the task decoder of the current training task.</p>
<p>The results of the ablation experiments are shown together in Figure 9 as prediction error versus training epoch curves.We can deduce the following from the ablation experiments.The model without both the attention mechanism and the task-identifying flag bit (Figure 9, w/o attn&amp;flag) showed the poorest performance, indicating that the absence of both components severely hinders the model's ability to share information across tasks and distinguish between them.Introducing only the flag bit (Figure 9, w/o attn) improved the results, as it allowed for explicit task differentiation, though the lack of attention limited the model's capacity to capture dynamic task relationships.Conversely, using the attention mechanism without the flag (Figure 9, w/o flag ) led to better performance, as the model could exploit shared information across tasks despite the absence of explicit task cues.The full model, incorporating both the attention layer and the flag bit, consistently outperformed all other configurations, demonstrating that the two mechanisms are complementary and together provide significant benefits for multi-task learning from the early stages of training.</p>
<p>Object-wise Skill Transfer Analysis</p>
<p>In this section, we present skill transfer among tasks that were quantified by ablating task-to-task projections in the shared attention layer after training is completed.To assess a source task's contribution to a target task, we define the loss obtained on the target task when all task representations are intact in the key/value matrices as L full and use L ablate to denote the loss obtained after zeroing out the representation of the source task.If the source task is irrelevant for the target task, a large increase in the loss is not expected.On the other hand, if the source task is utilized, then the loss should increase.With this logic, we define ∆L = L ablate − L full and interpret the results based on this metric.For example, given a (source, target) task pair, a positive ∆L indicates positive transfer from the source task to the target task.For skill transfer analysis, we focused on our IMTL-LP model and repeated training &amp; testing 10 times with random seeds.The average impact of the ablations are measured by ∆L and its standard deviation and used for numeric and visual representation (see Figure 10).The skill transfer analysis is carried out per-object basis; so, the reported average ∆L for a source and target task pair indicates the skill transfer between the tasks when a particular object is being handled.First, we present the results for single-object interactions (Push and Hit), and then look at two-object interactions (Stack).Figure 10a shows skill transfer levels between source tasks (columns) and target tasks (rows) as a color coded matrix.During interaction with sphere, skill transfer was lower than interacting with the other objects.With sphere, the highest ∆L is observed in Stack → Hit case with 0.86, whereas the lowest is observed in Stack → P ush (0.22).Hit seems to better exploit Stack compared to Push suggesting that richer dynamics of multiple reflections from the walls (due to hit action) and the falls during unstable stacking shares shares certain computational similarities.Supporting this interpretation, in the horizontal cylinder a similar pattern can be observed where Stack benefits Hit more than Push.The common rollability affordance of sphere (2D) and the horizontal cylinder (1D) seem to limit the skill transfer among tasks when the interacted object is rollable.This may be explained by the fact that the behavior variety one can observe from rollable objects is higher compared to non-rollable objects, thereby making consistent skill transfer difficult.One difference in the sphere and the horizontal cylinder is the relative high skill transfer observed in P ush → Hit.This can be understood by considering that the push action is applied from arbitrary directions with many of them not inducing a rolling behavior.In those cases, the behavior of the object is closer to a stable object such as cube or horizontal-prism and thus the transfer level is somewhat shifted toward higher levels as in the stable objects (see horizontal prism and cube P ush → Hit skill transfers).The effects of affordance and orientation can be seen in the skill transfer level of the cylinder and the prism: the P ush → Hit transfer is limited when the objects are vertical (cylinder: 0.55; prism: 0.28), whereas the transfer is enhanced when they are placed horizontally (cylinder: 1.55; prism: 3.26).It seems that Hit action becomes beneficial to Push when objects are oriented vertical as opposed to horizontal so that they are knock-able.In the cylinder case, altering the orientation from horizontal to vertical causes a transfer change of 1.15 to 1.85; similarly in the prism case, a change of 3.07 to 3.33 is observed.</p>
<p>Moving on to the two-object interactions in Figure 10b for the Stack task, each matrix is titled with the base object on to which the picked object is placed.Rows show the picked object and columns show the source tasks (Push and Hit).The most prominent observation is that Hit task is strongly utilized by the Stack task compared to the Push task that have a non-negative small contribution.The high transfers level (∆L ≈ 3.7 − 5.2) from Hit task are observed when the object the base object stays intact, as in when the pair can be stacked successfully.The transfer levels when the base object is a sphere or a horizontal cylinder follow a similar pattern to that of one-object interactions.Their rollability affordances in 2D and 1D, respectively, cause them to behave differently depending on whether they receive a low impact force (push) or high impact force (hit).Cube or vertical cylinder as the base object show almost the same skill transfer pattern.</p>
<p>Considering these object-wise analyzes, along with the task-specific learning performance figures (see Figure 4), we see that Push task learns the effect prediction problem mostly on its own during the early stages of learning, while Hit and Stack tasks co-learn and benefit from each other throughout the training.It is intriguing to see that even though the task selection and learning performance is affected by the random initializations, the standard deviation of the effects of the ablations is about 14%.This indicates that although different skill transfer patterns are possible, the current results reflect a general pattern for the tasks addressed.Overall, we can conclude that, our task agnostic task selection mechanism, albeit being not geared to elicit skill transfer, sculpts an emergent transfer landscape that leads to faster and improved learning.</p>
<p>Conclusion</p>
<p>This paper introduced a biologically inspired interleaved multi-task learning framework that selects tasks dynamically based on learning progress and neural computational energy cost.Motivated by how humans interleave tasks and regulate cognitive effort, our method, IMTL-LP, prioritizes tasks that exhibit positive learning progress, while the energy-aware version, IMTL-EMLP, considers learning efficiency by discounting LP by the energy it takes to achieve it, shifting the preference to the tasks with lower energy consumption.The learning architecture developed follows a shared encoder-decoder structure equipped with bi-directional skill transfer mechanisms that can learn multiple tasks with dynamic task engagement.</p>
<p>The developed framework is evaluated on a robotic manipulation scenario where the robot is tasked with learning the effects of its actions in different task environments.The experimental results demonstrate that LPbased task arbitration not only improves overall learning performance and convergence speed compared to baseline strategies like random interleaving, single-task learning, and blocked training, but also facilitates beneficial knowledge transfer across tasks.The results indicate that it is not only the training time allocated for the tasks that affect the learning performance, but also the task scheduling is critical, which is autonomously generated by our model.Moreover, the inclusion of energy-awareness in the task selection process enables a tunable trade-off between learning accuracy and resource efficiency, which is especially valuable in energy-constrained settings.Our ablation studies further validate the complementary roles of the attention mechanism and task flag bits in enhancing multi-task learning dynamics.</p>
<p>While the model is tested in a controlled simulation environment with a limited number of tasks, the results point to broader implications for continuous and sustainable learning systems.By drawing from principles of in-trinsic motivation and neural efficiency, the proposed framework contributes a novel perspective on how task interleaving and energy-awareness can be adopted in artificial agents.Future work may explore extensions to realworld robotic platforms, integration with reinforcement learning settings, or scaling to larger task sets with more diverse dynamics.</p>
<p>Figure 1 :
1
Figure 1: Overview of the proposed multi-task effect prediction architecture and an example training iteration for T * where * = 2 and total number of tasks m = 3.The model features shared and task-specific components, with only the selected task being trained at each step based on the arbitration mechanism.</p>
<p>Figure 2 :
2
Figure 2: Visual examples of tasks executed in the PyBullet simulation environment.Arrows in Push (a) and Hit (b) show the trajectory of the object after the action is executed.For Stack (c), arrows show the end-effector trajectory while executing the stacking action.</p>
<p>Figure 3 :
3
Figure 3: Overall performance of baselines is shown for isolated task learning (SINGLE), proposed architecture but random task selection (IMTL-RAND) and proposed architecture with LP-based task selection model (IMTL-LP).As can be seen proposed model (IMTL-LP) converges faster than the baselines.Even though the random task selection is slightly better than the single task learning, it could not surpass learning with LP-based task selection.</p>
<p>Figure 5 :
5
Figure 5: Learning performance of baselines (SINGLE, IMTL-RAND) and our model (IMTL-LP) in different network complexity configurations.Results are taken at midtraining.</p>
<p>Figure 6 :
6
Figure 6: Interleaved learning versus blocked learning prediction error plots for each task order combination.Each figure has three regions divided by vertical dashed lines, indicating where the BLOCK model switches to the next task in the order.P stands for Push, H stands for Hit and S stands for Stack task.Rightmost region in each figure shows where both models, BLOCK and IMTL-LP, have seen all the tasks to be learned.In all the task orders, IMTL-LP surpasses block learning.</p>
<p>Figure 7 :
7
Figure 7: Comparison of baselines and LP-based model (IMTL-LP) to the EMLP-based model (IMTL-EMLP) where the sensitivity constant is set to k = 1.</p>
<p>Figure 8 :
8
Figure 8: Effect of varying the energy sensitivity coefficient k in the IMTL-EMLP model on (a) prediction performance and (b) total energy consumption, at the midpoint of training.Higher values of k prioritize learning progress over energy efficiency, leading to lower prediction error but increased energy usage, while lower k values reduce energy consumption at the cost of predictive accuracy.</p>
<p>Figure 9 :
9
Figure 9: Contributions of the shared attention and flag bit from the proposed multi-task learning architecture to the model performances.Neither attention nor flag alone can surpass the convergence speed of our proposed model, which combines them.</p>
<p>(a) Task-wise performance degradation with per-object data, where rows are target tasks and columns are source tasks respectively.(b) Stack task performance degradation with per-target object data, where rows are picked objects by the robotic arm and columns are source tasks respectively.</p>
<p>Figure 10 :
10
Figure10: Impact of source-to-target task ablations on target task performance across objects.Each sub-figure corresponds to a specific object and visualizes the performance degradation (increase in loss) for target tasks (columns) when the representation of a particular source task (rows) is removed from the key and query inputs of the shared multi-head attention module.This experiment reveals the dependency of each target task on the shared representation of other source tasks, highlighting the inter-task transfer and interference dynamics in the multi-effect prediction framework.</p>
<p>Table 2 :
2
The average total number of training iterations received by each task as the result of autonomous task selection in IMTL-LP is compared with uniformly random task selection (IMTL-RAND).The average and standard deviation is over 10 training sessions.</p>
<p>The actual layer sizes for both single-task and multi-task learning setups are provided in the Appendix A.1.
AcknowledgementsThis work was supported by the Japan Society for the Promotion of Science KAKENHI Grant Number JP23K24926.Additional support was provided by the project JPNP16007 commissioned by the New Energy and Industrial Technology Development Organization (NEDO) and Japan Society for the Promotion of Science KAKENHI Grant Number JP25H01236 and by the European Union under the INVERSE project (101136067).
Balancing cognitive and environmental constraints when deciding to switch tasks: Exploring self-reported task-selection strategies in self-organised multitasking. V Mittelstädt, I Schaffernak, J Miller, A Kiesel, Quarterly Journal of Experimental Psychology. 7442021</p>
<p>A systematic review of interleaving as a concept learning strategy. J Firth, I Rivers, J Boyle, Review of Education. 922021</p>
<p>Lifelong machine learning. Z Chen, B Liu, 2018Morgan &amp; Claypool Publishers</p>
<p>A survey on multi-task learning. Y Zhang, Q Yang, IEEE transactions on knowledge and data engineering. 34122021</p>
<p>Lifelong robot learning, Encyclopedia of Robotics. E Oztop, E Ugur, 2020</p>
<p>Continual lifelong learning with neural networks: A review. G I Parisi, R Kemker, J L Part, C Kanan, S Wermter, Neural networks. 1132019</p>
<p>A domain-agnostic approach for characterization of lifelong learning systems. M M Baker, A New, M Aguilar-Simon, Z Al-Halah, S M Arnold, E Ben-Iwhiwhu, A P Brna, E Brooks, R C Brown, Z Daniels, A Daram, F Delattre, R Dellana, E Eaton, H Fu, K Grauman, J Hostetler, S Iqbal, C Kent, N Ketz, S Kolouri, G Konidaris, D Kudithipudi, E Learned-Miller, S Lee, M L Littman, S Madireddy, J A Mendez, E Q Nguyen, C Piatko, P K Pilly, A Raghavan, A Rahman, S K Ramakrishnan, N Ratzlaff, A Soltoggio, P Stone, I Sur, Z Tang, S Tiwari, K Vedder, F Wang, Z Xu, A Yanguas-Gil, H Yedidsion, S Yu, G K Vallabha, 10.1016/j.neunet.2023.01.007.URLhttps://www.sciencedirect.com/science/article/pii/S0893608023000072Neural Networks. 1602023</p>
<p>S J Bell, N D Lawrence, arXiv:2205.13323The effect of task ordering in continual learning. 2022arXiv preprint</p>
<p>Energy-efficient neural information processing in individual neurons and neuronal networks. L Yu, Y Yu, Journal of Neuroscience Research. 95112017</p>
<p>Brain plasticity and behaviour in the developing brain. B Kolb, R Gibb, Journal of the Canadian Academy of Child and Adolescent Psychiatry. 2042652011</p>
<p>The economy of brain network organization. E Bullmore, O Sporns, Nature reviews neuroscience. 1352012</p>
<p>Adolescent neurodevelopment. L P Spear, Journal of adolescent health. 5222013</p>
<p>J Schmidhuber, A possibility for implementing curiosity and boredom in model-building neural controllers, in: Proc. of the international conference on simulation of adaptive behavior: From animals to animats. 1991</p>
<p>Developmental robotics, optimal artificial curiosity, creativity, music, and the fine arts. J Schmidhuber, Connection Science. 1822006</p>
<p>The goldilocks effect in infant auditory attention. C Kidd, S T Piantadosi, R N Aslin, Child development. 8552014</p>
<p>The goldilocks effect: Infants' preference for stimuli that are neither too predictable nor too surprising. C Kidd, S T Piantadosi, R N Aslin, Proceedings of the annual meeting of the cognitive science society. the annual meeting of the cognitive science society201032</p>
<p>Multitask learning. R Caruana, Machine Learning. 281997</p>
<p>A Argyriou, T Evgeniou, M Pontil, Convex multi-task feature learning. 200873</p>
<p>Sparse coding for multitask and transfer learning. A Maurer, M Pontil, B Romera-Paredes, International conference on machine learning. PMLR2013</p>
<p>P Liu, X Qiu, X Huang, arXiv:1704.05742Adversarial multi-task learning for text classification. 2017arXiv preprint</p>
<p>Adversarial multi-task learning of deep neural networks for robust speech recognition. Y Shinohara, 2016InterspeechSan Francisco, CA, USA</p>
<p>Adaptive multi-task lasso: with application to eqtl detection. S Lee, J Zhu, E Xing, Advances in neural information processing systems. 232010</p>
<p>Safe screening for multi-task feature learning with multiple data matrices. J Wang, J Ye, International conference on machine learning. PMLR2015</p>
<p>Cross-stitch networks for multi-task learning. I Misra, A Shrivastava, A Gupta, M Hebert, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2016</p>
<p>A model for cognitively valid lifelong learning. H Say, E Oztop, 2023 IEEE International Conference on Robotics and Biomimetics (ROBIO). IEEE2023</p>
<p>Trace norm regularization: Reformulations, algorithms, and multi-task learning. T K Pong, P Tseng, S Ji, J Ye, SIAM Journal on Optimization. 2062010</p>
<p>L Han, Y Zhang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201630Multi-stage multi-task learning with reduced rank</p>
<p>Learning multiple tasks using shared hypotheses. K Crammer, Y Mansour, Advances in Neural Information Processing Systems. 252012</p>
<p>Convex multi-task learning by clustering. A Barzilai, K Crammer, Artificial Intelligence and Statistics. 2015PMLR</p>
<p>Flexible clustered multi-task learning by learning representative tasks. Q Zhou, Q Zhao, IEEE transactions on pattern analysis and machine intelligence. 201538</p>
<p>Learning sparse task relations in multi-task learning. Y Zhang, Q Yang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201731</p>
<p>Asymmetric multi-task learning based on task relatedness and loss. G Lee, E Yang, S Hwang, International conference on machine learning. PMLR2016</p>
<p>Robust multi-task feature learning. P Gong, J Ye, C Zhang, Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. the 18th ACM SIGKDD international conference on Knowledge discovery and data mining2012</p>
<p>A dirty model for multi-task learning. A Jalali, S Sanghavi, C Ruan, P Ravikumar, Advances in Neural Information Processing Systems. J Lafferty, C Williams, J Shawe-Taylor, R Zemel, A Culotta, Curran Associates, Inc201023</p>
<p>Distral: Robust multitask reinforcement learning, Advances in neural information processing systems. Y Teh, V Bapst, W M Czarnecki, J Quan, J Kirkpatrick, R Hadsell, N Heess, R Pascanu, 201730</p>
<p>A Hallak, D Di Castro, S Mannor, arXiv:1502.02259Contextual markov decision processes. 2015arXiv preprint</p>
<p>Multi-task reinforcement learning with context-based representations. S Sodhani, A Zhang, J Pineau, International Conference on Machine Learning. PMLR2021</p>
<p>Multi-task reinforcement learning with mixture of orthogonal experts. A Hendawy, J Peters, C Eramo, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. A Kendall, Y Gal, R Cipolla, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. Z Chen, V Badrinarayanan, C.-Y Lee, A Rabinovich, International Conference on Learning Representations (ICLR). 2018</p>
<p>Multi-task learning as multi-objective optimization. O Sener, V Koltun, Proceedings of the 35th International Conference on Machine Learning (ICML). the 35th International Conference on Machine Learning (ICML)2018</p>
<p>Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. T Ma, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>The shuffling of mathematics problems improves learning. D Rohrer, K Taylor, Instructional Science. 3562007</p>
<p>Learning concepts and categories: Is spacing the enemy of induction?. N Kornell, R A Bjork, Psychological Science. 1962008</p>
<p>High contextual interference improves retention in motor learning: systematic review and meta-analysis. S H Czyż, A M Wójcik, P Solarská, P Kiper, Scientific Reports. 141159742024</p>
<p>Contextual interference enhances motor learning through increased resting brain connectivity during memory consolidation. C.-H J Lin, H.-C Yang, B J Knowlton, A D Wu, M Iacoboni, Y.-L Ye, S.-L Huang, M.-C Chiang, 10.1016/j.neuroimage.2018.06.081.URLhttps://www.sciencedirect.com/science/article/pii/S1053811918305883NeuroImage. 1812018</p>
<p>Brain-behavior correlates of optimizing learning through interleaved practice. C.-H J Lin, B J Knowlton, M.-C Chiang, M Iacoboni, P Udompholkul, A D Wu, Neuroimage. 5632011</p>
<p>Contextual interference effects on the acquisition, retention, and transfer of a motor skill. J B Shea, R L Morgan, Journal of Experimental psychology: Human Learning and memory. 521791979</p>
<p>Optimizing long-term retention and transfer, In the mind's eye: Enhancing human performance. R Christina, R Bjork, 1991</p>
<p>New conceptualizations of practice: Common principles in three paradigms suggest new concepts for training. R A Schmidt, R A Bjork, Psychological science. 341992</p>
<p>Interleaved practice improves mathematics learning. D Rohrer, R F Dedrick, S Stershic, Journal of Educational Psychology. 10739002015</p>
<p>Interleaved practice enhances memory and problem-solving ability in undergraduate physics. J Samani, S C Pan, Science of Learning. 61322021</p>
<p>D Mayo, T Scott, M Ren, G Elsayed, K Hermann, M Jones, M Mozer, Multitask learning via interleaving: A neural network investigation, in: 44th Annual Meeting of the Cognitive Science Society. 2023</p>
<p>N Kamra, U Gupta, Y Liu, arXiv:1710.10368Deep generative dual memory network for continual learning. 2017arXiv preprint</p>
<p>Learning without forgetting. Z Li, D Hoiem, 201740</p>
<p>Overcoming catastrophic forgetting in neural networks. J Kirkpatrick, R Pascanu, N Rabinowitz, J Veness, G Desjardins, A A Rusu, K Milan, J Quan, T Ramalho, A Grabska-Barwinska, Proceedings of the national academy of sciences. the national academy of sciences2017114</p>
<p>Learning and relearning in boltzmann machines, Parallel distributed processing. G E Hinton, T J Sejnowski, Explorations in the microstructure of cognition. 121986</p>
<p>Intrinsic and extrinsic motivations: Classic definitions and new directions. R M Ryan, E L Deci, Contemporary educational psychology. 2512000</p>
<p>Information-seeking, curiosity, and attention: computational and neural mechanisms. J Gottlieb, P.-Y Oudeyer, M Lopes, A Baranes, Trends in cognitive sciences. 17112013</p>
<p>Learning to recognize objects through curiosity-driven manipulation with the icub humanoid robot. S M Nguyen, S Ivaldi, N Lyubova, A Droniou, D Gérardeaux-Viret, D Filliat, V Padois, O Sigaud, P.-Y Oudeyer, 10.1109/DevLrn.2013.66525252013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL). 2013</p>
<p>A theory for mentally developing robots. J Weng, 10.1109/DEVLRN.2002.1011821Proceedings 2nd International Conference on Development and Learning. 2nd International Conference on Development and Learning2002. 2002</p>
<p>Novelty and reinforcement learning in the value system of developmental robots. X Huang, J Weng, 2002</p>
<p>Surprise-based intrinsic motivation for deep reinforcement learning. J Achiam, S Sastry, arXiv:1703.017322017arXiv preprint</p>
<p>Motivation reconsidered: the concept of competence. R W White, Psychological review. 6652971959</p>
<p>D E Berlyne, Conflict, arousal, and curiosity. McGraw-Hill Book Company1960</p>
<p>Flow: The psychology of optimal experience. M Csikszentmihalyi, M Csikzentmihaly, 1990Harper &amp; Row New York1990</p>
<p>Intrinsic motivation and mental replay enable efficient online adaptation in stochastic recurrent networks. D Tanneberg, J Peters, E Rueckert, org/10.1016/j.neunet.2018.10.005Neural Networks. 1092019</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007</p>
<p>Exploration in model-based reinforcement learning by empirically estimating learning progress. M Lopes, T Lang, M Toussaint, P.-Y Oudeyer, Advances in neural information processing systems. 252012</p>
<p>Effect regulated projection of robot's action space for production and prediction of manipulation primitives through learning progress and predictability-based exploration. S Bugur, E Oztop, Y Nagai, E Ugur, IEEE Transactions on Cognitive and Developmental Systems. 1322019</p>
<p>Exploration with intrinsic motivation using object-action-outcome latent space. M I Sener, Y Nagai, E Oztop, E Ugur, 10.1109/TCDS.2021.3062728IEEE Transactions on Cognitive and Developmental Systems. 1522023</p>
<p>Multitask curriculum learning in a complex, visual, hard-exploration domain. I Kanitscheider, J Huizinga, D Farhi, W H Guss, B Houghton, R Sampedro, P Zhokhov, B Baker, A Ecoffet, J Tang, arXiv:2106.148762021MinecraftarXiv preprint</p>
<p>Curious: intrinsically motivated modular multi-goal reinforcement learning. C Colas, P Fournier, M Chetouani, O Sigaud, P.-Y Oudeyer, International conference on machine learning. PMLR2019</p>
<p>Bidirectional progressive neural networks with episodic return progress for emergent task sequencing and robotic skill transfer. S E Ada, H Say, E Ugur, E Oztop, IEEE Access. 2024</p>
<p>The energy homeostasis principle: neuronal energy regulation drives local network dynamics generating behavior. R C Vergara, S Jaramillo-Riveri, A Luarte, C Moënne-Loccoz, R Fuentes, A Couve, P E Maldonado, Frontiers in computational neuroscience. 13492019</p>
<p>The energy budget for signaling in the grey matter of the brain. D Attwell, S B Laughlin, Journal of Cerebral Blood Flow &amp; Metabolism. 21102001</p>
<p>The cost of cortical computation. P Lennie, Nature Reviews Neuroscience. 462003</p>
<p>The brain's dark energy. M E Raichle, Science. 31458032006</p>
<p>M M Botvinick, S Huffstetler, J T Mcguire, Effort discounting in human nucleus accumbens, Cognitive, affective, &amp; behavioral neuroscience. 20099</p>
<p>Decision making and the avoidance of cognitive demand. W Kool, J T Mcguire, Z B Rosen, M M Botvinick, Journal of Experimental Psychology: General. 13942010</p>
<p>Cognitive effort: A neuroeconomic approach. A Westbrook, T S Braver, Cognitive, Affective, &amp; Behavioral Neuroscience. 152015</p>
<p>An opportunity cost model of subjective effort and task performance. R Kurzban, A Duckworth, J W Kable, J Myers, Behavioural and Brain Sciences. 3662013</p>
<p>Control strategies in object manipulation tasks. J R Flanagan, M C Bowman, R S Johansson, Current opinion in neurobiology. 1662006</p>
<p>Learning object affordances: from sensory-motor coordination to imitation. L Montesano, M Lopes, A Bernardino, J Santos-Victor, Ieee transactions on robotics. 2412008</p>
<p>Forward models in visuomotor control. B Mehta, S Schaal, Journal of Neurophysiology. 8822002</p>
<p>Push-manipulation of complex passive mobile objects using experimentally acquired motion models. T Meriçli, M Veloso, H L Akın, Autonomous Robots. 382015</p>
<p>Iterative linear quadratic regulator design for nonlinear biological movement systems. W Li, E Todorov, First International Conference on Informatics in Control, Automation and Robotics. SciTePress20042</p>
<p>Synthesis and stabilization of complex behaviors through online trajectory optimization. Y Tassa, T Erez, E Todorov, 2012. 2012</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. M Watter, J T Springenberg, J Boedecker, M Riedmiller, Advances in Neural Information Processing Systems (NeurIPS). 2015</p>
<p>Learning to poke by poking: Experiential learning of intuitive physics. P Agrawal, A Nair, P Abbeel, J Malik, S Levine, Advances in Neural Information Processing Systems. 2016</p>
<p>D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, Learning latent dynamics for planning from pixels, in: International conference on machine learning. PMLR2019</p>
<p>D Hafner, T Lillicrap, J Ba, M Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. 2019arXiv preprint</p>
<p>D Ha, J Schmidhuber, arXiv:1803.10122World models. 2018arXiv preprint</p>
<p>World models and predictive coding for cognitive and developmental robotics: frontiers and challenges. T Taniguchi, S Murata, M Suzuki, D Ognibene, P Lanillos, E Ugur, L Jamone, T Nakamura, A Ciria, B Lara, Advanced Robotics. 37132023</p>
<p>Object and relation centric representations for push effect prediction. A E Tekden, A Erdem, E Erdem, T Asfour, E Ugur, Robotics and Autonomous Systems. 1741046322024</p>
<p>Object-centric forward modeling for model predictive control. Y Ye, D Gandhi, A Gupta, S Tulsiani, Conference on Robot Learning. PMLR2020</p>
<p>Symbolic manipulation planning with discovered object and relational predicates. A Ahmetoglu, E Oztop, E Ugur, IEEE Robotics and Automation Letters. 2025</p>
<p>E Ugur, A Ahmetoglu, Y Nagai, T Taniguchi, M Saveriano, E Oztop, 10.36227/techrxiv.175000864.40759665/v1Neuro-symbolic robotics. 2025</p>
<p>Multi-step planning with learned effects of partial action executions. H Aktas, U Bozdogan, E Ugur, Advanced Robotics. 3882024</p>
<p>A neural probabilistic language model. Y Bengio, R Ducharme, P Vincent, C Jauvin, Journal of machine learning research. 3Feb. 2003</p>
<p>T Mikolov, arXiv:1301.3781Efficient estimation of word representations in vector space. 20133781arXiv preprint</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016-2023</p>
<p>Py-Torch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. J Ansel, E Yang, H He, N Gimelshein, A Jain, M Voznesensky, B Bao, P Bell, D Berard, E Burovski, G Chauhan, A Chourdia, W Constable, A Desmaison, Z Devito, E Ellison, W Feng, J Gong, M Gschwind, B Hirsh, S Huang, K Kalambarkar, L Kirsch, M Lazos, M Lezcano, Y Liang, J Liang, Y Lu, C Luk, B Maher, Y Pan, C Puhrsch, M Reso, M Saroufim, M Y Siraichi, H Suk, M Suo, P Tillet, E Wang, X Wang, W Wen, S Zhang, X Zhao, K Zhou, R Zou, A Mathews, G Chanan, P Wu, S Chintala, 10.1145/3620665.364036629th ACM International Conference on Architectural Support for Programming Languages and Operating Systems. ACM22024</p>
<p>I Loshchilov, F Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
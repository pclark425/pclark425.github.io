<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9455 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9455</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9455</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-4fc3542e18d685102e0b334cb9bcce4c8b63ed93</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4fc3542e18d685102e0b334cb9bcce4c8b63ed93" target="_blank">Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work investigates the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve, and observes a “wayward” behavior between the task solved by continuous prompts and their nearest neighbor discrete projections.</p>
                <p><strong>Paper Abstract:</strong> Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a “wayward” behavior between the task solved by continuous prompts and their nearest neighbor discrete projections: We can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task), while being within a very small (2%) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of various parameters. For instance, for larger model sizes we observe higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop in accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9455.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9455.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ContPrompt_vs_Dproj</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continuous Prompting constrained to project to Discrete text vs Unconstrained Continuous Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Main experimental comparison showing that continuous prompts fine-tuned to solve a task can be constrained to have discrete nearest-neighbor projections equal to arbitrary (irrelevant) texts while retaining most task performance, demonstrating the 'Prompt Waywardness' phenomenon.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (GPT2 Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2; SST-5; AGNews; Subj; TREC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Five classification tasks: SST-2 (binary sentiment), SST-5 (5-way sentiment), AGNews (4-way topic), Subj (subjectivity binary), TREC (6-way question type).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous prompt tuning: a learned continuous prompt p_c (length L, i.e., L embedding vectors) is concatenated to input embeddings and the frozen GPT2 model is used; objective augmented with a distance term (gamma * c-dist(p_c, p_d)) to encourage p_c to be near the continuous embedding of a chosen discrete target prompt p_d, and evaluated by projecting p_c back to discrete tokens via nearest-neighbor (d-proj) and measuring token F1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unconstrained continuous prompt tuning (gamma = 0), i.e., standard prompt tuning optimized only for task loss with same prompt length L.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table 2 (gamma = 0.01, average over 62 discrete target prompts and 3 seeds): SST-2: unconstrained 92.4% -> constrained 91.9% (Δ=0.6%); SST-5: 50.3% -> 49.3% (Δ=2.0%); AGNews: 88.1% -> 87.3% (Δ=0.8%); Subj: 90.5% -> 89.2% (Δ=1.5%); TREC: 88.0% -> 86.0% (Δ=2.3%). Prompt F1 for constrained prompts: typically ≥ 94% (average ~98% for many datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>See 'performance' which lists unconstrained -> constrained accuracies for each dataset (same prompt length).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Relative drops (hatΔ) reported above: SST-2 -0.6pp, SST-5 -2.0pp, AGNews -0.8pp, Subj -1.5pp, TREC -2.3pp (pp = percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Waywardness hypothesis: for any discrete prompt p_d there exist continuous prompts p_c that project to p_d (via nearest-neighbor) yet perform nearly as well as the optimal continuous prompt because (1) the discrete→continuous mapping is many-to-one (Voronoi regions in embedding space), and (2) continuous prompts sit before deep expressive networks (earlier layers have large expressive power), enabling diverse behaviors from nearby embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gamma = 0.01 used for main table; 62 discrete target prompts (32 from NaturalInstructions, 30 random sentences from PILE), 3 random seeds; batch size 8, learning rate 0.01, 2000 training steps; initialization of p_c from c-proj(p_d); prompt lengths matched to p_d length; evaluation metrics: task accuracy and word-level token overlap F1 (prompt F1) after d-proj; reported averages over prompts and seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9455.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9455.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gamma_Tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trade-off between Projection Strength (gamma) and Task Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis of how the weight gamma on the distance-to-target discrete projection term affects prompt discreteness (prompt F1) and task performance: larger gamma increases prompt F1 (closeness to target discrete text) but reduces task accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (GPT2 Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2; AGNews (examples shown)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Classification tasks (SST-2 sentiment and AGNews topic) used to illustrate trade-off curves.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous prompt tuning with varying gamma in the joint loss ell'(p_c; D, gamma) = ell(p_c; D) + gamma * dist(p_c, p_d), where gamma is swept in [0, 0.03]. Distances used are c-dist (L2 in embedding space) during training and d-dist (F1 after d-proj) for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparison across gamma values; baseline is unconstrained gamma=0 (no projection penalty).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As gamma increases, prompt F1 rises toward ~1.00 while task accuracy declines; authors report that it is possible to achieve prompt F1 near 1.00 with an accuracy drop <1% relative to unconstrained prompts (example reported for SST-2/AGNews).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Baseline (gamma=0) vs higher gamma: baseline accuracy is unconstrained value (e.g., SST-2 ~92.4%); at high gamma with F1≈1.0 accuracy drop is <1 percentage point.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Accuracy drop at high gamma (prompt F1≈1.0) reported as less than 1 percentage point in many cases; exact curves depend on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Increasing gamma forces the continuous prompt into regions that project to the desired discrete text, improving d-proj F1 but constraining the optimization for task-specific behavior; however, because of the large Voronoi regions and expressive power of deep models, high F1 can be achieved with small loss in task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gamma swept from 0 to 0.03; figures averaged over 32 NaturalInstructions prompts and 3 seeds; dotted lines indicate unconstrained accuracy; use c-dist during training and d-proj F1 for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9455.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9455.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt_Length_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of Continuous Prompt Length (L) on Projectability and Accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study of how the length (number of embedding tokens L) of continuous prompts affects the ability to both project to a target discrete prompt (high prompt F1) and retain task accuracy; short prompts lack expressivity and fail to simultaneously achieve both.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (GPT2 Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AGNews (illustrative), also aggregated over other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Text classification (AGNews used for the length sweep illustration).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous prompt tuning with target discrete sentences sampled from PILE constrained to have length L (L ∈ {4,7,14,28,56}), optimized with gamma=0.01 for projection closeness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unconstrained continuous prompts of the same length L (gamma=0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>When L is very small (e.g., 4), constrained prompts achieve low prompt F1 (<60%) while retaining accuracy is difficult; for moderate-to-large L (≈14 and larger), constrained prompts reach near 1.0 prompt F1 with little accuracy drop compared to unconstrained prompts (plots show both accuracies increasing with L and gap decreasing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Accuracy and prompt F1 both increase with L; gap between unconstrained and projection-constrained prompts decreases as L increases (marginal relative drop when L >= 7–14).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Severe at very small L (e.g., F1 < 60% at L=4) and small (<< 2 percentage points) when L ≥ 14; exact values vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Short prompts lack expressive capacity (insufficient degrees of freedom) to simultaneously match a target discrete projection and encode task-specific behavior; longer prompts allow more parameters to lie inside the discrete token's Voronoi region while still encoding task-relevant signals.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt lengths L ∈ {4,7,14,28,56}; target sentences sampled from PILE with length L; gamma = 0.01; averages over 32 NaturalInstructions prompts and 3 seeds for plotted curves (AGNews shown).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9455.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9455.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model_Size_Effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of LM Size on Waywardness (degree to which projections can be arbitrary with small accuracy loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Analysis showing that larger GPT2 models exhibit stronger waywardness: constrained prompts that project to arbitrary text suffer smaller accuracy drops on target tasks in larger models compared to smaller ones.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2 (family: small, medium, large, XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>124M (small); 355M (medium); 774M (large); 1.5B (XL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2 (illustrative)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Binary sentiment classification (SST-2) used to evaluate trends across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous prompt tuning constrained to project to target discrete text (gamma chosen per experiment to get prompt F1 > 0.98 where possible); optimization performed for each model size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Unconstrained prompts (same prompt length) for each corresponding model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported relative accuracy drops (constrained vs unconstrained) on SST-2: XL: ~0.2 percentage points; medium & large: ~0.5–0.7 pp; small: ~1.2 pp. Constrained prompts still achieve high prompt F1 (>0.98) across sizes, but the accuracy penalty is smaller for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Smaller models show larger relative accuracy degradation under projection constraint than larger models (e.g., small ~1.2pp vs XL ~0.2pp).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Effect size (relative accuracy drop) decreases monotonically with model size in reported experiments (e.g., ≈1.2pp small -> ≈0.2pp XL).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Deeper/larger models have higher expressivity in early layers so continuous prompts that lie anywhere inside a token's Voronoi region can still be transformed by the deep network into task-specific behavior; hence larger models permit closer mapping to arbitrary discrete text with smaller accuracy loss.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models evaluated: GPT2 small (124M), medium (355M), large (774M), XL (1.5B); experiments averaged over 30 PILE prompts and 3 seeds; gamma values in {0.01, 0.005, 0.003} chosen to achieve prompt F1 > 0.98 where possible; plotted trends show relative accuracy drop decreasing with model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9455.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9455.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TruePrompt_Projection_Null</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Projection to True Task Definitions vs Irrelevant Text (No Benefit)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Finding that constraining continuous prompts to project to human-authored 'true' task descriptions does not improve task performance compared to constraining to irrelevant target texts; both show similar small performance gaps relative to unconstrained prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>774M (GPT2 Large)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>SST-2; SST-5; AGNews; Subj; TREC</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same five classification datasets, with manually authored 'true' textual task descriptions used as target discrete prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Continuous prompt tuning with constraint to project to 'true' human-authored task definition sentences (gamma = 0.01), same objective as for arbitrary projections.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Constrained-to-irrelevant-text (Table 2) and unconstrained prompts (gamma = 0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per Table 4 (gamma = 0.01): SST-2: 91.9% -> 90.9% (Δ_T=1.0pp); SST-5: 51.4% -> 50.5% (Δ_T=0.9pp); AGNews: 91.8% -> 90.4% (Δ_T=1.4pp); Subj: 89.8% -> 85.6% (Δ_T=4.1pp); TREC: 88.6% -> 88.1% (Δ_T=0.5pp). Average Δ_T ≈ 1.6pp.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Average gap when projecting to irrelevant targets (Table 2) ≈ 1.4pp; projection to true definitions yields similar average gap ≈ 1.6pp, i.e., no meaningful advantage from true-text projections.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Average performance gap relative to unconstrained prompts: ≈1.6 percentage points for true-task projections vs ≈1.4pp for irrelevant projections (difference negligible).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Projection to human-readable task descriptions does not help because continuous prompt-to-behavior mapping is not aligned with discrete textual semantics; many continuous prompts in the Voronoi region of a 'true' description do not necessarily encode the same task semantics despite projecting to the same text.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Authors manually authored 5 'true' prompts (one per dataset); used same optimization protocol and gamma=0.01; unconstrained prompts of same lengths trained for fair comparison; results averaged over seeds as with other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts', 'publication_date_yy_mm': '2021-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The power of scale for parameter-efficient prompt tuning <em>(Rating: 2)</em></li>
                <li>Cross-task generalization via natural language crowdsourcing instructions <em>(Rating: 2)</em></li>
                <li>Noisy channel language model prompting for few-shot text classification <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Eliciting knowledge from language models using automatically generated prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9455",
    "paper_id": "paper-4fc3542e18d685102e0b334cb9bcce4c8b63ed93",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "ContPrompt_vs_Dproj",
            "name_full": "Continuous Prompting constrained to project to Discrete text vs Unconstrained Continuous Prompting",
            "brief_description": "Main experimental comparison showing that continuous prompts fine-tuned to solve a task can be constrained to have discrete nearest-neighbor projections equal to arbitrary (irrelevant) texts while retaining most task performance, demonstrating the 'Prompt Waywardness' phenomenon.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2",
            "model_size": "774M (GPT2 Large)",
            "task_name": "SST-2; SST-5; AGNews; Subj; TREC",
            "task_description": "Five classification tasks: SST-2 (binary sentiment), SST-5 (5-way sentiment), AGNews (4-way topic), Subj (subjectivity binary), TREC (6-way question type).",
            "presentation_format": "Continuous prompt tuning: a learned continuous prompt p_c (length L, i.e., L embedding vectors) is concatenated to input embeddings and the frozen GPT2 model is used; objective augmented with a distance term (gamma * c-dist(p_c, p_d)) to encourage p_c to be near the continuous embedding of a chosen discrete target prompt p_d, and evaluated by projecting p_c back to discrete tokens via nearest-neighbor (d-proj) and measuring token F1.",
            "comparison_format": "Unconstrained continuous prompt tuning (gamma = 0), i.e., standard prompt tuning optimized only for task loss with same prompt length L.",
            "performance": "Per Table 2 (gamma = 0.01, average over 62 discrete target prompts and 3 seeds): SST-2: unconstrained 92.4% -&gt; constrained 91.9% (Δ=0.6%); SST-5: 50.3% -&gt; 49.3% (Δ=2.0%); AGNews: 88.1% -&gt; 87.3% (Δ=0.8%); Subj: 90.5% -&gt; 89.2% (Δ=1.5%); TREC: 88.0% -&gt; 86.0% (Δ=2.3%). Prompt F1 for constrained prompts: typically ≥ 94% (average ~98% for many datasets).",
            "performance_comparison": "See 'performance' which lists unconstrained -&gt; constrained accuracies for each dataset (same prompt length).",
            "format_effect_size": "Relative drops (hatΔ) reported above: SST-2 -0.6pp, SST-5 -2.0pp, AGNews -0.8pp, Subj -1.5pp, TREC -2.3pp (pp = percentage points).",
            "explanation_or_hypothesis": "Waywardness hypothesis: for any discrete prompt p_d there exist continuous prompts p_c that project to p_d (via nearest-neighbor) yet perform nearly as well as the optimal continuous prompt because (1) the discrete→continuous mapping is many-to-one (Voronoi regions in embedding space), and (2) continuous prompts sit before deep expressive networks (earlier layers have large expressive power), enabling diverse behaviors from nearby embeddings.",
            "null_or_negative_result": false,
            "experimental_details": "Gamma = 0.01 used for main table; 62 discrete target prompts (32 from NaturalInstructions, 30 random sentences from PILE), 3 random seeds; batch size 8, learning rate 0.01, 2000 training steps; initialization of p_c from c-proj(p_d); prompt lengths matched to p_d length; evaluation metrics: task accuracy and word-level token overlap F1 (prompt F1) after d-proj; reported averages over prompts and seeds.",
            "uuid": "e9455.0",
            "source_info": {
                "paper_title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Gamma_Tradeoff",
            "name_full": "Trade-off between Projection Strength (gamma) and Task Accuracy",
            "brief_description": "Analysis of how the weight gamma on the distance-to-target discrete projection term affects prompt discreteness (prompt F1) and task performance: larger gamma increases prompt F1 (closeness to target discrete text) but reduces task accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2",
            "model_size": "774M (GPT2 Large)",
            "task_name": "SST-2; AGNews (examples shown)",
            "task_description": "Classification tasks (SST-2 sentiment and AGNews topic) used to illustrate trade-off curves.",
            "presentation_format": "Continuous prompt tuning with varying gamma in the joint loss ell'(p_c; D, gamma) = ell(p_c; D) + gamma * dist(p_c, p_d), where gamma is swept in [0, 0.03]. Distances used are c-dist (L2 in embedding space) during training and d-dist (F1 after d-proj) for evaluation.",
            "comparison_format": "Comparison across gamma values; baseline is unconstrained gamma=0 (no projection penalty).",
            "performance": "As gamma increases, prompt F1 rises toward ~1.00 while task accuracy declines; authors report that it is possible to achieve prompt F1 near 1.00 with an accuracy drop &lt;1% relative to unconstrained prompts (example reported for SST-2/AGNews).",
            "performance_comparison": "Baseline (gamma=0) vs higher gamma: baseline accuracy is unconstrained value (e.g., SST-2 ~92.4%); at high gamma with F1≈1.0 accuracy drop is &lt;1 percentage point.",
            "format_effect_size": "Accuracy drop at high gamma (prompt F1≈1.0) reported as less than 1 percentage point in many cases; exact curves depend on dataset.",
            "explanation_or_hypothesis": "Increasing gamma forces the continuous prompt into regions that project to the desired discrete text, improving d-proj F1 but constraining the optimization for task-specific behavior; however, because of the large Voronoi regions and expressive power of deep models, high F1 can be achieved with small loss in task performance.",
            "null_or_negative_result": false,
            "experimental_details": "Gamma swept from 0 to 0.03; figures averaged over 32 NaturalInstructions prompts and 3 seeds; dotted lines indicate unconstrained accuracy; use c-dist during training and d-proj F1 for evaluation.",
            "uuid": "e9455.1",
            "source_info": {
                "paper_title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Prompt_Length_Effect",
            "name_full": "Effect of Continuous Prompt Length (L) on Projectability and Accuracy",
            "brief_description": "Empirical study of how the length (number of embedding tokens L) of continuous prompts affects the ability to both project to a target discrete prompt (high prompt F1) and retain task accuracy; short prompts lack expressivity and fail to simultaneously achieve both.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2",
            "model_size": "774M (GPT2 Large)",
            "task_name": "AGNews (illustrative), also aggregated over other tasks",
            "task_description": "Text classification (AGNews used for the length sweep illustration).",
            "presentation_format": "Continuous prompt tuning with target discrete sentences sampled from PILE constrained to have length L (L ∈ {4,7,14,28,56}), optimized with gamma=0.01 for projection closeness.",
            "comparison_format": "Unconstrained continuous prompts of the same length L (gamma=0).",
            "performance": "When L is very small (e.g., 4), constrained prompts achieve low prompt F1 (&lt;60%) while retaining accuracy is difficult; for moderate-to-large L (≈14 and larger), constrained prompts reach near 1.0 prompt F1 with little accuracy drop compared to unconstrained prompts (plots show both accuracies increasing with L and gap decreasing).",
            "performance_comparison": "Accuracy and prompt F1 both increase with L; gap between unconstrained and projection-constrained prompts decreases as L increases (marginal relative drop when L &gt;= 7–14).",
            "format_effect_size": "Severe at very small L (e.g., F1 &lt; 60% at L=4) and small (&lt;&lt; 2 percentage points) when L ≥ 14; exact values vary by dataset.",
            "explanation_or_hypothesis": "Short prompts lack expressive capacity (insufficient degrees of freedom) to simultaneously match a target discrete projection and encode task-specific behavior; longer prompts allow more parameters to lie inside the discrete token's Voronoi region while still encoding task-relevant signals.",
            "null_or_negative_result": false,
            "experimental_details": "Prompt lengths L ∈ {4,7,14,28,56}; target sentences sampled from PILE with length L; gamma = 0.01; averages over 32 NaturalInstructions prompts and 3 seeds for plotted curves (AGNews shown).",
            "uuid": "e9455.2",
            "source_info": {
                "paper_title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "Model_Size_Effect",
            "name_full": "Effect of LM Size on Waywardness (degree to which projections can be arbitrary with small accuracy loss)",
            "brief_description": "Analysis showing that larger GPT2 models exhibit stronger waywardness: constrained prompts that project to arbitrary text suffer smaller accuracy drops on target tasks in larger models compared to smaller ones.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2 (family: small, medium, large, XL)",
            "model_size": "124M (small); 355M (medium); 774M (large); 1.5B (XL)",
            "task_name": "SST-2 (illustrative)",
            "task_description": "Binary sentiment classification (SST-2) used to evaluate trends across model sizes.",
            "presentation_format": "Continuous prompt tuning constrained to project to target discrete text (gamma chosen per experiment to get prompt F1 &gt; 0.98 where possible); optimization performed for each model size.",
            "comparison_format": "Unconstrained prompts (same prompt length) for each corresponding model size.",
            "performance": "Reported relative accuracy drops (constrained vs unconstrained) on SST-2: XL: ~0.2 percentage points; medium & large: ~0.5–0.7 pp; small: ~1.2 pp. Constrained prompts still achieve high prompt F1 (&gt;0.98) across sizes, but the accuracy penalty is smaller for larger models.",
            "performance_comparison": "Smaller models show larger relative accuracy degradation under projection constraint than larger models (e.g., small ~1.2pp vs XL ~0.2pp).",
            "format_effect_size": "Effect size (relative accuracy drop) decreases monotonically with model size in reported experiments (e.g., ≈1.2pp small -&gt; ≈0.2pp XL).",
            "explanation_or_hypothesis": "Deeper/larger models have higher expressivity in early layers so continuous prompts that lie anywhere inside a token's Voronoi region can still be transformed by the deep network into task-specific behavior; hence larger models permit closer mapping to arbitrary discrete text with smaller accuracy loss.",
            "null_or_negative_result": false,
            "experimental_details": "Models evaluated: GPT2 small (124M), medium (355M), large (774M), XL (1.5B); experiments averaged over 30 PILE prompts and 3 seeds; gamma values in {0.01, 0.005, 0.003} chosen to achieve prompt F1 &gt; 0.98 where possible; plotted trends show relative accuracy drop decreasing with model size.",
            "uuid": "e9455.3",
            "source_info": {
                "paper_title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
                "publication_date_yy_mm": "2021-12"
            }
        },
        {
            "name_short": "TruePrompt_Projection_Null",
            "name_full": "Projection to True Task Definitions vs Irrelevant Text (No Benefit)",
            "brief_description": "Finding that constraining continuous prompts to project to human-authored 'true' task descriptions does not improve task performance compared to constraining to irrelevant target texts; both show similar small performance gaps relative to unconstrained prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT2",
            "model_size": "774M (GPT2 Large)",
            "task_name": "SST-2; SST-5; AGNews; Subj; TREC",
            "task_description": "Same five classification datasets, with manually authored 'true' textual task descriptions used as target discrete prompts.",
            "presentation_format": "Continuous prompt tuning with constraint to project to 'true' human-authored task definition sentences (gamma = 0.01), same objective as for arbitrary projections.",
            "comparison_format": "Constrained-to-irrelevant-text (Table 2) and unconstrained prompts (gamma = 0).",
            "performance": "Per Table 4 (gamma = 0.01): SST-2: 91.9% -&gt; 90.9% (Δ_T=1.0pp); SST-5: 51.4% -&gt; 50.5% (Δ_T=0.9pp); AGNews: 91.8% -&gt; 90.4% (Δ_T=1.4pp); Subj: 89.8% -&gt; 85.6% (Δ_T=4.1pp); TREC: 88.6% -&gt; 88.1% (Δ_T=0.5pp). Average Δ_T ≈ 1.6pp.",
            "performance_comparison": "Average gap when projecting to irrelevant targets (Table 2) ≈ 1.4pp; projection to true definitions yields similar average gap ≈ 1.6pp, i.e., no meaningful advantage from true-text projections.",
            "format_effect_size": "Average performance gap relative to unconstrained prompts: ≈1.6 percentage points for true-task projections vs ≈1.4pp for irrelevant projections (difference negligible).",
            "explanation_or_hypothesis": "Projection to human-readable task descriptions does not help because continuous prompt-to-behavior mapping is not aligned with discrete textual semantics; many continuous prompts in the Voronoi region of a 'true' description do not necessarily encode the same task semantics despite projecting to the same text.",
            "null_or_negative_result": true,
            "experimental_details": "Authors manually authored 5 'true' prompts (one per dataset); used same optimization protocol and gamma=0.01; unconstrained prompts of same lengths trained for fair comparison; results averaged over seeds as with other experiments.",
            "uuid": "e9455.4",
            "source_info": {
                "paper_title": "Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts",
                "publication_date_yy_mm": "2021-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The power of scale for parameter-efficient prompt tuning",
            "rating": 2
        },
        {
            "paper_title": "Cross-task generalization via natural language crowdsourcing instructions",
            "rating": 2
        },
        {
            "paper_title": "Noisy channel language model prompting for few-shot text classification",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Eliciting knowledge from language models using automatically generated prompts",
            "rating": 1
        }
    ],
    "cost": 0.013773499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts</h1>
<p>Daniel Khashabi ${ }^{\ddagger}$ Xinxi Lyu ${ }^{\dagger}$ Sewon Min ${ }^{\dagger}$<br>Lianhui Qin ${ }^{\dagger}$ Kyle Richardson ${ }^{\ddagger}$ Sean Welleck ${ }^{\dagger \ddagger}$<br>Hannaneh Hajishirzi ${ }^{\dagger \ddagger}$ Tushar Khot ${ }^{\ddagger}$ Ashish Sabharwal ${ }^{\ddagger}$ Sameer Singh ${ }^{\ddagger \bigcirc}$ Yejin Choi ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ University of Washington ${ }^{\ddagger}$ Allen Institute for AI ${ }^{\bigcirc}$ University of California-Irvine</p>
<h4>Abstract</h4>
<p>Fine-tuning continuous prompts for target tasks has recently emerged as a compact alternative to full model fine-tuning. Motivated by these promising results, we investigate the feasibility of extracting a discrete (textual) interpretation of continuous prompts that is faithful to the problem they solve. In practice, we observe a "wayward" behavior between the task solved by continuous prompts and the nearest neighbor discrete projections of these prompts: One can find continuous prompts that solve a task while being projected to an arbitrary text (e.g., definition of a different or even a contradictory task) and simultaneously being within a very small ( $2 \%$ ) margin of the best continuous prompt of the same size for the task. We provide intuitions behind this odd and surprising behavior, as well as extensive empirical analyses quantifying the effect of design choices. For instance, larger models exhibit higher waywardness, i.e, we can find prompts that more closely map to any arbitrary text with a smaller drop of accuracy. These findings have important implications relating to the difficulty of faithfully interpreting continuous prompts and their generalization across models and tasks, providing guidance for future progress in prompting language models.</p>
<h2>1 Introduction</h2>
<p>Recent work has shown the surprising power of continuous prompts to language models (LMs) for controlled generation and for solving a wide range of tasks (Li and Liang, 2021; Lester et al., 2021; Min et al., 2022). Despite these successes, the resulting continuous prompts are not easy to interpret (Shin et al., 2020). Is it possible to come up with meaningful discrete (textual) interpretations of continuous prompts, especially ones that provide a faithful explanation of the prompt's behavior?</p>
<p>Towards addressing this question, we propose and investigate the Prompt Waywardness hypothesis (§3.2), a surprising disconnect between the
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We show that one can find accurate continuous prompts (that do well on a given task, e.g., sentiment classification here) such that they can be projected to any arbitrary text, such as the definition of a different task (e.g., generating a question) or even an irrelevant statement (e.g., a piece of code) - suggesting a disconnect between the outcome of continuous prompts and their discrete interpretations.
intended behavior of continuous prompts and their nearest-neighbor discrete (language) representations. ${ }^{\dagger}$ In particular, we show that one can find continuous prompts that perform a desired task while, at the same time, project to any given target text. This indicates that there is little correspondence between continuous prompts and their discrete interpretation. For instance, a continuous prompt that effectively solves the sentiment classification task in Fig.1, when projected onto discrete space, might appear as the definition of a different task ("flip the sentiment"). Intuitively, continuous prompt optimization is a highly non-convex problem with numerous local minima. More surprisingly, many of these prompts (e.g., those near embedded text) are very effective at solving a desired task.</p>
<p>We conduct extensive analyses showing Waywardness on five classification datasets (§4). Empirically, we find the existence of wayward prompts — prompts that solve each of these tasks while pro-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>jecting to arbitrary natural language text. To study a variety of projected text, we experiment with 60+ sentences, either discrete prompts from other tasks (from Mishra et al. 2022b) or random sentences from a large text corpus. We observe that it is possible to find prompts that project to a given discrete prompt (token overlap $94 \% \mathrm{~F} 1$ ) while scoring within $2 \%$ accuracy of the best continuous promptsbased solution for the task. Further analysis shows that the effect of Waywardness gets worse for larger models and longer prompts. We explain this surprising behavior by relating it to several structural properties of large language models (§5).</p>
<p>We discuss several social and research implications of prompt waywardness, to help guide future research on prompt based models (§6). First and foremost, despite many promising attributes of continuous prompts, interpreting them is non-trivial and will require further research. In fact, careless interpretation of continuous prompts can result in vulnerabilities against malicious attacks concealed under the guise of benign discrete representation. Further, the loose correspondence between continuous and discrete prompts poses a challenge for future research in differentiable interpretable-prompt optimization - optimization in search of human readable discrete prompts through the continuous space. Our work shows that continuous and discrete prompts, despite their seeming similarity, are quite different and the results from one may not always transfer to the other. We hope these findings will motivate further innovations in the prompting literature for NLP models.</p>
<h2>2 Related Work</h2>
<p>Continuous prompts. There is a line of work focused on tuning continuous prompts (Li and Liang, 2021; Lester et al., 2021; Zhong et al., 2021; Qin and Eisner, 2021; Zhou et al., 2021; Zhong et al., 2021). These works present different approaches to discovering a continuous prompt (which is an array of real numbers) for addressing an end task, though the interpretability of the resulting prompts remains an open question. This paper investigates the feasibility of interpreting a learned continuous prompt and its connection to discrete prompts.</p>
<p>Discrete prompts. The release of GPT-3 (Brown et al., 2020) initiated a body of work on the emergent ability of LMs to follow discrete natural language prompts. Consequently, several followup studies have used manually-designed discrete
prompts for probing LMs (Petroni et al., 2019; Jiang et al., 2020), improving LMs' few-shot ability (Schick and Schütze, 2021; Gao et al., 2021; Le Scao and Rush, 2021), and their zero-shot ability as well as transferability (Mishra et al., 2022a; Reynolds and McDonell, 2021). Most importantly, discrete prompts have the advantages of being human-readable and thus easily interpretable though we do not have efficient and algorithmic ways of reconstructing them. For example, Shin et al. (2020)'s algorithm discovers discrete prompts, yet the results are not human readable. Prior work also finds that model performance is highly sensitive to small changes in wordings (Mishra et al., 2022a) and that optimization over the discrete prompt space is non-trivial and often highly unstable. Our findings here about the disconnect between continuous prompts and their discrete interpretation provides another perspective on the difficulty of discovering discrete prompts via continuous optimization algorithms that (directly or indirectly) leverage the continuous space (more discussion in §6).</p>
<h2>3 Prompt Waywardness</h2>
<h3>3.1 Preliminaries: Setup and Terminology</h3>
<p>We begin with some notation and the setup of our study, starting with the space of discrete and continuous prompts (Fig.2). Let $p_{d} \in{0,1}^{L \times V}$ denote a discrete prompt represented as an $L$-length sequence of one-hot vectors over a lexicon of size $V$ (corners of a hyper-cube). Similarly, let $p_{c} \in \mathbb{R}^{L \times d}$ denote a continuous prompt, represented as a $L$ length sequence of $d$-dimensional real vectors.</p>
<p>Projection operators. We define operators that project these two spaces to one another. Define the $c$-projection as one that maps discrete inputs to a continuous space by multiplying with a fixed (often pre-trained) embedding matrix ${ }^{2} E \in \mathbb{R}^{V \times d}$ :</p>
<p>$$
c \text {-proj }\left(p_{d}\right)=p_{d} E \in \mathbb{R}^{L \times d}
$$</p>
<p>The $d$-projection maps the continuous inputs to nearest neighbor discrete elements, where for each position $l(1 \leq l \leq L)$, one of the possible (and perhaps most straightforward) methods for interpreting a continuous prompt is defined as a projection</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The general problem setup: Similar to Lester et al. (2021) 's setup, each prompt (usually a continuous one) is appended to the given input and fed to a frozen language model.</p>
<p>onto nearest neighbor representations (Mikolov et al., 2013; Hashimoto et al., 2016):</p>
<p>$$d\text{-proj}(p_c) = [\delta_1 \cdots \delta_l \cdots \delta_L] \in {0, 1}^{L \times V},\qquad(2)$$</p>
<p>where $\delta_l$ is a one-hot vector corresponding to the word with the closest (highest dot product) embedding to the $l$-th position of continuous prompt $p_c$.</p>
<p>These projections are used in the first and last layer of virtually all modern LMs, such as GPT2.</p>
<p><strong>Solving tasks with continuous prompts.</strong> Consider any machine learning model $M$ (typically a pre-trained model) that takes textual input $x$ and produces output $y$. Normally, the parameters of $M$ are learned so as to optimize behavior on a task with a dataset $D = {(x, y)}$ of input/output pairs. In prompt tuning (Lester et al., 2021), one freezes the parameters of $M$ and instead optimizes for a prompt $p$ that, when fed in conjunction with $x$, makes $M$ produce the desired output $y$. Thus, $p$ represents the only learnable parameters in this method. When $p$ is a discrete prompt with $k$ tokens, it can be simply concatenated with $x$, denoted $p + x$. In our study, $p$ will be a <em>continuous</em> prompt (of length equal to the embedding of $k$ tokens). We will concatenate it with the embedding of the input $x$. For simplicity and with some abuse of notation, we use $p + x$ to denote concatenation in this continuous case as well.</p>
<p>One can quantify the amount of loss incurred when using a continuous prompt $p$ as follows:</p>
<p>$$\ell(p; D) = \mathbb{E}_{x,y \sim D} [\text{loss}(M(p + x), y)],\qquad(3)$$</p>
<p>Minimizing this loss function (empirical risk minimization) over $p$ recovers a minimum risk continuous prompt for this dataset:</p>
<p>$$p_c^* = \arg\min_{\mathbf{p}_c \in \mathbb{R}^L \times d} \ell(p_c; D^{\text{train}}).\tag{4}$$</p>
<p>Given this prompt, its generalization to the test data can be measured in terms of the loss incurred on the test set: $\ell(p_c^*; D^{\text{test}})$.</p>
<h3>3.2 The Waywardness Hypothesis</h3>
<p>How should one interpret the resultant continuous prompt $\hat{p}_c$? Empirically, one can easily verify that such continuous prompts are not unique (e.g., random initializations lead to different outcomes). Additionally, the resultant prompts get projected to seemingly irrelevant discrete elements. Taking this to an extreme, we hypothesize that next to the continuous projection $c\text{-proj}(p_d)$ of any discrete prompt $p_d$, there exists a variety of continuous prompts $p_c$ that trigger responses from model $M$ that are orthogonal to the intentions described by the discrete prompt $p_d$. We formalize this idea as the following hypothesis, where $L \in \mathbb{N}$ is the length of the discrete target prompt, $M$ is a prompt-based model, and $D$ is a dataset for a desired task:</p>
<p>Hypothesis 1 (Prompt Waywardness) For all $L, M, D$, there is a small $\Delta$ such that for any discrete target prompt $p_d$ with length $L$, there exists a continuous prompt $\hat{p}_c \in \mathbb{R}^{L \times d}$ such that:</p>
<ol>
<li>$|\ell(\hat{p}_c; D^{\text{test}}) - \ell(p_c^*; D^{\text{test}})| &lt; \Delta$, yet</li>
<li>$d\text{-proj}(\hat{p}_c) = p_d$.</li>
</ol>
<p>In other words, $\hat{p}_c$ is nearly as effective at making $M$ solve the task as the optimal continuous prompt (Eq.4), and yet it projects to $p_d$. In this statement, $\Delta$ (prompt performance gap relative to the optimal prompt $p_c^*$) is a function of the prompt length $L$, the model $M$ (e.g., its embedding size and depth when $M$ is transformer based), and inherent properties of the target dataset $D$. The analysis in §4.3 will provide an empirical estimate of this gap $\hat{\Delta}$ as a function of various parameters like model size and prompt length.</p>
<p>It is worth emphasizing that the hypothesis is stated for any task and any set of discrete prompts, even if they are irrelevant or contradictory.(^3)</p>
<p>(^3)While our focus is on the use of continuous prompts for solving datasets (one prompt shared among many instances), one can imagine applications of the same conjecture to special use cases such as controlled generation (Dathathri et al., 2019) with one prompt per instance.</p>
<h3>3.3 Finding Wayward Prompts</h3>
<p>While the above hypothesis promises the existence of $\tilde{p}_{c}$, it does not say how to discover them. We now discuss a practical method for their discovery.</p>
<p>We learn a continuous prompt $p_{c}$ using a modification of the prompt tuning objective of Lester et al. (2021). Our modification jointly minimizes the standard downstream task cross-entropy loss $\ell($.$) for the task (Eq.3) and a distance measure$ dist(.) between $p_{c}$ and the discrete target prompt $p_{d} \in{0,1}^{L \times V}$ :</p>
<p>$$
\begin{aligned}
\ell^{\prime}\left(p_{c} ; \mathcal{D}, \gamma\right) &amp; =\ell\left(p_{c} ; \mathcal{D}\right)+\gamma \operatorname{dist}\left(p_{c}, p_{d}\right) \
\tilde{p}<em c="c">{c} &amp; =\underset{p</em>, \gamma\right)
\end{aligned}
$$} \in \mathbb{R}^{L \times d}}{\arg \min } \ell^{\prime}\left(p_{c} ; \mathcal{D</p>
<p>where $p_{c}$ is the only learnable parameter, and $\gamma$ is a hyperparameter.</p>
<p>When $\gamma=0$, the modified objective is reduced to the standard objective (Eq.4), $\ell^{\prime}($.$) = \ell($.$) . We$ refer to this case and its resulting prompt $p_{c}^{*}$ as the 'unconstrained' setting. A large value of $\gamma$ will make $p_{c}$ even closer (possibly identical) to $c-\operatorname{proj}\left(p_{d}\right)$ but lead to poor accuracy on a target dataset. Most of the experiments below are conducted via a range of $\gamma$ values to better understand the trade off between the two terms in the objective function. In practice, we find $\gamma=0.01$ to give a reasonable trade-off regardless of the target dataset and the choice of $p_{d}$.</p>
<p>There are at least two natural ways to define the distance measure $\operatorname{dist}\left(p_{c}, p_{d}\right)$ between a continuous prompt $p_{c}$ and a discrete target prompt $p_{d}$, by converting one so that both are in the same space:</p>
<p>$$
\begin{aligned}
&amp; c-\operatorname{dist}\left(p_{c}, p_{d}\right)=\frac{\left|p_{c}-c-\operatorname{proj}\left(p_{d}\right)\right|<em c="c">{2}^{2}}{L} \
&amp; d-\operatorname{dist}\left(p</em>\right)
\end{aligned}
$$}, p_{d}\right)=\mathrm{F} 1\left(d-\operatorname{proj}\left(p_{c}\right), p_{d</p>
<p>The first of these places both $p_{c}$ and $p_{d}$ in the continuous space and computes the squared- $\mathrm{L}^{2}$ norm, normalized by the prompt length. This is used in our training loss (Eq.5) implementation. The second places both in discrete space (text) and computes the standard word-level token overlap F1 score. ${ }^{4}$ This is used during our evaluation.</p>
<h2>4 Empirical Support of Waywardness</h2>
<p>We empirically investigate the Prompt Waywardness hypothesis (§3.2) using our modification</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(§3.3) of the prompt tuning method from Lester et al. (2021). We show that given an arbitrary and irrelevant discrete prompt $p_{d}$, it is possible to learn a continuous prompt that is mapped to $p_{d}$ while retaining its accuracy on a given dataset. ${ }^{5}$</p>
<h3>4.1 Setup</h3>
<p>Target tasks. Following the setup of Min et al. (2022), we select a diverse set of 5 classification datasets: SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), AGNews (Zhang et al., 2015), Subj (Pang and Lee, 2004) and TREC (Voorhees and Tice, 2000). Statistics and the unconstrained accuracy of each dataset are provided in Table 1.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Dataset</td>
<td>Task</td>
<td>$|C|$</td>
<td>Acc</td>
</tr>
<tr>
<td>SST-2</td>
<td>Sentiment analysis (movie)</td>
<td>2</td>
<td>92.4</td>
</tr>
<tr>
<td>SST-5</td>
<td>Sentiment analysis (movie)</td>
<td>5</td>
<td>50.3</td>
</tr>
<tr>
<td>AGNews</td>
<td>News classification (topic)</td>
<td>4</td>
<td>88.1</td>
</tr>
<tr>
<td>Subj</td>
<td>Subjectivity classification</td>
<td>2</td>
<td>90.5</td>
</tr>
<tr>
<td>TREC</td>
<td>Answer type classification</td>
<td>6</td>
<td>88.0</td>
</tr>
</tbody>
</table>
<p>Table 1: The collection of downstream tasks used in the experiments (§4.1). $|C|$ indicates the output size (number of classes); Acc indicates the unconstrained accuracy of a prompt tuning method (Lester et al., 2021) using GPT2 Large, as a reference point.</p>
<p>Discrete Target Projections. We compile two sets of discrete target prompts: (1) 32 target prompts for solving tasks from NaturalInstructions ${ }^{6}$ dataset (Mishra et al., 2022b) that are distinct from and intentionally orthogonal to the end tasks considered here. These were chosen by excluding discrete target prompts that have high lexical overlap with other discrete prompts; this is because we found lexically similar prompts are often semantically similar even when written for different subtasks. (2) 30 random sentences from PILE, ${ }^{7}$ a large-scale, diverse text corpus used to pretrain GPT-J, the largest public causal language model (Wang and Komatsuzaki, 2021). The sampled sentences were drawn from a Poisson distribution with $\lambda=14$, which makes the average length of the sentence to be consistent to those in NaturalInstructions. These sentences are selected to have little or no token overlap with the true definition of the target tasks. See Table 3 for a few examples.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{4}$ Ignoring punctuation marks and articles, and applying lemmatization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">$p_{d}$</th>
<th style="text-align: center;">Task Accuracy (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">$\hat{\Delta}\left(\operatorname{Ace}\left(p_{c}^{*}\right) \rightarrow \operatorname{Ace}\left(\hat{p}_{c}\right)\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1 (\%)</td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">NI</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$(92.4 \rightarrow 91.8)$</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PILE</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$(92.5 \rightarrow 92.0)$</td>
<td style="text-align: center;">97.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">$(92.4 \rightarrow 91.9)$</td>
<td style="text-align: center;">98.1</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">NI</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">$(50.2 \rightarrow 48.5)$</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PILE</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">$(50.5 \rightarrow 50.2)$</td>
<td style="text-align: center;">92.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">$(50.3 \rightarrow 49.3)$</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">NI</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">$(88.0 \rightarrow 86.6)$</td>
<td style="text-align: center;">97.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PILE</td>
<td style="text-align: center;">-0.1</td>
<td style="text-align: center;">$(88.1 \rightarrow 88.2)$</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">$(88.1 \rightarrow 87.3)$</td>
<td style="text-align: center;">97.4</td>
</tr>
<tr>
<td style="text-align: center;">Subj</td>
<td style="text-align: center;">NI</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">$(91.3 \rightarrow 89.5)$</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PILE</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">$(89.6 \rightarrow 88.8)$</td>
<td style="text-align: center;">94.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">$(90.5 \rightarrow 89.2)$</td>
<td style="text-align: center;">95.9</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">NI</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">$(87.5 \rightarrow 84.7)$</td>
<td style="text-align: center;">86.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">PILE</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">$(88.5 \rightarrow 87.5)$</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Avg</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">$(88.0 \rightarrow 86.0)$</td>
<td style="text-align: center;">86.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Main Results: Accuracy of solving five classification datasets, in an unconstrained setting $\left(p_{c}^{*}\right)$ vs. constrained by the projection to various irrelevant pieces of text $\left(\tilde{p}_{c}\right)$. Optimization is done using $\gamma=0.01$ in the objective function (Eq.5). $\hat{\Delta}$ indicates the relative accuracy drop (in \%) from unconstrained accuracy. Each reported score (Accuracy and Prompt F1) are the average over 62 discrete target prompts and 3 random seeds. Overall, it is possible to achieve $\geq 94 \%$ prompt F1 with under $2 \%$ drop in accuracy.</p>
<p>Evaluation metrics. For all experiments, we report two metrics: (1) the task accuracy ${ }^{8}$ as well as (2) prompt F1, the word-level token overlap F1 score computed as in Eq.8, since it easy to interpret and is commonly used for evaluating the textual output of models (Rajpurkar et al., 2016).</p>
<p>Models. For evaluation, we use GPT2 (Radford et al., 2019) an auto-regressive LM which has extensively been used in many NLP applications. Unless otherwise specified, we use a 'large' variant consisting of 774 M parameters.</p>
<p>Implementation details. We use a batch size of 8 , learning rate 0.01 , and 2000 training steps. When experimenting with a discrete target prompt $p_{d}$, we initialize the search for continuous prompts (both $\tilde{p}<em c="c">{c}$ and $p</em>$ For all experiments, report accuracy averaged over three random seeds.}^{*}$ ) using $c-\operatorname{proj}\left(p_{d}\right) .{ }^{9</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>4.2 Main Results</h3>
<p>For each of the 5 tasks $T$ and for each of the 62 discrete target prompts $p_{d}$, we use the objective in Eq. 5 to find a prompt $\tilde{p}<em d="d">{c}$ such that it solves $T$ with a high accuracy while, at the same time, having a discrete projection that is close to $p</em>^{}$. For comparison, we also train unconstrained prompts $p_{c<em>}(\gamma=0.0)$ which solve task $T$ without any projection constraint. To ensure a fair comparison between $\tilde{p}<em c="c">{c}$ and $p</em>^{</em>}$, we ensure that they have the same size $L$. In other words, for each $\tilde{p}<em d="d">{c}$ (that has the same length as $p</em>^{}$ ), we train another $p_{c<em>}$ with the same length. We denote the relative accuracy drop from $p_{c}^{</em>}$ to $\tilde{p}_{c}$ as $\hat{\Delta}$.</p>
<p>Table 2 summarizes the results. Across all datasets, we find that it is possible to learn a continuous prompt $p_{c}$ whose discrete projection is very close to $p_{d}$ and mostly retains the task accuracy. There is a trade-off between the task accuracy and prompt F1, which can be controlled by the choice of $\gamma$ (more extensive ablations in the forthcoming paragraphs (\$4.3)). Overall, with $\gamma=0.01$, it is possible to achieve $\geq 94 \%$ prompt F1 with under $2 \%$ relative drop in task accuracy. The only outlier is the TREC dataset where we achieved a prompt F1 score of $86 \%$ for a $\hat{\Delta}=2.3 \%$ relative drop in accuracy. This might be due to the difficulty of learning effective prompts on TREC (also discussed by Min et al. (2022)).</p>
<p>Example prompts with varying values of prompt F1 scores are shown in Table 3. A prompt F1 $\geq$ $94 \%$ generally indicates one word mismatch with almost no semantically meaningful difference.</p>
<h3>4.3 Further Analysis</h3>
<p>Effect of Gamma. Fig. 3 shows the trade-off between task accuracy and the prompt F1 when varying $\gamma$ from 0 to 0.03 . As $\gamma$ increases, the task accuracy goes down while the prompt F1 increases. The drop in task accuracy is relatively minor-it is possible to learn a continuous prompt for which prompt F1 is near 1.00 and the accuracy drop relative to the unconstrained accuracy is less than $1 \%$.</p>
<p>Effect of Prompt Length ( $L$ ). We randomly sample sentences from The PILE with a constraint that its length must be $L$ (chosen from ${4,7,14,28,56})$. The left and the middle parts of Fig. 4 illustrate the results. We find that when $L$ is very small (e.g., 4) it is relatively difficult to learn a continuous prompt $p_{c}$ that is close to $p_{d}(\mathrm{~F} 1&lt;60 \%)$ while retaining the task accuracy. This is likely be-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$d$-proj $\left(p_{v}\right)$</th>
<th style="text-align: center;">Prompt F1</th>
<th style="text-align: center;">$\operatorname{Acc}\left(\hat{p}_{v}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Task: AGNews $p_{d}$ : Write down the conclusion you can reach by combining the given Fact 1 and Fact 2.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Write down the conclusion you can reach by combining the given Fact 1 and Fact 2.</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">89.2</td>
</tr>
<tr>
<td style="text-align: left;">Write down the conclusion you can reach by combining the given Fact 1. Fact 2.</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: left;">Write down the conclusion you can reach by combining the given Fact 1 Category Fact 2.</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">89.0</td>
</tr>
<tr>
<td style="text-align: left;">Write Messi in conclusion you can reach by combining the given Fact 1 and Fact 2.</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">88.8</td>
</tr>
<tr>
<td style="text-align: left;">Task: SST-5 $p_{d}$ : "If they have other interests and aims in life it does not necessarily follow that they are passive sheep."</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">"If they have other interests and aims in life it does not necessarily follow that they are passive sheep."</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">51.2</td>
</tr>
<tr>
<td style="text-align: left;">"If they have other interests and aims in life it does not necessarily follow that they are terrible sheep."</td>
<td style="text-align: center;">94.7</td>
<td style="text-align: center;">53.6</td>
</tr>
<tr>
<td style="text-align: left;">"If they have other interests and aims in life it does not necessarily follow that they are terrible GoPro."</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">52.3</td>
</tr>
<tr>
<td style="text-align: left;">Task: SST-5 $p_{d}$ : int clamp(int val, int min_val, int max_val) { return std::max(min_val, std::min(max_val, val)); }</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">int clamp(int val, int min_val, int max_val) { return std::max(min_val, std::min(max_val, val)); }</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">50.5</td>
</tr>
<tr>
<td style="text-align: left;">int clamp(int val, int min_val, int max_val) { return std::max(min_val, std::min(max_val terrible val)); }</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">52.0</td>
</tr>
<tr>
<td style="text-align: left;">int clamp(int val, int min_val, int max_val) { return std::max(min_val, std::min(max_val terrible val)); This}</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">53.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of the target prompts $p_{d}$ and their reconstructions via $d$-proj $\left(p_{v}\right)$ for different ranges of prompt F1 scores. The first $p_{d}$ is from Natural-Instructions; the rest two are sampled from The PILE. The mismatches with the original prompt are color-coded.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The effect of $\gamma$ on SST-2 and AGNews. Accuracy is the average over 32 discrete target prompts from Natural Instructions and 3 random seeds. A dotted line indicates unconstrained accuracy $p_{c}^{*}$ (same as when $\gamma=0$ ). Numbers inside parentheses in the y-axis indicate relative drop in accuracy against unconstrained accuracy. There is a clear trade-off between the task accuracy and the prompt F1.
cause the prompt being too short significantly hurts the expressivity of the prompt. Nonetheless, when $L$ is reasonably larger, e.g., 14 (the average length of in Natural Instructions) or longer, all cases lead to a continuous prompt with near 1.0 prompt F1 and little accuracy drop.</p>
<p>Effect of Model Size. We vary the size of the GPT2 models-small, medium, large, and XLwith $124 \mathrm{M}, 355 \mathrm{M}, 774 \mathrm{M}$, and 1.5 B parameters, respectively. Figure 5 (right) reports the result on SST-2. We find that (1) across different sizes of the LM, our findings in learning continuous prompts with the prompt F1 of near 1.0 and little drop in the accuracy generally hold, and (2) in particular, the drop in accuracy is more negligible with larger LMs ( $0.2 \%$ with XL, $0.5-0.7 \%$ with medium and large, $1.2 \%$ with small).</p>
<p>Projection onto true task definitions. In all our results so far in $\S 4$, the target projected text was</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">Task Accuracy (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Prompt <br> F1 (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\hat{\Delta}<em v="v">{T}\left(\operatorname{Acc}\left(p</em>\right)\right)$}^{*}\right) \rightarrow \operatorname{Acc}\left(\hat{p}_{v</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">SST-2</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">$(91.9 \rightarrow 90.9)$</td>
<td style="text-align: center;">98.5</td>
</tr>
<tr>
<td style="text-align: center;">SST-5</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">$(51.4 \rightarrow 50.5)$</td>
<td style="text-align: center;">96.1</td>
</tr>
<tr>
<td style="text-align: center;">AGNews</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">$(91.8 \rightarrow 90.4)$</td>
<td style="text-align: center;">95.7</td>
</tr>
<tr>
<td style="text-align: center;">Subj</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">$(89.8 \rightarrow 85.6)$</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">TREC</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$(88.6 \rightarrow 88.1)$</td>
<td style="text-align: center;">99.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Accuracy of solving five classification datasets, unconstrained setting $\left(p_{v}^{*}\right)$ vs. constrained by the projection to the true definition of tasks $\left(\hat{p}<em T="T">{v}\right)$ using $\gamma=0.01$ in the objective function (Eq.5). Subscript $T$ in $\Delta</em>$ denotes this being the case for true task definitions. Projecting to the true definition of a task does not help continuous prompts solve a task.
orthogonal to the tasks being solved. One might naturally wonder whether there is any benefit in projecting continuous prompts to the texts that truly describe the task being solved, i.e., a "true" prompt for the task. To this end, we manually authored 5</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The effect of the length of the prompt (L) on AGNews. Each point computed via the average over 32 discrete target prompts from Natural Instructions and 3 random seeds (γ = 0.01 used). The corresponding prompt F1 is reported as a orange line. The accuracy of $p^s_c$ and $\tilde{p}_c$ increase as a function of prompt length, however, the gap between them tends to decrease. The relative accuracy drop is marginal when L is not too small (e.g., 7 or larger).</p>
<p>"true" prompts for each of the tasks. We then follow the exact same setup used earlier for Table 2 to fine-tune continuous prompts $\tilde{p}_c$ for the task while projecting onto these true task definitions. As before, we fine-tune unconstrained prompts $p^s_c$ of the same length, without any projection requirement.</p>
<p>By design, $\tilde{p}_c$ can be no more effective at solving the task than the unconstrained prompt $p^s_c$ (barring suboptimal search issues), which is what we find in practice. For completeness, we report detailed results for "true" target prompts (analogous to Table 2) in Table 4.</p>
<p>More interestingly, as shown in Table 5, continuous prompts that project to "true" target prompts are no more effective at solving the task than continuous prompts that project to the 62 irrelevant target prompts considered earlier (Table 2). Specifically, the average performance gap $\Delta$ (relative to unconstrained prompts of the same length) is about the same (≈ 1.5%) for continuous prompts that map to true task definitions compared to prompts that map to irrelevant text. This further bolsters the waywardness hypothesis—continuous prompts don't relate to the task being solved.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The effect of the size of the model—small, medium, large, and XL—on SST-2. Each point in the experiment is computed by averaging over 30 experiments each with a different discrete target prompt from PILE and 3 random seeds. We vary $\gamma = {0.01, 0.005, 0.003}$ and choose the one for which prompt F1 is larger than 0.98. The relative accuracy drop (gap between the two trends) decreases as models become larger.</p>
<table>
<thead>
<tr>
<th></th>
<th>SST-2</th>
<th>SST-5</th>
<th>AGNews</th>
<th>Subj</th>
<th>TREC</th>
<th>Avg</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\hat{\Delta}_T$</td>
<td>1.0</td>
<td>0.9</td>
<td>1.4</td>
<td>4.1</td>
<td>0.5</td>
<td>1.6</td>
</tr>
<tr>
<td>$\hat{\Delta}$</td>
<td>0.6</td>
<td>2.0</td>
<td>0.8</td>
<td>1.5</td>
<td>2.3</td>
<td>1.4</td>
</tr>
</tbody>
</table>
<p>Table 5: Task accuracy gap comparison between unconstrained prompts and those fine-tuned to project to a true task definition ($\hat{\Delta}_T$) as reported in Table 4. For comparison, we also show the corresponding performance gaps with irrelevant ($\hat{\Delta}$) from Table 2. The average performance gaps are about the same (around 1.5) for true and irrelevant target prompts—further evidence that continuous prompts don't relate to the task being solved.</p>
<h2>5 Explaining Waywardness</h2>
<p>Here we provide intuitions behind the factors that enable Prompt Waywardness.</p>
<p>The mapping between continuous and discrete spaces is not one-to-one. While a discrete target prompt is mapped to exactly one continuous prompt (via its embedding, Eq.1; cf. Fig.2), the reverse is not true. In fact, except for a very small fraction of unnatural or degenerate edge cases, for every target discrete prompt, there are infinitely many continuous prompts that project back to it (via Eq.2). While simple counting-based arguments are insufficient in continuous spaces, we formally prove (Appendix B) that this property holds for all nearest-neighbor projections under any metric distance, and broadly for all but a negligible (measure zero) portion of possible projection operators.</p>
<p>^{10}Such as using a non-metric distance in nearest-neighbor mapping, or mapping all of $\mathbb{R}^d$ to a single discrete prompt.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The projection discrete space (Eq.2) induces a clustering (a Voronoi diagram) of the continuous space. Each cluster has infinitely many points that get mapped to the same discrete token.</p>
<p>This intuitively suggests that there is a whole region of continuous prompts that corresponds to a fixed discrete representation (Fig.6). The remaining question is, how is this region able to have a diverse set of prompts that can solve a variety of tasks? This is addressed next.</p>
<h3>Deep models give immense expressive power to earlier layers.</h3>
<p>The deeper a network is, the more expressivity it has with respect to its inputs (Telgarsky, 2016; Raghu et al., 2017). Since continuous prompts reside just before the first layer, they enjoy a lot of expressivity. Therefore, no matter how narrow the regions corresponding to individual tokens are (Fig.6), they are extremely powerful in solving a variety of tasks. Previously in §4.2 we provide an empirical analysis showing evidence that the effect of Waywardness is stronger in deeper models.</p>
<h2>6 Implications of Prompt Waywardness</h2>
<p>We discuss the implications of these findings on several inter-related lines of research. Note that all the following statements are valid within the boundaries of the existing architectures. Moving beyond these barriers likely requires major innovations in terms of LM architectures or how continuous prompts are optimized.</p>
<h3>Faithful interpretation of continuous prompts is difficult.</h3>
<p>Given the intuitions behind and empirical support for the Waywardness hypothesis (§5), faithful discrete interpretations of continuous prompts via common discrete projections (like nearest-neighbor projection) are unlikely to be robust based on current approaches. It is an open question whether there is a better way of interpreting continuous prompts with human language, or whether explaining and interpreting continuous prompts via human language is inherently impossible because they lie in completely different spaces. Future work may investigate more on this topic in order to improve the interpretability of prompt-based language models.</p>
<h3>Risk of interpreting continuous prompts: concealed adversarial attacks.</h3>
<p>It is not difficult to imagine a future where proprietary model development is driven by fine-tuned continuous prompts. In such a world, not addressing the challenges involved in discrete interpretation of continuous prompts can lead to harmful (and potentially, adversarial) consequences (Slack et al., 2020; Wallace et al., 2021), as discussed below.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Waywardness implies that continuous prompts can be mapped to seemingly innocuous descriptions while acting maliciously.</p>
<p>We consider the following scenario: a model designer comes up with a set of continuous prompts that solve a target task (e.g., ranking resumes according to each applicant's qualifications and merits). Whether intentionally or not, such prompts may maliciously target, for example, a minority group. To assure their customers, the model designer uses the projection of the prompt that expresses a benign definition for the task, which does not reveal the true nature of the egregious behavior. The customers might even evaluate the prompt on a few instances but not notice this harmful behavior, e.g., when it effects a minority group not in the evaluation set. In a way, the benign discrete projections may provide a false sense of security.</p>
<h3>Optimizing discrete prompts through continuous prompts can be degenerate.</h3>
<p>Manually-written discrete prompts have many nice properties (Schick and Schütze, 2021; Mishra et al., 2022a), yet we do not have an efficient algorithmic way of finding them. One way to operationalize this is to formulate differentiable objective functions via LMs like GPT (Radford et al., 2019). Consider the following problem which is defined in the space</p>
<p>of continuous embeddings $p_{c} \in \mathbb{R}^{d}$ :</p>
<p>$$
\max <em c="c">{p</em>
$$} \in \mathbb{R}^{d}} \overbrace{\mathbb{P}\left(\mathcal{D} \mid p_{c}\right)}^{\text {utility }} \times \overbrace{\mathbb{P}\left(d \text {-proj }\left(p_{c}\right)\right)}^{\text {readability }</p>
<p>This a joint optimization towards a utility objective (the extent to which it can solve dataset $\mathcal{D}$ ) and a human readability objective. According to the Waywardness hypothesis, there are $p_{c}$ 's that assign high mass to the utility term while also mapping to human interpretable text that is irrelevant (or even contradictory) to the task solved by the prompt hence, degenerate solutions.</p>
<p>The same challenge holds if this optimization objective, instead of continuous prompts, is reformulated in terms of word probabilities (e.g., similar to Kumar et al. (2021, Sec 2.2)). This is the case, since searching in the space of word probabilities is analogous to a search in embedding spaces. ${ }^{11}$</p>
<p>In summary, Waywardness presents a challenge for searching effective discrete prompts via continuous optimization. The recent works have used additional signals such as domain-specific constraints (Qin et al., 2020; Khot et al., 2021; Qin et al., 2022) to alleviate these challenges. We hope to see more design innovations in this direction.</p>
<p>Gradients alone are insufficient to reverse engineer a model. Suppose we are given a fixed (fine-tuned or otherwise) model $M$ (e.g., an open question-answering model) and an expected output $y$ from this model (e.g., $y=$ "Joe Biden"). Can we use gradients with respect to an LM's output to find a semantically meaningful input that makes a frozen model $M$ generate a particular output?</p>
<p>Our findings and the earlier argument about continuous differentiable optimization suggests this may not be feasible with current methods. To see the correspondence to Prompt Waywardness, we can replace $\mathcal{D}$ in Eq. 9 with the desired outcome $y$ and run the optimization over word distributions (cf. Footnote 11). While gradients can guide towards some input that makes $M$ produce $y$, their interpretation is likely unfaithful to the task being solved by $M$. In the context of the above example ( $M$ being a QA system), gradients might lead to inputs maximize the probability assigned to "Joe Biden", although this input will likely be neither fluent nor semantically descriptive of "Joe Biden".</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Nevertheless, as noted earlier, gradients are still useful when they are applied using domain-specific constraints. For example, one can find local (wordlevel) perturbations that lead to a certain adversarial outcome, if the perturbations are restricted to welldefined semantic categories (e.g., "blue" can be perturbed to any other color name) (Sha, 2020; Guo et al., 2021; Yuan et al., 2021).</p>
<p>Continuous prompt tuning does not necessitate task-specific initialization. Recent works on continuous prompt-tuning have shown the effectiveness of initialization from embeddings of random common words (Lester et al., 2021; Min et al., 2022), despite these words being irrelevant to the task solved by these prompts. This, however, makes sense given the observations made in this work regarding the existence of effective prompts around word embeddings.</p>
<h2>7 Conclusion</h2>
<p>The prompting literature has seen many parallel developments around continuous and discrete prompts, as efficient alternatives to fine-tuning models with tens of millions of parameters. Our work introduced the Prompt Waywardness hypothesis, which expresses a surprising disconnect between continuous and discrete prompts: given a downstream task, for any discrete target prompt $p_{d}$, there exists a continuous prompt that projects to $p_{d}$ while achieving strong performance on the task. We provided empirical evidence for this hypothesis, studied various parameters around it, and ended with several implications of this hypothesis.</p>
<p>While our experiments are done on the GPT family, we expect our findings to apply to a broader set of architectures that, in one way or another, use similar mechanisms for mapping discrete elements to continuous representations and vice versa. Similarly, while our projection to the discrete space (Eq.2) is a popular operator in the field (cf. Footnote 1), the intuition explained in Propositions 1 and 2 of the Appendix suggests similar behavior for a broad class of projection operators.</p>
<p>Prompt Waywardness identifies challenges for future progress on algorithmic methods for the discovery of human readable prompts that are faithful to the task they solve. We hope the observations made in this work motivate architectural innovations that overcome such challenges and guide future steps in the prompting literature.</p>
<h2>Acknowledgment</h2>
<p>The authors are thankful to Lisa Li, Nicholas Lourie, and Vered Shwartz for helpful discussions, and the Beaker team at AI2 for their support with the experiments. This work was supported in part by DARPA MCS program through NIWC Pacific (N66001-19-2-4031), DARPA SemaFor program, and Google Cloud Compute.</p>
<h2>References</h2>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.</p>
<p>Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation. In Proceedings of ICLR.</p>
<p>Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Proceedings of ACL.</p>
<p>Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe Kiela. 2021. Gradient-based adversarial attacks against text transformers. In Proceedings of EMNLP.</p>
<p>Tatsunori B Hashimoto, David Alvarez-Melis, and Tommi S Jaakkola. 2016. Word embeddings as metric recovery in semantic spaces. TACL, 4:273-286.</p>
<p>Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? TACL, 8:423-438.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text Modular Networks: Learning to decompose tasks in the language of existing models. Proceedings of NAACL, page 1264-1279.</p>
<p>Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. 2021. Controlled text generation as continuous optimization with multiple constraints. Proceedings of NeurIPS, 34.</p>
<p>Teven Le Scao and Alexander M Rush. 2021. How many data points is a prompt worth? In Proceedings of NAACL, pages 2627-2636.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of EMNLP.</p>
<p>Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of ACL.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of NeurIPS, pages 3111-3119.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Noisy channel language model prompting for few-shot text classification. In Proceedings of ACL.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing instructional prompts to GPTk's language. In Proceedings of ACL - Findings.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022b. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of ACL.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL.</p>
<p>Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowledge bases? In Proceedings of EMNLP.</p>
<p>Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In Proceedings of NAACL.</p>
<p>Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D Hwang, Ronan Le Bras, Antoine Bosselut, and Yejin Choi. 2020. Backpropagationbased decoding for unsupervised counterfactual and abductive reasoning. In Proceedings of EMNLP.</p>
<p>Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. 2022. COLD decoding: Energy-based constrained text generation with Langevin dynamics. arXiv preprint arXiv:2202.11705.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR, 21:1-67.</p>
<p>Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. 2017. On the expressive power of deep neural networks. In Proceedings of ICML, pages 2847-2854.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP, pages 2383-2392.</p>
<p>Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Proceedings of CHI.</p>
<p>Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of EACL, pages 255-269.</p>
<p>Lei Sha. 2020. Gradient-guided unsupervised lexically constrained text generation. In Proceedings of EMNLP, pages 8692-8703.</p>
<p>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. 2020. Eliciting knowledge from language models using automatically generated prompts. In Proceedings of EMNLP.</p>
<p>Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pages 180-186.</p>
<p>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP.</p>
<p>Matus Telgarsky. 2016. Benefits of depth in neural networks. In Proceedings of COLT, pages 1517-1539.</p>
<p>Ellen M Voorhees and Dawn M Tice. 2000. Building a question answering test collection. In Proceedings of SIGIR.</p>
<p>Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Concealed data poisoning attacks on NLP models. In Proceedings of NAACL.</p>
<p>Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax.</p>
<p>Lifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei. 2021. Bridge the gap between CV and NLP! a gradient-based textual adversarial attack framework. arXiv preprint arXiv:2110.15317.</p>
<p>Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Proceedings of NeurIPS.</p>
<p>Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]: Learning vs. learning to recall. In Proceedings of NAACL, pages 50175033 .</p>
<p>Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. 2021. Learning to prompt for visionlanguage models. arXiv preprint arXiv:2109.01134.</p>
<h2>Supplementary Material</h2>
<h2>A Additional Experimental Details</h2>
<p>Here we include several experimental details (§4) that did not fit in the main text. For the experiments we used A100 GPUs with 40G memory. In terms of the time GPU time of the experiments, each round of training and inference for each seed took about around 6 min . Therefore, the total GPU hours for our main experiment (Table 2) adds up to 93 hours ( $6 \mathrm{mins} \times 3$ seeds $\times 5$ datasets $\times 62$ prompts $=$ 5580 mins).</p>
<h2>B The mapping between continuous and discrete space is not one-to-one</h2>
<p>As argued in $\S 5$, the mapping between the space of discrete input and that of word embeddings (Fig.2) is not a bijection. While a discrete target prompt is mapped to exactly one continuous prompt (via its embedding, Eq.1), the reverse is not true: except for some unnatural or rare cases (as formalized in the following propositions) there are infinitely many continuous prompts that project back to a fixed discrete target prompt (via Eq.2).</p>
<p>Nearest-neighbor projections are arguably natural, computationally efficient, and useful in practice. Although we have considered them in the Euclidean space so far, they can be defined for an arbitrary distance metric ${ }^{12} m$ on $\mathbb{R}^{d}$. As before, consider an embedding of a lexicon of size $V$ into $\mathbb{R}^{d}$ and the corresponding one-hot vectors in ${0,1}^{V}$. We call $d$-proj a nearest-neighbor projection operator w.r.t. $m$ if it maps each $x \in \mathbb{R}^{d}$ to the one-hot vector in ${0,1}^{V}$ that corresponds to the lexicon item whose embedding is closest to $x$ under metric $m$ (breaking ties arbitrarily).
Proposition 1 Every nearest-neighbor projection operator, under any metric, maps infinitely many elements of $\mathbb{R}^{d}$, forming one or more continuous subspaces, to every one-hot vector in ${0,1}^{V}$.</p>
<p>A proof is included in Appendix B.1. In effect, the projection operators induce a clustering of the space of continuous prompts $\mathbb{R}^{d \times L}$ into regions that have the same discrete projection (Fig.6).</p>
<p>The infinite-to-one mapping aspect is not limited to the class of nearest-neighbor projection operators. It is rather an inherent property of the interaction between continuous and discrete spaces, and</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>holds for a broader family consisting of all but a negligible portion of possible projection operators:
Proposition 2 Let $\mathbb{D}$ denote the space of all projection operators that map $\mathbb{R}^{d}$ to one-hot vectors in ${0,1}^{V}$. Let d-proj be a random projection drawn uniformly from $\mathbb{D}$. Then, with probability 1, d-proj maps infinite elements of $\mathbb{R}^{d}$ to every one-hot vector in ${0,1}^{V}$.</p>
<h2>B. 1 Proofs</h2>
<p>Proof of Prop. 1: Let $c_{i} \in \mathbb{R}^{d}$ for $i \in$ ${1, \ldots, V}$ be fixed vectors (denoting the embedding of words in a lexicon of size $V$ ). Let $e_{i} \in{0,1}^{V}$ denote the one-hot vector with 1 in the $i$-th position and 0 elsewhere. Since $d$-proj is a nearest-neighbor projection operator w.r.t. $m$, by definition it maps $x \in \mathbb{R}^{d}$ to $e_{i}$ whenever $x$ is closest to $c_{i}$, i.e., $i=\arg \min <em j="j">{j} m\left(x, c</em>\right)$ (breaking ties arbitrarily).</p>
<p>Let $S_{i} \subseteq \mathbb{R}^{d}$ denote the pre-image of $e_{i}$, i.e., the elements that the nearest-neighbor projection $d$-proj maps to the $i$-th one-hot vector. By definition, $c_{i} \in S_{i}$. Let $d^{\prime}=\min <em i="i">{j} m\left(c</em>$,e.g. also has infinite elements in one or more continuous subspaces.}, e_{j}\right)&gt;0$ denote the distance of $c_{i}$ to the nearest $c_{j}$ w.r.t. the metric $m$. Consider the subspace $C_{i}=\left{x \mid m\left(x, c_{i}\right)&lt;\right.$ $\left.d^{\prime} / 2\right}$. By design, we have $C_{i} \subseteq S_{i}$. Further, moving $x$ by some small distance $\epsilon$ (w.r.t. $m$ ) to another point $x^{\prime}$ changes its distance to $c_{i}$ only by at most $\epsilon$ (by the triangle inequality property of $m$ ). This implies that if $\epsilon$ is chosen to be small enough such that $m\left(x, c_{i}\right)+\epsilon&lt;d^{\prime} / 2$, then $x^{\prime}$ must also be in $C_{i}$. In other words, if $x \in C_{i}$, then, for a small enough $\epsilon$, the entire $\epsilon$-neighborhood of $x$ is also in $C_{i}$. It follows that $C_{i}$ is an open subset of $R^{d}$ and thus contains infinitely many elements forming a continuous subspace. Hence $S_{i}$, which contains $C_{i</p>
<p>Proof of Prop. 2: For simplicity, assume $V=$ 2. A projection operator $d$-proj $\in \mathbb{D}$ can then be fully characterized by the subset $S \subseteq \mathrm{R}^{d}$ that it maps to any one arbitrarily chosen one-hot vector. Choosing $d$-proj uniformly at random from $\mathbb{D}$ thus amounts to choosing the subset $S$ uniformly at random from $\mathbb{R}^{d}$. We show that the probability of choosing an $S$ such that $|S|$ is finite, is 0 . (The same argument applies to $|\mathbb{R} \backslash S|$ being finite.)</p>
<p>To see this, let $\mathbb{S}<em 0="0">{i}$ denote the set of all (finite) subsets of $\mathbb{R}^{d}$ that have size exactly $i$. First, observe that the probability of choosing an $S$ that lies in $\mathbb{S}</em>$ that has at} \cup \mathbb{S}_{1}$ (i.e., a subset of $\mathbb{R}^{d</p>
<p>most 1 element) is 0 ; this is a degenerate case in the underlying continuous probability space. Second, for any $i \geq 2, \mathbb{S}<em 1="1">{i}$ has the same "size" (in the measure theoretic sense) as $\mathbb{S}</em>}$, because one can construct an injective map from either one to the other-which follows from the fact that they both have the same cardinality as the set $\mathbb{R} .{ }^{13}$ Lastly, the space $\mathbb{S}$ of all finite subsets of $\mathbb{R}^{d}$ is the countable union $\cup_{i} \mathbb{S<em i="i">{i}$ of disjoint sets. Therefore, $\operatorname{Pr}[S \in \mathbb{S}]=\sum</em>\right]=0$.} \operatorname{Pr}\left[S \in \mathbb{S}_{i</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ This can be proved using the rules of cardinal multiplication applied to $\mathbb{S}<em 1="1">{i}$ viewed as (a subset of) the Cartesian product of $\mathbb{S}</em>$ with itself, $i$ times.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{5}$ Scripts needed to reproduce our results: https:// github.com/Alrope123/prompt-waywardness ${ }^{6}$ https://instructions.apps.allenai.org ${ }^{7}$ https://pile.eleuther.ai&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
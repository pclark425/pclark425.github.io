<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8771 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8771</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8771</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-f11198dc277af1c5547ab3ae8d562fc8c16a7c70</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f11198dc277af1c5547ab3ae8d562fc8c16a7c70" target="_blank">Graph-to-Sequence Learning using Gated Graph Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a new model that encodes the full structural information contained in the graph, couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work.</p>
                <p><strong>Paper Abstract:</strong> Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8771.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8771.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>linearisation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Linearisation / Sequence Serialization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transforming a graph into a linear token sequence so that standard sequence-to-sequence models can be applied; commonly used in prior AMR-to-text work and treated as input sentences for RNN encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearisation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs (e.g., AMR) are converted into a single token sequence by traversing the graph and emitting node/edge labels and structural markers (often with additional scope markers and anonymisation). This produces a linearized string that is fed to a standard s2s encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Abstract Meaning Representation (AMR) graphs; general rooted DAGs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Heuristic traversal/serialization used in prior work (Konstas et al. 2017): nodes and relations are serialized into a bracketed token sequence with scope markers; reentrant nodes are typically copied in the linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Surface realization / text generation from AMR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In this paper, s2s baselines using linearised AMRs achieve (median single-model): BLEU 21.7, CHRF++ 49.1 (28.4M params). A baseline without scope marking: BLEU 18.4, CHRF++ 46.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to the paper's Levi-graph GGNN model (g2s), linearisation-based s2s underperforms: g2s single-model BLEU 23.3 / CHRF++ 50.4 and g2s ensemble BLEU 27.5 / CHRF++ 53.5. Linearisation is used in prior state-of-the-art (Konstas et al. 2017) but ignores full graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows use of off-the-shelf sequence models (RNN/CNN/transformer); simple to implement; benefits from standard sequence preprocessing (e.g., BPE).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Discards explicit graph topology and long-range structure; reentrant nodes are duplicated in the linear sequence (can cause overgeneration); requires designing scope markers/anonymisation heuristics; can propagate alignment errors when grammar-based.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Paper gives a concrete failure mode: overgeneration due to reentrancies (s2s linearised model repeated 'India and China' phrase) because reentrant nodes are copied in the linear form; more generally leads to loss of structural signals needed to avoid such errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8771.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8771.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Levi</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Levi Graph Transformation (edges-to-nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph transformation that turns original edges into explicit nodes (Levi graph), so the encoder produces hidden states for both original nodes and edges and edge labels are represented as embeddings rather than as label-specific GGNN parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Finite Geometrical Systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Levi graph transformation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace each edge in the original graph by a new node representing that edge and connect that new node to the two incident original nodes. Original edge labels become node labels/embeddings; transformed graph edges are unlabeled (only default/reverse/self labels are used for GGNN).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs and rooted DAGs (applied to AMR graphs and labelled dependency trees in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocessing step: for graph G=(V,E,Lv,Le) create V' = V ∪ E, treat original edge labels as node labels in V', create E' linking each original edge-node to its incident original nodes; then add reverse and self-loop edges with a small fixed label set (default, reverse, self).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation (AMR surface realization); syntax-aware NMT (source dependency trees to sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AMR generation: g2s (GGNN on Levi graphs) single-model BLEU 23.3 / CHRF++ 50.4 (28.3M params); ensemble BLEU 27.5 / CHRF++ 53.5 (141M params). NMT: g2s on dependency trees (no sequential edges) single-model En-De BLEU 15.2 CHRF++ 41.4; with sequential edges (g2s+) BLEU 16.7 CHRF++ 42.4. Reported differences vs baselines are statistically significant (p<0.05).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Directly compared to linearisation-based s2s baselines (Konstas et al. style) and to GCN-based approaches: Levi+GGNN outperforms s2s linearisation baselines on AMR generation and, when augmented with sequential edges, outperforms s2s baselines on NMT. Paper argues that prior GCN approaches mitigate parameter explosion by grouping labels, whereas Levi transformation preserves labels as embeddings and yields instance-specific edge states.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Avoids parameter explosion in GGNNs (only a small label set for transformed edges); allows the encoder to generate instance-specific hidden representations for original edges (enabling attention over edges); preserves full graph topology; helps avoid reentrancy overgeneration in AMR generation; flexible — can add linguistic biases (e.g., sequential edges) without changing architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Transforms edge labels into the same token/vocabulary space as nodes, causing nodes and edges to share the same semantic embedding space (authors note this is not ideal). Also increases number of nodes (graph size) which may affect processing cost. Does not by itself solve fixed-layer-depth issue of GGNNs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When applied to dependency trees without adding sequential edges, the Levi-based GGNN (g2s) performed worse than s2s baselines on NMT (loss of sequential information); also, Levi transformation's shared vocabulary for nodes and edges is a conceptual limitation noted by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8771.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8771.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>g2s+</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequential Edge Augmentation (g2s+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting a dependency-tree Levi graph with explicit sequential (left/right) edges connecting contiguous words to reintroduce surface-order bias without adding an RNN encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequential-edge-augmented Levi graph (g2s+)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>After transforming a dependency tree into a Levi graph, add directed sequential edges between adjacent word nodes (with distinct left/right labels) so that GGNN message passing can propagate surface-order information.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Dependency trees derived from source sentences (syntax trees); rooted labelled trees</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Levi transform dependency tree into nodes+edge-nodes, then add extra labeled edges between word nodes representing original token order (left and right sequential labels); also keep reverse and self edges.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Neural Machine Translation (source syntax-aware encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>En-De single-model: g2s+ BLEU 16.7 / CHRF++ 42.4 vs s2s BLEU 15.5 / CHRF++ 40.8; ensembles: g2s+ BLEU 19.6 / CHRF++ 45.1 outperform s2s ensemble BLEU 19.0. En-Cs single-model: g2s+ BLEU 9.8 / CHRF++ 33.3 vs s2s BLEU 8.9 / CHRF++ 33.8; ensembles: g2s+ BLEU 11.7 vs s2s 11.3.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to g2s without sequential edges, g2s+ recovers and surpasses s2s baselines, demonstrating that adding sequential bias via explicit edges can replace the need for RNN encoders (contrasts with Bastings et al. 2017 who required a BiRNN layer to obtain best results).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reintroduces surface-order inductive bias while keeping a fully graph-based encoder; avoids adding RNN layers; achieves improved BLEU under comparable parameter budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires manual design of sequential-edge labels and increases graph connectivity; still relies on positional/sequential labeling choices and increases graph size.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Plain g2s (Levi graph without sequential edges) performed worse than s2s baselines on NMT, indicating that without explicit sequential signals the graph encoder loses surface-order information important for translation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8771.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8771.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scope+anonym</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scope Markers and Anonymisation (AMR preprocessing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Preprocessing steps applied to AMR graphs before conversion/linearization or Levi transformation: anonymise entities and add scope markers to help s2s models learn alignments and produce surface tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>scope markers & anonymisation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Entity simplification and anonymisation replace named entities with placeholders; scope markers explicitly indicate nested scopes in linearized AMRs to help sequence models handle structural boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs (rooted DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply entity simplification/anonymisation heuristics, then (for s2s baselines) add explicit scope markers during linearization. In this paper the same preprocessing is applied before Levi transformation for g2s experiments (scope markers used for s2s baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports s2s baseline with scope marking: BLEU 21.7 / CHRF++ 49.1; without scope marking s2s (-s) BLEU drops to 18.4 / CHRF++ 46.3, showing scope marking substantially helps linearised s2s.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Scope markers/anonymisation are part of preprocessing used in Konstas et al. (2017) and were applied to the s2s baselines here; g2s models achieved good performance without relying on the scoping heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Improves s2s baseline performance markedly; reduces sparsity by anonymising entities; helps linearized sequences encode structural boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Heuristic and external; adds preprocessing complexity; is not necessary for Levi+GGNN approach (which retains structural information natively).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Without scope marking, s2s performance degrades substantially (example numbers above).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8771.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8771.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HRG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hyperedge Replacement Grammars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Grammar-based approach that transduces graphs to sequences using learned grammar rules; historically used for AMR-to-text and semantic MT but requires node-to-token alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hyperedge Replacement Graph Grammars</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>hyperedge replacement grammar (HRG) transduction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Learn or extract graph-to-string transduction rules (hyperedge replacement rules) mapping subgraphs to token sequences; generation is performed by applying grammar rules to cover the input graph and emit surface tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>General graphs (used for semantic graphs such as AMR), hypergraphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Induce HRG rules from aligned graph-sentence pairs; requires alignments between graph nodes/edges and surface tokens; apply grammar-based transduction to produce an output sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR-to-text generation and semantics-based machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper references prior HRG-based AMR and MT work (e.g., Flanigan et al., Song et al., Jones et al.) but does not report HRG numeric results itself. Prior reported BLEU in literature vary; not directly comparable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Paper contrasts HRG/grammar-based approaches with neural methods: grammar-based methods require alignments which may be noisy and are a key limitation; neural graph encoders (GGNN + Levi) avoid the need for explicit node-to-token alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Explicit, interpretable transduction rules; can capture structured, compositional mappings between graph fragments and tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Needs node-to-token alignments (often not available gold-standard), so errors propagate into grammars; brittle to alignment/noise; less flexible than end-to-end neural encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Alignment errors in induced grammars lead to poor generation; authors cite the need for alignments as 'key limitation' motivating neural methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8771.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8771.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>label-grouping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Edge-label Grouping Heuristic (to limit GGNN parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic that reduces GGNN parameter explosion by grouping multiple edge labels into a single label class so that the number of label-specific parameter matrices is reduced.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>edge-label grouping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Aggregate many distinct edge labels into a smaller set of grouped labels so that GGNN or GCN architectures need fewer label-specific parameter matrices, trading label granularity for parameter efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Label-rich graphs such as AMR predicate-labeled edges</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Heuristically map original edge label vocabulary (hundreds) into a small set (e.g., default/reverse/self or grouped classes) before model training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used in prior work for graph encoders in NLP (e.g., semantic role labeling, syntax-aware NMT, AMR processing) to limit model size</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper does not report numeric results for label-grouping itself, but notes prior GCN approaches employed it; Levi+GGNN approach avoids needing label grouping while matching or outperforming baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to Levi transformation: label-grouping reduces parameter counts but loses label-specific information; Levi encodes original labels as embeddings (as nodes) preserving label identity without parameter explosion.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces GGNN/GCN parameter count, enabling training when label vocabulary is large.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lossof label-specific information; heuristic grouping may hamper model performance; grouping choices are ad hoc.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors argue grouping can incur loss of information and is 'not an ideal solution' especially for rich label sets like AMR predicates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Graph-to-Sequence Learning using Gated Graph Neural Networks', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-Sequence Models for Parsing and Generation <em>(Rating: 2)</em></li>
                <li>Generating English from Abstract Meaning Representations <em>(Rating: 2)</em></li>
                <li>Generation from Abstract Meaning Representation using Tree Transducers <em>(Rating: 2)</em></li>
                <li>Graph Convolutional Encoders for Syntax-aware Neural Machine Translation <em>(Rating: 2)</em></li>
                <li>Hyperedge Replacement Graph Grammars <em>(Rating: 1)</em></li>
                <li>Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8771",
    "paper_id": "paper-f11198dc277af1c5547ab3ae8d562fc8c16a7c70",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "linearisation",
            "name_full": "Graph Linearisation / Sequence Serialization",
            "brief_description": "Transforming a graph into a linear token sequence so that standard sequence-to-sequence models can be applied; commonly used in prior AMR-to-text work and treated as input sentences for RNN encoders.",
            "citation_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
            "mention_or_use": "use",
            "representation_name": "linearisation",
            "representation_description": "Graphs (e.g., AMR) are converted into a single token sequence by traversing the graph and emitting node/edge labels and structural markers (often with additional scope markers and anonymisation). This produces a linearized string that is fed to a standard s2s encoder.",
            "graph_type": "Abstract Meaning Representation (AMR) graphs; general rooted DAGs",
            "conversion_method": "Heuristic traversal/serialization used in prior work (Konstas et al. 2017): nodes and relations are serialized into a bracketed token sequence with scope markers; reentrant nodes are typically copied in the linearization.",
            "downstream_task": "Surface realization / text generation from AMR",
            "performance_metrics": "In this paper, s2s baselines using linearised AMRs achieve (median single-model): BLEU 21.7, CHRF++ 49.1 (28.4M params). A baseline without scope marking: BLEU 18.4, CHRF++ 46.3.",
            "comparison_to_others": "Compared to the paper's Levi-graph GGNN model (g2s), linearisation-based s2s underperforms: g2s single-model BLEU 23.3 / CHRF++ 50.4 and g2s ensemble BLEU 27.5 / CHRF++ 53.5. Linearisation is used in prior state-of-the-art (Konstas et al. 2017) but ignores full graph structure.",
            "advantages": "Allows use of off-the-shelf sequence models (RNN/CNN/transformer); simple to implement; benefits from standard sequence preprocessing (e.g., BPE).",
            "disadvantages": "Discards explicit graph topology and long-range structure; reentrant nodes are duplicated in the linear sequence (can cause overgeneration); requires designing scope markers/anonymisation heuristics; can propagate alignment errors when grammar-based.",
            "failure_cases": "Paper gives a concrete failure mode: overgeneration due to reentrancies (s2s linearised model repeated 'India and China' phrase) because reentrant nodes are copied in the linear form; more generally leads to loss of structural signals needed to avoid such errors.",
            "uuid": "e8771.0",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Levi",
            "name_full": "Levi Graph Transformation (edges-to-nodes)",
            "brief_description": "A graph transformation that turns original edges into explicit nodes (Levi graph), so the encoder produces hidden states for both original nodes and edges and edge labels are represented as embeddings rather than as label-specific GGNN parameters.",
            "citation_title": "Finite Geometrical Systems",
            "mention_or_use": "use",
            "representation_name": "Levi graph transformation",
            "representation_description": "Replace each edge in the original graph by a new node representing that edge and connect that new node to the two incident original nodes. Original edge labels become node labels/embeddings; transformed graph edges are unlabeled (only default/reverse/self labels are used for GGNN).",
            "graph_type": "General graphs and rooted DAGs (applied to AMR graphs and labelled dependency trees in experiments)",
            "conversion_method": "Preprocessing step: for graph G=(V,E,Lv,Le) create V' = V ∪ E, treat original edge labels as node labels in V', create E' linking each original edge-node to its incident original nodes; then add reverse and self-loop edges with a small fixed label set (default, reverse, self).",
            "downstream_task": "AMR-to-text generation (AMR surface realization); syntax-aware NMT (source dependency trees to sentence)",
            "performance_metrics": "AMR generation: g2s (GGNN on Levi graphs) single-model BLEU 23.3 / CHRF++ 50.4 (28.3M params); ensemble BLEU 27.5 / CHRF++ 53.5 (141M params). NMT: g2s on dependency trees (no sequential edges) single-model En-De BLEU 15.2 CHRF++ 41.4; with sequential edges (g2s+) BLEU 16.7 CHRF++ 42.4. Reported differences vs baselines are statistically significant (p&lt;0.05).",
            "comparison_to_others": "Directly compared to linearisation-based s2s baselines (Konstas et al. style) and to GCN-based approaches: Levi+GGNN outperforms s2s linearisation baselines on AMR generation and, when augmented with sequential edges, outperforms s2s baselines on NMT. Paper argues that prior GCN approaches mitigate parameter explosion by grouping labels, whereas Levi transformation preserves labels as embeddings and yields instance-specific edge states.",
            "advantages": "Avoids parameter explosion in GGNNs (only a small label set for transformed edges); allows the encoder to generate instance-specific hidden representations for original edges (enabling attention over edges); preserves full graph topology; helps avoid reentrancy overgeneration in AMR generation; flexible — can add linguistic biases (e.g., sequential edges) without changing architecture.",
            "disadvantages": "Transforms edge labels into the same token/vocabulary space as nodes, causing nodes and edges to share the same semantic embedding space (authors note this is not ideal). Also increases number of nodes (graph size) which may affect processing cost. Does not by itself solve fixed-layer-depth issue of GGNNs.",
            "failure_cases": "When applied to dependency trees without adding sequential edges, the Levi-based GGNN (g2s) performed worse than s2s baselines on NMT (loss of sequential information); also, Levi transformation's shared vocabulary for nodes and edges is a conceptual limitation noted by the authors.",
            "uuid": "e8771.1",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "g2s+",
            "name_full": "Sequential Edge Augmentation (g2s+)",
            "brief_description": "Augmenting a dependency-tree Levi graph with explicit sequential (left/right) edges connecting contiguous words to reintroduce surface-order bias without adding an RNN encoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "sequential-edge-augmented Levi graph (g2s+)",
            "representation_description": "After transforming a dependency tree into a Levi graph, add directed sequential edges between adjacent word nodes (with distinct left/right labels) so that GGNN message passing can propagate surface-order information.",
            "graph_type": "Dependency trees derived from source sentences (syntax trees); rooted labelled trees",
            "conversion_method": "Levi transform dependency tree into nodes+edge-nodes, then add extra labeled edges between word nodes representing original token order (left and right sequential labels); also keep reverse and self edges.",
            "downstream_task": "Neural Machine Translation (source syntax-aware encoder)",
            "performance_metrics": "En-De single-model: g2s+ BLEU 16.7 / CHRF++ 42.4 vs s2s BLEU 15.5 / CHRF++ 40.8; ensembles: g2s+ BLEU 19.6 / CHRF++ 45.1 outperform s2s ensemble BLEU 19.0. En-Cs single-model: g2s+ BLEU 9.8 / CHRF++ 33.3 vs s2s BLEU 8.9 / CHRF++ 33.8; ensembles: g2s+ BLEU 11.7 vs s2s 11.3.",
            "comparison_to_others": "Compared to g2s without sequential edges, g2s+ recovers and surpasses s2s baselines, demonstrating that adding sequential bias via explicit edges can replace the need for RNN encoders (contrasts with Bastings et al. 2017 who required a BiRNN layer to obtain best results).",
            "advantages": "Reintroduces surface-order inductive bias while keeping a fully graph-based encoder; avoids adding RNN layers; achieves improved BLEU under comparable parameter budgets.",
            "disadvantages": "Requires manual design of sequential-edge labels and increases graph connectivity; still relies on positional/sequential labeling choices and increases graph size.",
            "failure_cases": "Plain g2s (Levi graph without sequential edges) performed worse than s2s baselines on NMT, indicating that without explicit sequential signals the graph encoder loses surface-order information important for translation.",
            "uuid": "e8771.2",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "scope+anonym",
            "name_full": "Scope Markers and Anonymisation (AMR preprocessing)",
            "brief_description": "Preprocessing steps applied to AMR graphs before conversion/linearization or Levi transformation: anonymise entities and add scope markers to help s2s models learn alignments and produce surface tokens.",
            "citation_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
            "mention_or_use": "use",
            "representation_name": "scope markers & anonymisation",
            "representation_description": "Entity simplification and anonymisation replace named entities with placeholders; scope markers explicitly indicate nested scopes in linearized AMRs to help sequence models handle structural boundaries.",
            "graph_type": "AMR graphs (rooted DAGs)",
            "conversion_method": "Apply entity simplification/anonymisation heuristics, then (for s2s baselines) add explicit scope markers during linearization. In this paper the same preprocessing is applied before Levi transformation for g2s experiments (scope markers used for s2s baselines).",
            "downstream_task": "AMR-to-text generation",
            "performance_metrics": "Paper reports s2s baseline with scope marking: BLEU 21.7 / CHRF++ 49.1; without scope marking s2s (-s) BLEU drops to 18.4 / CHRF++ 46.3, showing scope marking substantially helps linearised s2s.",
            "comparison_to_others": "Scope markers/anonymisation are part of preprocessing used in Konstas et al. (2017) and were applied to the s2s baselines here; g2s models achieved good performance without relying on the scoping heuristics.",
            "advantages": "Improves s2s baseline performance markedly; reduces sparsity by anonymising entities; helps linearized sequences encode structural boundaries.",
            "disadvantages": "Heuristic and external; adds preprocessing complexity; is not necessary for Levi+GGNN approach (which retains structural information natively).",
            "failure_cases": "Without scope marking, s2s performance degrades substantially (example numbers above).",
            "uuid": "e8771.3",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "HRG",
            "name_full": "Hyperedge Replacement Grammars",
            "brief_description": "Grammar-based approach that transduces graphs to sequences using learned grammar rules; historically used for AMR-to-text and semantic MT but requires node-to-token alignments.",
            "citation_title": "Hyperedge Replacement Graph Grammars",
            "mention_or_use": "mention",
            "representation_name": "hyperedge replacement grammar (HRG) transduction",
            "representation_description": "Learn or extract graph-to-string transduction rules (hyperedge replacement rules) mapping subgraphs to token sequences; generation is performed by applying grammar rules to cover the input graph and emit surface tokens.",
            "graph_type": "General graphs (used for semantic graphs such as AMR), hypergraphs",
            "conversion_method": "Induce HRG rules from aligned graph-sentence pairs; requires alignments between graph nodes/edges and surface tokens; apply grammar-based transduction to produce an output sequence.",
            "downstream_task": "AMR-to-text generation and semantics-based machine translation",
            "performance_metrics": "Paper references prior HRG-based AMR and MT work (e.g., Flanigan et al., Song et al., Jones et al.) but does not report HRG numeric results itself. Prior reported BLEU in literature vary; not directly comparable in this paper.",
            "comparison_to_others": "Paper contrasts HRG/grammar-based approaches with neural methods: grammar-based methods require alignments which may be noisy and are a key limitation; neural graph encoders (GGNN + Levi) avoid the need for explicit node-to-token alignments.",
            "advantages": "Explicit, interpretable transduction rules; can capture structured, compositional mappings between graph fragments and tokens.",
            "disadvantages": "Needs node-to-token alignments (often not available gold-standard), so errors propagate into grammars; brittle to alignment/noise; less flexible than end-to-end neural encoders.",
            "failure_cases": "Alignment errors in induced grammars lead to poor generation; authors cite the need for alignments as 'key limitation' motivating neural methods.",
            "uuid": "e8771.4",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "label-grouping",
            "name_full": "Edge-label Grouping Heuristic (to limit GGNN parameters)",
            "brief_description": "A heuristic that reduces GGNN parameter explosion by grouping multiple edge labels into a single label class so that the number of label-specific parameter matrices is reduced.",
            "citation_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
            "mention_or_use": "mention",
            "representation_name": "edge-label grouping",
            "representation_description": "Aggregate many distinct edge labels into a smaller set of grouped labels so that GGNN or GCN architectures need fewer label-specific parameter matrices, trading label granularity for parameter efficiency.",
            "graph_type": "Label-rich graphs such as AMR predicate-labeled edges",
            "conversion_method": "Heuristically map original edge label vocabulary (hundreds) into a small set (e.g., default/reverse/self or grouped classes) before model training.",
            "downstream_task": "Used in prior work for graph encoders in NLP (e.g., semantic role labeling, syntax-aware NMT, AMR processing) to limit model size",
            "performance_metrics": "This paper does not report numeric results for label-grouping itself, but notes prior GCN approaches employed it; Levi+GGNN approach avoids needing label grouping while matching or outperforming baselines.",
            "comparison_to_others": "Compared to Levi transformation: label-grouping reduces parameter counts but loses label-specific information; Levi encodes original labels as embeddings (as nodes) preserving label identity without parameter explosion.",
            "advantages": "Reduces GGNN/GCN parameter count, enabling training when label vocabulary is large.",
            "disadvantages": "Lossof label-specific information; heuristic grouping may hamper model performance; grouping choices are ad hoc.",
            "failure_cases": "Authors argue grouping can incur loss of information and is 'not an ideal solution' especially for rich label sets like AMR predicates.",
            "uuid": "e8771.5",
            "source_info": {
                "paper_title": "Graph-to-Sequence Learning using Gated Graph Neural Networks",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Generating English from Abstract Meaning Representations",
            "rating": 2,
            "sanitized_title": "generating_english_from_abstract_meaning_representations"
        },
        {
            "paper_title": "Generation from Abstract Meaning Representation using Tree Transducers",
            "rating": 2,
            "sanitized_title": "generation_from_abstract_meaning_representation_using_tree_transducers"
        },
        {
            "paper_title": "Graph Convolutional Encoders for Syntax-aware Neural Machine Translation",
            "rating": 2,
            "sanitized_title": "graph_convolutional_encoders_for_syntaxaware_neural_machine_translation"
        },
        {
            "paper_title": "Hyperedge Replacement Graph Grammars",
            "rating": 1,
            "sanitized_title": "hyperedge_replacement_graph_grammars"
        },
        {
            "paper_title": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling",
            "rating": 1,
            "sanitized_title": "encoding_sentences_with_graph_convolutional_networks_for_semantic_role_labeling"
        }
    ],
    "cost": 0.013752499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Graph-to-Sequence Learning using Gated Graph Neural Networks</h1>
<p>Daniel Beck ${ }^{\dagger}$ Gholamreza Haffari ${ }^{\ddagger}$ Trevor Cohn ${ }^{\dagger}$<br>${ }^{\dagger}$ School of Computing and Information Systems<br>University of Melbourne, Australia<br>{d.beck,t.cohn}@unimelb.edu.au<br>${ }^{\ddagger}$ Faculty of Information Technology<br>Monash University, Australia<br>gholamreza.haffari@monash.edu</p>
<h4>Abstract</h4>
<p>Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on this setting obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work, we propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results show that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.</p>
<h2>1 Introduction</h2>
<p>Graph structures are ubiquitous in representations of natural language. In particular, many wholesentence semantic frameworks employ directed acyclic graphs as the underlying formalism, while most tree-based syntactic representations can also be seen as graphs. A range of NLP applications can be framed as the process of transducing a graph structure into a sequence. For instance, language generation may involve realising a semantic graph into a surface form and syntactic machine translation involves transforming a tree-annotated source sentence to its translation.</p>
<p>Previous work in this setting rely on grammarbased approaches such as tree transducers (Flanigan et al., 2016) and hyperedge replacement gram-
mars (Jones et al., 2012). A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation (Pourdamghani et al., 2016) or neural sequence-to-sequence (henceforth, s2s) models (Konstas et al., 2017). Such approaches ignore the full graph structure, discarding key information.</p>
<p>In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks (Li et al., 2016, GGNNs), which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized label vocabularies (in the order of hundreds). To address this limitation, we also introduce a graph transformation that changes edges to additional nodes, solving the parameter explosion problem. This also ensures that edges have graphspecific hidden vectors, which gives more information to the attention and decoding modules in the network.</p>
<p>We benchmark our model in two graph-tosequence problems, generation from Abstract Meaning Representations (AMRs) and Neural Machine Translation (NMT) with source dependency information. Our approach outperforms strong s2s baselines in both tasks without relying on standard RNN encoders, in contrast with previous work. In particular, for NMT we show that we avoid the need for RNNs by adding sequential edges between contiguous words in the dependency tree. This illustrates the generality of our</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Left: the AMR graph representing the sentence "The boy wants the girl to believe him.". Right: Our proposed architecture using the same AMR graph as input and the surface form as output. The first layer is a concatenation of node and positional embeddings, using distance from the root node as the position. The GGNN encoder updates the embeddings using edge-wise parameters, represented by different colors (in this example, ARGO and ARG1). The encoder also add corresponding reverse edges (dotted arrows) and self edges for each node (dashed arrows). All parameters are shared between layers. Attention and decoder components are similar to standard s2s models. This is a pictorial representation: in our experiments the graphs are transformed before being used as inputs (see §3).
approach: linguistic biases can be added to the inputs by simple graph transformations, without the need for changes to the model architecture.</p>
<h2>2 Neural Graph-to-Sequence Model</h2>
<p>Our proposed architecture is shown in Figure 1, with an example AMR graph and its transformation into its surface form. Compared to standard s2s models, the main difference is in the encoder, where we employ a GGNN to build a graph representation. In the following we explain the components of this architecture in detail. ${ }^{1}$</p>
<h3>2.1 Gated Graph Neural Networks</h3>
<p>Early approaches for recurrent networks on graphs (Gori et al., 2005; Scarselli et al., 2009) assume a fixed point representation of the parameters and learn using contraction maps. Li et al. (2016) argues that this restricts the capacity of the model and makes it harder to learn long distance relations between nodes. To tackle these issues, they propose Gated Graph Neural Networks, which extend these architectures with gating mechanisms</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>in a similar fashion to Gated Recurrent Units (Cho et al., 2014). This allows the network to be learnt via modern backpropagation procedures.</p>
<p>In following, we formally define the version of GGNNs we employ in this study. Assume a directed graph $\mathcal{G}=\left{\mathcal{V}, \mathcal{E}, L_{\mathcal{V}}, L_{\mathcal{E}}\right}$, where $\mathcal{V}$ is a set of nodes $\left(v, \ell_{v}\right), \mathcal{E}$ is a set of edges $\left(v_{i}, v_{j}, \ell_{e}\right)$ and $L_{\mathcal{V}}$ and $L_{\mathcal{E}}$ are respectively vocabularies for nodes and edges, from which node and edge labels $\left(\ell_{v}\right.$ and $\left.\ell_{e}\right)$ are defined. Given an input graph with nodes mapped to embeddings $\mathbf{X}$, a GGNN is defined as</p>
<p>$$
\begin{aligned}
&amp; \mathbf{h}<em v="v">{v}^{0}=\mathbf{x}</em> \
&amp; \mathbf{r}<em v="v">{v}^{t}=\sigma\left(c</em>}^{r} \sum_{u \in \mathcal{N<em _ell__e="\ell_{e">{v}} \mathbf{W}</em>}}^{r} \mathbf{h<em _ell__e="\ell_{e">{u}^{(t-1)}+\mathbf{b}</em>\right) \
&amp; \mathbf{z}}}^{r<em v="v">{v}^{t}=\sigma\left(c</em>}^{z} \sum_{u \in \mathcal{N<em _ell__e="\ell_{e">{v}} \mathbf{W}</em>}}^{z} \mathbf{h<em _ell__e="\ell_{e">{u}^{(t-1)}+\mathbf{b}</em>\right) \
&amp; \widetilde{\mathbf{h}}}}^{z<em v="v">{v}^{t}=\rho\left(c</em>} \sum_{u \in \mathcal{N<em _ell__e="\ell_{e">{v}} \mathbf{W}</em>}}\left(\mathbf{r<em u="u">{u}^{t} \odot \mathbf{h}</em>}^{(t-1)}\right)+\mathbf{b<em e="e">{\ell</em>\right) \
&amp; \mathbf{h}}<em v="v">{v}^{t}=\left(1-\mathbf{z}</em>}^{t}\right) \odot \mathbf{h<em v="v">{v}^{(i-1)}+\mathbf{z}</em>
\end{aligned}
$$}^{t} \odot \widetilde{\mathbf{h}}_{v}^{t</p>
<p>where $e=\left(u, v, \ell_{e}\right)$ is the edge between nodes $u$ and $v, \mathcal{N}(v)$ is the set of neighbour nodes for $v, \rho$ is a non-linear function, $\sigma$ is the sigmoid function</p>
<p>and $c_{v}=c_{v}^{s}=c_{v}^{r}=\left|\mathcal{N}_{v}\right|^{-1}$ are normalisation constants.</p>
<p>Our formulation differs from the original GGNNs from Li et al. (2016) in some aspects: 1) we add bias vectors for the hidden state, reset gate and update gate computations; 2) labelspecific matrices do not share any components; 3) reset gates are applied to all hidden states before any computation and 4) we add normalisation constants. These modifications were applied based on preliminary experiments and ease of implementation.</p>
<p>An alternative to GGNNs is the model from Marcheggiani and Titov (2017), which add edge label information to Graph Convolutional Networks (GCNs). According to Li et al. (2016), the main difference between GCNs and GGNNs is analogous to the difference between convolutional and recurrent networks. More specifically, GGNNs can be seen as multi-layered GCNs where layer-wise parameters are tied and gating mechanisms are added. A large number of layers can propagate node information between longer distances in the graph and, unlike GCNs, GGNNs can have an arbitrary number of layers without increasing the number of parameters. Nevertheless, our architecture borrows ideas from GCNs as well, such as normalising factors.</p>
<h3>2.2 Using GGNNs in attentional encoder-decoder models</h3>
<p>In s2s models, inputs are sequences of tokens where each token is represented by an embedding vector. The encoder then transforms these vectors into hidden states by incorporating context, usually through a recurrent or a convolutional network. These are fed into an attention mechanism, generating a single context vector that informs decisions in the decoder.</p>
<p>Our model follows a similar structure, where the encoder is a GGNN that receives node embeddings as inputs and generates node hidden states as outputs, using the graph structure as context. This is shown in the example of Figure 1, where we have 4 hidden vectors, one per node in the AMR graph. The attention and decoder components follow similar standard s2s models, where we use a bilinear attention mechanism (Luong et al., 2015) and a 2-layered LSTM (Hochreiter and Schmidhuber, 1997) as the decoder. Note, however, that other decoders and attention mechanisms can be
easily employed instead. Bastings et al. (2017) employs a similar idea for syntax-based NMT, but using GCNs instead.</p>
<h3>2.3 Bidirectionality and positional embeddings</h3>
<p>While our architecture can in theory be used with general graphs, rooted directed acyclic graphs (DAGs) are arguably the most common kind in the problems we are addressing. This means that node embedding information is propagated in a top down manner. However, it is desirable to have information flow from the reverse direction as well, in the same way RNN-based encoders benefit from right-to-left propagation (as in bidirectional RNNs). Marcheggiani and Titov (2017) and Bastings et al. (2017) achieve this by adding reverse edges to the graph, as well as self-loops edges for each node. These extra edges have specific labels, hence their own parameters in the network.</p>
<p>In this work, we also follow this procedure to ensure information is evenly propagated in the graph. However, this raises another limitation: because the graph becomes essentially undirected, the encoder is now unaware of any intrinsic hierarchy present in the input. Inspired by Gehring et al. (2017) and Vaswani et al. (2017), we tackle this problem by adding positional embeddings to every node. These embeddings are indexed by integer values representing the minimum distance from the root node and are learned as model parameters. ${ }^{2}$ This kind of positional embedding is restricted to rooted DAGs: for general graphs, different notions of distance could be employed.</p>
<h2>3 Levi Graph Transformation</h2>
<p>The g2s model proposed in $\S 2$ has two key deficiencies. First, GGNNs have three linear transformations per edge type. This means that the number of parameters can explode: AMR, for instance, has around 100 different predicates, which correspond to edge labels. Previous work deal with this problem by explicitly grouping edge labels into a single one (Marcheggiani and Titov, 2017; Bastings et al., 2017) but this is not an ideal solution since it incurs in loss of information.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Top: the AMR graph from Figure 1 transformed into its corresponding Levi graph. Bottom: Levi graph with added reverse and self edges (colors represent different edge labels).</p>
<p>The second deficiency is that edge label information is encoded in the form of GGNN parameters in the network. This means that each label will have the same "representation" across all graphs. However, the latent information in edges can depend on the content in which they appear in a graph. Ideally, edges should have instance-specific hidden states, in the same way as nodes, and these should also inform decisions made in the decoder through the attention module. For instance, in the AMR graph shown in Figure 1, the ARG1 predicate between want-01 and believe-01 can be interpreted as the preposition "to" in the surface form, while the ARG1 predicate connecting believe-01 and boy is realised as a pronoun. Notice that edge hidden vectors are already present in s2s networks that use linearised graphs: we would like our architecture to also have this benefit.</p>
<p>Instead of modifying the architecture, we propose to transform the input graph into its equivalent Levi graph (Levi, 1942; Gross and Yellen, 2004, p. 765). Given a graph $\mathcal{G}=\left{\mathcal{V}, \mathcal{E}, L_{\mathcal{V}}, L_{\mathcal{E}}\right}$,
a Levi graph ${ }^{3}$ is defined as $\mathcal{G}=\left{\mathcal{V}^{\prime}, \mathcal{E}^{\prime}, L_{\mathcal{V}^{\prime}}, L_{\mathcal{E}^{\prime}}\right}$, where $\mathcal{V}^{\prime}=\mathcal{V} \cup \mathcal{E}, L_{\mathcal{V}^{\prime}}=L_{\mathcal{V}} \cup L_{\mathcal{E}}$ and $L_{\mathcal{E}^{\prime}}=\varnothing$. The new edge set $\mathcal{E}^{\prime}$ contains a edge for every (node, edge) pair that is present in the original graph. By definition, the Levi graph is bipartite.</p>
<p>Intuitively, transforming a graph into its Levi graph equivalent turns edges into additional nodes. While simple in theory, this transformation addresses both modelling deficiencies mentioned above in an elegant way. Since the Levi graph has no labelled edges there is no risk of parameter explosion: original edge labels are represented as embeddings, in the same way as nodes. Furthermore, the encoder now naturally generates hidden states for original edges as well.</p>
<p>In practice, we follow the procedure in $\S 2.3$ and add reverse and self-loop edges to the Levi graph, so the practical edge label vocabulary is $L_{\mathcal{E}^{\prime}}={$ default, reverse, self $}$. This still keeps the parameter space modest since we have only three labels. Figure 2 shows the transformation steps in detail, applied to the AMR graph shown in Figure 1. Notice that the transformed graphs are the ones fed into our architecture: we show the original graph in Figure 1 for simplicity.</p>
<p>It is important to note that this transformation can be applied to any graph and therefore is independent of the model architecture. We speculate this can be beneficial in other kinds of graph-based encoder such as GCNs and leave further investigation to future work.</p>
<h2>4 Generation from AMR Graphs</h2>
<p>Our first g2s benchmark is language generation from AMR, a semantic formalism that represents sentences as rooted DAGs (Banarescu et al., 2013). Because AMR abstracts away from syntax, graphs do not have gold-standard alignment information, so generation is not a trivial task. Therefore, we hypothesize that our proposed model is ideal for this problem.</p>
<h3>4.1 Experimental setup</h3>
<p>Data and preprocessing We use the latest AMR corpus release (LDC2017T10) with the default split of 36521/1368/1371 instances for training,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>development and test sets. Each graph is preprocessed using a procedure similar to what is performed by Konstas et al. (2017), which includes entity simplification and anonymisation. This preprocessing is done before transforming the graph into its Levi graph equivalent. For the s2s baselines, we also add scope markers as in Konstas et al. (2017). We detail these procedures in the Supplementary Material.</p>
<p>Models Our baselines are attentional s2s models which take linearised graphs as inputs. The architecture is similar to the one used in Konstas et al. (2017) for AMR generation, where the encoder is a BiLSTM followed by a unidirectional LSTM. All dimensionalities are fixed to 512.</p>
<p>For the g2s models, we fix the number of layers in the GGNN encoder to 8 , as this gave the best results on the development set. Dimensionalities are also fixed at 512 except for the GGNN encoder which uses 576. This is to ensure all models have a comparable number of parameters and therefore similar capacity.</p>
<p>Training for all models uses Adam (Kingma and Ba, 2015) with 0.0003 initial learning rate and 16 as the batch size. ${ }^{4}$ To regularise our models we perform early stopping on the dev set based on perplexity and apply 0.5 dropout (Srivastava et al., 2014) on the source embeddings. We detail additional model and training hyperparameters in the Supplementary Material.</p>
<p>Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHRF++ (Popović, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics.</p>
<p>Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using different seeds. From each pool, we report</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  | BLEU | CHRF++ | #params |
| :-- | :-- | :-- | :-- |
| Single models |  |  |  |
| s2s | 21.7 | 49.1 | 28.4 M |
| s2s (-s) | 18.4 | 46.3 | 28.4 M |
| g2s | 23.3 | 50.4 | 28.3 M |
| Ensembles |  |  |  |
| s2s | 26.6 | 52.5 | 142 M |
| s2s (-s) | 22.0 | 48.9 | 142 M |
| g2s | $\mathbf{2 7 . 5}$ | $\mathbf{5 3 . 5}$ | 141 M |
| Previous work (early AMR treebank versions) |  |  |  |
| KIYCZ17 | 22.0 | - | - |
| Previous work (as above + unlabelled data) |  |  |  |
| KIYCZ17 | 33.8 | - | - |
| PKH16 | 26.9 | - | - |
| SPZWG17 | 25.6 | - | - |
| FDSC16 | 22.0 | - | - |</p>
<p>Table 1: Results for AMR generation on the test set. All score differences between our models and the corresponding baselines are significantly different ( $\mathrm{p}&lt;0.05$ ). " $(-\mathrm{s})$ " means input without scope marking. KIYCZ17, PKH16, SPZWG17 and FDSC16 are respectively the results reported in Konstas et al. (2017), Pourdamghani et al. (2016), Song et al. (2017) and Flanigan et al. (2016).
results using the median model according to performance on the dev set (simulating what is expected from a single run) and using an ensemble of the 5 models.</p>
<p>Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder.</p>
<h3>4.2 Results and analysis</h3>
<p>Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure of Konstas et al. (2017). Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. In particular, we obtain these results without relying on scoping heuristics.</p>
<p>On Figure 3 we show an example where our model outperforms the baseline. The AMR graph contains four reentrancies, predicates that refer-</p>
<div class="codehilite"><pre><span></span><code>Original AMR graph
(p / propose-01
    :ARG0 (c / country
        :wiki &quot;Russia&quot;
        :name (n / name
            :op1 &quot;Russia&quot;))
    :ARG1 (c5 / cooperate-01
        :ARG0 c
        :ARG1 (a / and
            :op1 (c2 / country
                :wiki &quot;India&quot;
                :name (n2 / name
                :op1 &quot;India&quot;))
            :op2 (c3 / country
                :wiki &quot;China&quot;
                :name (n3 / name
                :op1 &quot;China&quot;))))
:purpose (i / increase-01
    :ARG0 c5
    :ARG1 (s / security)
    :location (a2 / around
        :op1 (c4 / country
            :wiki &quot;Afghanistan&quot;
            :name (n4 / name
                :op1 &quot;Afghanistan&quot;)))
    :purpose (b / block-01
        :ARG0 (a3 / and
            :op1 c :op2 c2 :op3 c3
            :ARG1 (s2 / supply-01
            :ARG1 (d / drug)))))
Reference surface form
Russia proposes cooperation with India and China to in-
crease security around Afghanistan to block drug supplies.
s2s output (CHRF++ 61.8)
Russia proposed cooperation with India and China to in-
crease security around the Afghanistan to block security
around the Afghanistan, India and China.
g2s output (CHRF++ 78.2)
Russia proposed cooperation with India and China to in-
crease security around Afghanistan to block drug supplies.
</code></pre></div>

<p>Figure 3: Example showing overgeneration due to reentrancies. Top: original AMR graph with key reentrancies highlighted. Bottom: reference and outputs generated by the s 2 s and g 2 s models, highlighting the overgeneration phenomena.
ence previously defined concepts in the graph. In the s2s models including Konstas et al. (2017), reentrant nodes are copied in the linearised form, while this is not necessary for our g2s models. We can see that the s2s prediction overgenerates the "India and China" phrase. The g2s prediction avoids overgeneration, and almost perfectly matches the reference. While this is only a single example, it provides evidence that retaining the full graphical structure is beneficial for this task, which is corroborated by our quantitative results.</p>
<p>Table 1 also show BLEU scores reported in previous work. These results are not strictly comparable because they used different training set versions and/or employ additional unlabelled corpora; nonetheless some insights can be made. In particular, our g2s ensemble performs better than many previous models that combine a smaller training set with a large unlabelled corpus. It is also most informative to compare our s2s model with Konstas et al. (2017), since this baseline is very similar to theirs. We expected our single model baseline to outperform theirs since we use a larger training set but we obtained similar performance. We speculate that better results could be obtained by more careful tuning, but nevertheless we believe such tuning would also benefit our proposed g2s architecture.</p>
<p>The best results with unlabelled data are obtained by Konstas et al. (2017) using Gigaword sentences as additional data and a paired trained procedure with an AMR parser. It is important to note that this procedure is orthogonal to the individual models used for generation and parsing. Therefore, we hypothesise that our model can also benefit from such techniques, an avenue that we leave for future work.</p>
<h2>5 Syntax-based Neural Machine Translation</h2>
<p>Our second evaluation is NMT, using as graphs source language dependency syntax trees. We focus on a medium resource scenario where additional linguistic information tends to be more beneficial. Our experiments comprise two language pairs: English-German and English-Czech.</p>
<h3>5.1 Experimental setup</h3>
<p>Data and preprocessing We employ the same data and settings from Bastings et al. (2017), ${ }^{5}$ which use the News Commentary V11 corpora from the WMT16 translation task. ${ }^{6}$ English text is tokenised and parsed using SyntaxNet ${ }^{7}$ while German and Czech texts are tokenised and split into subwords using byte-pair encodings (Sennrich et al., 2016, BPE) (8000 merge operations).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We refer to Bastings et al. (2017) for further information on the preprocessing steps.</p>
<p>Labelled dependency trees in the source side are transformed into Levi graphs as a preprocessing step. However, unlike AMR generation, in NMT the inputs are originally surface forms that contain important sequential information. This information is lost when treating the input as dependency trees, which might explain why Bastings et al. (2017) obtain the best performance when using an initial RNN layer in their encoder. To investigate this phenomenon, we also perform experiments adding sequential connections to each word in the dependency tree, corresponding to their order in the original surface form (henceforth, g2s+). These connections are represented as edges with specific left and right labels, which are added after the Levi graph transformation. Figure 4 shows an example of an input graph for g2s+, with the additional sequential edges connecting the words (reverse and self edges are omitted for simplicity).</p>
<p>Models Our s2s and g2s models are almost the same as in the AMR generation experiments (§4.1). The only exception is the GGNN encoder dimensionality, where we use 512 for the experiments with dependency trees only and 448 when the inputs have additional sequential connections. As in the AMR generation setting, we do this to ensure model capacity are comparable in the number of parameters. Another key difference is that the s2s baselines do not use dependency trees: they are trained on the sentences only.</p>
<p>In addition to neural models, we also report results for Phrase-Based Statistical MT (PB-SMT), using Moses (Koehn et al., 2007). The PB-SMT models are trained using the same data conditions as s2s (no dependency trees) and use the standard setup in Moses, except for the language model, where we use a 5 -gram LM trained on the target side of the respective parallel corpus. ${ }^{8}$</p>
<p>Evaluation We report results in terms of BLEU and CHRF++, using case-sensitive versions of both metrics. Other settings are kept the same as in the AMR generation experiments (§4.1). For PBSMT, we also report the median result of 5 runs, obtained by tuning the model using MERT (Och and Ney, 2002) 5 times.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Top: a sentence with its corresponding dependency tree. Bottom: the transformed tree into a Levi graph with additional sequential connections between words (dashed lines). The full graph also contains reverse and self edges, which are omitted in the figure.</p>
<h3>5.2 Results and analysis</h3>
<p>Table 2 shows the results on the respective test set for both language pairs. The g2s models, which do not account for sequential information, lag behind our baselines. This is in line with the findings of Bastings et al. (2017), who found that having a BiRNN layer was key to obtain the best results. However, the g2s+ models outperform the baselines in terms of BLEU scores under the same parameter budget, in both single model and ensemble scenarios. This result show that it is possible to incorporate sequential biases in our model without relying on RNNs or any other modification in the architecture.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">English-German</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">#params</td>
</tr>
<tr>
<td style="text-align: center;">Single models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PB-SMT</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">43.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">s2s</td>
<td style="text-align: center;">15.5</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">41.4 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">40.8 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s+</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">41.2 M</td>
</tr>
<tr>
<td style="text-align: center;">Ensembles</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">s2s</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">207 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">204 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s+</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">45.1</td>
<td style="text-align: center;">206 M</td>
</tr>
<tr>
<td style="text-align: center;">Results from (Bastings et al., 2017)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BoW+GCN</td>
<td style="text-align: center;">12.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN+GCN</td>
<td style="text-align: center;">16.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">English-Czech</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">CHRF++</td>
<td style="text-align: center;">#params</td>
</tr>
<tr>
<td style="text-align: center;">Single models</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PB-SMT</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">s2s</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">39.1 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">38.4 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s+</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">38.8 M</td>
</tr>
<tr>
<td style="text-align: center;">Ensembles</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">s2s</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">195 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">192 M</td>
</tr>
<tr>
<td style="text-align: center;">g2s+</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">194 M</td>
</tr>
<tr>
<td style="text-align: center;">Results from (Bastings et al., 2017)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BoW+GCN</td>
<td style="text-align: center;">7.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN</td>
<td style="text-align: center;">8.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">BiRNN+GCN</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different ( $\mathrm{p}&lt;0.05$ ), including the negative CHRF++ result for En-Cs.</p>
<p>Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both language pairs, while also showing improved performance for s2s in En-Cs. CHRF++ has been shown to better correlate with human judgments compared to BLEU, both at system and sentence level for both language pairs (Bojar et al., 2017), which motivated our choice as an additional metric. We leave further investigation of this phenomena for future work.</p>
<p>We also show some of the results reported by Bastings et al. (2017) in Table 2. Note that their results were based on a different implementation, which may explain some variation in performance. Their BoW+GCN model is the most similar to ours, as it uses only an embedding layer and a GCN encoder. We can see that even our simpler g2s model outperforms their results. A key difference between their approach and ours is the Levi graph transformation and the resulting hidden vectors for edges. We believe their architecture would also benefit from our proposed transformation. In terms of baselines, s2s performs better than their BiRNN model for En-De and comparably for En-Cs, which corroborates that our baselines are strong ones. Finally, our g2s+ single models outperform their BiRNN+GCN results, in particular for En-De, which is further evidence that RNNs are not necessary for obtaining the best performance in this setting.</p>
<p>An important point about these experiments is that we did not tune the architecture: we simply employed the same model we used in the AMR generation experiments, only adjusting the dimensionality of the encoder to match the capacity of the baselines. We speculate that even better results would be obtained by tuning the architecture to this task. Nevertheless, we still obtained improved performance over our baselines and previous work, underlining the generality of our architecture.</p>
<h2>6 Related work</h2>
<p>Graph-to-sequence modelling Early NLP approaches for this problem were based on Hyperedge Replacement Grammars (Drewes et al., 1997, HRGs). These grammars assume the transduction problem can be split into rules that map portions of a graph to a set of tokens in the output sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a complexity analysis, while Jones et al. (2012) report experiments on semantic-based machine translation using HRGs. HRGs were also used in previous work on AMR parsing (Peng et al., 2015). The main drawback of these grammar-based approaches though is the need for alignments between graph nodes and surface tokens, which are usually not available in gold-standard form.</p>
<p>Neural networks for graphs Recurrent networks on general graphs were first proposed un-</p>
<p>der the name Graph Neural Networks (Gori et al., 2005; Scarselli et al., 2009). Our work is based on the architecture proposed by Li et al. (2016), which add gating mechanisms. The main difference between their work and ours is that they focus on problems that concern the input graph itself such as node classification or path finding while we focus on generating strings. The main alternative for neural-based graph representations is Graph Convolutional Networks (Bruna et al., 2014; Duvenaud et al., 2015; Kipf and Welling, 2017), which have been applied in a range of problems. In NLP, Marcheggiani and Titov (2017) use a similar architecture for Semantic Role Labelling. They use heuristics to mitigate the parameter explosion by grouping edge labels, while we keep the original labels through our Levi graph transformation. An interesting alternative is proposed by Schlichtkrull et al. (2017), which uses tensor factorisation to reduce the number of parameters.</p>
<p>Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data.</p>
<h2>7 Discussion and Conclusion</h2>
<p>We proposed a novel encoder-decoder architecture for graph-to-sequence learning, outperforming baselines in two NLP tasks: generation from AMR graphs and syntax-based NMT. Our approach addresses shortcomings from previous work, including loss of information from lineari-
sation and parameter explosion. In particular, we showed how graph transformations can solve issues with graph-based networks without changing the underlying architecture. This is the case of the proposed Levi graph transformation, which ensures the decoder can attend to edges as well as nodes, but also to the sequential connections added to the dependency trees in the case of NMT. Overall, because our architecture can work with general graphs, it is straightforward to add linguistic biases in the form of extra node and/or edge information. We believe this is an interesting research direction in terms of applications.</p>
<p>Our architecture nevertheless has two major limitations. The first one is that GGNNs have a fixed number of layers, even though graphs can vary in size in terms of number of nodes and edges. A better approach would be to allow the encoder to have a dynamic number of layers, possibly based on the diameter (longest path) in the input graph. The second limitation comes from the Levi graph transformation: because edge labels are represented as nodes they end up sharing the vocabulary and therefore, the same semantic space. This is not ideal, as nodes and edges are different entities. An interesting alternative is Weave Module Networks (Kearnes et al., 2016), which explicitly decouples node and edge representations without incurring in parameter explosion. Incorporating both ideas to our architecture is an research direction we plan for future work.</p>
<h2>Acknowledgements</h2>
<p>This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data from his paper's experiments.</p>
<h2>References</h2>
<p>Roee Aharoni and Yoav Goldberg. 2017. Towards String-to-Tree Neural Machine Translation. In Proceedings of ACL. pages 132-140.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin</p>
<p>Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. pages 178-186.</p>
<p>Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima'an. 2017. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Proceedings of EMNLP. pages 1947-1957.</p>
<p>Ondej Bojar, Yvette Graham, and Amir Kamran. 2017. Results of the WMT17 Metrics Shared Task. In Proceedings of WMT. volume 2, pages 293-301.</p>
<p>Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. 2014. Spectral Networks and Locally Connected Networks on Graphs. In Proceedings of $I C L R$. page 14.</p>
<p>Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems. In Proceedings of the Workshop on Machine Learning Systems. pages 1-6.</p>
<p>David Chiang, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, Bevan Jones, and Kevin Knight. 2013. Parsing Graphs with Hyperedge Replacement Grammars. In Proceedings of ACL. pages 924-932.</p>
<p>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN EncoderDecoder for Statistical Machine Translation. In Proceedings of EMNLP. pages 1724-1734.</p>
<p>Frank Drewes, Hans Jörg Kreowski, and Annegret Habel. 1997. Hyperedge Replacement Graph Grammars. Handbook of Graph Grammars and Computing by Graph Transformation .</p>
<p>David Duvenaud, Dougal Maclaurin, Jorge AguileraIparraguirre, Rafael Gómez-Bombarelli, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In Proceedings of NIPS. pages 2215-2223.</p>
<p>Akiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-Sequence Attentional Neural Machine Translation. In Proceedings of ACL.</p>
<p>Akiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun Cho. 2017. Learning to Parse and Translate Improves Neural Machine Translation. In Proceedings of $A C L$.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from Abstract Meaning Representation using Tree Transducers. In Proceedings of NAACL. pages 731-739.</p>
<p>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What's in a translation rule? In Proceedings of NAACL. pages 273-280.</p>
<p>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. arXiv preprint .</p>
<p>Marco Gori, Gabriele Monfardini, and Franco Scarselli. 2005. A New Model for Learning in Graph Domains. In Proceedings of IJCNN. volume 2, pages 729-734.</p>
<p>Yvette Graham, Nitika Mathur, and Timothy Baldwin. 2014. Randomized Significance Tests in Machine Translation. In Proceedings of WMT. pages 266274.</p>
<p>Jonathan Gross and Jay Yellen, editors. 2004. Handbook of Graph Theory. CRC Press.</p>
<p>Kazuma Hashimoto and Yoshimasa Tsuruoka. 2017. Neural Machine Translation with Source-Side Latent Graph Parsing. In Proceedings of EMNLP. pages 125-135.</p>
<p>Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem Sokolov, Ann Clifton, and Matt Post. 2017. Sockeye: A Toolkit for Neural Machine Translation. arXiv preprint pages 1-18.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9(8):1735-1780.</p>
<p>Bevan Jones, Jacob Andreas, Daniel Bauer, Karl Moritz Hermann, and Kevin Knight. 2012. Semantics-Based Machine Translation with Hyperedge Replacement Grammars. In Proceedings of COLING. pages 1359-1376.</p>
<p>Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. 2016. Molecular Graph Convolutions: Moving Beyond Fingerprints. Journal of Computer-Aided Molecular Design 30(8):595-608.</p>
<p>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. 2017. On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. In Proceedings of ICLR. pages 1-16.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of ICLR. pages 1-15.</p>
<p>Thomas N. Kipf and Max Welling. 2017. SemiSupervised Classification with Graph Convolutional Networks. In Proceedings of ICLR.</p>
<p>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of ACL Demo Session. pages 177-180.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-Sequence Models for Parsing and Generation. In Proceedings of ACL. pages 146-157.</p>
<p>Friedrich Wilhelm Levi. 1942. Finite Geometrical Systems.</p>
<p>Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. 2016. Gated Graph Sequence Neural Networks. In Proceedings of ICLR. 1, pages 120 .</p>
<p>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL - ACL '06. pages 609-616.</p>
<p>Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of EMNLP. pages 1412-1421.</p>
<p>Diego Marcheggiani and Ivan Titov. 2017. Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling. In Proceedings of EMNLP.</p>
<p>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics - ACL '02. page 295. https://doi.org/10.3115/1073083.1073133.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL. pages 311-318.</p>
<p>Xiaochang Peng, Linfeng Song, and Daniel Gildea. 2015. A Synchronous Hyperedge Replacement Grammar based approach for AMR parsing. In Proceedings of CoNLL. pages 32-41.</p>
<p>Maja Popović. 2017. chrF ++: words helping character n-grams. In Proceedings of WMT. pages 612-618.</p>
<p>Nima Pourdamghani, Kevin Knight, and Ulf Hermjakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of INLG. volume 0 , pages $21-25$.</p>
<p>Nils Reimers and Iryna Gurevych. 2017. Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. In Proceedings of EMNLP. pages 338-348.</p>
<p>Franco Scarselli, Marco Gori, Ah Ching Tsoi, and Gabriele Monfardini. 2009. The Graph Neural Network Model. IEEE Transactions on Neural Networks 20(1):61-80.</p>
<p>Michael Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2017. Modeling Relational Data with Graph Convolutional Networks pages 1-12.</p>
<p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of ACL. pages 1715-1725.</p>
<p>Linfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2017. AMR-to-text Generation with Synchronous Node Replacement Grammar. In Proceedings of ACL. pages 7-13.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research 15:1929-1958.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In Proceedings of NIPS.</p>
<p>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics 23(3):377-403.</p>
<p>Kenji Yamada and Kevin Knight. 2001. A Syntaxbased Statistical Translation Model. In Proceedings of ACL. pages 523-530.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ Note that target data is segmented using BPE, which is not the usual setting for PB-SMT. We decided to keep the segmentation to ensure data conditions are the same.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
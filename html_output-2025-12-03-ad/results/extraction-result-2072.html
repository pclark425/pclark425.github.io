<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2072 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2072</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2072</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-280011325</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.21763v1.pdf" target="_blank">THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow.Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show ~60\% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured.This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \textbf{THE-Tree} (\textbf{T}echnology \textbf{H}istory \textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature.THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel"Think-Verbalize-Cite-Verify"process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded.We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research.Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8\% to 14\% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10\%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100\%.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2072.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2072.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>THE-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Technology History Evolution Tree</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational framework that constructs domain-specific, causally-linked evolution trees of scientific papers (nodes) and inferential edges, intended to provide verifiable historical context to validate new scientific claims and AI-generated ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>THE-Tree</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>computational framework (LLM-enhanced graph construction + search)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scholarly knowledge representation / scientific idea verification (cross-disciplinary)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>structured evolution trees (papers as nodes; validated inferential edges), retrospection paths for input papers</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (novel assembly and formalization of historical/causal paths from literature rather than novel scientific hypotheses per se)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) guided by LLM priorities and temporal coherence; node expansion via TVCV (LLM proposes candidate advances then grounds them with citations)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Retrieval-Augmented Natural Language Inference (RA-NLI) to verify proposed edges (embedding retrieval of evidence passages + fine-tuned NLI model + LLM fallback evaluator); expert refinement used to create ground truth</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Graph completion: improves Hit@1 by 8%–14% across multiple models vs traditional citation networks; future-path prediction: improves Hit@1 by ~10%; when combined with other methods, nearly doubles performance for identifying important papers (reported 'almost 100%' boost in some evaluations). Reported improvements are relative percentages; absolute baseline values are in appendix tables.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>On a 71k fact verification dataset covering 27k papers, the RA-NLI-backed THE-Tree workflow produced a fact-missing rate of 4.75% and overall accuracy of 95.60% (reported as 'RA-NLI (Ours)'). MCTS-generated tree entity recall 0.84, precision 0.67, F1 0.75; relation recall 0.78, precision 0.64, F1 0.70 (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported as an FPR; the paper reports 'fact missing rate' (4.75% for RA-NLI) and accuracy (95.60%) but does not provide explicit false positive / false negative breakdowns for THE-Tree edges.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not explicitly reported. Paper provides fact-missing rate (4.75%) which is a type of omission measure but not exactly per-class FNR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Paper argues validation performance is substantially improved by grounding with historical, causally-validated paths; explicit quantitative breakdown by novelty level is not provided. Authors note that standalone LLM verifiers have high omission/hallucination rates (~60% unawareness of relevant papers), while THE-Tree+RA-NLI reduces fact-missing dramatically.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Yes — the paper highlights an asymmetry: LLMs and search methods generate many candidate advances easily but standalone LLM verification is weak (high fact-missing/hallucination). THE-Tree narrows the gap by generating structured candidate paths with internal RA-NLI validation; reported metrics show generation tasks (e.g., candidate path recall) are improved, and validation accuracy (RA-NLI) is high (95.6%). Exact numerical gap between raw generation quality and post-validation accepted links is not given, but improvement metrics (Hit@1 gains, reduced fact-missing) indicate validation substantially filters/sanitizes generated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No explicit probabilistic uncertainty calibration reported for THE-Tree outputs as a whole; RA-NLI returns normalized entailment scores in [0,1] (NLI probability used) and LLMeval returns a score; these are combined with a weighting α=0.7, but no calibration metrics (e.g., ECE) are reported. Authors list quantifying uncertainty in evolutionary links as future work.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported. The paper provides normalized scores from NLI and LLM evaluators but does not present calibration analyses or calibration degradation with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified with explicit OOD metrics. The authors claim THE-Tree improves prediction of plausible future developments and reduces reliance on citation topology; reported Hit@k and rank metrics improve vs citation-only graphs, but explicit performance on transformational / out-of-distribution discoveries is not given.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses proxy metrics such as DPO (degree to which a node continues a path, approximated by an LLM), TemporalCoherence, node importance scores (combining graph centrality and LLM importance), and RA-NLI entailment probabilities as proxies for causal/evidential validity. Graph completion and Hit@k are used as proxy downstream evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts were used to refine MCTS-generated trees as ground truth (expert curation step); recommended for high-novelty or final curation. Frequency depends on task: expert refinement used to create benchmark ground truth; suggested that human review remains necessary for final validation, especially for highly novel claims.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (scientific literature, cross-disciplinary); semi-formal insofar as claims are textual and require evidence retrieval, not formal proofs; authors note this affects validation difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Combine generation and retrieval+NLI verification: SGT-MCTS (generation/search) + TVCV (LLM propose, verbalize, cite) + RA-NLI (evidential NLI + LLM fallback) and expert refinement. Also start from human-validated surveys to mitigate noisy initialization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Multiple pieces: authors report standalone LLM verification shows 'nearly 60%' factual omissions / unawareness of relevant papers; Table 2 shows many models have high fact-missing rates (40–69%) and modest accuracy, while RA-NLI reduces fact-missing to 4.75% and raises accuracy to 95.60%; authors also report that citation networks lack causal clarity and LLMs hallucinate, supporting a generation>validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some models (e.g., DeepReviewer-7B in Table 2) show high accuracy (93.95) even without RA-NLI, indicating not all generator/verifier models are equally weak; however, these are exceptions and the general pattern in the paper supports the gap. The paper does not present systematic counter-evidence that validation already matches generation across novelty levels.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Qualitative only: authors state significant computational cost due to extensive LLM usage and large-scale graph processing; no numeric ratio provided. They note validation (RA-NLI + retrieval + LLM fallbacks + expert refinement) increases cost relative to raw generation/search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2072.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2072.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RA-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Natural Language Inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A validation mechanism that retrieves supporting passages from cited documents and performs a fine-grained NLI (DeBERTa-based) classification to assess whether a proposed evolutionary link is entailed, contradicted, or neutral, with an LLM fallback for ambiguous cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RA-NLI</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval + fine-tuned NLI model + LLM evaluator (hybrid NLP verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific citation verification / textual entailment in scholarly text</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>entailment/contradiction/neutral scores and a scalar Rattr reward used to validate edges</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>incremental (applies retrieval-augmented NLI principles to the space of citation/evolutionary link validation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (validator rather than generator); it generates validation scores by embedding-based retrieval of evidence and NLI inference</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Embedding-based retrieval of context passages from cited documents followed by a fine-tuned DeBERTa NLI classifier returning normalized entailment probabilities; ambiguous 'neutral' cases further evaluated by a large LLM (Qwen2.5-72B-Instruct) to decide citation validity.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (not a generator); used within SGT-MCTS to compute Rattr rewards that influence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>On the 71k fact verification dataset: fact-missing rate 4.75% and accuracy 95.60% (reported as RA-NLI (Ours) in Table 2), outperforming many standalone LLMs and LLMs with 'factual supplement'.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported. The paper provides overall accuracy and 'fact missing rate' but does not report FPR/FNR per class.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not explicitly reported; 'fact missing rate' is given (4.75%) but is not a direct per-class FNR.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>No explicit per-novelty breakdown. RA-NLI is designed to reduce hallucinations on citation-linked claims; authors claim it substantially reduces fact-missing even for diverse papers, but explicit degradation with novelty not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>RA-NLI is positioned as improving validation relative to LLM-only verification; the paper reports large reductions in fact-missing compared to standalone LLM verifiers (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>NLI returns normalized probability scores [0,1]; RA-NLI composes NLI(si,sj) and LLMeval(si,sj,C) with weighting α=0.7 to compute Rattr. No calibration statistics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported specifically; evaluation corpus covers 27k papers and 71k verification items, but explicit OOD performance claims are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Relies on entailment probability and LLMeval score as proxies for validity; also uses fact-missing rate and overall accuracy to evaluate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Used in expert refinement of trees and for ground-truth construction; recommended especially for ambiguous/neutral NLI outputs and for final dataset curation.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / textual (scientific publications); not a formal domain.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Combines retrieval, a domain-fine-tuned NLI model (DeBERTa), and an LLM fallback to handle ambiguous/neutral cases; this hybrid approach is presented as effective at reducing hallucinations and fact-missing.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Direct comparison table (Table 2) shows many LLMs with high fact-missing rates (40–69%) and modest accuracies; RA-NLI reduces fact-missing to 4.75% and yields 95.6% accuracy, supporting that improved retrieval+NLI validation is needed to close the generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No systematic contradictory evidence presented; some strong standalone models exist (e.g., DeepReviewer-7B) but RA-NLI still outperforms them on fact-missing and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not numerically specified; RA-NLI involves retrieval, fine-tuned model inference, and LLM fallback which increases computation compared to a single LLM pass. Authors note computational cost is substantial overall.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2072.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2072.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TVCV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Think-Verbalize-Cite-Verify</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A four-step LLM-guided node expansion protocol where the model: (1) Thinks of candidate advancements, (2) Verbalizes concise proposition statements, (3) Cites supporting literature to ground the proposition, and (4) Verifies the proposed relationship via RA-NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TVCV (Think-Verbalize-Cite-Verify)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-guided procedural prompting/workflow for generation+grounding</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific hypothesis/advance generation and grounding in literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>candidate technological advancements (propositions), citation-grounded proposed edges (parent→child)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>methodological (process to produce grounded candidate advances rather than producing raw novel scientific claims); enables moderately novel recombinations by LLMs but grounded via citations</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-generated candidate advancements informed by the current path and domain knowledge (Think and Verbalize), followed by retrieval/selection of supporting papers (Cite).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Proposed links are validated by RA-NLI (Verify) which retrieves evidence and applies NLI and LLM-based refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used within SGT-MCTS to produce candidate nodes; specific generation success rates are not separately reported, but overall tree construction metrics (node recall/precision, Hit@k improvements) reflect combined TVCV+MCTS performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation of TVCV-proposed links is performed by RA-NLI; RA-NLI metrics (fact-missing 4.75%, accuracy 95.60%) apply. No separate TVCV-only validation numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported for TVCV proposals. The paper reports that RA-NLI reduces fact-missing and improves accuracy, implying TVCV proposals are substantially filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not quantified. The workflow is explicitly designed to ground proposed novel ideas in citations to mitigate hallucination, but per-novelty effect sizes are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>TVCV is explicitly designed to tie generation with immediate validation (Verify) to reduce the gap; reported system-level improvements suggest this reduces invalid proposals, though separate numeric comparisons are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported beyond RA-NLI scores returned in Verify step.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses RA-NLI entailment scores and retrieval evidence as proxies for link validity; DPO and TemporalCoherence used for generation scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts used for ground-truth refinement; TVCV outputs are intended to be audited and RA-NLI still used as automatic guardrails. Frequency: unspecified but implied when high novelty or ambiguity arises.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical textual domain; TVCV operates on textual claims and citation evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>In-line verification (Verify) of every LLM-proposed link using RA-NLI; combining retrieval, NLI, temporal coherence penalties, and expert-starting points (surveys) to reduce spurious generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper describes LLM hallucination and high fact-missing rates (~60%) when used alone; TVCV+RA-NLI pipeline reduces these errors as shown by RA-NLI metrics and overall improvement in downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No direct contradiction presented; specific per-component failure rates for TVCV are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided numerically; TVCV requires LLM proposals and retrieval+NLI validation per candidate, increasing compute compared to single-pass generation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2072.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2072.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGT-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Guided Temporal Monte Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Monte Carlo Tree Search variant that incorporates LLM-derived priorities and temporal coherence into node selection and expansion to search for high-reward historical evolutionary paths across literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SGT-MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>reinforcement/search algorithm (MCTS variant) augmented with LLM priorities</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>search over hypothesis/history/path spaces in scholarly literature</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>candidate technology evolutionary paths and tree expansions (candidate nodes/edges)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>methodological; combines classic MCTS with LLM guidance to explore novel combinatorial paths (moderately novel)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>UCT-based tree search with added LLMpriority(v) and TempCoherence(v|Pprev); generation reward uses DPO approximated by LLM and temporal coherence penalties.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Expanded nodes/edges are validated via TVCV's Verify step (RA-NLI) which provides Rattr used in reward backpropagation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to construct THE-Trees showing node recall 0.84, precision 0.67, F1 0.75 (entities) and relation recall 0.78, precision 0.64, F1 0.70 vs expert ground truth; improved Hit@k and rank metrics on graph completion and future prediction tasks vs citation graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation is external to SGT-MCTS (RA-NLI); SGT-MCTS relies on Rattr scores to guide search. No separate validation performance reported for SGT-MCTS itself beyond integrated tree metrics in Table 3.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported separately; overall precision numbers (entities precision 0.67, relations precision 0.64) imply some false positives in generated nodes/edges but exact FPR not given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported; entity recall 0.84 indicates some missed nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Not explicitly measured; temporal coherence and LLMpriority aim to favor plausible continuations but no per-novelty performance curve provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>SGT-MCTS emphasizes generation guided by validation-aware rewards (Rattr), explicitly coupling generation to validation scores to reduce invalid proposals; integrated metrics show improved reconstruction vs baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported for SGT-MCTS; search uses reward estimates (Q/N) but no explicit uncertainty outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported distinctly.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses composite reward S(v) + Rgen + Rattr where Rattr is RA-NLI score and Rgen uses DPO and temporal coherence as proxies for path validity.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts used to refine outputs and build ground truth; frequency unspecified.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical/textual scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Incorporate validation reward (Rattr) from RA-NLI into the MCTS reward to bias search towards evidentially-supported paths; also use temporal coherence to avoid anachronistic proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors motivate SGT-MCTS by noting that standard citation graphs and LLM-only search produce noisy or hallucinated links; combined guidance + RA-NLI yields improved reconstruction metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>No explicit contradiction; SGT-MCTS precision<1 indicates generation still produces some invalid items requiring curation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not quantified; inclusion of LLM guidance and RA-NLI validation increases cost per expansion relative to vanilla MCTS.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2072.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2072.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (standalone verifiers/generators)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as hypothesis generators and verifiers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General category of pretrained LLMs (e.g., GPT-4o, Qwen2.5, Claude-3.5) used to generate hypotheses, propose advances, and (when used naively) to verify claims, but which the paper finds prone to hallucination and incomplete domain awareness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large Language Models (e.g., GPT-4o, Qwen2.5-72B, Claude 3.5 Sonnet, DeepReviewer-* )</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model / transformer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>hypothesis generation, paper evaluation, idea synthesis across scientific domains</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>scientific hypotheses, paper reviews/assessments, candidate technological advancements, natural language summaries</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>varies from in-distribution recombination to moderately novel suggestions; paper reports that LLMs can generate many ideas but with variable novelty and factual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>pattern extrapolation and recombination from training corpora; prompted generation and in some systems used with chain-of-thought or reasoning scaffolding</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>When used naively, validation is via their own internal confidence or judgement; the paper contrasts naive LLM-only verification with RA-NLI retrieval+NLI validation and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not reported as a single metric; in downstream tasks LLM-augmented baselines show various performances (Table 1 shows acceptance/status prediction accuracies for models: e.g., Qwen2.5-72b-instruct reported Acc% etc.). The paper states LLMs accelerate idea generation but often produce numerous superficial propositions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Table 2 reports fact-missing rates and accuracy for several LLMs when used for verification: Claude 3.5 Sonnet fact-missing 47.93% accuracy 60.22%; GPT-4o fact-missing 58.19% accuracy 60.18%; Qwen2.5 fact-missing 58.29% accuracy 53.95%; DeepSeek-R1 fact-missing 48.16% accuracy 76.40%; DeepReviewer-7B fact-missing 40.88% accuracy 93.95%; DeepReviewer-14B fact-missing 68.84% accuracy 76.41; LLaMA 3.1 fact-missing 42.29% accuracy 60.40. Factual supplementation improves these metrics but RA-NLI outperforms them.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported per model in terms of FPR; only 'fact missing rate' and accuracy are reported. The paper does not provide explicit false-positive counts.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Authors note standalone LLM verifiers are especially weak for novel or domain-specific verification tasks (e.g., '60% unawareness of relevant papers in specific domains'), implying validation accuracy degrades with increased novelty, but no numeric per-novelty curve is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper emphasizes a gap: generation (idea production) is prolific, but validation (truthful grounding) by standalone LLMs is poor and prone to hallucination; RA-NLI and THE-Tree aim to close this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>LLM internal confidences are not systematically used or calibrated in the paper; authors report using LLM-derived scores (DPO approximations, LLMpriority) as heuristics but provide no calibration metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; the paper explicitly criticizes LLMs for superficial plausibility assessments and potential overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Indicated as poor for verification tasks (high fact-missing/unawareness rates), but specific OOD metrics not given.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Standalone LLM verification often relies on proxies like textual plausibility, coherence, or internal entailment judgments; the paper flags this as inadequate compared to retrieval+NLI.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Authors state manual verification is ideal but too slow; recommend human oversight particularly for highly novel outputs and for constructing ground-truth trees. Frequency unspecified but higher for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>varies by application; for scholarly text it's empirical/semi-formal.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Factual supplementation, augmenting LLMs with retrieval, coupling with RA-NLI, incorporating THE-Tree historical grounding, and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors' experiments: many LLMs show high fact-missing rates (40–69%) and limited accuracy on fact-verification tasks; authors claim ~60% unawareness of relevant papers in domains when LLMs are used as verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Some LLM-based systems (e.g., DeepReviewer-7B) show high verification accuracy in the table, indicating variability across models and that gap may not be uniform.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported numerically. The paper emphasizes that augmentations (retrieval, RA-NLI, expert refinement) increase computational cost substantially.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2072.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2072.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepReviewer-14b (case)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepReviewer-14B (LLM used for paper evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 14-billion parameter LLM used as a baseline reviewer in NeurIPS 2024 evaluation experiments; when augmented with THE-Tree it showed improved accuracy in acceptance/status prediction and better identification of high-impact papers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepReviewer-14B</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model (14B), LLM-based reviewer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>automated paper review / acceptance prediction</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>paper acceptance/rejection decisions, predicted status (Poster/Spotlight/Oral), review text</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution to moderately novel (reviewing new submissions using learned review patterns)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Standard LLM inference with domain-specific prompting (DeepReviewer architecture described in related works)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Standalone LLM judgment when unaugmented; when augmented, uses THE-Tree retrospection paths to ground judgments and RA-NLI validated evidence to inform decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Table 1: standalone DeepReviewer-14b on NeurIPS 2024 had various metrics (example: Accuracy of accept and reject: Acc% 92.61, Rej% 18.28, Total% 37.45? — table formatting is complex). With THE-Tree augmentation, performance shifts (DeepReviewer-14b_tree reported improved Total% and better status prediction). The paper reports that THE-Tree augmentation 'substantially boosts' DeepReviewer-14b's recognition of high-impact papers, in some instances nearly doubling accuracy for those categories.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Standalone DeepReviewer-14b fact-missing (from Table 2-like comparisons) is reported among other models: DeepReviewer-14B fact-missing 68.84% accuracy 76.41 in the general fact-verification table; however, when augmented with THE-Tree, the model's paper-evaluation accuracy and ability to reject low-quality submissions improved (case studies and aggregate table entries show substantial gains).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not explicitly reported. The table data does not provide a direct FPR for accept decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not explicitly reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Augmentation with THE-Tree improved assessment of high-impact / novel papers (e.g., better alignment with ground-truth Oral/Spotlight designations), indicating standalone model underestimates impact for some novel papers; precise per-novelty metrics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper shows a concrete example where standalone DeepReviewer-14b accepted a paper but rated it as Poster, while THE-Tree-augmented version rated it Oral (closer to ground truth), illustrating a generation/assessment gap closed by grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not separately quantified; case studies suggest improved performance on impactful (possibly more novel) papers when augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>When augmented, uses THE-Tree retrospection paths and their validated evidence as proxies for historical novelty/impact assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts used for ground truth and to analyze model augmentation effects; recommended for high-stakes decisions like acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical (conference review process)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Augmentation with THE-Tree retrospection paths and RA-NLI to ground review judgments; demonstrated improvement in case studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Case studies and aggregate evaluation tables show THE-Tree changes model judgments toward expert ground truth (e.g., moving rating from Poster to Oral), supporting that standalone generation/assessment can be insufficient and benefits from historical grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None provided specifically for DeepReviewer-14b; the model had non-trivial baseline performance, but augmentation improved alignment with experts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided numerically; augmentation entails additional retrieval and RA-NLI validation, increasing compute relative to standalone inference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CycleResearcher: Improving Automated Research via Automated Review <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2072",
    "paper_id": "paper-280011325",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "THE-Tree",
            "name_full": "Technology History Evolution Tree",
            "brief_description": "A computational framework that constructs domain-specific, causally-linked evolution trees of scientific papers (nodes) and inferential edges, intended to provide verifiable historical context to validate new scientific claims and AI-generated ideas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "THE-Tree",
            "system_type": "computational framework (LLM-enhanced graph construction + search)",
            "scientific_domain": "scholarly knowledge representation / scientific idea verification (cross-disciplinary)",
            "output_type": "structured evolution trees (papers as nodes; validated inferential edges), retrospection paths for input papers",
            "novelty_level": "moderately novel (novel assembly and formalization of historical/causal paths from literature rather than novel scientific hypotheses per se)",
            "generation_method": "Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) guided by LLM priorities and temporal coherence; node expansion via TVCV (LLM proposes candidate advances then grounds them with citations)",
            "validation_method": "Retrieval-Augmented Natural Language Inference (RA-NLI) to verify proposed edges (embedding retrieval of evidence passages + fine-tuned NLI model + LLM fallback evaluator); expert refinement used to create ground truth",
            "generation_performance": "Graph completion: improves Hit@1 by 8%–14% across multiple models vs traditional citation networks; future-path prediction: improves Hit@1 by ~10%; when combined with other methods, nearly doubles performance for identifying important papers (reported 'almost 100%' boost in some evaluations). Reported improvements are relative percentages; absolute baseline values are in appendix tables.",
            "validation_performance": "On a 71k fact verification dataset covering 27k papers, the RA-NLI-backed THE-Tree workflow produced a fact-missing rate of 4.75% and overall accuracy of 95.60% (reported as 'RA-NLI (Ours)'). MCTS-generated tree entity recall 0.84, precision 0.67, F1 0.75; relation recall 0.78, precision 0.64, F1 0.70 (Table 3).",
            "false_positive_rate": "Not explicitly reported as an FPR; the paper reports 'fact missing rate' (4.75% for RA-NLI) and accuracy (95.60%) but does not provide explicit false positive / false negative breakdowns for THE-Tree edges.",
            "false_negative_rate": "Not explicitly reported. Paper provides fact-missing rate (4.75%) which is a type of omission measure but not exactly per-class FNR.",
            "performance_vs_novelty": "Paper argues validation performance is substantially improved by grounding with historical, causally-validated paths; explicit quantitative breakdown by novelty level is not provided. Authors note that standalone LLM verifiers have high omission/hallucination rates (~60% unawareness of relevant papers), while THE-Tree+RA-NLI reduces fact-missing dramatically.",
            "generation_validation_comparison": "Yes — the paper highlights an asymmetry: LLMs and search methods generate many candidate advances easily but standalone LLM verification is weak (high fact-missing/hallucination). THE-Tree narrows the gap by generating structured candidate paths with internal RA-NLI validation; reported metrics show generation tasks (e.g., candidate path recall) are improved, and validation accuracy (RA-NLI) is high (95.6%). Exact numerical gap between raw generation quality and post-validation accepted links is not given, but improvement metrics (Hit@1 gains, reduced fact-missing) indicate validation substantially filters/sanitizes generated outputs.",
            "uncertainty_quantification": "No explicit probabilistic uncertainty calibration reported for THE-Tree outputs as a whole; RA-NLI returns normalized entailment scores in [0,1] (NLI probability used) and LLMeval returns a score; these are combined with a weighting α=0.7, but no calibration metrics (e.g., ECE) are reported. Authors list quantifying uncertainty in evolutionary links as future work.",
            "calibration_quality": "Not reported. The paper provides normalized scores from NLI and LLM evaluators but does not present calibration analyses or calibration degradation with novelty.",
            "out_of_distribution_performance": "Not quantified with explicit OOD metrics. The authors claim THE-Tree improves prediction of plausible future developments and reduces reliance on citation topology; reported Hit@k and rank metrics improve vs citation-only graphs, but explicit performance on transformational / out-of-distribution discoveries is not given.",
            "validation_proxy_metrics": "Yes — uses proxy metrics such as DPO (degree to which a node continues a path, approximated by an LLM), TemporalCoherence, node importance scores (combining graph centrality and LLM importance), and RA-NLI entailment probabilities as proxies for causal/evidential validity. Graph completion and Hit@k are used as proxy downstream evaluation metrics.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts were used to refine MCTS-generated trees as ground truth (expert curation step); recommended for high-novelty or final curation. Frequency depends on task: expert refinement used to create benchmark ground truth; suggested that human review remains necessary for final validation, especially for highly novel claims.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (scientific literature, cross-disciplinary); semi-formal insofar as claims are textual and require evidence retrieval, not formal proofs; authors note this affects validation difficulty.",
            "gap_mitigation_strategies": "Combine generation and retrieval+NLI verification: SGT-MCTS (generation/search) + TVCV (LLM propose, verbalize, cite) + RA-NLI (evidential NLI + LLM fallback) and expert refinement. Also start from human-validated surveys to mitigate noisy initialization.",
            "evidence_supporting_gap": "Multiple pieces: authors report standalone LLM verification shows 'nearly 60%' factual omissions / unawareness of relevant papers; Table 2 shows many models have high fact-missing rates (40–69%) and modest accuracy, while RA-NLI reduces fact-missing to 4.75% and raises accuracy to 95.60%; authors also report that citation networks lack causal clarity and LLMs hallucinate, supporting a generation&gt;validation gap.",
            "evidence_contradicting_gap": "Some models (e.g., DeepReviewer-7B in Table 2) show high accuracy (93.95) even without RA-NLI, indicating not all generator/verifier models are equally weak; however, these are exceptions and the general pattern in the paper supports the gap. The paper does not present systematic counter-evidence that validation already matches generation across novelty levels.",
            "computational_cost_ratio": "Qualitative only: authors state significant computational cost due to extensive LLM usage and large-scale graph processing; no numeric ratio provided. They note validation (RA-NLI + retrieval + LLM fallbacks + expert refinement) increases cost relative to raw generation/search.",
            "uuid": "e2072.0"
        },
        {
            "name_short": "RA-NLI",
            "name_full": "Retrieval-Augmented Natural Language Inference",
            "brief_description": "A validation mechanism that retrieves supporting passages from cited documents and performs a fine-grained NLI (DeBERTa-based) classification to assess whether a proposed evolutionary link is entailed, contradicted, or neutral, with an LLM fallback for ambiguous cases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "RA-NLI",
            "system_type": "retrieval + fine-tuned NLI model + LLM evaluator (hybrid NLP verifier)",
            "scientific_domain": "scientific citation verification / textual entailment in scholarly text",
            "output_type": "entailment/contradiction/neutral scores and a scalar Rattr reward used to validate edges",
            "novelty_level": "incremental (applies retrieval-augmented NLI principles to the space of citation/evolutionary link validation)",
            "generation_method": "N/A (validator rather than generator); it generates validation scores by embedding-based retrieval of evidence and NLI inference",
            "validation_method": "Embedding-based retrieval of context passages from cited documents followed by a fine-tuned DeBERTa NLI classifier returning normalized entailment probabilities; ambiguous 'neutral' cases further evaluated by a large LLM (Qwen2.5-72B-Instruct) to decide citation validity.",
            "generation_performance": "N/A (not a generator); used within SGT-MCTS to compute Rattr rewards that influence generation.",
            "validation_performance": "On the 71k fact verification dataset: fact-missing rate 4.75% and accuracy 95.60% (reported as RA-NLI (Ours) in Table 2), outperforming many standalone LLMs and LLMs with 'factual supplement'.",
            "false_positive_rate": "Not explicitly reported. The paper provides overall accuracy and 'fact missing rate' but does not report FPR/FNR per class.",
            "false_negative_rate": "Not explicitly reported; 'fact missing rate' is given (4.75%) but is not a direct per-class FNR.",
            "performance_vs_novelty": "No explicit per-novelty breakdown. RA-NLI is designed to reduce hallucinations on citation-linked claims; authors claim it substantially reduces fact-missing even for diverse papers, but explicit degradation with novelty not quantified.",
            "generation_validation_comparison": "RA-NLI is positioned as improving validation relative to LLM-only verification; the paper reports large reductions in fact-missing compared to standalone LLM verifiers (Table 2).",
            "uncertainty_quantification": "NLI returns normalized probability scores [0,1]; RA-NLI composes NLI(si,sj) and LLMeval(si,sj,C) with weighting α=0.7 to compute Rattr. No calibration statistics reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported specifically; evaluation corpus covers 27k papers and 71k verification items, but explicit OOD performance claims are not provided.",
            "validation_proxy_metrics": "Relies on entailment probability and LLMeval score as proxies for validity; also uses fact-missing rate and overall accuracy to evaluate performance.",
            "human_validation_required": true,
            "human_validation_frequency": "Used in expert refinement of trees and for ground-truth construction; recommended especially for ambiguous/neutral NLI outputs and for final dataset curation.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / textual (scientific publications); not a formal domain.",
            "gap_mitigation_strategies": "Combines retrieval, a domain-fine-tuned NLI model (DeBERTa), and an LLM fallback to handle ambiguous/neutral cases; this hybrid approach is presented as effective at reducing hallucinations and fact-missing.",
            "evidence_supporting_gap": "Direct comparison table (Table 2) shows many LLMs with high fact-missing rates (40–69%) and modest accuracies; RA-NLI reduces fact-missing to 4.75% and yields 95.6% accuracy, supporting that improved retrieval+NLI validation is needed to close the generation-validation gap.",
            "evidence_contradicting_gap": "No systematic contradictory evidence presented; some strong standalone models exist (e.g., DeepReviewer-7B) but RA-NLI still outperforms them on fact-missing and accuracy.",
            "computational_cost_ratio": "Not numerically specified; RA-NLI involves retrieval, fine-tuned model inference, and LLM fallback which increases computation compared to a single LLM pass. Authors note computational cost is substantial overall.",
            "uuid": "e2072.1"
        },
        {
            "name_short": "TVCV",
            "name_full": "Think-Verbalize-Cite-Verify",
            "brief_description": "A four-step LLM-guided node expansion protocol where the model: (1) Thinks of candidate advancements, (2) Verbalizes concise proposition statements, (3) Cites supporting literature to ground the proposition, and (4) Verifies the proposed relationship via RA-NLI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TVCV (Think-Verbalize-Cite-Verify)",
            "system_type": "LLM-guided procedural prompting/workflow for generation+grounding",
            "scientific_domain": "scientific hypothesis/advance generation and grounding in literature",
            "output_type": "candidate technological advancements (propositions), citation-grounded proposed edges (parent→child)",
            "novelty_level": "methodological (process to produce grounded candidate advances rather than producing raw novel scientific claims); enables moderately novel recombinations by LLMs but grounded via citations",
            "generation_method": "LLM-generated candidate advancements informed by the current path and domain knowledge (Think and Verbalize), followed by retrieval/selection of supporting papers (Cite).",
            "validation_method": "Proposed links are validated by RA-NLI (Verify) which retrieves evidence and applies NLI and LLM-based refinement.",
            "generation_performance": "Used within SGT-MCTS to produce candidate nodes; specific generation success rates are not separately reported, but overall tree construction metrics (node recall/precision, Hit@k improvements) reflect combined TVCV+MCTS performance.",
            "validation_performance": "Validation of TVCV-proposed links is performed by RA-NLI; RA-NLI metrics (fact-missing 4.75%, accuracy 95.60%) apply. No separate TVCV-only validation numbers provided.",
            "false_positive_rate": "Not explicitly reported for TVCV proposals. The paper reports that RA-NLI reduces fact-missing and improves accuracy, implying TVCV proposals are substantially filtered.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Not quantified. The workflow is explicitly designed to ground proposed novel ideas in citations to mitigate hallucination, but per-novelty effect sizes are not given.",
            "generation_validation_comparison": "TVCV is explicitly designed to tie generation with immediate validation (Verify) to reduce the gap; reported system-level improvements suggest this reduces invalid proposals, though separate numeric comparisons are not provided.",
            "uncertainty_quantification": "Not reported beyond RA-NLI scores returned in Verify step.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported.",
            "validation_proxy_metrics": "Uses RA-NLI entailment scores and retrieval evidence as proxies for link validity; DPO and TemporalCoherence used for generation scoring.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts used for ground-truth refinement; TVCV outputs are intended to be audited and RA-NLI still used as automatic guardrails. Frequency: unspecified but implied when high novelty or ambiguity arises.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical textual domain; TVCV operates on textual claims and citation evidence.",
            "gap_mitigation_strategies": "In-line verification (Verify) of every LLM-proposed link using RA-NLI; combining retrieval, NLI, temporal coherence penalties, and expert-starting points (surveys) to reduce spurious generation.",
            "evidence_supporting_gap": "Paper describes LLM hallucination and high fact-missing rates (~60%) when used alone; TVCV+RA-NLI pipeline reduces these errors as shown by RA-NLI metrics and overall improvement in downstream tasks.",
            "evidence_contradicting_gap": "No direct contradiction presented; specific per-component failure rates for TVCV are not provided.",
            "computational_cost_ratio": "Not provided numerically; TVCV requires LLM proposals and retrieval+NLI validation per candidate, increasing compute compared to single-pass generation.",
            "uuid": "e2072.2"
        },
        {
            "name_short": "SGT-MCTS",
            "name_full": "Self-Guided Temporal Monte Carlo Tree Search",
            "brief_description": "A Monte Carlo Tree Search variant that incorporates LLM-derived priorities and temporal coherence into node selection and expansion to search for high-reward historical evolutionary paths across literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SGT-MCTS",
            "system_type": "reinforcement/search algorithm (MCTS variant) augmented with LLM priorities",
            "scientific_domain": "search over hypothesis/history/path spaces in scholarly literature",
            "output_type": "candidate technology evolutionary paths and tree expansions (candidate nodes/edges)",
            "novelty_level": "methodological; combines classic MCTS with LLM guidance to explore novel combinatorial paths (moderately novel)",
            "generation_method": "UCT-based tree search with added LLMpriority(v) and TempCoherence(v|Pprev); generation reward uses DPO approximated by LLM and temporal coherence penalties.",
            "validation_method": "Expanded nodes/edges are validated via TVCV's Verify step (RA-NLI) which provides Rattr used in reward backpropagation.",
            "generation_performance": "Used to construct THE-Trees showing node recall 0.84, precision 0.67, F1 0.75 (entities) and relation recall 0.78, precision 0.64, F1 0.70 vs expert ground truth; improved Hit@k and rank metrics on graph completion and future prediction tasks vs citation graphs.",
            "validation_performance": "Validation is external to SGT-MCTS (RA-NLI); SGT-MCTS relies on Rattr scores to guide search. No separate validation performance reported for SGT-MCTS itself beyond integrated tree metrics in Table 3.",
            "false_positive_rate": "Not reported separately; overall precision numbers (entities precision 0.67, relations precision 0.64) imply some false positives in generated nodes/edges but exact FPR not given.",
            "false_negative_rate": "Not reported; entity recall 0.84 indicates some missed nodes.",
            "performance_vs_novelty": "Not explicitly measured; temporal coherence and LLMpriority aim to favor plausible continuations but no per-novelty performance curve provided.",
            "generation_validation_comparison": "SGT-MCTS emphasizes generation guided by validation-aware rewards (Rattr), explicitly coupling generation to validation scores to reduce invalid proposals; integrated metrics show improved reconstruction vs baselines.",
            "uncertainty_quantification": "Not reported for SGT-MCTS; search uses reward estimates (Q/N) but no explicit uncertainty outputs.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not reported distinctly.",
            "validation_proxy_metrics": "Uses composite reward S(v) + Rgen + Rattr where Rattr is RA-NLI score and Rgen uses DPO and temporal coherence as proxies for path validity.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts used to refine outputs and build ground truth; frequency unspecified.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical/textual scientific literature",
            "gap_mitigation_strategies": "Incorporate validation reward (Rattr) from RA-NLI into the MCTS reward to bias search towards evidentially-supported paths; also use temporal coherence to avoid anachronistic proposals.",
            "evidence_supporting_gap": "Authors motivate SGT-MCTS by noting that standard citation graphs and LLM-only search produce noisy or hallucinated links; combined guidance + RA-NLI yields improved reconstruction metrics.",
            "evidence_contradicting_gap": "No explicit contradiction; SGT-MCTS precision&lt;1 indicates generation still produces some invalid items requiring curation.",
            "computational_cost_ratio": "Not quantified; inclusion of LLM guidance and RA-NLI validation increases cost per expansion relative to vanilla MCTS.",
            "uuid": "e2072.3"
        },
        {
            "name_short": "LLMs (standalone verifiers/generators)",
            "name_full": "Large Language Models used as hypothesis generators and verifiers",
            "brief_description": "General category of pretrained LLMs (e.g., GPT-4o, Qwen2.5, Claude-3.5) used to generate hypotheses, propose advances, and (when used naively) to verify claims, but which the paper finds prone to hallucination and incomplete domain awareness.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Large Language Models (e.g., GPT-4o, Qwen2.5-72B, Claude 3.5 Sonnet, DeepReviewer-* )",
            "system_type": "large language model / transformer",
            "scientific_domain": "hypothesis generation, paper evaluation, idea synthesis across scientific domains",
            "output_type": "scientific hypotheses, paper reviews/assessments, candidate technological advancements, natural language summaries",
            "novelty_level": "varies from in-distribution recombination to moderately novel suggestions; paper reports that LLMs can generate many ideas but with variable novelty and factual grounding",
            "generation_method": "pattern extrapolation and recombination from training corpora; prompted generation and in some systems used with chain-of-thought or reasoning scaffolding",
            "validation_method": "When used naively, validation is via their own internal confidence or judgement; the paper contrasts naive LLM-only verification with RA-NLI retrieval+NLI validation and expert review.",
            "generation_performance": "Not reported as a single metric; in downstream tasks LLM-augmented baselines show various performances (Table 1 shows acceptance/status prediction accuracies for models: e.g., Qwen2.5-72b-instruct reported Acc% etc.). The paper states LLMs accelerate idea generation but often produce numerous superficial propositions.",
            "validation_performance": "Table 2 reports fact-missing rates and accuracy for several LLMs when used for verification: Claude 3.5 Sonnet fact-missing 47.93% accuracy 60.22%; GPT-4o fact-missing 58.19% accuracy 60.18%; Qwen2.5 fact-missing 58.29% accuracy 53.95%; DeepSeek-R1 fact-missing 48.16% accuracy 76.40%; DeepReviewer-7B fact-missing 40.88% accuracy 93.95%; DeepReviewer-14B fact-missing 68.84% accuracy 76.41; LLaMA 3.1 fact-missing 42.29% accuracy 60.40. Factual supplementation improves these metrics but RA-NLI outperforms them.",
            "false_positive_rate": "Not explicitly reported per model in terms of FPR; only 'fact missing rate' and accuracy are reported. The paper does not provide explicit false-positive counts.",
            "false_negative_rate": "Not reported explicitly.",
            "performance_vs_novelty": "Authors note standalone LLM verifiers are especially weak for novel or domain-specific verification tasks (e.g., '60% unawareness of relevant papers in specific domains'), implying validation accuracy degrades with increased novelty, but no numeric per-novelty curve is provided.",
            "generation_validation_comparison": "Paper emphasizes a gap: generation (idea production) is prolific, but validation (truthful grounding) by standalone LLMs is poor and prone to hallucination; RA-NLI and THE-Tree aim to close this gap.",
            "uncertainty_quantification": "LLM internal confidences are not systematically used or calibrated in the paper; authors report using LLM-derived scores (DPO approximations, LLMpriority) as heuristics but provide no calibration metrics.",
            "calibration_quality": "Not reported; the paper explicitly criticizes LLMs for superficial plausibility assessments and potential overconfidence.",
            "out_of_distribution_performance": "Indicated as poor for verification tasks (high fact-missing/unawareness rates), but specific OOD metrics not given.",
            "validation_proxy_metrics": "Standalone LLM verification often relies on proxies like textual plausibility, coherence, or internal entailment judgments; the paper flags this as inadequate compared to retrieval+NLI.",
            "human_validation_required": true,
            "human_validation_frequency": "Authors state manual verification is ideal but too slow; recommend human oversight particularly for highly novel outputs and for constructing ground-truth trees. Frequency unspecified but higher for novel outputs.",
            "formal_verification_used": false,
            "domain_formalization_level": "varies by application; for scholarly text it's empirical/semi-formal.",
            "gap_mitigation_strategies": "Factual supplementation, augmenting LLMs with retrieval, coupling with RA-NLI, incorporating THE-Tree historical grounding, and expert review.",
            "evidence_supporting_gap": "Authors' experiments: many LLMs show high fact-missing rates (40–69%) and limited accuracy on fact-verification tasks; authors claim ~60% unawareness of relevant papers in domains when LLMs are used as verifiers.",
            "evidence_contradicting_gap": "Some LLM-based systems (e.g., DeepReviewer-7B) show high verification accuracy in the table, indicating variability across models and that gap may not be uniform.",
            "computational_cost_ratio": "Not reported numerically. The paper emphasizes that augmentations (retrieval, RA-NLI, expert refinement) increase computational cost substantially.",
            "uuid": "e2072.4"
        },
        {
            "name_short": "DeepReviewer-14b (case)",
            "name_full": "DeepReviewer-14B (LLM used for paper evaluation)",
            "brief_description": "A 14-billion parameter LLM used as a baseline reviewer in NeurIPS 2024 evaluation experiments; when augmented with THE-Tree it showed improved accuracy in acceptance/status prediction and better identification of high-impact papers.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "DeepReviewer-14B",
            "system_type": "large language model (14B), LLM-based reviewer",
            "scientific_domain": "automated paper review / acceptance prediction",
            "output_type": "paper acceptance/rejection decisions, predicted status (Poster/Spotlight/Oral), review text",
            "novelty_level": "in-distribution to moderately novel (reviewing new submissions using learned review patterns)",
            "generation_method": "Standard LLM inference with domain-specific prompting (DeepReviewer architecture described in related works)",
            "validation_method": "Standalone LLM judgment when unaugmented; when augmented, uses THE-Tree retrospection paths to ground judgments and RA-NLI validated evidence to inform decisions.",
            "generation_performance": "Table 1: standalone DeepReviewer-14b on NeurIPS 2024 had various metrics (example: Accuracy of accept and reject: Acc% 92.61, Rej% 18.28, Total% 37.45? — table formatting is complex). With THE-Tree augmentation, performance shifts (DeepReviewer-14b_tree reported improved Total% and better status prediction). The paper reports that THE-Tree augmentation 'substantially boosts' DeepReviewer-14b's recognition of high-impact papers, in some instances nearly doubling accuracy for those categories.",
            "validation_performance": "Standalone DeepReviewer-14b fact-missing (from Table 2-like comparisons) is reported among other models: DeepReviewer-14B fact-missing 68.84% accuracy 76.41 in the general fact-verification table; however, when augmented with THE-Tree, the model's paper-evaluation accuracy and ability to reject low-quality submissions improved (case studies and aggregate table entries show substantial gains).",
            "false_positive_rate": "Not explicitly reported. The table data does not provide a direct FPR for accept decisions.",
            "false_negative_rate": "Not explicitly reported.",
            "performance_vs_novelty": "Augmentation with THE-Tree improved assessment of high-impact / novel papers (e.g., better alignment with ground-truth Oral/Spotlight designations), indicating standalone model underestimates impact for some novel papers; precise per-novelty metrics are not provided.",
            "generation_validation_comparison": "Paper shows a concrete example where standalone DeepReviewer-14b accepted a paper but rated it as Poster, while THE-Tree-augmented version rated it Oral (closer to ground truth), illustrating a generation/assessment gap closed by grounding.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not separately quantified; case studies suggest improved performance on impactful (possibly more novel) papers when augmented.",
            "validation_proxy_metrics": "When augmented, uses THE-Tree retrospection paths and their validated evidence as proxies for historical novelty/impact assessment.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts used for ground truth and to analyze model augmentation effects; recommended for high-stakes decisions like acceptance.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical (conference review process)",
            "gap_mitigation_strategies": "Augmentation with THE-Tree retrospection paths and RA-NLI to ground review judgments; demonstrated improvement in case studies.",
            "evidence_supporting_gap": "Case studies and aggregate evaluation tables show THE-Tree changes model judgments toward expert ground truth (e.g., moving rating from Poster to Oral), supporting that standalone generation/assessment can be insufficient and benefits from historical grounding.",
            "evidence_contradicting_gap": "None provided specifically for DeepReviewer-14b; the model had non-trivial baseline performance, but augmentation improved alignment with experts.",
            "computational_cost_ratio": "Not provided numerically; augmentation entails additional retrieval and RA-NLI validation, increasing compute relative to standalone inference.",
            "uuid": "e2072.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CycleResearcher: Improving Automated Research via Automated Review",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 1
        }
    ],
    "cost": 0.02137775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?
26 Jun 2025</p>
<p>Xin Wang 
Westlake University</p>
<p>Shanghai Artificial Intelligence Laboratory</p>
<p>Jiyao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Innovation Institute</p>
<p>Yulong Xiao 
Fudan University</p>
<p>Junzhi Ning 
Shanghai Artificial Intelligence Laboratory</p>
<p>Fuzhou University</p>
<p>Lihao Liu 
Shanghai Artificial Intelligence Laboratory</p>
<p>Junjun He 
Shanghai Artificial Intelligence Laboratory</p>
<p>Botian Shi shibotian@pjlab.org.cn 
Shanghai Artificial Intelligence Laboratory</p>
<p>Shanghai Innovation Institute</p>
<p>Kaicheng Yu 
Westlake University</p>
<p>Shanghai Innovation Institute</p>
<p>Imperial College London Homepage Code</p>
<p>THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?
26 Jun 202566C5A59A3AFE29CA78407D661F6C391AarXiv:2506.21763v1[cs.AI]
Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow.Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured.This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this, we introduce THE-Tree (Technology History Evolution Tree), a computational framework that constructs such domainspecific evolution trees from scientific literature.THE-Tree employs a search algorithm to explore evolutionary paths.During its node expansion, it utilizes a novel "Think-Verbalize-Cite-Verify" process: an LLM proposes potential advancements and cites supporting literature.Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded.We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research.Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8% to 14% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100%.By constructing explicit, verifiable pathways of scientific progression, THE-Tree provides a robust historical foundation for evaluating new hypotheses (human or AI-generated) and enables a computable science history, fostering evidence-based AI-driven scientific discovery.</p>
<p>Introduction</p>
<p>Automating scientific discovery has been a long-standing goal [22,18].The recent rise of Large Language Models (LLMs) offers new avenues, with applications from hypothesis generation [35,5,3] to simulating autonomous AI scientists [8,6].However, a critical bottleneck remains: the effective evaluation and validation of scientific ideas, whether AI or human-generated.Current idea validation approaches face several critical challenges.First, manual verification, while ideal, is prohibitively time-consuming [28].Second, automated validation using LLMs [17,3] exhibits multiple limitations: (1) potential for hallucination and incomplete domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), (2) susceptibility to superficial textual features, often highly rating plausible but erroneous propositions [28,35], and (3) inheritance of biases when trained on human review data (e.g., CycleReviewer [34]).Third, existing knowledge representation methods are inadequate, as (1) citation networks [7,16] contain noise and lack explicit causal links, and (2) narrative surveys remain unstructured.These limitations fundamentally stem from the absence of structured, causally linked historical data, hindering reliable AI-driven idea validation.These challenges stem from a critical absence of structured, causally linked historical data, hindering reliable AI-driven validation of ideas.</p>
<p>To address this, we propose leveraging the authentic patterns and causal evolutionary pathways from scientific history for more reliable assessment.We introduce THE-Tree (Technology History Evolution Tree), a computational framework to construct structured, verifiable, domain-specific technology evolution trees from scientific literature (illustrated in Figure 1).THE-Tree builds a topic's evolution by representing individual papers as nodes and the inferential relationships between these papers, specific to the topic, as edges.This aims to provide a solid factual basis and clear historical context for evaluating new hypotheses.THE-Tree utilizes a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) and a novel Think-Verbalize-Cite-Verify (TVCV) methodology for node expansion.This process prompts an LLM to generate potential evolutionary steps (Think), summarize them concisely (Verbalize), ground them in specific supporting literature (Cite), and critically, validate the proposed relationship (Verify).Crucially, the 'Verify' step employs a Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism to assess the causal and logical coherence of proposed relationships based on cited evidence, ensuring semantic soundness and fidelity of the identified evolutionary relationships.Tree construction often starts from human-validated knowledge like scientific surveys, providing a reliable starting point.</p>
<p>We demonstrate THE-Tree's efficacy by constructing trees for 88 distinct topics across relevant scientific domains.Their quality, validated by automated metrics and human assessment, confirms their effectiveness in reconstructing meaningful and accurate technological trajectories.Downstream tasks, including future node prediction and graph completion, showcase THE-Tree's potential to enhance AI-assisted scientific reasoning by anticipating subsequent developments and highlight its superiority over traditional citation networks.For instance, in graph completion, it outperforms traditional citation networks (e.g., on hit@1 across all tested models); for future node and trajectory prediction, it improves hit@1 by nearly 10%; and in paper evaluation, it enhances the ability of other models to assess important papers by almost 100%.In summary, our main contributions are:</p>
<p>• A novel computational framework, THE-Tree, incorporating the TVCV methodology with LLMguided SGT-MCTS to construct and validate verifiable technology evolution trees from scientific literature, addressing the lack of structured, causal historical data for scientific evaluation.</p>
<p>• A RA-NLI mechanism within TVCV for rigorous validation of the logical and causal coherence of evolutionary relationships, ensuring tree fidelity.</p>
<p>• The construction and validation of THE-Trees dataset, comprising 88 technology evolution trees across AI domains and a benchmark dataset of 71k fact verification evaluations from 27k scientific papers, with extensive experiments demonstrating superior performance in downstream tasks.</p>
<p>• A structured, verifiable foundation based on historical evolution patterns for evaluating scientific ideas (both human and AI-generated) and supporting grounded AI-driven discovery processes, enabling systematic assessment of scientific progress.</p>
<p>Related Work</p>
<p>Our work intersects with several research areas, primarily AI for scientific discovery, scholarly knowledge representation, and the evaluation of scientific novelty.</p>
<p>AI for Scientific Discovery and Evaluation.The ambition to automate scientific discovery using AI has gained significant momentum with the rise of LLMs [19,20].Current applications range from hypothesis generation [35,5,3] to simulating autonomous AI agents for specific scientific tasks [8,6].A persistent challenge, as highlighted in our Introduction, is the rigorous evaluation of the novelty and feasibility of AI-generated outputs.Manual verification remains a bottleneck [13].While some emerging efforts aim to automate aspects of this evaluation, they often focus on simulating existing human processes [17,24,34].For instance, CycleResearcher [34] employs an LLM within an automated research-review loop to mimic peer review by predicting scores and providing feedback.</p>
<p>Although valuable for replicating current assessment workflows, such approaches primarily model established paradigms and may not provide the deep.Our work complements these efforts by focusing on constructing the underlying historical structure itself, offering a fact-based pathway for evaluation grounded in demonstrable scientific lineage.</p>
<p>Scholarly Knowledge Representation and Analysis.Representing and analyzing the vast body of scientific literature has long been a goal.Traditional bibliometric methods, including citation analysis [14,29] and co-word analysis [10], offer insights into publication impact and thematic trends.Science mapping tools like VOSviewer [32] and CiteSpace [11] provide valuable visualizations of research landscapes.However, as noted in our Introduction, these approaches face limitations.Citation networks are often noisy and fail to capture the explicit causal or logical dependencies signifying true intellectual inheritance [7], making them a "poor substrate for tracing idea lineage".Co-word analysis identifies term co-occurrence but not necessarily causal links.While useful for broad overviews, these methods generally lack the granularity and causal structure needed for deep reasoning about technological evolution or predictive analysis of research trajectories.</p>
<p>Scholarly Knowledge Graphs.More recently, large-scale scholarly knowledge graphs, such as the Microsoft Academic Knowledge Graph [33] and AMiner [30], have emerged, integrating diverse metadata.Knowledge graph construction techniques [2] have also been applied to scientific literature.While these graphs offer rich resources, they often focus on entity relationships (e.g., author collaborations, affiliations) or represent relatively static snapshots of knowledge domains.They typically do not explicitly model the dynamic, temporal, and causal evolutionary pathways of scientific ideas -how one concept or technology directly enables or influences the next.Capturing this validated, directed evolution is precisely the gap THE-Tree aims to fill.</p>
<p>THE-Tree as Scientific Verifier</p>
<p>THE-Tree: A Structured Representation of Scientific Evolution</p>
<p>The pursuit of scientific discovery is increasingly aided by AI, yet verifying the novelty and validity of numerous AI-generated or human-conceived hypotheses presents a significant bottleneck, as highlighted in our Abstract and Introduction.Traditional methods like citation networks lack the necessary semantic depth (i.e., the relationship between nodes is often limited to a simple "cite" declaration), offering only noisy and superficial links (e.g., perfunctory citations, negative citations, or citations to general background rather than specific conceptual building blocks), while LLMs as standalone verifiers can hallucinate or miss crucial domain knowledge (our experiments indicate that large models may exhibit factual omissions or fabrications in nearly 60% of cases when used for direct verification).This underscores the urgent need for a structured, verifiable, and causally-linked representation of scientific evolution.</p>
<p>To address this challenge, we introduce the THE-Tree (Technology History Evolution Tree).In a THE-Tree, each scientific paper is conceptualized as a node.Each node encapsulates rich metadata crucial for understanding the paper's context, contribution, and significance.This information typically includes the paper's title, abstract, authors, publication venue, and publication year.Furthermore, each node v is associated with a computationally derived importance score S v , reflecting its relevance and impact within the specific domain, as will be detailed in Section 3.3.1.</p>
<p>The edges in a THE-Tree represent the historical, inferential, and evolutionary relationships between these paper nodes (see Figure 3).</p>
<p>Unlike traditional citation networks, where an edge might merely indicate a citation without specifying the nature of the relationship, edges in THE-Tree are imbued with deeper semantic meaning.They are constructed to signify how one paper (or the ideas and technologies presented therein) causally contributes to, logically enables, or provides an essential foundation for the advancements detailed in a subsequent, connected paper.This distinction is critical.While traditional citation links are valuable, they are often too simplistic or noisy (e.g., perfunctory citations, negative citations, or citations to general background rather than specific conceptual building blocks) to accurately map the nuanced, multi-step evolution of scientific ideas.Our methodology, further detailed in Section 3.3.1 (referring to SGT-MCTS and TVCV parts), therefore focuses on identifying and establishing edges that reflect substantive intellectual lineage and direct technological dependence, aiming to filter out superficial connections and capture the true pathways of innovation.THE-Tree thus facilitates a more evidence-based approach to scientific idea validation.</p>
<p>The primary purpose of constructing and utilizing THE-Trees is to provide a structured, verifiable, and causally-linked historical tapestry of scientific evolution for specific domains.This detailed, graph-based representation serves as a robust knowledge scaffold.It allows new scientific propositions-whether generated by humans or AI-to be situated within an explicit, evidence-backed evolutionary context.By tracing connections and analyzing pathways within the THE-Tree, we can more rigorously assess a new idea's novelty (i.e., does it genuinely extend or diverge from known paths?), its factual consistency with established knowledge, and its potential impact, thereby addressing the limitations of standalone LLM evaluators and often unstructured narrative surveys.THE-Tree thus facilitates a more evidence-based approach to scientific idea validation.</p>
<p>Leveraging THE-Tree for Scientific Idea Verification</p>
<p>Once a THE-Tree, with its richly annotated nodes and semantically meaningful edges, is constructed for a specific scientific domain, it serves as a powerful instrument for the verification and contextualization of new scientific ideas or papers.We propose a straightforward yet effective methodology to utilize THE-Tree for this purpose, enabling the retrospection of relevant historical evolutionary paths for a given input scientific paper, P in , defined by its title T in and abstract A in .This approach provides critical context by situating new research within established knowledge frameworks, thereby aiding in the assessment of its novelty and potential contribution.</p>
<p>The core steps of this verification and retrospection process are as follows:</p>
<ol>
<li>Initialization: This methodology assumes access to:</li>
</ol>
<p>• A collection of pre-computed THE-Trees {G k }, where each G k = (V k , E k ) corresponds to a specific scientific topic T opic k .Nodes v ∈ V k represent scientific papers with attributes such as publication year Y v , title T v , abstract A v , and an importance score S v , as defined in Section 3.1.Edges e ∈ E k signify directed evolutionary relationships, capturing inferential and developmental dependencies.</p>
<p>• A Large Language Model (LLM) for semantic tasks such as similarity assessment and topic matching.They can be ranked based on various criteria, such as the semantic similarity score Sim(P in , v * term ) of their terminal nodes to the input paper P in , the cumulative importance of nodes in the path, or the overall coherence of the path.The top N P unique paths are selected and formatted for presentation.This presentation details the sequence of papers, their key attributes (title, year, summary of contribution derived from abstract/node info), and the nature of the connecting evolutionary relationships.</p>
<p>This methodology provides a simple yet powerful way to leverage the structured knowledge within THE-Trees to verify a new scientific idea by exploring its historical context and connections to established research.The retrieved paths can highlight the foundations upon which P in builds, identify potentially overlooked prior art, or help assess its incremental novelty versus a more radical departure from existing trajectories.Users can adapt this general approach based on specific analytical needs, such as modifying the LLM prompts, similarity thresholds, path selection heuristics, or the depth of retrospection.</p>
<p>Automated Construction of THE-Tree</p>
<p>The manual construction of comprehensive and accurate THE-Trees for diverse scientific domains would be a prohibitively laborious task.Therefore, we develop a computational framework for the automated construction of THE-Trees from scientific literature.Our approach formulates this construction as an optimization problem: the goal is to identify and assemble evolutionary paths through the literature that maximize a composite reward.This reward is designed to reflect the significance of the constituent papers (nodes) and the logical coherence and evidential support of the evolutionary steps (edges) they represent:
max Path v∈Path S(v) + R gen + R attr ,(1)
where S(v) is the importance score of a node (paper) v (see Section 3.3.1 for how S(v) is determined), R gen is the generation process reward reflecting the coherence of the path (how well a new node continues an existing path), and R attr is the attribution process reward validating the evidential support for the link (edge) between connected nodes.This objective is pursued using a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) algorithm, as detailed in the pipeline stages below.</p>
<p>THE-Tree Construction Pipeline Overview</p>
<p>The construction of a THE-Tree involves several key stages, from initial data preparation to the iterative refinement of the tree structure.Figure 3 provides a schematic overview of this pipeline.Dataset Construction: Initialization and Foundation.The journey of constructing a THE-Tree begins with scientific surveys.These documents are invaluable as they encapsulate rich historical and inferential information about scientific progression, making them an ideal starting point for tracing technological evolution.However, directly utilizing raw surveys presents challenges due to their predominantly unstructured narrative format, lacking the explicit relational information needed for a computational model.To address this, our first crucial step is to transform these surveys into a structured dataset.We strategically select surveys from diverse time periods for each topic to mitigate recency bias and capture a broader, less noisy historical perspective.The core output of this stage is an interconnected dataset comprising survey documents, identified paper nodes (from citations within the surveys), concept nodes (core ideas extracted from paragraphs), and their explicit relationships.This structured foundation overcomes the limitations of raw narratives and noisy citation networks, enabling the detailed construction of technology evolution histories.Further details on this data construction pipeline are elaborated in Appendix A.2.</p>
<p>Core Component: Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS).With the structured dataset (Section 3.3.1)as our bedrock, we employ a Self-Guided Temporal Monte Carlo Tree Search (SGT-MCTS) [9] algorithm to navigate the vast space of potential technological evolutionary paths and construct the THE-Tree.SGT-MCTS iteratively builds the tree by strategically selecting nodes for expansion, simulating potential future paths, and backpropagating rewards to update the value of explored paths (see Algorithm 1).The search is guided by the composite reward function introduced earlier (Equation 1: S(v) + R gen + R attr ), which balances node importance with path coherence and link validity.</p>
<p>Central to the SGT-MCTS guidance are the node importance assessment and generation process reward:
LLM-Enhanced Node Importance Assessment (S(v)):
The importance of a paper (node v) is a weighted combination of its structural significance within the citation graph and its semantic relevance as assessed by an LLM:
S(v) = γ • S graph (v) + (1 − γ) • S LLM (v)
, where γ is a weighting factor.S graph (v) combines multiple centrality measures (PageRank, citation count, Degree, Betweenness, Eigenvector Centrality) with dynamic weighting.S LLM (v) is obtained by prompting an LLM to assess the paper's importance within the specified topic.</p>
<p>Generation Process Reward (R gen ): This reward encourages the formation of coherent and temporally sound evolutionary paths.It is defined for a node v given a preceding path P prev : R gen (v|P prev ) = DPO(v|P prev ) • TemporalCoherence(v|P prev ).The DPO score [27], approximated by an LLM, evaluates how well node v continues the trajectory of P prev .The Temporal Coherence term penalizes achronological or large time gaps.The SGT-MCTS UCT formula is enhanced with LLM guidance and temporal coherence:</p>
<p>The selection of nodes during the SGT-MCTS process is guided by an Upper Confidence Bound 1 applied to Trees (UCT) formula [21].Our SGT-UCT variant, presented below, incorporates LLM guidance and temporal coherence to refine this selection:
SGT-UCT(v) = Q(v) N (v) + c • ln N (p) N (v) + λ • LLMpriority(v) • TempCoherence(v|Pprev)(2)
In this formula (Equation 2), Q(v)/N (v) is the exploitation component (empirical mean reward of node v).</p>
<p>The term c • ln N (p)/N (v) is the exploration component, encouraging visits to less explored nodes (N (p) is parent visit count, N (v) is current node visit count).LLMpriority(v) injects semantic guidance from an LLM.The entire expression is modulated by TempCoherence(v|Pprev) to enforce chronological path consistency.Constants c and λ are balancing hyperparameters.</p>
<p>We also implement reward-sensitive exploration where the exploration coefficient c(t) increases with the average reward, encouraging broader exploration in promising regions.</p>
<p>Node Expansion Method: Think-Verbalize-Cite-Verify (TVCV) Methodology.Node expansion within SGT-MCTS, the process of adding new paper nodes and establishing connections (edges with rich semantic meaning as discussed in Section 3.1), is performed using our novel Think-Verbalize-Cite-Verify (TVCV) methodology (see Algorithm 2).This process leverages an LLM to systematically generate, ground, and validate new nodes (potential technological advancements) and their links within the tree:</p>
<p>• Think: The LLM generates candidate technological advancements or scientific contributions that could logically follow from the current path history and domain knowledge.</p>
<p>• Verbalize: The LLM summarizes these generated ideas into concise statements or propositions that represent potential new nodes.</p>
<p>• Cite: For each summarized proposition, the LLM retrieves or identifies specific supporting scientific literature (i.e., existing paper nodes from the dataset or newly found papers) that grounds the proposed advancement, thereby proposing a potential link between an existing paper node and a new one.</p>
<p>• Verify: The proposed relationship (edge) between the current path's terminal node and the newly cited paper node, along with the relevance of the new node itself, is rigorously validated for logical consistency, causal coherence, and temporal soundness.This crucial step ensures the factual and logical soundness of the link, and is performed by the RA-NLI mechanism described in Section 3.3.1.1, is performed by our Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism.RA-NLI is designed to rigorously assess the causal and logical coherence of the evolutionary link (edge) proposed between a parent node (e.g., vparent) and a newly cited child node (v),
Rattr(vparent → v) = RA-NLI(sparent, sv)(3)
This mechanism (illustrated in Figure 4) integrates embedding-based retrieval to fetch relevant contextual passages from the cited documents, followed by fine-grained Natural Language Inference using a specialized model (fNLI) to determine if the textual description of the child node (sv) is entailed, contradicted by, or neutral with respect to the parent node's description (sparent) and supporting evidence.An LLM-based verifier (fLLM) further refines ambiguous cases.This ensures semantic soundness and high fidelity for the identified evolutionary relationships, forming the backbone of the THE-Tree's verifiability.</p>
<p>Experiments and Results</p>
<p>For detailed information on dataset construction, statistics, and the definitions of evaluation metrics, please refer to Appendix B.1 and Appendix B.3 respectively.</p>
<p>THE-Tree Verifier for Scientific Evaluation</p>
<p>Further experiments are conducted to investigate whether THE-Tree can enhance the verification capabilities of Large Language Models (LLMs) for evaluating scientific claims and research contributions.We employ a straightforward methodology, leveraging the structural and semantic information within THE-Trees to augment LLM-based assessments.This approach is detailed in Appendix C.These investigations focus on two primary scenarios:</p>
<p>• Assessing Paper Acceptance with Factual Grounding: We evaluate whether the factual basis provided by THE-Tree can assist in determining if a paper merits acceptance.To avoid potential data contamination, as our THE-Tree might include previously published conference papers, the experiments involving THE-Tree augmentation were conducted exclusively on submissions to NeurIPS 2024.The core idea is to assess if grounding a paper's claims and contributions within the historical and causal context of a THE-Tree correlates with acceptance decisions.• Identifying High-Quality Papers Among Accepted Submissions: For papers that are accepted, we further investigate if THE-Tree can aid in distinguishing truly high-impact or high-quality research from other accepted works.This involves analyzing whether deeper integration or stronger alignment with the evolutionary trajectories and validated knowledge within THE-Tree can serve as an indicator of superior quality among the pool of accepted papers.The experimental results from the NeurIPS 2024 dataset for THE-Tree augmented evaluations are presented in Table 1.These findings indicate that by providing the scientific evolutionary context, THE-Tree significantly enhances the LLM's capability to determine paper acceptance.This augmentation notably improves the model's ability to reject low-quality submissions, rather than indiscriminately accepting them.Furthermore, THE-Tree augmentation proves highly effective in identifying high-impact papers , Orals, Spotlights.This enhancement substantially boosts the LLM's recognition capability for such papers, in some instances nearly doubling the accuracy or pushing it towards perfect identification in specific high-impact categories.For a detailed analysis and case study on the enhancement provided by THE-Tree, particularly for the DeepReviewer-14b model, please refer to Appendix C.1.In our RA-NLI-based validation system, we compared its accuracy and fact consistency against several state-of-theart models, both with and without factual supplementation.</p>
<p>THE-Tree</p>
<p>As shown in Table 2, our method demonstrates significantly lower fact-missing rates and the highest overall accuracy.</p>
<p>To evaluate the fact missing rate and NLI validation capability of our RA-NLI system, we conducted experiments on a dataset of 71k fact verifications covering 27k papers.This dataset was extracted from scientific papers and includes fact titles along with their corresponding evidence content from the original text.We compared RA-NLI's accuracy and fact consistency against several state-of-the-art models, both with and without factual supplementation.As shown in Table 2, our method demonstrates significantly lower fact-missing rates and the highest overall accuracy.</p>
<p>The MCTS-generated THE-Trees are compared against this ground truth using several quantitative metrics; these metrics are detailed in Appendix B.3.Table 3 summarizes these quantitative results, demonstrating that the MCTS-generated THE-Trees achieve strong performance in recalling entities and relations validated by experts, with reasonable precision and F1-scores.The average time difference in years for entity reconstruction is also comparable.</p>
<p>Structural and Semantic Properties: Graph Completion</p>
<p>We evaluated THE-Tree's ability to represent scientific knowledge structures via a graph completion task (Table 4).By using the graph completion method from [23], this task involved predicting missing evolutionary entities within our THE-Tree by masking entities from a year.The entities and relations from before that year, as historical information, were then used to predict the masked entities and their relations.This process benchmarked whether our THE-Tree could capture richer structural and semantic information compared to traditional citation-based networks.As detailed in Table 4, THE-Tree robustly outperformed traditional citation graphs, particularly in models with larger sizes, such as Qwen2.5-72b.This was evidenced by consistently higher prediction accuracy across metrics from Hit@1 through Hit@5, along with significantly improved mean reciprocal ranks (MRR) and notably lower median and mean ranks (MR).These results highlight THE-Tree's superior efficacy in modelling latent knowledge structures between entities.</p>
<p>Model</p>
<p>Hit@1 (↑) Hit@2 (↑) Hit@3 (↑) Hit@4 (↑) Hit@5 (↑) MR (↓) MRR (↑) MedianRank (↓)</p>
<p>Graph built with Traditional Citations Relations</p>
<p>Qwen2. 5</p>
<p>Future Path and Trajectory Prediction</p>
<p>To assess THE-Tree's proficiency in capturing scientific evolutionary dynamics, we conducted a future path prediction task.The experiment aims to validate THE-Tree's depth in modeling historical evolution and its capability to forecast 'reasonable next steps' in research trajectories, rather than claiming precise scientific discovery.Performance is evaluated using standard metrics (see Appendix B.3).The task involves predicting future entities (e.g., papers or concepts) and the semantic relations that lead to them, given a THE-Tree constructed with data up to year Y .This forecasting is benchmarked by comparing our semantically-enriched THE-Tree against a traditional citation-only graph, with results presented in Table 5.The citation-only graph shows limited predictive power (Hit@1 ≈10-18%, MR ≈ 4.5), underscoring the inadequacy of relying solely on citation topology for foresight.In contrast, THE-Tree, with its rich, context-aware semantic relations, demonstrates markedly superior performance.It achieves substantial gains, increasing Hit@3 and Hit@5 by approximately 5-10 percentage points, and significantly reduces both Mean Rank (MR) and Median Rank for entity and relation predictions alike.This consistent improvement across metrics highlights that the semantic signals encoded by THE-Tree, not just its structural density, are pivotal for sharper anticipation of research trajectory extensions.These findings validate the significant value of THE-Tree's semantic knowledge for scientific foresight and path discovery tasks.Model Graph Entity Relation Hit@1 (↑) Hit@3 (↑) Hit@5 (↑) (↓) MedianRank (↓) Hit@1 (↑) Hit@3 (↑) Hit@5 (↑) MR (↓) MedianRank (↓) Qwen2.5</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we introduced the Technology Evolution Tree (THE-Tree), a novel computational framework for constructing structured, verifiable, and causally-linked representations of scientific and technological evolution from literature.THE-Tree addresses critical challenges in evaluating scientific ideas, particularly those generated by AI, by moving beyond traditional bibliometrics and ungrounded LLM evaluations.</p>
<p>Our core contributions include: 1) A robust methodology integrating SGT-MCTS with TVCV and RA-NLI for verifiable tree construction.2) A novel dataset of 88 THE-Trees covering diverse AI topics.3) Demonstrated utility through quantitative evaluations, qualitative studies, and downstream tasks, showing THE-Treeś effectiveness in representing scientific knowledge and predicting its trajectory.</p>
<p>While THE-Tree offers a significant advancement, key limitations that guide our future work include:</p>
<p>• Data Coverage and Bias: Reliance on primarily English-language peer-reviewed literature.</p>
<p>• Granularity and Scope: Subjectivity in concept definition and a yearly temporal resolution for analysis.</p>
<p>• Validation Complexity: Inherent complexity of validating scientific evolution without a universal "gold standard".</p>
<p>• Computational Cost: Significant computational demands from extensive LLM usage and large-scale graph processing.</p>
<p>Future work will focus on addressing these limitations and expanding THE-Treeś capabilities.Key directions include:</p>
<p>• Enhancing Data Coverage and Representation: Broadening data sources (e.g., non-English literature, patents, technical reports), refining temporal granularity, and improving representation of diverse research outcomes (e.g., negative results).</p>
<p>• Improving Validation and Reasoning: Developing advanced automated validation techniques (e.g., incorporating causal inference), quantifying uncertainty in evolutionary links, and enhancing reasoning capabilities, particularly for understanding underexplored or abandoned research paths.</p>
<p>• Expanding Applications and Usability: Creating user-friendly interactive visualization tools, integrating THE-Tree insights into scientific workflows, refining predictive models, and developing cross-disciplinary analysis tools.</p>
<p>By providing a structured, dynamic, and causally validated map of scientific progress, THE-Tree offers a more reliable foundation for understanding scientific evolution, evaluating new ideas, and guiding future research.We believe THE-Tree represents a significant step towards a more transparent, evidence-based, and computationally-assisted approach to navigating and shaping the future of science and technology.The processing pipeline applied to these curated surveys involves the following key steps designed to extract structured information:</p>
<p>A.2 Detailed Dataset Construction from Surveys for THE-Trees</p>
<ol>
<li>
<p>Document Processing and Metadata Extraction: Each survey document (typically PDF) is parsed.Metadata for the survey itself is extracted.Crucially, its reference list is parsed to create initial 'paper nodes' for each cited work, populated with available metadata (title, authors, year, etc.).The survey text is segmented into paragraphs and sentences.</p>
</li>
<li>
<p>Sentence-Citation Pairing: Sentences containing citations within the survey text are systematically identified.For sentences in the paper that originally contain citations, we do not directly use them as citation sentences; instead, we perform a series of post-processing steps on their factual content, including the removal of invalid facts.For each processed sentence, its textual content is extracted and explicitly linked to the corresponding cited paper node(s) (identified via citation markers like ( [12]).This step generates numerous '<citing sentence, cited paper>' pairs.These pairs form the basis for two critical downstream tasks: (a) creating a dataset for subsequent causal relationship validation (e.g., using NLI to assess if the citing sentence is supported by the cited paper's content); and (b) creating a dataset for evaluating the model's ability to accurately extract citation context.</p>
</li>
</ol>
<p>Paragraph-Level Concept Graph Construction:</p>
<p>To capture the substantive content of scientific contributions beyond simple entity mentions and address the limitations of relying solely on named entities for tracing the evolution of <em>ideas</em>, we construct a paragraph-level concept knowledge graph.</p>
<p>Traditional entity-based KGs can face challenges in citation alignment for our task: (a) Granularity Mismatch: A citation often supports a broader claim or methodology within a sentence/paragraph, not just a specific entity mentioned nearby.(b) Synonymy/Paraphrasing: The core scientific concept might be described using different terminology or entities across papers (or even within the same paper), potentially breaking entity-based links.(c) Implicit Concepts and Alignment Omission: Important ideas (e.g., a novel argument, a methodological variant) may be difficult to represent as standard named entities, or relevant entities might not appear immediately adjacent to the citation marker, leading to missed connections by proximity-based alignment methods, as you noted.</p>
<p>To overcome these issues, we employ NLP techniques (e.g., keyphrase extraction, relation extraction, or summarisation) to extract core scientific/technical concepts from each paragraph.A 'concept' here refers to a core idea, method, or finding, often represented as a phrase or concise statement, rather than just an isolated named entity.A 'concept node' is created for each extracted concept.Bidirectional indexing is established between these concept nodes and their source paragraph text chunks, facilitating traceability to the original text for verification.Crucially, if a concept is derived from a sentence that contains a citation, this 'concept node' is linked to the 'paper node(s)' referenced by that sentence.This approach allows us to directly connect the core <em>ideas</em> expressed in the literature to their claimed sources (cited papers), better capturing intellectual lineage even when specific entity mentions vary or are absent.</p>
<p>This pipeline yields a structured dataset comprising surveys, cited papers (with metadata), concepts, sentences, paragraphs, and explicit links representing relationships such as '<citing sentence, cited paper>', '<concept, paragraph>', and '<concept, cited paper>'.This rich dataset covers content related to up to 27k papers, with as many as 71k entries used for factual verification evaluation.It forms the foundation for the subsequent construction of detailed technology evolution histories using the THE-Tree framework (integrating SGT-MCTS and TVCV).</p>
<p>A.3 Model Architecture Details</p>
<p>Construction Model Implementation: Our technology tree construction model uses a multi-layer graph neural network with the following architecture:</p>
<p>• Node embedding layer: 768-dimensional vectors initialised from SciBERT [4] • Graph attention layers: 3 layers with 8 attention heads each</p>
<p>• Temporal encoding: Sinusoidal position encoding based on publication year</p>
<p>• Edge type encoding: Learned embeddings for different relationship types</p>
<p>Training was performed using Adam optimizer with a learning rate of 0.0001 and batch size of 32.We used early stopping with a patience of 10 epochs based on validation loss.</p>
<p>NLI Model Implementation:</p>
<p>To assess the textual entailment within scientific citations in RA-NLI, we employ a fine-tuned DeBERTa model specifically adapted for inference tasks in the scientific domain.The model categorizes the relationship between a citation claim (hypothesis) and its associated source content (premise) into one of the following three classes:</p>
<p>• Entailment: The premise logically supports or implies the claim.</p>
<p>• Contradiction: The premise directly contradicts the claim.</p>
<p>• Neutral: There is insufficient evidence to determine a clear inferential relationship.</p>
<p>In cases where the NLI model outputs a neutral label-indicating ambiguity or lack of strong inferential evidence-we introduce a secondary validation step using a large language model (Qwen2.5-72B-Instruct).This LLM-based evaluator conducts a more nuanced assessment by considering broader contextual and semantic factors, further determining whether the cited relationship constitutes a direct citation, a paraphrase, or no meaningful connection.</p>
<p>Our validation process can be formalized as follows:</p>
<p>For a given pair of technology nodes (vi, vj) where vi is claimed to influence vj, we compute:
Rattr(vi → vj) = α • NLI(si, sj) + (1 − α) • LLMeval(si, sj, C)
where si and sj are the textual descriptions of nodes vi and vj respectively, C represents the retrieved context from the literature, and α is a weighting parameter determined empirically (set to 0.7 in our experiments).</p>
<p>The NLI function returns a normalised score in the range [0, 1] based on the entailment probability, while LLMeval returns a similar score based on the LLM's assessment of the citation validity.</p>
<p>A. 4 The data structure of THE-tree B Supplementary Details for Experiments</p>
<p>B.1 Dataset Construction and Statistics</p>
<p>Following the methodology described in Section 3, we constructed a dataset comprising 88 THE-Trees.These trees cover distinct technological topics primarily within core AI and its applications across diverse scientific domains (e.g., Computer Science, Biomedicine, Materials Science).The 88 THE-Trees represent validated evolutionary trajectories within this broader knowledge space.</p>
<p>B.2 Expert-Refined Ground Truth Construction Methodology</p>
<p>Our primary benchmark consists of THE-Trees meticulously curated by domain experts.This process involved two main stages: 1) Initial Tree Generation by MCTS: Our self-guided temporal Monte Carlo Tree Search (MCTS) algorithm, incorporating the Think-Verbalize-Cite-Verify (TVCV) methodology with Retrieval-Augmented Natural Language Inference (RA-NLI), first generated initial THE-Trees for each topic.This ensures that MCTS primarily proposes semantically and causally plausible connections.2) Expert Refinement and  Avg/Topic values computed using harmonic mean.</p>
<p>Augmentation: Domain experts then reviewed these MCTS-generated trees, performing comprehensive modifications.This included validating, correcting, or removing paths and nodes; augmenting trees with crucial missing links, milestone papers, or overlooked developmental trajectories; and ensuring overall semantic coherence, causal validity, and accurate representation of the field's historical evolution.The resulting expert-curated THE-Trees form the ground truth dataset used for validation as described in the main experimental sections.</p>
<p>B.3 Evaluation Metrics Definitions</p>
<p>We employ a comprehensive evaluation framework to assess both the quality of constructed THE-Trees and their performance on downstream tasks.The primary quantitative metrics used for comparing MCTS-generated THE-Trees against ground truth, and for other evaluations, are defined below:</p>
<p>• • Metrics for Future Node Prediction and Graph Completion (e.g., Hits@k, MR, MRR): These standard link prediction metrics are used as described in the main text when evaluating future node prediction (Section 4.3) and graph completion (Section 4.2.2).Their standard definitions apply.• Overall Accuracy Metrics in NeurIPS Paper Evaluation (Total% in Table 1): The experiment table includes two "Total%" overall accuracy metrics, calculated as explained below:</p>
<p>-"Total%" in the "Accuracy of accept and reject" section: This metric is a weighted average of the model's accuracy in correctly predicting acceptances and rejections, based on the actual acceptance and rejection rates for that year.The formula is:</p>
<p>Total%accept/reject = P (Actual Accept) × Accuracy(Predicted Accept|Actual Accept) +P (Actual Reject) × Accuracy(Predicted Reject|Actual Reject)</p>
<p>Where P (Actual Accept) and P (Actual Reject) represent the actual proportion of accepted and rejected papers in that year's dataset, respectively.Accuracy(Predicted Accept|Actual Accept) is the accuracy of the model in predicting a paper as accepted, given it was actually accepted (corresponding to the "Acc%" column in the table).Accuracy(Predicted Reject|Actual Reject) is the accuracy of the model in predicting a paper as rejected, given it was actually rejected (corresponding to the "Rej%" column in the table).-"Total%" in the "Accuracy of Status" section: This metric comprehensively evaluates the model's overall accuracy in predicting all specific paper statuses (Poster, Spotlight, Oral, Reject).</p>
<p>As per your description, its formula is: is the accuracy of the model in predicting a paper's status as s, given its actual status was s (corresponding to the "Poster%", "Spot%", "Oral%" columns in the table, respectively).Accuracy(Predicted Reject|Actual Reject) typically refers to the accuracy of the model in correctly predicting a paper as rejected, given it was actually rejected (e.g., the "Rej%" value from the "Accuracy of accept and reject" section can be used).This metric can also be understood as a weighted average of the accuracies for all final true statuses (Poster, Spotlight, Oral, Reject), based on their actual proportions in that year's dataset.Further detailed analysis of the DeepReviewer-14b model from the NeurIPS 2024 paper evaluation task (see Table 1 in the main text) provides a clear illustration of THE-Tree's impact.Figure 7 specifically highlights the performance differences when DeepReviewer-14b is augmented with THE-Tree versus its standalone performance.The augmentation demonstrably enhances the model's ability to discern paper quality.Notably, effectiveness in rejecting papers that do not meet the acceptance criteria, thus reducing the likelihood of erroneously endorsing lower-quality work.
Total%status = P (Actual Accept)× s∈{Post.,
Figure 8 delves into the prediction distribution for papers whose ground truth status was Oral or Spotlight.When THE-Tree augmentation is applied, the predictions made by DeepReviewer-14b for these high-impact papers shift more decisively towards categories indicating higher quality (e.g., predicting them as Oral or Spotlight with greater confidence or frequency).This contrasts with the standalone model, which may exhibit a more dispersed or less accurate prediction pattern for these important papers.This case study underscores THE-Tree's -Decision: accept -Rating: oral -Reason: "The paper introduces a novel approach to solving the many-electron Schrödinger equation using a learnable Pfaffian-based neural wave function.This is a significant departure from traditional methods that rely on Slater determinants and non-learnable orbital selection, addressing a key limitation in the field of computational quantum chemistry.-Status: Oral</p>
<p>• LLM Evaluation (without THE-Tree):</p>
<p>-Decision: accept -Rating: poster -Reason: "The paper introduces a novel approach to learning generalized wave functions for manyelectron systems by utilizing Pfaffians instead of Slater determinants, addressing the challenge of enforcing permutation antisymmetry in a fully learnable manner.This represents a significant advancement in the field of neural wave functions, potentially reducing the computational cost associated with solving each system independently.The empirical results demonstrate that the proposed 'neural Pfaffian' achieves chemical accuracy across various systems and outperforms the CCSD(T) CBS reference energies on the TinyMol dataset.The claim of reducing energy errors compared to previous generalized neural wave functions by up to an order of magnitude, if substantiated, indicates a substantial improvement in accuracy.The method's ability to generalize across molecules without constraints on electronic spin configurations or molecular structure further enhances its potential impact.However, the paper's clarity could be improved by providing more details on the specific architecture of the overparametrized neural network and the implementation of the Pfaffian.Additionally, a more detailed comparison with existing methods, including a discussion of their limitations and how the proposed approach overcomes them, would strengthen the paper's contribution.The significance of this work lies in its potential to revolutionize the way we approach quantum chemistry calculations, making them more efficient and accurate.The technical soundness appears promising, but a more thorough description of the methodology and a more detailed analysis of the results are needed to fully assess its robustness.Overall, the paper presents a compelling idea with strong empirical support, suggesting that it would be a valuable addition to the conference."</p>
<p>C.2.3 Comparative Analysis</p>
<p>The comparison between the two evaluation scenarios for the "Neural Pfaffians" paper, which had a ground truth status of Oral, reveals the positive impact of THE-Tree augmentation.</p>
<p>• Rating Accuracy: The LLM augmented with THE-Tree correctly predicted the paper's status as "oral", aligning with the ground truth.In contrast, the standalone LLM, while still deciding to "accept" the paper, rated it as "poster", underestimating its eventual impact tier.</p>
<p>• Identification of Novelty and Impact: The reasoning provided by the THE-Tree augmented LLM, despite noting that "the provided historical evolution paths primarily focus on general deep learning advancements," still identified that "the core idea of using Pfaffians in this context appears to be a novel contribution."This suggests that even with somewhat general historical context, THE-Tree helped the LLM anchor the paper's specific contribution within an evolutionary landscape, allowing it to better discern the novelty.It also highlights the "high impact potential" based on the claimed error reduction.The standalone LLM also recognized the significance ("potential to revolutionize"), but its reasoning carried more reservations regarding the need for "a more thorough description of the methodology and a more detailed analysis of the results ... to fully assess its robustness."While these are valid points for any review, the overall tone and the resulting "poster" rating suggest a slightly diminished confidence in the paper's immediate high-tier impact compared to the augmented evaluation.</p>
<p>• Confidence in Assessment: THE-Tree augmentation appears to provide the LLM with additional contextual evidence that strengthens its assessment.By situating the paper's contributions within a (even broadly) relevant historical trajectory, the LLM can form a more nuanced understanding of its specific advancements and their potential significance.This leads to an evaluation ("oral") that more closely mirrors the expert consensus reflected in the ground truth.The non-augmented LLM, lacking this explicit historical grounding, relies more heavily on the paper's intrinsic claims and presentation, which, while positive, led to a more conservative (and in this case, less accurate) impact assessment.</p>
<p>This case study demonstrates that THE-Tree enhancement can guide LLMs to more accurately assess the novelty and impact of scientific work, leading to evaluations that better align with expert judgments, particularly for high-quality papers.-Decision: accept -Rating: oral -Reason: "The paper introduces a novel perspective on Graph Contrastive Learning (GCL) by identifying a common mechanism, termed 'representation scattering,' that underlies the effectiveness of various GCL frameworks.This unifying concept is a significant contribution, offering a deeper understanding of why different GCL approaches work.The proposed Scattering Graph Representation Learning (SGRL) framework, which incorporates a center-away strategy to enhance representation diversity and a topology-based constraint to prevent excessive scattering, appears to be a technically sound and innovative approach.The paper's motivation is clear, and the proposed method is well-articulated.The experimental results, though not detailed in the provided text, are claimed to demonstrate the efficacy and superiority of SGRL over existing GCL methods, which, if substantiated, would indicate a significant impact on the field.The paper effectively bridges the gap between different GCL paradigms by highlighting their shared mechanism, and the proposed SGRL framework builds upon this understanding to improve performance.The technical evolution paths provided highlight the progression of GNNs and contrastive learning separately, and this paper attempts to synthesize these advancements in the context of GCL, which is a valuable contribution."-Decision: accept -Rating: poster -Reason: "The paper introduces a novel perspective on Graph Contrastive Learning (GCL) by identifying a common mechanism, termed 'representation scattering,' across different GCL frameworks.This unifying concept is a significant contribution, offering a deeper understanding of why these methods work.The proposed Scattering Graph Representation Learning (SGRL) framework, which explicitly leverages this mechanism with a center-away strategy and a topology-based constraint, demonstrates a clear technical innovation.The paper's claim of SGRL's efficacy and superiority is supported by evaluations on benchmark datasets, although the specific details of these evaluations are not provided in the abstract.The clarity of the abstract is good, outlining the problem, the proposed solution, and the key results.The potential impact of this work is substantial, as it not only provides a theoretical insight into GCL but also proposes a practical framework that could advance the field of graph representation learning."</p>
<p>C.3.3 Comparative Analysis</p>
<p>The distinct outcomes for the "Representation Scattering" paper highlight how THE-Tree enables a deeper, more contextual evaluation.</p>
<p>• Rating Accuracy: The THE-Tree augmented LLM correctly identified the paper as "oral," matching the ground truth.The standalone LLM, while positive, assigned a "poster" rating, failing to capture its full impact.</p>
<p>• Contextual Understanding of Contribution: Both evaluations acknowledge the novel "representation scattering" concept.However, the reasoning from the augmented LLM is more insightful.It explicitly references the historical context provided by THE-Tree, stating, "The technical evolution paths provided highlight the progression of GNNs and contrastive learning separately, and this paper attempts to synthesize these advancements..." This demonstrates that the LLM used the evolutionary context to understand how the paper unified two distinct research threads, a key factor in its high impact.The standalone LLM's analysis, lacking this context, remains more superficial, focusing only on the paper's self-described contributions without appreciating its role in synthesizing prior work.</p>
<p>• Assessment Confidence and Nuance: The augmented evaluation confidently points to the paper's value as a "synthesis of advancements."The standalone LLM, while acknowledging the "substantial" potential impact, gives a more standard review focused on the abstract's contents.The ability to place the work within its historical and technical lineage allowed the augmented LLM to make a more decisive and accurate judgment, mirroring the expert consensus of an "oral" presentation.</p>
<p>This case study further validates that by providing verifiable, historical context, THE-Tree empowers LLMs to move beyond surface-level text analysis and perform evaluations that are more aligned with nuanced, expert-level scientific assessment.</p>
<p>Figure 1 :
1
Figure 1: Overview of THE-Tree.(1) Limitations of existing data structures (publication databases, citation networks) for scientific idea generation versus THE-Tree's approach of constructing historical reasoning connections by screening important scientific history nodes and building pathways between them.(2) Methods for verifying LLM-generated ideas: implicit LLM-based cycle-review, human evaluation, and explicit THE-Tree-enhanced LLM verification.(3) Comparison of verification methods, highlighting issues like fact missing in LLM-only approaches, high cost in human evaluation, and THE-Tree's use of verifiable historical facts and scientific reasoning trajectories.(4) Performance improvements with THE-Tree in different tasks</p>
<p>Figure 2 :
2
Figure 2: A simple way to explicitly use THE-tree</p>
<p>Figure 3 :
3
Figure 3: THE-Tree construction overview.(a) Extracting chunks and references from surveys to build an initial concept graph structure; (b) Generating a structured knowledge tree using the SGT-MCTS algorithm, guided by the TVCV methodology for node expansion and RA-NLI for relationship validation.</p>
<p>Figure 4 :
4
Figure 4: Overview of RA-NLI process.This figure illustrates the RA-NLI process, including citation existence verification, document retrieval, and semantic relation assessment using NLI and LLM, which forms the core of the Verify step in TVCV and the R attr calculation.Relationship Validation: Retrieval-Augmented Natural Language Inference (RA-NLI) Mechanism.The critical Verify step of the TVCV methodology (Section 3.3.1),and the basis for the attribution process reward (Rattr) in Equation1, is performed by our Retrieval-Augmented Natural Language Inference (RA-NLI) mechanism.RA-NLI is designed to rigorously assess the causal and logical coherence of the evolutionary link (edge) proposed between a parent node (e.g., vparent) and a newly cited child node (v),</p>
<p>Figure 5 :
5
Figure 5: Document processing and metadata extraction pipeline applied to curated surveys for THE-Tree dataset construction.</p>
<p>Figure 6 :
6
Figure 6: The data structure of THE-tree</p>
<p>•</p>
<p>Node Recall: The proportion of nodes from the expert-refined ground truth trees successfully identified by the MCTS process.Node Recall = |MCTS-identified Nodes ∩ Ground Truth Nodes| |Total Nodes in Ground Truth| .• Node Precision: The proportion of nodes in MCTS-generated trees that are present in the expertrefined ground truth.Node Precision = |MCTS-identified Nodes ∩ Ground Truth Nodes| |Total Nodes in MCTS Tree| .• Edge Recall: The proportion of evolutionary connections from the expert-refined ground truth trees successfully identified by the MCTS process.Edge Recall = |MCTS-identified Edges ∩ Ground Truth Edges| |Total Edges in Ground Truth| .Edge Precision: The proportion of evolutionary connections in MCTS-generated trees that are present in the expert-refined ground truth.Edge Precision = |MCTS-identified Edges ∩ Ground Truth Edges| |Total Edges in MCTS Tree| .• F1 Score: The harmonic mean of precision and recall, calculated separately for nodes and edges.F1 = 2 × Precision×Recall Precision+Recall .• Average Temporal Interval: Given that our MCTS ensures chronological validity (parent node year ≤ child node year), this metric calculates the mean time difference in publication years between directly connected parent (vp) and child (vc) nodes: AvgInterval = 1 |E| (vp,vc)∈E (Year(vc) − Year(vp)).It characterizes the typical evolutionary pace captured.</p>
<p>C</p>
<p>Utilizing THE-Tree for Historical Path Retrospection C.1 Case Study: THE-Tree Augmentation Impact on DeepReviewer-14b for NeurIPS Paper Evaluation</p>
<p>Figure 7 :
7
Figure 7: Performance comparison of DeepReviewer-14b with and without THE-Tree augmentation on identifying high-quality papers and rejecting low-quality submissions in NeurIPS 2024 evaluation.</p>
<p>Figure 8 :
8
Figure 8: Prediction distribution of DeepReviewer-14b (with and without THE-Tree) for papers with ground truth status of Oral or Spotlight in NeurIPS 2024 evaluation.role as a powerful enhancement for LLM-based scientific evaluation.By providing structured historical context and verifiable evolutionary pathways, THE-Tree equips models like DeepReviewer-14b with a more robust foundation for assessing scientific contributions, leading to more accurate identification of impactful research and more reliable filtering of less meritorious submissions.</p>
<p>C. 3
3
Detailed Case Study: LLM Evaluation of "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering" with and without THE-Tree Augmentation To further demonstrate the nuanced evaluation capabilities enabled by THE-Tree, this case study examines the LLM's assessment of the paper "Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering."The ground truth for this paper was also Oral.C.3.1 Case 1: LLM Evaluation with THE-Tree Augmentation • Paper Title: Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering • Original Output (Ground Truth):-Status: Oral• LLM Evaluation (with THE-Tree):</p>
<p>C. 3 . 2
32
Case 2: LLM Evaluation without THE-Tree Augmentation • Paper Title: Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering • Original Output (Ground Truth): -Status: Oral • LLM Evaluation (without THE-Tree):</p>
<p>3 .
3
Candidate Path Origination in Relevant THE-Trees: For each identified topic T opic k ∈ T Pin and its corresponding THE-Tree G k : (a) Identify a candidate set of M k recent papers {v k,j } ⊂ V k from G k based on publication year Y v k,j (e.g., papers published within a certain recent time window or up to the year of P in if known).(b) Calculate a semantic similarity score Sim(P in , v k,j ) ∈ [0, 1] between the input paper P in (using its title and abstract) and each candidate v k,j (using its stored title and abstract) using an LLM or other embedding-based techniques.(c) Select the top N S papers from these candidates, denoted S k = {v * Path Aggregation and Presentation: All generated paths {P ath k,l } are collected.
k,1 , . . . , v  *  k,N S }, that k,l ) ≥ θ sim , where θ sim is a predefined minimum exhibit a similarity score Sim(P in , v  *  similarity threshold. These papers v  *  k,l serve as terminal nodes (or "entry points") forbackward path tracing within the THE-Tree, representing the most closely relatedestablished works to P in .4. Historical Path Retrospection: For each selected terminal paper v  *  term ∈ S k in graph G k :• A historical path P ath k,l = (v 1 , v 2 , . . . , v n = v  *  term ) is constructed by iteratively selecting predecessors. This path traces the lineage of v  *  term backward through theTHE-Tree.• Starting from v curr = v  *  term , the predecessor v  *  pred is chosen from the set of allpredecessors P red(v curr ) (i.e., nodes from which an edge points to v curr ) according tothe lexicographical optimization that prioritizes strong evolutionary links and historicalrelevance:v  *  pred = argmin v∈P red(vcurr),Yv≤Yv curr((Y vcurr − Y v ), −S v )This heuristic prioritizes the predecessor with the smallest non-negative time difference(Y vcurr − Y v ), reflecting direct chronological succession, and secondarily, the one withthe highest importance score S v , indicating significant contributions. The use of nodeattributes Y v (year) and S v (importance) from the THE-Tree structure is central here.• Traversal continues backward until no valid predecessor (respecting chronologicalorder and edge semantics) is found, a predefined maximum path length is exceeded, ora root node of the THE-Tree (a node with no predecessors in the context of this path'shistory) is reached. The resulting path is then presented in chronological order fromthe earliest paper to v  *  term .5.</p>
<p>Table 1 :
1
Impact of THE-Tree Augmentation on LLM-based NeurIPS Paper Evaluation.The table compares the performance of various LLMs in predicting paper acceptance/rejection and status (Poster, Spotlight, Oral) for NeurIPS 2023 and NeurIPS 2024.For the NeurIPS 2024 dataset, results are presented with and without THE-Tree augmentation to demonstrate its effect on evaluation accuracy.Performance is evaluated using standard metrics (see Appendix B.3).
ModelNeurIPS 2023NeurIPS 2024Accuracy of accept and rejectAccuracy of StatusAccuracy of accept and rejectAccuracy of StatusAcc% Rej%Total%Poster% Spot% Oral% Total Acc% Rej%Total%Poster% Spot% Oral% Total%Qwen2.5-72b-instruct99.480.5226.343.5910003.3899.63025.701.2410002.42Deep-Reviewer-14b93.70 16.0636.3273.6531.82029.59 92.61 18.2837.4574.0322.776.9431.03Deep-Reviewer-7b83.94 18.7535.7657.4918.18027.54 86.79 19.7837.0760.4219.05028.9GPT4-O99.482.5927.8830.5490.91010.95 99.632.2426.0229.4686.96010.26Claude-3.5-Sonnet99.750.5226.426.275544.163.3899.750.3827.2810.9077.9618.574.59Deepseek-R199.573.4228.5154.4952.66016.27 100.02.0026.0354.1353.31015.00With THE-tree AugmentationQwen2.5-72b-instruct_tree-------99.840.3726.031.7697.3802.76Deep-Reviewer-14b_tree-------89.93 22.3939.8263.234.5534.7232.08Deep-Reviewer-7b_tree-------76.12 63.6966.905724.992.3360.84GPT4-O_tree-------99.662.6627.6934.45722.611.41Claude-3.5-Sonnet_tree-------71.46 36.5745.5728.7238.1336.5734.81Deepseek-R1_tree-------99.573.4228.2356.4956.662.616.68</p>
<p>Table 2 :
2
Comparison of Methods for Technol-
Quality Validation4.2.1 Effectiveness of Technology Trajectory ReconstructionTo validate THE-Tree's effectiveness in reconstructingmeaningful, accurate technology development trajecto-ries, we conducted comprehensive quantitative evaluationsagainst expert-refined ground truth. The methodology forogy Tree Relationship Verificationconstructing this expert-refined ground truth dataset is de-MethodFact Missing Rate (%) Accuracy (%)tailed in Appendix B.2.Claude 3.5 Sonnet [1]47.9360.22GPT-4o [26]58.1960.18DeepSeek R1 [15]48.1676.40Qwen 2.5-72B [31]58.2953.95DeepReviewer-7B [36]40.8893.95DeepReviewer-14B [36]68.8476.41LLaMA 3.1 [25]42.2960.40Factual SupplementClaude 3.5 Sonnet w/ Fact-65.34GPT-4o w/ Fact-69.40DeepSeek R1 w/ Fact-81.60Qwen 2.5-72B w/ Fact-66.80DeepReviewer-7B w/ Fact-95.38DeepReviewer-14B w/ Fact-82.09LLaMA 3.1 w/ Fact-65.35RA-NLI (Ours)4.7595.60</p>
<p>Table 3 :
3
Quantitative Comparison of THE-Tree Reconstruction Quality (MCTS-generated) against Expert-Refined Ground Truth.The methodology for constructing this expert-refined ground truth dataset is detailed in Appendix B.2. Metrics assess the ability to reconstruct entities (papers) and their evolutionary relations across the 88 topics.
EntityRelationMethodRecall PrecisionF1Avg_Time_Diff Recall PrecisionF1Expert1.001.001.002.931.001.001.00THE-Tree0.840.670.753.080.780.640.70</p>
<p>Table 4 :
4
Comparison</p>
<p>of Graph Completion Performance Between THE-Tree and Traditional Citation Graphs.Detailed definitions of the evaluation metrics (Hit@k, MR, MedianRank, MRR) can be found in Appendix B.3.</p>
<p>Table 5 :
5
Comparison of THE-Tree and Citation Graph on Future Path Prediction.Metrics reported for Entity and Relation predictions (see the definition of metrics in Appendix B.3).</p>
<p>Table 6 :
6
Enhanced Statistical Summary with Standardization
MetricTotalAvg/Topic Avg/THE-tree Avg/human selectProcessed topics88---Paper nodes35,392402.18103.1446.49Paper edges140,6161597.91255.57204.67
Note: All metrics calculated over 1950-2023 temporal scope.</p>
<p>Spot., Oral} (P (Actual is s|Actual Accept) × Accuracy(Predicted is s|Actual is s)) Actual is s|Actual Accept) represents the proportion of papers whose actual status is s (Poster, Spotlight, or Oral) among all accepted papers for that year.Accuracy(Predicted is s|Actual is s)
+ P (Actual Reject) × Accuracy(Predicted Reject|Actual Reject)whereP (Actual Accept)andP (Actual Reject)areasdefinedabove.P (</p>
<p>The use of Pfaffians to enforce permutation antisymmetry without constraints on spin configurations or molecular structure represents a notable innovation.The empirical results, demonstrating chemical accuracy on a range of systems and outperforming the CCSD(T) CBS reference on the TinyMol dataset, suggest a substantial improvement over existing generalized neural wave function methods.The paper's claim of reducing energy errors by up to an order of magnitude compared to previous methods, if substantiated, indicates a high impact potential for the field.However, the abstract lacks detailed information on the specific neural network architecture and training procedure, which would be crucial for a full assessment of the technical soundness.The claim of a single neural Pfaffian calculating both ground and ionization energies warrants further scrutiny regarding the methodology used for ionization energy calculations."
C.2.2 Case 2: LLM Evaluation without THE-Tree Augmentation• Paper Title: Neural Pfaffians: Solving Many Many-Electron Schrödinger Equations• Original Output (Ground Truth):
THE-Tree improves DeepReviewer-14b's capacity to correctly identify high-quality submissions, such as those ultimately designated as Oral or Spotlight presentations. Concurrently, it significantly boosts the model's
A Technical Appendices and Supplementary MaterialTechnical appendices with additional results, figures, graphs and proofs may be submitted with the paper submission before the full submission deadline (see above), or as a separate PDF in the ZIP file below before the supplementary material deadline.There is no page limit for the technical appendices.A.1 Data Collection and ProcessingWe collected data from multiple sources, including Web of Science, Scopus, arXiv, IEEE Xplore, and PubMed.The data processing pipeline consisted of the following steps:1. Metadata extraction: We extracted titles, abstracts, authors, venues, and publication dates using custom parsers for each data source.2. Citation network construction: We built a directed graph where nodes represent papers and edges represent citation relationships.3. Text preprocessing: We applied standard NLP preprocessing techniques, including tokenisation, stopword removal, and lemmatisation.4. Entity recognition: We used a combination of dictionary-based and machine learning approaches to identify technical terms and concepts.5. Temporal alignment: We aligned papers along a timeline, accounting for publication delays and citation patterns.To evaluate the accuracy of our metadata extraction component, we measured its precision on two commonly used formatting styles: 98.29% for IEEE format and 97.30% for APA format.These high precision scores demonstrate the robustness of our system in handling different citation conventions.
. Anthropic, 2024Claude 3.5 sonnet</p>
<p>Towards a knowledge graph for science. Sören Auer, Markus Stocker, Proceedings of the 11th International Conference on Semantic Systems. the 11th International Conference on Semantic Systems2018</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:1903.106762019arXiv preprint</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>What do citation counts measure? a review of studies on citing behavior. Lutz Bornmann, Hans-Dieter Daniel, Journal of documentation. 6412008</p>
<p>Sam Andres M Bran, Philippe Schilter, Delfosse, Anica Tobias, Apeksha Ivanova, Yoojin Thakkar, Philippe Jung, Alain C Schwaller, Kevin Maik Vaucher, Chang, arXiv:2304.05376Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>A survey of monte carlo tree search methods. Edward Cameron B Browne, Daniel Powley, Simon M Whitehouse, Peter I Lucas, Philipp Cowling, Stephen Rohlfshagen, Diego Tavener, Spyridon Perez, Simon Samothrakis, Colton, IEEE Transactions on Computational Intelligence and AI in games. 412012</p>
<p>Mapping the dynamics of science and technology: Sociology of science in the real world. John Michel Callon, Arie Law, Rip, Sociology of science in the real world. Macmillan London1983</p>
<p>CiteSpace II: Detecting and visualizing emerging trends and transient patterns in scientific literature. Chaomei Chen, Journal of the American society for information science and technology. 5732006</p>
<p>To cite, or not to cite? detecting citation contexts in text. Michael Färber, Alexander Thiemann, Adam Jatowt, Advances in Information Retrieval: 40th European Conference on IR Research. Grenoble, FranceSpringer2018. March 26-29. 2018. 201840</p>
<p>Validating ai-generated code with live programming. Kasra Ferdowsi, Ruanqianqian Huang, Michael B James, Nadia Polikarpova, Sorin Lerner, Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing Systems2024</p>
<p>Citation indexing: Its theory and application in science, technology, and humanities. Eugene Garfield, 1979Wiley</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Hlm-cite: Hybrid language model workflow for text-based scientific citation prediction. Qianyue Hao, Jingyang Fan, Fengli Xu, Jian Yuan, Yong Li, arXiv:2410.091122024arXiv preprint</p>
<p>Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, Mlagentbench, arXiv:2310.03302Evaluating language agents on machine learning experimentation. 2023arXiv preprint</p>
<p>A theory of universal artificial intelligence based on algorithmic complexity. Marcus Hutter, arXiv preprint cs/00040012000</p>
<p>Towards a universal theory of artificial intelligence based on algorithmic probability and sequential decisions. Marcus Hutter, Machine Learning: ECML 2001. Springer2001</p>
<p>Codescientist: End-to-end semi-automated scientific discovery with code-based experimentation. Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S Weld, Peter Clark, arXiv:2503.227082025arXiv preprint</p>
<p>Levente Kocsis and Csaba Szepesv ári. Bandit based monte-carlo planning. European conference on machine learning. Springer2006</p>
<p>Scientific discovery: Computational explorations of the creative processes. Pat Langley, 1987MIT press</p>
<p>Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, Jay Pujara, arXiv:2305.10613Temporal knowledge graph forecasting without knowledge using in-context learning. 2023arXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Introducing llama 3.1: Our most capable models to date. A I Meta, 2024</p>
<p>Hello gpt-4o. Openai, 2024</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances Neural Information Processing Systems. 202336</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024arXiv preprint</p>
<p>Co-citation in the scientific literature: A new measure of the relationship between two documents. Henry Small, Journal of the American Society for information Science. 2441973</p>
<p>ArnetMiner: extraction and mining of academic social networks. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, Zhong Su, Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining. the 14th ACM SIGKDD international conference on Knowledge discovery and data mining2008</p>
<p>Qwen2.5: A party of foundation models. Qwen Team, September 2024</p>
<p>Software survey: Vosviewer, a computer program for bibliometric mapping. Nees Jan, Van Eck, Ludo Waltman, Scientometrics. 8422010</p>
<p>Microsoft academic graph: When experts are not enough. Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Darrin Eide, Yuxiao Dong, Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2020</p>
<p>CycleResearcher: Improving Automated Research via Automated Review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, International Conference on Learning Representations (ICLR). 2025</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, Erik Cambria, arXiv:2309.027262023arXiv preprint</p>
<p>Deepreview: Improving llm-based paper review with human-like deep thinking process. Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang, arXiv:2503.085692025arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
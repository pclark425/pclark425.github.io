<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7531 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7531</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7531</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-138.html">extraction-schema-138</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <p><strong>Paper ID:</strong> paper-9f5b8d5310a2bafad2838dcf91c88a02b0aca106</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9f5b8d5310a2bafad2838dcf91c88a02b0aca106" target="_blank">JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work proposes an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data, and craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in \url{https://github.com/RUCAIBox/JiuZhang3.0}.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7531.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7531.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large closed-source transformer LLM from OpenAI used in this paper as a high-quality teacher/text-based simulator to synthesize math problem–solution pairs (knowledge distillation data) conditioned on crafted prompts and math-related texts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (large proprietary model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>base large LLM (teacher) used as text generator / data synthesizer</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning / mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Synthesize math problem–solution pairs (both natural-language solutions and executable-program solutions) conditioned on prompts representing education stages and on math-related textual contexts; used to construct a knowledge-distillation (KD) dataset for training a smaller data-synthesis LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Prompt templates covering education stages and competitions (Grade School, Middle School, High School, College, AMC8/10/12, AIME) for natural-language problems; separate, more format-strict prompts for tool-manipulation problems (explicit sections [Problem Description] and [Solution], require Python code); each GPT-4 input was the concatenation of a randomly sampled math-related text and a randomly selected prompt instance; examples and guidelines included in templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>No direct intrinsic metric reported for GPT-4's synthesis quality; evaluation of usefulness is indirect via downstream model performance (accuracy on standard math benchmarks) and via a gradient-based influence score (cosine similarity of projected LoRA gradients used to rank synthetic instances).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>No direct accuracy for synthesis itself reported; GPT-4 baseline solver performance reported in paper tables (as reasoning model): GSM8k = 92.2% accuracy, MATH = 65.4% accuracy, SVAMP = 92.9% (these are shown as baseline model performances, not synthesis-quality metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>Not applicable for synthesis quality (paper treats GPT-4 as high-quality teacher); downstream baseline comparisons use other LLMs' solver accuracies (e.g., ChatGPT GSM8k 76.6%, many open-source models listed in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt design (education-stage templates, explicit format/guidelines)', 'Math-related context texts provided with prompts', 'Using GPT-4 selectively via a gradient-based high-value-data selection to reduce API calls', 'Post-processing extraction of problem/solution (regex)', 'Amount of KD instances generated (they limited GPT-4 calls to ~9.3k by selection)', 'Use-case (natural-language vs tool-manipulation formats)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>Initial KD construction: randomly sampled ~5,336 math-related texts concatenated with randomly selected prompts and fed to GPT-4; extraction of problems/solutions via regex. Later boosting: GPT-4 re-generated outputs only for top-ranked high-value texts (2k per setting in implementation). Total GPT-4 API calls reported: 9,335; average input length ~300 tokens, average output length ~877 tokens; synthesized KD outputs used to train a small synthesizer and to produce final 6M problem pre-training corpus (4.6B tokens). No temperature or decoding hyperparameters for GPT-4 synthesis are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>No direct quality metric for GPT-4 synthesis was provided; reliance on a single teacher (GPT-4) noted as a limitation (authors did not test other teachers like Claude 3 or GPT-4o). Cost is still non-zero; paper aims to reduce GPT-4 calls but still calls it ~9.3k times. The paper does not claim perfect coverage — prompts and selected texts may not cover knowledge from other STEM subjects (e.g., MMLU-STEM weakness).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7531.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7531.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the use of large language models as text-based simulators for scientific subdomains, including details about the model, domain, simulation task, prompting strategy, evaluation metrics, reported accuracy, baselines, and any factors identified as influencing accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeekMath-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeekMath-7B (small data-synthesis LLM distilled from GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A relatively small (7B) language model trained by knowledge distillation from GPT-4 on a crafted KD dataset to act as an efficient text-based simulator that generates large-scale math problem–solution pairs for pre-training downstream models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeekMath-7B (used as the data synthesis model; variants include DeepSeekMath-7B-RL used in initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B parameters (referred to in paper as a 'small' LLM compared to GPT-4 / 72B models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>fine-tuned / distilled small LLM (trained on KD dataset to imitate GPT-4 synthesis behavior); used as a generator for large-scale synthetic pre-training data</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Mathematical reasoning / mathematics (math problem generation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task_description</strong></td>
                            <td>Generate diverse and high-quality math problem–solution pairs (both natural-language solutions and code-based tool-manipulation solutions) conditioned on prompts and math-related texts; used to create ~6M problems (4.6B tokens) for pre-training JiuZhang3.0.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_strategy</strong></td>
                            <td>Same crafted prompt set as for GPT-4: eight prompts for natural-language reasoning covering human education stages and competitions (Grade School, Middle School, High School, College, AMC examples), and templates for tool-manipulation tasks requiring explicit [Problem Description] and [Solution] sections with executable Python code; inputs were math-related text snippets embedded in templates and randomly sampled prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Effectiveness measured indirectly via downstream solver accuracy after pre-training on synthetic data (standard classification/QA exact-match/accuracy metrics on GSM8k, MATH, SVAMP, ASDiv, MAWPS, CARP, TabMWP, AQuA, OCW-Math, etc.); for data-value selection, used cosine similarity of low-dimensional projected LoRA gradients as the influence/value metric.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Downstream evaluation (examples reported in paper resulting from synthetic data produced by this synthesis model): JiuZhang3.0-7B (trained on the synthetic corpus) achieved GSM8k = 88.6% and MATH = 52.8% (natural-language reasoning setting, in main comparison tables); in an ablation/efficient-test setting where 100k synthetic + 50k instruction data were used, the pipeline using this synthesis model yielded GSM8k = 78.6%, MATH = 32.8% (these smaller-scale numbers are from ablation experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_accuracy</strong></td>
                            <td>When other LLMs were used as the synthesis model for the same pipeline (ablation), downstream results were worse: e.g., using ChatGPT as synthesizer yielded GSM8k = 77.0% and MATH = 26.6% (same efficient-test setting); Mixtral-8×7B yielded GSM8k = 77.6%, MATH = 26.8%; DeepSeekMath-RL-7B yielded GSM8k = 77.1%, MATH = 27.2%; random sampling / other selection strategies also gave lower downstream scores compared to the gradient-based selection used in the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_reported</strong></td>
                            <td>['Prompt set design (multi-stage, competition-specific prompts) — ablation shows prompt set improves accuracy', 'Inclusion of math-related texts as conditioning context (ablation shows performance drop without texts)', 'Boosting retraining with GPT-4-regenerated high-value KD data (retraining improves synthesizer)', 'Gradient-based value estimation for selecting high-value texts (outperforms random, perplexity, reward-model selections in many cases)', 'Quantity of synthetic pre-training data (performance scales up with more synthetic data; but quality matters — fewer high-quality examples can outperform larger quantities)', 'Pre-train data mixture proportion (NLC:TM ratio; 2:1 found best for some settings)', 'Choice of base model for downstream (e.g., Mistral-7B vs LLaMA-3-8B) affects adaptation and final performance', 'Number of selected top high-value texts (top 2k worked best; adding more degraded performance)', 'Using LoRA reference model and random projection for influence estimation (implementation details affecting selection results)']</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_conditions</strong></td>
                            <td>KD init: trained on GPT-4 outputs for ~5,336 math-related texts; retraining after selecting top-ranked high-value texts (2k per setting) where GPT-4 regenerated outputs; final synthesis produced ~5,984,584 problems (~6M, 4.6B tokens). Total GPT-4 usage for KD: ~9,335 calls. Value estimation: trained LoRA reference model on a subset of synthetic data, computed LoRA-parameter gradients per instance, applied random Rademacher projection to reduce dimensionality, and computed cosine similarity against average downstream gradients. Pre-training of JiuZhang3.0 used synthetic corpus then instruction fine-tuning; hyperparameters: learning rates (1e-5 pre-train), cosine LR schedule, BFloat16, FlashAttention2, DeepSpeed stages; training context length 2048, batch size 512. Pre-train data proportion tuning found 2:1 (natural language : tool-manipulation) worked well under some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Synthesizer trained only on math-related data — may not generalize to other scientific domains; synthesized data quality matters more than scale (adding more lower-value KD data degraded performance); pipeline depends on GPT-4 for high-value regeneration (authors note not testing other teacher models); downstream weaknesses remain on cross-discipline benchmarks such as MMLU-STEM; ablations show removing components (prompt set, math texts, boosting retraining, value estimation) reduces downstream accuracy, especially on hard/competition datasets (MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Key-point-driven data synthesis with its enhancement on mathematical reasoning <em>(Rating: 2)</em></li>
                <li>Metamath: Bootstrap your own mathematical questions for large language models <em>(Rating: 2)</em></li>
                <li>Tora: A tool-integrated reasoning agent for mathematical problem solving <em>(Rating: 1)</em></li>
                <li>Deepseekmath: Pushing the limits of mathematical reasoning in open language models <em>(Rating: 2)</em></li>
                <li>Mammoth2: Scaling instructions from the web <em>(Rating: 1)</em></li>
                <li>Let's synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7531",
    "paper_id": "paper-9f5b8d5310a2bafad2838dcf91c88a02b0aca106",
    "extraction_schema_id": "extraction-schema-138",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "A large closed-source transformer LLM from OpenAI used in this paper as a high-quality teacher/text-based simulator to synthesize math problem–solution pairs (knowledge distillation data) conditioned on crafted prompts and math-related texts.",
            "citation_title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": "not specified (large proprietary model)",
            "model_type": "base large LLM (teacher) used as text generator / data synthesizer",
            "scientific_domain": "Mathematical reasoning / mathematics",
            "simulation_task_description": "Synthesize math problem–solution pairs (both natural-language solutions and executable-program solutions) conditioned on prompts representing education stages and on math-related textual contexts; used to construct a knowledge-distillation (KD) dataset for training a smaller data-synthesis LLM.",
            "prompting_strategy": "Prompt templates covering education stages and competitions (Grade School, Middle School, High School, College, AMC8/10/12, AIME) for natural-language problems; separate, more format-strict prompts for tool-manipulation problems (explicit sections [Problem Description] and [Solution], require Python code); each GPT-4 input was the concatenation of a randomly sampled math-related text and a randomly selected prompt instance; examples and guidelines included in templates.",
            "evaluation_metric": "No direct intrinsic metric reported for GPT-4's synthesis quality; evaluation of usefulness is indirect via downstream model performance (accuracy on standard math benchmarks) and via a gradient-based influence score (cosine similarity of projected LoRA gradients used to rank synthetic instances).",
            "reported_accuracy": "No direct accuracy for synthesis itself reported; GPT-4 baseline solver performance reported in paper tables (as reasoning model): GSM8k = 92.2% accuracy, MATH = 65.4% accuracy, SVAMP = 92.9% (these are shown as baseline model performances, not synthesis-quality metrics).",
            "baseline_accuracy": "Not applicable for synthesis quality (paper treats GPT-4 as high-quality teacher); downstream baseline comparisons use other LLMs' solver accuracies (e.g., ChatGPT GSM8k 76.6%, many open-source models listed in paper tables).",
            "factors_reported": [
                "Prompt design (education-stage templates, explicit format/guidelines)",
                "Math-related context texts provided with prompts",
                "Using GPT-4 selectively via a gradient-based high-value-data selection to reduce API calls",
                "Post-processing extraction of problem/solution (regex)",
                "Amount of KD instances generated (they limited GPT-4 calls to ~9.3k by selection)",
                "Use-case (natural-language vs tool-manipulation formats)"
            ],
            "experimental_conditions": "Initial KD construction: randomly sampled ~5,336 math-related texts concatenated with randomly selected prompts and fed to GPT-4; extraction of problems/solutions via regex. Later boosting: GPT-4 re-generated outputs only for top-ranked high-value texts (2k per setting in implementation). Total GPT-4 API calls reported: 9,335; average input length ~300 tokens, average output length ~877 tokens; synthesized KD outputs used to train a small synthesizer and to produce final 6M problem pre-training corpus (4.6B tokens). No temperature or decoding hyperparameters for GPT-4 synthesis are reported.",
            "limitations_or_failure_modes": "No direct quality metric for GPT-4 synthesis was provided; reliance on a single teacher (GPT-4) noted as a limitation (authors did not test other teachers like Claude 3 or GPT-4o). Cost is still non-zero; paper aims to reduce GPT-4 calls but still calls it ~9.3k times. The paper does not claim perfect coverage — prompts and selected texts may not cover knowledge from other STEM subjects (e.g., MMLU-STEM weakness).",
            "uuid": "e7531.0",
            "source_info": {
                "paper_title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "DeepSeekMath-7B",
            "name_full": "DeepSeekMath-7B (small data-synthesis LLM distilled from GPT-4)",
            "brief_description": "A relatively small (7B) language model trained by knowledge distillation from GPT-4 on a crafted KD dataset to act as an efficient text-based simulator that generates large-scale math problem–solution pairs for pre-training downstream models.",
            "citation_title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
            "mention_or_use": "use",
            "model_name": "DeepSeekMath-7B (used as the data synthesis model; variants include DeepSeekMath-7B-RL used in initialization)",
            "model_size": "7B parameters (referred to in paper as a 'small' LLM compared to GPT-4 / 72B models)",
            "model_type": "fine-tuned / distilled small LLM (trained on KD dataset to imitate GPT-4 synthesis behavior); used as a generator for large-scale synthetic pre-training data",
            "scientific_domain": "Mathematical reasoning / mathematics (math problem generation)",
            "simulation_task_description": "Generate diverse and high-quality math problem–solution pairs (both natural-language solutions and code-based tool-manipulation solutions) conditioned on prompts and math-related texts; used to create ~6M problems (4.6B tokens) for pre-training JiuZhang3.0.",
            "prompting_strategy": "Same crafted prompt set as for GPT-4: eight prompts for natural-language reasoning covering human education stages and competitions (Grade School, Middle School, High School, College, AMC examples), and templates for tool-manipulation tasks requiring explicit [Problem Description] and [Solution] sections with executable Python code; inputs were math-related text snippets embedded in templates and randomly sampled prompts.",
            "evaluation_metric": "Effectiveness measured indirectly via downstream solver accuracy after pre-training on synthetic data (standard classification/QA exact-match/accuracy metrics on GSM8k, MATH, SVAMP, ASDiv, MAWPS, CARP, TabMWP, AQuA, OCW-Math, etc.); for data-value selection, used cosine similarity of low-dimensional projected LoRA gradients as the influence/value metric.",
            "reported_accuracy": "Downstream evaluation (examples reported in paper resulting from synthetic data produced by this synthesis model): JiuZhang3.0-7B (trained on the synthetic corpus) achieved GSM8k = 88.6% and MATH = 52.8% (natural-language reasoning setting, in main comparison tables); in an ablation/efficient-test setting where 100k synthetic + 50k instruction data were used, the pipeline using this synthesis model yielded GSM8k = 78.6%, MATH = 32.8% (these smaller-scale numbers are from ablation experiments).",
            "baseline_accuracy": "When other LLMs were used as the synthesis model for the same pipeline (ablation), downstream results were worse: e.g., using ChatGPT as synthesizer yielded GSM8k = 77.0% and MATH = 26.6% (same efficient-test setting); Mixtral-8×7B yielded GSM8k = 77.6%, MATH = 26.8%; DeepSeekMath-RL-7B yielded GSM8k = 77.1%, MATH = 27.2%; random sampling / other selection strategies also gave lower downstream scores compared to the gradient-based selection used in the pipeline.",
            "factors_reported": [
                "Prompt set design (multi-stage, competition-specific prompts) — ablation shows prompt set improves accuracy",
                "Inclusion of math-related texts as conditioning context (ablation shows performance drop without texts)",
                "Boosting retraining with GPT-4-regenerated high-value KD data (retraining improves synthesizer)",
                "Gradient-based value estimation for selecting high-value texts (outperforms random, perplexity, reward-model selections in many cases)",
                "Quantity of synthetic pre-training data (performance scales up with more synthetic data; but quality matters — fewer high-quality examples can outperform larger quantities)",
                "Pre-train data mixture proportion (NLC:TM ratio; 2:1 found best for some settings)",
                "Choice of base model for downstream (e.g., Mistral-7B vs LLaMA-3-8B) affects adaptation and final performance",
                "Number of selected top high-value texts (top 2k worked best; adding more degraded performance)",
                "Using LoRA reference model and random projection for influence estimation (implementation details affecting selection results)"
            ],
            "experimental_conditions": "KD init: trained on GPT-4 outputs for ~5,336 math-related texts; retraining after selecting top-ranked high-value texts (2k per setting) where GPT-4 regenerated outputs; final synthesis produced ~5,984,584 problems (~6M, 4.6B tokens). Total GPT-4 usage for KD: ~9,335 calls. Value estimation: trained LoRA reference model on a subset of synthetic data, computed LoRA-parameter gradients per instance, applied random Rademacher projection to reduce dimensionality, and computed cosine similarity against average downstream gradients. Pre-training of JiuZhang3.0 used synthetic corpus then instruction fine-tuning; hyperparameters: learning rates (1e-5 pre-train), cosine LR schedule, BFloat16, FlashAttention2, DeepSpeed stages; training context length 2048, batch size 512. Pre-train data proportion tuning found 2:1 (natural language : tool-manipulation) worked well under some settings.",
            "limitations_or_failure_modes": "Synthesizer trained only on math-related data — may not generalize to other scientific domains; synthesized data quality matters more than scale (adding more lower-value KD data degraded performance); pipeline depends on GPT-4 for high-value regeneration (authors note not testing other teacher models); downstream weaknesses remain on cross-discipline benchmarks such as MMLU-STEM; ablations show removing components (prompt set, math texts, boosting retraining, value estimation) reduces downstream accuracy, especially on hard/competition datasets (MATH).",
            "uuid": "e7531.1",
            "source_info": {
                "paper_title": "JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Key-point-driven data synthesis with its enhancement on mathematical reasoning",
            "rating": 2
        },
        {
            "paper_title": "Metamath: Bootstrap your own mathematical questions for large language models",
            "rating": 2
        },
        {
            "paper_title": "Tora: A tool-integrated reasoning agent for mathematical problem solving",
            "rating": 1
        },
        {
            "paper_title": "Deepseekmath: Pushing the limits of mathematical reasoning in open language models",
            "rating": 2
        },
        {
            "paper_title": "Mammoth2: Scaling instructions from the web",
            "rating": 1
        },
        {
            "paper_title": "Let's synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models",
            "rating": 2
        }
    ],
    "cost": 0.016580249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models</h1>
<p>Kun Zhou ${ }^{1,3 <em>}$, Beichen Zhang ${ }^{2,3 </em>}$, Jiapeng Wang ${ }^{2}$, Zhipeng Chen ${ }^{2,3}$, Wayne Xin Zhao ${ }^{2,3 \dagger}$, Jing Sha ${ }^{3}$, Zhichao Sheng ${ }^{3}$, Shijin Wang ${ }^{4,5}$, Ji-Rong Wen ${ }^{1,2,3}$<br>${ }^{1}$ School of Information, Renmin University of China.<br>${ }^{2}$ Gaoling School of Artificial Intelligence, Renmin University of China.<br>${ }^{3}$ Beijing Key Laboratory of Big Data Management and Analysis Methods.<br>${ }^{4}$ iFLYTEK Research. ${ }^{5}$ iFLYTEK AI Research (Central China).<br>francis_kun_zhou@163.com, {zhangbeichen724, batmanfly}@gmail.com,<br>{jingsha, sjwang3}@iflytek.com, jrwen@ruc.edu.cn</p>
<h4>Abstract</h4>
<p>Mathematical reasoning is an important capability of large language models (LLMs) for real-world applications. To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (e.g., GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis. To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data. To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM. Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels. Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts. The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM. We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model, which only needs to invoke GPT-4 API 9.3k times and pre-train on 4.6B data. Experimental results have shown that JiuZhang3.0 achieves state-of-theart performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings. Our code and data will be publicly released in https://github.com/RUCAIBox/JiuZhang3.0.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable capabilities on a variety of tasks [1-3]. However, they still struggle in solving complex mathematical problems [4]. Recent work has shown that it is an effective approach to training LLMs on math-related data for improving the mathematical reasoning ability [5, 6]. Typically, they either collect the math-related data from the available corpora (e.g., webpages and books) for pre-training [7-9], or rely on stronger LLMs to synthesize high-quality math problems for fine-tuning [10-12]. Despite the success, existing approaches would generally cause large training or inference costs. Due to the complexity and diversity of mathematical problems, the former type of work mostly needs to collect a large-scale corpus (e.g., 120B data for DeepseekMath) for training, which greatly increases the training cost [5, 7, 8]. Similarly, to guarantee the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>knowledge coverage and effectiveness of the synthetic problems, the latter type of work relies on stronger LLMs with larger scales (e.g., GPT-4) to create massive math problems, leading to larger inference cost [6, 10, 11]. In Figure 1, we show our estimated total costs of re-implementing two math-related LLMs ( $&gt;\$ 40000$ ), details are in Appendix C</p>
<p>In this work, we aim to develop a relatively lowcost data synthesis approach for improving the mathematical reasoning abilities of LLMs. Our key idea is that the data synthesis capability can be well learned by small LLMs. Here, small is a relative wording, which is in contrast with the extremely large or costly data synthesis models used in prior studies [6, 13], such as GPT-4 or Qwen-72B. Actually, existing work $[10,14,15]$ has extensive evidence of strong learning and adaptation abilities of small LLMs for new tasks and domains with suitable strategies (e.g., training with high-quality supervised data), including math, science, and complex multimodal tasks. However, this exploration has been neglected in prior efforts on data synthesis. This attempt can be essentially generalized to a broader problem: whether a small (or weak) model can produce high-quality data that is useful for training a large (or strong) model? Inspired by this motivation, we seek to train a relatively small yet powerful LLM for synthesizing high-quality math-related data.</p>
<p>However, due to the diverse and complex nature of math problems, it is challenging to train wellperforming small LLMs for synthesizing high-quality ones. Although we can leverage GPT-4, it is not efficient to use it for synthesizing a large-scale knowledge distillation (KD) dataset. Specifically, we aim to build the KD dataset through a low-cost strategy, but it can sufficiently capture diverse and useful knowledge about math problem synthesis. Thus, we should guarantee the knowledge coverage and usefulness of the instances within the KD dataset. To achieve it, we first craft a set of prompts, and each prompt corresponds to an education stage of humans, e.g., middle school and college. Using the above prompts, the instances within the dataset can well cover broad mathematical knowledge and different difficulty levels. Besides, for usefulness, we estimate the influence of the available math-related texts and downstream tasks, by computing the gradient similarity between their corresponding synthetic data and the task instances. Then, we select the top-ranking math-related texts into the KD dataset, which are high-value ones with more positive influence on downstream tasks. By feeding the texts with prompts into GPT-4, we collect the outputs to build the KD dataset.</p>
<p>Based on the KD dataset, we train DeepSeekMath-7B as our data synthesis model, which is much smaller than other commonly-used LLMs in existing work [6, 12, 13], e.g., GPT-4 and Qwen-72B. Owing to our data selection strategy, we only require GPT-4 to generate 9,335 instances based on the selected most valuable texts for training it. Then, we utilize the crafted prompts to guide it for synthesizing high-quality problems. Benefiting from the strong data synthesis capability of the small model, we only need to synthesize 5,984,584 high-quality math problems ( 4.6 B tokens) for pre-training our JiuZhang3.0. Thus, the total inference and training cost is much less than existing work, as shown in Figure 1. After pre-training, we also collect open-source math instructions to fine-tune JiuZhang3.0. The experimental results have shown that JiuZhang3.0 can mostly outperform state-of-the-art methods across 18 evaluation datasets, in both the natural language reasoning and tool manipulation settings. Our contributions are summarized as follows:
(1) our research provides compelling evidence that it is feasible to efficiently train a small LLM (7B) for synthesizing training data to improve the mathematical reasoning of LLMs. As results shown in Section 3.3, its synthetic data is more useful than larger LLMs in improving the performance.
(2) we propose an efficient solution for training LLMs to improve mathematical reasoning, which only needs to invoke GPT-4 API 9.3 k times and pre-train on 4.6 B high-quality synthetic data, with nearly $20 \%$ total cost of existing state-of-the-art methods.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The pipeline of our approach. We first initialize the data synthesis LLM by distilling the knowledge from GPT-4 on randomly sampled data, then boost it using the high-value data selected by gradient-based value estimation strategy, finally utilize it for synthesizing data to train JiuZhang3.0.</p>
<p>(3) JiuZhang3.0 achieves state-of-the-art performance among open-source LLMs on several tasks and settings, <em>e.g.,</em> 52.8 (JiuZhang3.0-7B) vs. 50.2 (DeepSeekMath-7B-RL) on MATH, 89.8 (JiuZhang3.0-8×7B) vs. 86.4 (MAmmoTH2-8×7B-Plus) on GSM8k in the natural language reasoning setting.</p>
<h2>2 Approach</h2>
<p>In this section, we present our approach that aims to train a small LLM for synthesizing math problems. First, we initialize the data synthesis model by training it on the KD dataset, composed of crafted prompts, randomly sampled math-related texts, and the corresponding synthetic problems and solutions from GPT-4. Then, we improve its data synthesis capability by retraining it on the updated knowledge distillation dataset, where we add the high-value math-related texts selected by gradient-based influence estimation strategy. Finally, we utilize the model to synthesize massive high-quality math problems for training JiuZhang3.0, based on the multi-source math-related corpus.</p>
<h3>2.1 Preliminary</h3>
<p>We focus on training a small data synthesis LLM, for synthesizing high-quality math problems—solution pairs to pre-train LLMs and improve its mathematical reasoning capability. To guarantee the quality of the synthetic data, we utilize GPT-4 to create the knowledge distillation (KD) dataset $\mathcal{D}<em i="i">{KD}={p</em>}, t_{i}, \hat{q<em i="i">{i}, \hat{s}</em>}<em i="i">{i=1}^{N}$ for training the small LLM, where the math-related text $t</em>}$ and the prompt $p_{i}$ are the input of GPT-4, $\hat{q<em i="i">{i}$ and $\hat{s}</em>}$ are its synthetic math problem and solution respectively. Then, we train the small LLM on $\mathcal{D<em i="i">{KD}$ to imitate the data synthesis ability of GPT-4. Finally, we leverage it to synthesize the pre-training dataset $\mathcal{D}={q</em>}}, s_{i<em i="i">{i=1}^{M}$ based on all the collected math-related texts with randomly selected prompts ${p</em>$, which are used for training our JiuZhang3.0.}, t_{i}}_{i=1}^{M</p>
<h3>2.2 Initializing Data Synthesis Model</h3>
<p>In this work, we consider the natural language reasoning and tool manipulation settings, where LLMs require solving the problem by generating a natural language solution [16] and an executable program with external interpreters [17], respectively. Thus, we train the data synthesis LLMs on the initial KD dataset, containing special prompts, math-related texts, and GPT-4 outputs for the two settings.</p>
<h4>2.2.1 Prompts for Math Problem Synthesis</h4>
<p>We aim to craft a prompt set that can well cover the knowledge points and difficulty levels in human math education. Thus, for the natural language reasoning and tool manipulation settings, we manually craft prompt templates respectively, and each corresponds to a certain education stage.</p>
<p>Prompts for Natural Language Reasoning. We consider the following 4 human education stages and 4 worldwide competitions, i.e., Grade School, Middle School, High School, and College; AMC (American Mathematics Competition) 8, AMC 10, AMC 12, and AIME (American Invitational Mathematics Examination). Based on these, we design 8 prompts with corresponding instructions and guidelines. We show an example of grade school math problem synthesis:
## Instruction: Create an age-appropriate math word problem for grade school students based on the provided math content.
## Guidelines: [Problem]: Craft a concise math word problem suitable for grade school, focusing on basic arithmetic operations, number sense, simple shapes, ... [Solution]: Provide a clear, step-by-step solution to the problem using simple language that a grade school student could understand, ...</p>
<p>Prompts for Tool Manipulation. We consider 2 types of math problems,i.e., Grade School and Secondary School Competitions, as the competition math problems may need tools for advanced math operations (e.g., integral computation), while grade school math problems are much easier and can be solved by basic operations (e.g., $+-{ }^{\circ} /$ ). As tool manipulation solves the problem via executable programs, we use more words in prompts to emphasize the data format and show an example as follows:
## Instruction: Please gain inspiration from the following random math content to create a highquality ... Present your output in two distinct sections: [Problem Description] and [Solution].
## Guidelines: [Problem]: This should be completely self-contained, providing all the contextual information one needs to understand, ... [Solution]: Offer a comprehensive, correct solution that accurately addresses the [Problem] you provided using Python code, ...
## Example:[Problem Description] Janet buys 3 pounds of broccoli, ... [/Problem Description] [Solution] def spending(): cost = 4 ...[/Solution]</p>
<h1>2.2.2 Knowledge Distillation from GPT-4</h1>
<p>Based on the prompts, we build the KD dataset to train our data synthesis LLM. We randomly sample 5,336 math-related texts, and concatenate each one with a randomly selected prompt, to compose the input i.e., $\left[p_{i} ; w_{i}\right]$. Then, we feed it into GPT-4, and extract the synthetic math problem $\hat{q}<em i="i">{i}$ and solution $\hat{s}</em>}$ from its output using regular expressions, to compose the KD dataset $\mathcal{D<em i="i">{K D}=\left{p</em>}, w_{i}, \hat{q<em i="i">{i}, \hat{s}</em>\right}<em c="c" t="t" x="x">{i=1}^{N</em>$. Next, we utilize it to train the synthesis model, and the learning objective is:}</p>
<p>$$
L\left(\theta_{s y n}\right)=\sum_{i=1}^{N_{t x c}} \log P\left(\left[\hat{q}<em i="i">{i} ; \hat{s}</em>\right]\right)
$$}\right] \mid\left[p_{i} ; w_{i</p>
<p>where $\theta_{s y n}$ denotes the parameters of the data synthesis LLM. In this way, we can teach it to generate new math problem-solution pairs based on prompts and math-related texts by imitating GPT-4.</p>
<h3>2.3 Boosting Synthesis Model using High-Value Data</h3>
<p>After initialization, we further improve the data synthesis LLM using high-value KD data. However, it would be costly if we first utilize GPT-4 to generate candidates and then select high-value ones. Therefore, we propose an efficient way that leverages the data synthesis LLM for generating candidates, and then selects valuable ones to feed into GPT-4. Specifically, we incorporate the gradient-based method [18] to estimate the influence of each synthetic instance for downstream math-related tasks, and select the top-ranking ones to update the KD dataset for retraining the data synthesis LLM.</p>
<h3>2.3.1 Gradient-based Data Value Estimation</h3>
<p>According to the influence formulation [19], at a certain training step of a model parameterized by $\theta$, the influence of a training instance $z$ on another instance $z^{\prime}$ can be estimated by computing the similarity between their produced gradients, denoted as:</p>
<p>$$
\operatorname{Inf}\left(z, z^{\prime}\right) \propto \operatorname{Sim}\left(\nabla l(z, \theta), \nabla l\left(z^{\prime}, \theta\right)\right)
$$</p>
<p>By using it, we can measure the value of each synthetic data by computing its gradient similarity with downstream math-related task data. Concretely, we first train a reference model using LoRA, then compute its gradients on LoRA parameters as the features, to help estimate the data value.</p>
<p>Training Reference Model using LoRA. Inspired by existing work [18], we train a LLM for mathematical problem solving using LoRA [20] as the reference model. As LoRA only requires to optimize the low-rank adapters in the LLM, we can efficiently train the reference model on limited computation resources, and reduce the number of trainable parameters for efficient computation of gradient similarity. Besides, to further reduce the training cost, we randomly select a subset of synthetic math problems generated by the data synthesis model, denoted as $\mathcal{D}<em i="i">{\text {lora }}=\left{q</em>\right}},s_{i<em l="l">{i=1}^{M</em>$. Then, we train the reference model to predict the solution based on the given problem, denoted as:}</p>
<p>$$
L_{\text {ref }}\left(\theta_{\text {lora }}\right)=\sum_{i=1}^{M_{l}} \log P\left(s_{i} \mid q_{i}\right)
$$</p>
<p>where $\theta_{\text {lora }}$ denotes the parameters of LoRA, and $m_{l}$ is the number of training data.
Computing Gradient Features. After training the reference model, we compute the gradients of LoRA parameters as the feature of each synthetic instance. As their dimension is large, we follow existing work [21] that performs random projection to obtain the low-dimensional features as:</p>
<p>$$
\hat{\nabla} l_{\text {ref }}\left(z, \theta_{\text {lora }}\right)=\Pi^{\top} \nabla l_{\text {ref }}\left(z, \theta_{\text {lora }}\right)
$$</p>
<p>where $z=\left\langle p_{i}, w_{i}, q_{i}, s_{i}\right\rangle$ denotes a synthetic instance, $\Pi \in \mathbb{R}^{d^{\prime} \times d}$ is a projection matrix initialized by the Rademacher distribution, its entries are -1 or $1, d^{\prime}$ and $d$ are the dimensions before and after projection, respectively. According to the Johnson-Lindenstrauss Lemmas [22], this operation can nearly preserve the gradient distances, ensuring the usefulness of the low-dimensional features.</p>
<p>Estimating Data Value. By using Eq. 4, we can compute the gradient features for synthetic instances. Then, we randomly sample $M_{D}$ instances from the training sets of downstream mathrelated datasets $\left{z_{i}^{\prime}\right}<em D="D">{i=1}^{M</em>}}$, where $z_{i}^{\prime}=\left\langle\hat{q<em i="i">{i}, \hat{s}</em>\right\rangle$, and also compute their gradient features. Next, we estimate the value of each synthetic instance by computing the similarity between its gradient feature and the average feature of all the sampled downstream instances as:</p>
<p>$$
V(z)=\operatorname{Cosine}\left(\hat{\nabla} l_{\text {ref }}\left(z, \theta_{\text {lora }}\right), \frac{1}{M_{D}} \sum_{i=1}^{M_{D}} \hat{\nabla} l_{\text {ref }}\left(z_{i}^{\prime}, \theta_{\text {lora }}\right)\right)
$$</p>
<p>where $\operatorname{Cosine}(x, y)$ computes the cosine similarity between the two vectors. In this way, the instance with higher data value would lead to a more positive influence on the downstream math-related tasks.</p>
<h1>2.3.2 Retraining Data Synthesis Model</h1>
<p>Based on the estimated values, we can rank all the synthetic instances, and the top-ranking $N_{\text {add }}$ ones can be regarded as the most valuable data $\left{\left\langle p_{i}, w_{i}, q_{i}, s_{i}\right\rangle\right}<em _add="{add" _text="\text">{i=1}^{N</em>}}}$ for improving downstream math-related tasks. Thus, we utilize GPT-4 to regenerate the synthetic math problems based on their prompts and original math-related texts, to acquire corresponding more high-quality math problems and solutions. Then, we add the new GPT-4 synthetic data $\left{p_{i}, w_{i}, \hat{q<em i="i">{i}, \hat{s}</em>\right}<em _add="{add" _text="\text">{i=1}^{N</em>$ into the KD dataset, and the new data is capable of guiding the small LLM to generate more useful math problems for downstream tasks. Next, we retrain the data synthesis LLM with the updated KD dataset using Eq. 1.}}</p>
<h3>2.4 Pre-training JiuZhang3.0 using Synthetic Data</h3>
<p>After training the data synthesis LLM, we construct the multi-source corpus containing rich mathrelated texts to cover more knowledge and scenarios. Then, we synthesize massive math problems based on it, which are used for pre-training JiuZhang3.0.</p>
<p>Constructing Multi-source Corpus. We consider the following data types and select the corresponding open-source datasets to compose the math-related multi-source corpus.</p>
<ul>
<li>Webpages: we use the OpenWebText corpus [23], which consists of 6.3M math-related web documents extracted from Common Crawl.</li>
<li>
<p>Books: we use the Mathpile-textbook dataset [24], including 4K educational textbooks, lecture notes and synthetic books.</p>
</li>
<li>
<p>Papers: we use the Mathpile-Arxiv dataset [24], and select the high-quality ones according to the estimated scores (0.6-0.9), which are released by AutoMathText [25].</p>
</li>
<li>QA Data: we select the StackExchange subset of the MMIQC dataset [26], which contains 1.2M processed real-world math question-answering pairs.</li>
<li>Wikipedia: we use the Mathpile-Wikipedia dataset [24], consisting of 106K documents from math-related entries in Wikipedia.</li>
</ul>
<p>Data Synthesis for Training JiuZhang3.0. For each instance within the multi-source corpus, we randomly select a prompt from the prompt set and embed the text into the prompt to compose the input. Then, we feed inputs into the data synthesis model, to generate the math problems and solutions for composing the synthesis dataset $\mathcal{D}=\left{q_{i}, s_{i}\right}_{i=1}^{M}$. Here, we follow existing work [5, 13] to filter out the instances with 10-grams overlap to both inputs and outputs from test sets of downstream evaluation tasks. We synthesize about 6M math problems ( 4.6 B tokens) in total, which are used for pre-training JiuZhang3.0 to predict the solution based on the given problems.</p>
<h1>3 Experiments</h1>
<h3>3.1 Experimental Settings</h3>
<p>For our JiuZhang3.0, we follow existing work [13] that train the 7B, 8B and $8 \times 7$ B versions based on Mistral-7B [27], LLaMA-3-8B [28], and Mixtral-8×7B [29]. During training, we first pre-train it on our synthetic 4.6B math problem-solution pairs and then fine-tune it on the collected multiple open-source instruction datasets. We evaluate JiuZhang3.0 in two settings, i.e., natural language reasoning and tool manipulation. More details about the fine-tuning data, evaluation datasets, baseline methods, and implementation details are in Appendix D, E, F and G, respectively.</p>
<h3>3.2 Results and Analysis</h3>
<p>Natural Language Reasoning. The results of this setting are shown in Table 1. First, the baseline methods trained on math-related data perform better than others. Among them, DeepSeekMath-7B is the best-performed base LLM, and DeepSeekMath-7B-RL also performs better than other baselines, since they have been pre-trained on 120B corpus containing rich math-related data. Besides, KPMathDSMath-7B and MAmmoTH2 also perform well. Concretely, KPMath-DSMath-7B is trained on nearly 1M synthetic math problems produced by GPT-4, and MAmmoTH2 also utilizes the GPT-4, Mixtral-8×7B, and Qwen-72B to extract and refine the problems existing in the webpages. The acquired problems can greatly improve their performance in math problem solving. In our approach, we also utilize synthetic math problems to train our JiuZhang3.0-7B and 8B models. Differently, our used synthesis model is a much smaller 7B LLM, which has been trained by distilling the data synthesis capability from GPT-4. Thus, it can guarantee the quality of the synthetic data, and helps JiuZhang3.0 models perform the best across most of the dataset. The higher quality also reduces the data amount requirement for pre-training. Owing to our designed high-value data selection strategy, we can also reduce the times of invoking GPT-4 API for knowledge distillation. As noted in Figure 1, the total cost of our approach is nearly only $20 \%$ of the compared baselines, indicating its efficiency.</p>
<p>The results of other datasets with different data formats or related to other fields are shown in Table 2. As the listed datasets focus on evaluating the different aspects, the performance of LLMs also differ a lot. For TabMWP, AQuA, and OCW-Math, our JiuZhang3.0-8B and JiuZhang3.0-8×7B achieve the best performance. The three datasets require the understanding of table data, algebra, and undergraduate-level science knowledge respectively, which may have been covered in our synthetic math problems guided by the multi-source corpus. However, our JiuZhang3.0 models perform not well on MMLU-STEM. It indicates the shortcoming of our approach that our prompts and math-related texts might not well cover the knowledge from other subjects.</p>
<p>Tool Manipulation. The results are shown in Table 3. We can see that our JiuZhang3.0-7B and 8B models outperform all the baseline methods by a large margin, indicating the effectiveness of our approach in this setting. The reason is that we synthesize massive math problems in this format, which can teach JiuZhang3.0 models to accurately utilize tools by generating programs. Besides, the mixed synthetic math problem from the natural language reasoning setting can also benefit the required</p>
<p>Table 1: Results on 6 datasets in the natural language reasoning setting. The best and second-best ones among LLMs with similar scales are marked in bold and underlined respectively.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>GSM8k</th>
<th>MATH</th>
<th>SVAMP</th>
<th>ASDiv</th>
<th>MAWPS</th>
<th>CARP</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>ChatGPT</td>
<td>76.6</td>
<td>38.2</td>
<td>83.7</td>
<td>87.7</td>
<td>96.9</td>
<td>41.3</td>
<td>70.7</td>
</tr>
<tr>
<td>GPT-4</td>
<td>92.2</td>
<td>65.4</td>
<td>92.9</td>
<td>94.3</td>
<td>96.6</td>
<td>53.6</td>
<td>82.5</td>
</tr>
<tr>
<td>Qwen-1.5-110B</td>
<td>85.4</td>
<td>49.4</td>
<td>86.2</td>
<td>85.1</td>
<td>94.3</td>
<td>53.6</td>
<td>75.7</td>
</tr>
<tr>
<td>Qwen-1.5-72B</td>
<td>77.6</td>
<td>39.4</td>
<td>83.1</td>
<td>85.1</td>
<td>95.8</td>
<td>53.0</td>
<td>72.3</td>
</tr>
<tr>
<td>Mixtral-8×7B</td>
<td>74.4</td>
<td>29.0</td>
<td>76.5</td>
<td>78.5</td>
<td>93.9</td>
<td>38.8</td>
<td>65.2</td>
</tr>
<tr>
<td>Llemma-34B</td>
<td>60.2</td>
<td>24.6</td>
<td>68.0</td>
<td>75.6</td>
<td>89.8</td>
<td>36.5</td>
<td>59.1</td>
</tr>
<tr>
<td>Intern-Math-20B</td>
<td>64.9</td>
<td>27.4</td>
<td>74.9</td>
<td>79.6</td>
<td>94.4</td>
<td>42.3</td>
<td>63.9</td>
</tr>
<tr>
<td>ChatGLM-Math-32B</td>
<td>82.6</td>
<td>40.6</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>MAmmoTH2-8x7B-Plus</td>
<td>86.4</td>
<td>47.0</td>
<td>90.0</td>
<td>92.2</td>
<td>97.0</td>
<td>45.8</td>
<td>76.4</td>
</tr>
<tr>
<td>JiuZhang3.0-8x7B (Ours)</td>
<td>89.8</td>
<td>53.8</td>
<td>90.2</td>
<td>93.1</td>
<td>96.7</td>
<td>52.3</td>
<td>79.3</td>
</tr>
<tr>
<td>DeepSeek-7B</td>
<td>13.6</td>
<td>4.8</td>
<td>40.8</td>
<td>52.1</td>
<td>65.4</td>
<td>10.3</td>
<td>31.2</td>
</tr>
<tr>
<td>Mistral-7B</td>
<td>41.2</td>
<td>13.6</td>
<td>64.7</td>
<td>68.5</td>
<td>87.5</td>
<td>14.9</td>
<td>48.4</td>
</tr>
<tr>
<td>LLaMA-3-8B</td>
<td>54.5</td>
<td>19.6</td>
<td>68.5</td>
<td>72.8</td>
<td>90.5</td>
<td>29.2</td>
<td>55.9</td>
</tr>
<tr>
<td>Gemma-7B</td>
<td>54.1</td>
<td>19.6</td>
<td>69.7</td>
<td>74.2</td>
<td>89.0</td>
<td>30.5</td>
<td>56.2</td>
</tr>
<tr>
<td>Qwen-1.5-7B</td>
<td>60.5</td>
<td>28.2</td>
<td>64.9</td>
<td>74.9</td>
<td>90.1</td>
<td>38.6</td>
<td>59.5</td>
</tr>
<tr>
<td>Llemma-7B</td>
<td>39.2</td>
<td>18.4</td>
<td>56.9</td>
<td>69.0</td>
<td>82.7</td>
<td>31.8</td>
<td>49.7</td>
</tr>
<tr>
<td>InternLM-Math-7B</td>
<td>45.9</td>
<td>15.8</td>
<td>67.3</td>
<td>71.2</td>
<td>88.3</td>
<td>28.0</td>
<td>52.8</td>
</tr>
<tr>
<td>Rho-1-Math-7B</td>
<td>66.3</td>
<td>31.0</td>
<td>78.5</td>
<td>79.2</td>
<td>94.0</td>
<td>36.7</td>
<td>64.3</td>
</tr>
<tr>
<td>DeepSeekMath-7B</td>
<td>64.1</td>
<td>34.2</td>
<td>73.7</td>
<td>82.7</td>
<td>92.7</td>
<td>44.4</td>
<td>65.3</td>
</tr>
<tr>
<td>Mistral-7B-MMIQC</td>
<td>75.0</td>
<td>34.2</td>
<td>73.5</td>
<td>82.1</td>
<td>90.1</td>
<td>36.5</td>
<td>65.2</td>
</tr>
<tr>
<td>MetaMath-Mistral-7B</td>
<td>77.8</td>
<td>29.6</td>
<td>79.6</td>
<td>81.2</td>
<td>93.7</td>
<td>30.5</td>
<td>65.4</td>
</tr>
<tr>
<td>Abel-7B-002</td>
<td>80.4</td>
<td>29.6</td>
<td>78.8</td>
<td>82.7</td>
<td>93.5</td>
<td>33.2</td>
<td>66.4</td>
</tr>
<tr>
<td>WizardMath-7B-1.1</td>
<td>82.2</td>
<td>32.8</td>
<td>80.7</td>
<td>84.2</td>
<td>93.8</td>
<td>31.9</td>
<td>67.6</td>
</tr>
<tr>
<td>Math-Shepherd-Mistral-7B</td>
<td>84.3</td>
<td>34.4</td>
<td>82.9</td>
<td>82.8</td>
<td>92.5</td>
<td>32.9</td>
<td>68.3</td>
</tr>
<tr>
<td>KPMath-DSMath-7B</td>
<td>83.9</td>
<td>48.8</td>
<td>81.5</td>
<td>88.9</td>
<td>94.8</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>MAmmoTH2-7B-Plus</td>
<td>84.2</td>
<td>46.2</td>
<td>90.3</td>
<td>90.3</td>
<td>95.8</td>
<td>44.3</td>
<td>75.2</td>
</tr>
<tr>
<td>MAmmoTH2-8B-Plus</td>
<td>84.4</td>
<td>41.2</td>
<td>89.9</td>
<td>89.9</td>
<td>97.1</td>
<td>44.8</td>
<td>74.6</td>
</tr>
<tr>
<td>DeepSeekMath-7B-Instruct</td>
<td>82.3</td>
<td>45.8</td>
<td>83.7</td>
<td>90.1</td>
<td>95.7</td>
<td>45.8</td>
<td>73.9</td>
</tr>
<tr>
<td>DeepSeekMath-7B-RL</td>
<td>88.2</td>
<td>50.2</td>
<td>87.3</td>
<td>91.8</td>
<td>95.5</td>
<td>51.6</td>
<td>77.4</td>
</tr>
<tr>
<td>JiuZhang3.0-7B (Ours)</td>
<td>88.6</td>
<td>52.8</td>
<td>90.4</td>
<td>92.6</td>
<td>97.3</td>
<td>51.0</td>
<td>78.8</td>
</tr>
<tr>
<td>JiuZhang3.0-8B (Ours)</td>
<td>88.6</td>
<td>51.0</td>
<td>89.4</td>
<td>92.6</td>
<td>97.1</td>
<td>50.9</td>
<td>78.3</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Performance changes with the increasing of the pre-training data proportion for our approach. We also show the best-performed base LLM DeepSeekMath-7B using dashed line.</p>
<p>capabilities for this setting. Different from the natural language reasoning setting, JiuZhang3.0-8B (based on LLaMA-3-8B) performs better than the 7B version (based on Mistral-7B). The reason may be that LLaMA-3-8B owns stronger code synthesis and tool manipulation capability than Mistral-7B.</p>
<h3>3.3 Further Analysis</h3>
<p>Performance w.r.t. Pre-training Data Amount. In this part, we study how the scaling of synthetic data amount affects the model performance. We train Mistral-7B and LLaMA-3-8B using varying ratios of our synthetic entire dataset, i.e., 20%, 40%, 60%, 80%, 100%, and report the performance</p>
<p>Table 2: Results on 5 other datasets with different data formats or related to interdisciplinary fields, and we abbreviate MMLU-STEM into M-STEM. The best and second-best methods among LLMs with similar scales are marked in bold and underlined respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">TabMWP</th>
<th style="text-align: center;">AQuA</th>
<th style="text-align: center;">SAT-Math</th>
<th style="text-align: center;">M-STEM</th>
<th style="text-align: center;">OCW-Math</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">57.7</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">73.6</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-1.5-110B</td>
<td style="text-align: center;">$\underline{80.5}$</td>
<td style="text-align: center;">$\underline{64.6}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: center;">$\underline{71.5}$</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">$\underline{63.6}$</td>
</tr>
<tr>
<td style="text-align: left;">Qwen-1.5-72B</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">55.0</td>
</tr>
<tr>
<td style="text-align: left;">Mixtral-8x7B</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">50.4</td>
</tr>
<tr>
<td style="text-align: left;">Llemma-34B</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">11.8</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: left;">Intern-Math-20B</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">44.1</td>
<td style="text-align: center;">65.6</td>
<td style="text-align: center;">62.3</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">48.5</td>
</tr>
<tr>
<td style="text-align: left;">MAmmoTH2-8x7B-Plus</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">$\mathbf{7 1 . 8}$</td>
<td style="text-align: center;">$\underline{18.8}$</td>
<td style="text-align: center;">58.1</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-8x7B (Ours)</td>
<td style="text-align: center;">$\mathbf{8 4 . 7}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 4}$</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">$\mathbf{2 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Mistral-7B</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">36.1</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-3-8B</td>
<td style="text-align: center;">67.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">54.4</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">46.5</td>
</tr>
<tr>
<td style="text-align: left;">Gemma-7B</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">47.6</td>
</tr>
<tr>
<td style="text-align: left;">Llemma-7B</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">45.8</td>
<td style="text-align: center;">7.7</td>
<td style="text-align: center;">40.6</td>
</tr>
<tr>
<td style="text-align: left;">Rho-1-Math-7B</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">54.9</td>
<td style="text-align: center;">6.2</td>
<td style="text-align: center;">48.2</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeekMath-7B</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">$\underline{84.4}$</td>
<td style="text-align: center;">56.1</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">55.9</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeekMath-7B-Instruct</td>
<td style="text-align: center;">70.5</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">$\underline{84.4}$</td>
<td style="text-align: center;">57.9</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">58.6</td>
</tr>
<tr>
<td style="text-align: left;">MAmmoTH2-7B-Plus</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
<td style="text-align: center;">$\underline{84.4}$</td>
<td style="text-align: center;">$\underline{64.0}$</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">56.1</td>
</tr>
<tr>
<td style="text-align: left;">MAmmoTH2-8B-Plus</td>
<td style="text-align: center;">$\underline{75.1}$</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">$\mathbf{8 7 . 5}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 7}$</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">$\underline{60.1}$</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-7B (Ours)</td>
<td style="text-align: center;">74.8</td>
<td style="text-align: center;">59.4</td>
<td style="text-align: center;">81.2</td>
<td style="text-align: center;">53.6</td>
<td style="text-align: center;">$\underline{20.2}$</td>
<td style="text-align: center;">57.8</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-8B (Ours)</td>
<td style="text-align: center;">$\mathbf{7 9 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 2}$</td>
<td style="text-align: center;">$\underline{84.4}$</td>
<td style="text-align: center;">60.4</td>
<td style="text-align: center;">$\mathbf{2 1 . 3}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on 6 mathematical reasoning datasets under the tool manipulation setting. The best and second-best methods are marked in bold and underlined respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">G-Hard</th>
<th style="text-align: center;">SVAMP</th>
<th style="text-align: center;">TabMWP</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">MAWPS</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ChatGPT (PAL)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">38.7</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4 (PAL)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">95.9</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">89.3</td>
</tr>
<tr>
<td style="text-align: left;">CodeLLama</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MAmmoTH-7B-Mistral</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MathCoder-7B-CL</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">ToRA-7B-Code</td>
<td style="text-align: center;">72.6</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">66.5</td>
</tr>
<tr>
<td style="text-align: left;">MARIO-OVM-7B</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">MMOS-CODE-7B</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">44.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">OpenMath-Mistral-7B</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">82.4</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: left;">Rho-1-Math-7B-Code</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">75.3</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-7B (Ours)</td>
<td style="text-align: center;">$\underline{82.4}$</td>
<td style="text-align: center;">$\underline{53.0}$</td>
<td style="text-align: center;">$\mathbf{6 4 . 9}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 2}$</td>
<td style="text-align: center;">$\underline{75.6}$</td>
<td style="text-align: center;">$\mathbf{8 8 . 3}$</td>
<td style="text-align: center;">$\underline{96.6}$</td>
<td style="text-align: center;">$\underline{78.6}$</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-8B (Ours)</td>
<td style="text-align: center;">$\mathbf{8 2 . 9}$</td>
<td style="text-align: center;">$\mathbf{5 3 . 4}$</td>
<td style="text-align: center;">$\underline{64.4}$</td>
<td style="text-align: center;">$\mathbf{8 9 . 2}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 9}$</td>
<td style="text-align: center;">$\underline{87.5}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 3}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 2}$</td>
</tr>
</tbody>
</table>
<p>on GSM8k, MATH, and ASDiv under the natural language reasoning setting. For comparison, we also show the results of the best-performed base LLM, i.e., DeepSeekMath-Base-7B.</p>
<p>As shown in Figure 3, with the increasing of the training data ratio, the performance of our model improves consistently. Based on Mistral-7B, it can outperform the best-performed baseline using only $80 \%$ or $60 \%$ of the pre-training data, indicating the high quality of our synthetic pre-training data. Based on LLaMA-3-8B, it can perform better than the baseline using $40 \%$ or even $20 \%$ data, and the performance is consistently better than using Mistral-7B. It demonstrates that LLaMA-3-8B can better adapt into our synthetic data. Besides, the performance of our model can surpass the baseline more on MATH, which is a very complex dataset consisting of competitive problems, exhibiting the superiority of our method for improving the advanced mathematical reasoning capability.</p>
<p>Ablation Study. We conduct the ablation study to verify the effectiveness of key components in our proposed method. We test the following variations based on our approach, i.e., (1) w/o Prompt Set: uses a simple prompt for guiding data synthesis instead of our crafted prompt set; (2) w/o Mathrelated Texts: directly synthesizes the math problems without math related texts; (3) w/o Boosting</p>
<p>Table 4: Ablation and variation studies in the natural language reasoning setting. We randomly sample 100k synthetic data and 50k instruction data for efficient test.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Variation</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">GSM8k</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">ASDiv</th>
<th style="text-align: center;">CARP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">36.2</td>
</tr>
<tr>
<td style="text-align: center;">Ablation</td>
<td style="text-align: center;">w/o Prompt Set</td>
<td style="text-align: center;">76.9</td>
<td style="text-align: center;">27.8</td>
<td style="text-align: center;">81.4</td>
<td style="text-align: center;">34.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o Math-related Texts</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">31.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o Boosting Retraining</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o Value Estimation</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o using GPT-4 for Boosting</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: center;">Synthesis LLM</td>
<td style="text-align: center;">- ChatGPT</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Mixtral-8×7B</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- DeepSeekMath-RL-7B</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">32.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- LLaMA-3-8B-Instruct</td>
<td style="text-align: center;">75.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">31.4</td>
</tr>
<tr>
<td style="text-align: center;">Data Selection</td>
<td style="text-align: center;">- Random Sampling</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">34.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Perplexity</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">30.8</td>
<td style="text-align: center;">83.1</td>
<td style="text-align: center;">36.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- Reward Model</td>
<td style="text-align: center;">78.0</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">34.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">- One-shot ICL</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">36.0</td>
</tr>
</tbody>
</table>
<p>Retraining: uses the data synthesis model without retraining; (4) w/o Value Estimation: ignores the estimated value but randomly samples the instances for boosting training; (5) w/o using GPT-4 for Boosting: directly uses the high-value instance for boosting data synthesis model instead of using GPT-4. Limited by the computing resource, we conduct the ablation study under the natural language reasoning setting, and use 100k synthetic instances and randomly select 50k instructions from the instruction set. We report the results on GSM8K, MATH, ASDiv and CARP-en.</p>
<p>As shown in Table 4, all the variations mostly underperform the original model, indicating the effectiveness of all the components. Among them, the variation w/o using GPT-4 for Boosting performs slightly better in GSM8k and CARP, but degrades a lot in MATH (32.8 $\longrightarrow$ 27.8). A possible reason is that it can benefit from the selected high-value data. But without the help of GPT-4, it can not synthesize helpful complex math problems for the competitive problems within MATH dataset.</p>
<p>Variation Study for Data Synthesis LLMs. To verify the effectiveness of our trained data synthesis LLM, we conduct the variation study using other existing LLMs for synthesizing the pre-training data. We select the following four LLMs, i.e., ChatGPT, Mixtral-8×7B, DeepSeekMath-RL-7B, and LLaMA-3-8B-Instruct to replace our data synthesis LLM. We follow the efficient test setting in the ablation study, and report the results on GSM8K, MATH, ASDiv and CARP-en.</p>
<p>As shown in Table 4, all the variations mostly perform worse than the original model. It demonstrates that existing LLMs without adapted training might not be suitable to directly synthesize the data for pre-training. Besides, the performance of all the variation degrades a lot in MATH, which consists of complex competitive problems. It indicates that these existing LLMs are hard to synthesize the data that is useful for improving the performance in solving complex math problems.</p>
<p>Variation Study for Data Selection Strategies. To study the effectiveness of our gradient-based data selection strategy, we implement the following variations that replace it by other methods, i.e., (1) Random Sampling: randomly samples the same number of instances; (2) Perplexity: selects the instances with lowest perplexity evaluated by Mistral-7B; (3) Reward Model: uses a well-trained reward model [30] for scoring; (4) One-shot ICL: concatenates the synthetic math problem and solution with the downstream task data to construct the one-shot in-context learning (ICL) example, and computes the decrease of loss as the estimated value [31]. We follow the efficient test setting.</p>
<p>As shown in Table 4, our original model mostly performs the best among all the variations, indicating the superiority of our gradient-based strategy. Whereas, the variation using one-hot ICL performs relatively better than others, and achieves the best performance on GSM8k. As the problems in GSM8k typically require more natural language reasoning steps, the ICL loss can well detect the instances with helpful context for solving these problems. However, it performs not well on MATH, where the math problems are complex and require using more math symbols and formulas.</p>
<h1>4 Conclusion</h1>
<p>In this paper, we proposed an efficient way to improve the mathematical reasoning of LLMs, where we trained a small LLM to synthesize sufficient high-quality math problems for pre-training. Concretely, we crafted a set of prompts that cover the knowledge and difficulty levels of human education stages, and selected the high-value math-related texts for downstream math-related tasks via the gradientbased strategy. Then, we fed them into GPT-4 to create the knowledge distillation dataset, which can better teach the data synthesis model to generate diverse and useful math problems. We utilized the synthetic data to pre-train JiuZhang3.0, and the whole process only required to invoke GPT-4 API 9.3 k times and pre-train on 4.6B data. JiuZhang3.0 achieved state-of-the-art performance on several datasets under the natural language reasoning and tool manipulation settings, surpassing competitive LLMs that requires much larger cost on data synthesis or pre-training.</p>
<h2>References</h2>
<p>[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020.
[2] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
[3] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large language models. ArXiv, abs/2303.18223, 2023.
[4] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for mathematical reasoning. ArXiv, abs/2212.10535, 2022.
[5] Zhihong Shao, Peiyi Wang, Qihao Zhu, R. X. Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024.
[6] Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning. ArXiv, abs/2403.02333, 2024.
[7] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. ArXiv, abs/2310.10631, 2023.
[8] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.
[9] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. Internlm-math: Open math large language models toward verifiable reasoning. ArXiv, abs/2402.06332, 2024.
[10] Long Long Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zheng Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv, abs/2309.12284, 2023.
[11] Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. ArXiv, abs/2402.14830, 2024.
[12] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving. ArXiv, abs/2309.17452, 2023.</p>
<p>[13] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. 2024.
[14] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment. ArXiv, abs/2305.11206, 2023.
[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023.
[16] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. ArXiv, abs/2201.11903, 2022.
[17] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv, abs/2305.11738, 2023.
[18] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. ArXiv, abs/2402.04333, 2024.
[19] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33:19920-19930, 2020.
[20] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[21] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Mądry. Trak: attributing model behavior at scale. In Proceedings of the 40th International Conference on Machine Learning, pages 27074-27113, 2023.
[22] William B. Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into hilbert space. Contemporary mathematics, 26:189-206, 1984.
[23] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. ArXiv, abs/2310.06786, 2023.
[24] Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for math: Part i - mathpile: A billion-token-scale pretraining corpus for math. ArXiv, abs/2312.17120, 2023.
[25] Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Autonomous data selection with language models for mathematical texts. 2024.
[26] Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Chi-Chih Yao. Augmenting math word problems via iterative question composing. ArXiv, abs/2401.09003, 2024.
[27] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023.
[28] Meta. Introducing meta llama 3: The most capable openly available llm to date, April 2024. https://ai.meta.com/blog/meta-llama-3/.
[29] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts. ArXiv, abs/2401.04088, 2024.</p>
<p>[30] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees. ArXiv, abs/2404.02078, 2024.
[31] Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, and Yongbin Li. One shot learning as instruction data prospector for large language models. ArXiv, abs/2312.10302, 2023.
[32] Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical report, Anthropic, March 2024.
[33] Google Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv, abs/2403.05530, 2024.
[34] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. ArXiv, abs/2305.10601, 2023.
[35] K. Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J. Cole-Lewis, Stephen J. Pfohl, P A Payne, Martin G. Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P. A. Mansfield, Blaise Aguera y Arcas, Dale R. Webster, Greg S. Corrado, Yossi Matias, Katherine Hui-Ling Chou, Juraj Gottweis, Nenad Tomavsev, Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172 - 180, 2022.
[36] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. ArXiv, abs/2211.09085, 2022.
[37] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652, 2021.
[38] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.
[39] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.
[40] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.
[41] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Annual Meeting of the Association for Computational Linguistics, 2022.
[42] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.
[43] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv, abs/2211.12588, 2022.</p>
<p>[44] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.
[45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.
[46] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.
[47] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. 2023.
[48] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. ArXiv, abs/2309.05653, 2023.
[49] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities. ArXiv, abs/2403.04706, 2024.
[50] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, 2024.
[51] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. ArXiv, abs/2312.02120, 2023.
[52] Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, and Andrew M. Dai. Best practices and lessons learned on synthetic data for language models. ArXiv, abs/2404.07503, 2024.
[53] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. ArXiv, abs/2304.12244, 2023.
[54] Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. Reformatted alignment. ArXiv, abs/2402.12219, 2024.
[55] Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better synthetic data by retrieving and transforming existing datasets. 2024.
[56] Ruida Wang, Wangchunshu Zhou, and Mrinmaya Sachan. Let's synthesize step by step: Iterative dataset synthesis with large language models by extrapolating errors from small models. ArXiv, abs/2310.13671, 2023.
[57] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv, abs/2308.09583, 2023.
[58] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. ArXiv, abs/2402.16352, 2024.
[59] Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, Yuxian Gu, Xin Cheng, Xun Wang, Si-Qing Chen, Li Dong, Wei Lu, Zhifang Sui, Benyou Wang, Wai Lam, and Furu Wei. Synthetic data (almost) from scratch: Generalized instruction tuning for language models. ArXiv, abs/2402.13064, 2024.
[60] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv, abs/2103.03874, 2021.</p>
<p>[61] Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, and Ji rong Wen. Evaluating and improving tool-augmented computation-intensive math reasoning. ArXiv, abs/2306.02408, 2023.
[62] Peiyi Wang, Lei Li, Zhihong Shao, R. X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. ArXiv, abs/2312.08935, 2023.
[63] Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. Openmathinstruct-1: A 1.8 million math instruction tuning dataset. ArXiv, abs/2402.10176, 2024.
[64] Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, and Yi Zhou. An empirical study of data ability boundary in llms' math reasoning. ArXiv, abs/2403.00799, 2024.
[65] Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. ArXiv, abs/2106.15772, 2020.
[66] Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: A math word problem repository. In North American Chapter of the Association for Computational Linguistics, 2016.
[67] Arkil Patel, S. Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In North American Chapter of the Association for Computational Linguistics, 2021.
[68] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $158-167,2017$.
[69] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied Sanosi Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. ArXiv, abs/2304.06364, 2023.
[70] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. Measuring massive multitask language understanding. ArXiv, abs/2009.03300, 2020.
[71] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. ArXiv, abs/2206.14858, 2022.
[72] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. ArXiv, abs/2304.09842, 2023.
[73] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. ArXiv, abs/2309.16609, 2023.
[74] DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren,</p>
<p>Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu, Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. ArXiv, abs/2401.02954, 2024.
[75] Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline. ArXiv, abs/2404.02893, 2024.
[76] Gemma Team Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, L. Sifre, Morgane Riviere, Mihir Kale, J Christopher Love, Pouya Dehghani Tafti, L’eonard Hussenot, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am’elie H’eliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl’ement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Pier Giuseppe Sessa, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vladimir Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Brian Warkentin, Ludovic Peran, Minh Giang, Cl’ement Farabet, Oriol Vinyals, Jeffrey Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. ArXiv, abs/2403.08295, 2024.
[77] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023.
[78] Zheng-Wen Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need. ArXiv, abs/2404.07965, 2024.
[79] Ethan Chern, Haoyang Zou, Xuefeng Li, Jiewen Hu, Kehua Feng, Junlong Li, and Pengfei Liu. Generative ai for math: Abel. https://github.com/GAIR-NLP/abel, 2023.
[80] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. ArXiv, abs/2310.03731, 2023.
[81] Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. Mario: Math reasoning with code interpreter output - a reproducible pipeline. ArXiv, abs/2401.08190, 2024.
[82] Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning. ArXiv, abs/2404.01067, 2024.
[83] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chaochao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies. ArXiv, abs/2404.06395, 2024.</p>
<h1>A Related Work</h1>
<p>Large Language Models. LLMs have demonstrated remarkable capabilities in a variety of NLP tasks, and commercial LLMs like ChatGPT, Claude, and Gemini [2, 32, 33], represent cutting-edge capabilities. Meanwhile, the performance of open-source models (e.g., LLaMA-3, Mixtral) has also developed rapidly [28, 29]. To further improve the capability of LLMs on special tasks or domains, existing work mainly focuses on optimizing the following aspects: (1) prompt engineerings such as chain-of-thought and tree-of-thought [16, 34]; (2) continual pre-training on a domain-specific or task-specific corpus, improving the model to deal with downstream tasks [5, 7, 8, 35, 36]; (3) supervised fine-tuning, which involves fine-tuning the model on related instruction datasets, enhancing LLMs to follow special task instructions [37, 38]; (4) other strategies including RLHF [39], tool augmentation [40], decoding optimization [41] and et al. We aim to efficiently improve the capability of LLMs for mathematical reasoning by pre-training on synthetic data.</p>
<p>Mathematical Reasoning. Despite the impressive progress, mathematical reasoning remains a weak aspect of LLMs. To enhance LLMs' ability in mathematical reasoning, researchers have also proposed a surge of methods from the aspects of prompting, pre-training and fine-tuning. For prompting, the chain-of-thought (CoT) prompts have been widely used to guide LLMs for performing multi-step reasoning on complex math problems [16]. Based on it, following work utilizes tools [40, 42-45] and verifiers [9, 30, 46, 47], to further improve the accuracy of the mathematical reasoning process. For pre-training, existing work [5, 7, 9] collects a large-scale math-related corpus and continually pre-training open-source LLMs on it. In contrast, supervised fine-tuning methods focus on using relatively less high-quality data for training the LLM, which are typically mathrelated instructions [10, 12, 48]. Due to the complexity of mathematical reasoning, recent work also demands a large number of instructions, hence they rely on strong LLMs like GPT-4 to synthesize the data [6, 26, 49]. The pre-training and fine-tuning methods generally lead to large training and data annotation costs, respectively. Our work aims to train a small LLM specially for math problem synthesis, which can efficiently produce sufficient data for training.</p>
<p>Data Synthesis. For complex tasks and scenarios (e.g., mathematical reasoning), it is necessary to collect a substantial amount of data for training the LLM to enhance it. However, the available data may not be sufficient, hence researchers have explored using automatically synthetic data with consistent distribution to real data, to enrich the training corpus [50-56]. For data synthesis on mathematical reasoning tasks, existing work can be roughly categorized into the following two types, according to their based guided information. The first type of work starts with existing problems or math-related texts to synthesize similar problems or solutions [10, 26, 49, 57, 58]. The other type of work relies on available knowledge points, and devises special prompts to guide LLMs for synthesizing related problems with the solutions [6, 59]. As correctness is important, the two types of work generally design rules to check and remove wrong ones. In this work, based on the data synthesis model, we also construct the multi-source math corpora, and craft several prompts to guide it in producing diverse and useful math problems.</p>
<h2>B Limitation</h2>
<p>First, although we train a strong yet small LLM that can synthesize high-quality math problems for training, its capabilities on synthesizing the data for other domains or tasks might be relatively weaker. The reason is that we only use the math-related data to train it. In future work, we will try to train a general-purpose model, to enable the data synthesis for other requirements. Second, in this work, we only focus on mathematical reasoning capability, and our trained JiuZhang3.0 is also mainly for solving math problems. Limited by the computation resource, we do not test its performance on other complex reasoning tasks, e.g., planning, commonsense reasoning. We will also conduct the corresponding experiments in the future. Third, the 4.6B pre-training data is still a large scale for training, and we do not perform data filter to control its quality. Future work should focus on reducing its scale by proposing better data filter strategy. Forth, we only utilize GPT-4 for knowledge distillation, but do not use other well-performed LLMs, e.g., Claude 3, GLM-4, and the latest GPT-4o. More experiments should be conducted on these LLMs to study the effect of the teacher LLMs. Fifth, for cost estimation, our reported results are all estimated according to our experience, without the</p>
<p>Table 5: The price of the potential service during the training procedure.</p>
<table>
<thead>
<tr>
<th>Items</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenAI API</td>
<td>30.00 USD per 1M tokens for input, and 60.00 USD per 1M tokens for output.</td>
</tr>
<tr>
<td>AWS GPU Server</td>
<td>40.96 USD per $8 \times$ A100 per hour.</td>
</tr>
</tbody>
</table>
<p>Table 6: The estimated cost of different LLMs using the official GPT-4 API and 8 nodes of $8 \times$ A100 GPU servers for training.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Models</th>
<th style="text-align: center;">#API Calling (Token)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">#Server Time (Hour)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total Expenses</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Input</td>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">Data Num.</td>
<td style="text-align: center;">Synthesizing</td>
<td style="text-align: center;">Training</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">KPMath-DSMath-7B</td>
<td style="text-align: center;">1090</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">865 K</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">39,599 USD</td>
</tr>
<tr>
<td style="text-align: left;">DeepSeekMath-7B-RL</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">52,428 USD</td>
</tr>
<tr>
<td style="text-align: left;">JiuZhang3.0-7B</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">877</td>
<td style="text-align: center;">10 K</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">8,480 USD</td>
</tr>
</tbody>
</table>
<p>re-implementation of these methods. We also appeal to report the true cost of training LLMs in existing work.</p>
<h1>C Cost Estimation</h1>
<p>To estimate the expenses of previous work and our proposed JiuZhang3.0, we survey the price of potential service during the entire procedure, including calling the OpenAI API for GPT-4 and renting the AWS GPU server for LLMs training. The details of the price are presented in Table 5. For a fair comparison, we assume that GPT-4 is utilized to synthesize training data, and 8 nodes of $8 \times$ A100 GPU servers ( 64 GPUs in total) are leveraged for LLMs training.
For our data synthesis process, the average length of the prompting (including selected math-related texts) is about 300 tokens, and the average length of problems and solutions is 877 tokens. Under the setting of 64 GPUs, we spend 4 hours selecting valuable data for natural language reasoning and tool manipulation in total, and 10 hours synthesizing the 4.6B pre-training corpus. We train the JiuZhang3.0-7B model for 10 hours, including both pre-training and fine-tuning.
As the dataset and the construction process of KPMath are not publicly available, we adopt the average length of synthesis prompts, problems, and solutions in MetaMathQA for estimating the input and the output tokens, respectively. We estimate the training time on 120B tokens for a 7B model as 160 hours. Since the training details in the RLHF stage of DeepSeekMath-7B-RL are not publicly available, we do not count the fine-tuning cost for the KPMath model and the DeepSeekMath model. In this case, we estimate the expenses of the LLMs training process as follows,</p>
<p>$$
\begin{aligned}
\text { API Expenses } &amp; =(\text { Avg. Input Length } \times \text { API Input Price } \
&amp; + \text { Avg. Output Length } \times \text { API Output Price }) \times \text { Num Data } \
\text { Server Expenses } &amp; =\text { Num Nodes } \times \text { Price per Node } \times \text { Training Time } \
\text { Total Expenses } &amp; =\text { API Expenses }+ \text { Server Expenses. }
\end{aligned}
$$</p>
<p>The details and estimation of the expenses for different LLMs are present in Table 6.</p>
<h2>D Fine-tuning Data</h2>
<p>After pre-training, we collect a set of open-source math-related instructions to fine-tune JiuZhang3.0. For natural language reasoning, we collect the training sets of MATH [60], GSM8k [46], CARP [61], and open-source synthetic datasets based on them, i.e., MetaMATH [10], MMIQC [26], MathShepherd (without problems from original MATH test sets) [62], Orca-MATH [11]. Besides, we also collect the positive examples from the PRM800k dataset (without problems from original MATH test sets) [47], and the TAL-SCQ5K dataset consisting of multi-choice questions ${ }^{3}$. Whereas, we observe</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that the varying data styles in the above datasets might cause the LLM outputs to be irregular. Thus, we utilize a unified prompt DeepSeekMath-7B-RL, to synthesize 700k solutions for the problems from the above datasets.</p>
<p>For tool manipulation, we use the synthetic datasets, i.e., OpenMathInstruct-1 [63] and MMOS [64], consisting of a mixture of text reasoning and code blocks executed by a Python interpreter.</p>
<h1>E Evaluation Datasets</h1>
<p>We test our JiuZhang3.0 and baseline methods in the following two settings for evaluating the mathematical reasoning capability.</p>
<ul>
<li>Natural Language Reasoning: we prompt LLMs to perform multi-step reasoning via natural language, and select the following publicly avaibable datasets: GSM8k [46] contains grade school math problems to test the basic arithmetic and reasoning ability. MATH [60] and CARP-en [61] consist of complex competition-level problems, and CARP-en is the English version of the original CARP dataset. ASDiv [65], MAWPS [66] and SVAMP [67] are grade-school math word problem (MWP) datasets, and SVAMP focuses on the robust reasoning ability. Besides, we also consider the following datasets with different data formats or related to other interdisciplinary field, i.e., TabMWP, AQuA, SAT-Math, MathQA, MMLU-STEM. AQuA [68], SAT-Math [69], MMLU-STEM [70] are composed of multiple-choice questions for human exams across math and other STEM disciplines. OCW-Math [71] is a challenging dataset containing undergraduate-level math and science problems.</li>
<li>Tool Manipulation: we prompt LLMs to manipulate external tools via Python to solve the problems, and select GSM8k, MATH, GSM-Hard, SVAMP, TabMWP, ASDiv, and MAWPS for testing. GSMHard [40] replaces the numbers in the questions of GSM8K with larger numbers to increase the difficulty of calculation. TabMWP [72] is an open-domain MWP dataset containing tabular data.</li>
</ul>
<h2>F Baseline Methods</h2>
<p>We consider diverse types of baseline methods for comparison.</p>
<ul>
<li>Closed-source LLMs: ChatGPT and GPT-4 [2];</li>
<li>Larger LLMs ( $&gt;20 B$ ): Qwen-1.5-110B [73], Qwen-1.5-72B [73], Deepseek-LM-67B [74], Mixtral$8 \times 7$ B [29], Llemma-34B [7], Intern-Math-20B [9], MAmmoTH2-8×7B-Plus [13], ChatGLM-Math32B [75];</li>
<li>Smaller LLMs (&lt;10B): DeepSeek-7B [74], Qwen-1.5-7B [73], Mistral-7B [27], LLaMA-3-8B [28], Gemma-7B [76], and CodeLLama [77];</li>
<li>LLMs pre-trained on Math Corpus (&lt;10B): Llemma-7B [7], InternLM-Math-7B [9], Rho-1-Math7B [78], DeepSeekMath-7B [5];</li>
<li>LLMs fine-tuned on Math Instructions (&lt;10B): MetaMath-Mistral-7B [10], WizardMath-7B-1.1 [57], Abel-7B-002 [79], Mistral-7B-MMIQC [26], Math-Shepherd-Mistral-7B-RL [62], DeepSeekMath-7B-Instruct [5], DeepSeekMath-7B-RL [5], Llama-3-8B-Instruct [28], MAmmoTH2 [13], KPMath-DSMath-7B [6]</li>
<li>LLMs fine-tuned on tool-augmented math instructions (&lt;10B): MAmmoTH-7B-Mistral [48], MathCoder-7B-CL [80], ToRA-7B-Code [12], MARIO-OVM-7B [81], MMOS-CODE-7B [64], OpenMath-Mistral-7B [63], Rho-1-Math-7B-Code [78].
Our evaluation framework and in-context examples follow the existing work [6, 12, 82]. For general and math domain base models, we adopt the few-shot prompting method. For fine-tuned models, we adopt the zero-shot prompting method for open-ended natural language reasoning and tool manipulation tasks, and the few-shot prompting method for multiple choice problems. We cite the performance results reported in existing work $[6,12,48,63,64,75,78,80,81]$.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Hyper-parameter tuning results of the pre-training data proportion and high-value data amount, under the tool manipulation and natural language reasoning settings, respectively.</p>
<h1>G Implementation Details</h1>
<p>Data Synthesis Models. We train two data synthesis models for the natural language reasoning and tool manipulation settings, respectively. We first initialize them by training DeepSeekMath-7B-RL with 4 k and 1.3 k KD datasets, respectively. Then, we utilize the synthesis models to generate 100k problem-solution pairs for each one. During value estimation, we adopt the training set of GSM8k and MATH for natural language reasoning, and the 5 k subset from a mixture of OpenMathInstruct and MMOS for tool manipulation, as the instances from downstream math-related tasks. We select 2 k the most valuable math texts in each setting for GPT-4 to boost the quality, and then add them into the KD dataset. During training, following existing work [14], we adopt a cosine learning rate schedule with a $0 \%$ warm-up ratio and select a learning rate of 1e-5 for 5 epochs and 10 epochs for natural language reasoning and tool manipulation, respectively.</p>
<p>JiuZhang3.0. Before training, we first filter out the instance from the synthetic dataset that have 10-grams overlap to the test set data, and also remove deduplicate data and the synthetic tool manipulation data containing unexecutable code. Then, we follow existing work that trains 7B, 8B, and $8 \times 7$ B versions based on Mistral-7B, LLaMA-3-8B, and Mixtral-8×7B [13]. During training, we first pre-train it on our synthetic 4.6B math problem-solution pairs and then fine-tune it on the multi-source instruction set. We reuse the optimizer to initialize the fine-tuning stage and adopt the Warmup-Stable-Decay learning rate scheduler [83] with 3\% warm-up ratio and 85\% stable training ratio for 1 epoch in the whole training process. We set the maximum learning rate to 1e-5 and the minimum learning rate to 1e-6 with a total batch size of 512 . To boost the training efficiency, we pack multiple instances in the same context window of the model and modify the attention to avoid mutual interference among difference instances. The maximum length of model is set to 2048. We train all models with BFloat16 numerical format, Flash Attention 2.0, DeepSpeed Stage 2 for 7B and 8B models, and Stage 3 for $8 \times 7$ B models.</p>
<h2>H Hyper-parameter Tuning</h2>
<p>In this part, we conduct the experiments about tuning two important hyper-parameters, i.e., pretraining data proportion, and the number of high-value data.</p>
<p>Pre-train Data Proportion. In the synthetic pre-training data proportion, we should determine the ratio between the data from the natural language reasoning and tool manipulation settings (NLC : TM). We set it to $1: 1,2: 1,4: 1$, and $8: 1$, and control the total data amount unchanged (200k) for comparison. We follow the efficient test setting in Section 3.3, and report the MATH and GSM8k results under the tool manipulation setting, as there is only slight performance changes on natural language reasoning setting. As shown in Figure 4, 2:1 is more suited and leads to better performance than other proportions. The reason may be that smaller or larger proportion for tool manipulation data would cause underfitting or overfitting on the tool manipulation data, affecting the corresponding capability.</p>
<p>Number of Selected High-Value Data. We also study the effect of changing the number of selected high-value data according to the ranking, as more data requires larger cost for invoking the GPT-4 API. We set it to $2 \mathrm{k}, 4 \mathrm{k}, 6 \mathrm{k}$, and 8 k , and do not change other settings for fair comparison. We follow the efficient test setting in Section 3.3. As shown in Figure 4, using the top 2 k high-value data achieves the best performance, and more data even causes the performance degradation. It indicates that the data quality is more important than data amount for boosting the data synthesis capability, as using more data would inevitably involve low-quality data.</p>
<h1>I Case Study</h1>
<h2>Good Case 1</h2>
<p>Type: Natural Language Reasoning - Grade School
Math-Related Texts:
Question
The Huka Falls on the Waikato River is one of New Zealand's most visited natural tourist attractions (see Figure 12.30). On average the river has a flow rate of about 300,000 Ls. At the gorge, the river narrows to 20 m wide and averages 20 m deep. (a) What is the average speed of the river in the gorge? (b) What is the average speed of the water in the river downstream of the falls when it widens to 60 m and its depth increases to an average of 40 m ?
Question Image</p>
<ol>
<li>0.75 ms</li>
<li>0.13 ms</li>
</ol>
<p>Solution Video# OpenStax College Physics Solution, Chapter 12, Problem 5 (Problems \&amp; Exercises) (1:14)
View sample solution## Calculator Screenshots
Video Transcript
This is College Physics Answers with Shaun Dychko. At the gorge where the Waikato River of New Zealand narrows to a width of 20 meters and a height of 20 meters, we2019te going to figure out what its speed is. And so the volume flow rate is equal to the cross sectional area of the river multiply by its
Synthesis QA:
Question: The Waikato River flows through a gorge where it narrows down to a width of 20 meters and a depth of 20 meters. The average flow rate of the river is 300,000 liters per second.
<strong>Question:</strong> What is the average speed of the river in this gorge?
Answer:
To find the average speed of the river in the gorge, we need to use the formula for the volume flow rate:
Volume Flow Rate $=$ Cross-Sectional Area $\times$ Speed</p>
<p>First, let2019s calculate the cross-sectional area of the river:
Cross-Sectional Area $=$ Width $\times$ Depth
Given:</p>
<ul>
<li>Width $=20$ meters</li>
<li>Depth $=20$ meters</li>
</ul>
<p>So,</p>
<p>$$
\text { Cross-Sectional Area }=20 \text { meters } \times 20 \text { meters }=400 \text { square meters }
$$</p>
<p>Now, we know the volume flow rate and the cross-sectional area. The volume flow rate $(Q)$ is 300,000 liters per second, which we need to convert into cubic meters because the area is in square meters:</p>
<p>$$
1 \text { liter }=1 \text { cubic decimeter }=0.001 \text { cubic meters }
$$</p>
<p>So,</p>
<p>$$
Q=300,000 \text { liters per second }=300,000 \times 0.001 \text { cubic meters per second }=300 \text { cubic meters per second }
$$</p>
<p>We can now use the formula for the volume flow rate to find the speed $(v)$ :</p>
<p>$$
Q=\text { Cross-Sectional Area } \times v
$$</p>
<p>Rearranging to solve for $v$ :</p>
<p>$$
v=\frac{Q}{\text { Cross-Sectional Area }}
$$</p>
<p>Substituting the values we have:</p>
<p>$$
v=\frac{300 \text { cubic meters per second }}{400 \text { square meters }}=0.75 \text { meters per second }
$$</p>
<p>So, the average speed of the river in the gorge is ${ }^{<em> </em>} 0.75$ meters per second**</p>
<h1>Good Case 2</h1>
<p>Type: Natural Language Reasoning - Middle
Math-Related Texts:
Successive parabolic interpolation
Successive parabolic interpolation is a technique for finding the extremum (minimum or maximum) of a continuous unimodal function by successively fitting parabolas (polynomials of degree two) to a function of one variable at three unique points or, in general, a function of n variables at $1+\mathrm{n}(\mathrm{n}+3) 2$ points, and at each iteration replacing the tildestpoint with the extremum of the fitted parabola. Advantages
Only function values are used, and when this method converges to an extremum, it does so with an order of convergence of approximately 1.325. The superlinear rate of convergence is superior to that of other methods with only linear convergence (such as line search). Moreover, not requiring the computation or approximation of function derivatives makes successive parabolic interpolation a popular alternative to other methods that do require them (such as gradient descent and Newton's method).
Disadvantages
On the other hand, convergence (even
Synthesis QA:
Question: Alex and Jamie are working on a science project and need to find the maximum height of a rocket launched upward. They record the height of the rocket at three different times:</p>
<ul>
<li>At $t=0$ seconds, the height is 0 meters.</li>
<li>At $t=5$ seconds, the height is 25 meters.</li>
<li>At $t=10$ seconds, the height is 40 meters.</li>
</ul>
<p>To find the maximum height, Alex and Jamie decide to use successive parabolic interpolation. They start by fitting a parabola to these three points and then finding the maximum point of this parabola.</p>
<ol>
<li>Write the general form of a quadratic (parabolic) equation: $y=a x^{2}+b x+c$. 2. Use the three points to create a system of equations to find the coefficients $a, b$, and $c$. 3. Find the vertex of the parabola to determine the maximum height.</li>
</ol>
<p>Answer:</p>
<ol>
<li>${ }^{<em> </em>}$ General Form of a Quadratic Equation:**</li>
</ol>
<p>The general form of a quadratic equation is:</p>
<p>$$
y=a x^{2}+b x+c
$$</p>
<ol>
<li>${ }^{<em> </em>}$ Creating a System of Equations: ${ }^{<em> </em>}$</li>
</ol>
<p>We have three points:</p>
<p>$$
(0,0), \quad(5,25), \quad(10,40)
$$</p>
<p>Plugging these points into the equation $y=a x^{2}+b x+c$ :</p>
<ul>
<li>For $(0,0)$ :</li>
</ul>
<p>$$
0=a(0)^{2}+b(0)+c \Longrightarrow c=0
$$</p>
<ul>
<li>For $(5,25)$ :</li>
</ul>
<p>$$
25=a(5)^{2}+b(5)+0 \Longrightarrow 25=25 a+5 b \quad \text { (Equation } 1)
$$</p>
<ul>
<li>For $(10,40)$ :</li>
</ul>
<p>$$
40=a(10)^{2}+b(10)+0 \Longrightarrow 40=100 a+10 b \quad \text { (Equation } 2)
$$</p>
<ol>
<li>${ }^{<em> </em>}$ Solving the System of Equations: ${ }^{<em> </em>}$</li>
</ol>
<p>Use Equation 1:</p>
<p>$$
25=25 a+5 b \quad \text { (Equation } 1)
$$</p>
<p>Simplify:</p>
<p>$$
5=5 a+b \quad \text { (Equation } 3)
$$</p>
<p>Use Equation 2:</p>
<p>$$
40=100 a+10 b \quad \text { (Equation } 2)
$$</p>
<p>Simplify:</p>
<p>$$
4=10 a+b \quad \text { (Equation } 4)
$$</p>
<p>Subtract Equation 4 from Equation 3:</p>
<p>$$
(5 a+b)-(10 a+b)=5-4 \Longrightarrow-5 a=1 \Longrightarrow a=-\frac{1}{5}
$$</p>
<p>Plug $a=-\frac{1}{5}$ back into Equation 3:</p>
<p>$$
5=5\left(-\frac{1}{5}\right)+b \Longrightarrow 5=-1+b \Longrightarrow b=6
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://github.com/math-eval/TAL-SCQ5K&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
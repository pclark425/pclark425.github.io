<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8439 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8439</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8439</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-81b10e64133e775dab53153cc82277d276efe1f7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/81b10e64133e775dab53153cc82277d276efe1f7" target="_blank">Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that the language agents improve over time and that the approach considerably outperforms baselines that do not properly leverage gradients from the environment, and demonstrates that using policy gradient optimization to improve language agents seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.</p>
                <p><strong>Paper Abstract:</strong> Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment. This demonstrates that using policy gradient optimization to improve language agents, for which we believe our work is one of the first, seems promising and can be applied to optimize other models in the agent architecture to enhance agent performances over time.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8439.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8439.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-LLM agent architecture where a frozen cloud-hosted actor LLM acts in environments while a smaller, trainable retrospective LLM learns via policy-gradient (PPO) to generate self-reflections that are appended to the actor prompt; uses short-term trajectory context and long-term self-reflections stored/managed in a replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor LLM (frozen, e.g., GPT-3/GPT-4) executes ReAct-style Thought/Action/Observation; Retrospective LLM (local, e.g., longchat-7b-16k with LoRA) produces diagnostic reflections appended to actor prompt and is fine-tuned using PPO on ratings derived from delta episodic returns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (fine-tuned, LoRA adapters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Actor: large cloud-hosted autoregressive LLMs (text-davinci-003 / GPT-4) used frozen with T=0 for deterministic behavior; Retrospective: smaller local Llama-based longchat-7b-16k, LoRA-finetuned (r=1 or r=4) and optionally 4-bit quantized, fine-tuned via supervised + RLHF (PPO).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Search-based multi-hop question answering over Wikipedia via actions SEARCH[ENTITY], LOOKUP[KEYWORD], FINISH[ANSWER].</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / search-based multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term (trajectory) and long-term (self-reflections appended to prompt); replay buffer as offline memory for training</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Concatenation of recent trajectory (Thought/Action/Observation) as short-term memory into the actor prompt; the retrospective model generates concise self-reflections which are appended to the actor prompt as long-term memory; additionally a replay buffer stores (instruction prompt, reflection, episode return) triplets for offline RLHF/PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Previous states/observations, actions, per-step rewards, entire episode trajectories, and generated reflection responses with episode returns (G).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation (recency-based context) at inference; sampling from replay buffer for offline PPO training and reward-model learning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Retroformer (LoRA r=1): N=1: 45% (GPT-3), 48% (GPT-4); N=4: 53% (GPT-3), 53% (GPT-4). Retroformer (LoRA r=4): N=1: 48%/51%; N=4: 54%/54%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (no reflective memory): N=1: 34% (GPT-3), 40% (GPT-4); (baseline values from Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against ReAct (no learning from environment) and Reflexion (verbal self-reflection without gradient-based fine-tuning). Ablations include LoRA rank (r=1 vs r=4) showing modest gains with larger r; trials/retries analysis shows improvements mostly concentrated in early retries. Offline data collection and RLHF (reward-model + PPO) pipeline ablated vs frozen retrospective LM behavior in illustrative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory implemented as concise, learned self-reflections appended to the actor prompt accelerates learning and reduces repetitive errors. Fine-tuning the retrospective model via policy-gradient on return differences yields faster and higher final success rates than verbal-reflection-only baselines, especially in HotPotQA and AlfWorld; improvements are largest in early retries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance gains are environment-dependent: smaller improvement on WebShop where exploration and precise search queries are needed. Limitations include prompt length constraints (concatenation of trajectories), possible uninformative reflections from unfine-tuned LMs leading to loops, and reliance on accurate reward shaping for RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8439.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same Retroformer system evaluated on embodied/text-action tasks (AlfWorld) demonstrating memory via self-reflections and a replay buffer with PPO fine-tuning of the retrospective model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same as above; retrospective LM diagnoses root causes of failed multi-step plans and produces concise new plans appended to the actor prompt to avoid repeated failure patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AlfWorld</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Embodied text environment requiring multi-step low-level actions (GOTO, TAKE, OPEN, CLOSE, TOGGLE, etc.) to manipulate objects and accomplish tasks (find/move/manipulate objects).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>embodied multi-step decision making / action planning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term trajectory + long-term appended reflections; replay buffer for cross-task learning</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Actor uses recent trajectory and appended reflections in prompt; retrospective model summarizes failed attempts and issues a new plan referencing specific environment actions to change future decisions; replay buffer retains reflection-response/return examples across tasks for offline RL.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Episode trajectories (actions/observations), binary success/failure rewards, reflection responses, and rated response outcomes stored in replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation for inference; sampling from replay buffer when fine-tuning retrospective model; best-of-n sampling during online execution guided by learned reward model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Retroformer (LoRA r=1): N=1: 93.28% (GPT-3), 95.62% (GPT-4); N=4: 100%/100%. Retroformer (LoRA r=4): N=1: 97.76%/97.76%; N=4: 100%/100%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (no retrospective memory): N=1: 62.69% (GPT-3), 77.61% (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Comparison shows large gains over ReAct and Reflexion; LoRA rank increases (r=1->r=4) modestly improve performance. The paper reports that improvements mainly occur in early retries (first few attempts).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Learned reflections acting as long-term memory substantially reduce repetitive failures in low-level action tasks; Retroformer solved AlfWorld tasks within 3 retries when baselines often failed, demonstrating the effectiveness of learned credit assignment and actionable reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires correct mapping from reflection to actor behavior (actor is frozen); constrained by prompt length as trajectories grow; needs representative offline rollouts for reward-model training; possible sensitivity to quality of reward shaping (binary success here).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8439.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retroformer applied to a web-browsing product-selection environment where self-reflection memory is appended to prompts and the retrospective model is fine-tuned with PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same two-LLM architecture; retrospective LM refines actor prompts using past trials and rewards to propose better search strategies and constraints for browsing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Web browsing and shopping tasks where the agent must search, click, and choose items matching user constraints (attributes, options, price) using SEARCH and CHOOSE actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>web navigation / information retrieval / constrained product selection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term trajectory + long-term appended reflections; replay buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Reflections appended to the actor prompt to guide search queries and action choices; replay buffer and learned reward model used to fine-tune retrospective LM.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Search queries, clicked items, final selection, product attributes/options, episode return (reward based on attribute/option match & price), and generated reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation; offline sampling from replay buffer for PPO; best-of-n sampling with learned reward during online runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Retroformer (LoRA r=1): N=1: 36% (GPT-3), 43% (GPT-4); N=4: 36%/45%. Retroformer (LoRA r=4): N=1: 34%/43%; N=4: 36%/46%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (no retrospective memory): N=1: 33% (GPT-3), 42% (GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to ReAct and Reflexion; improvements are modest (â‰ˆ4% reported improvement with 4 retries) and smaller than in HotPotQA/AlfWorld. LoRA rank and retries both reported; offline vs online training pipeline described.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memory via learned reflections provides modest gains in web browsing tasks but is less effective where exploration and precise query formulation dominate performance; reflection-based memory may not substitute for better search/exploration strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>WebShop requires substantial exploration and precise queries; verbal reflections alone may not provide sufficient exploration guidance; improvements limited and highlight environment dependence of reflection-memory approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8439.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent architecture that equips LLM agents with dynamic memory and self-reflection capabilities by converting environmental feedback into textual self-reflections appended to prompts to iteratively refine behavior (no gradient-based fine-tuning of the reflection generator).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates verbal self-reflections from prior trials and appends them to prompts to avoid repetitive mistakes; reflections are produced by a frozen LM (or used as a prompt component) without explicit policy-gradient optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Frozen LM used for generating reflections in baseline (as in Shinn et al., 2023); in paper comparisons actor LLMs were GPT-3/GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reflexion uses a language model to produce textual reflections; in this paper Reflexion was evaluated as a baseline with frozen reflection-generation (no PPO fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA / AlfWorld / WebShop (evaluated as baseline across these tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as Retroformer: multi-hop QA (HotPotQA), embodied action tasks (AlfWorld), web shopping (WebShop).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>various: question answering, embodied action planning, web navigation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term self-reflection appended to prompts (dynamic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Verbal self-reflections generated from prior trials are appended to the actor prompt; no gradient-based optimization of the reflection generator is performed in Reflexion baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Summaries of prior failed attempts and suggested improvements (textual).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompt concatenation (reflection text included in subsequent attempts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reflexion: HotPotQA N=1: 42% (GPT-3), 46% (GPT-4); N=4: 50%/52%. AlfWorld N=1: 76.87%/81.34%; N=4: 84.33%/85.07%. WebShop N=1: 35%/42%; N=4: 35%/44%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ReAct (no reflective memory): HotPotQA N=1: 34%/40%; AlfWorld 62.69%/77.61%; WebShop 33%/42%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to ReAct (no reflection) and Retroformer (learned reflection). Paper shows Reflexion improves over ReAct, but Retroformer (PPO-finetuned retrospective LM) typically outperforms Reflexion, especially early in retries.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verbal reflections (dynamic memory) help avoid repeated errors and improve performance relative to no-memory baselines, but without gradient-based optimization they can be uninformative; fine-tuning the reflection generator (as in Retroformer) yields further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>When the reflection generator is frozen the produced reflections can merely rephrase failed action sequences and lead to repetitive errors or loops; lacks credit-assignment optimization and cannot exploit gradient-based reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8439.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent prompting paradigm that interleaves chain-of-thought style reasoning (Thought) and environment actions (Action) within an LLM prompt, enabling the model to both reason and act; uses prompt-context (short-term memory) but does not learn from environment reward.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor prompt instructs LLM to interleave Thought/Action/Observation steps to perform reasoning and tool/API calls; operates statelessly via concatenated prompt context across steps (short-term memory).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Actor: GPT-3 / GPT-4 (frozen in these experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs operated with T=0 in experiments; ReAct uses prompt-engineered instruction pattern to drive interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotPotQA / AlfWorld / WebShop (baseline evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same three tasks for which ReAct was used as a baseline to show performance without reflective long-term memory learning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering, embodied action planning, web browsing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term prompt-based memory (trajectory/history included in prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Concatenation of previous steps (thoughts/actions/observations) into the actor prompt; no separate learned long-term memory or reflection model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recent trajectory history (short-term), thoughts, actions, observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Recency-based prompt concatenation (no replay buffer or separate retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ReAct (short-term prompt memory only): HotPotQA N=1: 34% (GPT-3), 40% (GPT-4); AlfWorld 62.69%/77.61%; WebShop 33%/42%. (These are the baseline numbers in Table 2.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Serves as a baseline (no long-term learned reflections). Compared directly to Reflexion and Retroformer to demonstrate benefits of adding long-term reflection memory and learning the reflection generator.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Short-term trajectory memory (prompt concatenation) enables reasonable single-trial behavior, but lacks mechanisms to prevent repeated errors across retries; adding long-term reflection and learning to generate it improves multi-attempt learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt growth and length constraints limit long horizon memory; stateless LLM nature means no native persistent memory beyond text in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8439.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique where a single LLM acts as generator, refiner, and feedback provider to iteratively refine outputs via self-feedback (verbal), but not explicitly optimized for interactive environment rewards or multi-episode credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses the same model to produce an initial output, critique it, and iteratively refine the output based on generated feedback; designed for output refinement rather than environment-interaction learning from rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterative refinement / self-feedback (general generation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work as a verbal self-refinement approach; contrasted with Retroformer which uses gradient-based fine-tuning of the retrospective LM for credit assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-reflection provides iterative refinement but is not tailored to reward-driven, environment-interactive tasks and lacks specialized fine-tuning for credit assignment in environments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not designed for environment reward signals or multi-episode learning; may not solve credit assignment across complex multi-step environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8439.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8439.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAP (Reasoning as Planning) / 'Reasoning with language model is planning with world model'</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that repurposes LLMs as both world models and reasoning agents, integrating planning (e.g., Monte Carlo Tree Search) with LLM-based simulations to navigate large reasoning/search spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reasoning with language model is planning with world model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LLM as a world model and employs planning/search strategies to explore action/reasoning space, and includes memory/rollout traces as part of its mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>planning / reasoning with world model</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory of rollout traces / world model internal state (as discussed in related work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>LLM-based world model with planning (e.g., MCTS) over imagined trajectories; memory in the form of simulated rollout histories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Simulated trajectories / imagined actions and outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Search/planning over simulated rollouts (MCTS-style); not explicitly used in this paper's experiments but cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Cited as related work that uses internal rollout memory and planning; contrasted with Retroformer which learns explicit textual reflections with PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Planning with an LLM world model leverages internal simulated trajectories to improve decision making; included in related work as an alternative approach to incorporate memory/rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not evaluated experimentally in this paper; varies by approach and environment complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
                <li>Reasoning with language model is planning with world model <em>(Rating: 1)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8439",
    "paper_id": "paper-81b10e64133e775dab53153cc82277d276efe1f7",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "Retroformer",
            "name_full": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
            "brief_description": "A two-LLM agent architecture where a frozen cloud-hosted actor LLM acts in environments while a smaller, trainable retrospective LLM learns via policy-gradient (PPO) to generate self-reflections that are appended to the actor prompt; uses short-term trajectory context and long-term self-reflections stored/managed in a replay buffer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Retroformer",
            "agent_description": "Actor LLM (frozen, e.g., GPT-3/GPT-4) executes ReAct-style Thought/Action/Observation; Retrospective LLM (local, e.g., longchat-7b-16k with LoRA) produces diagnostic reflections appended to actor prompt and is fine-tuned using PPO on ratings derived from delta episodic returns.",
            "model_name": "Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (fine-tuned, LoRA adapters)",
            "model_description": "Actor: large cloud-hosted autoregressive LLMs (text-davinci-003 / GPT-4) used frozen with T=0 for deterministic behavior; Retrospective: smaller local Llama-based longchat-7b-16k, LoRA-finetuned (r=1 or r=4) and optionally 4-bit quantized, fine-tuned via supervised + RLHF (PPO).",
            "task_name": "HotPotQA",
            "task_description": "Search-based multi-hop question answering over Wikipedia via actions SEARCH[ENTITY], LOOKUP[KEYWORD], FINISH[ANSWER].",
            "task_type": "question answering / search-based multi-hop reasoning",
            "memory_used": true,
            "memory_type": "short-term (trajectory) and long-term (self-reflections appended to prompt); replay buffer as offline memory for training",
            "memory_mechanism": "Concatenation of recent trajectory (Thought/Action/Observation) as short-term memory into the actor prompt; the retrospective model generates concise self-reflections which are appended to the actor prompt as long-term memory; additionally a replay buffer stores (instruction prompt, reflection, episode return) triplets for offline RLHF/PPO.",
            "memory_representation": "Previous states/observations, actions, per-step rewards, entire episode trajectories, and generated reflection responses with episode returns (G).",
            "memory_retrieval_method": "Prompt concatenation (recency-based context) at inference; sampling from replay buffer for offline PPO training and reward-model learning.",
            "performance_with_memory": "Retroformer (LoRA r=1): N=1: 45% (GPT-3), 48% (GPT-4); N=4: 53% (GPT-3), 53% (GPT-4). Retroformer (LoRA r=4): N=1: 48%/51%; N=4: 54%/54%.",
            "performance_without_memory": "ReAct (no reflective memory): N=1: 34% (GPT-3), 40% (GPT-4); (baseline values from Table 2).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared against ReAct (no learning from environment) and Reflexion (verbal self-reflection without gradient-based fine-tuning). Ablations include LoRA rank (r=1 vs r=4) showing modest gains with larger r; trials/retries analysis shows improvements mostly concentrated in early retries. Offline data collection and RLHF (reward-model + PPO) pipeline ablated vs frozen retrospective LM behavior in illustrative examples.",
            "key_findings": "Memory implemented as concise, learned self-reflections appended to the actor prompt accelerates learning and reduces repetitive errors. Fine-tuning the retrospective model via policy-gradient on return differences yields faster and higher final success rates than verbal-reflection-only baselines, especially in HotPotQA and AlfWorld; improvements are largest in early retries.",
            "limitations_or_challenges": "Performance gains are environment-dependent: smaller improvement on WebShop where exploration and precise search queries are needed. Limitations include prompt length constraints (concatenation of trajectories), possible uninformative reflections from unfine-tuned LMs leading to loops, and reliance on accurate reward shaping for RL training.",
            "uuid": "e8439.0",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Retroformer",
            "name_full": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
            "brief_description": "Same Retroformer system evaluated on embodied/text-action tasks (AlfWorld) demonstrating memory via self-reflections and a replay buffer with PPO fine-tuning of the retrospective model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Retroformer",
            "agent_description": "Same as above; retrospective LM diagnoses root causes of failed multi-step plans and produces concise new plans appended to the actor prompt to avoid repeated failure patterns.",
            "model_name": "Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (LoRA fine-tuned)",
            "model_description": "As above.",
            "task_name": "AlfWorld",
            "task_description": "Embodied text environment requiring multi-step low-level actions (GOTO, TAKE, OPEN, CLOSE, TOGGLE, etc.) to manipulate objects and accomplish tasks (find/move/manipulate objects).",
            "task_type": "embodied multi-step decision making / action planning",
            "memory_used": true,
            "memory_type": "short-term trajectory + long-term appended reflections; replay buffer for cross-task learning",
            "memory_mechanism": "Actor uses recent trajectory and appended reflections in prompt; retrospective model summarizes failed attempts and issues a new plan referencing specific environment actions to change future decisions; replay buffer retains reflection-response/return examples across tasks for offline RL.",
            "memory_representation": "Episode trajectories (actions/observations), binary success/failure rewards, reflection responses, and rated response outcomes stored in replay buffer.",
            "memory_retrieval_method": "Prompt concatenation for inference; sampling from replay buffer when fine-tuning retrospective model; best-of-n sampling during online execution guided by learned reward model.",
            "performance_with_memory": "Retroformer (LoRA r=1): N=1: 93.28% (GPT-3), 95.62% (GPT-4); N=4: 100%/100%. Retroformer (LoRA r=4): N=1: 97.76%/97.76%; N=4: 100%/100%.",
            "performance_without_memory": "ReAct (no retrospective memory): N=1: 62.69% (GPT-3), 77.61% (GPT-4).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Comparison shows large gains over ReAct and Reflexion; LoRA rank increases (r=1-&gt;r=4) modestly improve performance. The paper reports that improvements mainly occur in early retries (first few attempts).",
            "key_findings": "Learned reflections acting as long-term memory substantially reduce repetitive failures in low-level action tasks; Retroformer solved AlfWorld tasks within 3 retries when baselines often failed, demonstrating the effectiveness of learned credit assignment and actionable reflection.",
            "limitations_or_challenges": "Requires correct mapping from reflection to actor behavior (actor is frozen); constrained by prompt length as trajectories grow; needs representative offline rollouts for reward-model training; possible sensitivity to quality of reward shaping (binary success here).",
            "uuid": "e8439.1",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Retroformer",
            "name_full": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
            "brief_description": "Retroformer applied to a web-browsing product-selection environment where self-reflection memory is appended to prompts and the retrospective model is fine-tuned with PPO.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Retroformer",
            "agent_description": "Same two-LLM architecture; retrospective LM refines actor prompts using past trials and rewards to propose better search strategies and constraints for browsing tasks.",
            "model_name": "Actor: GPT-3 / GPT-4 (frozen); Retrospective: longchat-7b-16k (LoRA fine-tuned)",
            "model_description": "As above.",
            "task_name": "WebShop",
            "task_description": "Web browsing and shopping tasks where the agent must search, click, and choose items matching user constraints (attributes, options, price) using SEARCH and CHOOSE actions.",
            "task_type": "web navigation / information retrieval / constrained product selection",
            "memory_used": true,
            "memory_type": "short-term trajectory + long-term appended reflections; replay buffer",
            "memory_mechanism": "Reflections appended to the actor prompt to guide search queries and action choices; replay buffer and learned reward model used to fine-tune retrospective LM.",
            "memory_representation": "Search queries, clicked items, final selection, product attributes/options, episode return (reward based on attribute/option match & price), and generated reflections.",
            "memory_retrieval_method": "Prompt concatenation; offline sampling from replay buffer for PPO; best-of-n sampling with learned reward during online runs.",
            "performance_with_memory": "Retroformer (LoRA r=1): N=1: 36% (GPT-3), 43% (GPT-4); N=4: 36%/45%. Retroformer (LoRA r=4): N=1: 34%/43%; N=4: 36%/46%.",
            "performance_without_memory": "ReAct (no retrospective memory): N=1: 33% (GPT-3), 42% (GPT-4).",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to ReAct and Reflexion; improvements are modest (â‰ˆ4% reported improvement with 4 retries) and smaller than in HotPotQA/AlfWorld. LoRA rank and retries both reported; offline vs online training pipeline described.",
            "key_findings": "Memory via learned reflections provides modest gains in web browsing tasks but is less effective where exploration and precise query formulation dominate performance; reflection-based memory may not substitute for better search/exploration strategies.",
            "limitations_or_challenges": "WebShop requires substantial exploration and precise queries; verbal reflections alone may not provide sufficient exploration guidance; improvements limited and highlight environment dependence of reflection-memory approach.",
            "uuid": "e8439.2",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A language-agent architecture that equips LLM agents with dynamic memory and self-reflection capabilities by converting environmental feedback into textual self-reflections appended to prompts to iteratively refine behavior (no gradient-based fine-tuning of the reflection generator).",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "Generates verbal self-reflections from prior trials and appends them to prompts to avoid repetitive mistakes; reflections are produced by a frozen LM (or used as a prompt component) without explicit policy-gradient optimization.",
            "model_name": "Frozen LM used for generating reflections in baseline (as in Shinn et al., 2023); in paper comparisons actor LLMs were GPT-3/GPT-4.",
            "model_description": "Reflexion uses a language model to produce textual reflections; in this paper Reflexion was evaluated as a baseline with frozen reflection-generation (no PPO fine-tuning).",
            "task_name": "HotPotQA / AlfWorld / WebShop (evaluated as baseline across these tasks)",
            "task_description": "Same tasks as Retroformer: multi-hop QA (HotPotQA), embodied action tasks (AlfWorld), web shopping (WebShop).",
            "task_type": "various: question answering, embodied action planning, web navigation",
            "memory_used": true,
            "memory_type": "long-term self-reflection appended to prompts (dynamic memory)",
            "memory_mechanism": "Verbal self-reflections generated from prior trials are appended to the actor prompt; no gradient-based optimization of the reflection generator is performed in Reflexion baseline.",
            "memory_representation": "Summaries of prior failed attempts and suggested improvements (textual).",
            "memory_retrieval_method": "Prompt concatenation (reflection text included in subsequent attempts).",
            "performance_with_memory": "Reflexion: HotPotQA N=1: 42% (GPT-3), 46% (GPT-4); N=4: 50%/52%. AlfWorld N=1: 76.87%/81.34%; N=4: 84.33%/85.07%. WebShop N=1: 35%/42%; N=4: 35%/44%.",
            "performance_without_memory": "ReAct (no reflective memory): HotPotQA N=1: 34%/40%; AlfWorld 62.69%/77.61%; WebShop 33%/42%.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Compared to ReAct (no reflection) and Retroformer (learned reflection). Paper shows Reflexion improves over ReAct, but Retroformer (PPO-finetuned retrospective LM) typically outperforms Reflexion, especially early in retries.",
            "key_findings": "Verbal reflections (dynamic memory) help avoid repeated errors and improve performance relative to no-memory baselines, but without gradient-based optimization they can be uninformative; fine-tuning the reflection generator (as in Retroformer) yields further gains.",
            "limitations_or_challenges": "When the reflection generator is frozen the produced reflections can merely rephrase failed action sequences and lead to repetitive errors or loops; lacks credit-assignment optimization and cannot exploit gradient-based reward signals.",
            "uuid": "e8439.3",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "An agent prompting paradigm that interleaves chain-of-thought style reasoning (Thought) and environment actions (Action) within an LLM prompt, enabling the model to both reason and act; uses prompt-context (short-term memory) but does not learn from environment reward.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "Actor prompt instructs LLM to interleave Thought/Action/Observation steps to perform reasoning and tool/API calls; operates statelessly via concatenated prompt context across steps (short-term memory).",
            "model_name": "Actor: GPT-3 / GPT-4 (frozen in these experiments)",
            "model_description": "Large autoregressive LLMs operated with T=0 in experiments; ReAct uses prompt-engineered instruction pattern to drive interaction.",
            "task_name": "HotPotQA / AlfWorld / WebShop (baseline evaluations)",
            "task_description": "Same three tasks for which ReAct was used as a baseline to show performance without reflective long-term memory learning.",
            "task_type": "question answering, embodied action planning, web browsing",
            "memory_used": true,
            "memory_type": "short-term prompt-based memory (trajectory/history included in prompt)",
            "memory_mechanism": "Concatenation of previous steps (thoughts/actions/observations) into the actor prompt; no separate learned long-term memory or reflection model.",
            "memory_representation": "Recent trajectory history (short-term), thoughts, actions, observations.",
            "memory_retrieval_method": "Recency-based prompt concatenation (no replay buffer or separate retrieval).",
            "performance_with_memory": "ReAct (short-term prompt memory only): HotPotQA N=1: 34% (GPT-3), 40% (GPT-4); AlfWorld 62.69%/77.61%; WebShop 33%/42%. (These are the baseline numbers in Table 2.)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Serves as a baseline (no long-term learned reflections). Compared directly to Reflexion and Retroformer to demonstrate benefits of adding long-term reflection memory and learning the reflection generator.",
            "key_findings": "Short-term trajectory memory (prompt concatenation) enables reasonable single-trial behavior, but lacks mechanisms to prevent repeated errors across retries; adding long-term reflection and learning to generate it improves multi-attempt learning.",
            "limitations_or_challenges": "Prompt growth and length constraints limit long horizon memory; stateless LLM nature means no native persistent memory beyond text in prompts.",
            "uuid": "e8439.4",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "A technique where a single LLM acts as generator, refiner, and feedback provider to iteratively refine outputs via self-feedback (verbal), but not explicitly optimized for interactive environment rewards or multi-episode credit assignment.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "mention",
            "agent_name": "Self-refine",
            "agent_description": "Uses the same model to produce an initial output, critique it, and iteratively refine the output based on generated feedback; designed for output refinement rather than environment-interaction learning from rewards.",
            "model_name": "",
            "model_description": "",
            "task_name": "",
            "task_description": "",
            "task_type": "iterative refinement / self-feedback (general generation tasks)",
            "memory_used": false,
            "memory_type": "",
            "memory_mechanism": "",
            "memory_representation": "",
            "memory_retrieval_method": "",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work as a verbal self-refinement approach; contrasted with Retroformer which uses gradient-based fine-tuning of the retrospective LM for credit assignment.",
            "key_findings": "Self-reflection provides iterative refinement but is not tailored to reward-driven, environment-interactive tasks and lacks specialized fine-tuning for credit assignment in environments.",
            "limitations_or_challenges": "Not designed for environment reward signals or multi-episode learning; may not solve credit assignment across complex multi-step environment interactions.",
            "uuid": "e8439.5",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "RAP",
            "name_full": "RAP (Reasoning as Planning) / 'Reasoning with language model is planning with world model'",
            "brief_description": "An approach that repurposes LLMs as both world models and reasoning agents, integrating planning (e.g., Monte Carlo Tree Search) with LLM-based simulations to navigate large reasoning/search spaces.",
            "citation_title": "Reasoning with language model is planning with world model",
            "mention_or_use": "mention",
            "agent_name": "RAP",
            "agent_description": "Uses an LLM as a world model and employs planning/search strategies to explore action/reasoning space, and includes memory/rollout traces as part of its mechanism.",
            "model_name": "",
            "model_description": "",
            "task_name": "",
            "task_description": "",
            "task_type": "planning / reasoning with world model",
            "memory_used": true,
            "memory_type": "memory of rollout traces / world model internal state (as discussed in related work)",
            "memory_mechanism": "LLM-based world model with planning (e.g., MCTS) over imagined trajectories; memory in the form of simulated rollout histories.",
            "memory_representation": "Simulated trajectories / imagined actions and outcomes.",
            "memory_retrieval_method": "Search/planning over simulated rollouts (MCTS-style); not explicitly used in this paper's experiments but cited as related work.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Cited as related work that uses internal rollout memory and planning; contrasted with Retroformer which learns explicit textual reflections with PPO.",
            "key_findings": "Planning with an LLM world model leverages internal simulated trajectories to improve decision making; included in related work as an alternative approach to incorporate memory/rollouts.",
            "limitations_or_challenges": "Not evaluated experimentally in this paper; varies by approach and environment complexity.",
            "uuid": "e8439.6",
            "source_info": {
                "paper_title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1
        },
        {
            "paper_title": "Reasoning with language model is planning with world model",
            "rating": 1
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 1
        }
    ],
    "cost": 0.020285249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>RETROFORMER: RETROSPECTIVE LARGE LANGUAGE AGENTS WITH POLICY GRADIENT OPTIMIZATION</h1>
<p>Weiran Yao ${ }^{\dagger}$, Shelby Heinecke ${ }^{\dagger}$, Juan Carlos Niebles ${ }^{\dagger}$, Zhiwei Liu ${ }^{\dagger}$, Yihao Feng ${ }^{\dagger}$, Le Xue ${ }^{\dagger}$, Rithesh Murthy ${ }^{\dagger}$, Zeyuan Chen ${ }^{\dagger}$, Jianguo Zhang ${ }^{\dagger}$, Devansh Arpit ${ }^{\dagger}$, Ran Xu ${ }^{\dagger}$, Phil Mui ${ }^{\dagger}$, Huan Wang ${ }^{\dagger, <em>}$, Caiming Xiong ${ }^{\dagger, </em>}$, Silvio Savarese ${ }^{\dagger, *}$<br>${ }^{\dagger}$ Salesforce AI Research</p>
<h4>Abstract</h4>
<p>Recent months have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language agents capable of performing objective oriented multi-step tasks on their own, rather than merely responding to queries from human users. Most existing language agents, however, are not optimized using environment-specific rewards. Although some agents enable iterative refinement through verbal feedback, they do not reason and plan in ways that are compatible with gradient-based learning from rewards. This paper introduces a principled framework for reinforcing large language agents by learning a retrospective model, which automatically tunes the language agent prompts from environment feedback through policy gradient. Specifically, our proposed agent architecture learns from rewards across multiple environments and tasks, for fine-tuning a pre-trained language model which refines the language agent prompt by summarizing the root cause of prior failed attempts and proposing action plans. Experimental results on various tasks demonstrate that the language agents improve over time and that our approach considerably outperforms baselines that do not properly leverage gradients from the environment.</p>
<h2>1 INTRODUCTION</h2>
<p>Recently, we have seen the emergence of a powerful new trend in which large language models (LLMs) are augmented to become autonomous language action agents capable of performing tasks on their own, ultimately in the service of a goal, rather than responding to queries from human users. Prominent studies, including ReAct (Yao et al., 2023), Toolformer (Schick et al., 2023), HuggingGPT (Shen et al., 2023), Generative Agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023), have successfully showcased the viability of creating autonomous decision-making agents by leveraging the capabilities of LLMs. These approaches use LLMs to generate text-based outputs and actions that can be further employed for making API calls and executing operations within a given environment.</p>
<p>Given the immense scale of LLMs with an extensive parameter count, the behaviors of most existing language agents, however, are not optimized or aligned with environment reward functions. An exception is a very recent language agent architecture, namely Reflexion (Shinn et al., 2023), and several other related work, e.g., Self-Refine (Madaan et al., 2023b) and Generative Agents (Park et al., 2023), which use verbal feedback, namely self-reflection, to help agents learn from prior failure. These reflective agents convert binary or scalar reward from the environment into verbal feedback in the form of a textual summary, which is then added as additional context to the prompt for the language agent. The self-reflection feedback acts as a semantic signal by providing the agent with a concrete direction to improve upon, helping it learn from prior mistakes and prevent repetitive errors to perform better in the next attempt.</p>
<p>Although the self-reflection operation enables iterative refinement, generating useful reflective feedback from a pre-trained, frozen LLM is challenging, as showcased in Fig. 1, since it requires the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>LLM to have a good understanding of where the agent made mistakes in a specific environment, i.e., the credit assignment problem (Sutton \&amp; Barto, 2018), as well as the ability to generate a summary containing actionable insights for improvement. The verbal reinforcement cannot be optimal, if the frozen language model has not been properly fine-tuned to specialize in credit assignment problems for the tasks in given environments. Furthermore, the existing language agents do not reason and plan in ways that are compatible with differentiable, gradient-based learning from rewards by exploiting the existing abundant reinforcement learning techniques. To address these limitations, this paper introduces Retroformer, a principled framework for reinforcing language agents by learning a plug-in retrospective model, which automatically refines the language agent prompts from environment feedback through policy optimization. Specifically, our proposed agent architecture can learn from arbitrary reward information across multiple environments and tasks, for iteratively fine-tuning a pre-trained language model, which refines the language agent prompts by reflecting on failed attempts and assigning credits of actions taken by the agent on future rewards.</p>
<h1>1. Task instruction</h1>
<p>Lollipop Chainsaw featured Juliet Starling, who was voiced by a Canadian-American actress who has done voice roles for what Teen Titans spinoff series?</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of uninformative self-reflections from a frozen LLM. The root cause of failure in prior trial is that the agent should have only submitted the spinoff series "Teen Titans Go" and not "Teen Titans" in the answer. The agent forgot its goal during a chain of lengthy interactions. The verbal feedback from a frozen LLM, however, only rephrases the prior failed actions sequences as the proposed plan, resulting repetitive, incorrect actions in the next trial.</p>
<p>We conduct experiments on a number of real-world tasks including HotPotQA (Yang et al., 2018), which involves search-based question answering tasks, AlfWorld (Shridhar et al., 2021), in which the agent solves embodied robotics tasks through low-level text actions, and WebShop (Yao et al., 2022), a browser environment for web shopping. We observe Retroformer agents are faster learners compared with Reflexion, which does not use gradient for reasoning and planning, and are better decision-makers and reasoners. More concretely, Retroformer agents improve the success rate in HotPotQA by $18 \%$ with 4 retries, $36 \%$ in AlfWorld with 3 retries and $4 \%$ in WebShop, which demonstrate the effectiveness of gradient-based learning for LLM action agents.</p>
<p>To summarize, our contributions are the following:</p>
<ul>
<li>The paper introduces Retroformer, which iteratively refines the prompts given to large language agents based on environmental feedback to improve learning speed and task completion. We take a policy gradient approach with the Actor LLM being part of the environment, allowing learning from a wide range of reward signals for diverse tasks.</li>
<li>The proposed method focuses on fine-tuning the retrospective model in the language agent system architecture, without accessing the Actor LLM parameters or needing to propagate gradients through it. The agnostic nature of Retroformer makes it a flexible plug-in module for various types of cloud-based LLMs, such as OpenAI GPT or Google Bard.</li>
</ul>
<h1>2 Related Work</h1>
<p>Autonomous Language Agents We summarize in Table 1 the recent language agent literature related to our work from five perspectives and differentiate our method from them. The completion of a complex task typically involves numerous stages. An AI agent must possess knowledge of these stages and plan accordingly. Chain-of-Thoughts or CoT (Wei et al., 2022) is the pioneering work that prompts the agent to decompose challenging reasoning tasks into smaller, more manageable steps. ReAct (Yao et al., 2023), on the other hand, proposes the exploitation of this reasoning and acting proficiency within LLM to encourage interaction with the environment (e.g. using the Wikipedia search API) by mapping observations to the generation of reasoning and action traces or API calls in natural language. This agent architecture has spawned various applications, such as HuggingGPT (Shen et al., 2023), Generative Agents (Park et al., 2023), WebGPT (Nakano et al., 2021), AutoGPT (Gravitas, 2023), and BabyAGI (Nakajima, 2023).</p>
<p>Table 1: Related work on large language agents.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Approach</th>
<th style="text-align: center;">Gradient <br> learning</th>
<th style="text-align: center;">Arbitrary <br> reward</th>
<th style="text-align: center;">Iterative <br> refinement</th>
<th style="text-align: center;">Hidden <br> constraints</th>
<th style="text-align: center;">Decision <br> making</th>
<th style="text-align: center;">Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT (Wei et al., 2022)</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: left;">ReAct (Yao et al., 2023)</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Self-refine (Madaan et al., 2023b)</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
</tr>
<tr>
<td style="text-align: left;">RAP (Hao et al., 2023)</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion (Shinn et al., 2023)</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\mathcal{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">Retroformer (our method)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>However, these approaches fail to learn from valuable feedback, such as environment rewards, to enhance the agent's behaviors, resulting in performances that are solely dependent on the quality of the pre-trained LLM. Self-refine (Madaan et al., 2023a) addresses this limitation by employing a single LLM as a generator, refiner, and provider of feedback, allowing for iterative refinement of outputs. However, it is not specifically tailored for real-world task-based interaction with the environment. On the other hand, RAP (Hao et al., 2023) repurposes the LLM to function as both a world model and a reasoning agent. It incorporates Monte Carlo Tree Search for strategic exploration within the extensive realm of reasoning with environment rewards. This approach enables effective navigation and decision-making in complex domains. Recently, Shinn et al. (2023) presents Reflexion, a framework that equips agents with dynamic memory and self-reflection capabilities, enhancing their reasoning skills. Self-reflection plays a pivotal role, allowing autonomous agents to iteratively refine past actions, make improvements, and prevent repetitive errors.</p>
<p>Transformer Reinforcement Learning Reinforcement learning with a provided reward function or a reward-labeled dataset, commonly referred to as RLHF, has become a standard practice within the LLM fine-tuning pipeline. These endeavors have convincingly demonstrated the efficacy of RL as a means to guide language models towards desired behaviors that align with predefined reward functions encompassing various domains, including machine translation, summarization, and generating favorable reviews. Among the prevalent transformer RL methods are online RL algorithms such as Proximal Policy Optimization or PPO (Schulman et al., 2017), and offline RL techniques such as Implicit Language Q-Learning or ILQL (Snell et al., 2022) and Direct Preference Optimization or DPO (Rafailov et al., 2023). These methods have been implemented in TRL/TRLX (von Werra et al., 2020; Max et al., 2023) distributed training framework.</p>
<h2>3 Notation and Formulation</h2>
<p>In this work, we denote a large language model (LLM) based action agent as a function $\mathcal{M}<em l="l">{\xi</em>$ denotes the re-parameterized random variables involved in the sampling process. Another note is, the LLM-based agent itself is stateless. All the states and possible memorization are characterized as text in the agent prompt $x$.}}: \mathcal{X} \rightarrow$ $\mathcal{A}$, where $\mathcal{X}$ is the space of prompts, which may include the actual prompts $x^{u}$ provided by the users, as well as some contextual information $c \in \mathcal{C}$. Here $\mathcal{C}$ is the space of context as a representation of the current state $\mathcal{S}$ returned by the environment $\Omega . \mathcal{A}$ is the space of actions. Note the actions taken by most language model based agents are sampled auto-repressively, so $\mathcal{M}$ is a random function. The subscript $\xi_{l</p>
<p>The environment is defined as a tuple $\left(\mathcal{T}<em o="o">{\xi</em>}}, \mathcal{R}\right) . \mathcal{T<em o="o">{\xi</em>\right)$ are mostly zero except very few states, such as in the terminal state for indicating task success or failure.}}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}$ is the state transition function, where $\mathcal{S}$ is the space of states and $\mathcal{A}$ is the action space. Here we assume the states and actions are represented using text. Again we used $\xi_{o}$ to represent the randomness involved in the state transition. For each state $s \in \mathcal{S}$, a reward function is defined as $\mathcal{R}: \mathcal{S} \rightarrow \mathbb{R}$. At each step of the play, the state $s$ is described using natural language, and integrated into the context $c$. In the context, previous states may also be described and embedded to help LLMs making a good guess on the next action to take. As in all the reinforcement learning setting, the final goal is to maximize the cumulative rewards, or episode returns $G_{\text {cum }}=\sum_{t=0}^{T} R\left(s_{t}\right)$. In many situations, the rewards are sparse, i.e., $R\left(s_{t</p>
<p>The retrospective model takes the all the previous states $s_{1, \cdots, t}$, actions $a_{1, \cdots, t}$, rewards $r_{1, \cdots, t}$, and the user prompt $x^{u}$ as input, and massage them into a new prompt $x$ to be consumed by the LLM:</p>
<p>$$
\Gamma_{\xi_{r}, \Theta}:\left[\mathcal{S}<em i="i">{i}, \mathcal{A}</em>}, \mathcal{R<em i="i">{i}, \mathcal{X}</em>
$$}^{u}\right]_{i=1}^{t} \rightarrow \mathcal{X</p>
<p>where $\xi_{r}$ stands for the randomness involved in the retrospective model, and $\Theta$ is the set of learnable parameters in the retrospective model. The goal of the RL optimization is</p>
<p>$$
\begin{aligned}
&amp; \arg \max <em _xi__t="\xi_{t">{\Theta} \quad \mathbb{E}</em> \
&amp; s_{t+1}=\mathcal{T}}, \xi_{o}, \xi_{r}}\left[\sum_{t=1}^{T} R\left(s_{t}\right)\right] \quad \text { s.t. <em o="o">{\xi</em>}}\left(s_{t}, \mathcal{L<em t="t">{\xi</em>\right)\right), \quad \forall t \in{1, \cdots, T-1}
\end{aligned}
$$}} \circ \Gamma_{\xi_{r}, \Theta}\left(\left[s_{i}, a_{i}, r_{i}, x_{i}^{u}\right]_{i=1}^{t</p>
<p>Note that the only learnable parameters are in the retrospective model $M_{r}$. Since LLM action agent is frozen, it can be considered as part of the environment. Specifically, if we construct another environment with the transition function $\mathcal{T}^{\prime}=\mathcal{T}(\mathcal{S}, \bullet) \circ \mathcal{L}: \mathcal{S} \times \mathcal{X} \rightarrow \mathcal{S}$, and the same reward function $\mathcal{R}$, then Eq. (2) is just a regular RL optimization so all the popular RL algorithms apply.</p>
<h1>4 OUR APPROACH: REINFORCING RETROSPECTIVE LANGUAGE AGENT</h1>
<p>As illustrated in Fig. 2, our proposed framework Retroformer is comprised of two language model components: an actor LLM, denoted as $M_{a}$, which generates reasoning thoughts and actions, and a retrospective LLM, denoted as $M_{r}$, which generates verbal reinforcement cues to assist the actor in self-improvement by refining the actor prompt with reflection responses.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Framework overview. (a) The retrospective agent system (Sec. 4.1) contains two LLMs communicating to refine agent prompts with environment feedback. (b) The retrospective LM is fine-tuned with response ratings using proximal policy optimization (Sec. 4.2).</p>
<p>We assume in this paper that the actor model is a frozen LLM whose model parameters are inaccessible (e.g., OpenAI GPT) and the retrospective model is a smaller, local language model that can be fine-tuned under low-resource settings (e.g., Llama-7b). In addition, Retroformer has an iterative policy gradient optimization step which is specifically designed to reinforce the retrospective model with gradient-based approach. We provide in this section a detailed description</p>
<p>of each of these modules and subsequently elucidate their collaborative functioning within the Retroformer framework. The implementation details are presented in Appendix C.</p>
<h1>4.1 RETROSPECTIVE AGENT ARCHITECTURE</h1>
<p>As illustrated in Fig. 2(a), for the actor and retrospective models, we apply a standard communication protocol modified from the Relexion agent architecture (Shinn et al., 2023), in which the retrospective model refines the actor prompt by appending verbal feedback to the prompt.</p>
<p>Actor Model The actor model is a LLM hosted in the cloud, whose model parameters are hidden and frozen all the time. The actor LM is instructed to generate actions with required textual content, taking into account the observed states. Similar to reinforcement learning, we select an action or generation, denoted as $a_{t}$, from the current policy $\pi_{\theta}$ at time step $t$ and receive an observation, represented by $s_{t}$, from the environment. We use ReAct (Yao et al., 2023) as our actor prompt.</p>
<p>$$
a_{k, i, t}=M_{a}\left(\left[s_{k, i, \tau}, a_{k, i, \tau}, r_{k, i, \tau}\right]<em i_="i," k_="k," t="t">{\tau=1}^{t-1}, s</em>\right)
$$</p>
<p>Retrospective Model The retrospective model $M_{r}$ is instantiated as a local LM. Its primary function is to produce self-reflections, offering valuable feedback for diagnosing a possible reason for prior failure and devising a new, concise, high-level plan that aims to mitigate same failure. Operating under a sparse reward signal, such as binary success status (success/failure), the model detects the root cause of failure by considering the current trajectory alongside its persistent memory.</p>
<p>$$
y_{k, i}=M_{r}(\underbrace{\left[s_{k, i, \tau}, a_{k, i, \tau}, r_{k, i, \tau}\right]<em i="i" k_="k,">{\tau=1}^{T}, G</em>}<em i="i" k_="k,">{\text {Reflection prompt } x</em>)
$$}</p>
<p>This self-reflection feedback $y_{k, i}$ is appended to the actor prompt to prevent repetitive errors in a specific environment in future attempts. Consider a multi-step task, wherein the agent failed in the prior trial. In such a scenario, the retrospective model can detect that a particular action, denoted as $a_{t}$, led to subsequent erroneous actions and final failure. In future trials, the actor LM can use these self-reflections, which are appended to the prompt, to adapt its reasoning and action steps at time $t$, opting for the alternative action $a_{t}^{\prime}$. This iterative process empowers the agent to exploit past experiences within a specific environment and task, thereby avoiding repetitive errors.</p>
<p>Memory Module The actor model generates thoughts and actions, by conditioning on its recent interactions (short-term memory) and reflection responses (long-term memory) in the text prompt.</p>
<ul>
<li>Short-term memory. The trajectory history $\tau_{i}$ of the current episode $i$ serves as the short-term memory for decision making and reasoning.</li>
<li>Long-term memory. The self-reflection responses that summarize prior failed attempts are appended to the actor prompt as the long-term memory.</li>
</ul>
<p>To facilitate policy optimization in Section 4.2, we store the instructions and responses of the retrospective model of each trial, together with the episode returns in a local dataset, which we call replay buffer. We sample from the replay buffer to fine-tune the retrospective model. The long and short-term memory components provide context that is specific to a given task over several failed trials and the replay buffer provides demonstrations of good and bad reflections across the tasks and environments, so that our Retroformer agent not only exploits lessons learned over failed trials in the current task, but also explores by learning from success in other related tasks.</p>
<ul>
<li>Replay buffer. The memory $D_{\mathrm{RL}}$ which stores the triplets $\left(x_{k, i}, y_{k, i}, G_{k, i}\right)$ of the reflection instruction prompt $x_{k, i}$, reflection response $y_{k, i}$ and episode return $G_{k, i}$ of trial $i$ and task $k$.</li>
</ul>
<p>Reward Shaping Instead of exactly matching the ground truth to produce a binary reward, we use soft matching (e.g., f1 score) whenever possible to evaluate the alignment of the generated output with the expected answer or product as the reward function. The details are in Appendix C.3.</p>
<p>4.2 Policy Gradient Optimization</p>
<p>The actor model $M_{a}$ is regarded as an frozen LLM, such as GPT, with inaccessible model parameters. In this scenario, the most direct approach to enhancing actor performance in a given environment is by refining the actor LMâ€™s prompt. Consequently, the retrospective model $M_{r}$, a smaller local language model, paraphrases the actorâ€™s prompt by incorporating a concise summary of errors and valuable insights from failed attempts. We therefore aim to optimize the $M_{r}$ model using environment reward. The desired behavior of $M_{r}$ is to improve the actor model $M_{a}$ in next attempt. Hence, the difference in episode returns between two consecutive trials naturally serves as a reward signal for fine-tuning the retrospective model $M_{r}$ with reinforcement learning.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Policy gradient optimization of retrospective LM using RLHF training pipeline.</p>
<p>Instruction and Response Generation The retrospective model generates a pair of instruction and response at the end of each episode $i$ in the environment $k$. In the episode $i$, the actor produces a trajectory $\tau_{i}$ by interacting with the environment. The reward function then produces a score $r_{i}$. At the end of the episode, to produce verbal feedback for refining the actor prompt, $M_{r}$ takes the set of ${\tau_{i},r_{i}}$ as the instruction $x_{k,i}$ and is prompted to produce a reflection response $y_{k,i}$. All these instruction-response pairs $(x_{k,i},y_{k,i})$ across tasks and trials are stored to a local dataset $D_{\mathrm{RL}}$, which we call â€œreplay bufferâ€, for fine-tuning the $M_{r}$.</p>
<p>Response Rating As illustrated in Fig. 2(b), let us assume a reflection prompt $x_{k,i}$ and the corresponding episode return $G_{k,i}$, and the retrospective model $M_{r}$ generates the response $y_{k,i}$ that summarizes the mistakes in $i$, which results in the return $G_{k,i+1}$ in the next attempt $i+1$. Because the actor is a frozen LM and the temperature is low as default (Yao et al., 2023), the injected randomness that leads to differences in returns $\Delta G_{k,i}=G_{k,i+1}-G_{k,i}$ are mostly from the reflection responses $y_{k,i}$, in which positive $\Delta G_{k,i}$ indicates better responses that help the actor learn from prior errors, and hence should be rated with higher scores; negative or zero $\Delta G_{k,i}$ indicates worse responses that needs to be avoided and hence should be rated with lower scores. Therefore, we approximate the rating score of a reflection instruction-response pair $(x_{k,i},y_{k,i})$ as:</p>
<p>$r(x_{k,i},y_{k,i}) \triangleq G_{k,i+1}-G_{k,i}.$ (5)</p>
<p>Proximal Policy Optimization The optimization step of Retroformer is visualized in Fig. 3. We use the differences of episode returns as the ratings of the generated reflection responses. The retrospective language model is fine-tuned with the response ratings following the RLHF training procedures (although we do not have human in the loop) with proximal policy optimization (PPO):</p>
<p>$\mathcal{L}<em D__mathrm_RL="D_{\mathrm{RL" _sim="\sim" x="x">{\text {PPO }}=\mathbb{E}</em>}}} \mathbb{E<em _theta="\theta">{y \sim \mathrm{LLM}</em>\right],$ (6)}^{\mathrm{RL}}(x)}\left[r_{\theta}(x, y)-\beta \log \frac{\mathrm{LLM}_{\rho}^{\mathrm{RL}}(y \mid x)}{\mathrm{LLM}^{\mathrm{Ref}}(y \mid x)</p>
<p>where $(x, y)$ are sampled from the replay buffer (note there is only 1 step in the Retrospective modelâ€™s trajectory), $r_{\theta}(x, y)$ is the defined reward model, and the second term in this objective is the KL divergence to make sure that the fine-tuned model $\mathrm{LLM}^{\mathrm{RL}}$ does not stray too far from the frozen reference model $\mathrm{LLM}^{\text {Ref }}$.</p>
<p>For offline training, we collected the dataset $D_{\mathrm{RL}}$ by rolling out a base policy, i.e., the frozen actor LM and the initialized retrospective LM, in the tasks in the training sets for $N$ trials and compute</p>
<p>the ratings. We apply the standard RLHF pipeline to fine-tune the retrospective model offline before evaluating the agent in the validation tasks. In online execution, we use best-of- $n$ sampler, with the scores evaluated by the learned reward model from RLHF pipeline (Ouyang et al., 2022), for generating better retrospective responses in each trial.</p>
<h1>5 EXPERIMENTS</h1>
<p>Extensive experiments are conducted to evaluate our method, including comparisons with ReAct and Reflexion performances, and visualization and discussion of agent's generated text and actions.</p>
<h3>5.1 EXPERIMENT SETUP</h3>
<h3>5.1.1 ENVIRONMENT</h3>
<p>We use open-source environments: HotPotQA (Yang et al., 2018), WebShop (Yao et al., 2022) and AlfWorld (Shridhar et al., 2021), which evaluates the agent's reasoning and tool usage abilities for question answering reasoning, multi-step decision making, and web browsing.</p>
<p>HotPotQA The agent is asked to solve a question answering task by searching in Wikipedia pages. At each time step, the agent is asked to choose from three action types or API calls:</p>
<ol>
<li>SEARCH[ENTITY], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.</li>
<li>LookUP[KEYWORD], which returns the next sentence containing keyword in the last passage successfully found by Search.</li>
<li>FINISH[ANSWER], which returns the answer and finishes the task.</li>
</ol>
<p>AlfWorld The agent is asked to perform six different tasks, including finding hidden objects (e.g., finding a spatula in a drawer), moving objects (e.g., moving a knife to the cutting board), and manipulating objects with other objects (e.g., chilling a tomato in the fridge) by planning with the following action APIs, including GOTO[LOCATION], TAKE[OBJ], OPEN[OBJ], CLOSE[OBJ] , TOGGLE[OBJ], CLEAN[OBJ], HEAT[OBJ], and COOL[OBJ], etc.</p>
<p>WebShop The agent is asked to solve a shopping task by browsing websites with detailed product descriptions and specifications. The action APIs include searching in the search bar, i.e., SEARCH[QUERY] and clicking buttons in the web pages, i.e., CHOOSE[Button]. The clickable buttons include, product titles, options, buy, back to search, prev/next page, etc.</p>
<h3>5.2 EXPERIMENT SETTINGS</h3>
<p>We use GPT-3 (model: text-davinci-003) and GPT-4 as the frozen actor model. For the retrospective model, we fine-tune it from LongChat (model: longchat-7b-16k). The implementation details, which include data collection and model training are in Appendix C.</p>
<p>Evaluation Metrics We report the success rate over validation tasks in an environment. The agent is evaluated on 100 validation tasks from the distractor dev split of open-source HotPotQA dataset, 134 tasks in AlfWorld and 100 tasks in WebShop, as in (Shinn et al., 2023).</p>
<p>Baselines We experiment with two language agent baselines: 1) ReAct (Yao et al., 2023). This is the state-of-the-art frozen language agent architecture, which does not learn from the environment rewards at all, thus serving as a baseline for showing how the agent performs without using environment feedback. 2) Reflexion (Shinn et al., 2023). This is the state-of-the-art language agent architecture that the authors identify from literature so far. This agent enhances from verbal feedback of the environment, but does not use gradient signals explicitly. It can serve as a baseline for showing the effectiveness of gradient-based learning. 3) SAC. Furthermore, we include one online RL algorithm, i.e., Soft Actor-Critic (Haarnoja et al., 2018), or SAC as baseline model for comparison.</p>
<h1>5.3 RESULTS</h1>
<p>We present the experiment results in Table 2 and discuss the details below.</p>
<p>Table 2: Results with Retroformer in the HotPotQA, AlfWorld and Webshop environments. We report the average success rate for the language agents over tasks in the environment. "#Params" denotes the learnable parameters of each approach. "#Retries" denotes the number of retry attempts. "LoRA $r$ " denotes the rank of low-rank adaptation matrices for fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">#Params</th>
<th style="text-align: left;">#Retries</th>
<th style="text-align: left;">HotPotQA</th>
<th style="text-align: left;">AlfWorld</th>
<th style="text-align: left;">WebShop</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SAC</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">2.25 M</td>
<td style="text-align: left;">$\mathrm{N}=1$</td>
<td style="text-align: left;">$27 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$58.95 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$30 \%$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{~N}=4$</td>
<td style="text-align: left;">$27 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$59.7 \%$</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$30 \%$</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Actor LLM</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">GPT-3</td>
<td style="text-align: left;">GPT-4</td>
</tr>
<tr>
<td style="text-align: left;">ReAct</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$34 \%$</td>
<td style="text-align: left;">$40 \%$</td>
<td style="text-align: left;">$62.69 \%$</td>
<td style="text-align: left;">$77.61 \%$</td>
<td style="text-align: left;">$33 \%$</td>
<td style="text-align: left;">$42 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Reflexion</td>
<td style="text-align: left;">0</td>
<td style="text-align: left;">$\mathrm{~N}=1$</td>
<td style="text-align: left;">$42 \%$</td>
<td style="text-align: left;">$46 \%$</td>
<td style="text-align: left;">$76.87 \%$</td>
<td style="text-align: left;">$81.34 \%$</td>
<td style="text-align: left;">$35 \%$</td>
<td style="text-align: left;">$42 \%$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{~N}=4$</td>
<td style="text-align: left;">$50 \%$</td>
<td style="text-align: left;">$52 \%$</td>
<td style="text-align: left;">$84.33 \%$</td>
<td style="text-align: left;">$85.07 \%$</td>
<td style="text-align: left;">$35 \%$</td>
<td style="text-align: left;">$44 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Retroformer</td>
<td style="text-align: left;">0.53 M</td>
<td style="text-align: left;">$\mathrm{~N}=1$</td>
<td style="text-align: left;">$45 \%$</td>
<td style="text-align: left;">$48 \%$</td>
<td style="text-align: left;">$93.28 \%$</td>
<td style="text-align: left;">$95.62 \%$</td>
<td style="text-align: left;">$36 \%$</td>
<td style="text-align: left;">$43 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(w/ LoRA r=1)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{~N}=4$</td>
<td style="text-align: left;">$53 \%$</td>
<td style="text-align: left;">$53 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$36 \%$</td>
<td style="text-align: left;">$45 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Retroformer</td>
<td style="text-align: left;">2.25 M</td>
<td style="text-align: left;">$\mathrm{~N}=1$</td>
<td style="text-align: left;">$48 \%$</td>
<td style="text-align: left;">$51 \%$</td>
<td style="text-align: left;">$97.76 \%$</td>
<td style="text-align: left;">$97.76 \%$</td>
<td style="text-align: left;">$34 \%$</td>
<td style="text-align: left;">$43 \%$</td>
</tr>
<tr>
<td style="text-align: left;">(w/ LoRA r=4)</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathrm{~N}=4$</td>
<td style="text-align: left;">$54 \%$</td>
<td style="text-align: left;">$54 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$100 \%$</td>
<td style="text-align: left;">$36 \%$</td>
<td style="text-align: left;">$46 \%$</td>
</tr>
</tbody>
</table>
<p>Question Answering - HotPotQA We visualize the performances of Retroformer against the baselines in Fig. 4. As shown in Table 2, we observe that our method consistently improve the agent performances over trials and the effects of fine-tuned retrospective model (Retroformer) are mostly significant in the first few trials.</p>
<p>Furthermore, as shown in Fig. 4, our agent outperforms the two strong baselines. Specifically, the results indicate that our reinforced model provides the language agents with better reflection responses in early trials, which enables the agents to learn faster, while also achieving better performances in the end. Our Retroformer agent achieves $54 \%$ success rate in 4 trials, which is better than the state-of-the-art $50 \%$ success rate reported in (Jang, 2023) that uses a much larger frozen language model, i.e., GPT-3 (model: text-davinci-003) as the reflection component. The results show the effectiveness of our policy gradient approach for fine-tuning the agent with offline samples.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Retroformer shows faster and consistent performance improvement of success rate.</p>
<p>We then examine how the retrospective model is improved with policy optimization by comparing the generated responses from the frozen LM and the ones from the fine-tuned, reinforced LM. As an example, Fig. 5 illustrates how the uninformative self-reflections from a frozen LLM, which we propose in Fig. 1, are tackled by RL. The agent failed in the last attempt because it submitted "Teen Tians" and "Teen Titans Go" as the answer to the Teen Titans spin-off series, while the correct answer includes only "Teen Titans Go"; The agent forgot its original goal during a chain of lengthy interactions. The self-reflection from the frozen model reiterated the prior action sequences that led to failure as the steps that the agent should have done, which prompts the agent to repeat these steps in the next attempt, resulting in an infinite loop. On the contrary, our reinforced response prompts the agent to focus on spinoff series and asks the agent to find the answer in the previous actions and observations with the search results of "Tara Strong". Retroformer presents better credit assignment and root cause analysis abilities, and has the power to generate actionable insights.</p>
<p>Decision Making - AlfWorld As showcased in Fig. 6(a), the performance improvement by Retroformer over the frozen baselines is significant and our method solves the environment</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Prior failed trial</th>
<th style="text-align: center;">Reflection (Green: Retroformer; Brown: Frozen LM)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Thought 1: I need to search Juliet Starling and find the Canadian-American actress who voiced her, then find the Teen Titans spinoff series she has done voice roles for.</td>
<td style="text-align: center;">I failed to find the Teen Titans spinoff series that Tara Strong has done voice roles for. Instead, I found Lollipop Chainsaw, a video game that Juliet Starling appeared in. I should have searched Tara Strong and Teen Titans or Teen Titans Go! specifically in my previous action to find the correct answer.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Action 1: Search[Juliet Starling] Observation 1: Could not find [Juliet Starling]</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Thought 2: Juliet Starling is a character from Lollipop Chainsaw, so I can search Lollipop Chainsaw.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Lollipop Chainsaw featured Juliet Starling, who was voiced by a Canadian-American actress who has done voice roles for what Teen Titans spinoff series?</td>
<td style="text-align: center;">Action 2: Search[Lollipop Chainsaw] Observation 2: Lollipop Chainsaw is a ...</td>
<td style="text-align: center;">My thought process was strong but my execution was flawed. I should have searched for Lollipop Chainsaw first and looked up the Canadian-American actress who voiced Juliet Starling afterwards. I also should have looked up Tara Strong's filmography and searched for any voice roles she did specifically for Teen Titans or Teen Titans Go!.Next trial Question: The novel was adapted into a film by a director known for his work on another iconic 1980s franchise. What novel is this film based on?Thought 1.</td>
</tr>
</tbody>
</table>
<p>Figure 5: Response refinement from the reinforced retrospective model. Note that the lengthy observation step in the prior failed trial column is abbreviated for better presentation purposes.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Comparisons of Retroformer against baselines in (a) AlfWorld and (b) WebShop environments under different base Actor LLM and LoRA rank $r=1,4$.
within 3 retries. Similar patterns are observed that the agent performs slightly better with more learnable parameters $(r=4)$ and that the improvements are mostly from early retries. We find that the reinforced retrospective model behaves like a summarization model of the prior failed plans and finds the differences of the prior plan with the task descriptions. With the permissible actions seen in the task instructions, this behavior effectively prevents repetitive failures and reduces search spaces.</p>
<p>Web Browsing - WebShop As in Fig. 6(b), the performance improvement by Retroformer over the frozen baselines is observed but the improvements may be limited, when compared with HotPotQA and AlfWorld, with $4 \%$ improvement in success rate with 4 retries. This limitation was also observed in (Shinn et al., 2023) as web browsing requires a significant amount of exploration with more precise search queries, if compared with HotPotQA. The results probably indicate that the verbal feedback approach (Reflexion, Retroformer) is not an optimal method for this environment, but our fine-tuning method still proves effective.</p>
<h1>6 CONCLUSION</h1>
<p>In this study, we present Retroformer, an elegant framework for iteratively improving large language agents by learning a plug-in retrospective model. This model, through the process of policy optimization, automatically refines the prompts provided to the language agent with environmental feedback. Through extensive evaluations on real-world datasets, the method has been proven to effectively improve the performances of large language agents over time both in terms of learning speed and final task completion.
By considering the LLM action agent as a component of the environment, our policy gradient approach allows learning from arbitrary reward signals from diverse environments and tasks. This facilitates the iterative refinement of a specific component within the language agent architecture the retrospective model, in our case, while circumventing the need to access the Actor LLM parameters or propagate gradients through it. This agnostic characteristic renders Retroformer a concise</p>
<p>and adaptable plug-in module for different types of cloud-hosted LLMs, such as OpenAI GPT and Bard. Furthermore, our approach is not limited to enhancing the retrospective model alone; it can be applied to fine-tune other components within the agent system architecture, such as the memory and summarization module, or the actor prompt. By selectively focusing on the component to be finetuned while keeping the remainder fixed, our proposed policy gradient approach allows for iterative improvements of the component with reward signals obtained from the environment.</p>
<h1>REFERENCES</h1>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Harrison Chase. Langchain. https://github.com/hwchase17/langchain, 2023.
Significant Gravitas. Autogpt. https://github.com/Significant-Gravitas/ Auto-GPT, 2023.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861-1870. PMLR, 2018.</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023.</p>
<p>Eric Jang. Can llms critique and iterate on their own outputs? evjang.com, Mar 2023. URL https://evjang.com/2023/03/26/self-reflection.html.</p>
<p>Aman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, and Amir Yazdanbakhsh. Learning performance-improving code edits. arXiv preprint arXiv:2302.07867, 2023a.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023b.</p>
<p>Max, Jonathan Tow, Leandro von Werra, Shahbuland Matiana, Alex Havrilla, cat state, Louis Castricato, Alan, Duy V. Phung, Ayush Thakur, Alexey Bukhtiyarov, aaronrmm, alexandremuzio, Fabrizio Milo, Mikael Johansson, Qing Wang, Chen9154, Chengxi Guo, Daniel, Daniel King, Dong Shin, Ethan Kim, Gabriel Simmons, Jiahao Li, Justin Wei, Manuel Romero, Nicky Pochinkov, Omar Sanseviero, and Reshinth Adithyan. CarperAI/trlx: v0.7.0: NeMO PPO, PEFT Migration, and Fixes, June 2023. URL https://doi.org/10.5281/zenodo.8076391.</p>
<p>Volodymyr Mnih, AdriÃ  PuigdomÃ¨nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. CoRR, abs/1602.01783, 2016.</p>
<p>Yohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Joon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.</p>
<p>John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization. CoRR, abs/1502.05477, 2015.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768.</p>
<p>Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.
R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12, volume 12, pp. 1057-1063. MIT Press, 2000.</p>
<p>Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd. html.</p>
<p>Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, and Nathan Lambert. Trl: Transformer reinforcement learning. https://github.com/lvwerra/trl, 2020.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757, 2022.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023.</p>
<p>Xingdi Yuan, Marc-Alexandre CÃ´tÃ©, Alessandro Sordoni, Romain Laroche, Remi Tachet des Combes, Matthew Hausknecht, and Adam Trischler. Counting to explore and generalize in textbased games. arXiv preprint arXiv:1806.11525, 2018.</p>
<h1>Appendix for</h1>
<h2>"Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization"</h2>
<h2>A Challenges</h2>
<p>Although LLMs are not designed to handle tool use or take actions, it has been observed (Gravitas, 2023; Nakajima, 2023; Chase, 2023) that empirically for text-rich environment, especially when the actions and states are accurately described using natural languages, LLMs work surprisingly well. However there are still plenty of challenges applying LLM-based agents. Here we list several below.</p>
<p>Spurious Actions LLMs are not pre-trained or designed with an action-agent application in mind. Even some restrictions are explicitly specified in the prompt, the LLM model may still generate spurious actions that are not in the action space $\mathcal{A}$.</p>
<p>Limited Prompt Length LLM itself is stateless. However, in applications it is preferred to empower agents with states or memories for better performance. It has been observed that LLM based agents are easy to run into infinite loops if the states are not handled nicely. Many LLM agents concatenate all the previous state descriptions and actions into the prompt so that LLM as a way to bestow "state" to the LLM. Inevitably this methodology runs into the prompt length issues. As the trajectory grows longer, the prompt runs out of spaces.</p>
<p>Heuristic Prompt Engineering Even though a lot of paradigms have been proposed to improve LLM agents' performance (Yao et al., 2023; Ahn et al., 2022), there is a lack of systematic methodologies for consistent model refinement. In fact, manual prompt tuning is still widely used in a lot of the application scenarios.</p>
<p>Prohibitive Training Most of the well-performing LLMs are too large to be fit in just one or two GPUs. It is technically challenging to optimize the LLMs directly as is done in the the classical reinforcement learning setting. In particular, OpenAI has not provided any solution for RL based finetuning. Most of the issues are caused by the fact that LLMs are not pre-trained or designed with an action-agent application in mind.</p>
<h2>B IntUition</h2>
<p>Compared to the LLM-based action agents, classical RL agents, though not able to handle text-based environments as nicely in the zero shot setting, are able to keep improving based on the feedback and rewards provided by the environment. Popular RL algorithms include Policy Gradient (Sutton et al., 2000), Proximal Policy Optimization Algorithm (PPO) (Schulman et al., 2017), Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), and Advantage Actor Critic methods (Mnih et al., 2016).</p>
<p>In this draft we are proposing a simple but powerful novel framework to tackle the challenges mentioned above. On one hand, we would like to leverage the classical RL based optimization algorithms such as policy gradient to improve the model performance. On the other hand, our framework avoids finetuning on the LLM directly. The key is, instead of training the LLM directly, we train a retrospective LM. The retrospective LM takes users' prompt, rewards and feedback from the environment as input. Its output will be prompt for the actual LLM to be consumed. RL algorithms are employed to optimize the weights in the retrospective LM model instead of directly on the LLM. In our framework the weights in the actual LLM is assumed to be fixed (untrainable), which aligns well with the application scenario when the LLM is either too large to tune or prohibited from any tuning.</p>
<p>Another perspective viewing our framework is, we train a retrospective LM to apply automatic prompt tuning for the LLM agents. In this case, the RL algorithms such as policy gradients are employed to optimize the prompts. Ideally the retrospective LM can help summarize the past "experience", the users' prompt, the environments' feedback into a condensed text with length limit</p>
<p>so that it is easier for the LLM to digest. To some extent, in our setting the original LLM can be considered as part of the environment since its parameters are all fixed.</p>
<h1>C IMPLEMENTATION DETAILS</h1>
<h2>C. 1 RETROFORMER</h2>
<p>Model We use GPT-3 (model: text-davinci-003) as the frozen actor model. For the retrospective model, we instantiate it from LongChat (model: longchat-7b-16k), which is a LM with 16k context length by fine-tuning llama-7b on instruction-following samples from ShareGPT. In all experiments, we set the temperature of actor LM as zero, i.e., $\mathrm{T}=0$ and top $\mathrm{p}=1$ to isolate the randomness of LM from the effects of reflections. We acknowledge that setting a higher temperature value can encourage exploration but it can obscure the impact of the proposed approaches, making it difficult to compare against existing baselines with T=0 (Yao et al., 2023; Shinn et al., 2023).</p>
<p>Setup Our proposed learning framework is developed by using multiple open-source tools as follows. We use the OpenAI connectors from langchain to build our actor models $M_{a}$. During inference of the retrospective model, we host an API server using FastChat and integrates it with langchain agents. The tool can host longchat-7b-16k with concurrent requests to speed up RL policy rollouts. For fine-tuning the retrospective model, we develop our training pipeline with $t r l$, which supports transformer reinforcement learning with PPO trainer.</p>
<p>We present the details of the specific prompts we used and the full agent demonstrations and examples for each environment in Appendix E.</p>
<p>Data Collection For HotPotQA environment, We collected 3,383 reflection samples by running the base rollout policy for 3 trials $(N=3)$ for 3,000 tasks in the training set, in which 1,084 instruction-response pairs have positive ratings. For AlfWorld, we collected 523 reflection samples and for WebShop, we collected 267 reflection samples.</p>
<p>Training We fine-tune the retrospective model $M_{r}$ with 4-bit quantized LoRA adapters ( $\mathrm{r}=1$ or $\mathrm{r}=4$ ) on the offline RL datasets with epochs=4; batch size=8; $\mathrm{l} r=1.4 \mathrm{e}-5$. The number of trainable parameters is $0.53 \mathrm{M}(0.015 \%$ of llama-7b) or 2.25 M . Since longchat-16k is based on Llama, we used the default llama recipes for finetuning. Specifically, we first run supervised fine-tuning trainer on the samples with positive ratings for 2 epochs and then the RLHF pipeline, including reward modeling, and RL fine-tuning with PPO, on the whole offline rating dataset using the default settings for llama-7b model. We list the key hyperparameters here:</p>
<ul>
<li>Supervised Finetuning: learning rate=1e-5, batch size=32, max steps=5,000</li>
<li>Reward Modeling: learning rate=2.5e-5, batch size=32, max steps=20,000</li>
<li>Policy Gradient Finetuning: learning rate=1.4e-5, max steps=20,000, output max length=128, batch size=64, gradient accumulation steps=8, ppo epochs=4</li>
</ul>
<p>Reproducibility All experiments are done in Google Cloud Platform (GCP) GKE environment with A100 40GB GPUs. The code can be found in https://anonymous.4open.science/ r/Retroformer-F107. We plan to open source the code repository after the review period.</p>
<p>Algorithm The offline PPO algorithm we used for finetuning the Retrospective component in this paper is presented below in Algorithm 1. It contains three steps: offline data collection, reward model learning, and policy gradient finetuning. We use the offline ratings data to train a reward model first, and plug in the reward model for PPO finetuning.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 Retroformer with Policy Gradient Optimization
    Initialize TEXT-DAVINCI-003 as the Retrospective model with LONGCHAT-16K. Set the maxi-
    mum trials for rollouts as \(N=3\). The temperature used for sampling \(t_{s}=0.9\).
    Step 1: Offline Data Collection. Collect multiple rollouts for each environments \(k(k=\)
    \(1, \cdots, K\) ) for the tasks in the training sets and save as \(D_{\mathrm{RL}}\).
    for episode \(t=1, \ldots, \mathrm{~N}\) do
        for source domain \(\mathrm{k}=1, \ldots, \mathrm{~K}\) do
            Receive trajectory \(\left[s_{k, i, \tau}, a_{k, i, \tau}, r_{k, i, \tau}\right]_{T=1}^{T}\) and episodic returns \(G_{k, i}\) for task \(i\).
            for unsuccessful tasks \(j\) do
                Randomly sample a pair of reflection responses \(\left(y_{k, j}^{(1)}, y_{k, j}^{(2)}\right)\) with Retrospective LM tem-
                perature set to \(t_{s}\), with the same instruction prompt defined in Eq. (4).
                Roll out the next episode with \(y_{k, j}\), and receive the episodic returns \(\left(G_{k, i+1}^{(1)}, G_{k, i+1}^{(2)}\right)\).
                Compute reflection response rating by \(r\left(x_{k, i}, y_{k, i}\right) \triangleq G_{k, i+1}-G_{k, i}\) in Eq. (5).
                Label the response with higher ratings as the accepted response while the lower response
                is labeled as the rejected response.
            end for
        end for
    end for
    Step 2. Reward Model Learning. Use the REWARDTRAINER in TRL to train a model for
    classifying accepted and rejected responses given instructions.
    Step 3: Policy Gradient Finetuning. Plug-in the trained reward model and use the PPOTRAINER
    in TRL to finetune the Retrospective model for generating reflection responses with higher ratings.
</code></pre></div>

<h1>C. 2 BASELINE: SOFT-ACTOR CRITIC AGENT</h1>
<p>Traditional reinforcement learning methods have been recognized to perform well within the same framework of interaction-feedback-learning. We include one online RL algorithm, i.e., Soft ActorCritic (Haarnoja et al., 2018), or SAC as baseline model for comparison. Given that the three environments are text-based games, inspired by (Yuan et al., 2018), we do mean-pooling for the embeddings of the generated text outputs, such as "Search[It Takes a Family]" as the agent actions. Therefore, the action space is continuous and is of 768 dimension. We apply LoRA adapters with $r=4$ on the agent Action model instantiated from longchat-16k, and use SAC to do the online updates, with discount factor gamma=0.99, interpolation factor polyak=0.995, learning rate=0.01, entropy regularzation alpha=0.2, and batch size=8.</p>
<h2>C. 3 REWARD FUNCTION</h2>
<p>HotPotQA F1 reward is used in the HotPotQA environment for comparing the matching of a generated answer to a question against the ground truth answer. After removing the stopwords in both answers, we calculate the number of common tokens in two answers. Then Precision is # of common tokens divided by # of generated answer tokens and the Recall is # common tokens divided by # ground truth answer tokens. We can then compute f1 from precision and recall.</p>
<p>AlfWorld The binary success (1) and failure of the tasks at the end of episode is used as the reward.</p>
<p>WebShop In each episode, the agent receives a reward $r=\mathcal{R}\left(s_{T}, a\right)$ in the end at timestep $T$, where $a=$ choose ${$ buy $}, y$ is the product chosen by the agent in the final state $s_{T}$, and $Y_{\text {att }}$ and $Y_{\text {opt }}$ are its corresponding attributes and options. The reward is defined as:</p>
<p>$$
r=r_{\text {type }} \cdot \frac{\left|U_{\text {att }} \cap Y_{\text {att }}\right|+\left|U_{\text {opt }} \cap Y_{\text {opt }}\right|+\mathbf{1}\left[y_{\text {price }} \leq u_{\text {price }}\right]}{\left|U_{\text {att }}\right|+\left|U_{\text {opt }}\right|+1}
$$</p>
<p>where the type reward $r_{\text {type }}=\operatorname{TextMatch}\left(\bar{y}, \bar{y}^{<em>}\right)$ is based on text matching heuristics to assign low reward when $y$ and $y^{</em>}$ have similar attributes and options but are obviously different types of products. For example, "butter" and "plant-based meat" differ in types but may both contain attributes "cruelty-free", "non-GMO", and an option "size: pack of 2".</p>
<h1>D ADDITIONAL EXPERIMENTS</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Frozen model response</th>
<th style="text-align: left;">Reinforced model response</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I got stuck in a loop where I kept searching for the same person,</td>
<td style="text-align: left;">I think the issue might have been with the search. I was not able to find</td>
</tr>
<tr>
<td style="text-align: left;">Henry Miller, instead of Beatrice Sylvas Wickens. It's possible</td>
<td style="text-align: left;">the name "Miller" in the paragraph, which is why I was not able to find</td>
</tr>
<tr>
<td style="text-align: left;">that I may have misheard or misunderstood the question. To avoid</td>
<td style="text-align: left;">James Henry Miller's wife. I should have searched for "Miller's wife"</td>
</tr>
<tr>
<td style="text-align: left;">this, I should have written down the question and made sure I</td>
<td style="text-align: left;">instead of "James Henry Miller's wife" to avoid this confusion.</td>
</tr>
<tr>
<td style="text-align: left;">understood it correctly before beginning the search. Additionally,</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">I should have tried searching for "James Henry Miller's wife"</td>
<td style="text-align: left;">New Plan:</td>
</tr>
<tr>
<td style="text-align: left;">instead of just "James Henry Miller".</td>
<td style="text-align: left;">To avoid confusion, I will search for "Miller's wife" instead of "James</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Henry Miller's wife" and directly look for the nationality of the wife.</td>
</tr>
</tbody>
</table>
<p>Figure 7: Response refinement from the reinforced retrospective model with structured format.
Structured Reflections and Action Plans. We discover one emergent behavior of the reinforced model that it can automatically paraphrase the original responses into two separate structured sections, namely Reflection section and New plan: section, although not being explicitly trained or prompted for. One such example is shown in Fig. 7. The paraphrased response retrospects in the first paragraph and provides actionable insights next, while the response from the frozen LM interleaved both parts in one paragraph, making it hard to comprehend. We can also observer from Fig. 5 that the reinforced response removes the messy, irrelevant "Next trial:" content in the end for cleaner format, which may very likely result from LLM hallucination.</p>
<h2>E Full Examples</h2>
<h2>E. 1 Actor Prompt Engineering</h2>
<p>An example of the HotPotQA actor language model prompt is shown below.</p>
<p>Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types:
(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup[keyword], which returns the next sentence containing the keyword in the last passage successfully found by Search.
(3) Finish[answer], which returns the answer and finishes the task.</p>
<p>You may take as many steps as necessary.
You have attempted to answer the following question before and failed. The following reflection(s) give a plan to avoid failing to answer the question in the same way you did previously. Use them to improve your strategy of correctly answering the given question.</p>
<h1>Reflections:</h1>
<ul>
<li>I should have searched for the title of the book instead of the author. I also didn't check for the proverb which was not the question.</li>
<li>I should have searched for "It Takes a Family by Rick Santorum" instead of just "It Takes a Family" to get the correct answer.</li>
<li>My search was not concise and I did not specify what I was looking for. I also did not check similar results. In this case, I should have searched for 'It Takes a Family by Rick Santorum' to find the correct answer.</li>
</ul>
<p>Question: It Takes a Family is a response to this 1996 book that was published by who Thought 1: I need to search for the title of the book, "It Takes a Family", and find who published it.
Action 1: Search["It Takes a Family"]
Observation 1: Richard John Santorum ( san-TOR-wn; born May 10, 1958) is an American politician, attorney, author, and political commentator who represented Pennsylvania in the United States Senate from 1995 to 2007. He was the Senate's third-ranking Republican during the final six years of his tenure. He also ran unsuccessfully for President of the United States in the 2012 Republican primaries, finishing second to Mitt Romney.Santorum was elected to the United States Senate from Pennsylvania in 1994. He served two terms until losing his 2006 reelection bid. A Roman Catholic, Santorum is a social conservative who opposes abortion and same-sex marriage and embraced a cultural warrior image during his Senate tenure. While serving as a senator, Santorum authored the Santorum Amendment, which would have promoted the teaching of intelligent design. He was a leading sponsor of the 2003 federal law known as the Partial-Birth Abortion Ban Act.In the years following his departure from the Senate, Santorum has worked as a consultant, private practice lawyer, and news contributor. He ran for the Republican nomination in the 2012 U.S. presidential election. Before suspending his campaign on April 10, 2012, Santorum exceeded expectations by winning 11 primaries and caucuses and receiving nearly four million votes, making him the runner-up to eventual nominee Mitt Romney. Santorum ran for president again in 2016, but ended his campaign in February 2016 after a poor showing in the Iowa caucuses. In January 2017, he became a CNN senior political commentator. However, he was terminated from his contract with CNN in May 2021 due to comments he made about Native Americans a few weeks prior which were deemed "dismissive".
Thought 2: The paragraph mentions Rick Santorum, so I need to search "It Takes a Family by Rick Santorum".
Action 2: Search["It Takes a Family by Rick Santorum"]
Observation 2: It Takes a Family is a 2005 book by then Pennsylvania Senator Rick Santorum. The title is a response to the 1996 book It Takes a Village by then-First Lady Hillary Clinton. In the book, Santorum states that the family structure is necessary. He argues that liberal social policies have devastated the family structure and that prevailing socially liberal attitudes have systematically devalued families across the board. He argues that the government should take a proactive role in promoting strong families.
Thought 3: It Takes a Family was published by Rick Santorum, so the answer is Rick Santorum.
Action 3: Finish[Rick Santorum]
Observation 3: Answer is INCORRECT</p>
<p>An example of the AlfWorld actor language model prompt is shown below.</p>
<div class="codehilite"><pre><span></span><code>You are in the middle of a room. Looking quickly around you, you see a cabinet 10, a cabinet 9, a
cabinet 8, a cabinet 7, a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a
cabinet 1, a coffeemachine 1, a countertop 2, a countertop 1, a diningtable 2, a diningtable 1, a
drawer 3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a sinkbasin 1, a
stoveburner 4, a stoveburner 3, a stoveburner 2, a stoveburner 1, and a toaster 1.
</code></pre></div>

<p>[Few-Shot Demonstrations]</p>
<h1>Your memory for the task below:</h1>
<p>Trial 0: I will try to find a different task to complete. I will look for other objects in the room and try to complete tasks related to those objects. For example, I could try to find a book to read or a glass to drink from. I will also try to use the objects I have found in the room in different ways to see if that helps me complete a task.</p>
<p>Your task is to: heat some mug and put it in coffeemachine.</p>
<blockquote>
<p>think: To solve the task, I need to find and take a mug, then heat it with stoveburner, then put it in coffeemachine.
Ok.
$&gt;$ look
You are in the middle of a room. Looking quickly around you, you see nothing.
think: First I need to find a mug. A mug is more likely to appear in countertop (1-2),
diningtable (1-2), drawer (1-3), fridge (1), sinkbasin (1), stoveburner (1-4), toaster (1). I can check one by one, starting with countertop 1.
Ok.
$&gt;$ look
You are in the middle of a room. Looking quickly around you, you see nothing.
$&gt;$ go to countertop 1
On the countertop 1, you see a butterknife 1, a cup 1, a fork 2, and a mug 1.
think: Now I find a mug (1). Next, I need to take it.
Ok.
$&gt;$ take mug 1 from countertop 1
You pick up the mug 1 from the countertop 1.
think: Now I take a mug (1). Next, I need to heat it with stoveburner (1-4).
Ok.
$&gt;$ go to stoveburner 1
On the stoveburner 1, you see a pan 2.
$&gt;$ examine stoveburner 1
On the stoveburner 1, you see a pan 2.
$&gt;$ examine stoveburner 1
On the stoveburner 1, you see a pan 2.
$&gt;$ examine stoveburner 1
On the stoveburner 1, you see a pan 2.
$&gt;$ examine stoveburner 1
On the stoveburner 1, you see a pan 2.
STATUS: FAIL
New plan: I was stuck in a loop in which I continually examined stoveburner 1 instead of heating mug 1 with stoveburner 1. I should have taken mug 1 from countertop 1, then heated it with stoveburner 1, then put it in coffeemachine 1. It did not help to execute two identical actions in a row. I will try to execute a different action if I am stuck in a loop again.</p>
</blockquote>
<p>An example of the WebShop actor language model prompt is shown below.</p>
<div class="codehilite"><pre><span></span><code>Webshop
[Few-Shot Demonstrations]
</code></pre></div>

<p>Instruction:
i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 50.00 dollars
[Search]</p>
<h1>Your memory for the task below:</h1>
<p>Trial 0: In this attempt, I was unsuccessful. I accidentally bought a product that was more than my budget of $\$ 50$. The initial search results were not good.</p>
<p>Action: search[3 ounce bright citrus deodorant sensitive skin]
Observation:
[Back to Search]
Page 1 (Total results: 50)
[Next &gt;]
[B078GWRC1J]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce
$\$ 10.99$
[B078GTKVXY]
Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce
$\$ 10.99$
[B08KBVJ4XN]
Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar \&amp; Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)
$\$ 15.95$</p>
<p>Action: think[B078GWRC1J and B078GTKVXY are bright citrus deodorant less then 50 dollars. I can check B078GWRC1J first.]
Observation: OK.
Action: click[B078GWRC1J]
Observation:
[Back to Search]
[&lt; Prev]
scent [assorted scents][bright citrus][calming lavender][ginger fresh][simply non-scents]
size [travel set (4-pack)][3 ounce (pack of 1)][3-ounce (2-pack)]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and
Breastfeeding, Contains Organic Calendula 3-Ounce
Price: $\$ 10.99$
Rating: N.A.
[Description]
[Features]
[Reviews]
[Buy Now]</p>
<p>Action: think[For 3 ounce bottle of bright citrus deodorant for sensitive skin, the item has options 'bright citrus' and ' 3 ounce (pack of 1)' and seems good to buy.]
Observation: OK.
Action: click[bright citrus]
Observation: You have clicked bright citrus.
Action: click[3 ounce (pack of 1)]
Observation: You have clicked 3 ounce (pack of 1).
Action: click[Buy Now]</p>
<h2>E. 2 Retrospective Prompt Engineering</h2>
<p>An example of the HotPotQA retrospective instruction prompt is shown below.</p>
<p>You are an advanced reasoning agent that can improve based on self reflection. You will be given a previous reasoning trial in which you were given access to an Docstore API environment and a question to answer. You were unsuccessful in answering the question either because you guessed the wrong answer with Finish[<answer>], or you used up your set number of reasoning steps. In a few sentences, Diagnose a possible reason for failure and devise a new, concise, high level plan that aims to mitigate the same failure. Use complete sentences.</p>
<p>Here are some examples.
[Few-Shot Demonstrations]</p>
<h1>Previous trial:</h1>
<p>Question: The 2016 Washington State Cougars were led by the coach who previously helmed which other team?</p>
<p>Thought: I need to search Washington State Cougars and find the coach of the 2016 team, then find which other team the coach previously helmed.
Action: Search[Washington State Cougars]
Observation: The Washington State Cougars (known informally as the Cougs) are the athletic teams that represent Washington State University. Located in Pullman, Washington, WSU is a member of the Pac-12 Conference in NCAA Division I. The athletic program comprises ten women's sports and six men's intercollegiate sports, and also offers various intramural sports.</p>
<p>Thought: The paragraph does not mention the coach of the 2016 team. I need to look up "coach" to find the coach of the 2016 team.
Action: Lookup[coach]
Observation: (Result 2/5) During the 2001-03 seasons, the Cougar football teams were distinguished by three ten-win seasons, three top ten poll rankings, and appearances in the Sun, Rose, and Holiday Bowls. The Cougars shared the Pac-10 title in 2002. Alumnus Paul Mulff, WSU's 31st head coach, was fired in late November 2011, after compiling a 9-40 (.184) record in four seasons. Mike Leach coached the team from 2012 to 2019, guiding them to six bowl games. Leach left to coach Mississippi State in 2020 and was replaced by Hawaii head coach Nick Rolovich. Rolovitch was fired in October 2021 for not complying with the state's COVID-19 vaccine mandate. Defensive coordinator Jake Dickert was named the interim head coach.</p>
<p>Thought: The 2016 Washington State Cougars were led by Mike Leach. I need to search Mike Leach and find which other team he previously helmed.
Action: Search[Mike Leach]
Observation: Michael Charles Leach (March 9, 1961 - December 12, 2022) was an American college football coach who primarily coached at the NCAA Division I FBS level. He was a two-time national coach of the year, three-time conference coach of the year and the mastermind behind the NCAA record-setting air raid offense. He was the head coach at Texas Tech University from 2000 to 2009, where he became the winningest coach in school history. After Texas Tech, he coached at Washington State University from 2012 to 2019, where he recorded the third-most wins of any coach in school history. He then coached at Mississippi State from 2020 until his death in 2022. Leach was known for directing offenses using lots of passing to several receivers, in a spread system known as the air raid, which Leach developed with Hal Mumme when Mumme was head coach and Leach was offensive coordinator at Iowa Wesleyan, Valdosta State, and Kentucky in the 1990s. Leach's offenses with Mumme, and later as a head coach himself, have broken numerous school and NCAA records. On eighteen occasions, Leach led his unranked team to victory over a team ranked in the AP poll, which is the most in the AP era.</p>
<p>Thought: Mike Leach previously helmed Texas Tech University and Kentucky. So the answer is Texas Tech University and Kentucky.
Action: Finish[Texas Tech University and Kentucky]
Observation: Answer is INCORRECT</p>
<h2>Reflection:</h2>
<p>An example of the AlfWorld retrospective instruction prompt is shown below.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">You</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">given</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">history</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">past</span><span class="w"> </span><span class="nv">experience</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">which</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">were</span><span class="w"> </span><span class="nv">placed</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">an</span><span class="w"> </span><span class="nv">environment</span><span class="w"> </span><span class="nv">and</span>
<span class="nv">given</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">task</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">complete</span>.<span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">were</span><span class="w"> </span><span class="nv">unsuccessful</span><span class="w"> </span><span class="nv">in</span><span class="w"> </span><span class="nv">completing</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">task</span>.<span class="w"> </span><span class="k">Do</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">summarize</span><span class="w"> </span><span class="nv">your</span>
<span class="nv">environment</span>,<span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">rather</span><span class="w"> </span><span class="nv">think</span><span class="w"> </span><span class="nv">about</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">strategy</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">path</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">took</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">attempt</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">complete</span><span class="w"> </span><span class="nv">the</span>
<span class="nv">task</span>.<span class="w"> </span><span class="nv">Devise</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">concise</span>,<span class="w"> </span><span class="nv">new</span><span class="w"> </span><span class="nv">plan</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">action</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">accounts</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">mistake</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">reference</span><span class="w"> </span><span class="nv">to</span>
<span class="nv">specific</span><span class="w"> </span><span class="nv">actions</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">taken</span>.<span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="nv">example</span>,<span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">tried</span><span class="w"> </span><span class="nv">A</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">B</span><span class="w"> </span><span class="nv">but</span><span class="w"> </span><span class="nv">forgot</span><span class="w"> </span><span class="nv">C</span>,<span class="w"> </span><span class="k">then</span>
<span class="nv">devise</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">plan</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">achieve</span><span class="w"> </span><span class="nv">C</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">environment</span><span class="o">-</span><span class="nv">specific</span><span class="w"> </span><span class="nv">actions</span>.<span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">will</span><span class="w"> </span><span class="nv">need</span><span class="w"> </span><span class="nv">this</span><span class="w"> </span><span class="nv">later</span><span class="w"> </span><span class="nv">when</span><span class="w"> </span><span class="nv">you</span>
<span class="nv">are</span><span class="w"> </span><span class="nv">solving</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">same</span><span class="w"> </span><span class="nv">task</span>.<span class="w"> </span><span class="nv">Give</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">plan</span><span class="w"> </span><span class="nv">after</span><span class="w"> </span><span class="s2">&quot;Plan&quot;</span>.<span class="w"> </span><span class="nv">Here</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">two</span><span class="w"> </span><span class="nv">examples</span>:
</code></pre></div>

<p>[Few-Shot Demonstrations]</p>
<div class="codehilite"><pre><span></span><code><span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="err">:</span><span class="w"> </span><span class="n">heat</span><span class="w"> </span><span class="ow">some</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">coffeemachine</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="k">To</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="p">,</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">heat</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">stoveburner</span><span class="p">,</span><span class="w"> </span><span class="k">then</span>
<span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">coffeemachine</span><span class="p">.</span>
<span class="n">Ok</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">look</span>
<span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">middle</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="p">.</span><span class="w"> </span><span class="n">Looking</span><span class="w"> </span><span class="n">quickly</span><span class="w"> </span><span class="n">around</span><span class="w"> </span><span class="n">you</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="k">First</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="p">.</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">more</span><span class="w"> </span><span class="n">likely</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">appear</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span>
<span class="n">diningtable</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="n">fridge</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">sinkbasin</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">),</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">4</span><span class="p">),</span><span class="w"> </span><span class="n">toaster</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">can</span>
<span class="k">check</span><span class="w"> </span><span class="n">one</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="n">one</span><span class="p">,</span><span class="w"> </span><span class="n">starting</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="mf">1.</span>
<span class="n">Ok</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">look</span>
<span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">middle</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="p">.</span><span class="w"> </span><span class="n">Looking</span><span class="w"> </span><span class="n">quickly</span><span class="w"> </span><span class="n">around</span><span class="w"> </span><span class="n">you</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">butterknife</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cup</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">fork</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mf">1.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="n">Ok</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="mi">1</span>
<span class="n">You</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">countertop</span><span class="w"> </span><span class="mf">1.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">heat</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">with</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mi">4</span><span class="p">).</span>
<span class="n">Ok</span><span class="p">.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pan</span><span class="w"> </span><span class="mf">2.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pan</span><span class="w"> </span><span class="mf">2.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pan</span><span class="w"> </span><span class="mf">2.</span>
<span class="o">&gt;</span><span class="w"> </span><span class="n">examine</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pan</span><span class="w"> </span><span class="mf">2.</span>
<span class="n">examine</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span>
<span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">stoveburner</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pan</span><span class="w"> </span><span class="mf">2.</span>
<span class="nl">STATUS</span><span class="p">:</span><span class="w"> </span><span class="n">FAIL</span>
</code></pre></div>

<p>New plan: I was stuck in a loop in which I continually examined stoveburner 1 instead of heating mug 1 with stoveburner 1. I should have taken mug 1 from countertop 1, then heated it with stoveburner 1, then put it in coffeemachine 1. It did not help to execute two identical actions in a row. I will try to execute a different action if I am stuck in a loop again.</p>
<p>An example of the WebShop retrospective instruction prompt is shown below.</p>
<div class="codehilite"><pre><span></span><code><span class="n">You</span><span class="w"> </span><span class="n">will</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">history</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">past</span><span class="w"> </span><span class="n">experience</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">placed</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">environment</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">task</span>
<span class="n">to</span><span class="w"> </span><span class="n">complete</span><span class="p">.</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">were</span><span class="w"> </span><span class="n">unsuccessful</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">completing</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="w"> </span><span class="n">Do</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">summarize</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">environment</span><span class="p">,</span><span class="w"> </span><span class="n">but</span><span class="w"> </span><span class="n">rather</span><span class="w"> </span><span class="n">think</span>
<span class="n">about</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">strategy</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">path</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">took</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">attempt</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">complete</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">.</span><span class="w"> </span><span class="n">Devise</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">concise</span><span class="p">,</span><span class="w"> </span><span class="n">new</span><span class="w"> </span><span class="n">plan</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">that</span>
<span class="n">accounts</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">mistake</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">reference</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">specific</span><span class="w"> </span><span class="n">actions</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">taken</span><span class="p">.</span><span class="w"> </span><span class="n">There</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">examples</span>
<span class="n">below</span><span class="p">.</span>
<span class="p">[</span><span class="n">Few</span><span class="o">-</span><span class="n">Shot</span><span class="w"> </span><span class="n">Demonstrations</span><span class="p">]</span>
<span class="nl">Instruction</span><span class="p">:</span>
<span class="n">i</span><span class="w"> </span><span class="n">am</span><span class="w"> </span><span class="n">looking</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">dairy</span><span class="w"> </span><span class="n">free</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">apple</span><span class="w"> </span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">chips</span><span class="p">,</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">price</span><span class="w"> </span><span class="n">lower</span><span class="w"> </span><span class="n">than</span><span class="w"> </span><span class="mf">30.00</span><span class="w"> </span><span class="n">dollars</span>
<span class="p">[</span><span class="n">Search</span><span class="p">]</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">search</span><span class="p">[</span><span class="n">dairy</span><span class="w"> </span><span class="n">free</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">apple</span><span class="w"> </span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">chips</span><span class="p">]</span>
<span class="nl">Observation</span><span class="p">:</span>
<span class="p">[</span><span class="n">Back</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Search</span><span class="p">]</span>
<span class="n">Page</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">(</span><span class="n">Total</span><span class="w"> </span><span class="n">results</span><span class="o">:</span><span class="w"> </span><span class="mi">50</span><span class="p">)</span>
<span class="p">[</span><span class="n">Next</span><span class="w"> </span><span class="o">&gt;</span><span class="p">]</span>
<span class="p">[</span><span class="n">B07HRFJWP8</span><span class="p">]</span>
<span class="n">Enjoy</span><span class="w"> </span><span class="n">Life</span><span class="w"> </span><span class="n">Foods</span><span class="w"> </span><span class="n">Soft</span><span class="w"> </span><span class="n">Baked</span><span class="w"> </span><span class="n">Ovals</span><span class="p">,</span><span class="w"> </span><span class="n">Breakfast</span><span class="w"> </span><span class="n">Bars</span><span class="p">,</span><span class="w"> </span><span class="n">Nut</span><span class="w"> </span><span class="n">Free</span><span class="w"> </span><span class="n">Bars</span><span class="p">,</span><span class="w"> </span><span class="n">Soy</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Dairy</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Non</span><span class="w"> </span><span class="n">GMO</span><span class="p">,</span><span class="w"> </span><span class="n">Gluten</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span>
<span class="n">Vegan</span><span class="p">,</span><span class="w"> </span><span class="n">Variety</span><span class="w"> </span><span class="n">Pack</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="n">Boxes</span><span class="w"> </span><span class="p">(</span><span class="mi">20</span><span class="w"> </span><span class="n">Bars</span><span class="w"> </span><span class="n">Total</span><span class="p">)</span>
<span class="n">$100</span><span class="mf">.0</span>
<span class="p">[</span><span class="n">B01RMNY5PG</span><span class="p">]</span>
<span class="n">Enjoy</span><span class="w"> </span><span class="n">Life</span><span class="w"> </span><span class="n">Soft</span><span class="w"> </span><span class="n">Baked</span><span class="w"> </span><span class="n">Chewy</span><span class="w"> </span><span class="n">Bars</span><span class="p">,</span><span class="w"> </span><span class="n">Variety</span><span class="w"> </span><span class="n">Pack</span><span class="p">,</span><span class="w"> </span><span class="n">Nut</span><span class="w"> </span><span class="n">Free</span><span class="w"> </span><span class="n">Bars</span><span class="p">,</span><span class="w"> </span><span class="n">Soy</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Dairy</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Gluten</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="n">Boxes</span><span class="w"> </span><span class="p">(</span><span class="mi">30</span>
<span class="n">Total</span><span class="w"> </span><span class="n">Bars</span><span class="p">)</span>
<span class="n">$21</span><span class="mf">.49</span>
<span class="p">[</span><span class="n">B008D2X8C4</span><span class="p">]</span>
<span class="n">Enjoy</span><span class="w"> </span><span class="n">Life</span><span class="w"> </span><span class="n">Lentil</span><span class="w"> </span><span class="n">Chips</span><span class="w"> </span><span class="n">Variety</span><span class="w"> </span><span class="n">Pack</span><span class="p">,</span><span class="w"> </span><span class="n">Dairy</span><span class="w"> </span><span class="n">Free</span><span class="w"> </span><span class="n">Chips</span><span class="p">,</span><span class="w"> </span><span class="n">Soy</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Nut</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="n">Non</span><span class="w"> </span><span class="n">GMO</span><span class="p">,</span><span class="w"> </span><span class="n">Vegan</span><span class="p">,</span><span class="w"> </span><span class="n">Gluten</span><span class="w"> </span><span class="n">Free</span><span class="p">,</span><span class="w"> </span><span class="mi">24</span>
<span class="n">Bags</span><span class="w"> </span><span class="p">(</span><span class="mf">0.8</span><span class="w"> </span><span class="n">oz</span><span class="p">)</span>
<span class="n">$100</span><span class="mf">.0</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">think</span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">two</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">not</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">want</span><span class="p">.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">third</span><span class="w"> </span><span class="n">item</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dairy</span><span class="w"> </span><span class="n">free</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">apple</span><span class="w"> </span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span>
<span class="n">of</span><span class="w"> </span><span class="n">chips</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">want</span><span class="p">.]</span>
<span class="nl">Observation</span><span class="p">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">click</span><span class="p">[</span><span class="n">B008D2X8C4</span><span class="p">]</span>
<span class="nl">Observation</span><span class="p">:</span>
<span class="p">[</span><span class="n">Back</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">Search</span><span class="p">]</span>
<span class="p">[</span><span class="o">=</span><span class="w"> </span><span class="n">Prev</span><span class="p">]</span>
<span class="n">flavor</span><span class="w"> </span><span class="n">name</span><span class="w"> </span><span class="p">[</span><span class="n">dill</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">sour</span><span class="w"> </span><span class="n">cream</span><span class="p">][</span><span class="n">garlic</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="n">parmesan</span><span class="p">][</span><span class="n">light</span><span class="w"> </span><span class="n">sea</span><span class="w"> </span><span class="n">salt</span><span class="p">][</span><span class="n">margherita</span><span class="w"> </span><span class="n">pizza</span><span class="p">][</span><span class="n">thai</span><span class="w"> </span><span class="n">chili</span><span class="w"> </span><span class="n">lime</span><span class="p">][</span><span class="n">variety</span>
<span class="n">pack</span><span class="p">]</span>
<span class="n">size</span><span class="w"> </span><span class="p">[</span><span class="mf">0.8</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="p">(</span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">24</span><span class="p">)][</span><span class="mi">4</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="p">(</span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">12</span><span class="p">)]</span>
<span class="nl">Price</span><span class="p">:</span><span class="w"> </span><span class="n">$100</span><span class="mf">.0</span>
<span class="nl">Rating</span><span class="p">:</span><span class="w"> </span><span class="n">H</span><span class="p">.</span><span class="n">A</span><span class="p">.</span>
<span class="p">[</span><span class="n">Description</span><span class="p">]</span>
<span class="p">[</span><span class="n">Features</span><span class="p">]</span>
<span class="p">[</span><span class="n">Reviews</span><span class="p">]</span>
<span class="p">[</span><span class="n">Buy</span><span class="w"> </span><span class="n">Now</span><span class="p">]</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">think</span><span class="p">[</span><span class="n">The</span><span class="w"> </span><span class="n">item</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="n">options</span><span class="w"> </span><span class="err">&#39;</span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span><span class="err">&#39;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="err">&#39;</span><span class="mf">0.8</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="p">(</span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">24</span><span class="p">)</span><span class="err">&#39;</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">seems</span><span class="w"> </span><span class="n">good</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">buy</span><span class="p">.]</span>
<span class="nl">Observation</span><span class="p">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">click</span><span class="p">[</span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span><span class="p">]</span>
<span class="nl">Observation</span><span class="p">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">clicked</span><span class="w"> </span><span class="n">variety</span><span class="w"> </span><span class="n">pack</span><span class="p">.</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">click</span><span class="p">[</span><span class="mf">0.8</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="p">(</span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">24</span><span class="p">)]</span>
<span class="nl">Observation</span><span class="p">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">clicked</span><span class="w"> </span><span class="mf">0.8</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="p">(</span><span class="n">pack</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="mi">24</span><span class="p">).</span>
<span class="nl">Action</span><span class="p">:</span><span class="w"> </span><span class="n">click</span><span class="p">[</span><span class="n">Buy</span><span class="w"> </span><span class="n">Now</span><span class="p">]</span>
<span class="nl">STATUS</span><span class="p">:</span><span class="w"> </span><span class="n">FAIL</span>
</code></pre></div>

<p>Next plan: In this attempt, I was unsuccessful. I accidentally bought a product that was $\$ 100$, which is more than my budget of $\$ 30$. Either way, the initial search results were not good. Next time, I will do search["variety pack of chips"] and then check if the results meet the dairy free and the $\$ 30$ budget constraints. I will continue to refine my searches so that I can find more products.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{\dagger}$ Corresponding Authors
${ }^{\dagger}$ Website for Retroformer \&amp; demos: https://Retroformer.github.io/
${ }^{\ddagger}$ Code: https://github.com/weirayao/Retroformer&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
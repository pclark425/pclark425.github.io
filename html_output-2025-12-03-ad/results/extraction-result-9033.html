<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9033 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9033</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9033</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-276482564</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.14359v4.pdf" target="_blank">Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests</a></p>
                <p><strong>Paper Abstract:</strong> We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9033.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9033.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluated models (study)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phi-4-Mini, OLMo-2-1124, Falcon3, Qwen2.5, Llama-3 (models evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Set of open-weight LLMs (3.8B–72B parameters) that the paper evaluates across standard benchmarks, interactive games, and cognitive-test-inspired datasets to triangulate model abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-weight transformer LLMs accessed via HuggingFace/transformers; series descriptions in Appendix A (Phi-4-Mini noted as strong on reasoning; OLMo-2-1124 fine-tuned with DPO; Qwen2.5 multilingual; Llama-3 instruction-tuned; Falcon3 multi-lingual with long context).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B, 7B, 10B, 13B, 32B, 70B, 72B (various models in study; full range 3.8B–72B)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Multiple cognitive-test-inspired datasets (see individual entries)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>The paper compiled a suite of existing datasets inspired by human cognitive tests (working memory, Theory of Mind, planning, logical/causal/common-sense reasoning, pragmatics, etc.) and evaluated the models on them to probe component cognitive abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Varied by test; overall qualitative findings reported (see per-test entries). All models scored >70% on IFEval (used as an instruction-following check).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baselines reported in this paper for most cognitive tests; comparisons are correlational across benchmarks and games rather than head-to-head with human norms.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Models run via transformers default chat templates; most tasks zero-shot (exceptions: MMLU 5-shot, BBH 3-shot, NATURAL PLAN 5-shot); generation temperature typically 0 except specific tasks (WM temp=1, LLM-Cognitive-Flexibility temp=0.7). IFEval used to screen for instruction-following (>70%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not report human baseline scores for the cognitive datasets; many cognitive datasets are recent and may not cleanly map to intended cognitive constructs for LLMs; numerical per-model per-test scores are in Appendix I (Table 3) but not reported in main text; no chain-of-thought or CoT elicitation used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9033.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IFEval (instruction-following check)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>IFEval (instruction-following evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset used in the paper as an indicator that models possess instruction-following capabilities necessary for interactive games and cognitive tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124 (7B/13B); Falcon3 10B; Qwen2.5 (7B/32B/72B); Llama-3 (8B/70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as study models; IFEval used as a filtering/qualification test rather than a cognitive construct assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>IFEval</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Instruction-following evaluation (prompt-level and instruction-level strict accuracy averaged as in Open Leaderboard practices).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>All evaluated models scored above 70% on IFEval (reported in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not compared to humans; IFEval used as a pass/fail indication for ability to follow task descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluation followed LM Evaluation Harness / Open Leaderboard 2 conventions; averaged prompt-level and instruction-level strict accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Score threshold used as heuristic to include models in interactive/cognitive evaluations; not a cognitive test per se.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9033.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NATURAL PLAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NATURAL PLAN (planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A planning benchmark included among the cognitive-test-inspired datasets; the task requires producing structured plans and is sensitive to strict output formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same evaluated LLMs; they performed very poorly zero-shot on NATURAL PLAN due to strict-format parsing requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>NATURAL PLAN</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A dataset that tests planning ability by requiring models to generate plans in a strict format (dates, items parsed by rules).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Close to 0% in zero-shot according to authors; the paper therefore used the 5-shot version provided by the benchmark authors for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs performed extremely poorly zero-shot; no human baseline reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Zero-shot performance was near-zero; authors ran the 5-shot protocol for NATURAL PLAN as per benchmark authors because the task requires strict-format outputs for parsing; parsing/rule-based evaluation reused from original work.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Poor zero-shot performance may reflect sensitivity to output format and parsing rather than pure absence of planning competence; the paper notes heavy resource/time cost for some NATURAL PLAN subtasks (meeting subtask not considered).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9033.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Working Memory (WM Verbal N-3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Working Memory — Verbal N-3 subtask (Gong et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A working memory subtask (Verbal N-3) used to probe core working-memory capacity by asking models to recall/transform verbal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same evaluated models; WM test used to probe core executive function (working memory) relevant for interactive games.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Working Memory (Verbal N-3)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Verbal working memory task (N-3) with 50 trials; assesses ability to maintain/manipulate recent items in memory.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Accuracy computed across the 50 trials for each model (paper does not report numerical accuracies in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline provided; correlations in the paper indicate working memory test performance correlates with better performance on interactive games (working memory 'beneficial for games').</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Only the Verbal N-3 subtask was considered; accuracy reported across the 50 trials defined in original work; evaluated with temperature=1 and averaged over multiple trials.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numeric human comparisons; task adaptations and generation randomness (temperature=1) were used and averaged to mitigate stochasticity; WM benchmark scope limited to that specific subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9033.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimpleToM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimpleToM (Theory of Mind / Attribution & Judgment dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset containing concise stories with questions probing attribution, judgment, and mental-state inference used to assess Theory of Mind and related social cognition abilities in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same evaluated LLMs; SimpleToM used to derive separate scores for Attribution & Judgment (aggregated) and mental-state inference (ToM).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SimpleToM (subtasks: Attribution & Judgment; Theory of Mind)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Stories with questions about behavior prediction, normative judgment, and awareness/mental states (size ~3450 data points); used to probe ToM and attribution/judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Models were evaluated on subtasks; the paper reports ToM correlates better with interactive games than static benchmarks, but does not provide numeric per-model accuracies in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline provided; comparative analysis is correlational (ToM test scores correlate more strongly with performance on interactive games).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Subtasks aggregated per cognitive-ability grouping: behavior + judgment under Attribution & Judgment; mental state subtask under ToM; evaluated as multiple-choice via likelihood of correct answer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors caution that currently available ToM datasets are recent and may not fully capture intended cognitive constructs for LLMs; no human norm scores reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9033.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wordle (base and variants)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Wordle (base), Wordle (Clue), Wordle (Critic) — clembench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Word-guessing interactive games (base and variants) from the clembench suite used to probe sequential hypothesis-updating, use of feedback, and social/critic interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLMs evaluated in clembench Wordle variants; gameplay requires interpreting letter-level feedback over multiple turns.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Wordle (base) and Wordle variants (Clue, Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Wordle base: iterative 5-letter word guessing with letter-position feedback; variants provide extra clues (Clue) or an external critic that evaluates guesses (Critic).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Classic Wordle: 'excessively low performance' across models (correlation analyses omitted for base due to low performance). Variants: models perform better with added clues/critic but numeric scores not reported in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs are well below human-level play on classic Wordle (qualitative); external critic/clue variants stimulate use of social capabilities and improve performance relative to the base variant.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Wordle episodes run via clembench; models receive letter-level feedback each turn; correlation with cognitive abilities measured only for variants where performance allowed it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Classic Wordle was so challenging for models that correlation with cognitive tests was not measured; improvements in variants may reflect task design (extra information) rather than emergence of underlying cognitive capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9033.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Taboo (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Taboo (descriptive clues without taboo words) — clembench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dialogue game where a describer must get a guesser to identify a target word while avoiding specified 'taboo' words; probes commonsense, pragmatics, working memory, and ToM in gameplay.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-70B-Instruct (example) and other evaluated LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>In Appendix D the paper shows an episode of Llama-3.1-70B-Instruct playing Taboo successfully in at least one example; task requires generating creative descriptions avoiding taboo terms and tracking dialogue history.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Example shown: 70B (Llama-3.1-70B-Instruct); other models in study varied 3.8B–72B</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Taboo (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Describer gives clues without using forbidden words; guesser must identify the target; assesses commonsense, pragmatic adaptation to the partner, working memory, and logical inference.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: some models (e.g., Llama-3 70B) can play and reach correct guesses in episodes shown; overall correlation analysis shows Taboo correlates with working memory, commonsense, logical reasoning, pragmatics, and Theory of Mind.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No quantitative human-baseline comparison provided; authors treat correlations with cognitive tests as indicative of required abilities rather than direct human-level claims.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Examples from clembench transcripts presented; correlation of Taboo performance with cognitive-ability scores reported in Figure 8 (Kendall's τ), but numerical τ values not reproduced in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors note some connections between cognitive tests and gameplay are speculative; role of pragmatics is less clear and requires further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9033.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference Game</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference Game (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive referential communication task where a describer must uniquely identify one of several similar textual grids to a guesser; used to probe planning and concise referring-expression generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models must produce referential expressions that disambiguate target grid from distractors; success depends on planning and compressing information into discriminative descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Reference Game (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Given three textual grids (one target), describer formulates a description enabling the guesser to select the correct grid; measures planning, working memory, and logical resolution of constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: models vary in success; paper reports Reference Game performance correlates with Planning more than other cognitive capabilities (no numeric accuracy reported in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline given; correlation indicates planning ability (as measured by the NATURAL PLAN benchmark) is predictive of Reference Game success.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Evaluated via clembench; descriptive analysis connects approaches (cell-by-cell vs. constraint-resolution) to planning scores; correlation computed via Kendall's τ.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>High difficulty for many models; authors note mapping between planning tests and referential strategy is interpretative and tentative.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9033.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9033.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Private/Shared (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Private/Shared (conversation + probing about shared/privileged knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational game where a customer and service agent converse while a third agent probes the customer's belief about what the service agent already knows; probes working memory and conversational tracking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3 (example episode with Llama-3.3-70B-Instruct shown)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interactive multi-turn conversation requiring tracking of shared vs. private information across parallel dialogues; resource-intensive and conversationally rich.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3.8B–72B (various); example Llama-3.3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Private/Shared (clembench)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Customer-service simulation with parallel conversations; after each turn a probe checks what the customer believes the service agent knows (private vs shared state).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative: Private/Shared correlates strongly with working memory (higher than other tasks); authors present an example transcript with Llama-3.3-70B-Instruct where probing turns are answered correctly in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No human baseline reported; authors note surprising low correlation with Theory of Mind and emphasize need for further investigation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Interactions mediated by a Game Master in clembench; example transcripts reproduced (Figure 17); performance metric for clembench computed from quality score scaled by games played.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Authors highlight the surprising low ToM correlation and suggest further work; Private/Shared episodes are resource- and time-intensive to evaluate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SimpleToM: Exposing the gap between explicit ToM inference and implicit ToM application in LLMs <em>(Rating: 2)</em></li>
                <li>Working memory capacity of ChatGPT: An empirical study <em>(Rating: 2)</em></li>
                <li>CLADDER: Assessing causal reasoning in language models <em>(Rating: 2)</em></li>
                <li>Are we done with MMLU? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9033",
    "paper_id": "paper-276482564",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "Evaluated models (study)",
            "name_full": "Phi-4-Mini, OLMo-2-1124, Falcon3, Qwen2.5, Llama-3 (models evaluated in this paper)",
            "brief_description": "Set of open-weight LLMs (3.8B–72B parameters) that the paper evaluates across standard benchmarks, interactive games, and cognitive-test-inspired datasets to triangulate model abilities.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Open-weight transformer LLMs accessed via HuggingFace/transformers; series descriptions in Appendix A (Phi-4-Mini noted as strong on reasoning; OLMo-2-1124 fine-tuned with DPO; Qwen2.5 multilingual; Llama-3 instruction-tuned; Falcon3 multi-lingual with long context).",
            "model_size": "3.8B, 7B, 10B, 13B, 32B, 70B, 72B (various models in study; full range 3.8B–72B)",
            "test_battery_name": "Multiple cognitive-test-inspired datasets (see individual entries)",
            "test_description": "The paper compiled a suite of existing datasets inspired by human cognitive tests (working memory, Theory of Mind, planning, logical/causal/common-sense reasoning, pragmatics, etc.) and evaluated the models on them to probe component cognitive abilities.",
            "llm_performance": "Varied by test; overall qualitative findings reported (see per-test entries). All models scored &gt;70% on IFEval (used as an instruction-following check).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baselines reported in this paper for most cognitive tests; comparisons are correlational across benchmarks and games rather than head-to-head with human norms.",
            "experimental_details": "Models run via transformers default chat templates; most tasks zero-shot (exceptions: MMLU 5-shot, BBH 3-shot, NATURAL PLAN 5-shot); generation temperature typically 0 except specific tasks (WM temp=1, LLM-Cognitive-Flexibility temp=0.7). IFEval used to screen for instruction-following (&gt;70%).",
            "limitations_or_caveats": "Paper does not report human baseline scores for the cognitive datasets; many cognitive datasets are recent and may not cleanly map to intended cognitive constructs for LLMs; numerical per-model per-test scores are in Appendix I (Table 3) but not reported in main text; no chain-of-thought or CoT elicitation used.",
            "uuid": "e9033.0",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "IFEval (instruction-following check)",
            "name_full": "IFEval (instruction-following evaluation)",
            "brief_description": "A dataset used in the paper as an indicator that models possess instruction-following capabilities necessary for interactive games and cognitive tests.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124 (7B/13B); Falcon3 10B; Qwen2.5 (7B/32B/72B); Llama-3 (8B/70B)",
            "model_description": "Same as study models; IFEval used as a filtering/qualification test rather than a cognitive construct assessment.",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "IFEval",
            "test_description": "Instruction-following evaluation (prompt-level and instruction-level strict accuracy averaged as in Open Leaderboard practices).",
            "llm_performance": "All evaluated models scored above 70% on IFEval (reported in paper).",
            "human_baseline_performance": null,
            "performance_comparison": "Not compared to humans; IFEval used as a pass/fail indication for ability to follow task descriptions.",
            "experimental_details": "Evaluation followed LM Evaluation Harness / Open Leaderboard 2 conventions; averaged prompt-level and instruction-level strict accuracy.",
            "limitations_or_caveats": "Score threshold used as heuristic to include models in interactive/cognitive evaluations; not a cognitive test per se.",
            "uuid": "e9033.1",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "NATURAL PLAN",
            "name_full": "NATURAL PLAN (planning benchmark)",
            "brief_description": "A planning benchmark included among the cognitive-test-inspired datasets; the task requires producing structured plans and is sensitive to strict output formatting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Same evaluated LLMs; they performed very poorly zero-shot on NATURAL PLAN due to strict-format parsing requirements.",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "NATURAL PLAN",
            "test_description": "A dataset that tests planning ability by requiring models to generate plans in a strict format (dates, items parsed by rules).",
            "llm_performance": "Close to 0% in zero-shot according to authors; the paper therefore used the 5-shot version provided by the benchmark authors for evaluation.",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs performed extremely poorly zero-shot; no human baseline reported in the paper.",
            "experimental_details": "Zero-shot performance was near-zero; authors ran the 5-shot protocol for NATURAL PLAN as per benchmark authors because the task requires strict-format outputs for parsing; parsing/rule-based evaluation reused from original work.",
            "limitations_or_caveats": "Poor zero-shot performance may reflect sensitivity to output format and parsing rather than pure absence of planning competence; the paper notes heavy resource/time cost for some NATURAL PLAN subtasks (meeting subtask not considered).",
            "uuid": "e9033.2",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Working Memory (WM Verbal N-3)",
            "name_full": "Working Memory — Verbal N-3 subtask (Gong et al., 2024)",
            "brief_description": "A working memory subtask (Verbal N-3) used to probe core working-memory capacity by asking models to recall/transform verbal sequences.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Same evaluated models; WM test used to probe core executive function (working memory) relevant for interactive games.",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "Working Memory (Verbal N-3)",
            "test_description": "Verbal working memory task (N-3) with 50 trials; assesses ability to maintain/manipulate recent items in memory.",
            "llm_performance": "Accuracy computed across the 50 trials for each model (paper does not report numerical accuracies in main text).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline provided; correlations in the paper indicate working memory test performance correlates with better performance on interactive games (working memory 'beneficial for games').",
            "experimental_details": "Only the Verbal N-3 subtask was considered; accuracy reported across the 50 trials defined in original work; evaluated with temperature=1 and averaged over multiple trials.",
            "limitations_or_caveats": "No numeric human comparisons; task adaptations and generation randomness (temperature=1) were used and averaged to mitigate stochasticity; WM benchmark scope limited to that specific subtask.",
            "uuid": "e9033.3",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SimpleToM",
            "name_full": "SimpleToM (Theory of Mind / Attribution & Judgment dataset)",
            "brief_description": "A dataset containing concise stories with questions probing attribution, judgment, and mental-state inference used to assess Theory of Mind and related social cognition abilities in LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Same evaluated LLMs; SimpleToM used to derive separate scores for Attribution & Judgment (aggregated) and mental-state inference (ToM).",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "SimpleToM (subtasks: Attribution & Judgment; Theory of Mind)",
            "test_description": "Stories with questions about behavior prediction, normative judgment, and awareness/mental states (size ~3450 data points); used to probe ToM and attribution/judgment.",
            "llm_performance": "Models were evaluated on subtasks; the paper reports ToM correlates better with interactive games than static benchmarks, but does not provide numeric per-model accuracies in main text.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline provided; comparative analysis is correlational (ToM test scores correlate more strongly with performance on interactive games).",
            "experimental_details": "Subtasks aggregated per cognitive-ability grouping: behavior + judgment under Attribution & Judgment; mental state subtask under ToM; evaluated as multiple-choice via likelihood of correct answer.",
            "limitations_or_caveats": "Authors caution that currently available ToM datasets are recent and may not fully capture intended cognitive constructs for LLMs; no human norm scores reported here.",
            "uuid": "e9033.4",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Wordle (base and variants)",
            "name_full": "Wordle (base), Wordle (Clue), Wordle (Critic) — clembench",
            "brief_description": "Word-guessing interactive games (base and variants) from the clembench suite used to probe sequential hypothesis-updating, use of feedback, and social/critic interactions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Same LLMs evaluated in clembench Wordle variants; gameplay requires interpreting letter-level feedback over multiple turns.",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "Wordle (base) and Wordle variants (Clue, Critic)",
            "test_description": "Wordle base: iterative 5-letter word guessing with letter-position feedback; variants provide extra clues (Clue) or an external critic that evaluates guesses (Critic).",
            "llm_performance": "Classic Wordle: 'excessively low performance' across models (correlation analyses omitted for base due to low performance). Variants: models perform better with added clues/critic but numeric scores not reported in main text.",
            "human_baseline_performance": null,
            "performance_comparison": "LLMs are well below human-level play on classic Wordle (qualitative); external critic/clue variants stimulate use of social capabilities and improve performance relative to the base variant.",
            "experimental_details": "Wordle episodes run via clembench; models receive letter-level feedback each turn; correlation with cognitive abilities measured only for variants where performance allowed it.",
            "limitations_or_caveats": "Classic Wordle was so challenging for models that correlation with cognitive tests was not measured; improvements in variants may reflect task design (extra information) rather than emergence of underlying cognitive capabilities.",
            "uuid": "e9033.5",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Taboo (clembench)",
            "name_full": "Taboo (descriptive clues without taboo words) — clembench",
            "brief_description": "A dialogue game where a describer must get a guesser to identify a target word while avoiding specified 'taboo' words; probes commonsense, pragmatics, working memory, and ToM in gameplay.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-70B-Instruct (example) and other evaluated LLMs",
            "model_description": "In Appendix D the paper shows an episode of Llama-3.1-70B-Instruct playing Taboo successfully in at least one example; task requires generating creative descriptions avoiding taboo terms and tracking dialogue history.",
            "model_size": "Example shown: 70B (Llama-3.1-70B-Instruct); other models in study varied 3.8B–72B",
            "test_battery_name": "Taboo (clembench)",
            "test_description": "Describer gives clues without using forbidden words; guesser must identify the target; assesses commonsense, pragmatic adaptation to the partner, working memory, and logical inference.",
            "llm_performance": "Qualitative: some models (e.g., Llama-3 70B) can play and reach correct guesses in episodes shown; overall correlation analysis shows Taboo correlates with working memory, commonsense, logical reasoning, pragmatics, and Theory of Mind.",
            "human_baseline_performance": null,
            "performance_comparison": "No quantitative human-baseline comparison provided; authors treat correlations with cognitive tests as indicative of required abilities rather than direct human-level claims.",
            "experimental_details": "Examples from clembench transcripts presented; correlation of Taboo performance with cognitive-ability scores reported in Figure 8 (Kendall's τ), but numerical τ values not reproduced in main text.",
            "limitations_or_caveats": "Authors note some connections between cognitive tests and gameplay are speculative; role of pragmatics is less clear and requires further investigation.",
            "uuid": "e9033.6",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Reference Game",
            "name_full": "Reference Game (clembench)",
            "brief_description": "An interactive referential communication task where a describer must uniquely identify one of several similar textual grids to a guesser; used to probe planning and concise referring-expression generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3",
            "model_description": "Models must produce referential expressions that disambiguate target grid from distractors; success depends on planning and compressing information into discriminative descriptions.",
            "model_size": "3.8B–72B (various)",
            "test_battery_name": "Reference Game (clembench)",
            "test_description": "Given three textual grids (one target), describer formulates a description enabling the guesser to select the correct grid; measures planning, working memory, and logical resolution of constraints.",
            "llm_performance": "Qualitative: models vary in success; paper reports Reference Game performance correlates with Planning more than other cognitive capabilities (no numeric accuracy reported in main text).",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline given; correlation indicates planning ability (as measured by the NATURAL PLAN benchmark) is predictive of Reference Game success.",
            "experimental_details": "Evaluated via clembench; descriptive analysis connects approaches (cell-by-cell vs. constraint-resolution) to planning scores; correlation computed via Kendall's τ.",
            "limitations_or_caveats": "High difficulty for many models; authors note mapping between planning tests and referential strategy is interpretative and tentative.",
            "uuid": "e9033.7",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Private/Shared (clembench)",
            "name_full": "Private/Shared (conversation + probing about shared/privileged knowledge)",
            "brief_description": "A conversational game where a customer and service agent converse while a third agent probes the customer's belief about what the service agent already knows; probes working memory and conversational tracking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Phi-4-Mini; OLMo-2-1124; Falcon3; Qwen2.5; Llama-3 (example episode with Llama-3.3-70B-Instruct shown)",
            "model_description": "Interactive multi-turn conversation requiring tracking of shared vs. private information across parallel dialogues; resource-intensive and conversationally rich.",
            "model_size": "3.8B–72B (various); example Llama-3.3-70B-Instruct",
            "test_battery_name": "Private/Shared (clembench)",
            "test_description": "Customer-service simulation with parallel conversations; after each turn a probe checks what the customer believes the service agent knows (private vs shared state).",
            "llm_performance": "Qualitative: Private/Shared correlates strongly with working memory (higher than other tasks); authors present an example transcript with Llama-3.3-70B-Instruct where probing turns are answered correctly in many cases.",
            "human_baseline_performance": null,
            "performance_comparison": "No human baseline reported; authors note surprising low correlation with Theory of Mind and emphasize need for further investigation.",
            "experimental_details": "Interactions mediated by a Game Master in clembench; example transcripts reproduced (Figure 17); performance metric for clembench computed from quality score scaled by games played.",
            "limitations_or_caveats": "Authors highlight the surprising low ToM correlation and suggest further work; Private/Shared episodes are resource- and time-intensive to evaluate.",
            "uuid": "e9033.8",
            "source_info": {
                "paper_title": "Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SimpleToM: Exposing the gap between explicit ToM inference and implicit ToM application in LLMs",
            "rating": 2,
            "sanitized_title": "simpletom_exposing_the_gap_between_explicit_tom_inference_and_implicit_tom_application_in_llms"
        },
        {
            "paper_title": "Working memory capacity of ChatGPT: An empirical study",
            "rating": 2,
            "sanitized_title": "working_memory_capacity_of_chatgpt_an_empirical_study"
        },
        {
            "paper_title": "CLADDER: Assessing causal reasoning in language models",
            "rating": 2,
            "sanitized_title": "cladder_assessing_causal_reasoning_in_language_models"
        },
        {
            "paper_title": "Are we done with MMLU?",
            "rating": 1,
            "sanitized_title": "are_we_done_with_mmlu"
        }
    ],
    "cost": 0.01667425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests
24 Sep 2025</p>
<p>Filippo Momentè 
University of Trento</p>
<p>Alessandro Suglia 
ILCC University of Edinburgh
3 UCL</p>
<p>Mario Giulianelli 
Ambra Ferrari 
University of Trento</p>
<p>Alexander Koller 
Saarland University</p>
<p>Oliver Lemon 
Heriot-Watt University</p>
<p>David Schlangen 
University of Potsdam
7 DFKI</p>
<p>Raquel Fernández 
University of Amsterdam</p>
<p>Raffaella Bernardi 
Faculty of Engineering
Free University of Bozen-Bolzano</p>
<p>Kyle Mahowald 
Anna A Ivanova 
Idan A Blank 
Nancy Kanwisher 
Joshua B Tenenbaum 
Evelina Fe 
Adam Atkinson 
Hany Awadalla 
Nguyen Bach 
Jianmin Bao 
Alon Benhaim 
Martin Cai 
Vishrav Chaudhary 
Congcong Chen 
Dongdong Chen 
Junkun Chen 
Weizhu Chen 
Yen-Chun Chen 
Yi Ling Chen 
Qi Dai 
Xiyang Dai 
Ruchao Fan 
Mei Gao 
Min Gao 
Amit Garg 
Abhishek Goswami 
Jun- Heng Hao 
Amr Hendy 
Yuxuan Hu 
Xin Jin 
Mah- Moud Khademi 
Dongwoo Kim 
Young Jin Kim 
Gina Lee 
Jinyu Li 
Yunsheng Li 
Chen Liang 
Xihui Lin 
Zeqi Lin 
Mengchen Liu 
Yang Liu 
Gilsinia Lopez 
Chong Luo 
Piyush Madan 
Vadim Mazalov 
Arindam Mitra 
Ali Mousavi 
Anh Nguyen 
Jing Pan 
Daniel Perez-Becker 
Jacob Platin 
Thomas Portet 
Haibin Wu 
Haoran Xu 
Weijian Xu 
Yifan Yang 
Ziyi Yang 
Donghan Yu 
Ishmam Zabir 
Jianwen Zhang 
Lyna Li 
Yunan Zhang 
Xiren Zhang 
2025 Zhou 
Aarohi Srivastava 
Abhinav Rastogi 
Abhishek Rao 
Abu Awal 
Md Shoeb 
Pete Walsh 
Luca Soldaini 
Dirk Groeneveld 
Kyle Lo 
Shane Arora 
Akshita Bhagia 
Yuling Gu 
Shengyi Huang 
Matt Jordan 
Nathan Lambert 
Dustin Schwenk 
Oyvind Tafjord 
Taira Anderson 
David Atkinson 
Faeze Brahman 
Christopher Clark 
Pradeep Dasigi 
Nouha Dziri 
Michal Guerquin 
PangHamish Ivison 
Wei Koh 
Jiacheng Liu 
Saumya Malik 
William Merrill 
James V Lester 
Ja- Cob Miranda 
Tyler Morrison 
Crystal Murray 
Valentina Nam 
Aman Pyatkin 
Michael Rangapur 
Sam Schmitz 
David Skjonsberg 
Christopher Wadden 
Wilhelm 
Alex Wang 
Amanpreet Singh 
Julian Michael 
Felix Hill 
Omer Levy 
Samuel R Bowman 
Thomas Wolf 
Lysandre Debut 
Victor Sanh 
Julien Chaumond 
Clement Delangue 
Anthony Moi 
Pier- Ric Cistac 
Tim Rault 
Rémi Louf 
Morgan Funtowicz 
Joe Davison 
Sam Shleifer 
Patrick Von Platen 
Clara Ma 
Yacine Jernite 
Julien Plu 
Canwen Xu 
Teven Le Scao 
Sylvain Gugger 
Mariama Drame 
Quentin Lhoest 
An Yang 
Baosong Yang 
Binyuan Hui 
Bo Zheng 
Bowen Yu 
Chang Zhou 
Chengpeng Li 
Chengyuan Li 
Dayiheng Liu 
Fei Huang 
Guanting Dong 
Hao- Ran Wei 
Huan Lin 
Jialong Tang 
Jialin Wang 
Jian Yang 
Jianhong Tu 
Jianwei Zhang 
Jianxin Ma 
Jin Xu 
Jingren Zhou 
Jinze Bai 
Jinzheng He 
Junyang Lin 
Kai Dang 
Keming Lu 
Keqin Chen 
Kexin Yang 
Mei Li 
Mingfeng Xue 
Na Ni 
Pei Zhang 
Peng Wang 
Ru Peng 
Rui Men 
Ruize Gao 
Runji Lin 
Shijie Wang 
Shuai Bai 
Sinan Tan 
Tianhang Zhu 
Tianhao Li 
Tianyu Liu 
Wenbin Ge 
Xiaodong Deng 
Shunyu Yao 
Dian Yu 
Jeffrey Zhao 
Izhak Shafran 
Tom Griffiths 
Yuan Cao 
Karthik Narasimhan 
Steven Huaixiu 
Swaroop Zheng 
Hugh Mishra 
Xinyun Zhang 
Minmin Chen 
Azade Chen 
Le Nova 
Heng-Tze Hou 
Quoc V Cheng 
Ed H Le 
Denny Chi 
2024 Zhou 
Jeffrey Zhou 
Tianjian Lu 
Swaroop Mishra 
Sid- Dhartha Brahma 
Sujoy Basu 
Yi Luan 
Denny Zhou </p>
<p>Heriot-Watt University</p>
<p>University of Trento</p>
<p>Bo Ren, Liliang Ren
Sambuddha Roy, Saksham SinghalKai Qiu, Ning Shang, Yelong Shen, Subhojit Som, Xia Song</p>
<p>Tetyana Sych
Praneetha Vaddamanu, Shuohang Wang, Yiming WangZhenghao Wang</p>
<p>philosophy of language
Cambridge University Press
CambridgeUK</p>
<p>Michael Wilson
Luke Zettlemoyer, Ali Farhadi</p>
<p>Noah A. Smith
and Hannaneh Hajishirzi. 2024. 2 OLMo 2 Furious. Alex Wang</p>
<p>Yada Pruksachatkun
Nikita Nangia</p>
<p>Aman-preet Singh
Julian Michael, Felix Hill</p>
<p>Omer Levy
and Samuel Bowman. 2019a</p>
<p>Yubo Wang
Xueguang Ma, Yuansheng Ni, Shiguang Guo, Weiming Ren, Aaran ArulrajGe Zhang, Abhranil Chandra</p>
<p>Xuan He
Ziyan Jiang</p>
<p>Tianle Li
Max Ku, Alex Zhuang, Rongqi Fan, Xiang YueKai Wang</p>
<p>New YorkNYUSA</p>
<p>Xiaohuan Zhou
Xingzhang Ren
Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei ChuXinyu Zhang, Xipin Wei, Xuancheng Ren, Yuqiong Liu, Zeyu</p>
<p>Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests
24 Sep 20259F2B81FD7AD313242FA105A40C5D6421arXiv:2502.14359v4[cs.CL]
We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind).First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality.Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games.Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models.Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games.We propose a new evaluation framework triangulating LLMs progress.Our findings highlight the importance of defining evaluation regimes that consider multiple paradigms.We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by human ability assessments but designed specifically for LLMs.The code for running the experiments is released at: https://github.com/momentino/playpen_eval/tree/triangulating.</p>
<p>Introduction</p>
<p>Evaluating LLMs is critical to track progress, identify blind spots, and ultimately advance towards the kind of language-based AI systems we want as a society (Wooldridge and Jennings, 1995).Currently, the most widespread way to evaluate LLMs is by means of large benchmarks made up of miscellaneous tasks.Pre-LLM benchmarks such as GLUE and SuperGLUE (Wang et al., 2019b,a) have been replaced by even larger evaluation suites such as MMLU (Measuring Massive Multitask Language Understanding; Hendrycks et al., 2021), MMLU-Pro (Wang et al., 2024), or BBH (BIG-Bench Hard;Suzgun et al., 2023;Srivastava et al., 2023).Models with high performance on these benchmarks are taken to possess extensive world knowledge along with complex problem-solving abilities.</p>
<p>This trend has promoted standardisation in LLM evaluation protocols, with online leaderboards constantly updated as new models are released.Despite this undeniable benefit, general-purpose benchmarks, like those mentioned above, are not without problems.Evaluation results may be inflated by data contamination (see, e.g., Gema et al. 2025 for MMLU) and distorted by model sensitivity to prompt format (Zhuo et al., 2024).Moreover, by design, such benchmarks overlook actual language use in favour of knowledge-intensive tasks where success is measured against gold reference answers provided in a single conversational turn.</p>
<p>This contrasts with the view, put forward by philosophers and psycholinguists alike (Wittgenstein, 1953;Austin, 1962;Searle, 1969;Clark, 1996), that the quintessence of language resides in situated language use, i.e., using language for a purpose in social and task-based multi-turn interactions (Bisk et al., 2020).This situated and interactive view underpins a parallel trend where LLMs are evaluated as goal-directed language users by means of interactive games (Bertolazzi et al., 2023;Schlangen, 2023;Suglia et al., 2024). 1his interactive evaluation paradigm goes beyond single-turn text generation, which is critical for deploying LLMs as agents.Additionally, it is less susceptible to data contamination because the vast space of possible multi-turn interactions is unlikely to be fully represented in the training data.Yet, despite these advantages, it is not easy to pinpoint which specific abilities underpin models' performance on interactive language games-a difficulty that to some extent also applies to static question-answering benchmarks such as MMLU.</p>
<p>In this paper, we study these two evaluation paradigms-general-purpose benchmarks and interactive games-and show that they can provide complementary perspectives.In particular, we investigate whether standard benchmarks or games are more effective in gauging qualitative differences between models, evaluate a selection of current LLMs from five model families, and find that games highlight more nuanced differences between LLMs compared to standard benchmarks.While scaling model size leads to systematic improvements on benchmarks, game-based evaluation reveals differences between models of the same size.</p>
<p>To shed light on the abilities underlying models' performance on these two evaluation frameworks, we resort to targeted cognitive tests.In the absence of specifically designed tests for LLMs, we propose a taxonomy of cognitive skills motivated by human neurocognitive science and compile a list of existing evaluation datasets designed to assess each skill in isolation.Specifically, we investigate to what extent increased performance on specific abilities correlates with performance gain in standard benchmarks and interactive games, respectively.</p>
<p>In sum, our work brings in a unique perspective by studying the interrelations between a diverse set of evaluation paradigms which, to our knowledge, have not been jointly investigated before.</p>
<p>Models</p>
<p>We apply our evaluation framework to the following open-weight LLMs ranging from 3.8B to 72B models: Phi-4-Mini with 3.8B parameters (Microsoft et al., 2025), OLMo-2-1124 with 7B and13B parameters (Walsh et al., 2024), Falcon3 with 10B parameters (Falcon Team, 2024), Qwen2.5 with 7B, 32B, and72B parameters (Yang et al., 2024;Team, 2024), and Llama-3 with 8B and 70B parameters (Grattafiori et al., 2024).All these models score above 70% on IFEval (Zhou et al., 2023), which we take as indication that they possess the instruction-following capabilities neces-sary for playing games and understanding task descriptions in the cognitive tests.See Appendix A for further model details.</p>
<p>How to Identify Blind Spots in LLMs</p>
<p>Evaluation paradigms have most practical use when they allow us to track progress by identifying blind spots in LLMs.Here we compare the two paradigms under study on the extent to which they highlight differences between current models, helping us form hypotheses about possible performance gaps and successful mitigation strategies.</p>
<p>General-purpose reference-based benchmarks</p>
<p>We take MMLU (Hendrycks et al., 2021) and BBH (Suzgun et al., 2023) as representative of standard benchmarks.MMLU evaluates whether LLMs can apply knowledge from specific domains: it consists of multiple-choice questions spanning 57 academic subjects.BBH assembles diverse tasks drawing problems from linguistics, child development, maths, and common-sense reasoning, a.o.Results Figure 1 shows models' performance on the standard benchmarks and interactive games.While the OLMo-2 models are more inconsistent across different model sizes, all the other models exhibit the expected pattern of showcasing better performance on both benchmarks and interactive games as the parameter count increases.At the same time, we observe that most of the interactive games highlight the benefits of larger model sizes much more strongly.This can more easily be appreciated in Figure 2 (left).In this visualisation, the further away a benchmark is from the diagonal, the more affected performance is by model size.While Wordle is an extremely challenging game for any model, scaling up the number of parameters appears to be fundamental to succeed at Private/Shared, Image Game, and Reference Game-much more than what can be observed for MMLU and BBH.</p>
<p>Interactive games</p>
<p>Is size however all we need? Figure 2 (right) shows that reference-based benchmarks do not substantially distinguish between models of comparable size: Scaling on the number of parameters results in performance boosts across model families.Hence, arguably, reference-based benchmarks test for abilities that can be expressed within parametric knowledge.Given that such benchmarks are currently the standard LLM evaluation paradigm, it is not surprising that scaling is high on the agenda of model developers.In contrast, interactive games seem to provide a different picture: models with comparable parametric capacity perform very differently on Image Game, Private/Shared, and Wordle (Clue/Critic).Similar trends can be observed in other model comparisons (see Appendix I).This result supports the hypothesis that size is not all there is behind the potential of LLMs to learn inferential strategies for effective language use in interaction.</p>
<p>Cognitive Abilities</p>
<p>We now turn to targeted cognitive tests, which assess specific cognitive abilities deemed essential for effective language use in real-world situations.We explore the use of such tests as a complementary evaluation method to reference-based benchmarks and interactive games.</p>
<p>Ideally, such an investigation should be carried out using tests tailored to machine intelligence, which unfortunately have not yet been systematically developed (Hagendorff et al., 2024).To make progress in this direction, Millière and Rathkopf (2024) advocate for the development of an "artificial cognition evaluation toolkit" through a cyclic process that takes cognitive tests designed for humans as the starting point.Following this literature, we present a taxonomy of abilities and a suite of existing evaluation datasets inspired by human cognition, with the aim to make a first step towards more adequate testing of artificial cognition.</p>
<p>Taxonomy and Datasets</p>
<p>We present a taxonomy of cognitive abilities involved in human functional linguistic competence (Mahowald et al., 2024) bilities into two distinct macro-categories known to recruit different brain networks: executive functions and socio-emotional skills.Executive functions are broadly defined as the complex processes by which we control and optimise our thoughts and behaviour (Baddeley, 1986), divided into core and higher-order abilities.Socio-emotional skills are abilities necessary to interact adaptively with other individuals (Higgins, 1987), including the ability to recognize their emotional and cognitive states.</p>
<p>For each cognitive ability, we select an existing evaluation dataset designed to test it in isolation.Socio-emotional skills have only recently entered the evaluation landscape in NLP, and they have done so with a forceful presence: remarkably, small benchmarks already exist for almost all of the abilities in this category.Tables 1 and 2 list the abilities in the taxonomy and the datasets we use to evaluate them.2More details are available in App.B and E.</p>
<p>Cognitive Ability Analysis</p>
<p>Equipped with our taxonomy and associated cognitive tests, we aim to shed some light on the cognitive abilities involved in interactive games and general benchmarks.Figure 3 reports Kendall's τ correlation coefficients.The analysis reveals that performance both on static and interactive evaluation correlates with performance on tests measur-ing higher-order reasoning abilities: while planning is more dominant in static problem-solving tasks, working memory seems to be beneficial for games.Among the social skills, pragmatics appears to be relevant for both static and interactive tests, while ToM correlates better with the latter.</p>
<p>These results suggest that interactive tests correlate more strongly with socio-emotional skills compared to static tests, despite this analysis remaining speculative in nature, as we still lack carefully curated cognitive tests specifically designed for LLMs.Expanded results reporting correlation coefficients for each game and benchmark considered are shown in Appendix I, Figure 8. Specifically for games, in Appendix D we provide a qualitative analysis where we aim to draw tentative connections between capabilities and gameplay.Remarkably, such connections appear to be clearer for executive functions compared to socio-emotional skills.</p>
<p>Related Work</p>
<p>A few recent papers have evaluated the abilities of LLMs beyond standard benchmarks.For example, Waldis et al. (2024) propose a framework to evaluate models' linguistic competence (in Chomsky 1965's sense) using probing classifiers to disentangle knowledge of, e.g., syntax and semantics from other abilities such as instruction following.Lu et al. (2024) posit that some perceived functional linguistic abilities (e.g., related to emotional intelligence) may be the result of in-context learning, model memory, and formal linguistic knowledge, whereas Ma et al. (2023) propose an evaluation protocol for assessing machine Theory of Mind based on a taxonomy of existing benchmarks driven by psychology research.</p>
<p>In contrast, the present work includes a uniquely comprehensive selection of evaluation tasks, encompassing standard benchmarks, interactive games, and cognitive ability assessments.To our knowledge, this diverse range of evaluations has not been jointly tested in prior work.Moreover, we systematically compare their discriminative power and analyse their interrelations-an approach that, to our knowledge, is unprecedented.</p>
<p>Conclusion</p>
<p>In this paper, we have examined three LLM evaluation paradigms.Our analyses revealed that interactive games have superior model discrimination power over standard reference-based benchmarks: Models with comparable parametric capacity perform similarly on standard benchmarks, but very differently on games.This suggests that scaling is not sufficient to learn strategies for effective language use in interaction.</p>
<p>We have further argued for the use of targeted cognitive tests as a complementary method to triangulate LLM progress.While each cognitive ability test alone does not get us very far in the quest for robust LLM evaluation, we contend that this type of evaluation paradigm has the potential to enhance our understanding of what fundamental abilities LLMs must develop to be able to function effectively as language agents, where multiple skills may be required and possibly interact.In our study, we have drawn inspiration from human cognitive tests as a first step in this direction.We hope to encourage the future development of targeted tests tailored to artificial cognition and supplemented with mechanistic analyses.</p>
<p>Limitations</p>
<p>The evaluation methods we have study in this papers have been applied without employing chain-ofthought (CoT) reasoning or similar elicitation techniques.While different elicitation strategies may enhance question-answering, interactive, and cognitive abilities in different ways (Yao et al., 2023;Hao et al., 2023;Li et al., 2024), we opted for an approach that remains agnostic to specific prompting methods and datasets.This ensures a consistent basis for comparison across models, though future work could explore how alternative prompting strategies influence performance across the three evaluation paradigms.</p>
<p>The set of models we evaluated is of course limited in scope, although representative of a wide range of open-weight model families and sizes.In particular, we did not evaluate any close-weight proprietary models, as we wanted to ensure full reproducibility of our experiments.</p>
<p>Regarding the cognitive ability assessments, we have used currently available datasets; such resources have started to be compiled only very recently, hence there is no guarantee that in their current state they truly assess the intended cognitive abilities in LLMs.Nevertheless, they help in establishing a starting point and encouraging further progress in this direction.</p>
<p>Finally, our evaluation is only on English data, hence we cannot claim the results hold for any language.Yet, we believe the main take-home message of the paper, viz.using different evaluation methods which include static and interactive tasks as well as controlled cognitive experiments, holds independently of the language analysed.Whether this is indeed the case, remains an interesting open question for future research.</p>
<p>Acknowledgments</p>
<p>This work emerged from discussions at a workshop organised by Raquel Fernández and Sandro Pezzelle at MFO, the Oberwolfach Research Institute for Mathematics in the German Black Forest, on behalf of the ELLIS NLP programme.The event was funded by the state of Baden Württemberg (Germany) and organised in collaboration with the ELLIS Institute Tübingen and the Max Planck Institute for Intelligent Systems.Alessandro Suglia acknowledges the use of the Heriot-Watt University high-performance computing facility (DMOG) and associated support services in the completion of this work.Alessandro Suglia, Raffaella Bernardi, and David Schlangen acknowledge ISCRA for awarding this project access to the LEONARDO supercomputer, owned by the EuroHPC Joint Undertaking, hosted by CINECA (Italy).We furthermore acknowledge our funders.</p>
<p>Appendix A Models</p>
<p>The Phi-4 series (Microsoft et al., 2025) includes a model with 3.8 billion parameters.Despite its compact size, it has been reported to surpass larger models on a wide selection of tasks including reasoning, general-purpose and coding.The Olmo-2-1124 series (Walsh et al., 2024) includes models with 7 billion and 13 billion parameters (OLMo-2-1124-*-Instruct).Both models are designed for a variety of tasks, including chat, mathematics, and reasoning.They have undergone supervised fine-tuning on the Tülu 3 dataset and further training using DPO techniques.</p>
<p>The Qwen2.5 series (Yang et al., 2024; Team, 2024) includes models with 7B, 32B, and 72B pa-rameters (Qwen2.5-*-Instruct).They are multilingual, supporting over 29 languages, and excel in coding, mathematics, and instruction following.</p>
<p>The Llama-3 series (Grattafiori et al., 2024) includes models with 8B (Llama3.1-8B-Instruct)and 70B parameters (Llama3.3-70B-Instruct).These models are optimized for multilingual dialogue and support various languages.They use an optimized transformer architecture and are finetuned for instruction following.</p>
<p>The Falcon3 series (Falcon Team, 2024) includes a model with 10 billion parameters.It achieves state-of-the-art results in reasoning, language understanding, instruction following, code, and mathematics tasks.It supports four languages (English, French, Spanish, Portuguese) and a context length of up to 32K.</p>
<p>Model Implementations All the models used in this study have been made available by Huggingface, and have been accessed through the transformers (Wolf et al., 2020) library.For text generation, we have been applying the default chat template specified by the models' Hugginface implementations.Attribution and Judgment: Interpret the causes of others' behavior, distinguishing between intentional and unintentional actions.</p>
<p>B Taxonomy of Cognitive Abilities</p>
<p>Pragmatics: Aspects of communication that go beyond formal language competence: considering communicative intentions, the communicative context of the utterance, shared knowledge between speakers, manners, social and cultural norms.</p>
<p>C Interactive Games</p>
<p>We leverage clembench (Chalamalasetti et al., 2023), a benchmark that assesses models' gameplay ability in well-defined dialogue games such as:</p>
<p>Taboo: A game where one player tries to get the other player to guess a word without using certain 'taboo' words in their clues;</p>
<p>Wordle: A game where one player thinks of a 5letter word and the other player tries to guess it.At each turn, the guesser receives feedback regarding whether each letter of the tentative word is present in the target word, and whether it is also in the same position in such word;</p>
<p>Wordle (Clue): A variant of Wordle where the guesser is given more clue to help them guess the target word;</p>
<p>Wordle (Critic): A variant of Wordle where the guesser's suggestions are evaluated by a third player, who provides feedback on their accuracy;</p>
<p>Reference Game: A game where one player is presented with three textual grids and is tasked to make another player-who is also presented with the same three grids, but potentially in a different order-identify the one it has in mind based on its textual description;</p>
<p>Private/Shared: A game where a customer agent goes through a form with a service agent and, after each turn, a third agent,3 probes the customer on what they believe that the service agent already knows;</p>
<p>Image Game: A game where given a certain grid structure similar to those in Reference Game, a player has to instruct a second one (who can't see the it) about how to reconstruct it step by step.</p>
<p>D Interactive Games: Samples</p>
<p>In this section we provide samples for the interactive games, and discuss their nature and links with cognitive abilities, as they are shown by the correlation analysis displayed in Figure 8.We provide samples deriving from our our evaluations however, as a reference, the clembench benchmark's maintainers made available a repository of transcripts for the conversations obtained by evaluating several models on the games.4</p>
<p>D.1 Taboo</p>
<p>Figure 10 shows an episode of Taboo played by Llama-3.1-70B-Instruct.In this case, the word to guess is clear, and the Describing player (on the left) is instructed to generate a description for the other player without using the taboo words crystal, obvious and confident.The description is then delivered to the guesser who guesses the word glass at first.Since this was not the correct word, the game proceeds for another round, when the guesser gets the right word.As shown in Figure 8, this game presents a higher correlation with working memory, commonsense and logical reasoning among the executive functions, as well as pragmatics and Theory of Mind among the socio-emotional skills.</p>
<p>We can observe from the example interaction that the correlation results are quite compatible with the abilities required for being successful at the game.Common-sense reasoning is exercised for guessing the appropriate word from the descriptions, and for the generation of descriptions themselves, along with Theory of Mind which is necessary for tuning the descriptions with the guesser's knowledge that can be inferred from its answers.Logical reasoning can be instead exercised along with common-sense reasoning for guessing.Working memory can be used to avoid repeating the same answer and being able to remember useful information from the conversation, while the role of pragmatics remains less clear.</p>
<p>D.2 Wordle (and its variants)</p>
<p>Figure 11 shows an episode of Wordle.The model here is asked to guess the word whine.At each turn, the model receives letter-level feedback which has to exploit in order to get to the solution.We have evaluated models also on the variants providing additional clue (Figure 12) and the support of a critic model judging every guess (Figure 13).While correlation has not been measured on the classic Wordle variant due to the excessively low performance of models, we can interestingly observe differences in the correlation between the two variants and the cognitive capabilities.Results suggest that the presence of an external model stimulates the use of social capabilities.</p>
<p>D.3 Image Game</p>
<p>Figure 14 shows a match of Image Game, where a describer player has to describe a certain grid step by step to a second player which has to draw it by following the instruction.The describer has then to decide when the grid is complete.Given the high difficulty of the game for most of the models in this study, inferring links with cognitive capabilities appears complicated.</p>
<p>D.4 Reference Game</p>
<p>In Figure 15 is displayed an episode of Reference Game where a describer model has to describe a given grid with a empty cross-like shape.The other player has to guess the grid by following the description.</p>
<p>In our study, this task has been shown to correlate with Planning more than other cognitive ca-pabilities.This connection appears reasonable: in order to build the best description a model may either describe the grid cell by cell, or it may look at the task as a constrain resolution problem where it has to resolve conflicts in order to provide the the simplest description that does not overlap with that of other grids.In this case, it is possible to observe analogies with schedule planning tasks such as those present in our planning benchmarks.In the case of other executive functions, working memory may be involved to remember the information regarding the grids within the context, and logical reasoning for guessing the appropriate grid.</p>
<p>Less clear are the correlations with social functions.</p>
<p>D.5 Private/Shared</p>
<p>Private/Shared is a game where two parallel conversations occur at the same time.Figure 16 shows the main conversation of an episode simulating a travel-agency interaction.Among the games taken into consideration, it is the most conversational task.At each turn, in a parallel conversation the player is questioned regarding its understanding of the state (private or public) of information.</p>
<p>The low correlation of this task with Theory of Mind is quite surprising and would be worth further investigations.On the contrary, it appears understandable its correlation with working memory superior to that of any other tasks, as conversations are generally longer and require more explicit recalling of information shared in prior messages.</p>
<p>E Benchmarks for Cognitive Abilities</p>
<p>Working Memory (Gong et al., 2024)  Attribution and Judgment/Theory of Mind SimpleToM(Gu et al., 2025) contains concise, diverse stories each with questions that ask models to predict behavior ("Will Mary pay for the chips or report the mold?"), judgment ("Mary paid for the chips.Was that reasonable?") or mental states ("Is Mary likely to be aware that 'The can of Pringles has moldy chips in it.'?Yes or No?") The first two subtasks have been taken as a reference for the Attribution and Judgment cognitive ability, while the last as a reference for Theory of Mind.Size: 3450 data points.</p>
<p>F Benchmark Implementations</p>
<p>For the majority of the static benchmarks evaluated in this work we relied on the popular framework for the evaluation of LLMs LM Evaluation Harness5 (ver.0.4.7), which already made available many of the selected benchmarks, and enabled a common interface for the implementation of most of the remaining ones.</p>
<p>The benchmarks which were already present within the framework are: SOCIAL IQA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), EQ-Bench (Paech, 2023), LogiQA 2.0 (Liu et al., 2023), MMLU (Hendrycks et al., 2021), BBH (Suzgun et al., 2023).The benchmarks which have been implemented in the framework over the course of the study are: CLADDER (Jin et al., 2023), LM-Pragmatics (Hu et al., 2023), SimpleToM (Gu et al., 2025), NATURAL PLAN (Zheng et al., 2024), IFEval (Zhou et al., 2023).</p>
<p>As for the interactive games, we have used the implementation provided by version 1.6 of the clembench (Chalamalasetti et al., 2023).The remaining benchmarks (WM, LLM-Cognitive-Flexibility) have been implemented outside of the framework, as LM Evaluation Harness did not provide support for the multi-turn nature of the tasks.</p>
<p>F.1 Zero-shot and Few-shot Tasks</p>
<p>The majority of the tasks have been evaluated in a zero-shot setting with the exception of MMLU (5shot), BBH (3-shot) (following common practices in model evaluation, e.g. in the Open Leaderboard 26 for BBH) and NATURAL PLAN (5-shot).In the case of NATURAL PLAN, our models performed really poorly when evaluated in a zero-shot fashionresulting in scores close to 0. Given that the task relies on the models producing answers in a strict format for parsing, we opted for using the 5-shot version provided by the benchmark's authors.</p>
<p>F.2 Metrics</p>
<p>Evaluation For the evaluation of models, we followed the original works' implementations as well as associated metrics.However, it may be the case that for a certain benchmark more metrics were defined, or that the original work did not aggregate results across subtasks.For this reason, we report here the metrics we used for evaluating models.</p>
<p>In the case of Clembench games, we computed performance by computing the ratio between the quality score (a number from 0 to 100) and the percentage of played games (a number between 0 and 1) divided by 100.</p>
<p>In the case of IFEval, following what was done in the Open Leaderboard 2, we averaged the results obtained on prompt-level and instruction-level strict accuracy.</p>
<p>As for EQ-Bench, we computed the task-specific score as it was implemented in the LM Evaluation Harness.</p>
<p>Regarding WM, we only considered the subtask Verbal N-3, and we computed the accuracy for the results obtained across the 50 trials defined in the original work.</p>
<p>In the case of LLM-Cognitive-Flexibility, we ran each subtask 8 times with 25 trials each, and computed the average of the accuracy obtained in each run.In this case, the accuracy was computed only on the trials for which response parsing was successful.We then averaged the accuracy obtained on both subtasks to compute the final score.</p>
<p>In the case of CLADDER, we followed the original work which treated the task as generative and probed for the presence of the substrings "yes"/"no" at the beginning of the model's answer.</p>
<p>In NATURAL PLAN, the original work defined a rule-based procedure to parse specific data from the generated plan (e.g., dates).We reuse their parsing procedure and verify whether the expected elements are all present in the parsed plan.</p>
<p>For the remaining tasks (LogiQA 2.0, Wino-Grande, LM-Pragmatics, SOCIAL IQA, MMLU, BBH, SimpleToM), we treated them as a multiplechoice question answering task that is evaluated based on the likelihood of the correct answer for the task.</p>
<p>In the case of BBH, the Open Leaderboard 2's evaluation code excludes three of the original tasks from the overall score's computing: dyck languages, navigate and word sorting.The performances on these subtasks are therefore also ignored in the performance reported in this study.</p>
<p>In the case multiple subtasks were present (LM-Pragmatics, MMLU, BBH, NATURAL PLAN, LLM-Cognitive-Flexibility), we computed the micro-average over the results achieved on each subtask.In the specific case of SimpleToM, since the subtasks were associated with two different Cognitive Abilities, we've aggregated the score of the subtasks behaviour and judgment into a single score (under Attribution and Judgment), and considered the mental state subtask separately (under Theory of Mind).</p>
<p>Correlation For measuring the pair-wise correlation between benchmarks, we've computed the Kendall rank correlation coefficient (or Kendall's Tau) (Tau-b version).It measures rank correlation according to this formula:
τ b = P − Q (P + Q + T x )(P + Q + T y )
where: P = number of concordant pairs, Q = number of discordant pairs, T x = tie correction for variable X, T y = tie correction for variable Y.</p>
<p>This method was preferable over others given its robustness in case of few data points, as it was in our case.We have also experimented with the Pearson correlation coefficient and observed that in the majority of the cases, the correlation patterns were similar, however with larger positive as well as negative correlations compared to Kendall.We've relied on the implementation provided by the pingouin Python package (Vallat, 2018).</p>
<p>F.3 Generation Settings</p>
<p>The tasks which required the models to generate text are: EQ-Bench, WM, BBH, IFEval, the clembench games, LLM-Cognitive-Flexibility, NATU-RAL PLAN, CLADDER.With the exception of Working Memory and LLM-Cognitive-Flexibility, all tasks have been evaluated by applying a temperature of 0. Following the original implementation, we have applied a temperature of 1 to WM and 0.7 to LLM-Cognitive-Flexibility. In these cases, however, the increased randomness caused by the higher temperature was mitigated by averaging the results obtained over multiple trials.</p>
<p>As for the other generation settings, we also have followed what was prescribed in the original works regarding the tokens for the termination of the generation, the maximum or minimum number of tokens.In the case of NATURAL PLAN, the original work did not provide specific information regarding the settings they have adopted for the evaluation.Given the highly challenging nature of the task, we have set the minimum and maximum number of tokens to 90 and 350, respectively.This was derived based on the minimum and maximum number of tokens in the gold plans.</p>
<p>G Limitations in the Evaluations</p>
<p>In certain cases, results have not been computed on all the subtasks available for that benchmark.In the case of NATURAL PLAN, we have not considered results coming from the meeting subtask, while for WM we have only considered those coming from the Verbal (Base) N-3 subtask.In the first case, the high amount of resources required for evaluating the task, especially for the larger models prevented us from doing so.As for the second, we've only considered the base version of the verbal subtask and excluded its variations as they would not provide meaningful information for this study.</p>
<p>H Computational Resources</p>
<p>As a reference, we provide the time required for running through all the benchmarks for the Llama-3.1-8B-Instructmodel on 1 A100 GPU with batch size set to 'auto' in the LM Evaluation Harness (i.e. it automatically fits into the memory the maximum batch size possible for each task).For the Clembench games, LLM-Cognitive-Flexibility and WM, the batch size is 1.The time also includes that required for procedures performed by the LM Evaluation Harness prior to the actual evaluation (relevant only for those datasets included in this framework) and for loading the model into the memory (all tasks).</p>
<p>I Additional Results</p>
<p>Table 3 includes the scores achieved by every model on each benchmark.Figure 4, Figure 5 and Figure 6 show the supplementary plots for the results in Section 3 (comparing models of different size but same family and models of similar size but coming from different families, respectively).Moreover, we provide supplementary plots for the results in Section 4. Figure 7 presents a direct comparison of models based on our selected cognitive tests.Figure 8 reports an extended version of Figure 3. Finally, Figure 9 reports two scatter plots exemplifying, respectively, situations of high and low correlation between two benchmarks (a game-based one and a cognitive ability one).[GM⟨B] GUESS: Glass 5</p>
<p>[A⟨GM] GUESS: Glass</p>
<p>6</p>
<p>[A⟩GM] CLUE: Not a material, but a state of something, like air or water.</p>
<p>7</p>
<p>[GM⟩B] CLUE: Not a material, but a state of something, like air or water.8</p>
<p>[GM⟨B] GUESS: Clear</p>
<p>5</p>
<p>[A⟩GM] guess: whine explanation: I'm replacing the "t" with a "w" to form a new word, as the letters "h", "i", "n", and "e" were all marked green, indicating they are in the correct positions, and "whine" is a valid English word that fits this pattern.[A⟩GM] guess: moans explanation:</p>
<p>The clue "complain" makes me think of words related to expressing dissatisfaction or discomfort, and "moans" is a word that fits this description and has 5 letters, making it a plausible initial guess.</p>
<p>3</p>
<p>[A⟨GM] guess_feedback: m<red> o<red> a<red> n<green> s<red> Provide your response only in this format.guess: word explanation: details 4</p>
<p>[GM|GM] [..]</p>
<p>5</p>
<p>[A⟩GM] guess: whine explanation:</p>
<p>The clue "complain" and the feedback that both "i" and "n" are in the correct positions suggest a word related to expressing dissatisfaction, and "whine" is a word that fits this description, incorporating the correctly positioned "i" and "n", and having 5 letters, making it a plausible next guess.</p>
<p>6</p>
<p>[GM|GM] game_result = WIN  [instruction prompt..] clue: complain 2</p>
<p>[A⟩GM] guess: moans explanation: The word "moans" is related to complaining, as people often moan when they are unhappy or dissatisfied with something, and it fits the 5-letter length.</p>
<p>3</p>
<p>[GM⟩B] [instruction prompt..]</p>
<p>4</p>
<p>[GM⟨B] agreement: yes explanation: the guess "moans" is a plausible match for the clue "complain" because moaning is a way of expressing dissatisfaction or unhappiness, which is a form of complaining, and it meets the 5-letter length requirement.</p>
<p>5</p>
<p>[GM|GM] [..]</p>
<p>6</p>
<p>[A⟩GM] guess: whine explanation: The word "whine" is a form of complaining, often through a high-pitched or nasal sound, and it contains the letter "n" which was indicated by the feedback to be in the word, making it a potential match for the clue "complain" with a 5-letter length.Answer this question with yes or no and then stop.3 [A⟩GM] ASIDE: Yes 4</p>
<p>[GM|GM] yes</p>
<p>5</p>
<p>[GM|GM] Answer for FROM valid after 1 tries.</p>
<p>6</p>
<p>[GM|GM] Answer is correct.</p>
<p>7</p>
<p>[A⟨GM] ME: Do you think the travel agent knows where you want to go to?[..] 8 [A⟩GM] ASIDE: Yes 9</p>
<p>[GM|GM] yes 10</p>
<p>[GM|GM] Answer for TO valid after 1 tries.</p>
<p>11</p>
<p>[GM|GM] Answer is correct.</p>
<p>12</p>
<p>[A⟨GM] ME: Is the travel agent aware of your means of transportation?[..] 13 [A⟩GM] ASIDE: Yes 14</p>
<p>[GM|GM] yes</p>
<p>15</p>
<p>[GM|GM] Answer for BY valid after 1 tries.</p>
<p>16</p>
<p>[GM|GM] Answer is correct.17 Answer for CLASS valid after 1 tries.</p>
<p>21</p>
<p>[GM|GM] Answer is correct.</p>
<p>22</p>
<p>[A⟨GM] ME: Is the travel agent aware of the dates of your trip?[..] 23 [A⟩GM] ASIDE: Yes 24</p>
<p>[GM|GM] yes</p>
<p>25</p>
<p>[GM|GM] Answer for WHEN valid after 1 tries.</p>
<p>26</p>
<p>[GM|GM] Answer is correct.</p>
<p>27</p>
<p>[GM|GM]</p>
<p>End probing Figure 17: An example of the final turn's probing interaction of a Private/Shared episode played by Llama-3.3-70B-Instruct with a programmatic player.The two players' messages are in green and orange, while the grey-colored text comes from the environment itself, represented by the Game Master.</p>
<p>We take clembench(Chalamalasetti et al., 2023)  as a benchmark to assess LLMs' gameplay ability in dialogue games.We consider the games 1) Taboo, 2) Wordle (base) and the two variants Wordle (Clue) and Wordle (Critic), 3) Reference Game, 4) Image Game, and 5) Private/Shared.Game descriptions and examples are provided in Appendices C and D.</p>
<p>Figure 1 :
1
Figure1: Accuracy for all models on standard benchmarks and interactive games.See Table3in App.I.</p>
<p>Figure 2 :
2
Figure 2: Comparing datasets in their power to discriminate between models of different size but same family (left) and of different families but similarly large (right).The number next to the benchmark's name indicates the ratio of performance between the two models.The asterisk '*' next to Wordle indicates that the ratio is undefined.</p>
<p>Figure 3 :
3
Figure 3: Correlation of cognitive abilities with performance on games and benchmarks (* = p &lt; 0.05).</p>
<p>•</p>
<p>LLM-Cognitive-Flexibility: ~1:50 min • LogiQA 2.0: ~5 min • CLADDER: ~19:30 min • WinoGrande: ~1 min • NATURAL PLAN: ~4:50 hours • WM ~2:40 min • EQ-Bench: ~3 min • LM-Pragmatics: ~6:30 min • SOCIAL IQA: ~1:30 min • SimpleToM: ~2:40 min • MMLU: ~14 min • BBH: ~24 min • IFEval: ~1:19 min • Taboo: ~3:30 min • Reference Game: ~3:00 min • Image Game: ~2.40 min • Wordle: ~7:50 min • Wordle (Critic): ~2:50 min • Wordle (Clue): ~2:15 min • Private/Shared: ~17:30 min</p>
<p>Figure 4 :
4
Figure 4: Comparing datasets in their power to discriminate models across size.</p>
<p>Figure 5 :
5
Figure 5: Comparing datasets in their power to discriminate models with similar size across families (Part 1).</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Comparing datasets in their power to discriminate models with similar size across families (Part 2).</p>
<p>Figure 9 :
9
Figure 9: High (left) and low (right) correlation</p>
<p>4</p>
<p>Figure 10 :
10
Figure 10: An example of an episode of Taboo played by Llama-3.3-70B-Instruct on both roles.The two players' messages are in green and orange, while the greycolored text comes from the environment itself, represented by the Game Master.</p>
<p>Figure 11 :
11
Figure 11: An example of an episode of Wordle played by Llama-3.3-70B-Instruct.The player's messages are in green, while the grey-colored text comes from the environment itself, represented by the Game Master.</p>
<p>Figure 12 :
12
Figure 12: An example of an episode of Wordle (variant with Clue) played by Llama-3.3-70B-Instruct.The player's messages are in green, while the grey-colored text comes from the environment itself, represented by the Game Master.</p>
<p>Figure 13 :
13
Figure 13: An example of an episode of Wordle (variant with Critic) played by Llama-3.3-70B-Instruct on both roles.The two players' messages are in green and orange, while the grey-colored text comes from the environment itself, represented by the Game Master.</p>
<p>Figure 16 :
16
Figure 16: An example of the main interaction of a Private/Shared episode played by Llama-3.3-70B-Instruct with a programmatic player.The two players' messages are in green and orange, while the grey-colored text comes from the environment itself, represented by the Game Master.</p>
<p>ME: Do you think the travel agent knows where you depart from?</p>
<p>Table 1 :
1
. It is guided by neurocognitive research (Ward, 2019), and it separates capa-Core and Higher-Order Executive Functions.
Cognitive AbilityBenchmarkCoreCognitive Flexibility Working MemoryKennedy and Nowak (2024) Gong et al. (2024)Inhibitory Control-Causal ReasoningJin et al. (2023)HOCommon-sense Reasoning Sakaguchi et al. (2021) Logical Reasoning Liu et al. (2023)PlanningZheng et al. (2024)Cognitive AbilityBenchmarkAttribution and JudgementGu et al. (2025)Emotional IntelligencePaech (2023)PragmaticsHu et al. (2023)Social Common-sense Reasoning Sap et al. (2019)Theory of MindGu et al. (2025)Emotion Regulation-Self-Awareness-Empathy-Social Problem-Solving-</p>
<p>Table 2 :
2
Social and Emotional Skills.</p>
<p>Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, AAAI'24/IAAI'24/EAAI'24. AAAI Press.
ReferencesJohn Langshaw Austin. 1962. How to do things withwords. Clarendon Press, London, UK.Alan Baddeley. 1986. Working memory. Oxford Uni-versity Press.D. A. Grant and E. A. Berg. 1948. Wisconsin card sorting test. Journal of Experimental Psychology.Leonardo Bertolazzi, Davide Mazzaccara, Filippo Merlo, and Raffaella Bernardi. 2023. ChatGPT'sAaron Grattafiori, Abhimanyu Dubey, and et al. Ab-hinav Jauhri. 2024. The Llama 3 herd of models. Preprint, arXiv:2407.21783.information seeking strategy: Insights from the 20-questions game. In Proceedings of the 16th Inter-national Natural Language Generation Conference, pages 153-162, Prague, Czechia. Association forYuling Gu, Oyvind Tafjord, Hyunwoo Kim, JaredComputational Linguistics.Moore, Ronan Le Bras, Peter Clark, and Yejin Choi. 2025. SimpleToM: Exposing the gap between ex-plicit ToM inference and implicit ToM application in LLMs.Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lap-ata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, and Joseph Turian. 2020.Thilo Hagendorff, Ishita Dasgupta, Marcel Binz,Experience grounds language. In Proceedings of theStephanie C.Y. Chan, Andrew Lampinen, Jane X.2020 Conference on Empirical Methods in NaturalWang, Zeynep Akata, and Eric Schulz. 2024. Ma-Language Processing (EMNLP), pages 8718-8735,chine psychology.Online. Association for Computational Linguistics.Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, ZhenWang, Daisy Wang, and Zhiting Hu. 2023. Rea-soning with language model is planning with worldmodel. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing,pages 8154-8173, Singapore. Association for Com-putational Linguistics.Dan Hendrycks, Collin Burns, Steven Basart, AndyZou, Mantas Mazeika, Dawn Song, and Jacob Stein-Yuyan Chen, Songzhou Yan, Sijia Liu, Yueze Li, andhardt. 2021. Measuring massive multitask languageYanghua Xiao. 2024. EmotionQueen: A benchmarkunderstanding. Proceedings of the International Con-for evaluating empathy of large language models.ference on Learning Representations (ICLR).In Findings of the Association for ComputationalLinguistics: ACL 2024, pages 2149-2176, Bangkok,Thailand. Association for Computational Linguistics.Noam Chomsky. 1965. Aspects of the Theory of Syntax.MIT Press.Herbert H Clark. 1996. Using language. CambridgeUniversity Press, Cambridge, UK.Y. Du, P. Rajivan, and C. Gonzalez. 2024. Large lan-Mario Giulianelli was supported in part by an ETH Zurich Postdoc-guage models for collective problem-solving: In-sights into group consensus decision-making. In Proceedings of the Annual Meeting of the Cognitivetoral Fellowship. David Schlangen acknowledgesScience Society.support by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -423217434 ("RECOLAGE") grant, and support from Univer-Technology Innovation Institute Falcon Team. 2024. The falcon 3 family of open models.sity of Potsdam (President's Fund). Raquel Fernán-dez was supported by the European Research Coun-cil (ERC Consolidator Grant DREAM 819455).Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Man-cino, Rohit Saxena, Xuanli He, Yu Zhao, XiaotangThis research has been partially supported andsponsored by the Province of Bolzano and the EUthrough the project Artificial Intelligence Labora-tory (AI-Lab) ERDF 2021-2027 EFRE1047, CUP:I53C23001670009 (01/01/2024 -31/12/2026).
Kranti Chalamalasetti, Jana Götze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen.2023.clembench: Using game play to evaluate chat-optimized language models as conversational agents.In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11174-11219, Singapore.Association for Computational Linguistics.Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini.2025.Are we done with MMLU?In NAACL 2025.Dongyu Gong, Xingchen Wan, and Dingmin Wang.2024.Working memory capacity of ChatGPT: An empirical study.In</p>
<p>The two players' messages are in green and orange, while the greycolored text comes from the environment itself, represented by the Game Master.</p>
<h1>AGame MasterB1[A⟨GM] [instruction prompt..][A⟩GM] guess: houseexplanation: this is a commonfive-letter English word, and2I am starting my guess with this word because it containsa mix of vowels and consonantsthat are frequently used inmany words.[A⟨GM]guess_feedback:h<yellow>o<red>u<red>s<red> e<green>3Provide your response only inthis format.guess: wordexplanation: details4[GM|GM] [...]</h1>
<p>Online leaderboards have started to appear for the interactive games evaluation paradigm; see, e.g., https:// textarena.ai/, https://clembench.github.io.
We found no dataset to evaluate inhibitory control. The datasets we found for Emotion-regulation, Self-awareness(Liu et al., 2024),Empathy (Chen et al., 2024)  and Social Problemsolving(Du et al., 2024)  require human evaluation.
In clembench, all interactions are mediated by a "Game Master". This agent plays a particularly active role in Private/Shared.
4 https://github.com/clembench/clembench-runs
https://github.com/EleutherAI/ LMEvaluationHarnessuation-harness
https://huggingface.co/docs/leaderboards/ open_llm_leaderboard/about
Rush. 2020.Transformers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online.Association for Computational Linguistics.Michael Wooldridge and Nicholas R Jennings.1995.Intelligent agents: Theory and practice.The knowledge engineering review, 10(2):115-152.Tree of thoughts: Deliberate problem solving with large language models.In Advances in Neural Information Processing Systems, volume 36, pages 11809-11822.Curran Associates, Inc.[instruction prompt (describer)..]Ok. Please do this for the following example, which is a 5 by 5 grid.What is your next command?2[A⟩GM] Command: Fill the first row with V and fill the last row with V.[GM⟩B][instruction prompt (drawer)..] Command: Fill the first row with V and fill the last row with V.What is your next command?6[A⟩GM] Command: Put a V in the second row, fifth column, and put a V in the third row, third column, and put a V in the fourth row, second column.7[GM⟩B] Command: Put a V in the second row, fifth column, and put a V in the third row, third column, and put a V in the fourth row, second column.What is your next command?10 [A⟩GM] Command: DONE [instruction prompt (describer)..] Target grid:Instruction: Describe the target grid.Generate the referring expression starting with the tag "Expression: " for the given target grid.Omit any other text.2[A⟩GM] Expression: The grid with an empty row in the middle.3[GM⟩B] [instruction prompt (guesser)..] First:The grid with an empty row in the middle.Question: Which grid does the expression refer to?Start with the tag "Answer: ", followed by your selection.Omit any other text.4[GM⟨B] Answer: Second  [A⟨GM] [instruction prompt..] 2[GM⟩B] What is the next request?3[GM⟨B] Where do you want to go?
Social cognition and social perception. Tory Higgins, 10.1146/annurev.ps.38.020187.002101Annual review of psychology. 3811987</p>
<p>A finegrained comparison of pragmatic language understanding in humans and language models. Jennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina Fedorenko, Edward Gibson, 10.18653/v1/2023.acl-long.230Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>CLadder: Assessing causal reasoning in language models. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, Fernando Gonzalez, Max Kleiman-Weiner, Mrinmaya Sachan, Bernhard Schölkopf, 2023In NeurIPS</p>
<p>Cognitive flexibility of large language models. Sean M Kennedy, Robert D Nowak, ICML 2024 Workshop on LLMs and Cognition. 2024</p>
<p>Enhancing emotional generation capability of large language models via emotional chain-of-thought. Zaijing Li, Gongwei Chen, Rui Shao, Yuquan Xie, Dongmei Jiang, Liqiang Nie, arXiv:2401.068362024Preprint</p>
<p>LogiQA 2.0-An improved dataset for logical reasoning in natural language understanding. Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, Yue Zhang, 10.1109/TASLP.2023.3293046IEEE/ACM Transactions on Audio, Speech, and Language Processing. 312023</p>
<p>InterIntent: Investigating social intelligence of LLMs via intention understanding in an interactive game context. Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-Tse Huang, Jieyu Zhao, 10.18653/v1/2024.emnlp-main.383Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USA2024Association for Computational Linguistics</p>
<p>Are emergent abilities in large language models just incontext learning?. Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych, 10.18653/v1/2024.acl-long.279Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Towards a holistic landscape of situated theory of mind in large language models. Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai, 10.18653/v1/2023.findings-emnlp.72Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>TRAVEL-AGENT: Where do you want to go? Answer only this question in one short sentence and then stop. A⟨GM</p>
<p>. A⟩gm] Answer, Dresden 6 [GM⟩B] Dresden 7 [GM⟩B] What is the next request? 8 [GM⟨B] How do you want to reach your destination? 9 [A⟨GM] TRAVEL-AGENT: How do you want to reach your destination? [..]</p>
<p>. A⟩gm] Answer, By overnight bus. 11 [GM⟩B] By overnight bus. 12 [GM⟩B] What is the next request? 13 [GM⟨B] Where are you departing from? 14 [A⟨GM] TRAVEL-AGENT: Where are you departing from? [..]</p>            </div>
        </div>

    </div>
</body>
</html>
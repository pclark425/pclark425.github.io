<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7325 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7325</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7325</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-276618032</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.19106v2.pdf" target="_blank">A Survey on Foundation-Model-Based Industrial Defect Detection</a></p>
                <p><strong>Paper Abstract:</strong> As industrial products become abundant and sophisticated, visual industrial defect detection receives much attention, including two-dimensional and three-dimensional visual feature modeling. Traditional methods use statistical analysis, abnormal data synthesis modeling, and generation-based models to separate product defect features and complete defect detection. Recently, the emergence of foundation models has brought visual and textual semantic prior knowledge. Many methods are based on foundation models (FM) to improve the accuracy of detection, but at the same time, increase model complexity and slow down inference speed. Some FM-based methods have begun to explore lightweight modeling ways, which have gradually attracted attention and deserve to be systematically analyzed. In this paper, we conduct a systematic survey with comparisons and discussions of foundation model methods from different aspects and briefly review non-foundation model (NFM) methods recently published. Furthermore, we discuss the differences between FM and NFM methods from training objectives, model structure and scale, model performance, and potential directions for future exploration. Through comparison, we find FM methods are more suitable for few-shot and zero-shot learning, which are more in line with actual industrial application scenarios and worthy of in-depth research.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7325",
    "paper_id": "paper-276618032",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005485749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on Foundation-Model-Based Industrial Defect Detection
27 Feb 2025</p>
<p>Tianle Yang tlyang@stu.suda.edu.cn 
Luyao Chang changluyao001@163.com 
Jiadong Yan 
Juntao Li 
Zhi Wang wangzhi@sz.tsinghua.edu.cn 
Ke Zhang kzhang19@suda.edu.cn </p>
<p>Soochow University
SuzhouChina</p>
<p>Soochow University
SuzhouChina</p>
<p>Shenzhen International Graduate School
Tsinghua Uni-versity
BeijingChina</p>
<p>Wuhan University of Science and Technology
WuhanChina</p>
<p>A Survey on Foundation-Model-Based Industrial Defect Detection
27 Feb 20251B6D18503D62CE24E8E1329E43D09F29arXiv:2502.19106v2[cs.CV]Industrial defect detectionfoundation modellarge language modelsegment anything model
As industrial products become abundant and sophisticated, visual industrial defect detection receives much attention, including two-dimensional and three-dimensional visual feature modeling.Traditional methods use statistical analysis, abnormal data synthesis modeling, and generation-based models to separate product defect features and complete defect detection.Recently, the emergence of foundation models has brought visual and textual semantic prior knowledge.Many methods are based on foundation models (FM) to improve the accuracy of detection, but at the same time, increase model complexity and slow down inference speed.Some FM-based methods have begun to explore lightweight modeling ways, which have gradually attracted attention and deserve to be systematically analyzed.In this paper, we conduct a systematic survey with comparisons and discussions of foundation model methods from different aspects and briefly review non-foundation model (NFM) methods recently published.Furthermore, we discuss the differences between FM and NFM methods from training objectives, model structure and scale, model performance, and potential directions for future exploration.Through comparison, we find FM methods are more suitable for few-shot and zero-shot learning, which are more in line with actual industrial application scenarios and worthy of in-depth research.</p>
<p>I. INTRODUCTION</p>
<p>V ISUAL defect detection [1], which is also called visual anomaly detection, is a key application area of artificial intelligence algorithms.This task plays a crucial role in ensuring the quality of industrial products.Traditional industrial anomaly detection algorithms [2], [3], [4], [5] focus on modeling the statistical distribution of normal features and detecting anomalies by analyzing the deviations in input samples from these learned patterns.To enhance the model's ability to identify anomalous patterns, some methods [6], [7] further explore contrastive learning mechanisms [8] between normal and abnormal features.These methods typically rely on a large amount of high-quality training data to establish reliable feature distributions and contrastive relationships.However, in real industrial scenarios, it is challenging to acquire specific high-quality training data due to the diversity and complexity of products and defects [9], [10], [11].For example, in chip defect detection, there are many types of chips and numerous defect categories, including structural defects and texture defects, making it difficult to collect data for various products and defects.In such cases, traditional models struggle to achieve satisfactory detection results.Recently, with the release of foundation models in vision and language, such as CLIP [12], GPT [13], [14] and SAM [15], industrial defect detection algorithms based on these models have made significant progress in both 2D and 3D visual environments [16], particularly in few-shot and zero-shot scenarios where data are limited.This has received a great deal of attention.The foundation models themselves possess strong capabilities in understanding general vision and language, making it an important issue to explore how to effectively apply their foundational knowledge to industrial detection problems without additional training samples and annotations.We categorize the application of different foundation models in 2D and 3D industrial defect detection as follows:</p>
<p>1) SAM-2D: Application of visual prior knowledge.As a powerful foundational model for visual segmentation, SAM provides semantic prior information acquired through extensive pre-training on vast amounts of data, significantly enhancing the accuracy of industrial defect detection.In 2D industrial defect detection tasks based on SAM, researchers have developed various methods [17], [18], [19], [20], [21], [22] to prompt SAM specifically for industrial scenarios.Additionally, object matching based on the masks generated by SAM is used to identify defect regions.2) CLIP-2D: Semantic matching of short texts and images.Image-text foundation models such as CLIP demonstrate fine-grained image-text matching.This ability effectively links subtle visual cues with descriptive text, so it is especially beneficial for defect detection.In 2D industrial defect detection tasks based on CLIP [23], [24], [25], [26], [27], [28], [29], [21], [30], [31], [32], [33], [34], it is essential to design and learn suitable text prompts while aligning image information at a finegrained level to further enhance performance.The design of text prompt templates has been extensively studied.3) GPT-2D: Long text semantic prior.Large language models like GPT can generate long-form descriptions, making them very suitable for complex scenarios that require detailed explanations and structured descriptions.Therefore, a key challenge in GPT-based 2D industrial defect detection methods [35], [36], [37], [38], [39], [40], [41] is designing prompts to obtain comprehensive Fig. 1.Organization of surveyed methods.We categorize the methods under investigation into two main categories: foundation models and non-foundation models.Each category is further divided into 2D and 3D scenarios.The foundation-model-based methods primarily include methods based on SAM, CLIP, and GPT, while non-foundation-model-based methods are classified into static methods, synthesis-based methods, methods combining 2D RGB and 3D point clouds, and 3D generative methods.Finally, we present the latest methods collected in this survey.</p>
<p>text descriptions and effectively leveraging the textual information.4) CLIP-3D: Short-text image semantic matching prior applied to cross-dimensional vision tasks.3D defect detection faces greater challenges due to its complex spatial information.To address this, image-text foundation models like CLIP offer a promising solution through cross-modal information complementarity [42], [43], [44].These models effectively combine visual information with textual descriptions, thereby enabling more precise high-dimensional spatial modeling that captures complex defects in 3D structures.</p>
<p>Although FMs demonstrate promising application prospects in industrial defect detection, NFM methods still possess irreplaceable advantages in specific application scenarios due to their smaller parameter sizes and higher computational efficiency.Based on this, this paper also provides a review of NFM methods, including 2D statistical modeling [45], [46], [47], [48], [49], 2D anomaly data synthesis [50], [51], [52], [53], [54], [55], [56], [57], [58], [59], [60], [61], 2D/3D cross-modal knowledge distillation [62], [63], [64], [65], [66], [44], [67], and algorithms based on 3D generative models [68], [69], [42], [70], [71].We believe that these methods can provide effective insight for FM methods and some of them can be applied to FM models.In addition, we systematically compare the differences between foundation and non-foundation approaches in terms of application scenarios, algorithm framework focus, detection performance, model complexity, and future development directions.Key areas for potential breakthroughs in both approaches are also highlighted.This paper aims to provide researchers and engineers with information on selecting the appropriate research methods for different scenarios and to offer valuable perspectives on the future development of industrial defect detection.</p>
<p>The organization of this survey paper is as follows.First, in Section 1, we introduce the challenges posed by FM in industrial defect detection, followed by a discussion of the mainstream methods currently adopted.In Section 2, we provide a detailed comparison between FM and NFM methods, focusing on their differences in training objectives, model architectures, algorithm framework and performance.Then we give an overview of the different types of FM methods applied to both 2D and 3D industrial defect detection in Section 3. Section 4 discusses the key approaches of NFM methods and insights they provide for FM methods.Finally, in Section 5, we examine the ongoing challenges faced by large models and highlight potential future directions for further exploration.A detailed organization of the methods we investigate is also shown in Figure 1.</p>
<p>II. COMPARISON OF FM AND NFM METHODS</p>
<p>With the diversification of industrial detection demands, the differences in model training objectives, structures, scales, and performance have become key factors influencing the choice of methods.The following comparison analyzes the performance of FM and NFM in industrial anomaly detection from the perspectives of training objectives, model structure and scale, algorithm framework and performance.A summary of the comparison is shown in Figure 2.</p>
<p>A. Model Training Objectives</p>
<p>FM and NFM exhibit significant differences in data requirements, training methods, computational resources, and the breadth of feature learning, which consequently leads to differences in their training objectives.2D FM methods (e.g., CLIP, SAM, GPT) leverage the cross-modal capabilities [72], [73], [74] of vision-language models to improve the accuracy and efficiency of anomaly detection.Their main objectives include: 1) Identifying unknown anomaly categories through unsupervised or zero-shot learning, reducing the dependence on labeled data; 2) Generating interpretable detection results that describe anomalies in terms of color, shape, and category; 3) Enhancing accuracy by integrating specific anomaly observation modules with the FM, addressing complex anomalies; 4) Improving model scalability and adaptability, enabling rapid adaptation to different industrial scenarios; 5) Integrating multimodal information to achieve precise anomaly localization and identification.3D FM methods focus on the geometric features of point cloud data [75], [76] and multi-view fusion [77], addressing issues of incomplete data and noise interference [78], [79].They perform classification and segmentation through multi-view rendering, while also handling inconsistencies between multimodal data.</p>
<p>In contrast, 2D NFM methods rely on traditional network architectures, utilizing techniques such as GANs [80] and diffusion models [81] to generate diverse anomaly samples to compensate for insufficient data.They emphasize feature selection and reconstruction [82], [83], [84], [85]strategies.3D NFM methods focus on detecting geometric defects and missing areas in point cloud data, using efficient architectures to reduce computational overhead.They also employ innovative techniques, such as normalization flows [86], [87] and diffusion-based reconstruction mechanisms, to enhance accuracy and robustness, avoiding dependence on design files or model libraries.</p>
<p>In summary, FM methods focus on cross-modal learning and generative capabilities, excelling in data-scarce scenarios and suitable for multi-task and multi-domain detection.In contrast, NFM methods emphasize feature selection, computational efficiency, and data synthesis, making them more suitable for resource-constrained environments.</p>
<p>B. Model Structure and Scale</p>
<p>1) Model structure: FM methods rely on powerful visionlanguage collaborative mechanisms, integrating large-scale foundational models such as CLIP, SAM, and GPT.These models employ multi-level feature fusion to establish a collaborative workflow: CLIP performs multi-modal feature extraction and alignment on image and point cloud data, SAM carries out fine-grained segmentation to isolate potential anomaly regions, and GPT provides semantic understanding and description of the detection results, assisting users in quickly obtaining analytical conclusions.To address the challenges of few-shot and zero-shot learning [88], CLIP's pretrained knowledge enables effective inference on unlabeled data, thereby enhancing the generalization ability of the detection model.NFM methods mainly include Teacher-Student Architecture [89], [90], [91], [92], Distribution Map, Memory Bank [93], Autoencoder-based [94], GAN-based, Transformerbased, and Diffusion-based frameworks.These approaches do not rely on large-scale data or pretraining tasks, focusing more on local feature selection, sample generation, and augmentation.Their aim is to optimize feature learning and anomaly detection capabilities with limited data.</p>
<p>2) Model scale: FM methods typically rely on large parameter sizes, utilizing complex network architectures and cross-modal learning to handle intricate anomaly detection tasks.This results in higher training times and computational resource demands.For example, SimCLIP [27] has parameter sizes of 428.77M.In contrast, NFM methods have smaller parameter sizes and primarily optimize models through efficient feature selection, adversarial training, and self-supervised learning.These methods can achieve more efficient training in Fig. 3.The left branch is framework of FM methods and the right one is of NFM methods.FM methods are primarily based on FM such as SAM, CLIP and GPT.During training, FM methods design appropriate loss functions to fine-tune the pre-trained foundational models, adapting them to the industrial defect detection domain.In contrast, NFM methods focus on designing task-specific models based on lightweight or specialized network architectures.Some NFM methods also design anomaly synthesis strategies to supplement training data.</p>
<p>resource-constrained environments.Since fast inference is an inevitable trend, the newly published FM methods are trying to explore ways to accelerate inference.For example, SAMbased STLM [17] requires only 16.56M for inference, making it one of the most efficient methods.</p>
<p>C. Framework</p>
<p>The frameworks of FM methods and NFM methods are shown in Figure 3. FM methods primarily leverage the prior knowledge embedded in foundation models, which have been pre-trained on large-scale general-purpose datasets and possess strong feature representation capabilities.Consequently, fine-tuning these models often requires only a small number of samples.Different types of foundation models, such as SAM and CLIP, can process data of different modalities, including images and textual information.During training, FM methods focus on designing suitable loss functions to adapt foundation models more effectively to anomaly detection tasks in industrial applications.Ultimately, the fine-tuned models achieve accurate segmentation or localization of anomalous regions in industrial images.</p>
<p>NFM methods focus on designing task-specific models.For example, reconstruction-based anomaly detection methods train a model that can accurately reconstruct normal data by learning the reconstruction process.During data preprocessing, some methods use anomaly synthesis strategies to expand the dataset since anomaly samples are rare.By training the model with the target data, it gradually improves and ultimately generates a model specialized in detecting or segmenting anomalous regions.</p>
<p>D. Model Performance 1) AUROC performance:</p>
<p>As shown in Table 1, using the commonly employed MVTec dataset as an example, 2D NFM methods generally achieve AUROC values close to 99%, performing the best.In contrast, some 2D FM methods have AUROC values around 98%.In 3D methods, both FM and NFM exhibit average AUROC values below 95%.This indicates that 2D methods outperform 3D methods overall, and NFM is currently more mature than FM in this context.</p>
<p>2) Inference time: Due to their large parameter sizes and complex model structures, FM methods generally require more inference time.Although STLM [17] has significantly optimized inference efficiency with an average inference time of 20ms, it still lags behind NFM methods like DRAEM [97], FastFlow [98], and PatchCore [99], which have shorter inference times.3D methods typically require longer inference times; however, some methods, such as SOWA [30], still demonstrate excellent inference speed, with a rate of 16.84 it/s (compared to 3.28 it/s for WinCLIP [23] and 1.82 it/s for April-GAN [31]).Compared to BTF [64], M3DM [100], and Reg 3D-AD [96], PointCore [69] achieves the highest AUROC and is the fastest, with a mean inference time per object on Real3D-AD [96] of 4.282s, excluding BTF <a href="2.19s">64</a>.The shape-guided [62] method has an inference time of 2.05s per sample, outperforming BTF [64].</p>
<p>3) Computational complexity: FM methods typically have higher FLOPs than those of NFM methods due to their large model sizes.For instance, STLM [17] has a FLOPs of 55G.SAM-LAD [19], which employs transformers and upsampled feature maps, has a FLOPs of 54.7G, higher than that of CNN-based NFM methods such as AE <a href="5.0G">2</a>and f-AnoGan <a href="7.7G">101</a>.Addressing the computational complexity of FM has become a popular research direction.SimCLIP [27], optimized for inference efficiency, requires fewer FLOPs (513.75G)than SOTA prompt learning methods like CoOp <a href="520.46G">102</a>and Co-CoOp <a href="520.46G">103</a>while maintaining the same parameter count.However, SimCLIP's [27] FLOPs are an order of magnitude higher than STLM [17] and SAM-LAD [19], as STLM [17] uses distillation from a fixed Use SAM to obtain object masks of the query and reference images and extract object features for matching ARXIV 98.4 SAA+ [18] Hybrid prompt regularization ARXIV -STLM [17] Utilize SAM as a teacher to guide student networks ARXIV 98.26 SPT [22] Adapt SAM to better understand the relationships between different regions in the image AAAI 2025 -</p>
<p>2D CLIP Based</p>
<p>WinCLIP [23] Compositional prompt ensemble, reference association method CVPR 2023 93.1 AnoCLIP [95] Local-aware visual tokens, domain-aware prompting, test-time adaptation method ARXIV -AnomalyCLIP [24] An object-agnostic text prompt template, global abnormality loss function ICLR 2024 -AdaCLIP [25] Hybrid (static and dynamic) learnable prompts, hybrid-semantic fusion module ECCV 2024 -VCP-CLIP [26] Visual context prompting model ARXIV -SimCLIP [27] Multi-hierarchy vision adapter, implicit prompt learning, prior-aware optimization algorithm ARXIV 95.3 CLIP-AD [28] Distribution of the text prompts, facilitate alignment via a linear layer ARXIV -CLIP-FSAC [29] Two-stage training strategy, visual-driven text features, fusion-text matching task IJCAI 2024 95.5 ClipSAM [21] CLIP and SAM Collaboration, unified multi-scale cross-modal interaction, multi-level mask refinement ARXIV -SOWA [30] Hierarchical frozen window self-attention, dual Learnable Prompts ARXIV -SAA+ [18] Hybrid prompts, domain expert knowledge and target image context ARXIV -APRIL-GAN [31] Employ a combination of state and template ensembles, memory bank-based approach ARXIV 92.0 PromptAD [32] Prompt learing, semantic concatenation, explicit anomaly margin CVPR 2024 94.6 FiLo [33] Fine-grained description, learnable vectors, position-enhanced high-quality localization method ARXIV -Dual-Image Enhanced CLIP [34] Dual image feature enhancement, test-time adaption with pseudo anomaly synthesis ARXIV -</p>
<p>2D GPT Based</p>
<p>AnomalyGPT [35] Lightweight and visual-textual feature-matching-based decoder, prompt embeddings AAAI 2024 94.1 Myriad [38] Apply vision experts, vision expert tokenizer ARXIV 94.1 ALFA [39] Run-time prompt adaptation strategy, fine-grained aligner ARXIV 94.5 GPT-4V-AD [40] Visual Question Answering paradigm, granular region division, prompt designing, Text2Segmentation method ARXIV -Customizable-VLM [37] Enhance foundation models by integrating expert knowledge as external memory via prompting ARXIV 82.9 LogiCode [41] Use LLMs to extract image logic and generate code for logical anomaly detection ARXIV -3D CLIP Based CLIP3D-AD [42] Address both few-shot anomaly classification and segmentation without memory banks and plenty of training samples ARXIV -MVTec3D-AD PointAD [43] Hybrid representation learning framework ARXIV 97.2 M3DM-NR [44] Use the suspected anomaly maps to achieve denoising ARXIV 94.5</p>
<p>Non-Foundation 2D Statistic SOFS [45] Introduce an abnormal prior map and mixed normal Dice loss CVPR 2024 93.3</p>
<p>MVTec AD Model PNI [46] Utilize position and neighborhood information ARXIV 99.56</p>
<p>Method REB [47] Reduce domain and local density biases ARXIV 99.5 BGAD [48] Strengthen the decision boundary by pulling together normal samples while pushing away anomalous samples CVPR 2023 99.3 COAD [49] Enhance model sensitivity to anomalies through controlled overfitting ARXIV 99.9 2D Synthesis GLASS [50] Anomaly synthesis based on Gaussian noise and gradient rise ARXIV 99.9 AdaBLDM [51] Latent diffusion model with feature editing ARXIV -RealNet [52] Strength-controllable diffusion anomaly synthesis CVPR 2024 99.6 CAGEN [53] Text-guided controllable anomaly generation ICASSP 97.7 AnomalyXFusion [54] Multi-modal anomaly synthesis for enhanced sample fidelity ARXIV 99.2 AnomalyDiffusion [55] Spatial anomaly embedding, adaptive attention re-weighting mechanism AAAI 2024 99.2 DFMGAN [56] Use defect-aware residual blocks in StyleGAN2 AAAI 2023 -DeSTSeg [57] Denoising student encoder-decoder, adaptive multi-level feature fusion CVPR 2023 98.6 CutSwap [58] Leverages saliency guidance to incorporate semantic cues ARXIV 98.0 Split Training [59] A split training strategy that alleviates the overftting issue ARXIV 98.3 DFD [60] Frequency-domain analysis with dual-path frequency discriminators ARXIV 93.3 PBAS [61] Use the compact distribution of normal sample features to guide the direction of feature-level anomaly synthesis TCSVT 99.8</p>
<p>2D RGB+3D PC Shape-Guided [62] Synergistic expert models for anomaly localization in color and shape WACV 94.7</p>
<p>MVTec3D-AD CPMF [63] Combine handcrafted PCD descriptions with pre-trained 2D neural networks Pattern Recognition 2023 92.93 Back to the Feature [64] Handcrafted 3D representations with PatchCore CVPR 2021 97.8 TransFusion [65] Address the overgeneralization and loss-of-detail problems utilizing transparency-based diffusion ECCV 2024 98.2 3DSR [66] Depth-aware discrete autoencoder and the simulated depth generation process WACV 97.8 M3DM [44] A hybrid fusion scheme to reduce the disturbance between multimodal features and encourage feature interaction CVPR 2023 94.5 AST [67] Introduce a network which compensates for wrongly estimated likelihoods by a normalizing flow WACV 93.7</p>
<p>3D Generation R3D-AD [68] Overcome the inefficiencies due to the memory bank module and low performance caused by incorrect rebuilds with MAE ECCV 2024 73.4 Real 3D-AD Reg 3D-AD [96] A dual-feature representation approach to preserve the training prototypes' local and global features NeurIPS 70.4 Real 3D-AD PointCore [69] Reduce the computational cost and mismatching disturbance in inference ARXIV 82.9 Real 3D-AD Uni-3DAD [70] Notable adaptability to model-free industrial products ARXIV -MVTec 3D-AD Group3AD [71] Enhance the resolution and accuracy of 3D anomaly detection through group level feature contrastive learning ACM MM 2024 75.1 Real 3D-AD SAM teacher, and SAM-LAD [19] employs FeatUp's [104] Upsampling Factors.And both STLM [17] and SAM-LAD [19] do not use foundation models during inference.Based on the above analysis, FM methods demonstrate strong potential in complex industrial detection and crossdomain applications, thanks to their powerful multi-modal capabilities and large parameter sizes.However, challenges such as overfitting in data-scarce scenarios and inference efficiency remain bottlenecks, particularly when handling 3D high-dimensional data, which still offers ample room for exploration.NFM methods, on the other hand, rely on targeted feature extraction and efficient computation, making them more advantageous in real-time inference and industrial scenarios with limited computational resources.</p>
<p>III. FOUNDATION MODEL METHODS</p>
<p>In recent years, visual-language models have shown significant advantages in anomaly detection.These models are able to better understand and describe complex features in images by effectively combining visual information with linguistic cues.Compared to traditional anomaly detection methods, visual-language models are able to exploit rich contextual information and reduce the dependence on manual annotation and domain knowledge, thus achieving more accurate detection.In this part, three main classes of methods based on visual-language models are introduced: methods based on SAM, CLIP and GPT.In Table 1, we give a summary and overview of different methods based on FM.And in Figure 4, the most important and popular works along the FM development are shown in the timeline.</p>
<p>A. 2D SAM-Based Methods</p>
<p>As a foundation model, the Segment Anything Model (SAM) [15], [105], [106] has a powerful ability to extract high-quality segmentation masks.By leveraging large-scale pre-training data, it can perform instance segmentation on any object in various scenarios without the need for taskspecific training.Consequently, SAM-based methods demonstrate good performance in zero-shot anomaly detection tasks.Cao et al. [18] utilize SAM and cascading prompt-guided object detection models [107] to construct a vanilla baseline, i.e., Segment Any Anomaly (SAA).SAA generates preliminary anomaly regions through simple language prompts such as "defect" or "anomaly," followed by a refinement process.They further introduce a mixed prompt regularization technique, enhancing the framework into Segment Any Anomaly+ (SAA+).To better process the masks generated by SAM, Li et al. propose ClipSAM [21], which combines the strengths of both CLIP and SAM.ClipSAM uses CLIP's semantic understanding capabilities for anomaly localization and rough 2024.12|CLIP-FSACTwo-stage training strategy, visual-driven text features, fusion-text matching task.</p>
<p>2024.11|STLM</p>
<p>Utilize SAM as a teacher to guide student networks.</p>
<p>2024.06|CLIP3D-AD</p>
<p>Address both few-shot anomaly classification and segmentation without memory banks and plenty of training samples.</p>
<p>2024.10|PointAD</p>
<p>Hybrid representation learning framework.</p>
<p>2023.05|SAA+</p>
<p>Hybrid prompt regularization.</p>
<p>2023.03|WinCLIP</p>
<p>Compositional prompt ensemble, reference association method.</p>
<p>2024.01|ClipSAM</p>
<p>CLIP and SAM Collaboration, unified multi-scale cross-modal interaction, multi-level mask refinement.</p>
<p>2023.08|AnomalyGPT</p>
<p>Lightweight and visual-textual feature-matching based decoder, prompt embeddings.</p>
<p>2024.10|Reg 3D-AD</p>
<p>A dual-feature representation approach to preserve the training prototypes' local and global features.</p>
<p>2024.07|R3D-AD</p>
<p>Overcome the inefficiencies due to the memory bank module and low performance caused by incorrect rebuilds with MAE.</p>
<p>2023.12|AnomalyDiffusion</p>
<p>Spatial anomaly embedding, adaptive attention re-weighting mechanism.</p>
<p>2024.04|AnomalyXFusion</p>
<p>Multi-modal anomaly synthesis for enhanced sample fidelity.</p>
<p>2022.10|AST</p>
<p>Introduce a network which compensates for wrongly estimated likelihoods by a normalizing flow.</p>
<p>2022.03|Back to the Future</p>
<p>Handcrafted 3D representations with PatchCore.</p>
<p>2023.11|Transfusion</p>
<p>Address the overgeneralization and loss-of-detail problems utilizing transparency-based diffusion.</p>
<p>2023.03|CPMF</p>
<p>Combine handcrafted PCD descriptions with pre-trained 2D neural networks.Fig. 4. Representative methods along the development of FM and NFM models.The orange box illustrates the evolution of FM methods.WinCLIP introduced the use of prompt ensemble and multi-scale feature extraction with CLIP.Subsequently, SAA+ and Anomaly GPT incorporated SAM and GPT techniques, fostering the exploration of cross-modal approaches exemplified by ClipSAM.3D FM methods emerged later, with CLIP3D-AD and PointAD focusing on addressing inconsistencies in multimodal data.Recently, 2D FM methods have achieved improvements in inference speed and accuracy, such as STLM based on a teacher-student framework and CLIP-FSAC employing vision-driven textual strategies.The green box presents the progression of NFM methods.The early method Back to the Future proposed handcrafted 3D representations but suffered from low efficiency and accuracy.Diffusion-based approaches, including TransFusion, AnomalyDiffusion, and AnomalyXFusion, effectively addressed these issues.In recent years, 3D generative techniques have been explored, with efforts concentrated on enhancing computational and storage efficiency.segmentation, then using the results as prompt constraints for SAM to refine the segmentation outcomes.In SAM-LAD [19], Peng et al. introduce SAM to obtain object masks of the query and reference images.Each object mask is multiplied with the entire image's feature map to obtain object feature maps, which are then used for object matching.Building on this, an Anomaly Measurement Model (AMM) is proposed to detect logical and structural anomalies.In UCAD [20], Liu et al. use SAM to enhance anomaly detection through Structurebased Contrastive Learning (SCL).By treating SAM-generated masks as structure, features within the same mask are drawn closer together, while others are pushed apart.This improves feature representation for anomaly detection.Li et al. [17] propose a SAM-guided Two-stream Lightweight Model (STLM), prioritizing efficiency and mobile compatibility.One stream extracts features to distinguish normal from anomalous regions, while the other reconstructs anomaly-free images to enhance differentiation.With a shared mask decoder and feature aggregation, STLM delivers precise anomaly maps.</p>
<p>In SPT [22], Yang et al. introduced a Visual-Relation-Aware Adapter (VRA-Adapter) to help SAM better understand the relationships between different regions in the image, enhancing SAM's fine-grained understanding of anomaly patterns.</p>
<p>B. 2D CLIP-Based Methods</p>
<p>CLIP-based methods use large-scale pre-trained visuallanguage models that combine image coding with textual cues, thereby strengthening the relationship between visual features and linguistic information, and perform particularly well in zero-and few-shot scenarios.</p>
<p>1) Text Prompt: Jeong et al. [23] propose a window-level anomaly detection method called WinCLIP, which achieves zero-shot anomaly segmentation [108] through combinatorial prompt ensemble and multi-scale feature extraction.Zhou et al. [24] design an object-agnostic text prompt template to capture anomalous regions in an image by learning generic normality and abnormality prompts [103] and combining global and local contextual information.APRIL-GAN [31] combines a text prompt integration strategy with a linear layer [109], [110] to improve the performance of zero and few-shot anomaly detection.SimCLIP reduces the reliance on handcrafted prompts by introducing multi-level visual adapters with implicit prompt tuning.Qu et al. [26] use visual contextual prompts to activate CLIP's anomaly semantic capability, eliminating the need for product-specific prompts.Cao et al. [25] propose to optimise zero-shot anomaly detection performance by combining static and dynamic learnable prompts.Li et al. [32] propose to convert normal prompts into anomaly prompts via semantic connectivity to build a large number of negative samples for prompt learning.</p>
<p>2) Fine-Grained Alignment: Gu et al. [33] improve the accuracy of anomaly localization by fine-grained local descriptions and optimised visual encoders.Zhang et al. [34] propose to improve the accuracy of anomaly detection by using dual-image as visual references.Zuo et al. [29] propose that the performance of few-shot anomaly classification can be effectively improved by a two-stage training strategy and an image-to-text cross-attention module.Chen et al. [28] propose to achieve fine-grained alignment through representative vector selection and a staged dual-path model.FiLo uses fine-grained description and high-quality localization to improve the accuracy and interpretability of anomaly detection.SAA+ [18] achieves more accurate anomaly localization through promptguided object detection and refinement techniques.ClipSAM [21] improves zero-shot anomaly segmentation by combining the strengths of CLIP and SAM [111], and leverages the semantic understanding capability of CLIP for anomaly localization and fine-grained segmentation [112].Hu et al. propose a hierarchical freezing window self-attention mechanism [113] that captures features at different levels by combining multilevel adapters for fine-grained localization [114].</p>
<p>C. 2D GPT-Based Methods</p>
<p>GPT-based methods exploit the advantages of large-scale language models in natural language understanding and adaptive learning to support the anomaly detection task by generating concrete textual descriptions, while being able to adapt their processing strategies to changing detection environments and anomaly types based on real-time input.</p>
<p>AnomalyGPT [35] is the first to apply Large Vision-Language Models (LVLMs) [115], [116] to anomaly detection tasks and supports multiple rounds of conversations, demonstrating excellent few-shot learning capabilities.Subsequently, Cao et al. [36] and xu et al. [37] explore how to use LVLMs for general anomaly detection tasks across various domains.At the same time, they incorporate information from different modalities, such as domain knowledge, class context, and reference images as prompts to improve LVLMs' detection performance.Myriad [38] reduces the reliance on labelled data by combining visual experts with a large-scale multimodal model.Zhu et al. [39] propose a run-time prompt adaptation strategy to generate informative anomaly prompts, which combined with a fine-grained aligner can achieve accurate anomaly localization and enhance the dynamic adaptability of the model, making it more useful in diverse industrial scenarios.Zhang et al. [40] explore the potential of Visual Question Answering (VQA)-oriented GPT-4V (ision) in anomaly detecion [36], introducing a GPT-4V-AD framework that integrates Granular Region Division, Prompt Designing, and Text2Segmentation.LogiCode [41] fully leverages the reasoning capabilities of LLMs.It extracts logical relationships from normal images and generates executable Python code to automatically detect logical anomalies in test images.The system also provides the specific location and detailed explanation of the anomalies.The innovative framework of LogiCode breaks through traditional anomaly detection methods, offering a more intelligent solution.</p>
<p>D. 3D CLIP-Based Methods</p>
<p>In contrast to traditional 2D CLIP models that primarily process RGB images, 3D CLIP handles three-dimensional data-point clouds-which encompass more complex spatial structures and geometric information.Currently, research in 3D anomaly detection is less developed compared to its 2D counterpart, largely because CLIP was initially trained on 2D RGB images paired with text.Consequently, 3D CLIP faces challenges in integrating point cloud data with images in a multimodal framework.Recent studies have made significant progress in overcoming the modality gap in 3D data processing, employing techniques such as multimodal noise reduction, multi-view processing and fusion of 3D data, as well as the integration of zero-shot learning to improve performance.</p>
<p>Zuo et al. [42] proposed a multi-view fusion module that integrates 2D image features from different perspectives, thereby enhancing the representation capability of point cloud data and overcoming the challenges posed by modality differences when processing 3D point clouds directly.PointAD [43] achieves zero-shot 3D anomaly detection by rendering 3D point clouds from multiple views into 2D images [117], and then jointly optimizing 2D and 3D features through Hybrid Representation Learning.M3DM-NR [44] significantly improves data quality and reduces noise interference through a three-stage multimodal noise removal method.It leverages pre-trained CLIP and Point-BIND models, and employs multiscale feature comparison and weighting to enhance the quality of training samples and improve the overall data purity.</p>
<p>IV. NON-FOUNDATION MODEL METHODS</p>
<p>Unlike large-scale model-based methods that depend on extensive pretraining and complex multimodal fusion techniques, lightweight models improve detection accuracy through optimized architectures, feature extraction techniques, and computational efficiency.These methods are particularly suited for resource-limited scenarios that require fast inference, offering significant benefits for real-world deployment in industrial environments.This section will discuss the four main types of current lightweight model methods: statistical approaches, anomaly synthesis strategies, detection methods combining 2D RGB images with 3D point clouds, and 3D generation techniques.The methods discussed in this section can be used as references for FM methods in the future, such as statistics-related methods, generation model, and data synthesis.Table 1 also shows the summary and overview of different methods based on NFM. Figure 4 presents the main timeline of NFM development.</p>
<p>A. Statistics-Related Methods</p>
<p>The statistical methods provide an effective theoretical foundation for improving the performance of anomaly detection models.Zhang et al. [45] propose a mixed normal Dice loss to improve the Dice loss.This loss function imposes a large penalty when the model predicts false positives, thus prioritizing the prevention of such incorrect predictions.Bae et al. [46] propose the PNI algorithm to address the impact of location and neighborhood information on the distribution of normal features.This algorithm employs a conditional probability based on neighborhood features, using a Multi-Layer Perceptron (MLP) network to model the distribution of normal features.Additionally, the method effectively captures positional information by constructing histograms of representative features at each location.LYU et al. [47] consider variations in local feature density and propose the Local Density K-Nearest Neighbors (LDKNN) method to reduce the density bias in patch-level features.COAD [49] views overfitting as a controllable mechanism that enhances sensitivity to anomalies through controlled overfitting.It introduces the Aberrance Retention Quotient (ARQ) metric to precisely quantify the degree of overfitting, thereby identifying an optimal "golden overfitting interval" (the optimal ARQ) to optimize anomaly detection performance.BGAD [48] designs a boundary-guided semi-push-pull (BG-SPP) loss.First, it generates an explicit boundary by learning the normal sample feature distribution.Based on this, it pulls together normal samples while pushing away anomalous samples, thereby strengthening the decision boundary.BGAD enables the model to effectively distinguish between seen and unseen anomalies using only a small number of anomalous samples.However, the scarcity of anomalous samples may still lead to inefficient feature learning, and BGAD does not fully address this key issue.</p>
<p>B. Anomaly Synthesis Strategies</p>
<p>Anomaly synthesis strategies aim to enhance the performance of anomaly detection models by generating diverse and realistic abnormal samples.Broadly, anomaly synthesis strategies can be categorized into the following types:</p>
<p>1) Generative Models: Based on Denoising Diffusion Probabilistic Models (DDPM), Zhang et al. [52] introduce additional noise in the reverse diffusion process to control the intensity of the generated anomalous samples.Besides, Hu et al. [54] aggregate multiple modality features and integrate them into a unified embedding space, optimizing modality alignment.They then facilitate controlled generation through adaptive adjustments of the embedding based on diffusion steps.Jiang et al. [53] enhance the controllability of anomaly generation through fine-tuning a ControlNet model with text prompts and binary masks.Hu et al. [55] propose Anoma-lyDiffusion, which uses a Latent Diffusion Model (LDM) to generate anomalous images.It combines spatial anomaly embedding with an adaptive attention mechanism to improve the alignment between the generated anomalies and their corresponding masks.Li et al. [51] build upon the Blended Latent Diffusion Model (BLDM) [118] with several innovations.They design a novel 'defect trimap' to delineate the object masks and defect regions in generated images.They also introduce a cascaded 'editing' stage in latent and pixel spaces to ensure structural coherence and detail fidelity.Additionally, they propose an online adaptation of the image encoder to further enhance image quality.Duan et al. [56] train a dataefficient StyleGAN2 on defect-free images as the backbone.Then, they add defect-aware residual blocks to generate defect masks and manipulate the features within the masked regions, generating new defect images.</p>
<p>2) Data Augmentation Techniques: Based on normal features, Chen et al. [50] guide Gaussian noise through gradient ascent and truncated projection to synthesize weak anomalies around normal points.Besides, they create binary masks using Perlin noise and combine them with external textures to synthesize strong anomalies that are further away from normal points.Zhang et al. [57] generate anomalous images using Perlin noise and use them as input for the student network.By training the student network to remove the synthetic anomalous noise, they enhance the student network's ability to represent features of anomalous samples, thereby improving the performance of the teacher-student framework in anomaly detection.Qin et al. [58] introduce semantic information for the generation of anomalous samples.They utilize LayerCAM to extract salient features from images and conduct clustering to identify the most significant regions.Subsequently, they select similar patch pairs and swap their positions.The negative samples generated in this way are more subtle yet realistic.Lin et al. [59] develope a comprehensive anomaly simulation framework that combines reconstruction strategies for both transparent and opaque anomalies.By using selective augmentation and segmentation-based training strategies, they address the challenges of anomaly generation diversity, reconstruction quality, and overfitting.Bai et al. [60] discover that small anomalies become more noticeable in the frequency domain.By transforming spatial images into multi-frequency representations, the discriminator learns joint representations between normal images and pseudoanomalies, thereby improving the performance of few-shot anomaly detection.PBAS [61] first learns a compact distribution of normal sample features with center constraints as an approximate decision boundary, which is used to guide the direction of feature-level anomaly synthesis.Then, it performs binary classification between the synthesized anomalies and normal features, further optimizing the decision boundary to ensure that the synthesized anomalies do not overlap with normal samples.</p>
<p>C. Methods Combining 2D RGB and 3D Point Clouds</p>
<p>The method of combining 2D RGB images with 3D point clouds improves the detection capabilities of traditional approaches, which are often limited by the lack of data from a single modality.This is done by fusing features from both modalities: the rich color and texture features of 2D RGB images and the spatial and geometric information provided by 3D point clouds.</p>
<p>Chu et al. propose a shape-guided expert-based learning framework that employs two expert models to detect anomalies in 3D structure and color appearance, respectively, and locates defects in test samples using a dual memory bank and shapeguided reasoning method.The model utilizes neural implicit functions (NIFs) [119] to represent local shapes and refines the complex structure of point clouds through signed distance fields, enabling point-level anomaly prediction.This significantly improves the accuracy of anomaly localization while reducing computational and memory costs.CPMF [63] generates pseudo-2D representations by projecting point clouds onto 2D and extracts semantic features using a pre-trained 2D neural network.These features complement 3D local features extracted from handcrafted point cloud descriptors and are unified into a global semantic and local geometric point cloud representation through feature alignment and fusion modules.Horwitz et al. [64] highlighted that 3D methods are currently outperformed by 2D methods and proposed a solution combining rotation-invariant handcrafted feature representations with deep learning-based color features to improve 3D anomaly detection performance.TransFusion [65] addresses the overgeneralization and detail loss issues by iteratively increasing the transparency of anomalous regions and gradually replacing them with the normal appearance while preserving the normal appearance of non-anomalous regions.Zavrtanik et al. [66] introduced 3DSR, where DADA learns a universal discrete latent space that jointly models RGB and depth data.3DSR performs discriminative anomaly detection in the feature space learned by DADA.M3DM constructs three separate memory banks for RGB, 3D, and fused features and performs anomaly detection by considering decisions from these memory banks through Decision Layer Fusion (DLF).To better align 3D point cloud features with 2D RGB features, Point Feature Alignment (PFA) was introduced.Rudolph et al. [67] presented the Asymmetric Student-Teacher Network (AST), which employs a normalized flow for density estimation as the teacher network and a conventional feedforward network as the student network, solving the issue of insufficient output differences for anomalous data caused by similar student and teacher architectures in previous methods.</p>
<p>D. 3D Generation Methods</p>
<p>3D generative techniques use generative models to reconstruct normal samples or missing regions, aiming to reduce computational overhead and improve model robustness, particularly addressing the challenges of model-free products and the difficulty in identifying missing regions.</p>
<p>Zhou et al. [68] employed a diffusion model-based data distribution transformation to completely mask abnormal geometries in the input, learning gradual displacement during the reverse diffusion process and explicitly controlling the reconstruction of abnormal shapes.Additionally, they proposed a 3D anomaly simulation strategy called Patch-Gen, designed to generate realistic defect shapes and bridge the gap between training and testing data.R3D-AD addresses challenges in 3D anomaly detection related to computational storage overhead and the detection of unmasked region anomalies.PointCore requires only a single memory bank to store local (coordinate) and global (PointMAE) representations, assigning different priorities to these local-global features to reduce computational costs and mitigate feature misalignment during inference.A ranking-based normalization method is used to eliminate distribution discrepancies between different anomaly scores, while the Iterative Closest Point (ICP) algorithm is applied to locally optimize point cloud registration results, enhancing decision robustness.Liu et al. [70] proposed a dual-branch structure where the feature-based branch and reconstructionbased branch detect surface defects and missing regions, respectively, with the latter incorporating Generative Adversarial Network Inversion (GAN-Inversion) for the first time to generate normal samples most similar to the input, thereby reducing false positives.Zhu et al. [71] introduced the Intercluster Uniformity Network (IUN) and Intra-cluster Alignment Network (IAN), which respectively achieve inter-cluster dispersion and intra-cluster alignment in feature space, enhancing the uniformity and consistency of features.Moreover, the adaptive group center selection design focuses on regions with potential issues, prioritizing areas with significant local geometric changes, thereby improving the model's sensitivity.</p>
<p>E. Conclusion and Outlooks</p>
<p>This paper reviews the methodologies in industrial defect detection, focusing on FM approaches.Section 1 introduces the challenges posed by FM methods.In Section 2, we compare FM and NFM systematically.Section 3 reviews FM methods for 2D and 3D defect detection, while Section 4 summarizes NFM approaches.</p>
<p>Despite progress, several challenges remain, and further exploration is needed in the following areas:</p>
<p> Improving Detection Accuracy on Single-Scene Datasets: While FM show impressive generalization across diverse scenarios, there is still a need to optimize their performance on specific scene datasets.Enhancing accuracy for a given dataset requires refining model finetuning processes, incorporating scene-specific features, and exploring specialized training techniques, such as transfer learning or domain adaptation.Further investigation into balancing model generalization and overfitting on limited datasets will be crucial to improving singlescene detection accuracy. Increasing Inference Speed in Few-Shot and Zero-Shot Scenarios: FM, due to their extensive parameters, face challenges in inference speed, particularly in fewshot or zero-shot learning contexts.Speed improvement strategies, such as knowledge distillation, quantization, and model pruning, hold promise.Moreover, methods for optimizing inference, like efficient transfer of learned knowledge from large datasets to smaller ones or leveraging feature extraction techniques, could be explored to accelerate inference while maintaining accuracy. Enhancing 3D Detection Performance: The performance of large models in 3D defect detection remains suboptimal, especially in single-scene scenarios.Improving 3D detection requires incorporating advanced 3D data processing methods, such as multi-view fusion, improved point cloud processing, and novel geometric feature extraction techniques.Additionally, coupling these methods with large models could enhance their ability to detect anomalies in complex 3D environments, where context and spatial relationships play a critical role. Synthetic Data for Specific 3D Scenarios: Synthetic data generation, particularly for specific 3D industrial environments, could significantly boost FM performance in these scenarios.By generating diverse, realistic 3D defect samples through simulation or augmentation techniques, we can alleviate data scarcity and improve model robustness.Exploring the synergy between synthetic data and large models, especially in underrepresented or highly specialized 3D defect scenarios, could provide new avenues for training and fine-tuning defect detection models in real-world applications.It is our hope that this survey provides a systematic summary and offers inspiration to readers for conducting research in related fields.</p>
<p>Methods</p>
<p>Paper URL</p>
<p>Fig. 2 .
2
Fig. 2. A summary of the comparison between FM and NFM methods.We conduct a systematic comparison of the FM and NFM methods from the following 5 aspects: 1) Model Training Objectives.2) Model Structure.3) Model Scale.4) Model Performance (AUROC Performance, Inference Time, and Computational Complexity).5) Advantages and Challenges.</p>
<p>TABLE I A
I
BRIEF SUMMARY AND OVERVIEW OF DIFFERENT FM AND NFM METHODS.THE NUMBERS OF PERFORMANCE ARE ALL COPIED FROM THEIR ORIGINAL PAPER.SPECIFICALLY, "PERFORMANCE" DENOTES THE AUROC METRIC ON THE DATASET SHOWN BEHIND.
CategorySub-categoryMethodDescriptionPublicationPerformanceClipSAM [21]Hierarchical mask refinement with multi-level promptsARXIV92.3UCAD [20]Structure-based contrastive learning with SAMAAAI 202493.02D SAM BasedSAM-LAD [19]FoundationMVTec ADModelMethod</p>
<p>TABLE II A
II
COLLECTION OF PUBLISHED PAPERS AND CODES FOR FM METHODS.</p>
<p>Code URL Framework ClipSAM[21]https://arxiv.org/pdf/2401.12665APPENDIX RESOURCESWe collect open-source information for FM and NFM methods, including the paper URL, code address (Github), and deep learning tools.Table2and Table3present the summarized information for FM and NFM methods respectively.
Deep learning for anomaly detection: A review. G Pang, C Shen, L Cao, A V D Hengel, ACM computing surveys (CSUR). 202154</p>
<p>Improving unsupervised defect segmentation by applying structural similarity to autoencoders. P Bergmann, S Lwe, M Fauser, D Sattlegger, C Steger, arXiv:1807.020112018arXiv preprint</p>
<p>Memorizing normality to detect anomaly: Memoryaugmented deep autoencoder for unsupervised anomaly detection. D Gong, L Liu, V Le, B Saha, M R Mansour, S Venkatesh, A V D Hengel, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Unsupervised two-stage anomaly detection. Y Liu, C Zhuang, F Lu, arXiv:2103.116712021arXiv preprint</p>
<p>Anomaly detection via reverse distillation from one-class embedding. H Deng, X Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Simplenet: A simple network for image anomaly detection and localization. Z Liu, Y Zhou, Y Xu, Z Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202320411</p>
<p>Tocoad: Two-stage contrastive learning for industrial anomaly detection. Y Liang, Z Hu, J Huang, D Di, A Su, L Fan, arXiv:2407.013122024arXiv preprint</p>
<p>A comprehensive survey on contrastive learning. H Hu, X Wang, Y Zhang, Q Chen, Q Guan, Neurocomputing. 1286452024</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Spotthe-difference self-supervised pre-training for anomaly detection and segmentation. Y Zou, J Jeong, L Pemula, D Zhang, O Dabeer, European Conference on Computer Vision. Springer2022</p>
<p>Real-iad: A real-world multi-view dataset for benchmarking versatile industrial anomaly detection. C Wang, W Zhu, B.-B Gao, Z Gan, J Zhang, Z Gu, S Qian, M Chen, L Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202422892</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, J Chen, X Shen, X Li, M Elhoseiny, arXiv:2304.105922023arXiv preprint</p>
<p>The dawn of lmms: Preliminary explorations with gpt-4v (ision). Z Yang, L Li, K Lin, J Wang, C.-C Lin, Z Liu, L Wang, arXiv:2309.17421202391arXiv preprint</p>
<p>Segment anything. A Kirillov, E Mintun, N Ravi, H Mao, C Rolland, L Gustafson, T Xiao, S Whitehead, A C Berg, W.-Y Lo, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Advancements in point cloud-based 3d defect detection and classification for industrial systems: A comprehensive survey. A Rani, D Ortiz-Arroyo, P Durdevic, arXiv:2402.129232024arXiv preprint</p>
<p>A sam-guided two-stream lightweight model for anomaly detection. C Li, L Qi, X Geng, ACM Transactions on Multimedia Computing, Communications and Applications. 2024</p>
<p>Segment any anomaly without training via hybrid prompt regularization. Y Cao, X Xu, C Sun, Y Cheng, Z Du, L Gao, W Shen, arXiv:2305.107242023arXiv preprint</p>
<p>Samlad: Segment anything model meets zero-shot logic anomaly detection. Y Peng, X Lin, N Ma, J Du, C Liu, C Liu, Q Chen, arXiv:2406.006252024arXiv preprint</p>
<p>Unsupervised continual anomaly detection with contrastively-learned prompt. J Liu, K Wu, Q Nie, Y Chen, B.-B Gao, Y Liu, J Wang, C Wang, F Zheng, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Clipsam: Clip and sam collaboration for zero-shot anomaly segmentation. S Li, J Cao, P Ye, Y Ding, C Tu, T Chen, arXiv:2401.126652024arXiv preprint</p>
<p>Promptable anomaly segmentation with sam through self-perception tuning. H.-Y Yang, H Chen, A Wang, K Chen, Z Lin, Y Tang, P Gao, Y Quan, J Han, G Ding, arXiv:2411.172172024arXiv preprint</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. J Jeong, Y Zou, T Kim, D Zhang, A Ravichandran, O Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202319616</p>
<p>Anomalyclip: Objectagnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, arXiv:2310.189612023arXiv preprint</p>
<p>Adaclip: Adapting clip with hybrid learnable prompts for zero-shot anomaly detection. Y Cao, J Zhang, L Frittoli, Y Cheng, W Shen, G Boracchi, European Conference on Computer Vision. Springer2025</p>
<p>Vcp-clip: A visual context prompting model for zero-shot anomaly segmentation. Z Qu, X Tao, M Prasad, F Shen, Z Zhang, X Gong, G Ding, arXiv:2407.122762024arXiv preprint</p>
<p>Simclip: Refining image-text alignment with simple prompts for zero-/few-shot anomaly detection. C Deng, H Xu, X Chen, H Xu, X Tu, X Ding, Y Huang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Clip-ad: A language-guided staged dual-path model for zeroshot anomaly detection. X Chen, J Zhang, G Tian, H He, W Zhang, Y Wang, C Wang, Y Liu, International Joint Conference on Artificial Intelligence. Springer2024</p>
<p>Clip-fsac: Boosting clip for few-shot anomaly classification with synthetic anomalies. Z Zuo, Y Wu, B Li, J Dong, Y Zhou, L Zhou, Y Qu, Z Wu, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-242024</p>
<p>Sowa: Adapting hierarchical frozen window self-attention to visual-language models for better anomaly detection. Z Hu, Z Zhang, arXiv:2407.036342024arXiv preprint</p>
<p>April-gan: A zero-/few-shot anomaly classification and segmentation method for cvpr 2023 vand workshop challenge tracks 1&amp;2: 1st place on zero-shot ad and 4th place on fewshot ad. X Chen, Y Han, J Zhang, arXiv:2305.173822023arXiv preprint</p>
<p>Promptad: Learning prompts with only normal samples for few-shot anomaly detection. X Li, Z Zhang, X Tan, C Chen, Y Qu, Y Xie, L Ma, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202416848</p>
<p>Filo: Zero-shot anomaly detection by fine-grained description and highquality localization. Z Gu, B Zhu, G Zhu, Y Chen, H Li, M Tang, J Wang, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Dual-image enhanced clip for zero-shot anomaly detection. Z Zhang, H Deng, J Bao, X Li, arXiv:2405.047822024arXiv preprint</p>
<p>Anomalygpt: Detecting industrial anomalies using large vision-language models. Z Gu, B Zhu, G Zhu, Y Chen, M Tang, J Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Towards generic anomaly detection and understanding: Large-scale visual-linguistic model (gpt-4v) takes the lead. Y Cao, X Xu, C Sun, X Huang, W Shen, arXiv:2311.027822023arXiv preprint</p>
<p>Customizing visuallanguage foundation models for multi-modal anomaly detection and reasoning. X Xu, Y Cao, Y Chen, W Shen, X Huang, arXiv:2403.110832024arXiv preprint</p>
<p>Myriad: Large multimodal model by applying vision experts for industrial anomaly detection. Y Li, H Wang, S Yuan, M Liu, D Zhao, Y Guo, C Xu, G Shi, W Zuo, arXiv:2310.190702023arXiv preprint</p>
<p>Do llms understand visual anomalies? uncovering llm's capabilities in zero-shot anomaly detection. J Zhu, S Cai, F Deng, B C Ooi, J Wu, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Gpt-4v-ad: Exploring grounding potential of vqa-oriented gpt-4v for zero-shot anomaly detection. J Zhang, H He, X Chen, Z Xue, Y Wang, C Wang, L Xie, Y Liu, International Joint Conference on Artificial Intelligence. Springer2024</p>
<p>Logicode: an llm-driven framework for logical anomaly detection. Y Zhang, Y Cao, X Xu, W Shen, arXiv:2406.046872024arXiv preprint</p>
<p>Clip3d-ad: Extending clip for 3d few-shot anomaly detection with multi-view images generation. Z Zuo, J Dong, Y Wu, Y Qu, Z Wu, arXiv:2406.189412024arXiv preprint</p>
<p>Pointad: Comprehending 3d anomalies from points and pixels for zero-shot 3d anomaly detection. Q Zhou, J Yan, S He, W Meng, J Chen, arXiv:2410.003202024arXiv preprint</p>
<p>M3dm-nr: Rgb-3d noisy-resistant industrial anomaly detection via multimodal denoising. C Wang, H Zhu, J Peng, Y Wang, R Yi, Y Wu, L Ma, J Zhang, arXiv:2406.022632024arXiv preprint</p>
<p>Small object few-shot segmentation for vision-based industrial inspection. Z Zhang, C Niu, Z Zhao, X Zhang, X Chen, arXiv:2407.213512024arXiv preprint</p>
<p>Pni: Industrial anomaly detection using position and neighborhood information. J Bae, J.-H Lee, S Kim, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Reb: Reducing biases in representation for industrial anomaly detection. S Lyu, D Mo, W Wong, Knowledge-Based Systems. 2901115632024</p>
<p>Explicit boundary guided semi-push-pull contrastive learning for supervised anomaly detection. X Yao, R Li, J Zhang, J Sun, C Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202324499</p>
<p>Friend or foe? harnessing controllable overfitting for anomaly detection. L Qian, B Zhu, Y Chen, M Tang, J Wang, arXiv:2412.005602024arXiv preprint</p>
<p>A unified anomaly synthesis strategy with gradient ascent for industrial anomaly detection and localization. Q Chen, H Luo, C Lv, Z Zhang, European Conference on Computer Vision. Springer2025</p>
<p>A novel approach to industrial defect generation through blended latent diffusion model with online adaptation. H Li, Z Zhang, H Chen, L Wu, B Li, D Liu, M Wang, arXiv:2402.193302024arXiv preprint</p>
<p>Realnet: A feature selection network with realistic synthetic anomaly for anomaly detection. X Zhang, M Xu, X Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202416708</p>
<p>Cagen: Controllable anomaly generator using diffusion model. B Jiang, Y Xie, J Li, N Li, Y Jiang, S.-T Xia, ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE2024</p>
<p>Anomalyxfusion: Multi-modal anomaly synthesis with diffusion. J Hu, Y Huang, Y Lu, G Xie, G Jiang, Y Zheng, Z Lu, arXiv:2404.194442024arXiv preprint</p>
<p>Anomalydiffusion: Few-shot anomaly image generation with diffusion model. T Hu, J Zhang, R Yi, Y Du, X Chen, L Liu, Y Wang, C Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Few-shot defect image generation via defect-aware feature manipulation. Y Duan, Y Hong, L Niu, L Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Destseg: Segmentation guided denoising student-teacher for anomaly detection. X Zhang, S Li, X Li, P Huang, J Shan, T Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Multilevel saliency-guided selfsupervised learning for image anomaly detection. J Qin, C Gu, J Yu, C Zhang, Signal, Image and Video Processing. 2024</p>
<p>A comprehensive augmentation framework for anomaly detection. J Lin, Y Yan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Dual-path frequency discriminators for few-shot anomaly detection. Y Bai, J Zhang, Y Dong, G Tian, Y Cao, Y Wang, C Wang, arXiv:2403.041512024arXiv preprint</p>
<p>Progressive boundary guided anomaly synthesis for industrial anomaly detection. Q Chen, H Luo, H Gao, C Lv, Z Zhang, IEEE Transactions on Circuits and Systems for Video Technology. 2024</p>
<p>Shapeguided dual-memory learning for 3d anomaly detection. Y.-M Chu, C Liu, T.-I Hsieh, H.-T Chen, T.-L Liu, International Conference on Machine Learning. PMLR2023</p>
<p>Complementary pseudo multimodal feature for point cloud anomaly detection. Y Cao, X Xu, W Shen, Pattern Recognition. 1561107612024</p>
<p>Back to the feature: classical 3d features are (almost) all you need for 3d anomaly detection. E Horwitz, Y Hoshen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Transfusion-a transparencybased diffusion model for anomaly detection. M Fuka, V Zavrtanik, D Skoaj, European conference on computer vision. Springer2025</p>
<p>Cheating depth: Enhancing 3d surface anomaly detection via depth simulation. V Zavrtanik, M Kristan, D Skoaj, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>Asymmetric student-teacher networks for industrial anomaly detection. M Rudolph, T Wehrbein, B Rosenhahn, B Wandt, Proceedings of the IEEE/CVF winter conference on applications of computer vision. the IEEE/CVF winter conference on applications of computer vision2023</p>
<p>R3d-ad: Reconstruction via diffusion for 3d anomaly detection. Z Zhou, L Wang, N Fang, Z Wang, L Qiu, S Zhang, European Conference on Computer Vision. Springer2025</p>
<p>Pointcore: Efficient unsupervised point cloud anomaly detector using local-global features. B Zhao, Q Xiong, X Zhang, J Guo, Q Liu, X Xing, X Xu, arXiv:2403.018042024arXiv preprint</p>
<p>Uni-3dad: Gan-inversion aided universal 3d anomaly detection on model-free products. J Liu, S Mou, N Gaw, Y Wang, arXiv:2408.162012024arXiv preprint</p>
<p>Towards high-resolution 3d anomaly detection via group-level feature contrastive learning. H Zhu, G Xie, C Hou, T Dai, C Gao, J Wang, L Shen, Proceedings of the 32nd ACM International Conference on Multimedia. the 32nd ACM International Conference on Multimedia2024</p>
<p>Speech motion anomaly detection via cross-modal translation of 4d motion fields from tagged mri. X Liu, F Xing, J Zhuo, M Stone, J L Prince, G El Fakhri, J Woo, Medical Imaging 2024: Image Processing. SPIE202412926129262W</p>
<p>Self-supervised feature adaptation for 3d industrial anomaly detection. Y Tu, B Zhang, L Liu, Y Li, J Zhang, Y Wang, C Wang, C Zhao, European Conference on Computer Vision. Springer2025</p>
<p>Learning a cross-modality anomaly detector for remote sensing imagery. J Li, X Wang, H Zhao, Y Zhong, IEEE Transactions on Image Processing. 2024</p>
<p>Evaluating saliency scores in point clouds of natural environments by learning surface anomalies. R Arav, D Wittich, F Rottensteiner, arXiv:2408.144212024arXiv preprint</p>
<p>Po3ad: Predicting point offsets toward better 3d point cloud anomaly detection. J Ye, W Zhao, X Yang, G Cheng, K Huang, arXiv:2412.126172024arXiv preprint</p>
<p>Network anomaly traffic detection via multi-view feature fusion. S Hao, W Fu, X Chen, C Jin, J Zhou, S Yu, Q Xuan, arXiv:2409.080202024arXiv preprint</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Niener, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. M A Uy, Q.-H Pham, B.-S Hua, T Nguyen, S.-K Yeung, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Enhanced anomaly detection in well log data through the application of ensemble gans. A Al-Fakih, A Koeshidayatullah, T Mukerji, S I Kaka, arXiv:2411.198752024arXiv preprint</p>
<p>Anomaly detection using diffusion-based methods. A Bhosale, S Mukherjee, B Banerjee, F Cuzzolin, arXiv:2412.075392024arXiv preprint</p>
<p>Rethinking reconstruction-based graph-level anomaly detection: Limitations and a simple remedy. S Kim, S Y Lee, F Bu, S Kang, K Kim, J Yoo, K Shin, arXiv:2410.203662024arXiv preprint</p>
<p>Glad: Towards better reconstruction with global and local adaptive diffusion models for unsupervised anomaly detection. H Yao, M Liu, Z Yin, Z Yan, X Hong, W Zuo, European Conference on Computer Vision. Springer2025</p>
<p>Dcor: Anomaly detection in attributed networks via dual contrastive learning reconstruction. H Rafiee Zade, H Zare, M Ghassemi Parsa, H Davardoust, M Shariat Bagheri, 20242412arXiv e-prints</p>
<p>Revisiting deep feature reconstruction for logical and structural industrial anomaly detection. S Patra, S B Taieb, arXiv:2410.162552024arXiv preprint</p>
<p>Gdflow: Anomaly detection with ncde-based normalizing flow for advanced driver assistance system. K Lee, M Kim, Y Jun, S S Woo, arXiv:2409.053462024arXiv preprint</p>
<p>Vq-flow: Taming normalizing flows for multi-class anomaly detection via hierarchical vector quantization. Y Zhou, X Xu, Z Sun, J Song, A Cichocki, H T Shen, arXiv:2409.009422024arXiv preprint</p>
<p>A survey on anomaly detection with few-shot learning. J Chen, C Wang, Y Hong, R Mi, L.-J Zhang, Y Wu, H Wang, Y Zhou, International Conference on Cognitive Computing. Springer2024</p>
<p>Memoryless multimodal anomaly detection via student-teacher network and signed distance learning. Z Sun, X Li, Y Li, Y Ma, Electronics. 131939142024</p>
<p>Structural teacher-student normality learning for multi-class anomaly detection and localization. H Deng, X Li, arXiv:2402.170912024arXiv preprint</p>
<p>Filter or compensate: Towards invariant representation from distribution shift for anomaly detection. Z Chen, X Luo, W Wang, Z Zhao, F Su, A Men, arXiv:2412.101152024arXiv preprint</p>
<p>Unlocking the potential of reverse distillation for anomaly detection. X Liu, J Wang, B Leng, S Zhang, arXiv:2412.075792024arXiv preprint</p>
<p>Visual anomaly detection via partition memory bank module and error estimation. P Xing, Z Li, IEEE Transactions on Circuits and Systems for Video Technology. 202333</p>
<p>An automated data mining framework using autoencoders for feature extraction and dimensionality reduction. Y Liang, X Li, X Huang, Z Zhang, Y Yao, arXiv:2412.022112024arXiv preprint</p>
<p>Anovl: Adapting visionlanguage models for unified zero-shot anomaly localization. H Deng, Z Zhang, J Bao, X Li, arXiv:2308.159392023arXiv preprint</p>
<p>Real3d-ad: A dataset of point cloud anomaly detection. J Liu, G Xie, R Chen, X Li, J Wang, Y Liu, C Wang, F Zheng, Advances in Neural Information Processing Systems. 202436</p>
<p>Draem-a discriminatively trained reconstruction embedding for surface anomaly detection. V Zavrtanik, M Kristan, D Skoaj, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2021</p>
<p>Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. J Yu, Y Zheng, X Wang, W Li, Y Wu, R Zhao, L Wu, arXiv:2111.076772021arXiv preprint</p>
<p>Towards total recall in industrial anomaly detection. K Roth, L Pemula, J Zepeda, B Schlkopf, T Brox, P Gehler, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202214328</p>
<p>Multimodal industrial anomaly detection via hybrid fusion. Y Wang, J Peng, J Zhang, R Yi, Y Wang, C Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. T Schlegl, P Seebck, S M Waldstein, G Langs, U Schmidt-Erfurth, Medical image analysis. 542019</p>
<p>Learning to prompt for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, International Journal of Computer Vision. 13092022</p>
<p>Conditional prompt learning for vision-language models. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022825</p>
<p>Featup: A model-agnostic framework for features at any resolution. S Fu, M Hamilton, L Brandt, A Feldman, Z Zhang, W T Freeman, arXiv:2403.105162024arXiv preprint</p>
<p>Collaborative discrepancy optimization for reliable image anomaly localization. Y Cao, X Xu, Z Liu, W Shen, IEEE Transactions on Industrial Informatics. 19112023</p>
<p>Industrial image anomaly localization based on gaussian clustering of pretrained feature. Q Wan, L Gao, X Li, L Wen, IEEE Transactions on Industrial Electronics. 6962021</p>
<p>Grounding dino: Marrying dino with grounded pre-training for open-set object detection. S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang, Q Jiang, C Li, J Yang, H Su, European Conference on Computer Vision. Springer2025</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PMLR2021</p>
<p>Focal loss for dense object detection. T.-Y Ross, G Dollr, 2017</p>
<p>V-net: Fully convolutional neural networks for volumetric medical image segmentation. F Milletari, N Navab, S.-A Ahmadi, 2016 fourth international conference on 3D vision (3DV. Ieee2016</p>
<p>Sam-clip: Merging vision foundation models towards semantic and spatial understanding. H Wang, P K A Vasu, F Faghri, R Vemulapalli, M Farajtabar, S Mehta, M Rastegari, O Tuzel, H Pouransari, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Cris: Clip-driven referring image segmentation. Z Wang, Y Lu, Q Li, X Tao, Y Guo, M Gong, T Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022695</p>
<p>Less is more: on the over-globalizing problem in graph transformers. Y Xing, X Wang, Y Li, H Huang, C Shi, arXiv:2405.011022024arXiv preprint</p>
<p>Dualcoop: Fast adaptation to multi-label recognition with limited annotations. X Sun, P Hu, K Saenko, Advances in Neural Information Processing Systems. 202235</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M Lachaux, T Lacroix, B Rozire, N Goyal, E Hambro, F Azhar, 10.48550/arXiv20232302Preprint at arXiv</p>
<p>Pointclip: Point cloud understanding by clip. R Zhang, Z Guo, W Zhang, K Li, X Miao, B Cui, Y Qiao, P Gao, H Li, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition202210695</p>
<p>Surface reconstruction from point clouds by learning predictive context priors. B Ma, Y.-S Liu, M Zwicker, Z Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Can multimodal large language models be guided to improve industrial anomaly detection. Z Chen, H Chen, M Imani, F Imani, arXiv:2501.157952025arXiv preprint</p>
<p>Kanoclip: Zero-shot anomaly detection through knowledge-driven prompt learning and enhanced cross-modal integration. C Li, S Zhou, J Kong, L Qi, H Xue, arXiv:2501.037862025arXiv preprint</p>
<p>Patch-aware vector quantized codebook learning for unsupervised visual defect detection. Q Cheng, S Qu, J Lee, 2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI). </p>
<p>S Wang, Y Hu, X Liu, S Wang, G Wang, C Xu, J Liu, P Chen, arXiv:2501.15211https://arxiv.org/pdf/2308.15939v2 - - AnomalyCLIP [24-TABLE III A COLLECTION OF PUBLISHED PAPERS AND CODES FOR NFM METHODS. Methods Paper URL Code URL Framework SOFS. 202518arXiv preprintZero-shot anomaly image synthesis via cross-domain anomaly injection. PyTorch BGAD [48] https://arxiv.org/pdf/2207.01463 https://github.com/xcyao00/BGAD PyTorch COAD [49] https://arxiv.org/pdf/2412.06510 --GLASS [50] https://arxiv.org/pdf/2407.09359 https://github.com/cqylunlun/GLASS PyTorch AdaBLDM [51] https://arxiv.org/pdf/2402.19330 https://github.com/GrandpaXun242/AdaBLDM.git PyTorch RealNet [52] https://arxiv.org/pdf/2403.05897 https://github.com/cnulab/RealNet PyTorch CAGEN [53. PyTorch Back to the Feature [64</p>            </div>
        </div>

    </div>
</body>
</html>
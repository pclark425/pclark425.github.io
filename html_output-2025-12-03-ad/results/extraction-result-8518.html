<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8518 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8518</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8518</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-151.html">extraction-schema-151</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <p><strong>Paper ID:</strong> paper-d671d62a1eb4d57343e4a0928297266dffc0c118</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d671d62a1eb4d57343e4a0928297266dffc0c118" target="_blank">SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> SwiftSage is introduced, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks, and significantly outperforms other methods such as SayCan, ReAct, and Reflexion in solving complex interactive tasks.</p>
                <p><strong>Paper Abstract:</strong> We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8518.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8518.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWIFTSAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWIFTSAGE: A Generative Agent with Fast and Slow Thinking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dual-process agent that combines a small behavior-cloned encoder-decoder LM (SWIFT) for fast, frequent decisions and a prompted LLM planner/grounder (SAGE) for deliberate subgoal planning, memory-augmented reasoning, and exception handling; integrates the two with heuristic switching and an action buffer to improve efficiency and robustness in long-horizon text-based interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SWIFTSAGE</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Dual-module agent: SWIFT (flan-T5-large, fine-tuned for action prediction with a multi-hop sliding window) handles routine action generation; SAGE (prompted LLM planner + grounding stages) is invoked under heuristics to plan subgoals, handle exceptions, and produce multi-step action buffers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary); GPT-3.5-turbo evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-4 is used as the LLM for SAGE's planning and grounding stages (instruction-tuned, large-capacity model). Paper also reports evaluations using GPT-3.5-turbo showing degraded but still usable performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>A textual interactive benchmark of 30 science experiment task types featuring long-horizon planning, 10 locations, 200+ object types with states, and ~25 action templates; requires subgoal decomposition, long-term memory, spatial reasoning, and exception handling.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid: short-term/working memory (sliding window and action history), episodic memory augmentation (objects from previously visited locations), and an explicit short action-buffer (planned multi-step actions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>SWIFT: a sliding window of the K=10 most recent actions/observations/rewards plus a de-duplicated visited-rooms field; SAGE: prompt-level condensed action history (A_<t and O_<t), augmented listing of objects observed in previously visited locations and location tags per action; SAGE outputs answers to planning queries (Q1–Q5) and a grounding-stage action buffer B = {Â_t, Â_{t+1}, ...}; the buffer is stored externally and consumed sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory fields are concatenated into structured textual input: SWIFT receives multi-hop history and visited rooms as explicit fields; SAGE receives a condensed history and augmented memory (objects from past rooms and location annotations) as part of the prompt; the SAGE planning output (Q1–Q5) is reused as input to grounding to produce an action buffer; heuristic controllers trigger SAGE only when certain conditions occur (stuck, invalid prediction, critical decision, unexpected observation), minimizing LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>84.68 overall score on ScienceWorld (GPT-4 SAGE + SWIFT; Table 1); subgroup scores: Short 92.22, Medium 77.79, Long 83.0. Cost metrics: 757.07 tokens-per-action (tpa) and 2.73 scores-per-action (spa) (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>49.22 overall score for SWIFT-only (behavior cloning without SAGE-planning; note SWIFT-only still uses a local sliding-window history) — used as the primary ablation/baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Direct comparisons: SWIFTSAGE vs SWIFT-only (shows large gain: 49.22 -> 84.68 overall); small-scale ablation within SAGE: including answers to Q1–Q3 in the grounding stage (vs only Q4/Q5) yields ~+2 points on short tasks; comparisons to LLM baselines REACT, REFLEXION, and SAYCAN show SWIFTSAGE is both more effective and more token-efficient. Sensitivity analysis: replacing GPT-4 with GPT-3.5-turbo reduces SWIFTSAGE performance (overall 62.22) but degrades other LLM methods more sharply (e.g., REACT drops much more).</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>A hybrid approach: keep a multi-hop sliding-window in a cheap, fine-tuned LM for frequent decisions; augment LLM prompts with explicit episodic information (objects from previously visited rooms and location-tagged action history) and reuse planning outputs between planning and grounding; return multi-step action buffers to amortize LLM cost; only invoke the LLM planner under heuristic triggers (stuck/invalid/critical/unexpected).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SWIFT can repeat meaningless actions when encountering exceptions; LLMs sometimes generate unparseable or invalid actions (these are dropped and may cause fallbacks); SAGE reliance on large closed-source LLMs raises cost and scalability concerns; some tasks remain imperfect (e.g., tasks 9-2 and 1-3); evaluation confined to a textual simulator (ScienceWorld).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use a dual-process design: a lightweight, fine-tuned LM with explicit short-term history for most time steps, and a memory-augmented LLM planner used sparingly for subgoal decomposition and exception handling; present objects from previously visited locations and location tags to the LLM, decouple planning and grounding (Q1–Q5 then action templates), produce multi-action buffers to reduce per-action LLM calls, and trigger LLM planning with clear heuristics (stuck/invalid/critical/unexpected). Including auxiliary planning outputs (Q1–Q3) in grounding helps ~2 points on short tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8518.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8518.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SWIFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SWIFT module / SWIFT-only agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small encoder–decoder LM (flan-T5-large, 770M) fine-tuned via behavior cloning on oracle trajectories; uses a multi-hop sliding window (K=10) of recent actions/observations/rewards plus a visited-rooms field to provide short-term memory and fast action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SWIFT-only</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Behavior cloning seq2seq agent that inputs Task D, Time, Score, a sliding window of the last K=10 action(+reward)->observation entries, current room, inventory, and visited rooms, and decodes the next action.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>flan-T5-large (770M)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Instruction-tuned encoder-decoder Transformer with ~770M parameters; fine-tuned on ~62k action-prediction examples for this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Text-based science experiment tasks requiring long-horizon planning; SWIFT is evaluated as a baseline and as SWIFT-only agent.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working/short-term memory (multi-hop sliding window of recent interactions + visited-rooms set)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Sliding window of K=10 previous actions/observations/rewards concatenated into the input, plus a de-duplicated 'visited rooms' field; these structured textual fields are presented to the seq2seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Concatenate multi-hop history and visited rooms as structured textual input to the seq2seq LM; SWIFT decodes the next action directly from this context.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>49.22 overall score as SWIFT-only baseline (reported in paper text and ablations vs SWIFTSAGE).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared to larger TDT (11B) and other baselines; SWIFT (770M) outperforms TDT due to balanced data and longer history window. Compared to SWIFTSAGE, SWIFT-only lacks SAGE-driven planning and exception handling, leading to lower final scores and tendency to repeat meaningless actions on exceptions.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Use a multi-hop sliding window (K=10) and explicit visited-rooms field to improve short-term accuracy and reduce redundant navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles to generalize to unseen situations, repeats actions when encountering exceptions, and lacks mechanisms to repair mistaken actions because behavior cloning imitates shortest oracle trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Retain sliding-window short-term memory for fast, frequent decisions but pair with an LLM planner for robust exception handling and long-horizon subgoal planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8518.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8518.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAGE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAGE module (planning and grounding with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM-based deliberate-thinking module: (1) a planning stage that answers structured questions Q1–Q5 about needed objects, subgoals, progress, and exceptions using condensed history and augmented episodic knowledge; (2) a grounding stage that converts planned subgoals into a multi-action 'action buffer' using action templates and recent history.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SAGE (module within SWIFTSAGE)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-driven planner/grounder invoked by SWIFTSAGE under heuristics; Planning: prompts LLM with condensed history and asks Q1–Q5; Grounding: feeds Q1–Q5 answers, past 10-step history, and action templates to produce a sequence of executable actions (the action buffer).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (primary); GPT-3.5-turbo evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Large instruction-tuned transformer used for planning and grounding; GPT-4 yields best results in the paper, GPT-3.5 yields lower but robust performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same ScienceWorld benchmark; SAGE is responsible for planning/grounding subgoals across tasks with exceptions and long horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic memory augmentation + short-term action buffer</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Prompt-level condensed action history (A_<t and O_<t) plus explicit augmentation of objects observed in previously visited locations and per-action location tags; outputs to be reused (Q1–Q5) and an action buffer B of multi-step actions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Memory is provided directly in the LLM prompt (condensed history + augmented episodic object lists); planning outputs (answers to Q1–Q5) are fed into grounding; grounding yields an action buffer which is stored and sequentially executed until empty, after which control returns to SWIFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Contributes to SWIFTSAGE's 84.68 overall (GPT-4). The paper reports SAGE usage reduces tokens-per-action versus per-step LLM-only baselines and improves final scores substantially when combined with SWIFT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablation: including Q1–Q3 answers in grounding (vs only Q4/Q5) yields ~+2 points for short tasks; SAGE two-stage + action-buffer yields much lower tpa (757.07) than per-step LLM methods (SayCan/ReAct/Reflexion). Compared with REFLEXION and REACT, SAGE handles exceptions on-the-fly and grounds subgoals into executable multi-action sequences more effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td>Prompt-augment with objects from previously visited rooms and include location tags for actions; decouple planning (Q1–Q5) and grounding; produce a multi-step action buffer instead of single actions; selectively invoke SAGE via heuristic triggers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLM outputs may include invalid/unparseable action types (dropped and cause fallback); invoking SAGE twice per invocation increases LLM usage but is amortized by multi-step buffers; reliance on closed-source LLMs may limit scalability.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Use memory augmentation (objects from visited rooms and location-tagged history) and decouple planning and grounding; produce multi-action buffers to amortize LLM calls; selectively invoke the planner for exceptions and critical decisions to balance cost and performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8518.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8518.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REFLEXION</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-based agent that records histories of failed attempts and uses multi-round self-reflection (dynamic memory across trials) to refine future plans; used as a baseline in this paper and compared against SWIFTSAGE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REFLEXION</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Baseline LLM agent that performs multi-round runs: after a failed attempt it stores the failure trace and uses self-reflection on that dynamic memory to improve planning in subsequent rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used in experiments); GPT-3.5-turbo also considered in sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>LLM used to perform self-reflection and re-planning across rounds; in experiments, REFLEXION can run several rounds and uses the failed-round memory to adjust future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Evaluated as an LLM baseline on ScienceWorld tasks; REFLEXION leverages trial-level memory of failures to improve subsequent attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic trial-level memory (episodic across attempts) and self-reflection logs</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Stores failed trial histories and feeds them into subsequent LLM prompts as context for reflection and plan revision; multi-round mechanism rather than single-step in-context memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>LLM uses memory of previous failed rounds to refine planning on the next round; the approach relies on multiple full runs and new LLM inferences per time step/round.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>45.34 overall score (GPT-4; Table 1). REFLEXION outperforms REACT on some short tasks but is not directly comparable in fairness because REFLEXION is allowed multiple rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with REACT, SAYCAN, and SWIFTSAGE; REFLEXION can run up to four rounds which makes direct comparisons unfair for single-round methods; the paper notes REFLEXION helps on short tasks but has practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multi-round trial-and-error may be impractical in real-world settings where actions are irreversible; expensive due to multiple LLM inferences and rounds; fairness issues when comparing to single-round agents.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Reflection across failed trials can improve planning but is less practical for embodied or irreversible tasks; SWIFTSAGE prefers on-the-fly detection and correction (Q5) instead of only end-of-trial reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8518.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8518.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REACT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that interleaves reasoning ('think') and acting: LLMs are taught to emit intermediate 'think' steps/subgoals (via few-shot subgoal examples) and then produce actions; used here as a baseline adapted to ScienceWorld.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REACT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent that uses virtual 'think' actions to generate subgoals during action planning; requires human-annotated subgoal examples (few-shot) and performs per-time-step LLM inference to decide next action or subgoal.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used in experiments); GPT-3.5-turbo evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Instruction-following LLM used in few-shot mode with subgoal annotations for in-context learning.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Adapted to ScienceWorld as a baseline; REACT generates 'think' tokens/subgoals and then immediate actions at each time step.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in-context working memory (history used as prompt) and few-shot subgoal examples</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Uses concatenated action and observation history and few-shot subgoal exemplars in the prompt each time step; no separate persistent external memory beyond the prompt and generation history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>History and subgoal exemplars are concatenated into the prompt at each decision step; LLM outputs may include 'think' subgoals which the system then attempts to ground into actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>36.43 overall score (GPT-4; Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared with SAYCAN, REFLEXION, and SWIFTSAGE; REACT performs competitively on shorter tasks but tends to plateau and is expensive due to per-step LLM calls; requires subgoal annotations for each task type.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires human-annotated subgoals for in-context learning; costly because it demands LLM inference each time step; grounding of generated subgoals into executable multi-step plans is weak, causing failures on long-horizon tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Subgoal 'think' actions help short tasks but need better grounding and mechanisms (like action buffers) to scale to long-horizon, exception-prone environments; fewer LLM calls with multi-action buffers are preferable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8518.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8518.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games that use memory, including details of the memory mechanism, how it is integrated, performance results with and without memory, comparisons between memory strategies, and any recommendations or conclusions about the best use of memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAYCAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that combines LLM-generated action candidates with an affordance/value function to rerank feasible/environment-grounded actions; adapted here as a baseline for ScienceWorld by ranking valid actions using SentenceBERT similarity to top LLM generations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Do as i can, not as i say: Grounding language in robotic affordances</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SayCan</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM generates a ranked list of candidate actions from the textual history and environment; a learned or heuristic value/affordance function (here, SentenceBERT similarity over valid actions) reranks candidates to select executable actions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (used in experiments); GPT-3.5-turbo also considered</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Large instruction-tuned LLM used to generate candidate actions given context; in experiments, generations are reranked by an affordance/value function.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ScienceWorld</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as LLM baseline on ScienceWorld; requires concatenated history and environment prompt to generate candidates that are then filtered/reranked against the valid action set.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in-context working memory (prompt history + current environment)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_architecture</strong></td>
                            <td>Concatenates history and current environment into the LLM prompt; generates candidate actions which are then matched to the simulator's valid actions and reranked using SentenceBERT-similarity to the top generations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_integration_strategy</strong></td>
                            <td>Use prompt history + environment text as input to LLM; rerank valid actions via an affordance/value function derived from embeddings (SentenceBERT) and select the top feasible action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>33.82 overall score (GPT-4; Table 1); high tokens-per-action (1855.84 tpa) compared to SWIFTSAGE.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against REACT, REFLEXION, and SWIFTSAGE; SWIFTSAGE outperforms SayCan substantially and is more token-efficient due to selective LLM invocation and action buffers.</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>High per-action LLM cost because it invokes the LLM every time step; poor scaling to long-horizon tasks where multi-step planning and grounding are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_conclusions</strong></td>
                            <td>Reranking LLM candidates with affordances helps grounding but should be combined with multi-step planning and memory-augmentation to handle longer tasks more efficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Keep calm and explore: Language models for action generation in text-based games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8518",
    "paper_id": "paper-d671d62a1eb4d57343e4a0928297266dffc0c118",
    "extraction_schema_id": "extraction-schema-151",
    "extracted_data": [
        {
            "name_short": "SWIFTSAGE",
            "name_full": "SWIFTSAGE: A Generative Agent with Fast and Slow Thinking",
            "brief_description": "A dual-process agent that combines a small behavior-cloned encoder-decoder LM (SWIFT) for fast, frequent decisions and a prompted LLM planner/grounder (SAGE) for deliberate subgoal planning, memory-augmented reasoning, and exception handling; integrates the two with heuristic switching and an action buffer to improve efficiency and robustness in long-horizon text-based interactive tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SWIFTSAGE",
            "agent_description": "Dual-module agent: SWIFT (flan-T5-large, fine-tuned for action prediction with a multi-hop sliding window) handles routine action generation; SAGE (prompted LLM planner + grounding stages) is invoked under heuristics to plan subgoals, handle exceptions, and produce multi-step action buffers.",
            "llm_model_name": "GPT-4 (primary); GPT-3.5-turbo evaluated",
            "llm_model_description": "GPT-4 is used as the LLM for SAGE's planning and grounding stages (instruction-tuned, large-capacity model). Paper also reports evaluations using GPT-3.5-turbo showing degraded but still usable performance.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "A textual interactive benchmark of 30 science experiment task types featuring long-horizon planning, 10 locations, 200+ object types with states, and ~25 action templates; requires subgoal decomposition, long-term memory, spatial reasoning, and exception handling.",
            "memory_used": true,
            "memory_type": "hybrid: short-term/working memory (sliding window and action history), episodic memory augmentation (objects from previously visited locations), and an explicit short action-buffer (planned multi-step actions)",
            "memory_architecture": "SWIFT: a sliding window of the K=10 most recent actions/observations/rewards plus a de-duplicated visited-rooms field; SAGE: prompt-level condensed action history (A_&lt;t and O_&lt;t), augmented listing of objects observed in previously visited locations and location tags per action; SAGE outputs answers to planning queries (Q1–Q5) and a grounding-stage action buffer B = {Â_t, Â_{t+1}, ...}; the buffer is stored externally and consumed sequentially.",
            "memory_integration_strategy": "Memory fields are concatenated into structured textual input: SWIFT receives multi-hop history and visited rooms as explicit fields; SAGE receives a condensed history and augmented memory (objects from past rooms and location annotations) as part of the prompt; the SAGE planning output (Q1–Q5) is reused as input to grounding to produce an action buffer; heuristic controllers trigger SAGE only when certain conditions occur (stuck, invalid prediction, critical decision, unexpected observation), minimizing LLM calls.",
            "performance_with_memory": "84.68 overall score on ScienceWorld (GPT-4 SAGE + SWIFT; Table 1); subgroup scores: Short 92.22, Medium 77.79, Long 83.0. Cost metrics: 757.07 tokens-per-action (tpa) and 2.73 scores-per-action (spa) (Table 4).",
            "performance_without_memory": "49.22 overall score for SWIFT-only (behavior cloning without SAGE-planning; note SWIFT-only still uses a local sliding-window history) — used as the primary ablation/baseline in the paper.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Direct comparisons: SWIFTSAGE vs SWIFT-only (shows large gain: 49.22 -&gt; 84.68 overall); small-scale ablation within SAGE: including answers to Q1–Q3 in the grounding stage (vs only Q4/Q5) yields ~+2 points on short tasks; comparisons to LLM baselines REACT, REFLEXION, and SAYCAN show SWIFTSAGE is both more effective and more token-efficient. Sensitivity analysis: replacing GPT-4 with GPT-3.5-turbo reduces SWIFTSAGE performance (overall 62.22) but degrades other LLM methods more sharply (e.g., REACT drops much more).",
            "best_memory_strategy": "A hybrid approach: keep a multi-hop sliding-window in a cheap, fine-tuned LM for frequent decisions; augment LLM prompts with explicit episodic information (objects from previously visited rooms and location-tagged action history) and reuse planning outputs between planning and grounding; return multi-step action buffers to amortize LLM cost; only invoke the LLM planner under heuristic triggers (stuck/invalid/critical/unexpected).",
            "limitations_or_failure_cases": "SWIFT can repeat meaningless actions when encountering exceptions; LLMs sometimes generate unparseable or invalid actions (these are dropped and may cause fallbacks); SAGE reliance on large closed-source LLMs raises cost and scalability concerns; some tasks remain imperfect (e.g., tasks 9-2 and 1-3); evaluation confined to a textual simulator (ScienceWorld).",
            "recommendations_or_conclusions": "Use a dual-process design: a lightweight, fine-tuned LM with explicit short-term history for most time steps, and a memory-augmented LLM planner used sparingly for subgoal decomposition and exception handling; present objects from previously visited locations and location tags to the LLM, decouple planning and grounding (Q1–Q5 then action templates), produce multi-action buffers to reduce per-action LLM calls, and trigger LLM planning with clear heuristics (stuck/invalid/critical/unexpected). Including auxiliary planning outputs (Q1–Q3) in grounding helps ~2 points on short tasks.",
            "uuid": "e8518.0",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SWIFT",
            "name_full": "SWIFT module / SWIFT-only agent",
            "brief_description": "A small encoder–decoder LM (flan-T5-large, 770M) fine-tuned via behavior cloning on oracle trajectories; uses a multi-hop sliding window (K=10) of recent actions/observations/rewards plus a visited-rooms field to provide short-term memory and fast action prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SWIFT-only",
            "agent_description": "Behavior cloning seq2seq agent that inputs Task D, Time, Score, a sliding window of the last K=10 action(+reward)-&gt;observation entries, current room, inventory, and visited rooms, and decodes the next action.",
            "llm_model_name": "flan-T5-large (770M)",
            "llm_model_description": "Instruction-tuned encoder-decoder Transformer with ~770M parameters; fine-tuned on ~62k action-prediction examples for this domain.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Text-based science experiment tasks requiring long-horizon planning; SWIFT is evaluated as a baseline and as SWIFT-only agent.",
            "memory_used": true,
            "memory_type": "working/short-term memory (multi-hop sliding window of recent interactions + visited-rooms set)",
            "memory_architecture": "Sliding window of K=10 previous actions/observations/rewards concatenated into the input, plus a de-duplicated 'visited rooms' field; these structured textual fields are presented to the seq2seq model.",
            "memory_integration_strategy": "Concatenate multi-hop history and visited rooms as structured textual input to the seq2seq LM; SWIFT decodes the next action directly from this context.",
            "performance_with_memory": "49.22 overall score as SWIFT-only baseline (reported in paper text and ablations vs SWIFTSAGE).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared to larger TDT (11B) and other baselines; SWIFT (770M) outperforms TDT due to balanced data and longer history window. Compared to SWIFTSAGE, SWIFT-only lacks SAGE-driven planning and exception handling, leading to lower final scores and tendency to repeat meaningless actions on exceptions.",
            "best_memory_strategy": "Use a multi-hop sliding window (K=10) and explicit visited-rooms field to improve short-term accuracy and reduce redundant navigation.",
            "limitations_or_failure_cases": "Struggles to generalize to unseen situations, repeats actions when encountering exceptions, and lacks mechanisms to repair mistaken actions because behavior cloning imitates shortest oracle trajectories.",
            "recommendations_or_conclusions": "Retain sliding-window short-term memory for fast, frequent decisions but pair with an LLM planner for robust exception handling and long-horizon subgoal planning.",
            "uuid": "e8518.1",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SAGE",
            "name_full": "SAGE module (planning and grounding with LLMs)",
            "brief_description": "A two-stage LLM-based deliberate-thinking module: (1) a planning stage that answers structured questions Q1–Q5 about needed objects, subgoals, progress, and exceptions using condensed history and augmented episodic knowledge; (2) a grounding stage that converts planned subgoals into a multi-action 'action buffer' using action templates and recent history.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SAGE (module within SWIFTSAGE)",
            "agent_description": "LLM-driven planner/grounder invoked by SWIFTSAGE under heuristics; Planning: prompts LLM with condensed history and asks Q1–Q5; Grounding: feeds Q1–Q5 answers, past 10-step history, and action templates to produce a sequence of executable actions (the action buffer).",
            "llm_model_name": "GPT-4 (primary); GPT-3.5-turbo evaluated",
            "llm_model_description": "Large instruction-tuned transformer used for planning and grounding; GPT-4 yields best results in the paper, GPT-3.5 yields lower but robust performance.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Same ScienceWorld benchmark; SAGE is responsible for planning/grounding subgoals across tasks with exceptions and long horizons.",
            "memory_used": true,
            "memory_type": "episodic memory augmentation + short-term action buffer",
            "memory_architecture": "Prompt-level condensed action history (A_&lt;t and O_&lt;t) plus explicit augmentation of objects observed in previously visited locations and per-action location tags; outputs to be reused (Q1–Q5) and an action buffer B of multi-step actions.",
            "memory_integration_strategy": "Memory is provided directly in the LLM prompt (condensed history + augmented episodic object lists); planning outputs (answers to Q1–Q5) are fed into grounding; grounding yields an action buffer which is stored and sequentially executed until empty, after which control returns to SWIFT.",
            "performance_with_memory": "Contributes to SWIFTSAGE's 84.68 overall (GPT-4). The paper reports SAGE usage reduces tokens-per-action versus per-step LLM-only baselines and improves final scores substantially when combined with SWIFT.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Ablation: including Q1–Q3 answers in grounding (vs only Q4/Q5) yields ~+2 points for short tasks; SAGE two-stage + action-buffer yields much lower tpa (757.07) than per-step LLM methods (SayCan/ReAct/Reflexion). Compared with REFLEXION and REACT, SAGE handles exceptions on-the-fly and grounds subgoals into executable multi-action sequences more effectively.",
            "best_memory_strategy": "Prompt-augment with objects from previously visited rooms and include location tags for actions; decouple planning (Q1–Q5) and grounding; produce a multi-step action buffer instead of single actions; selectively invoke SAGE via heuristic triggers.",
            "limitations_or_failure_cases": "LLM outputs may include invalid/unparseable action types (dropped and cause fallback); invoking SAGE twice per invocation increases LLM usage but is amortized by multi-step buffers; reliance on closed-source LLMs may limit scalability.",
            "recommendations_or_conclusions": "Use memory augmentation (objects from visited rooms and location-tagged history) and decouple planning and grounding; produce multi-action buffers to amortize LLM calls; selectively invoke the planner for exceptions and critical decisions to balance cost and performance.",
            "uuid": "e8518.2",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "REFLEXION",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A prior LLM-based agent that records histories of failed attempts and uses multi-round self-reflection (dynamic memory across trials) to refine future plans; used as a baseline in this paper and compared against SWIFTSAGE.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "use",
            "agent_name": "REFLEXION",
            "agent_description": "Baseline LLM agent that performs multi-round runs: after a failed attempt it stores the failure trace and uses self-reflection on that dynamic memory to improve planning in subsequent rounds.",
            "llm_model_name": "GPT-4 (used in experiments); GPT-3.5-turbo also considered in sensitivity",
            "llm_model_description": "LLM used to perform self-reflection and re-planning across rounds; in experiments, REFLEXION can run several rounds and uses the failed-round memory to adjust future behavior.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Evaluated as an LLM baseline on ScienceWorld tasks; REFLEXION leverages trial-level memory of failures to improve subsequent attempts.",
            "memory_used": true,
            "memory_type": "dynamic trial-level memory (episodic across attempts) and self-reflection logs",
            "memory_architecture": "Stores failed trial histories and feeds them into subsequent LLM prompts as context for reflection and plan revision; multi-round mechanism rather than single-step in-context memory.",
            "memory_integration_strategy": "LLM uses memory of previous failed rounds to refine planning on the next round; the approach relies on multiple full runs and new LLM inferences per time step/round.",
            "performance_with_memory": "45.34 overall score (GPT-4; Table 1). REFLEXION outperforms REACT on some short tasks but is not directly comparable in fairness because REFLEXION is allowed multiple rounds.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared with REACT, SAYCAN, and SWIFTSAGE; REFLEXION can run up to four rounds which makes direct comparisons unfair for single-round methods; the paper notes REFLEXION helps on short tasks but has practical limitations.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Multi-round trial-and-error may be impractical in real-world settings where actions are irreversible; expensive due to multiple LLM inferences and rounds; fairness issues when comparing to single-round agents.",
            "recommendations_or_conclusions": "Reflection across failed trials can improve planning but is less practical for embodied or irreversible tasks; SWIFTSAGE prefers on-the-fly detection and correction (Q5) instead of only end-of-trial reflection.",
            "uuid": "e8518.3",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "REACT",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A prompting method that interleaves reasoning ('think') and acting: LLMs are taught to emit intermediate 'think' steps/subgoals (via few-shot subgoal examples) and then produce actions; used here as a baseline adapted to ScienceWorld.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "REACT",
            "agent_description": "LLM-based agent that uses virtual 'think' actions to generate subgoals during action planning; requires human-annotated subgoal examples (few-shot) and performs per-time-step LLM inference to decide next action or subgoal.",
            "llm_model_name": "GPT-4 (used in experiments); GPT-3.5-turbo evaluated",
            "llm_model_description": "Instruction-following LLM used in few-shot mode with subgoal annotations for in-context learning.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Adapted to ScienceWorld as a baseline; REACT generates 'think' tokens/subgoals and then immediate actions at each time step.",
            "memory_used": true,
            "memory_type": "in-context working memory (history used as prompt) and few-shot subgoal examples",
            "memory_architecture": "Uses concatenated action and observation history and few-shot subgoal exemplars in the prompt each time step; no separate persistent external memory beyond the prompt and generation history.",
            "memory_integration_strategy": "History and subgoal exemplars are concatenated into the prompt at each decision step; LLM outputs may include 'think' subgoals which the system then attempts to ground into actions.",
            "performance_with_memory": "36.43 overall score (GPT-4; Table 1).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared with SAYCAN, REFLEXION, and SWIFTSAGE; REACT performs competitively on shorter tasks but tends to plateau and is expensive due to per-step LLM calls; requires subgoal annotations for each task type.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "Requires human-annotated subgoals for in-context learning; costly because it demands LLM inference each time step; grounding of generated subgoals into executable multi-step plans is weak, causing failures on long-horizon tasks.",
            "recommendations_or_conclusions": "Subgoal 'think' actions help short tasks but need better grounding and mechanisms (like action buffers) to scale to long-horizon, exception-prone environments; fewer LLM calls with multi-action buffers are preferable.",
            "uuid": "e8518.4",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SAYCAN",
            "name_full": "Do as i can, not as i say: Grounding language in robotic affordances",
            "brief_description": "An approach that combines LLM-generated action candidates with an affordance/value function to rerank feasible/environment-grounded actions; adapted here as a baseline for ScienceWorld by ranking valid actions using SentenceBERT similarity to top LLM generations.",
            "citation_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "mention_or_use": "use",
            "agent_name": "SayCan",
            "agent_description": "LLM generates a ranked list of candidate actions from the textual history and environment; a learned or heuristic value/affordance function (here, SentenceBERT similarity over valid actions) reranks candidates to select executable actions.",
            "llm_model_name": "GPT-4 (used in experiments); GPT-3.5-turbo also considered",
            "llm_model_description": "Large instruction-tuned LLM used to generate candidate actions given context; in experiments, generations are reranked by an affordance/value function.",
            "benchmark_name": "ScienceWorld",
            "benchmark_description": "Used as LLM baseline on ScienceWorld; requires concatenated history and environment prompt to generate candidates that are then filtered/reranked against the valid action set.",
            "memory_used": true,
            "memory_type": "in-context working memory (prompt history + current environment)",
            "memory_architecture": "Concatenates history and current environment into the LLM prompt; generates candidate actions which are then matched to the simulator's valid actions and reranked using SentenceBERT-similarity to the top generations.",
            "memory_integration_strategy": "Use prompt history + environment text as input to LLM; rerank valid actions via an affordance/value function derived from embeddings (SentenceBERT) and select the top feasible action.",
            "performance_with_memory": "33.82 overall score (GPT-4; Table 1); high tokens-per-action (1855.84 tpa) compared to SWIFTSAGE.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared against REACT, REFLEXION, and SWIFTSAGE; SWIFTSAGE outperforms SayCan substantially and is more token-efficient due to selective LLM invocation and action buffers.",
            "best_memory_strategy": null,
            "limitations_or_failure_cases": "High per-action LLM cost because it invokes the LLM every time step; poor scaling to long-horizon tasks where multi-step planning and grounding are needed.",
            "recommendations_or_conclusions": "Reranking LLM candidates with affordances helps grounding but should be combined with multi-step planning and memory-augmentation to handle longer tasks more efficiently.",
            "uuid": "e8518.5",
            "source_info": {
                "paper_title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Keep calm and explore: Language models for action generation in text-based games",
            "rating": 1
        }
    ],
    "cost": 0.02324625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SwiftSAGE: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks</h1>
<p>Bill Yuchen Lin ${ }^{1}$ Yicheng Fu ${ }^{4}$ Karina Yang ${ }^{2}$ Faeze Brahman ${ }^{13}$ Shiyu Huang ${ }^{5}$<br>Chandra Bhagavatula ${ }^{1}$ Prithviraj Ammanabrolu ${ }^{67}$ Yejin Choi ${ }^{31}$ Xiang Ren ${ }^{21}$<br>${ }^{1}$ Allen Institute for Artificial Intelligence<br>${ }^{2}$ University of Southern California ${ }^{3}$ University of Washington ${ }^{4}$ Tsinghua University<br>${ }^{5} 4$ Paradigm Inc. ${ }^{6}$ University of California, San Diego ${ }^{7}$ MosaicML<br>https://swiftsage.github.io</p>
<h4>Abstract</h4>
<p>We introduce SWIFTSAGE, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SWIFTSAGE integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the SWIFT module, representing fast and intuitive thinking, and the SAGE module, emulating deliberate thought processes. The SWIFT module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the SAGE module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SWIFTSAGE significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>The advancement of artificial general intelligence is largely dependent on the development of agents that are proficient in complex interactive reasoning tasks. These agents should be capable of exhibiting problem-solving abilities akin to humans within dynamic, open-world environments [26, 7]. For example, the ScienceWorld benchmark [36] features a task where an agent must determine the electrical conductivity of an unknown object. In a simulated environment, the agent must navigate to appropriate rooms, locate and acquire essential items, such as batteries and light bulbs, build a circuit, perform an experiment, and interpret the results. Tackling such a complex interactive task demands agents to exhibit long-horizon planning, long-term memorization, subgoal decomposition, spatial reasoning, exception handling, and commonsense knowledge capabilities [37].
There are three primary approaches to developing agents capable of addressing complex interactive reasoning tasks: (1) (deep) reinforcement learning (RL), (2) behavior cloning (BC) [34] through sequence-to-sequence (seq2seq) learning [33], and (3) prompting large language models (LLMs) [6]. In addition to conventional RL methods such as DRRN [14], interactive reasoning can be framed as a seq2seq task, where the input text serves as the current state description and the output text corresponds to the subsequent action [9, 3]. By leveraging numerous gold trajectories generated by oracle agents, it becomes feasible to fine-tune Transformer models [35], like T5 [25], to effectively imitate the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparing methods of prompting LLMs to build agents for interactive tasks.
behavior of these oracle agents. Recent studies have also demonstrated that generative agents based on prompting LLMs, such as GPT-4, can produce reasonable plans and actions [18, 15, 32].
Although the aforementioned methods exhibit remarkable performance in relatively simple tasks, their ability to generalize to more complex and demanding tasks is limited. Both RL-based and seq2seq-based BC approaches effectively acquire knowledge from the environment through largescale interactions and learn general action patterns from oracle agents. However, they face difficulties in decomposing tasks into subgoals, maintaining long-term memory, generalizing to unseen tasks, and handling exceptions. In contrast, instruction-tuned LLMs [24] demonstrate the ability to generate reasonable high-level plans for complex tasks and adapt their outputs based on human feedback. Yet, grounding their outputs to executable actions in the environment remains a challenge. These procedures also lack the capability to efficiently handle environment-specific exceptions that prevent agents from adhering to the LLM's plans. Additionally, previous methods such as SAYCAN [1], REACT [41] and REFLEXION [30] require a new inference with LLMs for each time step, making them considerably costly and inefficient (see Figure 1).
Inspired by the dual process theory [39, 16], we propose a novel framework that enables agents to closely emulate how humans solve complex, open-world tasks. The dual-process theory posits that human cognition is composed of two distinct systems: System 1, characterized by rapid, intuitive, and automatic thinking; and System 2, which entails methodical, analytical, and deliberate thought processes. System 1 is reminiscent of seq2seq methods, which learn through imitation of oracle agents and primarily operate utilizing shallow action patterns. Conversely, System 2 bears resemblance to LLMs that excel in applying commonsense knowledge, engaging in step-by-step reasoning, devising subgoal strategies, and exercising self-reflection. Thus, our proposed method, SWIFTSAGE, is designed to enable both fast and slow thinking in complex interactive reasoning tasks. It effectively integrates the strengths of behavior cloning (representing System 1) and prompting LLMs (emulating System 2), resulting in significant enhancements in task completion performance and efficiency.
Specifically, SWIFTSAGE consists of two primary modules: the SWIFT module and the SAGE module. The SWIFT module is a small encoder-decoder LM, fine-tuned on a T5-large (770m) checkpoint using the searched oracle trajectories of training tasks. It encodes short-term memory components, such as previous actions, observations, visited locations, as well as the current environment state. Then, it decodes the next individual action. This module simulates the fast, intuitive thinking characteristic of System 1. The SAGE module, representing the deliberate thinking of System 2, utilizes LLMs, such as GPT-4, and is structured around two prompting stages: planning and grounding. In the planning stage, we prompt LLMs to locate necessary items, plan and track subgoals, as well as detect and fix potential exceptions and mistakes. In the grounding stage, we focus on utilizing LLMs to transform the output subgoals derived from the planning stage into a sequence of actions by demonstrating potential action templates. Unlike prior methods, where LLMs only generate the next immediate action, our procedures engage in longer-term action planning. To harmoniously integrate the SWIFT and SAGE modules, we developed a heuristic algorithm that determines when to (de)activate the SAGE module and how to combine the outputs effectively with an action buffer mechanism.</p>
<p>In a comprehensive evaluation on 30 task types from the ScienceWorld benchmark, SWIFTSAGE significantly outperforms other methods, achieving a state-of-the-art average score of 84.7. In comparison, SAYCAN scores 33.8, REACT obtains 36.4, and REFLEXION reaches 45.3. Moreover, SWIFTSAGE is more cost-effective and efficient, requiring much fewer tokens per action for LLM inference than previous methods. This considerable performance advantage highlights the effectiveness and efficiency of the SWIFTSAGE framework in addressing complex interactive tasks.</p>
<h1>2 Background and Related Work</h1>
<h3>2.1 Complex Interactive Reasoning</h3>
<p>We define interactive reasoning as the problems where agents are tasked with accomplishing a goal within an interactive environment, typically simulated by engines such as AI2Thor [17] and TextWorld [11]. Our focus lies on the textual environment of ScienceWorld [36] and the complex interactive tasks it supports. Simple interactive tasks, like those created in ALFWorld [31] and TWC [21], primarily involve searching for and placing objects as well as performing basic actions within a single location. Many of these simple tasks have been almost solved by recent works.</p>
<p>In contrast, tasks in ScienceWorld exhibit greater complexity, characterized by more challenging task planning and a significantly larger action space (encompassing 10 locations, 200 types of objects with varying states, and 25 types of actions). Furthermore, agents may encounter random, unforeseen obstacles, such as broken stoves or missing soil, which hinder the execution of planned actions. As a result, agents must adapt and re-plan accordingly, for example, by seeking alternative heat sources or using a shovel on the outside ground to get soil. These challenges demand that agents possess skills in long-horizon planning, long-term memory, subgoal decomposition, exception handling, and commonsense knowledge-capabilities that are not explicitly required for simple interactive tasks.</p>
<h3>2.2 Reinforcement Learning and Imitation Learning Methods</h3>
<p>DRRN. Interactive tasks can naturally be framed as partially-observable Markov decision processes (POMDPs), enabling the application of RL-based methods. Deep Reinforced Relevance Network (DRRN) [14] is a standard baseline method to learn agents within text environment. It aims to learn representations of observations and actions separately and train a policy network to select actions from candidates based on feedback from the simulated environment. CALM [40] is a rerankingbased method that combines DRRN with a causal language model (LM) fine-tuned with oracle transcripts. In essence, the causal LM captures task-specific and environment-specific knowledge through imitation learning, and the DRRN learns to rerank the predictions from the LM.</p>
<p>The KG-A2C [2] method uses an OpenIE technique [4] to represent environment states with graph structures and dynamically update these graphs. These graphs guide policy networks by constraining the combinations of action templates and objects. This method has been shown to be effective in other domains such as for multimodal embodied agents [22].</p>
<p>Behavior cloning for offline imitation learning. Behavior cloning is an imitation learning method that trains a seq2seq Transformer offline with action transcripts of similar training tasks generated by oracle agents [34, 3]. During training, it uses the previous action, observation at time step $t-1$, and the current observation as input and learns to output the next action. The Text Decision Transformer (TDT) is a textual variant of the Decision Transformer [9], which also employs behavior cloning and uses the same data. The primary innovation of TDT is the introduction of reward-to-go as part of the inputs, enabling the model to learn predicting actions that maximize future expected rewards.</p>
<h3>2.3 Prompting LLMs for Action Planning.</h3>
<p>Language models (LLMs) such as GPT-4 have shown promise for action planning in interactive tasks [18, 15, 32, 38]. In this paper, we adapt three prominent methods to complex interactive reasoning tasks in ScienceWorld: SAYCAN [1], REACT [41], and REFLEXION [30].
SAYCAN [1] is a straightforward agent that integrates an LLM with a value function of underlying policies regarding grounding affordances (i.e., the feasibility of an action in the environment). We</p>
<p>need to provide the history and current environment as textual inputs to LLMs for generating a ranked list of action candidates. This action list is then reranked based on a value function.
REACT [41] presents a virtual 'think' action, enabling LLMs to generate subgoals during action planning. This approach requires human annotators to supply examples of correct subgoals for each task type, employing few-shot in-context learning to teach LLMs when and how to 'think' in order to plan subsequent subgoals, in addition to providing complete action trajectories.
REFLEXION [30], a recent work building on REACT, proposes a multi-round approach enabling LLMs to use the history of previously failed rounds to refine their planning for the next round. This self-reflection mechanism helps LLMs improve after each failed attempt. However, this may not be practical in real-world applications for many tasks, as actions in failed trials can be irrecoverable.
All three methods require a new LLM inference at each time step to predict the next immediate action, resulting in inefficient and costly agents. REACT and REFLEXION require human annotations of correct subgoals for each unseen task type. Moreover, it is difficult to generalize REFLEXION to real-world situations where trial-and-error approaches can be infeasible for embodied tasks.</p>
<h1>2.4 Dual-Process Theory</h1>
<p>The dual-process theory [39, 16] is a cognitive psychological framework proposing the existence of a fast and a slow thinking systems in the human brain. This influential theory has found widespread applications across various fields, highlighting the critical role of both systems in shaping human cognition [5, 8, 12, 20, 23]. By integrating the complementary strengths of both systems, agents can effectively and efficiently handle diverse challenges in real-world scenarios. Inspired by this, we aim to construct a generative agent that utilizes a small seq2seq LM as System 1 for associative reasoning via behavior cloning while developing System 2 for analytical reasoning by prompting LLMs.</p>
<h2>3 Swiftsage: A Generative Agent with Fast and Slow Thinking</h2>
<p>In this section, we first establish the problem. Then, we present the two core modules, SWIFT and SAGE, individually. Lastly, we demonstrate the integration of these two modules., resulting in a harmonious and effective interactive reasoning process.</p>
<h3>3.1 Problem Formulation</h3>
<p>Environment and tasks. We focus on complex interactive reasoning tasks situated in virtual textual environments such as ScienceWorld [36]. ScienceWorld provides an optimal setting for developing and evaluating agents in complex tasks, comprising 30 distinct task types covering 10 topics in science experiments. It features 10 locations, including an art studio, workshop, kitchen, living room, bedroom, bathroom, foundry, greenhouse, outdoor area, and a connecting hallway. The environment includes 200+ object types with multiple states (e.g., open, activated) and supports 25 action templates, resulting in an intractable search space. The simulator can generate numerous variations of each task type, providing a rich training ground. In each variation, the agent and environment initialization, such as the locations and states of objects, will differ. A plethora of training variations encompassing all task types are available for training agents. Additionally, it provides a handcrafted oracle agent to search for successful transcripts with minimal actions for offline learning.</p>
<p>Evaluation is done on a set of testing variations with unseen combinations of required objects and situations, thus substantially different from the training variations. For example, a training variation may involve boiling water, while a testing variation could require boiling tin. Therefore, it is crucial to ensure the agent's compositional generalization ability for effectively handling real-world scenarios.</p>
<p>Interactions. Given a task variation, an agent is provided with the task description $D$ and the initial environment state $(t=0)$. The task description $D$ is a text specifying a high-level goal, e.g., "Your task is to test if an unknown substance $A$ is electronically conductive." At each time step $t$, the agent generates an action $A_{t}$ based on a set of supported action templates (e.g., pick up X, use X on Y). $A_{0}$ is always "look around" for showing initial environment information. Upon receiving an action from the agent, the environment produces feedback in four dimensions:</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An example of how SwIFTSAGE works with fast and slow thinking. The SWIFT module is offline trained via imitation learning with a small LM such as T5-large ( 770 m ). When it is necessary, for example, encountering an exception, we switch to the SAGE module that prompts LLMs (e.g., GPT-4) for planning and grounding the next subgoals, resulting in an action buffer.</p>
<ul>
<li>Observation $O_{t}$ provides direct feedback on the action $A_{t}$ regarding its effects on the environment or the information queried. For example, an $A_{t}$ of "use thermometer on the substance in metal pot" may result in an $O_{t}$ like "The temperature is 80 F ."</li>
<li>Environment $E_{t}$ represents the current room in which the agent is situated and provides details about all visible objects. Object visibility is based on container states, e.g., objects within a closed fridge are not included in $E_{t}$ until the agent performs an action like "open fridge."</li>
<li>Inventory $I_{t}$ lists objects picked up by the agent, which is particularly useful when agents collect items from different locations to complete the task.</li>
<li>Score $S_{t}$ represents the agent's cumulative score ranging from 0 to 100 . When a required intermediate state is achieved, the score increases with a positive reward.</li>
</ul>
<h1>3.2 SWIFT: The Module for Intuitive and Associative Thinking via Imitation Learning</h1>
<p>Imitation learning is used to construct an agent that learns to mimic oracle agents in various training scenarios through seq2seq learning. Previous methods, such as TDT [36], mainly employ one-hop history as input context and learn to output the subsequent action $A_{t}$ [36]. However, these methods exhibit limitations due to their restricted context of action history and harmful biases arising from data imbalance. To address these issues, we introduce our SWIFT module, depicted in Figure 2.</p>
<p>Representation for longer history. We expand the conventional one-hop BC to multi-hop by incorporating a sliding window of observations and rewards for the $K=10$ recent actions. Additionally, we include a special field for visited rooms (without duplication). This approach aims to provide agents with a longer context and prevent unnecessary room navigation. The input format is as follows: "Task: $D$; Time: $t-1$; Score: $S_{t-1}$; Action history: $\left[A_{t-i}\left(+R_{t-i}\right) \rightarrow\right.$ $\left.O_{t-i}\right] / <em>$ i loops from $K$ to $1 * /$; Current room: $E_{t-1}$; Inventory: $I_{t-1}$; Visited rooms: $\left{E_{1}^{</em>}, \ldots, E_{t-1}^{<em>}\right}$ ". Here, $R_{t}=S_{t}-S_{t-1}$ represents the reward at $t$, and $E_{t}^{</em>}$ is the location name at $t$.</p>
<p>Balanced imitation learning. To avoid bias caused by data imbalance for seq2seq learning, we down-sampled specific types of tasks and actions to achieve a more balanced final dataset for training. We used the T5-large with 770 million parameter and instruction-following ability [10], creating an efficient agent that we named SWIFT. Our empirical results show that the SWIFT module performs much better than TDT ( 11 billion) despite being 15 x smaller in size.</p>
<p>The SWIFT module exhibits greater accuracy during initial time steps, enabling it to attain higher scores in the early stages of a complex task. However, it often fails to generalize to unseen situations. The module also has a tendency to repeat meaningless actions when its learned plans yield exceptions from the environment (e.g., the broken stove in Figure 2). This is partly due to the nature of imitation learning, which prioritizes emulating the observable actions of oracle agents rather than their intrinsic planning abilities. Besides, since the oracle trajectories contain only the shortest, correct actions, it is thus also challenging for the SWIFT to learn how to fix mistaken actions.</p>
<h1>3.3 SAGE: The Module for Deliberate and Analytical Thinking via Prompting LLMs</h1>
<p>While the SWIFT module acquires surface knowledge about the environment and task types through imitation learning, it lacks two key abilities essential for complex interactive reasoning: 1) generalizable planning and tracking of subgoals, and 2) robust handling of exceptions. Prior research has shown that LLMs outperform smaller LMs in these abilities. They can perform step-by-step reasoning to devise concrete plans for tasks and self-refine their outcomes. However, the performance of prior methods remains unsatisfactory in complex interactive tasks such as those in ScienceWorld.
We introduce a novel two-stage approach, named SAGE. This method initially acquires higher-level recommendations from LLMs during the planning stage, followed by their translation into specific action sequences in the grounding stage. By decoupling the planning and grounding processes, SWIFTSAGE effectively generates a series of actions for completing the planned subgoals.</p>
<p>Planning stage. In this stage, we leverage LLMs to plan based on the current state. Specifically, we prompt LLMs with a single prompt that includes a summarized version of the task description and action history, and asks the following five key questions:</p>
<ul>
<li>Q1 (locate objects): "To complete the task, which objects do I need to collect? Please list them and their possible locations one by one."</li>
<li>Q2 (track objects): "Are there any objects that have not been collected yet?"</li>
<li>Q3 (plan subgoals): "To complete the task most efficiently, what are the important subgoals to achieve? Please list the subgoals one by one."</li>
<li>Q4 (track progress): "Considering these subgoals, what have I already completed? And which subgoal should I focus on right now?"</li>
<li>Q5 (handle exceptions): "Have I made any mistakes that might prevent me from efficiently completing the next subgoal? If any, how should I fix them?"</li>
</ul>
<p>Before posing the five planning-related questions, we condense the entire action history ( $A_{&lt;t}$ and $O_{&lt;t}$ ), and the current environment information $E_{t-1}$. Q1 and Q2 pertain to objects, as acquiring all necessary objects serves as the foundation for effective task planning. By addressing these questions, we ensure that LLMs develop a comprehensive understanding of the current environment. Q3 prompts LLMs to engage in step-by-step planning by decomposing the task into a series of subgoals. Q4 acts as a follow-up question, allowing the agent to monitor its progress based on the action history and determine completed subgoals, subsequently focusing on the remaining tasks. Lastly, Q5 is employed to identify and address potential exceptions. These questions can be further tailored with additional environment-specific hints, thereby enhancing their adaptability.
To improve the structure of the LLMs' outputs and facilitate parsing, we incorporate additional instructions in the prompt. By utilizing a single input to obtain answers to all five questions in one output, rather than engaging in multiple rounds of interactive prompting, our approach is more efficient and cost-effective than the iterative prompting methods.
Q4 and Q5 are of primary importance, while Q1-Q3 serve as auxiliary guidance for the LLMs. If the action history indicates a mistaken action or an unachievable previous subgoal, the response to Q5 refines the answer to Q4 through self-reflection on the fly. This approach differs from the REFLEXION agent, which only prompts reflective questions at the end of a failed trial, allowing agents to improve their planning in subsequent attempts. In contrast, our method detects exceptions and errors each time the agent plans for the next subgoals, enabling earlier correction of the agent's behavior.</p>
<p>Grounding stage. While the answers to Q1-Q5 provide valuable guidance for agents, they are not directly executable. Converting plans into valid actions that can be accepted by the environment</p>
<p>remains a challenge. Previous methods using LLMs over-generate candidates, and they rely on reranking or filtering based on the action space to select the next action. However, this is inefficient and inaccurate for complex tasks with vast action spaces. Additionally, these methods generate a single action at a time, which can be both costly and ineffective for long-horizon tasks.</p>
<p>To tackle these issues, we first present supported action types using a formal style accompanied by remarks. For instance, the action type "pour X into Y" is introduced as "POUR (X, Y) : pour object $X$ into container $Y$; e.g., pour red paint into wood cup". More examples:</p>
<div class="codehilite"><pre><span></span><code><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">room</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">kitchen</span><span class="p">)</span>
<span class="n">PICK</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">inventory</span>
<span class="k">OPEN</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">search</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">things</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="k">OPEN</span><span class="w"> </span><span class="p">(</span><span class="n">freezer</span><span class="p">).</span>
<span class="n">ACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">stove</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="n">DEACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">deactivate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">off</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span>
<span class="n">EXAMINE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">look</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">carefully</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">EXAMINE</span><span class="w"> </span><span class="p">(</span><span class="n">light</span><span class="w"> </span><span class="n">bulb</span><span class="p">).</span>
<span class="n">MOVE</span><span class="p">(</span><span class="k">object</span><span class="p">,</span><span class="w"> </span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">move</span><span class="o">/</span><span class="n">place</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">place</span>
</code></pre></div>

<p>We then incorporate the LLM's outputs from the planning stage as part of the input for the grounding stage. Furthermore, we provide the recent action history of the past 10 time steps as context. Finally, we prompt LLMs to concentrate on the next subgoal and convert it into a list of actions (rather than a single action) to accomplish the next subgoal. Our formatting instructions enable the straightforward splitting and conversion of output actions from LLMs in the grounding stage back to their original action representations. We denote this list of actions generated by LLMs as the action buffer: $B=\left{\hat{A}<em t_1="t+1">{t}, \hat{A}</em>, \ldots\right}$. One can opt to use only answers to Q4 and Q5 to reduce computational costs. Our small-scale ablation study indicates that incorporating answers to Q1-Q3 in the grounding stage proves beneficial, yielding a gain of about 2 points for short tasks on average.</p>
<h1>3.4 Integration of Fast and Slow Thinking</h1>
<p>Having described the SWIFT and SAGE modules, we now address the question of how to merge both modules and effectively integrate fast and slow thinking within the SWIFTSAGE agent. We establish a heuristic algorithm to control the activation and deactivation of the two modules.</p>
<p>Initially, we employ the SWIFT module due to its superior intuitive reasoning capabilities, which facilitate accurate associations between the task description and the environment during the first few actions. We will switch from SWIFT mode to SAGE when any of the following conditions are met:</p>
<p>1) Stuck: There are $\mathrm{K}=5$ consecutive time steps with zero reward $\left(\sum_{i=t-5}^{t-1} R_{i}=0\right)$.
2) Invalid: The SWIFT's prediction for the next action $\left(A_{t}^{\prime}\right)$ is invalid in the current environment.
3) Critical: $A_{t}^{\prime}$ involves a critical decision, e.g., giving the final answer for the experiment result.
4) Unexpected: The observation of $A_{t}^{\prime}$ suggests that an exception is encountered.</p>
<p>Upon activating the SAGE module, we execute the two-stage prompting process and generate an action buffer. We attempt to execute each predicted action and revert to the SWIFT module when the buffer is empty. This approach enables a seamless integration of both modules, providing an efficient and robust problem-solving process for the SWIFTSAGE agent. The pseudo code for illustrating the SwiftSage framework is shown in Fig. 4 (Appendix).</p>
<h2>4 Evaluation</h2>
<h3>4.1 Evaluation Setup</h3>
<p>To evaluate the effectiveness of SWIFTSAGE and other baseline methods in complex interactive reasoning tasks, we use the ScienceWorld benchmark. In Section 2.1 and Section 3.1, we introduce the benchmark and problem formulation. Each task type is categorized as 'short' (S), 'medium' (M), or 'long' (L) based on the average length of the oracle truth trajectories. However, the length of the task does not necessarily indicate its level of difficulty as some tasks may require additional commonsense knowledge. Further evaluation details are provided in the appendix.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">*Len</th>
<th style="text-align: center;">DRRN</th>
<th style="text-align: center;">KGA2C</th>
<th style="text-align: center;">CALM</th>
<th style="text-align: center;">TDT</th>
<th style="text-align: center;">SayCan</th>
<th style="text-align: center;">ReAct</th>
<th style="text-align: center;">Reflexion</th>
<th style="text-align: center;">SwiftSage</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1 (L)</td>
<td style="text-align: center;">107.7</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">33.06</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">4.22</td>
<td style="text-align: center;">97.04</td>
</tr>
<tr>
<td style="text-align: center;">1-2 (L)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">3.52</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">10.39</td>
<td style="text-align: center;">13.70</td>
<td style="text-align: center;">10.61</td>
<td style="text-align: center;">87.04</td>
</tr>
<tr>
<td style="text-align: center;">1-3 (L)</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">7.78</td>
<td style="text-align: center;">7.78</td>
<td style="text-align: center;">72.78</td>
</tr>
<tr>
<td style="text-align: center;">1-4 (L)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">9.88</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">2-1 (M)</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">6.56</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6.16</td>
<td style="text-align: center;">26.37</td>
<td style="text-align: center;">7.19</td>
<td style="text-align: center;">5.92</td>
<td style="text-align: center;">99.17</td>
</tr>
<tr>
<td style="text-align: center;">2-2 (M)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">5.50</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">6.43</td>
<td style="text-align: center;">8.03</td>
<td style="text-align: center;">6.10</td>
<td style="text-align: center;">28.59</td>
<td style="text-align: center;">88.17</td>
</tr>
<tr>
<td style="text-align: center;">2-3 (L)</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">19.87</td>
<td style="text-align: center;">17.41</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">22.37</td>
<td style="text-align: center;">95.73</td>
</tr>
<tr>
<td style="text-align: center;">3-1 (S)</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">40.55</td>
<td style="text-align: center;">52.14</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">88.67</td>
</tr>
<tr>
<td style="text-align: center;">3-2 (M)</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">9.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">14.26</td>
<td style="text-align: center;">22.50</td>
<td style="text-align: center;">54.33</td>
<td style="text-align: center;">17.45</td>
<td style="text-align: center;">55.33</td>
</tr>
<tr>
<td style="text-align: center;">3-3 (M)</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">10.16</td>
<td style="text-align: center;">99.56</td>
<td style="text-align: center;">76.19</td>
<td style="text-align: center;">72.54</td>
<td style="text-align: center;">71.90</td>
</tr>
<tr>
<td style="text-align: center;">3-4 (M)</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">9.52</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">21.65</td>
<td style="text-align: center;">47.76</td>
<td style="text-align: center;">88.81</td>
<td style="text-align: center;">70.22</td>
<td style="text-align: center;">77.86</td>
</tr>
<tr>
<td style="text-align: center;">4-1 (S)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">18.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">41.93</td>
<td style="text-align: center;">22.87</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">64.93</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-2 (S)</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">45.0</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">55.76</td>
<td style="text-align: center;">58.18</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">87.27</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-3 (S)</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">21.67</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">27.82</td>
<td style="text-align: center;">20.87</td>
<td style="text-align: center;">53.33</td>
<td style="text-align: center;">16.42</td>
<td style="text-align: center;">91.67</td>
</tr>
<tr>
<td style="text-align: center;">4-4 (S)</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">19.17</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">47.15</td>
<td style="text-align: center;">31.43</td>
<td style="text-align: center;">27.50</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">5-1 (L)</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">6.89</td>
<td style="text-align: center;">9.92</td>
<td style="text-align: center;">9.06</td>
<td style="text-align: center;">7.33</td>
<td style="text-align: center;">74.59</td>
</tr>
<tr>
<td style="text-align: center;">5-2 (L)</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">14.29</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">13.93</td>
<td style="text-align: center;">18.57</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">93.93</td>
</tr>
<tr>
<td style="text-align: center;">6-1 (M)</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">15.10</td>
<td style="text-align: center;">47.81</td>
<td style="text-align: center;">51.04</td>
<td style="text-align: center;">70.35</td>
<td style="text-align: center;">49.40</td>
</tr>
<tr>
<td style="text-align: center;">6-2 (S)</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">26.67</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">15.70</td>
<td style="text-align: center;">39.26</td>
<td style="text-align: center;">58.89</td>
<td style="text-align: center;">70.67</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">6-3 (M)</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">10.37</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">5.25</td>
<td style="text-align: center;">19.72</td>
<td style="text-align: center;">40.74</td>
<td style="text-align: center;">15.77</td>
<td style="text-align: center;">91.48</td>
</tr>
<tr>
<td style="text-align: center;">7-1 (S)</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">43.0</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.0</td>
</tr>
<tr>
<td style="text-align: center;">7-2 (S)</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">8.43</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">67.50</td>
<td style="text-align: center;">84.37</td>
<td style="text-align: center;">85.0</td>
</tr>
<tr>
<td style="text-align: center;">7-3 (S)</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">33.33</td>
<td style="text-align: center;">23.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.34</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">93.33</td>
</tr>
<tr>
<td style="text-align: center;">8-1 (M)</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">5.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.86</td>
<td style="text-align: center;">20.91</td>
<td style="text-align: center;">27.67</td>
<td style="text-align: center;">2.58</td>
<td style="text-align: center;">89.0</td>
</tr>
<tr>
<td style="text-align: center;">8-2 (S)</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">68.50</td>
</tr>
<tr>
<td style="text-align: center;">9-1 (L)</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">21.94</td>
<td style="text-align: center;">40.50</td>
<td style="text-align: center;">50.63</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">9-2 (L)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">32.26</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">70.0</td>
</tr>
<tr>
<td style="text-align: center;">9-3 (L)</td>
<td style="text-align: center;">123.1</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">9.12</td>
<td style="text-align: center;">13.67</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">70.62</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">10-1 (L)</td>
<td style="text-align: center;">130.1</td>
<td style="text-align: center;">16.80</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.51</td>
<td style="text-align: center;">67.53</td>
<td style="text-align: center;">25.70</td>
<td style="text-align: center;">50.90</td>
<td style="text-align: center;">92.30</td>
</tr>
<tr>
<td style="text-align: center;">10-2 (L)</td>
<td style="text-align: center;">132.1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">11.0</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">1.29</td>
<td style="text-align: center;">59.45</td>
<td style="text-align: center;">16.80</td>
<td style="text-align: center;">23.69</td>
<td style="text-align: center;">77.60</td>
</tr>
<tr>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">28.08</td>
<td style="text-align: center;">22.70</td>
<td style="text-align: center;">11.30</td>
<td style="text-align: center;">28.37</td>
<td style="text-align: center;">43.83</td>
<td style="text-align: center;">48.79</td>
<td style="text-align: center;">71.47</td>
<td style="text-align: center;">92.22</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">10.85</td>
<td style="text-align: center;">6.88</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">10.36</td>
<td style="text-align: center;">36.58</td>
<td style="text-align: center;">44.01</td>
<td style="text-align: center;">35.43</td>
<td style="text-align: center;">77.79</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">94.30</td>
<td style="text-align: center;">8.26</td>
<td style="text-align: center;">4.92</td>
<td style="text-align: center;">1.33</td>
<td style="text-align: center;">6.11</td>
<td style="text-align: center;">23.65</td>
<td style="text-align: center;">21.07</td>
<td style="text-align: center;">30.17</td>
<td style="text-align: center;">83.0</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">15.56</td>
<td style="text-align: center;">11.37</td>
<td style="text-align: center;">5.07</td>
<td style="text-align: center;">14.66</td>
<td style="text-align: center;">33.82</td>
<td style="text-align: center;">36.43</td>
<td style="text-align: center;">45.34</td>
<td style="text-align: center;">84.68</td>
</tr>
</tbody>
</table>
<p>Table 1: Results on the ScienceWorld benchmark. <em>Len is the average length of the oracle agent's trajectories. In addition to overall results, we also report performance on three groups of </em>Len (short, medium, long). The last four methods use GPT-4 as the base LLM for prompting. We show more details of these tasks and results on other LLMs in the appendix.</p>
<h1>4.2 Baseline Agents</h1>
<p>In addition to the baseline methods evaluated in the ScienceWorld paper, such as DRRN, CALM, KG-A2C, and TDT, we incorporate three LLM-based prompting techniques: SAYCAN, REACT, and Reflexion, as detailed in Section 2.3 and Figure 1. This subsection presents the implementation details for adapting these methods to build ScienceWorld agents.</p>
<p>SAYCAN necessitates a value function from the environment for reranking purposes. We employ SentenceBERT [27] to rank all valid actions (generated by ScienceWorld's APIs) based on their similarity to the top 5 generations for $A_{t}$ from SAYCAN. We implemented REACT and Reflexion in a similar manner. Adhering to their released code, we utilized the best single generation and determined the valid action with the minimal edit distance, if required. Both REACT and Reflexion necessitate subgoal annotations for teaching LLMs to plan with virtual 'think' actions. We annotated such truth subgoals by translating ScienceWorld's APIs into natural language, which was also employed by the oracle agents. For all agents, we incorporated the complete trajectories of one or two training variations from the same task type for in-context learning. Our primary experiments were conducted using OpenAI's GPT-4; however, other LLMs can be readily substituted as required.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Visualizing trajectories of SWIFTSAGE, REACT and Oracle. $X$ : time steps $(0 \rightarrow T)$; $Y$ : scores $(0 \rightarrow 100)$. Each figure displays the merged trajectories of testing variations by an agent in each task. Task IDs are shown at the bottom-right, and the ordering is based on *Len in Tab 1.</p>
<h1>4.3 Results and analysis.</h1>
<p>Main Results Table 1 compares the performance of various agents across 30 types of tasks. Detailed descriptions of each task type can be found in the ScienceWorld paper [36] and our appendix. It is evident that LLM-based methods outperform conventional agents due to their superior generalization ability, albeit at a higher deployment cost. The behavior cloning model TDT [36, 9] (11b) performs on par with DRRN [14], but with greater efficiency in learning and inference. In contrast, our SWIFTonly agent ( 770 m ) achieves an overall performance of 49.22 , which we attribute to its balanced training data and the use of a sliding window for longer action histories.
REACT demonstrates a noticeable improvement over SAYCAN for short and medium tasks, owing to its subgoal annotations for in-context learning and the inclusion of 'think' actions. REFLEXION surpasses REACT in shorter tasks; however, comparing REFLEXION with other agents is not entirely fair. REFLEXION can run up to four rounds, while the others are limited to one round. This discrepancy is particularly unfair for tasks involving multiple-choice scenarios. Nevertheless, we include REFLEXION's results to analyze the potential of such methods.</p>
<p>Exception handling. Consider the example in Figure 2, where the stove is broken, presenting an exception. Agents like DRRN and TDT often resort to repeating meaningless action sequences (e.g., continuously attempting to activate the stove or moving between rooms aimlessly). Although the SWIFT module, when used independently, improves upon this due to its larger context window from imitation learning, it still struggles to address exceptions robustly. ReAct and Reflexion occasionally utilize the 'think' action or reflections to redirect agents towards alternative solutions, but the generated actions rarely achieve the new subgoals if they are not grounded. In contrast, the plan-and-ground prompts in our SAGE module handle exceptions more effectively.</p>
<p>Cost-effectiveness. Despite SAGE invoking LLMs APIs twice for inference, its overall cost remains lower, as the result is a sequence of actions typically containing about 5 actions. In comparison, SAYCAN and REACT require $\mathbf{1 , 8 5 5 . 8 4}$ and $\mathbf{1 , 9 7 1 . 0 3}$ tokens per action (tpa) respectively, while REFLEXION necessitates $\mathbf{2 , 9 8 3 . 4 6}$ tpa. SWIFTSAGE, on the other hand, only uses $\mathbf{7 5 7 . 0 7}$ tpa. Given its superior performance, SWIFTSAGE proves more cost-effective than other LLM-based methods. This efficiency is primarily attributed to invoking LLMs only when needed (courtesy of our strong SWIFT module) and the action buffer mechanism.</p>
<p>Efficiency. To thoroughly examine the efficiency of agents across all task types, we use Figure 3 to visualize the average trajectories of the first three testing variations for each task involving</p>
<p>SWIFTSAGE, REACT, and the oracle agent. We arrange the tasks based on their average lengths of oracle trajectories (*Len in Table 1). We observe that oracle trajectories consistently achieve perfect scores, yet SWIFTSAGE can reach similar scores more efficiently. This is particularly evident in longer tasks (the bottom two rows), although SWIFTSAGE does not achieve a perfect score for a few tasks (e.g., 9-2 and 1-3). Interestingly, we find that REACT performs competitively in shorter tasks (e.g., 4-2 and 3-4), but most trajectories plateau at an intermediate score and fail to reach 100 .</p>
<p>More analysis. Due to page limit, we have to provide further details and analysis in the appendix, including more detailed analysis on cost-effectiveness and efficiency, additional case studies and ablation studies, sensitivity to LLM choices, and an the evaluation of the SWIFT-only agent.</p>
<h1>5 Conclusion</h1>
<p>Contributions. We present SWIFTSAGE, a generative agent for complex interactive reasoning tasks, inspired by the dual-process theory of human cognition. The agent framework comprises two modules: SWIFT, responsible for fast thinking, and SAGE, dedicated to slow thinking. The SWIFT module is a smaller LM that is fast and specialized, while the SAGE module focuses on prompting LLMs (e.g., GPT-4) for subgoal planning and reflective thinking. Through extensive experiments on 30 distinct tasks within the ScienceWorld benchmark, SWIFTSAGE outperforms baseline agents, achieving state-of-the-art performance, increased efficiency, and reduced cost.</p>
<p>Implications. The success of SWIFTSAGE highlights the potential for collaborative frameworks combining smaller LMs and LLMs in complex reasoning tasks. Smaller LMs can be trained more easily to recognize task-specific and environment-specific patterns, fostering effective in-distribution generalization. On the other hand, LLMs demonstrate remarkable zero-shot generalization abilities and deliberate thinking, though grounding their outputs in real-world environments remains challenging. We posit that dual-process agents, harnessing the strengths of both approaches, constitute a crucial step towards addressing complex interactive reasoning tasks and building general AI agents. Additionally, we can regard SWIFTSAGE as a method within the broader context of utilizing LLMs as controllers or planners for decomposing complex tasks and leveraging APIs/tools [19, 13, 29, 28]. To this end, we have explored applying SWIFTSAGE in web tasks and coding for math problems.</p>
<p>Limitations. Our work has been evaluated solely within a textual simulator, ScienceWorld, which supports a limited set of actions and tasks compared to real-world situations. Also, we did not implement any safeguards to prevent agents from engaging in potentially hazardous actions that could occur in the real world, such as picking up substances from a blast furnace. We argue that one important future direction is to develop a true open-ended environment, allowing agents to interact with a much wider variety of actions and objects to better emulate real-world scenarios. Besides, the use of LLMs in SAGE may present scalability challenges, as LLMs require significant computational resources and may not be feasible in some settings. Future research should explore the generalizability of SWIFTSAGE to other domains and the potential for more lightweight approaches to slow thinking. In addition, we believe it is important to train agents beyond simple supervised fine-tuning and to learn a trainable module to decide when to switch between SWIFT and SAGE mode.</p>
<h2>Acknowledgements</h2>
<p>We thank Peter Jansen, Eric Xingdi Yuan, and Marc-Alexandre Côté for valuable discussions. We thank members of the INK lab at USC and the Mosaic team at AI2 for valuable feedback on this project. Xiang Ren is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200006, the DARPA MCS program under Contract No. N660011924033, the Defense Advanced Research Projects Agency with award W911NF-19-20271, NSF IIS 2048211, and gift awards from Google and Amazon. This research was also supported by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031) and Allen Institute for AI. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government.</p>
<h1>References</h1>
<p>[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu , and Mengyuan Yan. Do as i can, not as i say: Grounding language in robotic affordances. In Conference on Robot Learning, 2022.
[2] Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. In International Conference on Learning Representations, 2020 .
[3] Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur D. Szlam, Tim Rocktaschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In North American Chapter of the Association for Computational Linguistics, 2020 .
[4] Gabor Angeli, Melvin Johnson, and Christopher D. Manning. Leveraging linguistic structure for open domain information extraction. In Annual Meeting of the Association for Computational Linguistics, 2015.
[5] Thomas W. Anthony, Zheng Tian, and David Barber. Thinking fast and slow with deep learning and tree search. ArXiv, abs/1705.08439, 2017.
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.
[7] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments with gpt-4. ArXiv, abs/2303.12712, 2023.
[8] Di Chen, Yiwei Bai, Wenting Zhao, Sebastian Ament, J. Gregoire, and Carla P. Gomes. Deep reasoning networks: Thinking fast and slow. ArXiv, abs/1906.00855, 2019.
[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, P. Abbeel, A. Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Neural Information Processing Systems, 2021.
[10] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. ArXiv, abs/2210.11416, 2022.
[11] Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben A. Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew J. Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. Textworld: A learning environment for text-based games. In CGW@IJCAI, 2018.</p>
<p>[12] M. B. Ganapini, Murray Campbell, F. Fabiano, L. Horesh, Jonathan Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, and Kristen Brent Venable. Thinking fast and slow in ai: the role of metacognition. In International Conference on Machine Learning, Optimization, and Data Science, 2021.
[13] Yingqiang Ge, Wenyue Hua, Jianchao Ji, Juntao Tan, Shuyuan Xu, and Yongfeng Zhang. Openagi: When llm meets domain experts. arXiv, 2023.
[14] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement learning with a natural language action space. arXiv: Artificial Intelligence, 2015 .
[15] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. ArXiv, abs/2201.07207, 2022.
[16] Daniel Kahneman. Thinking, Fast and Slow. 2011.
[17] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017.
[18] Bill Yuchen Lin, Chengsong Huang, Qianchu Liu, Wenda Gu, Sam Sommerer, and Xiang Ren. On grounded planning for embodied tasks with language models. ArXiv, abs/2209.00465, 2022.
[19] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. ArXiv, abs/2304.09842, 2023.
[20] Antoine Miech, Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Andrew Zisserman. Thinking fast and slow: Efficient text-to-visual retrieval with transformers. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9821-9831, 2021.
[21] Keerthiram Murugesan, Mattia Atzeni, Pavan Kapanipathi, Pushkar Shukla, Sadhana Kumaravel, Gerald Tesauro, Kartik Talamadupula, Mrinmaya Sachan, and Murray Campbell. Text-based RL Agents with Commonsense Knowledge: New Challenges, Environments and Baselines. In Thirty Fifth AAAI Conference on Artificial Intelligence, 2021.
[22] Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making using language guided world modelling. In International Conference on Machine Learning (ICML), 2023.
[23] Maxwell Nye, Michael Henry Tessler, Joshua B. Tenenbaum, and Brenden M. Lake. Improving coherence and consistency in neural sequence models with dual-system, neuro-symbolic reasoning. In Neural Information Processing Systems, 2021.
[24] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.
[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020.
[26] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. ArXiv, abs/2205.06175, 2022.</p>
<p>[27] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China, 2019. Association for Computational Linguistics.
[28] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.
[29] Yongliang Shen, Kaitao Song, Xu Tan, Dong Sheng Li, Weiming Lu, and Yue Ting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. ArXiv, abs/2303.17580, 2023.
[30] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. ArXiv, abs/2303.11366, 2023.
[31] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew J. Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. ArXiv, abs/2010.03768, 2020.
[32] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. ArXiv, abs/2212.04088, 2022.
[33] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. ArXiv, abs/1409.3215, 2014.
[34] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. ArXiv, abs/1805.01954, 2018.
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008, 2017.
[36] Ruoyao Wang, Peter Alexander Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader? In Conference on Empirical Methods in Natural Language Processing, 2022.
[37] Zekun Wang, Ge Zhang, Kexin Yang, Ning Shi, Wangchunshu Zhou, Shaochun Hao, Guangzheng Xiong, Yizhi Li, Mong Yuan Sim, Xiuying Chen, Qingqing Zhu, Zhenzhu Yang, Adam Nik, Qi Liu, Chenghua Lin, Shi Wang, Ruibo Liu, Wenhu Chen, Ke Xu, Dayiheng Liu, Yike Guo, and Jie Fu. Interactive natural language processing. ArXiv, 2023.
[38] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. ArXiv, abs/2302.01560, 2023.
[39] Peter C Wason and J St BT Evans. Dual processes in reasoning? Cognition, 3(2):141-154, 1974.
[40] Shunyu Yao, Rohan Rao, Matthew J. Hausknecht, and Karthik Narasimhan. Keep calm and explore: Language models for action generation in text-based games. ArXiv, abs/2010.02903, 2020.
[41] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. ArXiv, abs/2210.03629, 2022.</p>
<h1>Appendix</h1>
<h2>A Dataset Statistics</h2>
<p>Table 2 presents the details of all 30 types of tasks in the ScienceWorld benchmark. To improve the training of SWIFT, we down-sampled Task 9-x, 10-x, and 3-3 from the original full dataset, as their large sizes resulted in a significant data imbalance. Additionally, we down-sampled less informative actions, such as 'close door to kitchen,' to produce a more effective dataset for imitation learning.</p>
<p>Evaluation. To save time while evaluating the numerous tasks and agents, we only used the first 10 variations for tasks with more than 10 test variations. This resulted in a total of 270 variations for fair and cost-effective comparisons among all agents. Some agents may receive a negative score from the engine and be unable to proceed any further due to their final action violating task requirements and being irrecoverable. In such cases, we used their last non-negative scores for evaluation.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Topic</th>
<th style="text-align: center;">Name</th>
<th style="text-align: center;">*Lens</th>
<th style="text-align: center;">#Vars: Train</th>
<th style="text-align: center;">Dev</th>
<th style="text-align: center;">Test</th>
<th style="text-align: center;"># Actions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Boiling)</td>
<td style="text-align: center;">107.7</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">694</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Melting)</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">427</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Freezing)</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">469</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">Matter</td>
<td style="text-align: center;">Changes of State (Any)</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">344</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Use Thermometer</td>
<td style="text-align: center;">21.4</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4278</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Measuring Boiling Point (known)</td>
<td style="text-align: center;">35.2</td>
<td style="text-align: center;">218</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6511</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">Measurement</td>
<td style="text-align: center;">Measuring Boiling Point (unknown)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9768</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Create a circuit</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">94</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Renewable vs Non-renewable Energy</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">169</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Test Conductivity (known)</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">48</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1341</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">Electricity</td>
<td style="text-align: center;">Test Conductivity (unknown)</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">6974</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a living thing</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1606</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a non-living thing</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">756</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find a plant</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1458</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">Classification</td>
<td style="text-align: center;">Find an animal</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1606</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Grow a plant</td>
<td style="text-align: center;">69.5</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3675</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Grow a fruit</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4283</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing (generic)</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">347</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing paints (secondary colours)</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">224</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">Chemistry</td>
<td style="text-align: center;">Mixing paints (tertiary colours)</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">350</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify longest-lived animal</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">298</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify shortest-lived animal</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">298</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify longest-then-shortest-lived animal</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">360</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify life stages (plant)</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">165</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Identify life stages (animal)</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">31</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Inclined Planes (determine angle)</td>
<td style="text-align: center;">97</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2733</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Friction (known surfaces)</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3644</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">Forces</td>
<td style="text-align: center;">Friction (unknown surfaces)</td>
<td style="text-align: center;">123.1</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3284</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Mendelian Genetics (known plants)</td>
<td style="text-align: center;">130.1</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3043</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">Biology</td>
<td style="text-align: center;">Mendelian Genetics (unknown plants)</td>
<td style="text-align: center;">132.1</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">2853</td>
</tr>
<tr>
<td style="text-align: center;">Short $(0&lt;*$ Len $\leq 20)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">11.76</td>
<td style="text-align: center;">81.80</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">8.80</td>
<td style="text-align: center;">673.10</td>
</tr>
<tr>
<td style="text-align: center;">Medium $(20&lt;*$ Len $\leq 50)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">28.58</td>
<td style="text-align: center;">110.75</td>
<td style="text-align: center;">8.13</td>
<td style="text-align: center;">8.38</td>
<td style="text-align: center;">2516.88</td>
</tr>
<tr>
<td style="text-align: center;">Long (*Len $&gt;50)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">94.30</td>
<td style="text-align: center;">37.75</td>
<td style="text-align: center;">9.00</td>
<td style="text-align: center;">9.58</td>
<td style="text-align: center;">2934.75</td>
</tr>
<tr>
<td style="text-align: center;">Overall (avg)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">49.26</td>
<td style="text-align: center;">71.90</td>
<td style="text-align: center;">8.63</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">2069.43</td>
</tr>
<tr>
<td style="text-align: center;">Overall (sum)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">2,157</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">62,083</td>
</tr>
</tbody>
</table>
<p>Table 2: The statistics of ScienceWorld benchmark. *Len is the average length of the oracle agent's trajectories. We show the number of our down-sampled variations in each split. The last column is the number of data points forr action-prediction seq2seq task in training SWIFT.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The SwiftSage framework explained with pseudocode.</p>
<h1>B Implementation Details</h1>
<h2>B. 1 Training Details of SWIFT</h2>
<p>We utilized flan-t5-large ( 770 m ) as the base model and fine-tuned it using the seq2seq actionprediction data ( 62 k ) as previously described. A learning rate of $1 \mathrm{e}-4$ and batch size of 128 were employed for training 500 steps, selected based on dev loss. Although we experimented with larger sizes of flan-t5 models, we observed only marginal improvements at a much higher training cost. We believe this is because the language used to describe the environment and actions covers a small vocabulary, and the language complexity does not warrant the use of more parameters.</p>
<h2>B. 2 Prompting in SAGE</h2>
<p>In Section 3.3, we provided an overview of the two-stage prompting framework: planning and grounding. In this section, we delve into further details of each stage.</p>
<p>Memory augmentation. Since the agent can only perceive objects in its current environment location, objects from previously visited locations are not displayed unless a prior 'look around' action has been executed. To augment memory for LLMs during planning and grounding, we also present the objects observed in previously visited locations. Additionally, we include the agent's location during each action in the action history, e.g., "pick up metal pot [location: kitchen]," to facilitate spatial reasoning for LLMs.</p>
<p>Connecting the two stages. We conveniently reuse the LLM output from the first stage (i.e., answers to Q1-Q5) as part of the input for the second stage. Our experiments involve using answers to all questions in the grounding stage. However, one can opt to use only answers to Q4 and Q5 to reduce computational costs. Our small-scale ablation study indicates that incorporating answers to Q1-Q3 in the grounding stage proves beneficial, yielding a gain of about 2 points for short tasks on average.</p>
<p>Grounding with action templates. We previously introduced an action template, 'POUR(object A, object B)', in Figure 2. Here, we present several additional templates to further illustrate the concept:</p>
<div class="codehilite"><pre><span></span><code><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">room</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">directly</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">TELEPORT</span><span class="w"> </span><span class="p">(</span><span class="n">kitchen</span><span class="p">)</span>
<span class="n">PICK</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">into</span><span class="w"> </span><span class="n">your</span><span class="w"> </span><span class="n">inventory</span>
<span class="k">OPEN</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">search</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">things</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span><span class="w"> </span><span class="k">OPEN</span><span class="w"> </span><span class="p">(</span><span class="n">freezer</span><span class="p">).</span>
<span class="n">ACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">activate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">such</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">sink</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">stove</span><span class="p">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">use</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="n">DEACTIVATE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">deactivate</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">turn</span><span class="w"> </span><span class="k">off</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span>
<span class="n">EXAMINE</span><span class="p">(</span><span class="k">object</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">look</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="n">carefully</span><span class="p">.</span><span class="w"> </span><span class="k">For</span><span class="w"> </span><span class="n">example</span><span class="p">,</span><span class="w"> </span><span class="n">EXAMINE</span><span class="w"> </span><span class="p">(</span><span class="n">light</span><span class="w"> </span><span class="n">bulb</span><span class="p">).</span>
<span class="n">MOVE</span><span class="w"> </span><span class="p">(</span><span class="k">object</span><span class="p">,</span><span class="w"> </span><span class="n">place</span><span class="p">)</span><span class="w"> </span><span class="err">:</span><span class="w"> </span><span class="n">move</span><span class="o">/</span><span class="n">place</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">object</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">place</span>
</code></pre></div>

<p>It should be noted that despite explicitly instructing the LLM to only utilize permitted action types, it may occasionally generate actions of disallowed types that cannot be parsed. These invalid actions will be disregarded in the action buffer, and if necessary, the system will revert to the SWIFT mode.</p>
<h1>B. 3 Action Buffer</h1>
<p>In Section 3.4, we presented four conditions for activating the SAGE module. To detect critical decisions (Condition 3), we primarily focus on the 'focus on' actions, as many tasks in ScienceWorld necessitate agents to concentrate on the correct substances and objects in the proper sequence. A single incorrect 'focus on' action can terminate the entire run. Thus, we restrict the SWIFT module from performing such actions if SAGE has not yet been activated.</p>
<p>For identifying exceptions (Condition 4), we examine phrases like "No known action can match," "... cannot/doesn't...," and so on. When processing an action buffer, we attempt to execute each action sequentially. If two consecutive actions are invalid or cause exceptions, we halt and revert to SWIFT.</p>
<h2>C Additional Results and Analysis</h2>
<h2>C. 1 Sensitivity to LLMs: GPT-3.5-turbo vs GPT-4</h2>
<p>Besides the empirical results in Table 1, we also evaluate performance using GPT-3.5-turbo instead of GPT-4, which is considerably larger and more expensive. Other methods exhibit a significant performance decline, for instance, ReAct's score drops from 36.43 to 19.76, which is close to nonLLM methods and even lower than the vanilla method, SayCan. In contrast, SWIFTSAGE maintains a respectable performance of 62.22 , indicating better robustness.
As discussed in Sec. 5 (limitations), we plan to utilize other open-source LLMs, such as Alpaca, and investigate distilling the planning ability from closed-source LLMs to open-source and smaller LMs. Nevertheless, a practical challenge arises due to the current open-source LLMs having more restrictive length limits for inputs and outputs.</p>
<h2>C. 2 Efficiency Analysis</h2>
<p>Figure 5 illustrates that most of SWIFTSAGE's curves are situated near the top-left corner, indicating that SWIFTSAGE attains higher scores than oracle agents at a faster rate. Although ReAct is competitive with our method for shorter tasks, its trajectories typically plateau at intermediate scores and do not reach 100. While the Oracle agent consistently achieves a perfect score (100.0), its efficiency, particularly in longer tasks, is often outperformed by SWIFTSAGE.</p>
<h2>C. 3 Cost-effectiveness</h2>
<p>Table 4 presents a comprehensive analysis of the cost-effectiveness of LLM-based methods. We examine two specific metrics: tokens per action (tpa) and scores per action (spa) for SayCan, ReAct, Reflexion, and SWIFTSAGE across all tasks. Despite SAGE invoking LLM APIs twice for inference, its overall cost remains lower, as the result is a sequence of actions typically containing about 5 actions. In contrast, SAYCAN and ReAct require $\mathbf{1 , 8 5 5 . 8 4}$ and $\mathbf{1 , 9 7 1 . 0 3}$ tokens per action (tpa) respectively, while REFLEXION necessitates $\mathbf{2 , 9 8 3 . 4 6}$ tpa. SWIFTSAGE, however, only uses $\mathbf{7 5 7 . 0 7}$ tpa. Given its superior performance, SWIFTSAGE proves to be more cost-effective than other LLMbased methods. This efficiency primarily stems from invoking LLMs only when necessary (thanks to our robust SWIFT module) and the action buffer mechanism.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Type</th>
<th style="text-align: center;">Swift-Only</th>
<th style="text-align: center;">SayCan $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">ReAct $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">Reflexion $_{\text {ChatGPT }}$</th>
<th style="text-align: center;">SwiftSage $_{\text {ChatGPT }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">15.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">8.5</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">58.5</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">38.5</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">57.4</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">9.4</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">2.2</td>
<td style="text-align: center;">47.9</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">6.7</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">7.2</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">53.3</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">5.7</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">19.6</td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;">48.6</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">72.7</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">8.7</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">50.3</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">59.5</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">55.5</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">78.1</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">76.0</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">19.9</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">15.8</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">13.4</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">57.5</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">50.9</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">13.5</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">43.2</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">11.1</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">27.4</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">60.0</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">33.0</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">75.6</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">33.0</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">28.5</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">54.0</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">4.2</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">17.5</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">77.0</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">76.0</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">17.0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">11.7</td>
<td style="text-align: center;">6.0</td>
<td style="text-align: center;">51.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">78.68</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">28.95</td>
<td style="text-align: center;">39.19</td>
<td style="text-align: center;">72.81</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">32.90</td>
<td style="text-align: center;">20.06</td>
<td style="text-align: center;">21.09</td>
<td style="text-align: center;">14.37</td>
<td style="text-align: center;">55.34</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">35.55</td>
<td style="text-align: center;">18.66</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">16.27</td>
<td style="text-align: center;">57.99</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">49.22</td>
<td style="text-align: center;">25.22</td>
<td style="text-align: center;">19.76</td>
<td style="text-align: center;">23.40</td>
<td style="text-align: center;">62.22</td>
</tr>
</tbody>
</table>
<p>Table 3: Additional results on the ScienceWorld benchmark. Different from Table 1, we use gpt-3.5-turbo instead of gpt-4 as the LLM for evaluating SayCan, ReAct, Relfexion, and our SWIFTSAGE. We also present the results of using SWIFT module only.</p>
<p>Interestingly, we observe that SWIFTSAGE has an even lower tpa for long tasks compared to its tpa in medium and short tasks. Upon further investigation, we attribute this finding to longer action buffers and the SWIFT module being more frequently effective. Additionally, regarding scores per action (spa), we discover that our SWIFTSAGE is more cost-effective by utilizing fewer tokens and achieving higher scores.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: An overview of visualizing trajectories of SWIFTSAGE, REACT and Oracle. $X$ : time steps $(0 \rightarrow T) ; Y$ : scores $(0 \rightarrow 100)$. Similar to Figure 3, each curve is a single trajectory by an agent in performing a task variation. A more efficient agent will achieve higher scores in a shorter time, resulting in curves positioned near the top-left corner.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average number of tokens per action (tpa)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average scores per action (spa)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Task Type</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">SwiftSage</td>
<td style="text-align: center;">SayCan</td>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">Reflexion</td>
<td style="text-align: center;">SwiftSage</td>
</tr>
<tr>
<td style="text-align: center;">1-1</td>
<td style="text-align: center;">1944.94</td>
<td style="text-align: center;">1503.60</td>
<td style="text-align: center;">2632.97</td>
<td style="text-align: center;">528.17</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.49</td>
</tr>
<tr>
<td style="text-align: center;">1-2</td>
<td style="text-align: center;">1125.76</td>
<td style="text-align: center;">1339.39</td>
<td style="text-align: center;">3066.70</td>
<td style="text-align: center;">545.34</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1.64</td>
</tr>
<tr>
<td style="text-align: center;">1-3</td>
<td style="text-align: center;">1034.33</td>
<td style="text-align: center;">1268.23</td>
<td style="text-align: center;">3307.30</td>
<td style="text-align: center;">550.17</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: center;">1-4</td>
<td style="text-align: center;">1295.03</td>
<td style="text-align: center;">1251.45</td>
<td style="text-align: center;">2439.34</td>
<td style="text-align: center;">754.05</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">1.69</td>
</tr>
<tr>
<td style="text-align: center;">2-1</td>
<td style="text-align: center;">1188.46</td>
<td style="text-align: center;">1545.03</td>
<td style="text-align: center;">1988.59</td>
<td style="text-align: center;">494.52</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">3.01</td>
</tr>
<tr>
<td style="text-align: center;">2-2</td>
<td style="text-align: center;">1862.11</td>
<td style="text-align: center;">1181.88</td>
<td style="text-align: center;">1596.03</td>
<td style="text-align: center;">394.29</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">2.32</td>
</tr>
<tr>
<td style="text-align: center;">2-3</td>
<td style="text-align: center;">939.17</td>
<td style="text-align: center;">1358.33</td>
<td style="text-align: center;">1753.17</td>
<td style="text-align: center;">574.05</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">1.71</td>
</tr>
<tr>
<td style="text-align: center;">3-1</td>
<td style="text-align: center;">1713.64</td>
<td style="text-align: center;">1846.91</td>
<td style="text-align: center;">2677.89</td>
<td style="text-align: center;">807.62</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">3-2</td>
<td style="text-align: center;">1785.01</td>
<td style="text-align: center;">1754.14</td>
<td style="text-align: center;">2337.02</td>
<td style="text-align: center;">823.28</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.22</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">0.31</td>
</tr>
<tr>
<td style="text-align: center;">3-3</td>
<td style="text-align: center;">1762.13</td>
<td style="text-align: center;">2441.79</td>
<td style="text-align: center;">2262.39</td>
<td style="text-align: center;">220.80</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.84</td>
</tr>
<tr>
<td style="text-align: center;">3-4</td>
<td style="text-align: center;">1698.85</td>
<td style="text-align: center;">1195.59</td>
<td style="text-align: center;">2859.30</td>
<td style="text-align: center;">287.25</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">1.13</td>
</tr>
<tr>
<td style="text-align: center;">4-1</td>
<td style="text-align: center;">411.08</td>
<td style="text-align: center;">579.70</td>
<td style="text-align: center;">1053.57</td>
<td style="text-align: center;">309.14</td>
<td style="text-align: center;">1.91</td>
<td style="text-align: center;">1.16</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">4-2</td>
<td style="text-align: center;">1332.83</td>
<td style="text-align: center;">1098.69</td>
<td style="text-align: center;">1250.37</td>
<td style="text-align: center;">298.48</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">4-3</td>
<td style="text-align: center;">1155.99</td>
<td style="text-align: center;">1314.74</td>
<td style="text-align: center;">2966.82</td>
<td style="text-align: center;">406.17</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">3.82</td>
</tr>
<tr>
<td style="text-align: center;">4-4</td>
<td style="text-align: center;">1126.67</td>
<td style="text-align: center;">591.15</td>
<td style="text-align: center;">1003.18</td>
<td style="text-align: center;">309.71</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">1.02</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">4.76</td>
</tr>
<tr>
<td style="text-align: center;">5-1</td>
<td style="text-align: center;">2323.43</td>
<td style="text-align: center;">2620.66</td>
<td style="text-align: center;">5091.49</td>
<td style="text-align: center;">168.95</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">5-2</td>
<td style="text-align: center;">2646.50</td>
<td style="text-align: center;">2575.11</td>
<td style="text-align: center;">5864.93</td>
<td style="text-align: center;">536.56</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: center;">6-1</td>
<td style="text-align: center;">1454.65</td>
<td style="text-align: center;">1802.62</td>
<td style="text-align: center;">2344.90</td>
<td style="text-align: center;">1388.89</td>
<td style="text-align: center;">0.28</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">6-2</td>
<td style="text-align: center;">2413.99</td>
<td style="text-align: center;">2763.66</td>
<td style="text-align: center;">4342.07</td>
<td style="text-align: center;">402.50</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">3.33</td>
</tr>
<tr>
<td style="text-align: center;">6-3</td>
<td style="text-align: center;">1371.50</td>
<td style="text-align: center;">2860.68</td>
<td style="text-align: center;">4551.96</td>
<td style="text-align: center;">6361.79</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.59</td>
</tr>
<tr>
<td style="text-align: center;">7-1</td>
<td style="text-align: center;">376.50</td>
<td style="text-align: center;">495.83</td>
<td style="text-align: center;">813.08</td>
<td style="text-align: center;">768.63</td>
<td style="text-align: center;">5.71</td>
<td style="text-align: center;">3.33</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">11.88</td>
</tr>
<tr>
<td style="text-align: center;">7-2</td>
<td style="text-align: center;">424.53</td>
<td style="text-align: center;">478.09</td>
<td style="text-align: center;">1180.58</td>
<td style="text-align: center;">772.00</td>
<td style="text-align: center;">2.11</td>
<td style="text-align: center;">6.14</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">10.63</td>
</tr>
<tr>
<td style="text-align: center;">7-3</td>
<td style="text-align: center;">424.73</td>
<td style="text-align: center;">564.69</td>
<td style="text-align: center;">1175.35</td>
<td style="text-align: center;">609.73</td>
<td style="text-align: center;">4.55</td>
<td style="text-align: center;">3.85</td>
<td style="text-align: center;">0.24</td>
<td style="text-align: center;">8.48</td>
</tr>
<tr>
<td style="text-align: center;">8-1</td>
<td style="text-align: center;">1505.39</td>
<td style="text-align: center;">1155.71</td>
<td style="text-align: center;">2466.59</td>
<td style="text-align: center;">249.38</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">2.23</td>
</tr>
<tr>
<td style="text-align: center;">8-2</td>
<td style="text-align: center;">3189.80</td>
<td style="text-align: center;">741.71</td>
<td style="text-align: center;">2886.09</td>
<td style="text-align: center;">2479.00</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">4.03</td>
</tr>
<tr>
<td style="text-align: center;">9-1</td>
<td style="text-align: center;">2066.06</td>
<td style="text-align: center;">2642.79</td>
<td style="text-align: center;">2652.56</td>
<td style="text-align: center;">307.30</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">1.06</td>
</tr>
<tr>
<td style="text-align: center;">9-2</td>
<td style="text-align: center;">2517.48</td>
<td style="text-align: center;">3031.95</td>
<td style="text-align: center;">3606.60</td>
<td style="text-align: center;">314.19</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.66</td>
</tr>
<tr>
<td style="text-align: center;">9-3</td>
<td style="text-align: center;">7002.72</td>
<td style="text-align: center;">7507.00</td>
<td style="text-align: center;">7785.29</td>
<td style="text-align: center;">366.06</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.65</td>
</tr>
<tr>
<td style="text-align: center;">10-1</td>
<td style="text-align: center;">3612.33</td>
<td style="text-align: center;">4218.44</td>
<td style="text-align: center;">4822.97</td>
<td style="text-align: center;">466.21</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">1.78</td>
</tr>
<tr>
<td style="text-align: center;">10-2</td>
<td style="text-align: center;">3969.62</td>
<td style="text-align: center;">5401.37</td>
<td style="text-align: center;">6724.81</td>
<td style="text-align: center;">218.00</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">1.69</td>
</tr>
<tr>
<td style="text-align: center;">Short</td>
<td style="text-align: center;">1256.98</td>
<td style="text-align: center;">1047.52</td>
<td style="text-align: center;">1934.90</td>
<td style="text-align: center;">716.30</td>
<td style="text-align: center;">1.56</td>
<td style="text-align: center;">1.76</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">5.69</td>
</tr>
<tr>
<td style="text-align: center;">Medium</td>
<td style="text-align: center;">1578.51</td>
<td style="text-align: center;">1742.18</td>
<td style="text-align: center;">2550.85</td>
<td style="text-align: center;">1277.52</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.35</td>
</tr>
<tr>
<td style="text-align: center;">Long</td>
<td style="text-align: center;">2539.78</td>
<td style="text-align: center;">2893.19</td>
<td style="text-align: center;">4145.68</td>
<td style="text-align: center;">444.09</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.17</td>
</tr>
<tr>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">1855.84</td>
<td style="text-align: center;">1971.03</td>
<td style="text-align: center;">2983.46</td>
<td style="text-align: center;">757.07</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">2.73</td>
</tr>
</tbody>
</table>
<p>Table 4: Cost-effectiveness analysis for LLM-based methods.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Contact: yuchen1@allenai.org&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
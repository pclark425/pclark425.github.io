<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3645 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3645</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3645</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-270062620</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.17044v3.pdf" target="_blank">Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines. Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone. But how compelling are these AI-generated ideas, and how can we improve their quality? Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas. We conduct a large-scale evaluation in which over 100 research group leaders -- from natural sciences to humanities -- ranked more than 4,400 personalized ideas based on their interest. This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models. Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3645.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3645.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMuse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SciMuse (knowledge-graph + LLM system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system introduced in this paper that combines a large co-occurrence/citation knowledge graph (built from millions of papers) with large language models (GPT family) to generate, refine, and rank personalized research ideas and concept-pair suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-4o, GPT-3.5 (used together with a knowledge-graph pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SciMuse uses off-the-shelf GPT-series LLMs (GPT-4, GPT-4o, GPT-3.5) via prompting (no fine-tuning reported). It couples these LLMs with a large knowledge graph (123,128 concepts; edges from >58M OpenAlex papers) and classical ML (small supervised neural network) to produce and rank ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Multidisciplinary (natural sciences, technology, mathematics, medicine, social sciences, humanities)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Concept extraction from ~2.44M preprints (arXiv, bioRxiv, medRxiv, chemRxiv); edges/occurrences and citation metadata from >58M papers in OpenAlex (data cutoff Apr 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Heuristic and association rules linking concept-pairs to predicted impact and human interest (concept-pair-level predictive/interpretive heuristics rather than formal physical/causal laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Construct a large concept co-occurrence + citation knowledge graph; compute graph-theoretic and citation-derived features (144 features, 25 top features used); use GPT prompting (iterative self-refinement) to generate idea texts from selected concept pairs; use LLM pairwise zero-shot comparisons (ELO tournament) and a supervised neural network on graph features to rank/ predict high-interest proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Large human-expert evaluation (110 Max Planck research group leaders, 4,451 ratings across >4,400 suggestions); supervised validation with Monte Carlo cross-validation (neural network AUC ≈ 0.645); LLM zero-shot ranking AUC ≈ 0.673; top-N precision and top-N success probability metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>SciMuse generated thousands of personalized research proposals; ~1,107 suggestions (≈25%) rated 4 or 5 by experts. Graph features correlated with interest (e.g., lower node degree and lower past citation associated with higher interest). Both a small supervised neural net (using graph features) and LLM zero-shot ranking could surface high-interest suggestions substantially above random chance (AUCs ≈ 2/3; high top-N precision for the supervised model).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The system provides heuristic associations rather than formal derivation of scientific laws; reliance on titles/abstracts and concept-extraction (RAKE + GPT + manual curation) may miss nuance from full texts; LLMs risk hallucination and their ranking lacks transparency; supervised model limited by modest labeled data size; moderate prediction accuracy (not near-perfect).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3645.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based large language model used in this study for concept refinement, iterative idea generation (self-refinement), and as part of zero-shot pairwise comparisons to rank generated research suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive LLM (OpenAI). In this paper GPT-4 is used via prompting for: (1) refining noisy researcher concept-lists, (2) generating and iteratively refining candidate project ideas from concept pairs (self-reflection loop), and (3) in small tests for idea quality comparisons. No model fine-tuning on the corpus is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Multidisciplinary scientific literature (titles/abstracts provided per researcher) used to contextualize suggestions.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Indirectly used with knowledge graph built from ~2.44M preprints and >58M OpenAlex papers; GPT-4 itself was accessed as a pre-trained model (no corpus size specified in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Not used here to extract formal qualitative laws; used to surface candidate hypotheses and heuristic concept-combinations (i.e., induce promising associations and research questions from concept inputs).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Structured prompting including: concept-filter/refinement prompts, a multi-stage self-refinement prompt that requests multiple contexts and iterative criticism, and pairwise comparison prompts for zero-shot ranking (used in tournament/ELO setup).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert ratings compared to GPT-4-generated ideas; comparative small-scale human preference tests vs other model variants; algorithmic evaluation of zero-shot pairwise ranking via AUC and top-N precision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 successfully produced and refined project ideas when given concept pairs and researcher paper titles; its outputs were part of the evaluated suggestion pool and contributed to LLM-based zero-shot rankings that placed high-interest ideas above random baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No claim of extracting interpretable scientific 'laws'; potential hallucination and limited transparency in why suggestions are ranked as they are; performance depends on model version and prompt engineering; the paper reports better performance with newer model variants (GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3645.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o / GPT-4o-mini (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (and GPT-4o-mini) (OpenAI variants used experimentally)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Newer OpenAI model variants evaluated in the study; used for idea generation comparisons and for zero-shot pairwise ranking, showing improved ranking/generation performance over older versions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (and GPT-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Improved/updated variants of the GPT-4 family used via API for idea generation and zero-shot ranking tasks; exact architecture/size not specified in this paper. GPT-4o showed better zero-shot ranking AUC and was preferred in small human preference tests versus GPT-4 and GPT-o1 in limited comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Multidisciplinary scientific domains (as above).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper; models accessed as pre-trained API endpoints and used with contextual prompts + knowledge-graph-derived inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Functionally used to surface hypotheses and rank promising concept-pair combinations rather than to derive formal laws.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Same prompting paradigm as GPT-4: structured prompts for generation with iterative self-refinement and pairwise zero-shot comparisons aggregated via ELO for ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human expert evaluations of generated ideas; AUC and top-N metrics comparing zero-shot rankings to human ratings; small-scale direct preference comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4o improved both generation quality in small preference tests and zero-shot ranking performance relative to GPT-4 and GPT-3.5 (reported AUCs and top-N precision improvements).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Details of internal model differences not provided here; improvements reported from small-scale tests and aggregate metrics but still subject to hallucination and limited explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3645.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier GPT-series LLM used here for concept refinement and for zero-shot pairwise ranking as a baseline against more capable models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based pre-trained LLM (OpenAI). Used in this study for concept refinement and as a baseline for zero-shot pairwise ranking of suggestions. No fine-tuning performed.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Multidisciplinary (same as other GPT variants in the study).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Used to induce and rank hypotheses/idea texts heuristically; not used to extract formal laws.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting for pairwise comparisons and concept filtering, aggregated via ELO ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared to human expert ratings and other LLM variants using AUC/top-N precision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-3.5 achieved lower zero-shot ranking performance than GPT-4o and GPT-4 (AUC and top-N precision lower), but still performed substantially better than random ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Lower performance relative to newer LLM variants; same general risks of hallucination and opaque reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3645.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference: Yang et al. (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for automated open-domain scientific hypotheses discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (arXiv:2309.02726) that, by title, addresses use of LLMs to discover scientific hypotheses across open domains; cited here as related work but not used experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for automated open-domain scientific hypotheses discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various, as reported in that cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in detail in this paper; referenced as prior work exploring hypothesis discovery with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Open-domain scientific literature (per cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Hypothesis discovery / automated generation of scientific claims (per cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified in this paper (see the cited work for methods).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified here (refer to the cited paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as related literature on LLM-enabled hypothesis discovery; no results from that work are reported in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper; see cited work for specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3645.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference: Wellawatte & Schwaller (2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting human interpretable structure-property relationships in chemistry using xai and large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (arXiv:2311.04047) that, by title, indicates extraction of interpretable structure–property relationships from chemical data using explainable AI and LLMs; cited here as related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting human interpretable structure-property relationships in chemistry using xai and large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs + XAI methods (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper; cited as an example of using LLMs and explainable AI to extract human-interpretable relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Chemistry (structure-property relationships).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of interpretable relationships / structure–property rules (per cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified in this paper (refer to that citation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Referenced as related work demonstrating extraction of interpretable relationships with LLMs/XAI; no experimental details included here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3645.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference: ResearchAgent (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited recent work (arXiv:2404.07738) about iterative idea generation over literature with LLMs; listed as related work in the paper's bibliography.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (as reported in that work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in this paper; cited as related contemporary work using LLMs iteratively over literature.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scientific literature (general/multidisciplinary).</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Iterative hypothesis/idea induction from literature (per cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example of LLM-based iterative idea-generation systems; details require consulting the cited manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3645.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3645.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference: Scimon (2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work (ACL 2024) on machines optimized for novelty in scientific inspiration; included in related work as a system that analyzes literature to propose novel directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph + ML/LLM hybrid approaches (as in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified here; referenced as related literature on automated novelty-driven suggestion systems.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Scientific literature / novelty detection.</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Novelty-driven suggestion heuristics (per cited title).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Listed among related works that aim to propose novel scientific directions from literature; no details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>Extracting human interpretable structure-property relationships in chemistry using xai and large language models <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 1)</em></li>
                <li>Agatha: automatic graph mining and transformer based hypothesis generation approach <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3645",
    "paper_id": "paper-270062620",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "SciMuse",
            "name_full": "SciMuse (knowledge-graph + LLM system)",
            "brief_description": "A system introduced in this paper that combines a large co-occurrence/citation knowledge graph (built from millions of papers) with large language models (GPT family) to generate, refine, and rank personalized research ideas and concept-pair suggestions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-4o, GPT-3.5 (used together with a knowledge-graph pipeline)",
            "model_description": "SciMuse uses off-the-shelf GPT-series LLMs (GPT-4, GPT-4o, GPT-3.5) via prompting (no fine-tuning reported). It couples these LLMs with a large knowledge graph (123,128 concepts; edges from &gt;58M OpenAlex papers) and classical ML (small supervised neural network) to produce and rank ideas.",
            "input_domain": "Multidisciplinary (natural sciences, technology, mathematics, medicine, social sciences, humanities)",
            "corpus_size": "Concept extraction from ~2.44M preprints (arXiv, bioRxiv, medRxiv, chemRxiv); edges/occurrences and citation metadata from &gt;58M papers in OpenAlex (data cutoff Apr 2023).",
            "law_type": "Heuristic and association rules linking concept-pairs to predicted impact and human interest (concept-pair-level predictive/interpretive heuristics rather than formal physical/causal laws).",
            "distillation_method": "Construct a large concept co-occurrence + citation knowledge graph; compute graph-theoretic and citation-derived features (144 features, 25 top features used); use GPT prompting (iterative self-refinement) to generate idea texts from selected concept pairs; use LLM pairwise zero-shot comparisons (ELO tournament) and a supervised neural network on graph features to rank/ predict high-interest proposals.",
            "evaluation_method": "Large human-expert evaluation (110 Max Planck research group leaders, 4,451 ratings across &gt;4,400 suggestions); supervised validation with Monte Carlo cross-validation (neural network AUC ≈ 0.645); LLM zero-shot ranking AUC ≈ 0.673; top-N precision and top-N success probability metrics.",
            "results_summary": "SciMuse generated thousands of personalized research proposals; ~1,107 suggestions (≈25%) rated 4 or 5 by experts. Graph features correlated with interest (e.g., lower node degree and lower past citation associated with higher interest). Both a small supervised neural net (using graph features) and LLM zero-shot ranking could surface high-interest suggestions substantially above random chance (AUCs ≈ 2/3; high top-N precision for the supervised model).",
            "limitations_or_challenges": "The system provides heuristic associations rather than formal derivation of scientific laws; reliance on titles/abstracts and concept-extraction (RAKE + GPT + manual curation) may miss nuance from full texts; LLMs risk hallucination and their ranking lacks transparency; supervised model limited by modest labeled data size; moderate prediction accuracy (not near-perfect).",
            "uuid": "e3645.0",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4 (used)",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A transformer-based large language model used in this study for concept refinement, iterative idea generation (self-refinement), and as part of zero-shot pairwise comparisons to rank generated research suggestions.",
            "citation_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Transformer-based autoregressive LLM (OpenAI). In this paper GPT-4 is used via prompting for: (1) refining noisy researcher concept-lists, (2) generating and iteratively refining candidate project ideas from concept pairs (self-reflection loop), and (3) in small tests for idea quality comparisons. No model fine-tuning on the corpus is reported.",
            "input_domain": "Multidisciplinary scientific literature (titles/abstracts provided per researcher) used to contextualize suggestions.",
            "corpus_size": "Indirectly used with knowledge graph built from ~2.44M preprints and &gt;58M OpenAlex papers; GPT-4 itself was accessed as a pre-trained model (no corpus size specified in this paper).",
            "law_type": "Not used here to extract formal qualitative laws; used to surface candidate hypotheses and heuristic concept-combinations (i.e., induce promising associations and research questions from concept inputs).",
            "distillation_method": "Structured prompting including: concept-filter/refinement prompts, a multi-stage self-refinement prompt that requests multiple contexts and iterative criticism, and pairwise comparison prompts for zero-shot ranking (used in tournament/ELO setup).",
            "evaluation_method": "Human expert ratings compared to GPT-4-generated ideas; comparative small-scale human preference tests vs other model variants; algorithmic evaluation of zero-shot pairwise ranking via AUC and top-N precision metrics.",
            "results_summary": "GPT-4 successfully produced and refined project ideas when given concept pairs and researcher paper titles; its outputs were part of the evaluated suggestion pool and contributed to LLM-based zero-shot rankings that placed high-interest ideas above random baseline.",
            "limitations_or_challenges": "No claim of extracting interpretable scientific 'laws'; potential hallucination and limited transparency in why suggestions are ranked as they are; performance depends on model version and prompt engineering; the paper reports better performance with newer model variants (GPT-4o).",
            "uuid": "e3645.1",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-4o / GPT-4o-mini (used)",
            "name_full": "GPT-4o (and GPT-4o-mini) (OpenAI variants used experimentally)",
            "brief_description": "Newer OpenAI model variants evaluated in the study; used for idea generation comparisons and for zero-shot pairwise ranking, showing improved ranking/generation performance over older versions.",
            "citation_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
            "mention_or_use": "use",
            "model_name": "GPT-4o (and GPT-4o-mini)",
            "model_description": "Improved/updated variants of the GPT-4 family used via API for idea generation and zero-shot ranking tasks; exact architecture/size not specified in this paper. GPT-4o showed better zero-shot ranking AUC and was preferred in small human preference tests versus GPT-4 and GPT-o1 in limited comparisons.",
            "input_domain": "Multidisciplinary scientific domains (as above).",
            "corpus_size": "Not specified in this paper; models accessed as pre-trained API endpoints and used with contextual prompts + knowledge-graph-derived inputs.",
            "law_type": "Functionally used to surface hypotheses and rank promising concept-pair combinations rather than to derive formal laws.",
            "distillation_method": "Same prompting paradigm as GPT-4: structured prompts for generation with iterative self-refinement and pairwise zero-shot comparisons aggregated via ELO for ranking.",
            "evaluation_method": "Human expert evaluations of generated ideas; AUC and top-N metrics comparing zero-shot rankings to human ratings; small-scale direct preference comparisons.",
            "results_summary": "GPT-4o improved both generation quality in small preference tests and zero-shot ranking performance relative to GPT-4 and GPT-3.5 (reported AUCs and top-N precision improvements).",
            "limitations_or_challenges": "Details of internal model differences not provided here; improvements reported from small-scale tests and aggregate metrics but still subject to hallucination and limited explainability.",
            "uuid": "e3645.2",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "GPT-3.5 (used)",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "An earlier GPT-series LLM used here for concept refinement and for zero-shot pairwise ranking as a baseline against more capable models.",
            "citation_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Transformer-based pre-trained LLM (OpenAI). Used in this study for concept refinement and as a baseline for zero-shot pairwise ranking of suggestions. No fine-tuning performed.",
            "input_domain": "Multidisciplinary (same as other GPT variants in the study).",
            "corpus_size": "Not specified in this paper.",
            "law_type": "Used to induce and rank hypotheses/idea texts heuristically; not used to extract formal laws.",
            "distillation_method": "Prompting for pairwise comparisons and concept filtering, aggregated via ELO ranking.",
            "evaluation_method": "Compared to human expert ratings and other LLM variants using AUC/top-N precision metrics.",
            "results_summary": "GPT-3.5 achieved lower zero-shot ranking performance than GPT-4o and GPT-4 (AUC and top-N precision lower), but still performed substantially better than random ranking.",
            "limitations_or_challenges": "Lower performance relative to newer LLM variants; same general risks of hallucination and opaque reasoning.",
            "uuid": "e3645.3",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reference: Yang et al. (2023)",
            "name_full": "Large language models for automated open-domain scientific hypotheses discovery",
            "brief_description": "Cited work (arXiv:2309.02726) that, by title, addresses use of LLMs to discover scientific hypotheses across open domains; cited here as related work but not used experimentally in this paper.",
            "citation_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "mention_or_use": "mention",
            "model_name": "LLMs (various, as reported in that cited work)",
            "model_description": "Not specified in detail in this paper; referenced as prior work exploring hypothesis discovery with LLMs.",
            "input_domain": "Open-domain scientific literature (per cited title).",
            "corpus_size": null,
            "law_type": "Hypothesis discovery / automated generation of scientific claims (per cited title).",
            "distillation_method": "Not specified in this paper (see the cited work for methods).",
            "evaluation_method": "Not specified here (refer to the cited paper).",
            "results_summary": "Mentioned as related literature on LLM-enabled hypothesis discovery; no results from that work are reported in detail in this paper.",
            "limitations_or_challenges": "Not discussed in this paper; see cited work for specifics.",
            "uuid": "e3645.4",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reference: Wellawatte & Schwaller (2023)",
            "name_full": "Extracting human interpretable structure-property relationships in chemistry using xai and large language models",
            "brief_description": "Cited work (arXiv:2311.04047) that, by title, indicates extraction of interpretable structure–property relationships from chemical data using explainable AI and LLMs; cited here as related literature.",
            "citation_title": "Extracting human interpretable structure-property relationships in chemistry using xai and large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs + XAI methods (as in cited work)",
            "model_description": "Not specified in this paper; cited as an example of using LLMs and explainable AI to extract human-interpretable relationships.",
            "input_domain": "Chemistry (structure-property relationships).",
            "corpus_size": null,
            "law_type": "Extraction of interpretable relationships / structure–property rules (per cited title).",
            "distillation_method": "Not specified in this paper (refer to that citation).",
            "evaluation_method": "Not specified in this paper.",
            "results_summary": "Referenced as related work demonstrating extraction of interpretable relationships with LLMs/XAI; no experimental details included here.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "uuid": "e3645.5",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reference: ResearchAgent (2024)",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "brief_description": "Cited recent work (arXiv:2404.07738) about iterative idea generation over literature with LLMs; listed as related work in the paper's bibliography.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (as reported in that work)",
            "model_description": "Not specified in this paper; cited as related contemporary work using LLMs iteratively over literature.",
            "input_domain": "Scientific literature (general/multidisciplinary).",
            "corpus_size": null,
            "law_type": "Iterative hypothesis/idea induction from literature (per cited title).",
            "distillation_method": "Not specified here.",
            "evaluation_method": "Not specified here.",
            "results_summary": "Cited as an example of LLM-based iterative idea-generation systems; details require consulting the cited manuscript.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "uuid": "e3645.6",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Reference: Scimon (2024)",
            "name_full": "Scimon: Scientific inspiration machines optimized for novelty",
            "brief_description": "Cited work (ACL 2024) on machines optimized for novelty in scientific inspiration; included in related work as a system that analyzes literature to propose novel directions.",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "mention",
            "model_name": "Graph + ML/LLM hybrid approaches (as in cited work)",
            "model_description": "Not specified here; referenced as related literature on automated novelty-driven suggestion systems.",
            "input_domain": "Scientific literature / novelty detection.",
            "corpus_size": null,
            "law_type": "Novelty-driven suggestion heuristics (per cited title).",
            "distillation_method": "Not specified in this paper.",
            "evaluation_method": "Not specified in this paper.",
            "results_summary": "Listed among related works that aim to propose novel scientific directions from literature; no details provided here.",
            "limitations_or_challenges": "Not discussed in this paper.",
            "uuid": "e3645.7",
            "source_info": {
                "paper_title": "Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Extracting human interpretable structure-property relationships in chemistry using xai and large language models",
            "rating": 2,
            "sanitized_title": "extracting_human_interpretable_structureproperty_relationships_in_chemistry_using_xai_and_large_language_models"
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2,
            "sanitized_title": "researchagent_iterative_research_idea_generation_over_scientific_literature_with_large_language_models"
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2,
            "sanitized_title": "scimon_scientific_inspiration_machines_optimized_for_novelty"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 1,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Agatha: automatic graph mining and transformer based hypothesis generation approach",
            "rating": 1,
            "sanitized_title": "agatha_automatic_graph_mining_and_transformer_based_hypothesis_generation_approach"
        }
    ],
    "cost": 0.016101249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders
7 Jan 2025</p>
<p>Xuemei Gu xuemei.gu@mpl.mpg.de 
Max Planck Institute for the Science of Light
Staudtstrasse 291058ErlangenGermany</p>
<p>Mario Krenn mario.krenn@mpl.mpg.de 
Max Planck Institute for the Science of Light
Staudtstrasse 291058ErlangenGermany</p>
<p>Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders
7 Jan 20250C809F4C3A04C4A126E74964855C0AFDarXiv:2405.17044v3[cs.AI]
The rapid growth of scientific literature makes it challenging for researchers to identify novel and impactful ideas, especially across disciplines.Modern artificial intelligence (AI) systems offer new approaches, potentially inspiring ideas not conceived by humans alone.But how compelling are these AI-generated ideas, and how can we improve their quality?Here, we introduce SciMuse, which uses 58 million research papers and a large-language model to generate research ideas.We conduct a large-scale evaluation in which over 100 research group leaders -from natural sciences to humanities -ranked more than 4,400 personalized ideas based on their interest.This data allows us to predict research interest using (1) supervised neural networks trained on human evaluations, and (2) unsupervised zero-shot ranking with large-language models.Our results demonstrate how future systems can help generating compelling research ideas and foster unforeseen interdisciplinary collaborations.</p>
<p>INTRODUCTION</p>
<p>An interesting idea is often at the heart of successful research projects, crucial for their success and impact.However, with the accelerating growth in the number of scientific papers published each year [1][2][3], it becomes increasingly difficult for researchers to uncover novel and interesting ideas.This challenge is even more pronounced for those seeking interdisciplinary collaborations, who have to navigate an overwhelming volume of literature.</p>
<p>Automated systems that extract insights from millions of scientific papers present a promising solution [2,4,5].Recent advances have demonstrated that analyzing relationships between research topics across vast scientific literature can reliably predict future research directions [6][7][8][9][10], forecast the potential impact of emerging work [11,12], and identify unconventional avenues for discovery [13].With the advent of powerful large-language models (LLMs), it is now possible to leverage knowledge from millions of scientific papers to generate concrete research ideas [14][15][16].</p>
<p>Yet, a crucial question remains: Are AI-generated research ideas compelling to experienced scientists?Previous studies have only conducted small-scale evaluations with six natural language processing (NLP) PhD students [14], three social science PhD students [15] and ten PhD students in computer science and biomedicine [16].However, perspectives from experienced researchers -who define and evaluate research projects through grant applications and shape their group's research agendaare essential for assessing the value of new ideas.Involving a larger group of more experienced evaluators could offer deeper insights into what makes a research idea compelling, how to generate and predict them.</p>
<p>Here, we introduce SciMuse, a system designed to suggest new personalized research ideas for individual scientists or collaborations.By using 58 million papers and their citation history, and leveraging automated access to GPT-4 [17], SciMuse formulates comprehensive research suggestions.The suggestions were evaluated by more than 100 research group leaders from the Max Planck Society across natural sciences and technology (e.g., from the Institutes for Biogeochemistry, Astrophysics, Quantum Optics, and Intelligent Systems) as well as social sciences and humanities (e.g., from the Institutes for Geoanthropology, Demographic Research, and Human Development).These experienced researchers rank the interest-level of more than 4,400 research ideas generated by SciMuse.This large dataset not only allows us to identify connections between properties of ideas and their interest-level, but also enables us to accurately predict the level of interest of new ideas with two fundamentally different methods: (1) training supervised neural networks and (2) using LLMs for zero-shot prediction without access to human evaluations, which will be important when expensive human-expert data is unavailable.Our results highlight SciMuse's potential to suggest compelling research directions and collaborations, revealing opportunities that might not be readily apparent and positioning AI as a source of inspiration in scientific discovery [18][19][20][21].</p>
<p>RESULTS</p>
<p>Knowledge graph generation -While we could directly use publicly available large language models such as GPT-4 [17] or Gemini [26] or Claude [27] to suggest new research ideas and collaborations, our control over the generated ideas would be limited to the structure of the prompt.Therefore, we decided to build a large knowledge graph from the scientific literature to identify the personalized research interests of scientists.</p>
<p>The knowledge graph, depicted in Fig. 1(a), consists of vertices, representing scientific concepts, and edges are drawn when two concepts jointly appear in a title or abstract of a scientific paper.The concept list is generated from the titles and abstracts of approximately Fig. 1.SciMuse suggests research ideas or collaborations using a knowledge graph and GPT-4.(a), Knowledge graph generation.Nodes represent scientific concepts extracted from 2.44 million paper titles and abstracts using the RAKE algorithm [22], further refined with custom NLP techniques, manual review, GPT, and Wikipedia (to restore mistakenly removed concepts), resulting in a final list of 123,128 concepts.Edges are formed when two concepts co-occur in titles or abstracts of over 58 million papers from OpenAlex [23], augmented with citation data as a proxy for impact.A mini-knowledge graph illustrates the connections for two example papers [24,25].(b), AI-generated research collaborations.We extract concepts from the publications of Researchers A and B, refine them using GPT-4, and identify relevant sub-networks in the knowledge graph.GPT-4 then uses these concept pairs, along with the researchers' research information, to generate personalized research ideas or collaboration projects.</p>
<p>2.44 million papers from arXiv, bioRxiv, ChemRxiv, and medRxiv, with a data cutoff in February 2023.Rapid Automatic Key-word Extraction (RAKE) algorithm based on statistical text analysis is used to extract candidate concepts [22].Those candidates are further refined using GPT, Wikipedia, and human annotators, resulting in 123,128 concepts in the natural and social sciences.We then use more than 58 million scientific papers from the open-source database OpenAlex [23] to create edges.These edges contain information about the co-occurrence of concepts in scientific papers (in titles and abstracts) and their subsequent citation rates.This new knowledge graph representation was recently introduced in [12] to predict the impact of future research topics.As a result, we have an evolving knowledge graph that captures part of the evolution of science from 1665 (a text by Robert Hooke on the observation of a great spot on Jupiter [29]) to April 2023.Details of the knowledge graph generation are shown in Fig. 1(a) and Supplementary Information.</p>
<p>Personalized research suggestions -We generate personalized research proposals for collaborations between two Max Planck Society group leaders, with one researcher evaluating the AI-suggested proposal.</p>
<p>To generate suggestions (Fig. 1(b)), we first identify each researcher's interests by analyzing all their publications from the past two years.Specifically, we extract their concepts from the titles and abstracts of these papers using the full concept list shown in Fig. 1(a).The extracted concepts are further refined by GPT-4, allowing us to build personalized subgraphs in the knowledge graph for each researcher.</p>
<p>With the researchers' subgraphs, we then generate a prompt for GPT-4 to create a research project (details in the Supplementary Information).In the prompt, we provide the titles of up to seven papers from each researcher and ask GPT-4 to create a research project based on two Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B).A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions.Blue dots denote natural sciences (nat) and red dots represent social sciences (soc).(c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.</p>
<p>selected scientific concepts.These concepts are chosen in one of three ways: (1) a randomly selected concept pair, (2) the concept pair with the highest predicted impact, or (3) no specific concept pair, relying only on paper titles.We predict the impact (in terms of expected future citations) by adapting the computational methods from [12], applying them to a different and much larger knowledge graph.While the third method does not directly use the knowledge graph features, it serves as a valuable sanity check for our approach (see Supplementary Information).The prompt incorporates self-reflection techniques [30], where GPT-4 generates three ideas, iteratively refines them twice, and then selects the most suitable one as the final result.</p>
<p>Large-scale human evaluation -To evaluate the interest level of AI-generated research ideas, we invited 110 research group leaders, who regularly evaluate research proposals and act upon research ideas, from 54 Max Planck Institutes within the Max Planck Society (one of the largest research organizations worldwide), to participate in the evaluation (see Fig. 2(a) and (b)).Each leader evaluated up to 48 personalized research projects on a scale from 1 ('not interesting') to 5 ('very interesting').Of the 110 researchers, 104 were from natural science institutes and 6 from social science institutes.On average, evaluators had published 59.7 papers (range: 9 to 402) and received 3,759.7 citations (range: 20 to 85,778).In total, we received 4,451 responses.As shown Fig. 3. Analysis of interest levels versus knowledge graph features.We analyzed how eight features of the knowledge graph correlate with researchers' interest levels.After normalizing these features using z-scores, we arranged them from lowest to highest and divided the data into 50 equal groups.For each group, we plotted the average feature value (x-axis) against the average interest level (y-axis) with standard deviations, to identify trends.(a) and (b) show node features, (c)-(e) show node citation metrics, (f ) shows an edge feature, (g) an edge citation metric, and (h) represents semantic distance between researchers' sub-networks (higher values indicate that the researchers' scientific fields are further apart).Data points include all 2,996 responses (blue), the top 50% of concept pairs by predicted impact (green), and the top 25% (red), using the neuralnetwork based impact prediction presented in [12].</p>
<p>in Fig. 2(c), 1,107 projects (nearly 25%) received a rating of 4 or 5, with 394 rated as very interesting.</p>
<p>Properties of interesting research suggestions -On average, we find no significant difference in interest levels between projects generated using random concept pairs, high-impact concept pairs, or without concept pairs.The similarity in results for the sanity test (projects generated without a concept pair used in the knowledge graph) and those using concept pairs enables us to analyze which knowledge graph features most strongly influence the perceived interest of a research project.Identifying these features can help us suggest future research projects with higher interest levels.</p>
<p>We use the 2,996 suggested research projects, which were created using concept pairs from the knowledge graph, and sort them by various knowledge graph features.Then we group them into 50 equal bins and calculated the mean interest and standard deviation for each bin.Fig. 3 shows these correlations and highlights several notable trends.For instance, the vertex degree and PageRank of the first concept, selected from the evaluating researcher's concept list, are strongly negatively correlated with human-assessed interest levels.This indicates that the more connected a concept is within the knowledge graph, the less appealing the research project appears.A similar trend is observed for citation rates: the more frequently a concept has been cited in the past, the less interesting the project is evaluated.Additionally, the semantic distance feature shows a negative correlation in Fig. 3(h), suggesting that research proposals involving researchers from similar fields are perceived as more interesting than those from more distant fields.This finding aligns with Fig. 2(c), where proposals from the same institute are generally rated higher than those from different institutes with distinct research focuses.We present these correlations for all 2,996 responses (blue), as well as for the top 50% and top 25% of concept pairs with the highest predicted impact (green and red, respectively) in Fig. 3, indicating that some correlations are more pronounced for suggestions using highimpact concept pairs.Predicting interest -We set out to predict which suggestions would receive high interest ratings (4 or 5 out of 5) using two fundamentally different methods.First, we trained a neural network on researchers' responses, using knowledge graph properties and human-evaluation rankings, without incorporating the GPT-generated text.Second, we employed GPT in a zero-shot manner to rank the 2,996 suggestions independently of human evaluations.Remarkably, both method showed high prediction accuracy despite the absence of crucial information (see Fig. 4).This suggests that intelligent concept pair selec-Fig.4. Predicting Scientific Interest.We use two distinct methods to predict interest levels: (1) a supervised neural network trained on human evaluations using only knowledge graph data (not the text of the actual suggestion), and (2) GPT in a zero-shot setting, ranking suggestions without getting any feedback from human evaluations.Both methods classify suggestions as highly interesting (ratings of 4 or 5) or not (below 4).The neural network uses 25 knowledge graph features and employs Monte Carlo cross-validation for accuracy estimation.For GPT, we conduct pairwise comparisons using personalized research details and rank suggestions through an ELO-based tournment system.(a), The ROC curve shows prediction accuracy of 64.5% for the neural network and 67.3% for GPT-4o.(b), The precision for top-N suggestions is significantly higher than random selection, with the top-1 precision reaching 70% for the neural network (51.0% for GPT-4o and 52.9% for GPT-3.5) and top-5 precision at 60.4% (46.7% for GPT-4o, 43.7% for GPT-3.5).(c), The probability of having at least one high-interest suggestion among the top N recommendations is significantly higher for the supervised neural network compared to random selection.Practically, evaluation data from experienced researchers may not always be available, thus it is very encouraging that LLMs, even without human evaluation, can rank suggestions effectively such that the highest interesting ones appear first.</p>
<p>tion alone can significantly influence interest rankings in the graph-based approach, while GPT's zero-shot ranking is valuable when human evaluations are unavailable.</p>
<p>For the supervised neural network, we used knowledge graph features to predict whether a research proposal would receive a high rating (4 or 5) or below 4. Given the limited training data -each of the 2,996 data point representing a research group leader's evaluation of a proposal's interest level -we employed a low-data machine learning approach with a small neural network (25 highperforming input features, 50 neurons in a single hidden layer, and one output neuron).Dropout was used for training [31], and Monte Carlo cross-validation [32] (which is also known as repeated random sub-sampling validation) was applied to ensure robust evaluation and maximize the utility of our limited data (see the Supplementary Information).Decision trees [33] were not able to outperform the quality of neural networks (see Supplementary Information).</p>
<p>In the second approach, we tasked GPT-3.5 and GPT-4o to rank all 2,996 suggestions from most to least interesting.Because the suggestions are personalized, we included relevant research details, such as recent paper titles, in the prompts.Specifically, GPT is asked to compare pairs of randomly selected suggestions and determine which is more interesting based on the personalized research interests of the evaluating researcher.This comparison was repeated between 22,000 and 45,000 times (depending on the GPT version), and suggestions were ranked using an ELO system, with each suggestion starting at an initial ELO score of 1400.The model's choices adjusted the rankings, resulting in a final sorted list of suggestions based on their predicted interest level.</p>
<p>For the binary classification task -ranking research ideas in as highly interesting (4 or 5 out of 5) or lowinterest (3 or below) -, both methods achieve an average Area Under the Curve (AUC) of the receiver operating characteristic (ROC) curve [34] of nearly 2/3 (Fig. 4(a)).More importantly, high precision is more relevant for our task, as we want to suggest highly interesting projects within a small subset of overall suggestions.To evaluate this, we calculate the precision for the top-N predicted concept pairs.For small N (e.g., N=3), the supervised approach achieves 66.4% while GPT-4o and GPT-3.5 reached 45.0% and 47.2%, respectively.This means that 66.4% of the top-3 suggestions were rated as highly interesting, significantly higher than random selection (23%), as shown in Fig. 4(b).Additionally, we also measured the probability of finding at least one highly interesting suggestion within the top-N suggestions.As shown in Fig. 4(c), our machine learning method offers a significantly higher probability of identifying interesting suggestions within the first few recommendations compared to random sampling.Surprisingly, GPT -without access to human evaluations -also ranks interesting suggestions much higher than a random approach.This capability is highly valuable in scenarios where human evaluations are costly or unavailable.Furthermore, it is encouraging that newer, more powerful models (e.g., GPT-4o) perform better at predicting human interests than earlier versions like GPT-3.5.</p>
<p>DISCUSSION</p>
<p>We demonstrate the largest human evaluation of AIgenerated research ideas to date, involving over 100 research group leaders across diverse scientific domains, including the natural sciences and humanities.</p>
<p>Beyond understanding the correlations between the properties of the generated research ideas and their interest rankings, we introduce two distinct methods to predict interest levels of AI-generated research ideas.First, we use thousands of human evaluations to train a supervised neural network that can predict interest values based on knowledge graph data alone -without needing the full suggestion text.This approach enables us to select more compelling abstract topics before generating specific ideas.Second, we demonstrate that LLMs can autonomously rank the interest levels of ideas with high quality, even without human evaluation data.This capability is especially valuable when human evaluations are unavailable.Furthermore, we observe that ranking quality improves with more advanced LLMs, which is promising for future developments.As publicly available models such as GPT [17], Gemini 1.5 [26], LLaMa3 [35], and Claude [27] continue to evolve at an accelerated pace [36] -especially when it comes to scientific domain knowledge [37,38] -we expect personalized research ideas to become increasingly targeted and relevant.</p>
<p>The methodologies employed by SciMuse have the potential to inspire novel and unexpected cross-disciplinary research on a large scale.By providing a broad view through the analysis of millions of scientific papers, SciMuse facilitates the discovery of interesting research collaborations between scientists from different fields that might otherwise remain undiscovered.Research in distant fields has been shown to have great potential for impactful, award-winning results [1,2,6,39].Therefore, large scientific organizations, national funding agencies, and other stakeholders may find value in adopting methodologies in the line of SciMuse to foster new highly interdisciplinary and interesting collaborations and ideas that might otherwise remain untapped.This, hopefully, could advance the progress and impact of science at a large scale.</p>
<p>One exciting possibility is in automated scientific experimentation [40].Currently, while large-language models have been integrated into laboratories [41,42], the main idea of the experiment has been provided by human scientists.In the future, one might envision the entire scientific process becoming fully automated -from the generation of an interesting idea, as we demonstrate here, to its automated execution and implementation.</p>
<p>We compiled a list of scientific concepts using metadata from arXiv, bioRxiv, medRxiv, and chemRxiv.The arXiv data is available on Kaggle, while metadata for other preprint sources can be accessed through their respective APIs.Our dataset includes ∼2.44 million prapers, with a cutoff date of February 2023.</p>
<p>For edge generation, we used the OpenAlex database snapshot (available on the OpenAlex bucket) with a cutoff date of April 2023.For more details, refer to the OpenAlex website [23].The original dataset was filtered to entries of journal papers that contain titles, abstracts, and citation data, resulting roughly 92 million papers.From these 92 million papers, 58 million contain at least two concepts of our concept list and can therefore for an edge in the knowledge graph.</p>
<p>B. The concept list</p>
<p>We analyzed the titles and abstracts of ∼2.44 million papers from four preprint datasets using the RAKE algorithm, enhanced with additional stopwords, to extract potential concept candidates.Initial filtering retained two-word concepts (e.g.gouy phase) appearing in at least nine articles and concepts with more than three words (e.g.recurrent neural network ) appearing in six or more, reducing the list to 726,439 concepts.</p>
<p>To further improve the quality of the identified concepts, we developed a suite of automated tools to eliminate domain-independent errors commonly associated with RAKE and performed a manual review to remove inaccuracies like non-conceptual phrases, verbs, and conjunctions.This step refined the list to 368,825 concepts.</p>
<p>Next, we used GPT-3.5 to further refine the concepts, which resulted in the removal of 286,311 concepts.We then employed Wikipedia to restore 40,614 mistakenly removed entries, resulting in a final refined list of 123,128 concepts.</p>
<p>C. Classification of Max Planck Institutes</p>
<p>We classified all 87 Max Planck Institutes into two categories: Class 1, abbreviated as nat, includes natural sciences, technology, mathematics, and medicine (68 institutes), while Class 2, abbreviated as soc, includes social sciences and humanities (19 institutes).The initial classification was done manually based on each institute's title and research field.To validate this, we further used GPT-4o for automatic classification, which perfectly matched with our manual classification.</p>
<p>D. Researcher Statistics</p>
<p>Over 100 highly experienced researchers, spanning fields from the natural sciences to the humanities, participated in evaluating the personalized research ideas.Table .S1 summarizes the researchers' publication and citation statistics as of January 1, 2024, when the evaluations were conducted.On average, the researchers had published 59 papers and received over 3,750 citations.The prompt to refine the researchers' concept list is shown below:</p>
<p>Prompt to Refine the Researchers' Concept List</p>
<p>A scientist has written the following papers: 0) title1 1) title2 2) title3 ... I have a noisy list of the researchers' topics of interest, and I would like your help in filtering them.Please look at the list below and return all concepts that are relevant to the scientist's research (based on their paper titles) and meaningful in the context of their research direction.The concepts can be detailed; I mainly want you to filter out concepts that are not meaningful, words that are not concepts, or concepts that are too general for the direction of the scientist (e.g., "artificial intelligence" might be a meaningful concept for a geologist, but not for a machine learning researcher).Do not change or add any conceptsonly remove or keep them.concept list=[c1, c2, c3, c4, c5, c6, ...]</p>
<p>F. Prompt to GPT-4 for project idea generation</p>
<p>The prompt used to suggest research ideas based on concept pairs from the knowledge graph is described as the follows:</p>
<p>Prompt to GPT-4 for Project Idea Generation Two researchers A and B, with expertise in "con-cept1" and "concept2" respectively, are eager to collaborate on a novel interdisciplinary project that leverages their unique strengths and creates synergy between their fields.</p>
<p>To better understand their backgrounds, here are the titles of recent publications from each researcher: Researcher A: 1: title1 2: title2 3: title3 ... Researcher B: 1: title1 2: title2 3: title3 ... Please suggest a creative and surprising scientific project that combines "concept1" and "con-cept2".In your response, follow this outline: First, explain "concept1" and "concept2" in one short sentence each.</p>
<p>Then, do the following three steps 3 times, improving in each time the response: A) Describe 4 interesting and new scientific contexts, in which those two concepts might appear together in a natural and useful way.B) Criticize the 4 contexts (one short sentence each), on how well the contexts merge the idea of the two concepts.C) Give a 2 sentence summary of your reflections above, on how well one can combine these concepts naturally and interestingly.</p>
<p>Then, start finding a project.Taking your reflections from (A-C) into account, define in your response a project title, followed by a brief explanation of the project's main objective.</p>
<p>Finally, address the following questions (Take the full reflections (A-C) into account): What specific interesting research questions will this project address, that will lead to innovative novel results?[2 bullet points, one sentence each] Rather than relying on a knowledge graph to supply "concept1" and "concept2", it is also possible to direct GPT-4 to extract these concepts from the research paper titles of Researchers A and B, respectively.GPT-4 can then use these identified concepts within the same prompting context to generate innovative research ideas.</p>
<p>G. Interest evaluation for three different generation methods</p>
<p>Fig. S1 presents the interest-level distributions for research suggestions generated using three different methods.The interest levels are notably similar between suggestions generated with and without concepts from the knowledge graph.This similarity enables us to analyze the correlations between knowledge graph properties and interest levels, and to use these properties for predicting the interest level of generated research proposals.(1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork.The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from [12], and applying them to a different and much larger knowledge graph).</p>
<p>H. Predicting high interest from knowledge graph features with neural networks</p>
<p>In Fig. 4 (main text), we aim to predict whether a research proposal will be rated with high interest.Specifically, using only data from the knowledge graph (excluding the final text generated by GPT), we predict if a proposal will receive an interest rating of 4 or 5 (on a scale of 1 to 5: not interesting to very interesting) or below 4.This is formulated as a binary classification task.</p>
<p>The input to the neural networks (using PyTorch [44]) consists of network-theoretical features extracted from the knowledge graph.For each concept pair in a re- search project, we compute 144 features.The first 141 features are derived from those used to predict the future impact of concept pairs, as described in [12].These features include node properties (e.g., node degree and PageRank [45]) and edge properties (e.g., Simpson similarity and Sørensen-Dice coefficient [46]).Several features also account for impact information, such as recent citation counts.The remaining three features include the predicted impact and two distance metrics between the researchers' subgraphs (Fig. 1(b)).The first distance metric measures the distance based solely on the concepts present in Researcher A and Researcher B's concept lists.In contrast, the second metric takes into account the entire neighborhood of these subgraphs by calculating semantic distances between all neighboring concepts and the concepts from the subgraphs.These features serve as the input to the neural network for predicting whether a proposal will achieve a high interest rating.</p>
<p>Given the small dataset size (2,996 answers with properties from the knowledge graph), we use a data-efficient learning method -a small neural network with dropout.The input layer consists of the 25 best-performing features (see Table.S2), selected from the total 144 by independently analyzing the feature importance of each and choosing the top 25.The neural network has one hidden layer with 50 neurons and a single output neuron.Mean square error is used as the loss function.</p>
<p>To ensure robust performance estimation for the small dataset, we use Monte Carlo cross-validation.The dataset is repeatedly split into training and validation sets, and the model is trained and evaluated on each split.This approach ensures that the performance metrics are robust and not dependent on a particular split of the data.This iterative process continues until the standard deviation of the mean AUC is less than 10 −2 3 , achieved after 130 iterations.This method provides a reliable estimate of the model's performance, which is crucial for small datasets where individual splits may lead to high variance in the evaluation metrics.</p>
<p>The neural network performance is not specifically sensitive to hyperparameter choices, thus we refrained from hyperparameter optimization, and instead used a reasonable defaults: learning rate=0.003,dropout=20%, weight decay=0.0007,training dataset=75%, validation dataset=15%, test dataset=10%.In Fig. S2, we investigate alternative hyper-parameters of the training process, and find that the results are robust under variations of the hyper-parameters.</p>
<p>I. Predicting high interest from knowledge graph features with decision trees</p>
<p>We experimented with other data-efficient learning methods, specifically with decision trees [33] using [47].However, decision trees did not outperform the neural network predictions, as can be seen in Fig. S3.These values confirm that neural networks are advantageous for the task of ranking research ideas by their interest value in a supervised way, which can also be confirmed in Fig. S5.</p>
<p>J. Zero-shot ranking of research suggestions by GPT</p>
<p>We ranked 2,996 research suggestions -previously evaluated by human experts -using GPT-3.5, GPT-4o, and GPT-4o-mini.For each pair of randomly selected suggestions, we asked the LLMs to rank which one was more interesting, considering the personalized research interests of the evaluating human expert.This pairwise comparison was repeated between 22,000 and 45,000 times (for GPT4o and GPT4o-mini, respectively).We treated this task as a tournament where all 2,996 suggestions compete pairwise against each other.Using the ELO ranking system, each suggestion started with an initial ELO score of 1400.Each comparison by GPT updated the ELO rankings based on the outcome, producing a final sorted list of suggestions from highest to lowest ELO score.We evaluated the ranking quality by calculating the AUC to determine how well the ranked list aligns with the human-expert evaluations of interest levels (Fig. S4).</p>
<p>K. Prediction of Interest with different methods</p>
<p>We show the full data of all five methods (supervised training with neural networks and decision trees, as well as unsupervised zero-shot prediction with GPT3.5, GPT4o and GPT4o-mini), with their corresponding AUC, top-N precision and top-N success probability, in Fig. S5.We see that the neural network outperforms decision trees when trained in a supervised way, and that GPT4o is better than the other tested models, when the ranking is performed in a zero-shot manner without giv-Fig.S4.Zero-Shot ranking of research suggestions by LLMs.The research suggestions are generated using the knowledge graph together with GPT4.They are then ranked using GPT4o, GPT4o-mini and GPT3.5, without feedback from the human evaluation.The human evaluation is used to compute the final quality of the ranking.the ranking is performed in a pair-wise choice where we ask the LLM to select the more interesting one given the research background of the researchers.One match is one pairwise selection.The LLMs perform 10,000 of these pairwise selections, which allows us to compute ELO scores for each generated research idea.</p>
<p>ing any information about the evaluations of humans.</p>
<p>L. Prompt engineering</p>
<p>We have explored manual and automated improvements of the prompts, both for the research question design and the zero-shot prediction.Specifically, we attempted to improve the prompts for the idea generation using GPT-4o.While the prompts were more structured, a small-scale evaluation did not show any improvement in terms of more interesting results.</p>
<p>M. GPT-4o and GPT-o1 for idea generation</p>
<p>We conducted two small-scale tests where GPT-4 and GPT-4o generated ideas using the exact same settings described above.</p>
<p>In the first test, three research group leaders evaluated 180 pairs of questions (one generated by GPT-4 and the other by GPT-4o using the same prompt).They found 31.1% of GPT-4 answers to be more interesting, while 60.56% favored GPT-4o answers (8.3% were draw).In the second test, a research group leader evaluated 11 pairs of questions (one generated by GPT-4o and the other by GPT-o1 with the same prompt), and all 11 ideas generated by GPT-o1 were ranked as more interesting.</p>
<p>These additional small-scale tests suggest that improved models can enhance idea generation, thus directly improving the results of SciMuse.</p>
<p>Prompt for Zero-Shot Ranking of Research Ideas I will present two research ideas.The first idea is for Researchers A1 and B1, and the second idea is for Researchers A2 and B2.Researchers A1 and A2 will evaluate how interesting they find the respective ideas.You will determine which of the two suggestions will be considered more interesting.</p>
<p>Fig. 2 .
2
Fig. 2. Large-scale human evaluation within the Max Planck Society.(a)-(b), The map of Germany, based on the GISCO statistical unit dataset from Eurostat [28], shows the locations of the Max Planck Institutes and the participating group leaders.A total of 4,451 personalized AI-generated research suggestions were evaluated by 110 research group leaders.Each suggestion represents a potential collaboration between the evaluating researcher (Researcher A) and another researcher (Researcher B) from the Max Planck Society, visualized as bi-colored edges (orange for Researcher A, green for Researcher B).A purple circle indicates collaborations within the same institute, and edge transparency reflects the number of evaluated suggestions.Blue dots denote natural sciences (nat) and red dots represent social sciences (soc).(c): Distribution of interest ratings on a scale from 1 ('not interesting') to 5 ('very interesting'), with 394 suggestions rated as very interesting and 713 rated 4. Ratings are further categorized by whether the collaborations are within or across institutes, and by research field affiliation in either the natural or social sciences.</p>
<p>Fig. S1 .
S1
Fig. S1.Interest levels across different generation methods.Research ideas are generated using three methods:(1) no concepts provided by the knowledge graph, (2) random concepts from the researchers' subnetwork, and (3) predicted high-impact concept pairs from the researchers' subnetwork.The figures displays: (a) overall interest levels (numbers within bars show the number of responses for that evaluation), (b) interest levels for ideas without using concepts from the knowledge graph, (c) interest levels with random concept pairs, and (d) interest levels using high-impact concept pairs (predicted by adapting the computational methods from[12], and applying them to a different and much larger knowledge graph).</p>
<p>Fig. S2 .
S2
Fig. S2.Choice of alternative hyper-parameters for training of neural network.We analyse the prediction of interest level quality (in terms of AUC) with different parameters of the neural network, such as different number of features, different number of layers and neurons, learning rate and drop-out rate.We see that the final results are robust under variations of the hyper-parameters.</p>
<p>Fig. S3 .
S3
Fig.S3.Choice of hyper-parameters for training of decision tree.The model is trained using Monte Carlo crossvalidation until a statistical uncertainty of σ=0.001 is reached.We find that no setting of number of features, maximum depth and minimal sample leaf can reach the performance of the data-efficient neural network.</p>
<p>The suggestions are randomly ordered, and you should evaluate each suggestion independently and without bias.### Researcher A1 Context and Suggestion 1: Here are a few papers of Researcher A1: (papersA1) Suggestion 1: [suggestion1] <strong>Summary for Researcher A1</strong>: Provide a one-sentence summary of Suggestion 1 in the context of Researcher A1. ### Researcher A2 Context and Suggestion 2: Here are a few papers of Researcher A2: (papersA2) Suggestion 2: [suggestion2] <strong>Summary for Researcher A2</strong>: Provide a one-sentence summary of Suggestion 2 in the context of Researcher A2.### Evaluation:Based on the summaries and the research interests of A1 and A2, evaluate which suggestion is more likely to be ranked higher in terms of interest.<strong>Result</strong>: If Suggestion 1 is ranked higher by Researcher A1 than Suggestion 2 is by Researcher A2, write 'RESULT: SUGGESTION 1'.Otherwise, write 'RESULT: SUGGESTION 2'.Remember, the suggestions are randomly ordered, and your evaluation should be impartial and based solely on the research interests of A1 and A2.</p>
<p>Fig. S5 .
S5
Fig.S5.Interest predictions with five methods.We reproduce Fig.4(main text), and add results from a supervised decision tree training, and an unsupervised GPT4o-mini.</p>
<p>Table S1 .
S1
Summary statistics of researchers' publications and citations.
Mean Median Min MaxNumber of papers59.736.09402Number of citations 3759.7 1630.020 85778E. Prompt to GPT-4 for concept refinement
ACKNOWLEDGEMENTSThe authors wholeheartedly thank all the researchers who spent the time participating in our study.The authors also thank the organizers of OpenAlex, arXiv, bioRxiv, medRxiv, and chemRxiv for making scientific resources freely accessible.X.G. acknowledges support from the Alexander von Humboldt Foundation.DATA AND CODE AVAILABILITYData for the knowledge graph is accessible on Zenodo at https://doi.org/10.5281/zenodo.13900962[43].Codes and evaluation data for this work are available on GitHub at https://github.com/artificial-scientist-lab/SciMuse.ETHICS STATEMENTThe research was reviewed and approved by the Ethics Council of the Max Planck Society.Supplementary InformationA. Datasets for creating knowledge graph
. S Fortunato, C T Bergstrom, K Börner, J A Evans, D Helbing, S Milojević, A M Petersen, F Radicchi, R Sinatra, B Uzzi, 10.1126/science.aao0185Science of science. 3591852018Science</p>
<p>D Wang, A.-L Barabási, The science of science. Cambridge University Press2021</p>
<p>Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. L Bornmann, R Haunschild, R Mutz, 10.1057/s41599-021-00903-wHumanities and Social Sciences Communications. 812021</p>
<p>. J A Evans, J G Foster, Metaknowledge , 10.1126/science.1201765Science. 3317212011</p>
<p>Sciscinet: A largescale open data lake for the science of science research. Z Lin, Y Yin, L Liu, D Wang, 10.1038/s41597-023-02198-9Scientific Data. 103152023</p>
<p>Choosing experiments to accelerate collective discovery. A Rzhetsky, J G Foster, I T Foster, J A Evans, 10.1073/pnas.150975711Proc. Natl. Acad. Sci. USA. 112145692015</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. M Krenn, A Zeilinger, 10.1073/pnas.1914370116Proc. Natl. Acad. Sci. USA. 11719102020</p>
<p>Agatha: automatic graph mining and transformer based hypothesis generation approach. J Sybrandt, I Tyagin, M Shtutman, I Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>R Nadkarni, D Wadden, I Beltagy, N A Smith, H Hajishirzi, T Hope, arXiv:2106.09700Scientific language models for biomedical knowledge base completion: an empirical study. 2021</p>
<p>Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. M Krenn, L Buffoni, B Coutinho, S Eppel, J G Foster, A Gritsevskiy, H Lee, Y Lu, J P Moutinho, N Sanjabi, 10.1038/s42256-023-00735-0Nature Machine Intelligence. 513262023</p>
<p>Surprising combinations of research contents and contexts are related to impact and emerge with scientific outsiders from distant disciplines. F Shi, J Evans, 10.1038/s41467-023-36741-4Nature Communications. 1416412023</p>
<p>X Gu, M Krenn, arXiv:2402.08640Forecasting high-impact research topics via machine learning on evolving knowledge graphs. 2024</p>
<p>Accelerating science with human-aware artificial intelligence. J Sourati, J A Evans, 10.1038/s41562-023-01648-zNature Human Behaviour. 716822023</p>
<p>Q Wang, D Downey, H Ji, T Hope, Scimon: Scientific inspiration machines optimized for novelty, Annual Meeting of the Association for Computational Linguistics. 2024</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, arXiv:2309.027262023</p>
<p>J Baek, S K Jauhar, S Cucerzan, S J Hwang, arXiv:2404.07738Researchagent: Iterative research idea generation over scientific literature with large language models. 2024</p>
<p>. J Achiam, S Adler, S Agarwal, L Ahmad, I Akkaya, F L Aleman, D Almeida, J Altenschmidt, S Altman, S Anadkat, arXiv:2303.08774Gpt-4 technical report. 2023</p>
<p>On scientific understanding with artificial intelligence. M Krenn, R Pollice, S Y Guo, M Aldeghi, A Cervera-Lierta, P Friederich, G Dos Passos, F Gomes, A Häse, A Jinich, Nigam, 10.1038/s42254-022-00518-3Nature Reviews Physics. 47612022</p>
<p>A computational inflection for scientific discovery. T Hope, D Downey, D S Weld, O Etzioni, E Horvitz, 10.1145/3576896Communications of the ACM. 66622023</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, 10.1038/s41586-023-06221-2Nature. 620472023</p>
<p>The impact of large language models on scientific discovery: a preliminary study using gpt-4. M R Ai4science, M A Quantum, arXiv:2311.073612023</p>
<p>Automatic keyword extraction from individual documents, Text mining: applications and theory. S Rose, D Engel, N Cramer, W Cowley, 10.1002/9780470689646.ch120101</p>
<p>J Priem, H Piwowar, R Orr, arXiv:2205.01833Openalex: A fullyopen index of scholarly works, authors, venues, institutions, and concepts. 2022</p>
<p>. K Wakonig, A Diaz, A Bonnin, M Stampanoni, A Bergamaschi, J Ihli, M Guizar-Sicairos, A Menzel, X-Ray Fourier Ptychography, 10.1126/sciadv.aav0282Science advances. 52822019</p>
<p>Ultrafast x-ray imaging of the light-induced phase transition in vo2. A S Johnson, D Perez-Salinas, K M Siddiqui, S Kim, S Choi, K Volckaert, P E Majchrzak, S Ulstrup, N Agarwal, K Hallman, 10.1038/s41567-022-01848-wNature Physics. 192152023</p>
<p>M Reid, N Savinov, D Teplyashin, D Lepikhin, T Lillicrap, J -B. Alayrac, R Soricut, A Lazaridou, O Firat, J Schrittwieser, arXiv:2403.05530Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, Papers with Code. 2024available at https://paperswithcode.com/paper/ the-claude-3-model-family-opus-sonnet-haiku</p>
<p>E Commission, Eurostat gisco -nuts geodata. 2024</p>
<p>A spot in one of the belts of jupiter. R Hooke, 10.1098/rstl.1665.0005Philosophical Transactions of the Royal Society of London. 131665</p>
<p>Self-refine: Iterative refinement with self-feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, Advances in Neural Information Processing Systems. 362024</p>
<p>Dropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 1519292014</p>
<p>Monte carlo cross validation. Q.-S Xu, Y.-Z Liang, 10.1016/S0169-7439(00)00122-2Chemometrics and Intelligent Laboratory Systems. 5612001</p>
<p>L Breiman, Classification and regression trees. 2017Routledge</p>
<p>Roc graphs: Notes and practical considerations for researchers. T Fawcett, Machine learning. 3112004</p>
<p>Llama 3: Open foundation and fine-tuned chat models. M Ai, 2024</p>
<p>W.-L Chiang, L Zheng, Y Sheng, A N Angelopoulos, T Li, D Li, H Zhang, B Zhu, M Jordan, J E Gonzalez, arXiv:2403.04132Chatbot arena: An open platform for evaluating llms by human preference. 2024</p>
<p>G P Wellawatte, P Schwaller, arXiv:2311.04047Extracting human interpretable structure-property relationships in chemistry using xai and large language models. 2023</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. W Liang, Y Zhang, H Cao, B Wang, D Y Ding, X Yang, K Vodrahalli, S He, D S Smith, Y Yin, 10.1056/AIoa2400196NEJM AI. 1AIoa2400196. 2024</p>
<p>Atypical combinations and scientific impact. B Uzzi, S Mukherjee, M Stringer, B Jones, 10.1126/science.1240474Science. 3424682013</p>
<p>Self-driving laboratories for chemistry and materials science. G Tom, S P Schmid, S G Baird, Y Cao, K Darvish, H Hao, S Lo, S Pablo-García, E M Rajaonson, M Skreta, 10.1021/acs.chemrev.4c00055Chemical Reviews. 12496332024</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, 10.1038/s41586-023-06792-0Nature. 6245702023</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, 10.1038/s42256-024-00832-8Nature Machine Intelligence. 12024</p>
<p>Interesting scientific idea generation using knowledge graphs and llms: Evaluations with 100 research group leaders [data set. X Gu, 10.5281/zenodo.139009622024</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in neural information processing systems. 322019</p>
<p>The pagerank citation ranking : Bringing order to the web. L Page, S Brin, R Motwani, T Winograd, 1999Stanford InfoLab</p>
<p>A.-L Barabási, Network Science. Cambridge University Press2016</p>
<p>Scikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 1228252011</p>            </div>
        </div>

    </div>
</body>
</html>
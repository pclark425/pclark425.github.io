<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5473 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5473</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5473</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8" target="_blank">Towards Mitigating LLM Hallucination via Self Reflection</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5473.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5473.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Interactive Self-Reflection Loop</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Self-Reflection Generate-Score-Refine Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative generate-score-refine methodology that has the model (1) generate background knowledge, (2) score factuality with an in-context learned, reference-free scorer, (3) prompt the model to refine knowledge until a factuality threshold is met, then (4) generate answers, score consistency (CTRLEval), refine answers, and (5) check question-entailment (sentence-BERT) and repeat the cycle if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>multiple (Vicuna-7B, Alpaca-Lora-7B, ChatGPT, MedAlpaca-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to several LLMs evaluated in the paper: Vicuna (LLaMA fine-tuned on ShareGPT, 7B), Alpaca-LoRA (LoRA adaptation of Alpaca, 7B), ChatGPT (RLHF-trained chat model), MedAlpaca (LLaMA-based model fine-tuned on medical data, 7B).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Interactive self-reflection loop / generate-score-refine</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Three nested loops: (A) Factual Knowledge Acquiring Loop: model generates background knowledge; a factuality scorer (in-context instruction scoring: sum log P(k_t | k_<t, T(D,Q))) evaluates knowledge; if score below threshold, model is prompted to refine; iterate until threshold. (B) Knowledge-Consistent Answering Loop: generate answer conditioned on final knowledge; evaluate with CTRLEval consistency; if below threshold prompt to refine answer; iterate. (C) Question-Entailment Answering Loop: compute entailment via sentence-BERT embedding similarity; if below threshold return to knowledge loop. The process uses exact numeric scores in prompts and repeats up to configured max iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medical generative question answering (PubMedQA, MedQuAD, MEDIQA2019, LiveMedQA2017, MASH-QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generative QA in the medical domain across multiple datasets (yes/no/long answers, consumer health QA, dataset-specific QA), evaluating factuality/faithfulness of free-form answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across datasets and models the paper reports higher MedNLI (entailment) and improved hallucination metrics when using the loop. Examples on PubMedQA: Vicuna-7B_L MedNLI(sample)=0.6380, sentence=0.6326; CTRLEval consistency=-1.74; F1=16.95; R-L=13.47. Alpaca-Lora-7B_L PubMedQA MedNLI(sample)=0.4640, sentence=0.4475; CTRLEval=-1.85; F1=13.69; R-L=13.42. ChatGPT_L PubMedQA MedNLI(sample)=0.6824, sentence=0.6598; CTRLEval=-1.73; F1=23.45; R-L=16.54. MedAlpaca-7B_L PubMedQA MedNLI(sample)=0.4720, sentence=0.4545; CTRLEval=-2.38; F1=15.41; R-L=14.45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines (direct generation) have lower MedNLI and worse CTRLEval and sometimes lower overlap scores. Examples on PubMedQA: Vicuna-7B baseline MedNLI(sample)=0.4684, sentence=0.5919; CTRLEval=-1.95; F1=15.51; R-L=12.06. Alpaca-Lora-7B baseline MedNLI(sample)=0.0940, sentence=0.1002; CTRLEval=-3.25; F1=9.15; R-L=11.09. ChatGPT baseline MedNLI(sample)=0.5850, sentence=0.4199; CTRLEval=-2.09; F1=18.17; R-L=13.48. In ablation (PubMedQA) where refinement is removed: Vicuna-7B_L w/o refinement MedNLI(sample)=0.4520, sentence=0.5799 (vs 0.6380/0.6326 with full loop); ChatGPT_L w/o refinement MedNLI(sample)=0.5180, sentence=0.5942 (vs 0.6824/0.6598 with full loop).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: consistent increases in MedNLI (entailment) across models and datasets (e.g., Alpaca-Lora sample-level MedNLI on PubMedQA increased from 0.0940 to 0.4640 with loop, ~5x relative improvement); ablation shows removal of refinement, aspect descriptions, or explicit score numbers reduces MedNLI/CTRLEval. Human evaluation: reduced Query-Inconsistent, Tangentiality and Fact-Inconsistent rates (PubMedQA, Vicuna: Query-Inconsistent 0.67% -> 0.00%, Tangentiality 6.04% -> 2.00%, Fact-Inconsistent 8.69% -> 7.38%; ChatGPT: Fact-Inconsistent 8.06% -> 6.33%). Qualitative: case studies (Tables 5 and 7) where looped outputs align better with golden answers and remove irrelevant/incorrect text.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Authors report the method reduces but does not eliminate hallucination; models can still produce ungrounded information especially for complex or ambiguous queries. The method is not yet ready for real-world deployment and is complementary to retrieval approaches. Limitations include focus on English medical queries (limited generalizability), dependence on thresholds, reliance on the model's internal knowledge (low-frequency terms remain problematic), and extra compute/latency due to iterative loops. Ablations show that incorrect/missing refinement aspects or score signals degrade effectiveness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5473.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5473.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B_L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna-7B with Interactive Self-Reflection Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna (7B LLaMA-based chat model) evaluated with the interactive generate-score-refine self-reflection loop, showing improved entailment and reduced hallucination on medical GQA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based model fine-tuned on ShareGPT conversations (open-source); used here at 7B parameter scale (referred to as Vicuna-7B).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Interactive self-reflection loop (generate-score-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Applied the three-loop generate-score-refine process: generate background knowledge, score factuality via in-context scorer, refine until threshold; generate answer conditioned on vetted knowledge, score consistency (CTRLEval) and refine; check entailment via sentence-BERT and potentially repeat. Implementation used thresholds (factuality -1.0, consistency -5.0, entailment 0.8) and max loop counts of 3.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PubMedQA (and other medical GQA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Biomedical generative QA (PubMedQA: yes/no/maybe and long answers) and other medical QA datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PubMedQA: MedNLI sample=0.6380, sentence=0.6326; CTRLEval=-1.74; F1=16.95; R-L=13.47. Human eval (PubMedQA): Query-Inconsistent 0.00%, Tangentiality 2.00%, Entailment 98.00%, Fact-Inconsistent 7.38%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>PubMedQA baseline Vicuna-7B: MedNLI sample=0.4684, sentence=0.5919; CTRLEval=-1.95; F1=15.51; R-L=12.06. Human eval baseline: Query-Inconsistent 0.67%, Tangentiality 6.04%, Entailment 93.29%, Fact-Inconsistent 8.69%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>MedNLI (sample) improved from 0.4684 -> 0.6380; human annotations show reductions in query-inconsistency (0.67% -> 0.00%) and tangentiality (6.04% -> 2.00%) and modest reduction in fact-inconsistency (8.69% -> 7.38%). Ablation (w/o refinement) reduces MedNLI to 0.4520, showing refinement loop is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement is present but modest on overlap metrics (F1/R-L) and does not fully eliminate hallucinations; still some fact-inconsistent and generic sentences remain. Gains depend on thresholds and quality of generated knowledge; low-frequency medical terms still pose problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5473.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5473.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca-Lora-7B_L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca-LoRA 7B with Interactive Self-Reflection Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alpaca-LoRA (LoRA adaptation of Alpaca) at 7B with the self-reflection loop; the loop notably increases MedNLI/entailment and reduces hallucination relative to the direct-generation baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca-LoRA-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LoRA-based adaptation replicating Alpaca instruction-following behavior; used here in 7B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Interactive self-reflection loop (generate-score-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-loop generate-score-refine process: generate background knowledge, in-context factuality scoring and refinement, answer generation conditioned on knowledge with CTRLEval-based refinement, and entailment check via sentence-BERT; repeated until thresholds met (max loops=3).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PubMedQA (and other medical GQA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Medical generative QA datasets assessing factuality and answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PubMedQA: MedNLI sample=0.4640, sentence=0.4475; CTRLEval=-1.85; F1=13.69; R-L=13.42.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>PubMedQA baseline Alpaca-Lora-7B: MedNLI sample=0.0940, sentence=0.1002; CTRLEval=-3.25; F1=9.15; R-L=11.09.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Large relative improvements in MedNLI (sample) from 0.0940 -> 0.4640 on PubMedQA and substantial CTRLEval and overlap gains, indicating fewer hallucinations and better consistency with knowledge when using the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although MedNLI improves markedly, overlap metrics remain modest and the method doesn't guarantee removal of hallucinations; performance still sensitive to refinement prompts, thresholds, and the model's internal knowledge for rare medical terms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5473.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5473.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT_L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT with Interactive Self-Reflection Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's ChatGPT (RLHF-trained instruction-following model) evaluated with the generate-score-refine loop, showing improved entailment and reduced factual errors on medical GQA, with notable gains in automatic and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RLHF-trained chat-oriented language model from OpenAI (API used for generation in experiments); exact parameter count not specified in this paper, but evaluated in medical GQA.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Interactive self-reflection loop (generate-score-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Applied the same three-loop process: generate background knowledge, factuality scoring (in-context scorer) and iterative refinement; generate answers guided by vetted knowledge, consistency scoring with CTRLEval and refinement; entailment checking with sentence-BERT to iterate if needed. Max main/response loops=3; knowledge loop max varied (1 or 3) per dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PubMedQA (and other medical GQA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Biomedical generative question answering across multiple datasets assessing factuality/consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PubMedQA: MedNLI sample=0.6824, sentence=0.6598; CTRLEval=-1.73; F1=23.45; R-L=16.54. Human eval (PubMedQA): Tangentiality reduced from 18.00% -> 17.33%, Fact-Inconsistent 8.06% -> 6.33%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>PubMedQA baseline ChatGPT: MedNLI sample=0.5850, sentence=0.4199; CTRLEval=-2.09; F1=18.17; R-L=13.48. Ablation (w/o refinement) ChatGPT_L: MedNLI sample=0.5180, sentence=0.5942; F1=19.60; R-L=15.25.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Automatic MedNLI improved (sample-level 0.5850 -> 0.6824) and CTRLEval improved; ablation removing refinement reduces MedNLI substantially. Human annotations show reductions in fact-inconsistent sentence rates (8.06% -> 6.33%) and small reductions in tangentiality. Case studies show looped outputs aligning better with gold answers and removing irrelevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are smaller than for smaller/flawed baselines in some overlap metrics; ChatGPT tends to produce more generic sentences that may still be present after looping. The method adds latency and does not fully eliminate hallucinations; effectiveness depends on the model's parametric knowledge and dataset characteristics.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5473.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5473.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MedAlpaca-7B_L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MedAlpaca-7B with Interactive Self-Reflection Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MedAlpaca (LLaMA-based model fine-tuned on medical dialogue/QA) evaluated with the self-reflection loop; shows measurable gains in entailment and consistency compared to direct generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MedAlpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A LLaMA-derived model fine-tuned on medical instruction/dialogue and QA data (medical-specialized 7B model).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Interactive self-reflection loop (generate-score-refine)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same three-loop generate-score-refine process (factual knowledge generation and scoring, answer generation with consistency scoring and refinement, and entailment verification), iterated until thresholds met (max loops=3).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>PubMedQA (and other medical GQA datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Medical generative QA evaluating factuality and answer consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>PubMedQA: MedNLI sample=0.4720, sentence=0.4545; CTRLEval=-2.38; F1=15.41; R-L=14.45.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>PubMedQA baseline MedAlpaca-7B: MedNLI sample=0.2050, sentence=0.2912; CTRLEval=-3.30; F1=9.90; R-L=11.20.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>MedNLI sample-level more than doubled (0.2050 -> 0.4720) and CTRLEval improved, indicating fewer hallucinations and better factual alignment after using the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although improvements are substantial versus baseline, overlap scores remain modest; domain-finetuned models still produce unrelated content and non-existent references in some cases. Method reliance on model's internal knowledge limits outcomes for rare concepts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Webgpt: Browser-assisted questionanswering with human feedback <em>(Rating: 2)</em></li>
                <li>Generate rather than retrieve: Large language models are strong context generators <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5473",
    "paper_id": "paper-aa7e3b2b88ae3b434e3d5a343c7e3109c9d48ea8",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "Interactive Self-Reflection Loop",
            "name_full": "Interactive Self-Reflection Generate-Score-Refine Loop",
            "brief_description": "An iterative generate-score-refine methodology that has the model (1) generate background knowledge, (2) score factuality with an in-context learned, reference-free scorer, (3) prompt the model to refine knowledge until a factuality threshold is met, then (4) generate answers, score consistency (CTRLEval), refine answers, and (5) check question-entailment (sentence-BERT) and repeat the cycle if needed.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "multiple (Vicuna-7B, Alpaca-Lora-7B, ChatGPT, MedAlpaca-7B)",
            "model_description": "Applied to several LLMs evaluated in the paper: Vicuna (LLaMA fine-tuned on ShareGPT, 7B), Alpaca-LoRA (LoRA adaptation of Alpaca, 7B), ChatGPT (RLHF-trained chat model), MedAlpaca (LLaMA-based model fine-tuned on medical data, 7B).",
            "reflection_method_name": "Interactive self-reflection loop / generate-score-refine",
            "reflection_method_description": "Three nested loops: (A) Factual Knowledge Acquiring Loop: model generates background knowledge; a factuality scorer (in-context instruction scoring: sum log P(k_t | k_&lt;t, T(D,Q))) evaluates knowledge; if score below threshold, model is prompted to refine; iterate until threshold. (B) Knowledge-Consistent Answering Loop: generate answer conditioned on final knowledge; evaluate with CTRLEval consistency; if below threshold prompt to refine answer; iterate. (C) Question-Entailment Answering Loop: compute entailment via sentence-BERT embedding similarity; if below threshold return to knowledge loop. The process uses exact numeric scores in prompts and repeats up to configured max iterations.",
            "num_iterations": 3,
            "task_name": "Medical generative question answering (PubMedQA, MedQuAD, MEDIQA2019, LiveMedQA2017, MASH-QA)",
            "task_description": "Generative QA in the medical domain across multiple datasets (yes/no/long answers, consumer health QA, dataset-specific QA), evaluating factuality/faithfulness of free-form answers.",
            "performance_with_reflection": "Across datasets and models the paper reports higher MedNLI (entailment) and improved hallucination metrics when using the loop. Examples on PubMedQA: Vicuna-7B_L MedNLI(sample)=0.6380, sentence=0.6326; CTRLEval consistency=-1.74; F1=16.95; R-L=13.47. Alpaca-Lora-7B_L PubMedQA MedNLI(sample)=0.4640, sentence=0.4475; CTRLEval=-1.85; F1=13.69; R-L=13.42. ChatGPT_L PubMedQA MedNLI(sample)=0.6824, sentence=0.6598; CTRLEval=-1.73; F1=23.45; R-L=16.54. MedAlpaca-7B_L PubMedQA MedNLI(sample)=0.4720, sentence=0.4545; CTRLEval=-2.38; F1=15.41; R-L=14.45.",
            "performance_without_reflection": "Baselines (direct generation) have lower MedNLI and worse CTRLEval and sometimes lower overlap scores. Examples on PubMedQA: Vicuna-7B baseline MedNLI(sample)=0.4684, sentence=0.5919; CTRLEval=-1.95; F1=15.51; R-L=12.06. Alpaca-Lora-7B baseline MedNLI(sample)=0.0940, sentence=0.1002; CTRLEval=-3.25; F1=9.15; R-L=11.09. ChatGPT baseline MedNLI(sample)=0.5850, sentence=0.4199; CTRLEval=-2.09; F1=18.17; R-L=13.48. In ablation (PubMedQA) where refinement is removed: Vicuna-7B_L w/o refinement MedNLI(sample)=0.4520, sentence=0.5799 (vs 0.6380/0.6326 with full loop); ChatGPT_L w/o refinement MedNLI(sample)=0.5180, sentence=0.5942 (vs 0.6824/0.6598 with full loop).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: consistent increases in MedNLI (entailment) across models and datasets (e.g., Alpaca-Lora sample-level MedNLI on PubMedQA increased from 0.0940 to 0.4640 with loop, ~5x relative improvement); ablation shows removal of refinement, aspect descriptions, or explicit score numbers reduces MedNLI/CTRLEval. Human evaluation: reduced Query-Inconsistent, Tangentiality and Fact-Inconsistent rates (PubMedQA, Vicuna: Query-Inconsistent 0.67% -&gt; 0.00%, Tangentiality 6.04% -&gt; 2.00%, Fact-Inconsistent 8.69% -&gt; 7.38%; ChatGPT: Fact-Inconsistent 8.06% -&gt; 6.33%). Qualitative: case studies (Tables 5 and 7) where looped outputs align better with golden answers and remove irrelevant/incorrect text.",
            "limitations_or_failure_cases": "Authors report the method reduces but does not eliminate hallucination; models can still produce ungrounded information especially for complex or ambiguous queries. The method is not yet ready for real-world deployment and is complementary to retrieval approaches. Limitations include focus on English medical queries (limited generalizability), dependence on thresholds, reliance on the model's internal knowledge (low-frequency terms remain problematic), and extra compute/latency due to iterative loops. Ablations show that incorrect/missing refinement aspects or score signals degrade effectiveness.",
            "uuid": "e5473.0"
        },
        {
            "name_short": "Vicuna-7B_L",
            "name_full": "Vicuna-7B with Interactive Self-Reflection Loop",
            "brief_description": "Vicuna (7B LLaMA-based chat model) evaluated with the interactive generate-score-refine self-reflection loop, showing improved entailment and reduced hallucination on medical GQA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna-7B",
            "model_description": "LLaMA-based model fine-tuned on ShareGPT conversations (open-source); used here at 7B parameter scale (referred to as Vicuna-7B).",
            "reflection_method_name": "Interactive self-reflection loop (generate-score-refine)",
            "reflection_method_description": "Applied the three-loop generate-score-refine process: generate background knowledge, score factuality via in-context scorer, refine until threshold; generate answer conditioned on vetted knowledge, score consistency (CTRLEval) and refine; check entailment via sentence-BERT and potentially repeat. Implementation used thresholds (factuality -1.0, consistency -5.0, entailment 0.8) and max loop counts of 3.",
            "num_iterations": 3,
            "task_name": "PubMedQA (and other medical GQA datasets)",
            "task_description": "Biomedical generative QA (PubMedQA: yes/no/maybe and long answers) and other medical QA datasets.",
            "performance_with_reflection": "PubMedQA: MedNLI sample=0.6380, sentence=0.6326; CTRLEval=-1.74; F1=16.95; R-L=13.47. Human eval (PubMedQA): Query-Inconsistent 0.00%, Tangentiality 2.00%, Entailment 98.00%, Fact-Inconsistent 7.38%.",
            "performance_without_reflection": "PubMedQA baseline Vicuna-7B: MedNLI sample=0.4684, sentence=0.5919; CTRLEval=-1.95; F1=15.51; R-L=12.06. Human eval baseline: Query-Inconsistent 0.67%, Tangentiality 6.04%, Entailment 93.29%, Fact-Inconsistent 8.69%.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "MedNLI (sample) improved from 0.4684 -&gt; 0.6380; human annotations show reductions in query-inconsistency (0.67% -&gt; 0.00%) and tangentiality (6.04% -&gt; 2.00%) and modest reduction in fact-inconsistency (8.69% -&gt; 7.38%). Ablation (w/o refinement) reduces MedNLI to 0.4520, showing refinement loop is critical.",
            "limitations_or_failure_cases": "Improvement is present but modest on overlap metrics (F1/R-L) and does not fully eliminate hallucinations; still some fact-inconsistent and generic sentences remain. Gains depend on thresholds and quality of generated knowledge; low-frequency medical terms still pose problems.",
            "uuid": "e5473.1"
        },
        {
            "name_short": "Alpaca-Lora-7B_L",
            "name_full": "Alpaca-LoRA 7B with Interactive Self-Reflection Loop",
            "brief_description": "Alpaca-LoRA (LoRA adaptation of Alpaca) at 7B with the self-reflection loop; the loop notably increases MedNLI/entailment and reduces hallucination relative to the direct-generation baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Alpaca-LoRA-7B",
            "model_description": "LoRA-based adaptation replicating Alpaca instruction-following behavior; used here in 7B variant.",
            "reflection_method_name": "Interactive self-reflection loop (generate-score-refine)",
            "reflection_method_description": "Same three-loop generate-score-refine process: generate background knowledge, in-context factuality scoring and refinement, answer generation conditioned on knowledge with CTRLEval-based refinement, and entailment check via sentence-BERT; repeated until thresholds met (max loops=3).",
            "num_iterations": 3,
            "task_name": "PubMedQA (and other medical GQA datasets)",
            "task_description": "Medical generative QA datasets assessing factuality and answer quality.",
            "performance_with_reflection": "PubMedQA: MedNLI sample=0.4640, sentence=0.4475; CTRLEval=-1.85; F1=13.69; R-L=13.42.",
            "performance_without_reflection": "PubMedQA baseline Alpaca-Lora-7B: MedNLI sample=0.0940, sentence=0.1002; CTRLEval=-3.25; F1=9.15; R-L=11.09.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Large relative improvements in MedNLI (sample) from 0.0940 -&gt; 0.4640 on PubMedQA and substantial CTRLEval and overlap gains, indicating fewer hallucinations and better consistency with knowledge when using the loop.",
            "limitations_or_failure_cases": "Although MedNLI improves markedly, overlap metrics remain modest and the method doesn't guarantee removal of hallucinations; performance still sensitive to refinement prompts, thresholds, and the model's internal knowledge for rare medical terms.",
            "uuid": "e5473.2"
        },
        {
            "name_short": "ChatGPT_L",
            "name_full": "ChatGPT with Interactive Self-Reflection Loop",
            "brief_description": "OpenAI's ChatGPT (RLHF-trained instruction-following model) evaluated with the generate-score-refine loop, showing improved entailment and reduced factual errors on medical GQA, with notable gains in automatic and human evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "RLHF-trained chat-oriented language model from OpenAI (API used for generation in experiments); exact parameter count not specified in this paper, but evaluated in medical GQA.",
            "reflection_method_name": "Interactive self-reflection loop (generate-score-refine)",
            "reflection_method_description": "Applied the same three-loop process: generate background knowledge, factuality scoring (in-context scorer) and iterative refinement; generate answers guided by vetted knowledge, consistency scoring with CTRLEval and refinement; entailment checking with sentence-BERT to iterate if needed. Max main/response loops=3; knowledge loop max varied (1 or 3) per dataset.",
            "num_iterations": 3,
            "task_name": "PubMedQA (and other medical GQA datasets)",
            "task_description": "Biomedical generative question answering across multiple datasets assessing factuality/consistency.",
            "performance_with_reflection": "PubMedQA: MedNLI sample=0.6824, sentence=0.6598; CTRLEval=-1.73; F1=23.45; R-L=16.54. Human eval (PubMedQA): Tangentiality reduced from 18.00% -&gt; 17.33%, Fact-Inconsistent 8.06% -&gt; 6.33%.",
            "performance_without_reflection": "PubMedQA baseline ChatGPT: MedNLI sample=0.5850, sentence=0.4199; CTRLEval=-2.09; F1=18.17; R-L=13.48. Ablation (w/o refinement) ChatGPT_L: MedNLI sample=0.5180, sentence=0.5942; F1=19.60; R-L=15.25.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Automatic MedNLI improved (sample-level 0.5850 -&gt; 0.6824) and CTRLEval improved; ablation removing refinement reduces MedNLI substantially. Human annotations show reductions in fact-inconsistent sentence rates (8.06% -&gt; 6.33%) and small reductions in tangentiality. Case studies show looped outputs aligning better with gold answers and removing irrelevant content.",
            "limitations_or_failure_cases": "Improvements are smaller than for smaller/flawed baselines in some overlap metrics; ChatGPT tends to produce more generic sentences that may still be present after looping. The method adds latency and does not fully eliminate hallucinations; effectiveness depends on the model's parametric knowledge and dataset characteristics.",
            "uuid": "e5473.3"
        },
        {
            "name_short": "MedAlpaca-7B_L",
            "name_full": "MedAlpaca-7B with Interactive Self-Reflection Loop",
            "brief_description": "MedAlpaca (LLaMA-based model fine-tuned on medical dialogue/QA) evaluated with the self-reflection loop; shows measurable gains in entailment and consistency compared to direct generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "MedAlpaca-7B",
            "model_description": "A LLaMA-derived model fine-tuned on medical instruction/dialogue and QA data (medical-specialized 7B model).",
            "reflection_method_name": "Interactive self-reflection loop (generate-score-refine)",
            "reflection_method_description": "Same three-loop generate-score-refine process (factual knowledge generation and scoring, answer generation with consistency scoring and refinement, and entailment verification), iterated until thresholds met (max loops=3).",
            "num_iterations": 3,
            "task_name": "PubMedQA (and other medical GQA datasets)",
            "task_description": "Medical generative QA evaluating factuality and answer consistency.",
            "performance_with_reflection": "PubMedQA: MedNLI sample=0.4720, sentence=0.4545; CTRLEval=-2.38; F1=15.41; R-L=14.45.",
            "performance_without_reflection": "PubMedQA baseline MedAlpaca-7B: MedNLI sample=0.2050, sentence=0.2912; CTRLEval=-3.30; F1=9.90; R-L=11.20.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "MedNLI sample-level more than doubled (0.2050 -&gt; 0.4720) and CTRLEval improved, indicating fewer hallucinations and better factual alignment after using the loop.",
            "limitations_or_failure_cases": "Although improvements are substantial versus baseline, overlap scores remain modest; domain-finetuned models still produce unrelated content and non-existent references in some cases. Method reliance on model's internal knowledge limits outcomes for rare concepts.",
            "uuid": "e5473.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2
        },
        {
            "paper_title": "Webgpt: Browser-assisted questionanswering with human feedback",
            "rating": 2
        },
        {
            "paper_title": "Generate rather than retrieve: Large language models are strong context generators",
            "rating": 2
        }
    ],
    "cost": 0.0183035,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Mitigating Hallucination in Large Language Models via Self-Reflection</h1>
<p>Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, Pascale Fung<br>Center for Artificial Intelligence Research (CAiRE)<br>Hong Kong University of Science and Technology<br>zjiad@connect.ust.hk, pascale@ece.ust.hk</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown promise for generative and knowledgeintensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks which require world or domain knowledge (Petroni et al., 2021). One representative task is generative question-answering (GQA) which provides relevant information in response to queries (Li et al., 2021a; Su et al., 2023; Nakano et al., 2021) and can also serve to probe the capabilities of language models (Wang et al., 2021). How-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of hallucinated Generative QA ${ }^{2}$. PTEN mutation is not typically associated with Noonan syndrome ${ }^{3}$. Thus, despite the plausible sounding answer, it is a hallucinated one. We highlight the hallucinated parts in pink and underline them.
ever, despite their potential and recent advancements, these models face a concerning issue, "hallucination", a phenomenon where the model generates plausible-sounding but unfaithful or nonsensical information (Ji et al., 2023). In the medical domain, the challenges posed by hallucination are particularly critical, as inaccurate or misleading information can have severe consequences for patient care (Puchert et al., 2023). Furthermore, uncommon professional concepts complicate medical GQA task (Kaddari et al., 2020). For the GQA example in Figure 1, PTEN (Phosphatase and tensin homolog) mutation is not typically associated with Noonan syndrome, so the answer sounds plausible but is hallucinated. Therefore, it is crucial to understand and mitigate these hallucinations to ensure the reliability and safety of AI-enabled medical services (Morley et al., 2020).</p>
<p>Parallel to retrieving external relevant knowledge (Lewis et al., 2020; Guu et al., 2020; Izacard et al., 2022), some current works (Yu et al., 2023; Wang et al., 2021; Roberts et al., 2020; Xu et al.,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>2022; Sun et al., 2023) explore leveraging the parametric knowledge in LLMs and tap into their potential for knowledge-intensive tasks. GQA works in other domains (Lin et al., 2021; Su et al., 2022) underscore the importance of addressing hallucination and improving faithfulness. However, the current understanding of the extent of hallucination in medical answers generated by LLMs remains unclear, and there is a need to explore the potential for further improvement in this aspect.</p>
<p>To fill the gap, this study investigates hallucination in the context of medical GQA systems, particularly in general LLMs like Vicuna (Chiang et al., 2023), Alpaca-LoRA (Wang, 2023), ChatGPT (OpenAI, 2023a), and medical LLMs like MedAlpaca (Han et al., 2023), and Robinmedical (Diao et al., 2023) on popular medical datasets: PubMedQA (Jin et al., 2019), MedQuAD (Ben Abacha and Demner-Fushman, 2019), MEDIQA2019 (Ben Abacha et al., 2019), LiveMedQA2017 (Ben Abacha et al., 2017), MASH-QA (Zhu et al., 2020). We evaluate the incidence of hallucinations, explore the potential cause, and propose a strategy to mitigate this issue. Through a comprehensive analysis of the recent models, we hope to increase the understanding of hallucination in these systems and provide a road map towards more reliable AI-assisted healthcare.</p>
<p>Current research works (Yin et al., 2023; Burns et al., 2022; Rajpurkar et al., 2018; Kadavath et al., 2022; Manakul et al., 2023) highlight a gap between surface realization and inherent knowledge in NLG tasks. Models can realize they are generating something hallucinated in some way. To reduce this gap and mitigate hallucinations in medical GQA, we devise an iterative, introspective process that leverages the multi-turn interactivity and multitask ability of LLMs. Our self-reflective methodology initiates the generation of pertinent background knowledge for a given question, followed by a factuality evaluation. Upon detection of discrepancies, the model is urged to self-correct, utilizing its inherent reflective capacities to refine the knowledge. This cyclical process is repeated until a satisfactory level of factuality is achieved. In the answering stage, we employ a similar generation-score-refine strategy to ensure consistency between the generated answer and the background knowledge. Additionally, an entailment evaluation is conducted between the answer and the question. If the generated answer fails to meet the standard, the process
returns to the initial stage, repeating the cycle. Our method fosters a dynamic interaction between the system and its knowledge, enhancing the model's ability to provide accurate, reliable, and factually grounded responses in healthcare settings.</p>
<p>The experimental results of our method showcase its effectiveness across LLMs with varying parameters, including 7B and 175B, on all five datasets. This robust performance highlights the generalizability and scalability of our approach, further validating its efficacy in the context of medical question-answering tasks. Our method explores extracting and digging knowledge from LLMs, leveraging their full potential, and strives to approach their upper performance bound. Our method can also combine other techniques such as utilizing external knowledge and more powerful LLMs in the future to contribute to the development of robust application systems. In summary, the major contributions of this work are threefold:</p>
<ul>
<li>We provide a comprehensive examination of the hallucination phenomenon in medical GQA systems, particularly those employing five LLMs across five medical GQA datasets.</li>
<li>We propose an innovative self-reflection method to mitigate hallucination in LLMs. The iterative feedback loop process generates, scores, and refines knowledge and answers until they reach satisfactory levels, enhancing the accuracy and reliability of the answers.</li>
<li>Experimental results demonstrate the effectiveness, generalizability, and scalability of our method. This investigation sheds light on the potential of LLMs as valuable tools for medical GQA tasks, showcasing their ability to provide meaningful insights without explicit training on specific datasets.</li>
</ul>
<h2>2 Related Work</h2>
<p>Medical Question Answering Medical QA systems have shown significant potential for enhancing information accessibility and understanding in the healthcare field. These systems respond to various question formats, including Yes/No (Tsatsaronis et al., 2015; Jin et al., 2019), multi-choice (Pal et al., 2022; Li et al., 2021b; Welbl et al., 2018; Berant et al., 2014; Abacha et al., 2019; Morante et al., 2012), extractive (Tsatsaronis et al., 2015; Dhingra et al., 2018; uster and Daelemans, 2018; Tang et al., 2020; Mller et al., 2020; Pampari et al., 2018), and generative (Savery et al., 2020;</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Generated Answer</th>
<th style="text-align: center;">Golden Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Fact <br> Inconsistency</td>
<td style="text-align: center;">Is Noonan syndrome inherited?</td>
<td style="text-align: center;">Noonan syndrome is not inherited. It is a genetic mutation that occurs sporadically.</td>
<td style="text-align: center;">Noonan syndrome is inherited in an autosomal dominant manner... Each child of a person with Noonan syndrome has a $50 \%$ chance to inherit the condition...</td>
</tr>
<tr>
<td style="text-align: center;">Query <br> Inconsistency</td>
<td style="text-align: center;">Cardiac reoperations in octogenarians: Do they really benefit?</td>
<td style="text-align: center;">Proper use of Vitamin C to stop smoking: Taking Vitamin C by mouth can help smokers quit...</td>
<td style="text-align: center;">Yes... sustainable recovery of the survivors offering the prospect of a highly independent and satisfying life...</td>
</tr>
<tr>
<td style="text-align: center;">Tangentiality</td>
<td style="text-align: center;">c-Kit-dependent growth of uveal melanoma cells: a potential therapeutic target?</td>
<td style="text-align: center;">A muscle tumor formed in the anterior eyeball of the body, called uveal melanoma, is a rare tumor that often leads to visual impairment ... the addition of GATA3 can reduce the scale of cancer...</td>
<td style="text-align: center;">Yes ... c-kit is vastly expressed in uveal melanoma, suggest that the c-kit molecular pathway may be important in uveal melanoma growth...</td>
</tr>
</tbody>
</table>
<p>Table 1: The category of problematic answers with corresponding representative examples.</p>
<p>Moll et al., 2016; Jin et al., 2019; Ben Abacha and Demner-Fushman, 2019; Ben Abacha et al., 2019, 2017). The introduction of pre-trained language models has further enhanced the capabilities of GQA systems, enabling them to generate fluent and meaningful responses to medical queries (Soni and Roberts, 2020; Liu et al., 2022; Savery et al., 2020; Alsentzer et al., 2019; Kaddari et al., 2020).</p>
<h2>Hallucination in Generative Question Answer-</h2>
<p>ing Faithful GQA, which aims to generate answers strictly grounded in the source text or valid external knowledge, has gained significant research attention (Nakano et al., 2021; Su et al., 2022, 2023). The more faithful the answer is, the less hallucinated content it contains. Other terms like semantic drift, factual correctness can also reflect hallucination level (Li et al., 2021a; Su et al., 2022). Rationale-Enriched Answer Generator (REAG) (Li et al., 2021a) add an extraction task to obtain answer rationale and generate answers with high confidence. Read-before-Generate (Su et al., 2022) combines answer generation with machine reading to incorporate fine-grained, answer-related salient information. A benchmark (Lin et al., 2021) measures the truthfulness of answers generated by language models across various domains. These studies underscore the importance of reducing hallucination, a key focus of our work.</p>
<p>Large Language Models The advent of LLMs, including GPT-3 (Brown et al., 2020), ChatGPT (OpenAI, 2023a), LLaMA (Touvron et al., 2023), and GPT-4 (OpenAI, 2023b), has revolutionized natural language processing tasks, showcasing their impressive language capabilities in generating fluent, contextually relevant responses (Brown et al., 2020; OpenAI, 2023a; Touvron et al., 2023; OpenAI, 2023b). In addition, emergent abilities are revealed from these models, such as incontext learning (Min et al., 2022), zero-shot in-
struction (Ouyang et al., 2022; Wei et al., 2021), and chain-of-thought reasoning (Wei et al.). However, their deployment in practical applications has also surfaced challenges related to the control, bias, and reliability (Tamkin et al., 2021), where hallucination has recently become an increasingly visible issue (OpenAI, 2023a; Bang et al., 2023).</p>
<h2>3 Analysis of Hallucination</h2>
<p>In this section, we directly ask LLMs medical questions from five datasets leveraging their zero-shot capability. We then comprehensively evaluate and analyze the generated answers, with a focus on examining the occurrence of hallucination.</p>
<h3>3.1 Models</h3>
<p>We evaluate the generated answers from five LLMs, including three general LLMs and two LLMs finetuned in the medical domain. Vicuna (Chiang et al., 2023) is trained by fine-tuning LLaMA on user-shared conversations from ShareGPT. AlpacaLoRA (Wang, 2023) employs Low-Rank Adaptation (LoRA) to replicate the results of Stanford's Alpaca model. ChatGPT (OpenAI, 2023a) interprets prompts and provides comprehensive responses using Reinforcement Learning from Human Feedback (RLHF). MedAlpaca (Han et al., 2023) is built upon the frameworks of LLaMA and fine-tuned on instruction-tuning formatted medical dialogue and QA texts. Robin-medical (Diao et al., 2023) is fine-tuned LLaMA in the medical domain using LMFlow.</p>
<h3>3.2 Dataset</h3>
<p>PubMedQA (Jin et al., 2019) is a biomedical QA dataset containing 1k expert-labeled instances which include questions derived from research article titles, abstracts as the context, long answers from abstract conclusions, and concise</p>
<p>yes/no/maybe answers. MedQuAD (Ben Abacha and Demner-Fushman, 2019) comprises 47,457 QA pairs from National Institutes of Health websites and covers various medical topics including diseases, medications, and diagnostic tests. We use the medical QA dataset from MEDIQA2019 (Ben Abacha et al., 2019) challenge and consider answers with scores 3 and 4 as golden answers. LiveMedQA2017 (Ben Abacha et al., 2017) contains annotated medical QA pairs for question analysis and answering systems. MASH-QA (Zhu et al., 2020) includes 34k QA pairs from the consumer health domain designed for Multiple Answer Spans Healthcare QA. Except for PubMedQA, answer annotation in these datasets involves manual extraction and copying from authentic web content. While the answers are pertinent and verifiable, there is room for improvement in terms of contextual coherence and question linkage. Please see Appendix A for details and an example.</p>
<h3>3.3 Evaluation Protocols</h3>
<p>To evaluate the generation quality, we follow the previous work (Su et al., 2022) utilizing GQA metrics: unigram F1 and ROUGE-L (Lin, 2004). However, the widely used n-gram similarity metrics often fail to discriminate the hallucinated/incorrect answers and correlate weakly with human judgments (Lee et al., 2021; Zhou et al., 2021). We introduce Med-NLI (Medical Natural Language Inference) to assess the logical consistency/entailment of generated answers with the provided context or the reference answer. We adopt SciFive (Phan et al., 2021), a T5 model pre-trained on extensive biomedical corpora. Our evaluation occurs at two levels: Sample-level Med-NLI evaluates whether each generated answer entails (1), is neutral (0), or contradicts (-1) the context or the reference answer. Sentence-level Med-NLI determines the same but for each individual sentence within the generated response. We also use CTRLEval (Ke et al., 2022), an unsupervised, referencefree, and task-agnostic evaluation metric that assesses generation from various aspects by formulating each aspect into multiple text-infilling tasks. In our work, we specifically employ the consistency aspect of this metric.</p>
<h3>3.4 Results and Discussion</h3>
<p>Table 2 shows the experimental results on automatic metrics over the test sets from five datasets.</p>
<p>Error Analysis After analyzing 250 directly generated examples from the five models, we classify problematic answers into three categories: Fact Inconsistency, Query Inconsistency, and Tangentiality. Please refer to Table 1 and Figure 2 for the representative example and incidence for each category and model. We consider the first two as the hallucination problem.</p>
<ol>
<li>Fact Inconsistency refers to answers that provide information that is inconsistent or in conflict with the fact. It arises when the model fails to appropriately recall relevant knowledge when responding to the question. The example answer in Table 1 incorrectly states that Noonan syndrome is not inherited while it is inherited in an autosomal dominant manner.</li>
<li>Query Inconsistency refers to answers that are unrelated to the query or nonsensical. It occurs when the model neither responds to the question nor invokes relevant knowledge appropriately. The example answer in Table 1 discusses the benefits of Vitamin but does not mention cardiac reoperations. 3. Tangentiality refers to answers that provide information related to the topic but do not directly address the question. It occurs when the model does not further process mastered knowledge, such as inductive, deductive, and logical reasoning. The example answer in Table 1 tangentially discusses uveal membrane but fails to mention the effect of c-Kit on uveal membrane.</li>
</ol>
<p>Addressing these challenges requires models to recall factual knowledge, contextual understanding, and reasoning abilities. Further exploration and development of these capabilities in LLMs are necessary to improve the reliability and trustworthiness of generation systems.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The incidence of each category of problematic answers in each model.</p>
<p>The Effect of Fine-Tuning on Medical Domain LLMs fine-tuned on medical domain texts (Han et al., 2023; Diao et al., 2023) have demonstrated</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MedNLI $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CtrlEval $\uparrow$</th>
<th style="text-align: center;">F1 $\uparrow$</th>
<th style="text-align: center;">R-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spl</td>
<td style="text-align: center;">Sent</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PubMedQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">.4684</td>
<td style="text-align: center;">.5919</td>
<td style="text-align: center;">-1.95</td>
<td style="text-align: center;">15.51</td>
<td style="text-align: center;">12.06</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L</td>
<td style="text-align: center;">.6380</td>
<td style="text-align: center;">.6326</td>
<td style="text-align: center;">-1.74</td>
<td style="text-align: center;">16.95</td>
<td style="text-align: center;">13.47</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B</td>
<td style="text-align: center;">.0940</td>
<td style="text-align: center;">.1002</td>
<td style="text-align: center;">-3.25</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">11.09</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B_L</td>
<td style="text-align: center;">.4640</td>
<td style="text-align: center;">.4475</td>
<td style="text-align: center;">-1.85</td>
<td style="text-align: center;">13.69</td>
<td style="text-align: center;">13.42</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.5850</td>
<td style="text-align: center;">.4199</td>
<td style="text-align: center;">-2.09</td>
<td style="text-align: center;">18.17</td>
<td style="text-align: center;">13.48</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L</td>
<td style="text-align: center;">.6824</td>
<td style="text-align: center;">.6598</td>
<td style="text-align: center;">-1.73</td>
<td style="text-align: center;">23.45</td>
<td style="text-align: center;">16.54</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B</td>
<td style="text-align: center;">.2050</td>
<td style="text-align: center;">.2912</td>
<td style="text-align: center;">-3.30</td>
<td style="text-align: center;">9.90</td>
<td style="text-align: center;">11.20</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B_L</td>
<td style="text-align: center;">.4720</td>
<td style="text-align: center;">.4545</td>
<td style="text-align: center;">-2.38</td>
<td style="text-align: center;">15.41</td>
<td style="text-align: center;">14.45</td>
</tr>
<tr>
<td style="text-align: center;">Robin-medical-7B</td>
<td style="text-align: center;">.2900</td>
<td style="text-align: center;">.2900</td>
<td style="text-align: center;">-6.73</td>
<td style="text-align: center;">3.50</td>
<td style="text-align: center;">3.18</td>
</tr>
<tr>
<td style="text-align: center;">MEDIQA2019</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">.8400</td>
<td style="text-align: center;">.6330</td>
<td style="text-align: center;">-3.06</td>
<td style="text-align: center;">23.94</td>
<td style="text-align: center;">12.81</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L</td>
<td style="text-align: center;">.8933</td>
<td style="text-align: center;">.6868</td>
<td style="text-align: center;">-2.50</td>
<td style="text-align: center;">24.65</td>
<td style="text-align: center;">13.80</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B</td>
<td style="text-align: center;">.7226</td>
<td style="text-align: center;">.6492</td>
<td style="text-align: center;">-2.48</td>
<td style="text-align: center;">5.96</td>
<td style="text-align: center;">4.83</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B_L</td>
<td style="text-align: center;">.8400</td>
<td style="text-align: center;">.6565</td>
<td style="text-align: center;">-2.37</td>
<td style="text-align: center;">10.93</td>
<td style="text-align: center;">8.35</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.7467</td>
<td style="text-align: center;">.5741</td>
<td style="text-align: center;">-2.77</td>
<td style="text-align: center;">20.02</td>
<td style="text-align: center;">11.35</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L</td>
<td style="text-align: center;">.8067</td>
<td style="text-align: center;">.7180</td>
<td style="text-align: center;">-2.70</td>
<td style="text-align: center;">21.53</td>
<td style="text-align: center;">11.85</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B</td>
<td style="text-align: center;">.6333</td>
<td style="text-align: center;">.5329</td>
<td style="text-align: center;">-3.08</td>
<td style="text-align: center;">8.06</td>
<td style="text-align: center;">6.95</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B_L</td>
<td style="text-align: center;">.7200</td>
<td style="text-align: center;">.5531</td>
<td style="text-align: center;">-2.84</td>
<td style="text-align: center;">11.14</td>
<td style="text-align: center;">9.04</td>
</tr>
<tr>
<td style="text-align: center;">Robin-medical-7B</td>
<td style="text-align: center;">.7200</td>
<td style="text-align: center;">.7414</td>
<td style="text-align: center;">-5.12</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">2.30</td>
</tr>
<tr>
<td style="text-align: center;">MAMI-QA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">.8103</td>
<td style="text-align: center;">.6403</td>
<td style="text-align: center;">-2.46</td>
<td style="text-align: center;">14.75</td>
<td style="text-align: center;">9.82</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L</td>
<td style="text-align: center;">.8381</td>
<td style="text-align: center;">.7518</td>
<td style="text-align: center;">-2.06</td>
<td style="text-align: center;">20.69</td>
<td style="text-align: center;">13.47</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B</td>
<td style="text-align: center;">.7226</td>
<td style="text-align: center;">.6492</td>
<td style="text-align: center;">-1.66</td>
<td style="text-align: center;">15.01</td>
<td style="text-align: center;">11.71</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B_L</td>
<td style="text-align: center;">.8363</td>
<td style="text-align: center;">.7812</td>
<td style="text-align: center;">-1.84</td>
<td style="text-align: center;">15.23</td>
<td style="text-align: center;">11.95</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.7685</td>
<td style="text-align: center;">.6425</td>
<td style="text-align: center;">-2.12</td>
<td style="text-align: center;">23.34</td>
<td style="text-align: center;">15.28</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L</td>
<td style="text-align: center;">.7904</td>
<td style="text-align: center;">.7476</td>
<td style="text-align: center;">-2.14</td>
<td style="text-align: center;">23.47</td>
<td style="text-align: center;">15.92</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B</td>
<td style="text-align: center;">.5629</td>
<td style="text-align: center;">.4705</td>
<td style="text-align: center;">-2.28</td>
<td style="text-align: center;">13.26</td>
<td style="text-align: center;">11.47</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B_L</td>
<td style="text-align: center;">.7445</td>
<td style="text-align: center;">.6983</td>
<td style="text-align: center;">-1.96</td>
<td style="text-align: center;">13.47</td>
<td style="text-align: center;">11.77</td>
</tr>
<tr>
<td style="text-align: center;">Robin-medical-7B</td>
<td style="text-align: center;">.0080</td>
<td style="text-align: center;">.6378</td>
<td style="text-align: center;">-4.13</td>
<td style="text-align: center;">4.39</td>
<td style="text-align: center;">5.66</td>
</tr>
<tr>
<td style="text-align: center;">MedQuAD</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">.8411</td>
<td style="text-align: center;">.6564</td>
<td style="text-align: center;">-2.56</td>
<td style="text-align: center;">19.64</td>
<td style="text-align: center;">11.87</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L</td>
<td style="text-align: center;">.8503</td>
<td style="text-align: center;">.7355</td>
<td style="text-align: center;">-2.47</td>
<td style="text-align: center;">24.04</td>
<td style="text-align: center;">14.73</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B</td>
<td style="text-align: center;">.8104</td>
<td style="text-align: center;">.7580</td>
<td style="text-align: center;">-2.29</td>
<td style="text-align: center;">11.86</td>
<td style="text-align: center;">9.59</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B_L</td>
<td style="text-align: center;">.8443</td>
<td style="text-align: center;">.7723</td>
<td style="text-align: center;">-2.26</td>
<td style="text-align: center;">14.34</td>
<td style="text-align: center;">11.25</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.8000</td>
<td style="text-align: center;">.6820</td>
<td style="text-align: center;">-2.75</td>
<td style="text-align: center;">25.59</td>
<td style="text-align: center;">16.01</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L</td>
<td style="text-align: center;">.8317</td>
<td style="text-align: center;">.7597</td>
<td style="text-align: center;">-2.57</td>
<td style="text-align: center;">27.19</td>
<td style="text-align: center;">16.08</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B</td>
<td style="text-align: center;">.6634</td>
<td style="text-align: center;">.5328</td>
<td style="text-align: center;">-2.80</td>
<td style="text-align: center;">12.19</td>
<td style="text-align: center;">10.61</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B_L</td>
<td style="text-align: center;">.8343</td>
<td style="text-align: center;">.7777</td>
<td style="text-align: center;">-2.60</td>
<td style="text-align: center;">12.19</td>
<td style="text-align: center;">10.96</td>
</tr>
<tr>
<td style="text-align: center;">Robin-medical-7B</td>
<td style="text-align: center;">.0775</td>
<td style="text-align: center;">.5656</td>
<td style="text-align: center;">-3.78</td>
<td style="text-align: center;">4.88</td>
<td style="text-align: center;">5.96</td>
</tr>
<tr>
<td style="text-align: center;">LiveMedQA2017</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B</td>
<td style="text-align: center;">.5481</td>
<td style="text-align: center;">.5212</td>
<td style="text-align: center;">-2.11</td>
<td style="text-align: center;">26.20</td>
<td style="text-align: center;">14.63</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L</td>
<td style="text-align: center;">.6731</td>
<td style="text-align: center;">.5967</td>
<td style="text-align: center;">-2.00</td>
<td style="text-align: center;">26.75</td>
<td style="text-align: center;">15.21</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B</td>
<td style="text-align: center;">.3365</td>
<td style="text-align: center;">.2460</td>
<td style="text-align: center;">-1.73</td>
<td style="text-align: center;">12.90</td>
<td style="text-align: center;">9.68</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-Lora-7B_L</td>
<td style="text-align: center;">.5962</td>
<td style="text-align: center;">.5090</td>
<td style="text-align: center;">-1.72</td>
<td style="text-align: center;">13.22</td>
<td style="text-align: center;">9.79</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">.6442</td>
<td style="text-align: center;">.4879</td>
<td style="text-align: center;">-2.15</td>
<td style="text-align: center;">25.47</td>
<td style="text-align: center;">14.70</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L</td>
<td style="text-align: center;">.8462</td>
<td style="text-align: center;">.5627</td>
<td style="text-align: center;">-2.09</td>
<td style="text-align: center;">25.93</td>
<td style="text-align: center;">14.74</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B</td>
<td style="text-align: center;">.2019</td>
<td style="text-align: center;">.1765</td>
<td style="text-align: center;">-2.18</td>
<td style="text-align: center;">10.79</td>
<td style="text-align: center;">8.87</td>
</tr>
<tr>
<td style="text-align: center;">MedAlpaca-7B_L</td>
<td style="text-align: center;">.3750</td>
<td style="text-align: center;">.2682</td>
<td style="text-align: center;">-2.17</td>
<td style="text-align: center;">13.82</td>
<td style="text-align: center;">10.47</td>
</tr>
<tr>
<td style="text-align: center;">Robin-medical-7B</td>
<td style="text-align: center;">.3077</td>
<td style="text-align: center;">.6827</td>
<td style="text-align: center;">-5.25</td>
<td style="text-align: center;">2.40</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody>
</table>
<p>Table 2: Automatic evaluation results for LLMs with our interactive self-reflection loop (_L) and baselines. "Spl" and "Sent" mean sample and sentence level.
improved performance on certain types of questions, such as multi-choice QA, benefiting from the availability of abundant training data. However, their performance in GQA tasks suffers from issues like unrelated content, grammar issues, unwarranted templates, non-existent references, and a lack of explanatory reasoning. As in Table 2, Robin-medical obtains the lowest F1 and Rouge-L scores. For example, given the question: "Who can</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Google Ngrams Frequency of each category of problematic answers.
get eczema?", Robin-medical generates: "(A) All (B) $10 \%$ (C) $20 \%$ (D) $30 \%$ \n Output: A. " The discrepancy between MedAplpaca and Robin-medical indicates that instruction learning is more suitable for LLMs than non-instruction tuning in our tasks. Due to Robin-medical's relatively poor generation performance, we exclude it from further experiments.</p>
<p>The Effect of Frequency Considering the impracticality of measuring frequency according to an LLM's pre-train corpus, we use Google N grams $^{4}$ as a proxy of the text distribution in the natural world and pre-training corpora. We randomly select 100 samples generated by the general models. We exact the keywords or topics of the questions, which usually are disease names. We take average frequencies between the years 19502019 (McKenna et al., 2023) of these keywords. As in Figure 3, the problematic answers have lower frequencies than fine ones. This suggests that low frequency may be a potential cause of hallucinations, which requires more exploration to prove.</p>
<h2>4 Hallucination Mitigation Method</h2>
<h3>4.1 Methodology</h3>
<p>To address the issue of hallucination, we propose an iterative self-reflection process that leverages the capabilities of LLMs in generating and refining responses. Illustrated in Figure 4, our methodology comprises three loops: Factual Knowledge Acquiring Loop, Knowledge-Consistent Answering Loop, and Question-Entailment Answering Loop.</p>
<p>Factual Knowledge Acquiring Loop First, the model generates background knowledge based on</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The overview of interactive self-reflection method, comprising (1) Factual Knowledge Acquiring Loop (yellow), (2) Knowledge-Consistent Answering Loop (green), and (3) Question-Entailment Answering Loop (black).
the provided question. This step capitalizes on the inherent ability of LLMs to synthesize contextrelevant information, forming the foundation for the subsequent scoring and refining stages.</p>
<p>Then, a factuality evaluation of the generated knowledge is conducted with a customized and reference-free scorer. The factuality scorer is designed through in-context instruction learning with the following formula:</p>
<p>$$
F s(\mathbf{k} \mid D, Q)=\sum_{t=1}^{m} \log P\left(k_{t} \mid \mathbf{k}_{&lt;t}, T(D, Q)\right)
$$</p>
<p>The knowledge to be evaluated is $\mathbf{k}=$ $\left{k_{1}, k_{2}, \ldots, k_{m}\right} . \quad D$ is the few-shot demonstrations that are annotated examples and $Q$ is the given question. $T(\cdot)$ is the prompt template including the definition of factuality and task description: "Based on Question, please generate the factual knowledge. To do this, please consider these factors: Verifiability, Objectivity, and Reliability of Source. Note that this evaluation should be based on the best available medical knowledge. \n\nQuestion:...\nKnowledge:..." In-context instruction learning is verified in the aspect of relevance, fluency, informativeness, etc. on text generation tasks (Fu et al., 2023).</p>
<p>If the factuality score is lower than the threshold set in the evaluation phase, we instruct the model self-reflect and refine the knowledge with the following prompt: "The factuality score for the knowledge is XXX (less than THRESHOLD_FACTUAL), which means the knowledge is not strongly supported by empirical evidence. Please refine the
knowledge to improve its factuality."
This generate-score-refine strategy is repeated interactively until the generated knowledge reaches the satisfactory factuality level. This iterative procedure fosters dynamic and iterative interaction between the system and its generated knowledge. And ensures that the model progressively refines the produced background knowledge, adhering it to established facts.</p>
<p>Knowledge-Consistent Answering Loop Once the generated knowledge attains the requisite quality, the model proceeds to generate an answer based on the provided question and the final knowledge with the template: "Refer to the knowledge: "final_knowledge" and answer the question: XXX with one paragraph." A consistency evaluation of the generated answer is conducted with CTRLEval (introduced in  3.3). If the generated answer's consistency score lowers the threshold, the model is prompted to introspect, self-correct, and revise the answer with "The consistency score for the knowledge is XXX (less than THRESHOLD_CONS), which means the alignment and consistency between response and knowledge are low. Please refine the response to improve its consistency."</p>
<p>This generate-score-refine strategy is repeated until the generated answer reaches the consistency level. This iterative procedure ensures that the model progressively refines the produced answer aligning with the vetted background knowledge, thus maintaining its integrity.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Query- <br> Inconsistent $\downarrow$</th>
<th style="text-align: center;">Tangentiality $\downarrow$</th>
<th style="text-align: center;">Fact- <br> Inconsistent $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">$0.67 \%$</td>
<td style="text-align: center;">$6.04 \%$</td>
<td style="text-align: center;">$8.69 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B_L</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$\underline{\mathbf{2 . 0 0 \%}}$</td>
<td style="text-align: center;">$\mathbf{7 . 3 8 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$18.00 \%$</td>
<td style="text-align: center;">$8.06 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT_L</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 7 . 3 3 \%}$</td>
<td style="text-align: center;">$\mathbf{6 . 3 3 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation results on PubMedQA.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">MedNLI $\uparrow$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CtrlEval $\uparrow$</th>
<th style="text-align: center;">F1 $\uparrow$</th>
<th style="text-align: center;">R-L $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Spl</td>
<td style="text-align: center;">Sent</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7B_L (ours)</td>
<td style="text-align: center;">.6380</td>
<td style="text-align: center;">.6326</td>
<td style="text-align: center;">-1.74</td>
<td style="text-align: center;">16.95</td>
<td style="text-align: center;">13.47</td>
</tr>
<tr>
<td style="text-align: center;">w/o refinement</td>
<td style="text-align: center;">.4520</td>
<td style="text-align: center;">.5799</td>
<td style="text-align: center;">-1.87</td>
<td style="text-align: center;">16.90</td>
<td style="text-align: center;">13.13</td>
</tr>
<tr>
<td style="text-align: center;">w/o aspect</td>
<td style="text-align: center;">.4940</td>
<td style="text-align: center;">.6276</td>
<td style="text-align: center;">-1.75</td>
<td style="text-align: center;">16.92</td>
<td style="text-align: center;">13.65</td>
</tr>
<tr>
<td style="text-align: center;">w/o num</td>
<td style="text-align: center;">.6320</td>
<td style="text-align: center;">.5915</td>
<td style="text-align: center;">-2.23</td>
<td style="text-align: center;">16.92</td>
<td style="text-align: center;">13.33</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT_L (ours)</td>
<td style="text-align: center;">.6824</td>
<td style="text-align: center;">.6598</td>
<td style="text-align: center;">-1.73</td>
<td style="text-align: center;">23.45</td>
<td style="text-align: center;">16.54</td>
</tr>
<tr>
<td style="text-align: center;">w/o refinement</td>
<td style="text-align: center;">.5180</td>
<td style="text-align: center;">.5942</td>
<td style="text-align: center;">-1.86</td>
<td style="text-align: center;">19.60</td>
<td style="text-align: center;">15.25</td>
</tr>
<tr>
<td style="text-align: center;">w/o aspect</td>
<td style="text-align: center;">.5520</td>
<td style="text-align: center;">.6373</td>
<td style="text-align: center;">-1.87</td>
<td style="text-align: center;">19.34</td>
<td style="text-align: center;">15.46</td>
</tr>
<tr>
<td style="text-align: center;">w/o num</td>
<td style="text-align: center;">.6708</td>
<td style="text-align: center;">.5989</td>
<td style="text-align: center;">-1.79</td>
<td style="text-align: center;">21.25</td>
<td style="text-align: center;">15.97</td>
</tr>
</tbody>
</table>
<p>Table 4: Automatic evaluation results for Ablation Study on PubMedQA.</p>
<p>Question-Entailment Answering Loop After the above two loops, we evaluate the generated answer's entailment via sentence-BERT embedding similariy (Reimers and Gurevych, 2019) ${ }^{5}$ to ensure the entailment and answerability. If the generated answer does not meet the satisfactory entailment level, the process returns to the initial stage of the framework, and the entire cycle is repeated, iterating through the aforementioned stages.</p>
<h3>4.2 Experiments</h3>
<h3>4.2.1 Evaluation</h3>
<p>In addition to the automatic metrics described in $\S 3.3$, we conduct human evaluations using Amazon Mechanical Turk ${ }^{6}$ to further assess the quality of generated answers. The human evaluation for question-consistency and tangentiality is conducted at the sample level, where we ask annotators to categorize each answer as Query-Inconsistent, Tangential, or Entailed. "Query-Inconsistent" means that the answer provides information unrelated to the query or is non-sensical and meaningless. "Tangential" means that the answer provides information related to the question but doesn't directly address the question. "Entailed" means that the answer directly addresses the question. The human evaluation for fact-consistency is conducted at the sentence level, where we ask annotators to categorize each sentence in the answer as FactInconsistent, Fact-Consistent, or Generic. "FactInconsistent" means that the answer sentence con-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tradicts or cannot be verified by the reference contexts or websites. "Fact-Consistent" means that the answer sentence is supported by the given contexts or websites. "Generic" means that the sentence in the answer has no statement to judge. Please see Appendix D for details.</p>
<h3>4.2.2 Results</h3>
<p>Automatic Evaluation Table 2 presents the automatic evaluation results for our self-reflection loop approach and the baselines that directly generate answers. We observe the superior performance of our method compared to the baselines, as evidenced by both classic overlap metrics and hallucination metrics across all five datasets.</p>
<p>Our method demonstrates a remarkable increase in MedNLI. For example, Alpaca-Lora-7B with self-reflection loop gains around three times larger than baseline for Sample- and Sentence-level MedNLI on PubMedQA. The improvement in F1 and Rouge-L scores, which are overlap-based metrics, is relatively modest sometimes. This is primarily due to the inherent reliance of these metrics on the accuracy of the golden answers. As such, even though the generated answers exhibit high quality, they may differ from the golden answers, thus impacting the performance of these metrics.</p>
<p>Notably, our method showcases its effectiveness across language models with varying parameters, including 7B and 175B, across all five datasets. This robust performance highlights the generalizability and scalability of our approach, further validating its efficacy in the context of medical question-answering tasks.
Human Evaluation The results in Table 3 demonstrate that our method successfully reduces the percentage of query inconsistency, tangentiality, and fact inconsistency in both Vicuna and ChatGPT, which aligns with the findings from the automatic evaluation. The inter-annotator agreement, measured using Krippendorff's alpha (Krippendorff, 2011), indicates high agreement among the annotators, with values exceeding 0.8 for question inconsistency and tangentiality, and exceeding 0.7 for fact consistency. Please see Appendix D for detailed results.</p>
<h3>4.3 Discussion</h3>
<h3>4.3.1 Ablation Study</h3>
<p>To assess the individual contributions of specific components in our method, we conduct an ablation analysis. The results in Table 4 demonstrate the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Directly Generated Answer</th>
<th style="text-align: center;">Generated Answer with Loop</th>
<th style="text-align: center;">Golden Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dose end-tidal carbon dioxide measurement correlate with arterial carbon dioxide in extremely low birth weight infants in the first week of life?</td>
<td style="text-align: center;">Measurement of end-tidal carbon dioxide (EtCO2) levels in extremely low birth weight infants in the first week of life may not be a reliable indicator of arterial carbon dioxide levels. This is because the relationship between EtCO2 and arterial CO2 is not well established in preterm infants, and there may be significant variations in the accumulation and elimination of CO 2 during the first week of life.</td>
<td style="text-align: center;">The evidence from the knowledge suggests that there is a positive correlation between end-tidal carbon dioxide (ETCO2) and arterial carbon dioxide (ACO2) measurements in extremely low birth weight infants in the first week of life. This means that the two measurements are likely to be reliable indicators of each other...</td>
<td style="text-align: center;">Yes. There is good correlation and agreement between end-tidal $\mathrm{CO}(2)$ and arterial $\mathrm{CO}(2)$ in ELBW infants in the EtCO(2) range 30-50 mmHg. Endtidal $\mathrm{CO}(2)$ monitoring can be helpful in trending or for screening abnormal $\mathrm{PaCO}(2)$ values in ELBW infants in first week of life.</td>
</tr>
<tr>
<td style="text-align: center;">Characterization of the gender dimorphism after injury and hemorrhagic shock are hormonal differences responsible?</td>
<td style="text-align: center;">Dear Nadine, <br> Thank you for your letter discussing the potential role of hormones in the gender dimorphism observed in response to injury and hemorrhagic shock. While early research in this area is promising, much more work remains to be done to fully understand the mechanisms involved. <br> In your experiments, you observed that PAG-based GABAAergic signaling was impaired in response to injury and hemorrhagic shock in adult male rats, which could contribute to the development of depression- and anxiety-like behavior...</td>
<td style="text-align: center;">The gender dimorphism of injury and hemorrhagic shock is a complex topic. Research has shown that males and females may have different responses to injury and hemorrhagic shock in terms of blood pressure, organ function, and hormone levels...</td>
<td style="text-align: center;">The independent protective effect of female gender on multiple organ failure and nosocomial infection rates remains significant in both premenopausal and postmenopausal women when compared with similarly aged men. This is contrary to previous experimental studies and the known physiologic sex hormone changes that occur after menopause in women. These results suggest that factors other than sex hormones may be responsible for genderbased differences after injury.</td>
</tr>
</tbody>
</table>
<p>Table 5: Example answers generated directly vs. with our self-reflection loop by Vicuna. We underline and bold the parts relevant to the question or aligning with the golden answer; underline and italicize the parts unrelated to the question or conflicting with the golden answer.
performance of different variations of our approach in terms of automatic hallucination metrics.
Effect of Refinement To evaluate the impact of refinement, we omit the scoring and refining stages and only conduct the generation stage. We acquire background knowledge based on the question and then answer based on the knowledge. As in Table 4 , the answers generated by the loop without refinement attain lower MedNLI and CtrlEval, which means the refinement is helpful for fewer hallucinations and higher consistency.
Effect of Aspect Description To evaluate the impact of providing explicit aspect descriptions for improvement, we omit mentioning the specific aspect that requires refinement. Instead, we instruct the model to engage in self-reflection by using a more general instruction: "Please refine the knowledge/response." As in Table 4, the answers generated by the loop without aspect attain lower MedNLI and CtrlEval, which means the aspect description can lead to fewer hallucinations and higher consistency.
Effect of Score Number To examine the influence of exact scores in the evaluation phase, we omit presenting exact values. Instead, we only describe the aspect that requires improvement in the instructions: "The knowledge is not strongly supported by empirical evidence. Please refine the knowledge to improve its factuality." and "The alignment and consistency between response and knowledge are low. Please refine the
response to improve its consistency." As in Table 4, the loop without score number attains lower MedNLI and CtrlEval than the full implementation, indicating that the explicit score numbers contribute to better refinement.</p>
<h3>4.3.2 Case Study</h3>
<p>The examples in Table 5 demonstrate the effectiveness of our method in addressing fact and query inconsistency. In the first line, the directly generated answer inaccurately states that EtCO2 levels may not be a reliable indicator of arterial CO2 levels, while ours accurately indicates the positive correlation between these measures, aligning with the golden answer and fact. In the second line, the directly generated answer is in an email format and mentions irrelevant information: It greets Nadine and mentions PAG-based GABAAergic signaling of adult male rats and depression- and anxiety-like behavior. In contrast, our responses are more relevant and directly address the given question. Please see Appendix E for more examples.</p>
<h2>5 Conclusion and Future Work</h2>
<p>Hallucinations in generation tasks pose significant challenges to AI's accountability and trustworthiness. We investigate this problem thoroughly and systematically in the context of medical GQA in general and domain-specific LLMs. To address this challenge, we propose an iterative self-reflection method by adopting a generate-score-refine strategy on background knowledge and answers. Our</p>
<p>method is empirically proven effective, generalizable, and scalable in reducing hallucinations. In future work, we will investigate underlying causes of hallucination, examine this phenomenon in other generation tasks and extend our method to address challenges associated with these tasks.</p>
<h2>Limitations</h2>
<p>While our methodology shows promise in mitigating hallucination, it does not entirely eliminate the possibility. The model might still generate ungrounded information, especially in complex or ambiguous scenarios. Currently, our method is still in its early stages and not yet ready for direct real-world deployment. It should be viewed as a complementary approach alongside other methods, such as retrieval, with the potential to contribute to more robust application systems in the future.</p>
<p>Our study primarily focuses on English medical queries, limiting the generalizability to other languages, domains, and modalities. Further research is necessary to investigate the potential language-specific challenges, domain adaptation challenges, and multimodality fusion challenges. By addressing these aspects, we can adapt our proposed methodology to different contexts, resulting in a more comprehensive understanding and application of our approach across diverse linguistic, multi-domain, and multimodal settings.</p>
<p>While this paper has addressed certain issues in this domain, numerous challenges remain, such as empowering LLMs with high-level ability.</p>
<h2>Ethics Statement</h2>
<p>We used publicly available or synthetic datasets for our experiments, avoiding any potential harm to individuals or groups. The data used in this study were carefully selected and processed to ensure privacy and confidentiality. No personally identifiable information is used, and all data were anonymized prior to analysis.</p>
<p>In considering the application of our research findings, we acknowledge the potential risks and ethical considerations associated with AI-assisted healthcare. The hallucination issue, in particular, can have significant implications for patient care and clinical decision-making. Our work is guided by the principle of "do no harm", aiming to enhance the reliability and safety of medical QA systems rather than substitute for professional medical judgment.</p>
<h2>Acknowledgement</h2>
<p>The authors would like to thank Samuel Cahyawijaya for the discussion and suggestions.</p>
<p>This work has been supported by the China NSFC Project (No. NSFC21EG14), SAAIR Project (No. Z1286), and HKJCCT21EG01 (RG192).</p>
<h2>References</h2>
<p>Asma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. 2019. Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 370-379.</p>
<p>Emily Alsentzer, John Murphy, William Boag, WeiHung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical bert embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages $72-78$.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.</p>
<p>Asma Ben Abacha, Eugene Agichtein, Yuval Pinter, and Dina Demner-Fushman. 2017. Overview of the medical question answering task at trec 2017 liveqa. In TREC 2017.</p>
<p>Asma Ben Abacha and Dina Demner-Fushman. 2019. A question-entailment approach to question answering. BMC Bioinform., 20(1):511:1-511:23.</p>
<p>Asma Ben Abacha, Chaitanya Shivade, and Dina Demner-Fushman. 2019. Overview of the mediqa 2019 shared task on textual inference, question entailment and question answering. In ACL-BioNLP 2019.</p>
<p>Jonathan Berant, Vivek Srikumar, Pei-Chun Chen, Abby Vander Linden, Brittany Harding, Brad Huang, Peter Clark, and Christopher D Manning. 2014. Modeling biological processes for reading comprehension. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1499-1510.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. 2022. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Bhuwan Dhingra, Danish Danish, and Dheeraj Rajagopal. 2018. Simple and effective semi-supervised question answering. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 582-587.</p>
<p>Shizhe Diao, Rui Pan, Hanze Dong, KaShun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023. Lmflow: An extensible toolkit for finetuning and inference of large foundation models.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR.</p>
<p>Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Lser, Daniel Truhn, and Keno K Bressem. 2023. Medalpaca-an open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. 2022. Few-shot learning with retrieval augmented language models. arXiv preprint arXiv:2208.03299.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 25672577, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221.</p>
<p>Zakaria Kaddari, Youssef Mellah, Jamal Berrich, Toumi Bouchentouf, and Mohammed G. Belkasmi. 2020. Biomedical question answering: A survey of methods and datasets. In 2020 Fourth International Conference On Intelligent Computing in Data Sciences (ICDS), pages 1-8.</p>
<p>Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, and Minlie Huang. 2022. Ctrleval: An unsupervised reference-free metric for evaluating controlled text generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2306-2319.</p>
<p>Klaus Krippendorff. 2011. Computing krippendorff's alpha-reliability.</p>
<p>Hwanhee Lee, Seunghyun Yoon, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Joongbo Shin, and Kyomin Jung. 2021. KPQA: A metric for generative question answering using keyphrase weights. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2105-2115, Online. Association for Computational Linguistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Chenliang Li, Bin Bi, Ming Yan, Wei Wang, and Songfang Huang. 2021a. Addressing semantic drift in generative question answering with auxiliary extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 942-947.</p>
<p>Jing Li, Shangping Zhong, and Kaizhi Chen. 2021b. Mlec-qa: A chinese multi-choice biomedical question answering dataset. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8862-8874.</p>
<p>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.</p>
<p>Lang Liu, Junxiang Ren, Yuejiao Wu, Ruilin Song, Zhen Cheng, and Sibo Wang. 2022. A pre-trained language model for medical question answering based on domain adaption. In Natural Language Processing and Chinese Computing: 11th CCF International Conference, NLPCC 2022, Guilin, China, September 24-25, 2022, Proceedings, Part II, pages 216-227. Springer.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.</p>
<p>Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. 2023. Sources of hallucination by large language models on inference tasks. arXiv preprint arXiv:2305.14552.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837.</p>
<p>Diego Moll, Mara Elena Santiago-Martnez, Abeed Sarker, and Ccile Paris. 2016. A corpus for research in text processing for evidence based medicine. Language Resources and Evaluation, 50:705-727.</p>
<p>Timo Mller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. Covid-qa: A question answering dataset for covid-19. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020.</p>
<p>Roser Morante, Martin Krallinger, Alfonso Valencia, and Walter Daelemans. 2012. Machine reading of biomedical texts about alzheimers disease. In CLEF 2012 Conference and Labs of the Evaluation ForumQuestion Answering For Machine Reading Evaluation (QA4MRE), Rome/Fornet, J.[edit.]; ea, pages $1-14$.</p>
<p>Jessica Morley, Caio CV Machado, Christopher Burr, Josh Cowls, Indra Joshi, Mariarosaria Taddeo, and Luciano Floridi. 2020. The ethics of ai in health care: a mapping review. Social Science \&amp; Medicine, 260:113172.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. 2021. Webgpt: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332.</p>
<p>OpenAI. 2023a. Chatgpt: Optimizing language models for dialogue.</p>
<p>OpenAI. 2023b. Gpt-4 technical report. ArXiv, abs/2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pages 248-260. PMLR.</p>
<p>Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: A large corpus for question answering on electronic medical records. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357-2368.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. 2021. Kilt: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544.</p>
<p>Long N Phan, James T Anibal, Hieu Tran, Shaurya Chanana, Erol Bahadroglu, Alec Peltekian, and Grgoire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature. arXiv preprint arXiv:2106.03598.</p>
<p>Patrik Puchert, Poonam Poonam, Christian van Onzenoodt, and Timo Ropinski. 2023. Llmmapsa visual metaphor for stratified evaluation of large language models. arXiv preprint arXiv:2304.00457.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426.</p>
<p>Max Savery, Asma Ben Abacha, Soumya Gayen, and Dina Demner-Fushman. 2020. Question-driven summarization of answers to consumer health questions. Scientific Data, 7(1):322.</p>
<p>Sarvesh Soni and Kirk Roberts. 2020. Evaluation of dataset selection for pre-training and fine-tuning transformer language models for clinical question answering. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 55325538 .</p>
<p>Dan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. 2022. Read before generate! faithful long form question answering with machine reading. In Findings of the Association for Computational Linguistics: ACL 2022, pages 744756.</p>
<p>Dan Su, Mostofa Patwary, Shrimai Prabhumoye, Peng Xu, Ryan Prenger, Mohammad Shoeybi, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. 2023. Context generation improves open domain question answering. In Findings of the Association for Computational Linguistics: EACL 2023, pages 793-808, Dubrovnik, Croatia. Association for Computational Linguistics.</p>
<p>Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. 2023. Recitation-augmented language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Simon uster and Walter Daelemans. 2018. Clicr: A dataset of clinical case reports for machine reading comprehension. In Proceedings of NAACL-HLT, pages $1551-1563$.</p>
<p>Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding the capabilities, limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.</p>
<p>Raphael Tang, Rodrigo Nogueira, Edwin Zhang, Nikhil Gupta, Phuong Cam, Kyunghyun Cho, and Jimmy Lin. 2020. Rapidly bootstrapping a question answering dataset for covid-19. arXiv preprint arXiv:2004.11339.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. 2015. An overview of the bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics, 16(1):1-28.</p>
<p>Cunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can generative pre-trained language models serve as knowledge bases for closed-book qa? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3241-3251.</p>
<p>Eric J. Wang. 2023. Alpaca-lora.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. Transactions of the Association for Computational Linguistics, 6:287302.</p>
<p>Yan Xu, Etsuko Ishii, Samuel Cahyawijaya, Zihan Liu, Genta Indra Winata, Andrea Madotto, Dan Su, and Pascale Fung. 2022. Retrieval-free knowledgegrounded dialogue response generation with adapters. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 93-107.</p>
<p>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don't know? In Findings of the Association for Computational Linguistics: ACL 2023, pages 8653-8665, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023. Generate rather than retrieve: Large language models are strong context generators. In The Eleventh International Conference on Learning Representations.</p>
<p>Hao Zhou, Minlie Huang, Yong Liu, Wei Chen, and Xiaoyan Zhu. 2021. Earl: Informative knowledge-grounded conversation generation with entity-agnostic representation learning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2383-2395.</p>
<p>Ming Zhu, Aman Ahuja, Da-Cheng Juan, Wei Wei, and Chandan K. Reddy. 2020. Question answering with long multiple-span answers. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3840-3849, Online. Association for Computational Linguistics.</p>
<h2>A Dataset</h2>
<p>PubMedQA (Jin et al., 2019) is a biomedical QA dataset consisting of 1 k expert labeled, 61.2 k unlabeled, and 211.3 k artificially generated instances. It includes questions derived from research article titles, abstracts as the context, long answers from abstract conclusions, and concise yes/no/maybe answers.
MedQuAD (Ben Abacha and Demner-Fushman, 2019) comprises 47,457 medical QA pairs derived from 12 National Institutes of Health (NIH) websites. It spans 37 question categories and covers various medical topics including diseases, medications, and diagnostic tests.
MEDIQA2019 (Ben Abacha et al., 2019) challenge includes three tasks: Natural Language Inference (NLI), Recognizing Question Entailment (RQE), and QA in the medical domain. For this particular study, we focus on Task 3's dataset, which centers around medical question answering and consider the golden answers with scores 3 and 4 as the correct responses.
LiveMedQA2017 (Ben Abacha et al., 2017) contains annotated medical QA pairs, aiding the development of question analysis and answering systems. The test questions span 26 types within five main categories, each question comprising one or more sub-questions, a focus, and a type. All reference answers, curated from reliable sources and vetted by medical experts, include a URL and relevant comments. A minimum of one reference answer is provided for each test question.
MASH-QA (Zhu et al., 2020) is a dataset from the consumer health domain designed for Multiple Answer Spans Healthcare QA. It includes 34k QA pairs where answers may be drawn from nonsequential sections of a lengthy document.</p>
<p>In these datasets, except PubMedQA, answer annotation is undertaken through a manually involved process of extracting and copying from authentic web content. It is imperative to note that although the annotated answers are indeed pertinent and verifiable, it has not undergone refinement, thereby improvements are needed in harmonizing the contextual coherence and problem linkage. For an example from LiveMedQA2017, given question: "Do Zolmitriptan 5mg tablets contain gluten?", the answer is "Zolmitriptan tablets are available as 2.5 mg (yellow and functionally-scored) and 5 mg (pink, not scored) film coated tablets for oral administration..."</p>
<h2>B Implementation Details</h2>
<p>Vicuna is an open-source foundation, equipped with an enriched dataset and a scalable, userfriendly infrastructure. It is trained by fine-tuning LLaMA (Touvron et al., 2023) on user-shared conversations derived from ShareGPT. Vicuna generates more detailed and well-structured responses compared to its predecessor, Alpaca, with a quality equivalent to ChatGPT. We use the code and checkpoint from the official library ${ }^{7}$. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3 . The factuality, consistency, and entailment threshold are $-1.0,-5.0$, and 0.8 , respectively. The demo number for the factuality scorer is 1 for PubMedQA; 3 for MedQuAD, MEDIQA2019, MASH-QA, and LiveMedQA2017.</p>
<p>Alpaca-LoRA replicates the results of Stanford's Alpaca model, employing a technique known as Low-Rank Adaptation (LoRA). This instruct model, comparable in quality to GPT-3.5, is capable of operating under low resource conditions. Remarkably, even without hyperparameter tuning, Alpaca-LoRA generates outputs on par with the original Stanford Alpaca model. We use the code and checkpoint from the official library ${ }^{8}$. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3 . The factuality, consistency, and entailment threshold are $-1.0,-5.0$, and 0.8 , respectively. The demo number for the factuality scorer is 1 for PubMedQA, MASH-QA, MEDIQA2019, and MedQuAD; 2 for LiveMedQA2017.</p>
<p>ChatGPT is designed to interpret prompts and furnish comprehensive responses. It employs Reinforcement Learning from Human Feedback (RLHF), similar to InstructGPT (Ouyang et al., 2022), albeit with minor differences in data collection methodology. The initial model is trained through supervised fine-tuning, where human AI trainers conducted dialogues, simulating both the user and AI assistant roles. We use the official API from ${ }^{9}$ to generate answers. The max number of</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the main loop and response loop is 3 . The max number of the knowledge loop is 1 for MedQuAD and 3 for others. The factuality, consistency, and entailment threshold are $-1.0,-5.0$, and 0.8 , respectively. The demo number for the factuality scorer is 1 for PubMedQA, MASH-QA, MEDIQA2019, and MedQuAD; 3 for LiveMedQA2017.</p>
<p>MedAlpaca is a large language model meticulously fine-tuned for medical dialogue and QA applications upon the frameworks of Alpaca and Alpaca-LoRA. These models have been trained using an array of medical texts, including medical flashcards, wikis, and dialogue datasets. We use the code and checkpoint from the official library ${ }^{10}$. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default. The max number of the main loop, knowledge loop, and response loop is 3 . The factuality, consistency, and entailment threshold are $-1.0,-5.0$, and 0.8 , respectively. The demo number for the factuality scorer is 1 for PubMedQA, MEDIQA2019, and LiveMedQA2017; 2 for MASH-QA, and MedQuAD.</p>
<p>Robin-medical is a large language model finetuned in the medical domain via LMFlow, an extensible toolkit for finetuning and inference of large foundation models. We use the code and checkpoint from the official library ${ }^{11}$. We run generation on two GeForce RTX 2080 GPUs with the following settings: temperature 1.0, max new tokens 512, others are default.</p>
<h2>C Factuality Scorer</h2>
<p>Here are demonstrations for our factuality scorer: Question: What are the risk factors for heart disease? \n Knowledge: Risk factors for heart disease can be categorized into modifiable and non-modifiable. Modifiable risk factors include high blood pressure, high cholesterol, smoking, unhealthy diet, physical inactivity, obesity, and excessive alcohol use. Non-modifiable risk factors include age, gender, family history, and race or ethnicity. \n Question: How does smoking affect lung health? \n Knowledge: Smoking damages the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>airways and small air sacs in your lungs, which can lead to a variety of lung diseases including chronic bronchitis, emphysema, and lung cancer. It also decreases your lung capacity and makes it harder for your lungs to defend against infections and clear out mucus. \n
Question: Is it safe to take aspirin every day? \n Knowledge: For some people, taking aspirin every day can help prevent heart attacks or strokes. However, daily aspirin isn't appropriate for everyone. It can cause side effects like gastrointestinal bleeding and isn't recommended for people with certain health conditions or who take certain medications. Always consult with a healthcare professional before starting any new medication regimen. \n</p>
<h2>D Human Evaluation</h2>
<p>We conduct the human evaluation to assess our method's performance in GQA for the ability to reduce hallucination. In detail, we randomly select 50 question-answer pairs directly generated by Vicuna and 50 question-answer pairs generated with our proposed loop. Each sample is evaluated by three different annotators to rule out potential bias. We specify that annotators must meet the following qualifications: Their Human Intelligence Task (HIT) approval rates are greater than or equal to $95 \%$, and the numbers of HITs approved are greater than or equal to 5000. The annotators are located in Australia, Canada, the United Kingdom, and the United States. The evaluation cost for fact inconsistency is 0.3 US dollars per sentence, and for question inconsistency and tangentiality, it is 0.15 US dollars per answer. Figure 5 and 6 are the user interfaces (UIs) on Amazon Mechanical Turk for human evaluation of fact-consistency and question-consistency and tangentiality, respectively. The instructions, questions, and examples for annotators are shown.</p>
<p>To identify the Fact Inconsistency in generated answers, we ask annotators to refer the context provided in dataset. As a supplement, we retrieve the related paragraph from WikiMedicalTerms Dataset ${ }^{12}$ via sentence-BERT embedding</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The UI for human evaluation on fact-consistency.</p>
<p>Figure 6: The UI for human evaluation on question-consistency and tangentiality.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Query-Inconsistent $\downarrow$</th>
<th style="text-align: center;">Tangentiality $\downarrow$</th>
<th style="text-align: center;">Entailment $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">$0.67 \%$</td>
<td style="text-align: center;">$6.04 \%$</td>
<td style="text-align: center;">$93.29 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B_L</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{2 . 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 0 0 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$18.00 \%$</td>
<td style="text-align: center;">$82.00 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT_L</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 \%}$</td>
<td style="text-align: center;">$\mathbf{1 7 . 3 3 \%}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 6 7 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Fact-Inconsistent $\downarrow$</td>
<td style="text-align: center;">Fact-Consistent</td>
<td style="text-align: center;">Generic</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">$8.69 \%$</td>
<td style="text-align: center;">$78.07 \%$</td>
<td style="text-align: center;">$13.24 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B_L</td>
<td style="text-align: center;">$\mathbf{7 . 3 8 \%}$</td>
<td style="text-align: center;">$\mathbf{7 6 . 9 6 \%}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 6 6 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT</td>
<td style="text-align: center;">$8.06 \%$</td>
<td style="text-align: center;">$65.62 \%$</td>
<td style="text-align: center;">$25.32 \%$</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT_L</td>
<td style="text-align: center;">$\mathbf{6 . 3 3 \%}$</td>
<td style="text-align: center;">$\mathbf{7 7 . 8 \%}$</td>
<td style="text-align: center;">$\mathbf{1 5 . 8 7 \%}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Human evaluation results on PubMedQA.
similariy ${ }^{13}$. This dataset contains over 6,000 medical terms and their Wikipedia text. In addition, we ask annotators to search for references in Wiki$\mathrm{Doc}^{14}$, a medical wiki encyclopedia used by the international community of healthcare professionals.</p>
<p>The results in Table 6 demonstrate that our method successfully reduces the percentage of query inconsistency, tangentially, and fact inconsistency in both Vicuna and ChatGPT. Furthermore, we observe that ChatGPT tends to generate more general sentences compared to Vicuna. Our approach can keep generic responses at a relatively low level while improving fact consistency.</p>
<h1>E Case Study</h1>
<p>Table 7 presents some example answers generated by ChatGPT, ChatGPT with our self-reflection loop, and GPT4. While ChatGPT provides a negative answer, both ChatGPT with our self-reflection loop and GPT4 offer positive answers that align better with the golden answer. ChatGPT suggests that there is no evidence in its available literature that shape analysis can differentiate free-floating internal carotid artery thrombus from atherosclerotic plaque in CTA. In contrast, the golden answer states shape signature highlights the potential to distinguish the two. ChatGPT with our self-reflection loop acknowledges the role of shape analysis in differentiation and mentions works on machine learning algorithms and radiomic features that show promising results in classification. GPT4 also suggests shape analysis can potentially help differentiate, but its subsequent content is unrelated to shape analysis itself. These results demonstrate the effectiveness of our method and the capabilities of more powerful LLMs.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Can shape analysis differentiate free-floating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke or transient ischemic attack?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Golden Answer</td>
<td style="text-align: center;">Yes. We identified five quantitative shape descriptors of carotid FFT. This shape "signature" shows potential for supplementing conventional lesion characterization in cases of suspected FFT.</td>
</tr>
<tr>
<td style="text-align: center;">Answer Generated by ChatGPT</td>
<td style="text-align: center;">As an AI language model, I don't have access to the most recent studies database, but based on the literature available to me so far, I couldn't find any evidence that shape analysis can differentiate freefloating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke or transient ischemic attack. However, other imaging modalities such as MRI and ultrasound may provide more information to differentiate between the two conditions. It is essential to consult with a radiologist or a vascular specialist to make a definitive diagnosis and develop the appropriate treatment plan.</td>
</tr>
<tr>
<td style="text-align: center;">Answer Generated by ChatGPT_L</td>
<td style="text-align: center;">According to recent studies, shape analysis can play a significant role in differentiating free-floating internal carotid artery thrombus from atherosclerotic plaque in patients evaluated with CTA for stroke or transient ischemic attack. Various machine learning algorithms and radiomic features have been used to analyze the shape of the thrombi and plaques in the internal carotid artery, leading to promising results in terms of accurate classification. This differentiation is crucial as the treatment approaches for thrombus and plaque are different.</td>
</tr>
<tr>
<td style="text-align: center;">Answer Generated by GPT4</td>
<td style="text-align: center;">Shape analysis can potentially help differentiate a free-floating internal carotid artery thrombus from an atherosclerotic plaque in patients evaluated with computed tomography angiography (CTA) for stroke or transient ischemic attack. However, it is essential to note that shape analysis alone might not be sufficient for a definitive diagnosis, and other imaging features and clinical information should be considered. <br> A free-floating thrombus is an intraluminal filling defect that is not attached to the arterial wall, while an atherosclerotic plaque is a focal thickening of the arterial wall due to the presence of lipid-rich and fibrous tissue. Here are some shape analysis and imaging features that may help differentiate between them: <br> 1. Shape and appearance: A free-floating thrombus usually appears as a well-defined, elongated, and tubular filling defect within the arterial lumen. In contrast, atherosclerotic plaques tend to be more irregularly shaped and may have calcifications. <br> 2. Location: Thrombi may occur at any location within the internal carotid artery, while atherosclerotic plaques are more commonly found at the carotid bifurcation, the proximal internal carotid artery, and the origin of the external carotid artery. <br> 3. Attachment: A free-floating thrombus is not attached to the arterial wall and may show a "bird's beak" or "meniscus" sign at its proximal end, suggesting that it is freely floating within the lumen. Atherosclerotic plaques are typically attached to the arterial wall and may cause luminal narrowing or stenosis. <br> 4. Enhancement: In some cases, a free-floating thrombus may show minimal or no enhancement after the administration of intravenous contrast, while atherosclerotic plaques typically enhance to varying degrees. <br> It is important to recognize that these imaging features are not always definitive, and there may be overlap between the appearance of a thrombus and an atherosclerotic plaque. A comprehensive evaluation that includes other imaging techniques (such as magnetic resonance imaging or ultrasound) and clinical information (such as patient history, symptoms, and risk factors) may be necessary for an accurate diagnosis. Additionally, further research and development of advanced imaging techniques and computational methods could improve the differentiation between these two entities in the future.</td>
</tr>
</tbody>
</table>
<p>Table 7: Example answers generated by ChatGPT, ChatGPT with our self-reflection loop, and GPT4. We underline and bold the concluding parts aligning with the golden answer and underline and italicize the concluding parts conflicting with the golden answer.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{13}$ https://huggingface.co/
sentence-transformers/all-MiniLM-L6-v2
${ }^{14}$ https://www.wikidoc.org/index.php/Main_Page&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{12}$ https://huggingface.co/datasets/gamino/wiki_ medical_terms&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9312 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9312</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9312</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-266210460</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.08901v3.pdf" target="_blank">Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning. In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning. Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples. The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window. A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach. As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements. Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K. CoT-Influx serves as a plug-and-play module for LLMs and is compatible with most existing reasoning prompting techniques, such as self-consistency and self-verification.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9312.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9312.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-Influx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought Influx (CoT-Influx)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A coarse-to-fine reinforcement-learned pruning module that (i) selects helpful Chain-of-Thought (CoT) few-shot examples and (ii) prunes unimportant tokens so more concise, informative CoTs can be concatenated within the model's context window, enabling many more shots without model fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (evaluated: 7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (primary), AddSub, MultiArith, Svamp, SingleEq</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problem benchmarks (GSM8K) and other math datasets testing multi-step arithmetic and algebraic reasoning; evaluation by exact-match answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Many-shot Chain-of-Thought few-shot prompting where CoT-Influx retrieves a large pool of CoT examples, applies a shot pruner to select examples, then a token pruner to remove redundant tokens; final input is concatenation of pruned CoTs followed by the question. Trained with RL reward combining LLM token loss, answer correctness, and token-length penalty. Typical CoT-Influx runs tested with 16–64 CoTs (e.g., 48 shots for 13B).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against standard few-shot-CoT (manual 8-shot), TopK retrieval (20 shots max within context), random retrieval, BM25 retrieval, prompt-compression methods (GPT-4 compression, Selective Context, LLMLingua), and context-window-extension via positional interpolation (32K).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K EM: LLaMA2-7B 15.92%, LLaMA2-13B 32.37%, LLaMA2-70B 59.59%. Across 5 datasets average: LLaMA2-7B avg 55.38% (other datasets included), LLaMA2-13B avg 69.26%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Against few-shot-CoT (8-shot GSM8K): LLaMA2-7B 13.79% → CoT-Influx 15.92 (+2.13); LLaMA2-13B 27.82% → 32.37% (+4.55); LLaMA2-70B 55.42% → 59.59% (+4.17). Against TopK retrieval (20-shot GSM8K): LLaMA2-13B 23.65% → 32.37% (+8.72). Against prompt-compression baselines: e.g., TopK+LLMLingua (40 shots) LLaMA2-13B 8.34% vs CoT-Influx 32.37%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+2.13% to +4.55% absolute EM improvement over manual 8-shot CoT on GSM8K (varies by model), and up to +8.72% vs TopK retrieval in reported experiments (LLaMA2-13B).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that many CoT examples (if concise and relevant) improve in-context learning because they provide more diverse step-by-step demonstrations; however, context windows have redundant tokens at both example and token levels which can be pruned to free space. CoT-Influx trains a difficulty-aware, model-tailored pruner because different LLM sizes prefer different difficulty levels of CoTs. The RL reward balances correctness, token-loss, and token budget to preserve structural/format tokens critical for math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Pruner uses frozen BERT-Large embeddings and a two-stage MLP policy trained with REINFORCE; token budget constrained by LLaMA2 context window (4096 tokens); CoT-Influx experiments used up to 48 CoTs (13B) and up to 40–48 shots for other models; MRD3 dataset (∼9.7K seed, GPT-4-evolved) used as CoT candidate pool; reward terms: R_acc (1/-0.1/0), LLM loss term 1/(1+LLM loss), and token-limit term (t/T)^w; training stabilizers: append manual 8-shot CoTs during early training and repeat questions (t_repeat). Additional ablations evaluated shot-only and token-only pruning and reward term ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9312.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot-CoT (8-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Few-shot Chain-of-Thought Prompting (8-shot manual CoTs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The common evaluation format where 8 human-engineered chain-of-thought (step-by-step) exemplars are prepended to the question to induce multi-step reasoning in LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B) and larger LLM comparators in discussion</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Grade-school math word problems; metric: exact-match answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>8 manually designed CoT exemplars concatenated before the target question (standard in CoT literature).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against zero-shot, zero-shot-CoT, retrieval-selected multi-shot CoTs, CoT-Influx many-shot compressed CoTs, and prompt-compression variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K EM: LLaMA2-7B 13.79%, LLaMA2-13B 27.82%, LLaMA2-70B 55.42% (reported baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-Influx improved these to 15.92% (7B), 32.37% (13B), and 59.59% (70B).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Baseline; CoT-Influx reported +2.13% to +4.55% absolute EM gains over this format on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Manual 8-shot CoTs are curated for quality; simply increasing shot count without targeted pruning or selection can be harmful due to redundancy and context-window constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>8-shot prompts follow Wei et al. (2022) standard; average token counts and context budget cause limits on how many examples can be inserted (conventionally 8).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9312.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TopK retrieval (semantic)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TopK Semantic Retrieval of CoT Examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Retrieval baseline that selects the top-k semantically similar CoT exemplars from the pool (by embedding similarity) and concatenates them as few-shot context up to context-window limits.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selecting most similar CoT exemplars for in-context learning and measuring resultant EM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Top-k most semantically similar CoTs (k up to 20 given 4096-token window) concatenated before question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to manual 8-shot CoT, random retrieval, BM25, CoT-Influx, and prompt-compression variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K EM (20-shot TopK reported): LLaMA2-7B ~14.56%, LLaMA2-13B ~23.65%, LLaMA2-70B ~54.59% (table entries).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-Influx (48-shot compressed) improved LLaMA2-13B to 32.37% (TopK 23.65% → +8.72).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>TopK retrieval can outperform naive few-shot in some cases but was substantially outperformed by CoT-Influx when CoT-Influx could fit more informative shots in the same token budget (+~8.7% on 13B in GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>TopK retrieves model-agnostic examples based on similarity; it does not consider token redundancy or model-specific example difficulty preferences, so retrieved examples may include redundant or harmful tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>TopK retrieval limited by LLaMA2 context window to 20 CoTs in reported comparisons; average token counts for retrieved sets reported in Table 2; retrieval performed from MRD3 candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9312.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random / BM25 retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random selection and BM25-based retrieval of CoT examples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that either randomly sample CoT exemplars or select exemplars via classical BM25 lexical retrieval, used to estimate average or lexical-retrieval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate effect of non-optimized example selection strategies for few-shot CoT prompting on EM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>20-shot (or as many as fit) few-shot CoTs selected either randomly or by BM25 lexical score, concatenated before the question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to TopK (semantic), manual 8-shot, prompt compression methods, and CoT-Influx.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Random retrieval GSM8K EM: LLaMA2-7B 12.51%, LLaMA2-13B 22.21%, LLaMA2-70B 53.07%. BM25 retrieval GSM8K EM: LLaMA2-7B ~13.42%, LLaMA2-13B ~25.17%, LLaMA2-70B ~54.21%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-Influx outperforms both random and BM25 retrievals by several absolute percentage points (varies by model), e.g., vs BM25 on 13B: ~25.17% → 32.37% (+7.20).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Random selection can underperform manual 8-shot (Table 1 shows random 16-shot might be worse than manual 8-shot); BM25 typically similar or slightly better than random but still inferior to CoT-Influx.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Random selection reflects average performance of the CoT pool; BM25 focuses on lexical overlap but misses format/difficulty preferences and token redundancy which are important for math CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Random and BM25 baselines selected as many CoTs as possible without exceeding context-window; empirical results reported in Table 2 and Table 1 (pilot).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9312.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Compression Baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Compression Methods (LLMLingua, Selective Context, GPT-4 Compression)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that compress prompts/tokens to fit more examples into context: LLMLingua and Selective Context (automated token pruning/abstraction) and GPT-4-based compression that asks GPT-4 to summarize/compress exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K and other math datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether compressing exemplar texts preserves the essential structure for math CoT prompting and its effect on EM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Apply an automated compression method to each retrieved CoT exemplar (or whole prompt) to reduce token count, then concatenate compressed exemplars before the question.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to uncompressed TopK retrieval and CoT-Influx pruned tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Examples on GSM8K (TopK+compression variants, 40-shot reported): TopK+GPT-4 Compression LLaMA2-13B ~11.01% EM; TopK+Selective Context LLaMA2-13B ~0.76% EM; TopK+LLMLingua LLaMA2-13B ~8.34% EM. (Selective Context and LLMLingua also showed very low figures for smaller models.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-Influx (32.37% on 13B) greatly outperforms all tested compression baselines (e.g., +~24 percentage points over LLMLingua on 13B in GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Prompt compression methods often caused large accuracy drops (e.g., LLMLingua and Selective Context reduce EM substantially), indicating naive compression tends to remove crucial numerical/format tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Generic prompt compression that prunes tokens by designed metrics can remove numerals or format tokens crucial for math reasoning; GPT-4-based compression tends to remove reasoning steps, hurting CoT effectiveness. CoT-Influx's RL-trained token pruner preserves structural tokens and avoids harmful deletions.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Compression methods were applied to TopK retrieved exemplars, with experiments reporting average tokens and final EM; authors show qualitative examples of important tokens removed by compressors (Figure 2 and Appendix A.5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9312.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Context Window Extension (PI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context Window Extension via Positional Interpolation (32K)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that extends a pre-trained LLM's context window (here LLaMA2-7B) to 32K tokens using positional interpolation, allowing many more exemplars without compression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B (extended to 32K)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Test whether extending context window lets the model benefit from more CoT exemplars without pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same prompt content as narrow-window experiments but allowed to exceed 4k tokens up to 32k via positional interpolation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to LLaMA2-7B with CoT-Influx pruning of prompts to fit original window.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported results show LLaMA2-7B-32K EM at several shot counts (e.g., 12-shot 11.37%, 16-shot 12.81%, vs unextended LLaMA2-7B 13.87% at similar shots).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report that context-window extension weakened reasoning ability on the same input prompt relative to original LLaMA2 (CoT-Influx enabled better unlocking of the context window).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Context extension did not improve and in reported cases reduced EM compared to narrower-window baseline on identical prompts (qualitative/quantitative decline observed).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Extending context without targeted pruning may dilute attention or change model behavior; the authors observe that their finding (more helpful CoTs helps) does not hold for LLMs with extended windows via PI, and that PI can weaken reasoning with the same prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Comparison uses positional interpolation (PI) to extend to 32K tokens; when input prompts did not exceed 4K, direct comparisons were made; when they exceeded, CoT-Influx-pruned prompts were also used for fair comparison (Appendix A.1 and Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9312.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot and Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot and Zero-shot Chain-of-Thought Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline formats where the model is given the task instruction only (zero-shot) or a zero-shot CoT trigger (e.g., 'Let's think step by step') to induce reasoning without few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess baseline reasoning capability with no exemplars or with a zero-shot CoT trigger.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot: question only. Zero-shot-CoT: question with an instruction or trigger encouraging chain-of-thought style response.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to few-shot-CoT and CoT-Influx many-shot CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K EM: Zero-shot (approx): LLaMA2-7B ~4.25% EM; Zero-shot-CoT increased but still low (LLaMA2-13B ~12.28% per Table 2 noisy formatting).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Zero-shot and zero-shot-CoT are substantially lower than few-shot-CoT and far below CoT-Influx many-shot compressed exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Zero-shot-CoT can help somewhat versus pure zero-shot, but few-shot CoTs (especially curated or many-shot compressed forms) substantially improve EM.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing worked examples supplies meta-training signals for in-context learning; zero-shot triggers are helpful but cannot substitute for diverse exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Standard zero-shot and zero-shot-CoT prompts as used in literature (Kojima et al., 2022). Exact numbers in paper tables are partially garbled but show low baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9312.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency / Self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (majority vote) and Self-verification applied to CoT outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding-time techniques that sample multiple chain-of-thought outputs and either take majority-voted final answers (self-consistency) or apply a verifier to filter/score outputs (self-verification), used on top of CoT prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-13B and LLaMA2-70B (applied on outputs from CoT-Influx)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate how decoding aggregation/verification methods further improve EM after using CoT-Influx pruned prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>CoT-Influx pruned prompts; sample multiple chain-of-thought decodings (maj@20 or verify@20) and aggregate/verify answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>CoT-Influx single-pass vs CoT-Influx+maj@20 vs CoT-Influx+verify@20.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LLaMA2-13B: CoT-Influx 32.37% → +maj@20 33.43% → +verify@20 34.04%. LLaMA2-70B: 59.59% → 60.73% (maj@20) → 61.79% (verify@20).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Both self-consistency and self-verification gave modest additional gains (≈+1.0–2.2 percentage points) on top of CoT-Influx.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1.06% (maj) and +1.67% (verify) on 13B; +1.14% and +2.20% on 70B respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Aggregating multiple sampled CoT chains reduces decoder sampling variance and helps recover correct final answers; verification helps filter reasoning chains that stray due to pruned context.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Sampling n=20 outputs for majority or verification aggregation; reported numbers in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9312.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation: Shot-only vs Token-only pruning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation of CoT-Influx pruner stages (retain only shot-level or token-level pruning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tests whether pruning entire CoT examples (shot pruner) or pruning tokens within retained shots (token pruner) is more important to performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B, 70B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare full coarse-to-fine pruning vs only shot-level pruning vs only token-level pruning using EM accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Three formats: (a) full CoT-Influx (shot+token), (b) prune shot only and keep tokens intact, (c) prune tokens only but keep same set of shots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>GSM8K EM: Full CoT-Influx — 7B 15.92%, 13B 32.37%, 70B 59.59%. Shot-only: 7B 15.69% (-0.23), 13B 31.08% (-1.29), 70B 57.77% (-1.82). Token-only: 7B 12.05% (-3.87), 13B 25.32% (-7.05), 70B 49.36% (-10.23).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Removing token-level pruning causes a much larger drop than removing shot-level pruning, indicating shot pruning is easier but token pruning crucial for preserving important information compactly.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Token-only pruning vs full: drops of ~3.9–10.2 absolute percentage points across model sizes; shot-only pruning incurs smaller drops (~0.2–1.8 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Shot-pruner achieves coarse redundancy reduction, but token pruner is essential to preserve crucial numerical/format tokens while trimming function words; token pruning is more challenging and critical for math CoTs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Ablations reported in Table 7; pruning ratios: total ~4.28× reduction in tokens, with shot pruner contributing ~3.87× (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9312.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-selection harms</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative effect of randomly adding more CoT exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation that simply adding more (random) CoT exemplars can degrade reasoning performance relative to a curated small set; example selection matters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2 (7B, 13B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GSM8K (pilot study / Table 1)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess impact of exemplar selection quality when increasing number of CoT shots.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Randomly selected 16-shot prompts vs curated manual 8-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random 16-shot vs manual 8-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 1 pilot: LLaMA2-7B manual 8-shot 13.79% vs random 16-shot 12.36% (random1) and 13.27% (random2). LLaMA2-13B manual 8-shot 27.82% vs random 16-shot 23.04% and 23.28%.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Randomly increasing shots can decrease EM by several absolute percentage points (e.g., ~4.78 pp drop on 13B in the reported random samples).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Not all CoT examples are beneficial; some are useless or harmful. Example selection must be model-aware and quality-aware rather than random or purely similarity-based.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Pilot study using MRD3 candidate pool; random retrieval selected 16 shots in these experiments (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9312.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9312.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MRD3 dataset</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Math Reasoning Dataset with Diverse Difficulty (MRD3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A curated and GPT-4-evolved math CoT dataset collected by merging GSM8K/MAWPS/AQuA seeds and using GPT-4 to generate diverse CoT exemplars with difficulty scores and evolved variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used for training/evaluating pruner and retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoT candidate pool for GSM8K and other math datasets</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides a large, diverse pool of CoT exemplars (varying difficulty and reasoning steps) used to train the CoT-Influx pruner and to retrieve few-shot exemplars for testing.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Each example: question, formatted reasoning steps (CoT), answer; each assigned a difficulty 1–10 via GPT-4 evaluation; dataset augmented via five mutation/evolution schemes (add_constraints, deepening, increase_reasoning, revise_difficulty, produce_easier).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Ablation: MRD3 w/o GPT-4 evolution and GSM8K training set as candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Using MRD3 as candidate pool: CoT-Influx GSM8K EM LLaMA2-13B 32.37%; MRD3 w/o evolution 30.55%; GSM8K training set 29.64% (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>CoT-Influx with MRD3 outperforms using MRD3 without evolution and using only GSM8K training set by ~1.8–2.7 absolute EM on LLaMA2-13B GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>GPT-4-evolved examples in MRD3 improve pruner effectiveness by a few percentage points in final EM.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Diversity in difficulty and reasoning steps helps pruner generalize; different LLM sizes prefer different example difficulty distributions (smaller models prefer easier CoTs).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Initial pool ~9.7K Q/A pairs, GPT-4 generated formatted CoTs and difficulty scores; evolution applied to increase diversity; difficulty filter used when training pruner (only easier questions below threshold d_thr used for RL stability).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>What makes good in-context examples for gpt-3? <em>(Rating: 2)</em></li>
                <li>Llmlingua: Compressing prompts for accelerated inference of large language models <em>(Rating: 2)</em></li>
                <li>Extending context window of large language models via positional interpolation <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Compressing context to enhance inference efficiency of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9312",
    "paper_id": "paper-266210460",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "CoT-Influx",
            "name_full": "Chain-of-Thought Influx (CoT-Influx)",
            "brief_description": "A coarse-to-fine reinforcement-learned pruning module that (i) selects helpful Chain-of-Thought (CoT) few-shot examples and (ii) prunes unimportant tokens so more concise, informative CoTs can be concatenated within the model's context window, enabling many more shots without model fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (evaluated: 7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K (primary), AddSub, MultiArith, Svamp, SingleEq",
            "task_description": "Grade-school math word problem benchmarks (GSM8K) and other math datasets testing multi-step arithmetic and algebraic reasoning; evaluation by exact-match answer accuracy.",
            "presentation_format": "Many-shot Chain-of-Thought few-shot prompting where CoT-Influx retrieves a large pool of CoT examples, applies a shot pruner to select examples, then a token pruner to remove redundant tokens; final input is concatenation of pruned CoTs followed by the question. Trained with RL reward combining LLM token loss, answer correctness, and token-length penalty. Typical CoT-Influx runs tested with 16–64 CoTs (e.g., 48 shots for 13B).",
            "comparison_format": "Compared against standard few-shot-CoT (manual 8-shot), TopK retrieval (20 shots max within context), random retrieval, BM25 retrieval, prompt-compression methods (GPT-4 compression, Selective Context, LLMLingua), and context-window-extension via positional interpolation (32K).",
            "performance": "GSM8K EM: LLaMA2-7B 15.92%, LLaMA2-13B 32.37%, LLaMA2-70B 59.59%. Across 5 datasets average: LLaMA2-7B avg 55.38% (other datasets included), LLaMA2-13B avg 69.26%.",
            "performance_comparison": "Against few-shot-CoT (8-shot GSM8K): LLaMA2-7B 13.79% → CoT-Influx 15.92 (+2.13); LLaMA2-13B 27.82% → 32.37% (+4.55); LLaMA2-70B 55.42% → 59.59% (+4.17). Against TopK retrieval (20-shot GSM8K): LLaMA2-13B 23.65% → 32.37% (+8.72). Against prompt-compression baselines: e.g., TopK+LLMLingua (40 shots) LLaMA2-13B 8.34% vs CoT-Influx 32.37%.",
            "format_effect_size": "+2.13% to +4.55% absolute EM improvement over manual 8-shot CoT on GSM8K (varies by model), and up to +8.72% vs TopK retrieval in reported experiments (LLaMA2-13B).",
            "explanation_or_hypothesis": "Authors hypothesize that many CoT examples (if concise and relevant) improve in-context learning because they provide more diverse step-by-step demonstrations; however, context windows have redundant tokens at both example and token levels which can be pruned to free space. CoT-Influx trains a difficulty-aware, model-tailored pruner because different LLM sizes prefer different difficulty levels of CoTs. The RL reward balances correctness, token-loss, and token budget to preserve structural/format tokens critical for math reasoning.",
            "null_or_negative_result": false,
            "experimental_details": "Pruner uses frozen BERT-Large embeddings and a two-stage MLP policy trained with REINFORCE; token budget constrained by LLaMA2 context window (4096 tokens); CoT-Influx experiments used up to 48 CoTs (13B) and up to 40–48 shots for other models; MRD3 dataset (∼9.7K seed, GPT-4-evolved) used as CoT candidate pool; reward terms: R_acc (1/-0.1/0), LLM loss term 1/(1+LLM loss), and token-limit term (t/T)^w; training stabilizers: append manual 8-shot CoTs during early training and repeat questions (t_repeat). Additional ablations evaluated shot-only and token-only pruning and reward term ablations.",
            "uuid": "e9312.0",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Few-shot-CoT (8-shot)",
            "name_full": "Standard Few-shot Chain-of-Thought Prompting (8-shot manual CoTs)",
            "brief_description": "The common evaluation format where 8 human-engineered chain-of-thought (step-by-step) exemplars are prepended to the question to induce multi-step reasoning in LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B) and larger LLM comparators in discussion",
            "model_size": null,
            "task_name": "GSM8K (primary)",
            "task_description": "Grade-school math word problems; metric: exact-match answer accuracy.",
            "presentation_format": "8 manually designed CoT exemplars concatenated before the target question (standard in CoT literature).",
            "comparison_format": "Compared against zero-shot, zero-shot-CoT, retrieval-selected multi-shot CoTs, CoT-Influx many-shot compressed CoTs, and prompt-compression variants.",
            "performance": "GSM8K EM: LLaMA2-7B 13.79%, LLaMA2-13B 27.82%, LLaMA2-70B 55.42% (reported baseline).",
            "performance_comparison": "CoT-Influx improved these to 15.92% (7B), 32.37% (13B), and 59.59% (70B).",
            "format_effect_size": "Baseline; CoT-Influx reported +2.13% to +4.55% absolute EM gains over this format on GSM8K.",
            "explanation_or_hypothesis": "Manual 8-shot CoTs are curated for quality; simply increasing shot count without targeted pruning or selection can be harmful due to redundancy and context-window constraints.",
            "null_or_negative_result": false,
            "experimental_details": "8-shot prompts follow Wei et al. (2022) standard; average token counts and context budget cause limits on how many examples can be inserted (conventionally 8).",
            "uuid": "e9312.1",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "TopK retrieval (semantic)",
            "name_full": "TopK Semantic Retrieval of CoT Examples",
            "brief_description": "Retrieval baseline that selects the top-k semantically similar CoT exemplars from the pool (by embedding similarity) and concatenates them as few-shot context up to context-window limits.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K",
            "task_description": "Selecting most similar CoT exemplars for in-context learning and measuring resultant EM accuracy.",
            "presentation_format": "Top-k most semantically similar CoTs (k up to 20 given 4096-token window) concatenated before question.",
            "comparison_format": "Compared to manual 8-shot CoT, random retrieval, BM25, CoT-Influx, and prompt-compression variants.",
            "performance": "GSM8K EM (20-shot TopK reported): LLaMA2-7B ~14.56%, LLaMA2-13B ~23.65%, LLaMA2-70B ~54.59% (table entries).",
            "performance_comparison": "CoT-Influx (48-shot compressed) improved LLaMA2-13B to 32.37% (TopK 23.65% → +8.72).",
            "format_effect_size": "TopK retrieval can outperform naive few-shot in some cases but was substantially outperformed by CoT-Influx when CoT-Influx could fit more informative shots in the same token budget (+~8.7% on 13B in GSM8K).",
            "explanation_or_hypothesis": "TopK retrieves model-agnostic examples based on similarity; it does not consider token redundancy or model-specific example difficulty preferences, so retrieved examples may include redundant or harmful tokens.",
            "null_or_negative_result": false,
            "experimental_details": "TopK retrieval limited by LLaMA2 context window to 20 CoTs in reported comparisons; average token counts for retrieved sets reported in Table 2; retrieval performed from MRD3 candidate pool.",
            "uuid": "e9312.2",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Random / BM25 retrieval",
            "name_full": "Random selection and BM25-based retrieval of CoT examples",
            "brief_description": "Baselines that either randomly sample CoT exemplars or select exemplars via classical BM25 lexical retrieval, used to estimate average or lexical-retrieval performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K",
            "task_description": "Evaluate effect of non-optimized example selection strategies for few-shot CoT prompting on EM accuracy.",
            "presentation_format": "20-shot (or as many as fit) few-shot CoTs selected either randomly or by BM25 lexical score, concatenated before the question.",
            "comparison_format": "Compared to TopK (semantic), manual 8-shot, prompt compression methods, and CoT-Influx.",
            "performance": "Random retrieval GSM8K EM: LLaMA2-7B 12.51%, LLaMA2-13B 22.21%, LLaMA2-70B 53.07%. BM25 retrieval GSM8K EM: LLaMA2-7B ~13.42%, LLaMA2-13B ~25.17%, LLaMA2-70B ~54.21%.",
            "performance_comparison": "CoT-Influx outperforms both random and BM25 retrievals by several absolute percentage points (varies by model), e.g., vs BM25 on 13B: ~25.17% → 32.37% (+7.20).",
            "format_effect_size": "Random selection can underperform manual 8-shot (Table 1 shows random 16-shot might be worse than manual 8-shot); BM25 typically similar or slightly better than random but still inferior to CoT-Influx.",
            "explanation_or_hypothesis": "Random selection reflects average performance of the CoT pool; BM25 focuses on lexical overlap but misses format/difficulty preferences and token redundancy which are important for math CoTs.",
            "null_or_negative_result": false,
            "experimental_details": "Random and BM25 baselines selected as many CoTs as possible without exceeding context-window; empirical results reported in Table 2 and Table 1 (pilot).",
            "uuid": "e9312.3",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Prompt Compression Baselines",
            "name_full": "Prompt Compression Methods (LLMLingua, Selective Context, GPT-4 Compression)",
            "brief_description": "Methods that compress prompts/tokens to fit more examples into context: LLMLingua and Selective Context (automated token pruning/abstraction) and GPT-4-based compression that asks GPT-4 to summarize/compress exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K and other math datasets",
            "task_description": "Assess whether compressing exemplar texts preserves the essential structure for math CoT prompting and its effect on EM accuracy.",
            "presentation_format": "Apply an automated compression method to each retrieved CoT exemplar (or whole prompt) to reduce token count, then concatenate compressed exemplars before the question.",
            "comparison_format": "Compared to uncompressed TopK retrieval and CoT-Influx pruned tokens.",
            "performance": "Examples on GSM8K (TopK+compression variants, 40-shot reported): TopK+GPT-4 Compression LLaMA2-13B ~11.01% EM; TopK+Selective Context LLaMA2-13B ~0.76% EM; TopK+LLMLingua LLaMA2-13B ~8.34% EM. (Selective Context and LLMLingua also showed very low figures for smaller models.)",
            "performance_comparison": "CoT-Influx (32.37% on 13B) greatly outperforms all tested compression baselines (e.g., +~24 percentage points over LLMLingua on 13B in GSM8K).",
            "format_effect_size": "Prompt compression methods often caused large accuracy drops (e.g., LLMLingua and Selective Context reduce EM substantially), indicating naive compression tends to remove crucial numerical/format tokens.",
            "explanation_or_hypothesis": "Generic prompt compression that prunes tokens by designed metrics can remove numerals or format tokens crucial for math reasoning; GPT-4-based compression tends to remove reasoning steps, hurting CoT effectiveness. CoT-Influx's RL-trained token pruner preserves structural tokens and avoids harmful deletions.",
            "null_or_negative_result": false,
            "experimental_details": "Compression methods were applied to TopK retrieved exemplars, with experiments reporting average tokens and final EM; authors show qualitative examples of important tokens removed by compressors (Figure 2 and Appendix A.5).",
            "uuid": "e9312.4",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Context Window Extension (PI)",
            "name_full": "Context Window Extension via Positional Interpolation (32K)",
            "brief_description": "A method that extends a pre-trained LLM's context window (here LLaMA2-7B) to 32K tokens using positional interpolation, allowing many more exemplars without compression.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLaMA2-7B (extended to 32K)",
            "model_size": "7B",
            "task_name": "GSM8K",
            "task_description": "Test whether extending context window lets the model benefit from more CoT exemplars without pruning.",
            "presentation_format": "Same prompt content as narrow-window experiments but allowed to exceed 4k tokens up to 32k via positional interpolation.",
            "comparison_format": "Compared to LLaMA2-7B with CoT-Influx pruning of prompts to fit original window.",
            "performance": "Reported results show LLaMA2-7B-32K EM at several shot counts (e.g., 12-shot 11.37%, 16-shot 12.81%, vs unextended LLaMA2-7B 13.87% at similar shots).",
            "performance_comparison": "Authors report that context-window extension weakened reasoning ability on the same input prompt relative to original LLaMA2 (CoT-Influx enabled better unlocking of the context window).",
            "format_effect_size": "Context extension did not improve and in reported cases reduced EM compared to narrower-window baseline on identical prompts (qualitative/quantitative decline observed).",
            "explanation_or_hypothesis": "Extending context without targeted pruning may dilute attention or change model behavior; the authors observe that their finding (more helpful CoTs helps) does not hold for LLMs with extended windows via PI, and that PI can weaken reasoning with the same prompt.",
            "null_or_negative_result": true,
            "experimental_details": "Comparison uses positional interpolation (PI) to extend to 32K tokens; when input prompts did not exceed 4K, direct comparisons were made; when they exceeded, CoT-Influx-pruned prompts were also used for fair comparison (Appendix A.1 and Table 9).",
            "uuid": "e9312.5",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zero-shot and Zero-shot-CoT",
            "name_full": "Zero-shot and Zero-shot Chain-of-Thought Prompting",
            "brief_description": "Baseline formats where the model is given the task instruction only (zero-shot) or a zero-shot CoT trigger (e.g., 'Let's think step by step') to induce reasoning without few-shot exemplars.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K",
            "task_description": "Assess baseline reasoning capability with no exemplars or with a zero-shot CoT trigger.",
            "presentation_format": "Zero-shot: question only. Zero-shot-CoT: question with an instruction or trigger encouraging chain-of-thought style response.",
            "comparison_format": "Compared to few-shot-CoT and CoT-Influx many-shot CoTs.",
            "performance": "GSM8K EM: Zero-shot (approx): LLaMA2-7B ~4.25% EM; Zero-shot-CoT increased but still low (LLaMA2-13B ~12.28% per Table 2 noisy formatting).",
            "performance_comparison": "Zero-shot and zero-shot-CoT are substantially lower than few-shot-CoT and far below CoT-Influx many-shot compressed exemplars.",
            "format_effect_size": "Zero-shot-CoT can help somewhat versus pure zero-shot, but few-shot CoTs (especially curated or many-shot compressed forms) substantially improve EM.",
            "explanation_or_hypothesis": "Providing worked examples supplies meta-training signals for in-context learning; zero-shot triggers are helpful but cannot substitute for diverse exemplars.",
            "null_or_negative_result": false,
            "experimental_details": "Standard zero-shot and zero-shot-CoT prompts as used in literature (Kojima et al., 2022). Exact numbers in paper tables are partially garbled but show low baseline performance.",
            "uuid": "e9312.6",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Self-consistency / Self-verification",
            "name_full": "Self-consistency (majority vote) and Self-verification applied to CoT outputs",
            "brief_description": "Decoding-time techniques that sample multiple chain-of-thought outputs and either take majority-voted final answers (self-consistency) or apply a verifier to filter/score outputs (self-verification), used on top of CoT prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2-13B and LLaMA2-70B (applied on outputs from CoT-Influx)",
            "model_size": null,
            "task_name": "GSM8K",
            "task_description": "Evaluate how decoding aggregation/verification methods further improve EM after using CoT-Influx pruned prompts.",
            "presentation_format": "CoT-Influx pruned prompts; sample multiple chain-of-thought decodings (maj@20 or verify@20) and aggregate/verify answers.",
            "comparison_format": "CoT-Influx single-pass vs CoT-Influx+maj@20 vs CoT-Influx+verify@20.",
            "performance": "LLaMA2-13B: CoT-Influx 32.37% → +maj@20 33.43% → +verify@20 34.04%. LLaMA2-70B: 59.59% → 60.73% (maj@20) → 61.79% (verify@20).",
            "performance_comparison": "Both self-consistency and self-verification gave modest additional gains (≈+1.0–2.2 percentage points) on top of CoT-Influx.",
            "format_effect_size": "+1.06% (maj) and +1.67% (verify) on 13B; +1.14% and +2.20% on 70B respectively.",
            "explanation_or_hypothesis": "Aggregating multiple sampled CoT chains reduces decoder sampling variance and helps recover correct final answers; verification helps filter reasoning chains that stray due to pruned context.",
            "null_or_negative_result": false,
            "experimental_details": "Sampling n=20 outputs for majority or verification aggregation; reported numbers in Table 5.",
            "uuid": "e9312.7",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Ablation: Shot-only vs Token-only pruning",
            "name_full": "Ablation of CoT-Influx pruner stages (retain only shot-level or token-level pruning)",
            "brief_description": "Tests whether pruning entire CoT examples (shot pruner) or pruning tokens within retained shots (token pruner) is more important to performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B, 70B)",
            "model_size": null,
            "task_name": "GSM8K",
            "task_description": "Compare full coarse-to-fine pruning vs only shot-level pruning vs only token-level pruning using EM accuracy.",
            "presentation_format": "Three formats: (a) full CoT-Influx (shot+token), (b) prune shot only and keep tokens intact, (c) prune tokens only but keep same set of shots.",
            "comparison_format": null,
            "performance": "GSM8K EM: Full CoT-Influx — 7B 15.92%, 13B 32.37%, 70B 59.59%. Shot-only: 7B 15.69% (-0.23), 13B 31.08% (-1.29), 70B 57.77% (-1.82). Token-only: 7B 12.05% (-3.87), 13B 25.32% (-7.05), 70B 49.36% (-10.23).",
            "performance_comparison": "Removing token-level pruning causes a much larger drop than removing shot-level pruning, indicating shot pruning is easier but token pruning crucial for preserving important information compactly.",
            "format_effect_size": "Token-only pruning vs full: drops of ~3.9–10.2 absolute percentage points across model sizes; shot-only pruning incurs smaller drops (~0.2–1.8 pp).",
            "explanation_or_hypothesis": "Shot-pruner achieves coarse redundancy reduction, but token pruner is essential to preserve crucial numerical/format tokens while trimming function words; token pruning is more challenging and critical for math CoTs.",
            "null_or_negative_result": false,
            "experimental_details": "Ablations reported in Table 7; pruning ratios: total ~4.28× reduction in tokens, with shot pruner contributing ~3.87× (Figure 5).",
            "uuid": "e9312.8",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Random-selection harms",
            "name_full": "Negative effect of randomly adding more CoT exemplars",
            "brief_description": "Empirical observation that simply adding more (random) CoT exemplars can degrade reasoning performance relative to a curated small set; example selection matters.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA2 (7B, 13B)",
            "model_size": null,
            "task_name": "GSM8K (pilot study / Table 1)",
            "task_description": "Assess impact of exemplar selection quality when increasing number of CoT shots.",
            "presentation_format": "Randomly selected 16-shot prompts vs curated manual 8-shot prompts.",
            "comparison_format": "Random 16-shot vs manual 8-shot baseline.",
            "performance": "Table 1 pilot: LLaMA2-7B manual 8-shot 13.79% vs random 16-shot 12.36% (random1) and 13.27% (random2). LLaMA2-13B manual 8-shot 27.82% vs random 16-shot 23.04% and 23.28%.",
            "format_effect_size": "Randomly increasing shots can decrease EM by several absolute percentage points (e.g., ~4.78 pp drop on 13B in the reported random samples).",
            "explanation_or_hypothesis": "Not all CoT examples are beneficial; some are useless or harmful. Example selection must be model-aware and quality-aware rather than random or purely similarity-based.",
            "null_or_negative_result": true,
            "experimental_details": "Pilot study using MRD3 candidate pool; random retrieval selected 16 shots in these experiments (Table 1).",
            "uuid": "e9312.9",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MRD3 dataset",
            "name_full": "Math Reasoning Dataset with Diverse Difficulty (MRD3)",
            "brief_description": "A curated and GPT-4-evolved math CoT dataset collected by merging GSM8K/MAWPS/AQuA seeds and using GPT-4 to generate diverse CoT exemplars with difficulty scores and evolved variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used for training/evaluating pruner and retrieval",
            "model_size": null,
            "task_name": "CoT candidate pool for GSM8K and other math datasets",
            "task_description": "Provides a large, diverse pool of CoT exemplars (varying difficulty and reasoning steps) used to train the CoT-Influx pruner and to retrieve few-shot exemplars for testing.",
            "presentation_format": "Each example: question, formatted reasoning steps (CoT), answer; each assigned a difficulty 1–10 via GPT-4 evaluation; dataset augmented via five mutation/evolution schemes (add_constraints, deepening, increase_reasoning, revise_difficulty, produce_easier).",
            "comparison_format": "Ablation: MRD3 w/o GPT-4 evolution and GSM8K training set as candidate pools.",
            "performance": "Using MRD3 as candidate pool: CoT-Influx GSM8K EM LLaMA2-13B 32.37%; MRD3 w/o evolution 30.55%; GSM8K training set 29.64% (Table 6).",
            "performance_comparison": "CoT-Influx with MRD3 outperforms using MRD3 without evolution and using only GSM8K training set by ~1.8–2.7 absolute EM on LLaMA2-13B GSM8K.",
            "format_effect_size": "GPT-4-evolved examples in MRD3 improve pruner effectiveness by a few percentage points in final EM.",
            "explanation_or_hypothesis": "Diversity in difficulty and reasoning steps helps pruner generalize; different LLM sizes prefer different example difficulty distributions (smaller models prefer easier CoTs).",
            "null_or_negative_result": false,
            "experimental_details": "Initial pool ~9.7K Q/A pairs, GPT-4 generated formatted CoTs and difficulty scores; evolution applied to increase diversity; difficulty filter used when training pruner (only easier questions below threshold d_thr used for RL stability).",
            "uuid": "e9312.10",
            "source_info": {
                "paper_title": "Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "What makes good in-context examples for gpt-3?",
            "rating": 2,
            "sanitized_title": "what_makes_good_incontext_examples_for_gpt3"
        },
        {
            "paper_title": "Llmlingua: Compressing prompts for accelerated inference of large language models",
            "rating": 2,
            "sanitized_title": "llmlingua_compressing_prompts_for_accelerated_inference_of_large_language_models"
        },
        {
            "paper_title": "Extending context window of large language models via positional interpolation",
            "rating": 2,
            "sanitized_title": "extending_context_window_of_large_language_models_via_positional_interpolation"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Compressing context to enhance inference efficiency of large language models",
            "rating": 2,
            "sanitized_title": "compressing_context_to_enhance_inference_efficiency_of_large_language_models"
        }
    ],
    "cost": 0.025018999999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
15 Feb 2024</p>
<p>Xijie Huang 
Hong Kong University of Science and Technology</p>
<p>Microsoft Research</p>
<p>Li Lyna Zhang lzhani@microsoft.com 
Microsoft Research</p>
<p>Kwang-Ting Cheng 
Hong Kong University of Science and Technology</p>
<p>Fan Yang 
Microsoft Research</p>
<p>Mao Yang 
Microsoft Research</p>
<p>Microsoft Research</p>
<p>Fewer is More: Boosting LLM Reasoning with Reinforced Context Pruning
15 Feb 2024A16B083D284FC1EBBACF8F81FBDC142BarXiv:2312.08901v3[cs.CL]
Large Language Models (LLMs) have shown impressive capabilities, yet they still struggle with math reasoning.In this work, we propose CoT-Influx, a novel approach that pushes the boundary of few-shot Chain-of-Thoughts (CoT) learning to improve LLM mathematical reasoning.Motivated by the observation that adding more concise CoT examples in the prompt can improve LLM reasoning performance, CoT-Influx employs a coarse-to-fine pruner to maximize the input of effective and concise CoT examples.The pruner first selects as many crucial CoT examples as possible and then prunes unimportant tokens to fit the context window.A math reasoning dataset with diverse difficulty levels and reasoning steps is used to train the pruner, along with a math-specialized reinforcement learning approach.As a result, by enabling more CoT examples with double the context window size in tokens, CoT-Influx significantly outperforms various prompting baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 math datasets, achieving up to 4.55% absolute improvements.Remarkably, without any fine-tuning, LLaMA2-70B with CoT-Influx surpasses GPT-3.5 and a wide range of larger LLMs (PaLM, Minerva 540B, etc.) on the GSM8K.CoT-Influx serves as a plug-and-play module for LLMs and is compatible with most existing reasoning prompting techniques, such as self-consistency and self-verification.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable capabilities across a range of tasks (Brown et al., 2020;OpenAI, 2023a).However, it remains a significant challenge to improve LLM performance on reasoning tasks, especially for smaller LLMs like LLaMA (Touvron et al., 2023a) on math reasoning.</p>
<p>While existing efforts focus on optimizing Chain-of-Thought (CoT) prompts (Wei et al., 2022;Wang et al., 2023d;Yao et al., 2023) and fine-tuning LLMs (Luo et al., 2023) under the zero-shot setting, the potential of few-shot learning in improving LLM reasoning has not been fully explored.Inspired by the human reasoning process, we propose the hypothesis: if LLMs are exposed to more step-by-step problem-solving examples (i.e., CoTs) before answering questions, it could potentially improve LLMs reasoning capability to generate a correct solution.This leads to our question: what's the boundary of LLM reasoning capability achievable through inputting more CoT examples?</p>
<p>However, we face two major obstacles.First, the limited token length of LLMs' context window restricts the number of few-shot examples.Extending the context window is one solution, but it requires expensive fine-tuning and increases inference overhead (Chen et al., 2023a;Peng et al., 2023a).While prompt compression (Li et al., 2023b;Jiang et al., 2023) is another approach, it underperforms in math reasoning.Tokens like numerical and format ones, though identified redundant, are crucial for few-shot math problem solving.</p>
<p>Second, it's challenging to select helpful CoT examples.Section 3 reveals that random choices can even harm reasoning performance.Existing retrieval-based methods (Liu et al., 2021;Scarlatos and Lan, 2023) are not tailored for math reasoning, making them suboptimal.These retrieved examples are model-agnostic, while we found that different LLMs favor CoT examples of varying characteristics (e.g., diverse difficulty levels).</p>
<p>In this work, we propose CoT-Influx, which addresses all the above challenges and pushes the boundaries of utilizing few-shot learning to improve LLM math reasoning capability.CoT-Influx is motivated by the observation that current LLM context window has not been fully utilized due to redundancy at both the example and token levels in natural language input.As such, these redundant inputs can be pruned to free up space for more informative context.The central idea of CoT-Influx is to input long lengthy CoT examples, select the crucial examples for the target LLM, and then prune redundant tokens to fit within the original LLM context window.As a result, by inputting much more helpful CoT examples, each composed solely of informative tokens and with a shorter length, we greatly improve LLM ability to solve math problems.Moreover, as all these inputs remain within the context window, we do not increase any inference overhead.This stands in stark contrast to other methods (Hao et al., 2022;Chen et al., 2023a).</p>
<p>CoT-Influx treats the target LLM as a black box, and serves as a plug-and-play module for LLMs as shown in Fig. 3.The key module is a coarse-to-fine pruner involving two steps: (i) a shot pruner first selects the most helpful CoT examples from a large batch of shots, and (ii) a token pruner then removes unimportant tokens from these selected CoT examples.To effectively train the pruner module tailored for math reasoning, CoT-Influx is built upon the following novel techniques.</p>
<p>First, CoT-Influx requires a CoT dataset for training and inference.Existing CoT examples, heavily reliant on costly human engineering, often struggle with diversity and quality.To address this, we employ GPT-4 (OpenAI, 2023a) and Evol-Instruct (Xu et al., 2023) to create a math reasoning dataset, called MRD 3 .With problems of varying difficulty and reasoning steps, MRD 3 enables CoT-Influx to generalize across a wide range of math problems.</p>
<p>Second, training the pruner presents two challenges: (1) since we identify discrete tokens before the LLM tokenizer, the loss gradient cannot be backpropagated through the tokenizer to update the pruner; (2) The high difficulty of many math problems, which consistently yield incorrect answers regardless of the quality of compressed fewshot examples, poses a challenge to the effective training of the pruner.To this end, we introduce a novel training approach with reinforcement learning to mitigate the gradient issue.We design a reward function to measure the LLM loss, few-shot math reasoning effectiveness, and token length constraints.Then, we design a difficulty-aware dataloader filtering appropriate problems and introduce two techniques to stabilize the RL training.</p>
<p>Extensive experiments on various LLMs and five math datasets demonstrate the effectiveness of CoT-Influx.CoT-Influx significantly boosts LLM reasoning capability, achieving 1.36%-14.09%absolute improvements over SOTA baselines, and establishes a new prompting-based benchmark in math reasoning accuracy without any fine-tuning or additional inference costs.Remarkably, LLaMA2-70B with CoT-Influx outperforms a broad range of larger LLMs and surpasses GPT-3.5 by 2.5% on GSM8K.Moreover, CoT-Influx excels over retrieval and prompt compression baselines in example selection and identifying crucial tokens.</p>
<p>Related Works</p>
<p>LLMs for Math Reasoning.Drawing from the Chain-of-Thought (CoT) (Wei et al., 2022), recent research has greatly improved the reasoning capabilities of LLMs by providing step-by-step reasoning paths.The main efforts are twofold: enhancing CoT prompts, such as Program-of-Thoughts (Chen et al., 2023b), Tree-of-Thoughts (Yao et al., 2023), and Everything-of-Thoughts (Ding et al., 2023), and innovating CoT-based training data for finetuning LLMs like WizardMath (Luo et al., 2023).</p>
<p>However, most works focus on the zero-shot setting with only task instruction or CoT prompts, leaving the potential of few-shot CoT largely untapped.We explore leveraging few-shot CoT learning to improve LLMs' math reasoning capabilities.Prompt Compression.To address the challenge of limited few-shot examples due to restricted context window length, one related work involves prompt compression.Key approaches include: (1) token pruning (Kim et al., 2022;Li et al., 2023a); (2) soft prompt compression methods (Wingate et al., 2022;Mu et al., 2023;Chevalier et al., 2023;Ge et al., 2023); and (3) information-entropy-based approaches (Li et al., 2023b;Jiang et al., 2023).</p>
<p>However, they do not effectively solve our problem for two reasons.First, they prune tokens based on designed metrics, often failing to remove redundancy of the entire CoT examples.Second, some tokens such as numerical and format tokens, although redundant, are crucial for math reasoning.Prompt Retrieval optimizes task performance by selecting high-quality few-shot examples using either heuristics or a supervised retriever model.Heuristic methods, such as the widely used TopK retrieval (Liu et al., 2021;Gao et al., 2021), BM25 (Robertson et al., 2009), VoteK (Hongjin et al., 2022), and entropy (Lu et al., 2022) A standard practice for evaluating LLMs' math reasoning capability is the use of 8-shot manuallydesigned CoTs (Wei et al., 2022).We increase the number of CoT shots to see if reasoning accuracy improves.To avoid poor-quality examples, we use the TopK method (Liu et al., 2021) to select the k most relevant CoT examples for each question.Given LLaMA2's context window limit of 4096 tokens, we could only input up to 20 CoT examples ‡ .As Fig. 1 shows, increasing CoT examples improves LLaMA2-7B's reasoning accuracy on the GSM8K dataset, significantly outperforming the standard 8-shot setting.However, the limited LLM context window hinders the full potential of fewshot CoT learning for improving math reasoning.For instance, even with 20 CoTs not hitting the token limit, accuracy drops as the large input context limits the LLM's response space.Observation 2: CoT example selection is crucial for math reasoning.Simply adding CoT examples randomly doesn't boost performance.‡ The input token length is less than the context window token limit, as the answer generation also shares this limit.</p>
<p>The prior study suggests that more CoT examples can improve LLM reasoning performance.However, the quality of CoT examples is crucial to the final performance.As shown in Table 1, even with up to 16 CoT shots, random selection underperforms the standard 8-shot setting, which is manually curated for quality.Observation 3: A CoT example contains redundant tokens for math reasoning, which can be pruned to free up space for more informative content.</p>
<p>Observation 2 indicates that few-shot CoT examples contain useless or even harmful examples that can be pruned.We further observe that a CoT example often has redundant tokens.For instance, the blue tokens in Fig. 2 can be removed without affecting LLM performance.However, identifying redundant tokens for math reasoning poses a challenge.Simply using existing prompt compression methods (Jiang et al., 2023;Li et al., 2023b) leads to a significant performance decline.Fig. 2 shows a compressed example using LLMLingua (Jiang et al., 2023).Some numerical and format tokens (colored in red), while identified as redundant, are crucial for LLM to comprehend the context for solving a math problem.</p>
<p>Figure 2: A compressed CoT example using the prompt compression tool of LLMLingua (Jiang et al., 2023).The pruned tokens contain truly redundant tokens (colored in blue) and crucial tokens (colored in red).</p>
<p>CoT Dataset Collection</p>
<p>We start by collecting a high-quality math reasoning dataset, comprising diverse CoT examples with varying steps and difficulties.We merge the training set of GSM8K (Cobbe et al., 2021), MAWPS, MAWPS-single (Koncel-Kedziorski et al., 2016), and 1000 random examples from AQuA (Ling et al., 2017) to create an initial dataset of 9.7K questionanswer pairs.Then, we prompt GPT-4 to generate formatted CoT reasoning steps.Notably, it's crucial to maintain a consistent format for each example in few-shot learning.Our dataset also assigns a difficulty score from 1 to 10 for each question, based on GPT-4's evaluation, where 1 signifies the easiest questions and 10 is the most difficult.</p>
<p>We observe that most questions in this initial dataset score between 2-4.To improve difficulty diversity, we use GPT-4 to mutate questions, generating corresponding CoTs with varied difficulty levels.We apply 5 mutation schemes, three to increase reasoning difficulty and two to simple questions.The final dataset is referred to as Math Reasoning Dataset with Diverse Difficulty (MRD 3 ).</p>
<p>Problem Formulation</p>
<p>Let D denote the CoT dataset (i.e., the MRD 3 ), and D = {x cot i } k i=1 be a subset of k CoT examples, each composed of a question, reasoning steps, and an answer.The total number of tokens in these k CoT examples far exceeds the LLM context window length limit of T .CoT-Influx is designed to perform a two-step pruning process:
D = {x cot i } k i=1 Shot Pruner − −−−−− → {x cot j } k ′ j=1 Token Pruner − −−−−−− → {x cot j } k ′ j=1(1)
Initially, non-useful CoT examples are pruned from D, resulting in a reduced set of k ′ examples.Then, for each retained CoT example x cot , redundant tokens are pruned, yielding a shorter example, x cot .Let x question denote the question that LLM is tasked to solve.For final input x input , we concatenate all tokens from {x cot j } k ′ j=1 and place them before x question .Our goal is to optimize the input x input , so that LLM can correctly answer the question under x input .Meanwhile, the token count of x input , t, must be less than the LLM context window limit T .Formally, we optimize the following:
min D⊆D L LLM x input , max D⊆D R Acc y LLM x input , y answer , s.t. t x input = k ′ 1 |x cot | + |x question | ≤ T (2)
where L LLM is LLM loss, and R Acc evaluates if LLM's answer y LLM (x input ) matches the correct answer y answer , this will be elaborated in Sec.4.4.Overview.Fig. 3 illustrates our approach.The core component is a lightweight, plug-and-play module (Sec.4.3), which consists of a small text embedding extractor and a coarse-to-fine pruner.</p>
<p>To train the pruner, we face the challenge of gradient backpropagation when pruning discrete tokens outside the LLM.The LLM loss gradient cannot be backpropagated through the tokenizer.</p>
<p>To address this, we design a multi-objective reward function and use reinforcement learning for effective training (Sec.4.4).The overall training process is outlined in Algorithm 1.</p>
<p>Coarse-to-fine Pruner Design</p>
<p>Text embedding extractor.As CoT-Influx serves as an external module, we need to extract text embedding as prediction features.However, it's nontrivial to extract features for long inputs beyond the LLM context window.To address this, we use a small encoder model, BERT-Large (Devlin et al., 2018), to extract sentence-level (i.e., a CoT example) embedding instead of extracting token embedding from the entire long context.For a batch of k CoT examples, each example is padded to N =512 tokens.BERT then inferences these examples to obtain the final layer of text embedding, denoted as H s shot ∈ R k×N ×D BERT , where D BERT is BERT's hidden dimension size.State.As shown in Fig. 3, we define state s shot ∈ R k×N for the first shot pruner, representing the input batch of k CoT examples ∈ D. For the second token pruner, we define state s token ∈ R k ′ ×N , which represents all remaining tokens after the shot pruner.k ′ is the number of retained examples.Action.Let a shot and a token denote the actions predicted by the shot and token pruner, respectively.The action space is defined as {0, 1}, where 1 signifies retention and 0 indicates pruning.a shot determines whether each CoT example should be pruned, while a token predicts the pruning of each token in the retained CoT examples.Two-stage policy network.The pruner module is a two-stage policy network, each stage is a two-layer feed-forward network (MLP) with GELU activation.This module outputs a continuous categorical distribution π, used for sampling discrete actions (i.e., {0, 1}).Let θ denote the MLP's trainable parameters and σ(•) the sigmoid function.Based on the current states {s shot , s shot } and the hidden states {H s shot , H s token }, the policy network sequentially make two action predictions as follows:
π(a shot |s shot ; θ) = σ MLP Hs shot (3) π(a token |s token ; θ) = σ MLP Hs token ,(4)
where a shot and a token are the predicted actions, sequentially predicting whether to prune each of the k CoT examples and each token within the retained examples, respectively.We predict the discrete action by sampling from the categorical distribution π with a softmax function.Get π (ashot|sshot; θ) with Eq. 3, sample ashot 10:
{x cot } k i=1 − → {x cot } k ′ i=1
11:
Hs token = BERT {x cot } k ′ i=1
12: Get π (atoken|stoken; θ) with Eq. 4, sample atoken 13:
{x cot } k ′ i=1 − → {x cot } k ′ i=1 14: x input = {x cot } k ′ i=1 , x few-shot , x question 15:
Output LLM(x input ); Compute R with Eq. 5 16:</p>
<p>end for 17:</p>
<p>Compute policy gradient using Eq.6, update θ 18: end for 19: ▶ Phase 3: LLM reasoning with pruner and MRD 3 20: Retrieve Top-k shots {x cot q } k ∈ D for target question q 21: Do pruning:
{x cot q } k θ − → {x cot q } k ′ , reconstruct {x cot q } k ′ 22: x input q = {x cot q } k ′ , x few-shot , x question q 23: Get LLM reasoning output LLM(x input q )
4.4 End-to-end RL Optimization Multi-objective Reward.Our objective in Eq. 2 is to train the pruner module to identify the most crucial CoT examples and useful tokens for math problem solving, while keeping the final tokens within the original LLM context window.To achieve this, we design a multi-objective reward.Let x input be the final input to LLM, which includes the retained CoT tokens from the policy network and the target question.t represents the token count of x input , and T is the token count limit.The reward R is defined as follows:
R x input = ( 1 1 + L LLM + R Acc ) × t T w (5)
where the first term evaluates the effectiveness of inputted CoT tokens, and the second term ensures they are within the LLM context window.L LLM is LLM's prediction loss under x input , R Acc evaluates the reasoning accuracy (to be discussed later).w is a hyperparameter that penalizes the token count t for being too short (i.e., w &lt; 0) or exceeding (i.e.,w &gt; 0 ) the token limit T .</p>
<p>In addition to L LLM , we introduce R Acc to evaluate math reasoning accuracy.This is because L LLM , the average loss of generated tokens, doesn't fully reflect LLM's ability to generate correct answers.Specifically, R Acc is set to 1 for a correct answer and 0 for an incorrect one.Notably, we found that if the format or crucial tokens are pruned, LLM struggles to interpret the input context correctly, leading to irrelevant answers for math problem solving.In such cases, we penalize R Acc with a value of -0.1.Optimization with REINFORCE.We employ reinforcement learning to maximize the reward and train the two-stage policy network.According to REINFORCE (Williams, 1992), the network parameters are updated by the gradients:
R • ∇ θ logπ(a shot |s shot )π(a token |s token ) (6)
Notably, as shown in Fig. 3 First, despite the optimization of question set D question through the filter, LLM performance for a randomly sampled question under different fewshot prompts can still be unpredictable.This unpredictability, where a question might yield correct results under low-quality pruned prompts and a complex question might fail under carefully pruned prompts, can affect the pruner's training effectiveness.To address this, we continuously repeat a sampled question multiple times, t repeat , each time with a different pruned few-shot prompt from the pruner.Moreover, we use exponential moving average (EMA) to smooth reward R Acc in Eq. 5.</p>
<p>Second, during the early training, our pruner module makes random decisions, leading to arbitrary removal of CoT examples and tokens.These randomly pruned few-shot prompts can cause instability in RL training.Empirically, we append the manually-designed 8-shot CoTs (Wei et al., 2022)  to the pruned prompts.This ensures a good lower bound and stabilizes the training.</p>
<p>Evaluation</p>
<p>Models, datasets and metric.We evaluate CoT-Influx on LLaMA2-7B, LLaMA2-13B, and LLaMA2-70B.The mathematical datasets for evaluation include GSM8K (Cobbe et al., 2021), AddSub (Hosseini et al., 2014), Multiarith (Roy and Roth, 2015), Svamp (Patel et al., 2021), and Singleeq (Koncel-Kedziorski et al., 2015).For evaluation metric, we report Exact Match (EM) accuracy of the predicted answers.</p>
<p>Baselines We set three baselines for comparison:</p>
<p>• CoT and few-shot CoT prompting: We compare with widely-used prompts for LLM reasoning, including zero-shot, zero-shot-CoT (Kojima et al., 2022), and the standard few-shot-CoT (Wei et al., 2022) with 8 shots.• Prompt retrieval: we also compare with retrieval baselines, specifically using random, TopK (Liu et al., 2021), and BM25 (Robertson et al., 2009) methods.We select as many CoT examples as possible using each method, without exceeding LLM context window.Random retrieval is to reflect the average quality of our CoT dataset.• Prompt compression: To evaluate the effectiveness of identifying crucial tokens, we compare the resulting compressed prompts from the same batch of CoT shots with state-of-the-art prompt compression baselines: Selective Context (Li et al., 2023b), LLMLingua (Jiang et al., 2023), and compression through GPT-4.</p>
<p>Main Results</p>
<p>Effectiveness of enabling more few-shot CoTs.</p>
<p>We first evaluate how far the boundary of few-shot learning can be pushed using CoT-Influx.For comparison, we set up two baselines: (i) Few-shot CoT, using 8 manual-designed CoT shots as the default  LLM evaluation setting on GSM8K.(ii) TopK retrieves 20 CoT shots from our dataset, denoting the max shot number within LLaMA2 context window.</p>
<p>For CoT-Influx, we test LLaMA2 7B and 13B on GSM8K, adjusting the number of CoT shots from 16 to 64 examples, which corresponds to 0.7× to 2.8× the token count of LLaMA2 context window.As shown in Fig. 4, we make two observations: (1) More CoT shots, facilitated by CoT-Influx, indeed boosts LLM math reasoning performance, particularly for larger LLMs.On LLaMA2-13B, by inputting 48 CoTs, we significantly outperform the standard few-shot CoT and TopK by 4.55% and 8.72%, respectively.(2) There is an optimal number of CoT shots for CoT-Influx.Its peak performance on LLaMA2 7B and 13B are at 40 and 48 shots, respectively.We attribute this to two potential reasons.First, an extremely large number of shots complicates CoT-Influx's optimization.Second, there may be an upper limit to improving LLM reasoning capability through few-shot learning.</p>
<p>Comparison with state-of-the-art baselines.Table 2 and Table 3 present the comparison results of CoT-Influx with state-of-the-art baselines across LLaMA2 family and 5 mathematical datasets, highlighting the following observations: (1) by utilizing more few-shot CoTs that are twice the LLM context window, CoT-Influx significantly outper-  vanced reasoning-based prompts.To prove this, we apply self-consistency (Wang et al., 2023d) and self-verification (Weng et al., 2023) to compressed prompts generated by CoT-influx.For evaluation efficiency, we sampled 20 times.As Table 5 shows, applying self-consistency and self-verification further improve LLaMA2's performance on GSM8k.</p>
<p>Ablation Study and Analysis</p>
<p>The effectiveness of MRD 3 dataset.Beyond our pruner, we introduce MRD 3 dataset, which is evolved by GPT-4 for diverse reasoning steps and difficulties.We compare with two baselines: (1) MRD 3 without evolution, excluding GPT-4 evolved examples, and (2) the human-labeled GSM8K training set, which excludes GPT-4's reformatted generation.We apply our pruner on these datasets under the same setting.As shown in  Token pruning ratios.We now investigate token pruning ratios by our pruner.Fig. 5 shows the remaining token length for LLaMA2-70B after our pruner.In total, we achieve a 4.28× pruning ratio, with shot pruner contributing a 3.87× ratio.The results suggest that our pruner favors pruning more coarse-grained shots over fine-grained tokens.</p>
<p>Inference cost.CoT-Influx is a lightweight plugand-play module, including a 336MB BERT-Large model and a tiny 4MB coarse-to-fine pruner.We measure its additional inference cost.</p>
<p>Appendix</p>
<p>This appendix includes additional analysis, the evolution of MRD 3 , pruner training details, additional related works, and prompt settings.These contents are organized in separate sections as follows:</p>
<p>• Sec.A provides additional analysis and case studies, including the comparison of CoT-Influx with context window extension methods, performance of CoT-Influx on finetuned LLMs (LLaMA2-13B-Chat and GPT-3.5-Turbo),ablation study on the reward design, and sensitivity analysis on hyperparameters of the pruner.Additional case studies on the GSM8K with different prompting methods are given to extensively prove the effectiveness of our method.</p>
<p>• Sec.B introduces the prompt we used for the evolution of the examples in our MRD 3 .Both the original input and the evolution results are given as examples.We also analyze the difficulty and reasoning step distribution of different evolution methods and derive a new observation regarding difficulty preference for different LLMs.</p>
<p>• Sec.C includes the algorithm for training data preparation as a supplement to Algorithm 1.The hyperparameter settings, the training dynamic of the pruner, and the detailed introduction of the evaluation dataset are also included.</p>
<p>• Sec.D elaborates previous LLM context window extension and LLM in-context learning methods, and analyzes the advantage of our proposed CoT-Influx compared with various previous methods.</p>
<p>• Sec.E demonstrates the prompt we used in this work for difficulty and reasoning step evaluation, and GPT-4 based compression on input few-shot prompts.</p>
<p>A Additional Analysis and Case Study</p>
<p>A.1 Comparison with context window extension methods</p>
<p>While our work tackle the challenge of limited context window by pruning the redundant input few-shot prompts, another solution is to extend the context window of LLMs.We compare the math reasoning performance of LLaMA2-7B with CoT-Influx and LLaMA2-7B with 32K token context window extended with Positional Interpolation (PI) (Chen et al., 2023a).The results are listed in Table 9.When the input prompt does not exceed the window token limit (the number of input shots is not more than 20), we compare the performance of LLaMA2-7B-32K with LLaMA2-7B.When the input prompt exceed the context window length, we apply our CoT-Influx to prune the prompts to make sure that they can be directly input to LLaMA2-7B without PI.The results show that the context window extension weaken the reasoning ability with the same input prompt.The limit of context window can be unlocked with our CoT-Influx.Moreover, our observation that LLMs can improve reasoning with more helpful CoT examples does not hold true for LLMs with extended context window.</p>
<p>A.2 CoT-Influx on finetuned LLMs</p>
<p>In Sec.5.1, we verify the effectiveness of CoT-Influx on LLaMA2-7B, 13B, and 70B.LLaMA2-chat (Touvron et al., 2023b) and GPT-3.5-Turbo (OpenAI, 2023b) are also the widely adopted LLMs that are acquired after supervised instruction finetuning (SIFT) and Reinforcement Learning from Human Feedback (RLHF), respectively.The different finetuning strategy and the various finetuning data result in unique properties of the LLMs.For example, LLaMA2-Chat-13B perform significantly better than LLaMA2-13B on math reasoning tasks with zero-shot-cot prompts.To show that our CoT-Influx can also help improve the reasoning ability of these finetuned LLMs, we conduct experiments of LLaMA2-13B-Chat and GPT-3.5-Turbo(gpt35-turbo-0613) on GSM8K dataset.As shown from the results listed in Table 10, our CoT-Influx also surpass a wide range of prompting baselines with more input shots and fewer tokens.Specifically on LLaMA2-13B-Chat, CoT-Influx achieve an absolute improvement 9.78% compared to TopK retrieval baseline with only 57.6% average tokens.</p>
<p>A.3 Ablation study on reward design</p>
<p>The reward of our CoT-Influx pruner are made up of three parts: math reasoning accuracy reward R Acc , LLM loss reward R Loss = 1 1+L LLM , and context window token limit reward R Limit = t T w .Each part of the full reward function are important for the effective learning of the pruner.We perform ablation study on these components and the results are listed in Table 11.As can be seen from the results, whenever a reward component are removed, the CoT-Influx pruner give sub-optimal prompt selection and compression results, which cause a decrease compared to the full reward baseline.Among these three reward function parts, the token limit reward R Limit is the most important because training without this term will cause the pruner not to prune any shot or token and directly output the truncated prompt of the redundant input.</p>
<p>A.4 Sensitivity analysis on hyperparameters</p>
<p>We perform sensitivity analysis on the hyperparameters to investigate the robustness of our CoT-Influx pruner training.The most important setting in the training of our CoT-Influx pruner is the token target T and token penalty co-efficient w.Table 12 presents the results of CoT-Influx using different sets of hyperparameters T and w.The results demostrate that the training of our CoT-Influx pruner are highly robust as long as the token target T is not overly aggressive (token target T should not be too small).</p>
<p>C.2 Detailed settings and hyperparameters</p>
<p>The detailed hyper-parameters setting of different LLMs' pruner are listed in Table 14.Majority of these hyperparameters are shared across different LLMs.The evolution subset as the prompt candidates for evaluation are determined by searching the performance of math reasoning on 100 random examples.</p>
<p>C.3 Training dynamics</p>
<p>We visualize the RL training dynamics of the LLaMA2-13B pruner in Figure 7 including the LLM loss reward 1 1+L LLM , prediction reward R Acc , moving average of the final pruner reward R, and remaining token count t.We can see from the results that reward increases steadily with the steps of RL training.The number of remaining tokens decreases rapidly in early steps and then oscillates around the token target.Since our prediction reward R Acc are discrete value of {−0.1, 0, 1}, the oscillation phenomenon are more obvious compared with other reward term.This highlight the effectiveness of question repetition and using Exponential Moving Average (EMA) of final reward to suppress this oscillation.</p>
<p>C.4 Detailed introduction of dataset for evaluation</p>
<p>We introduce the details of the datasets we used for evaluation as follows: • SVAMP (Patel et al., 2021) representing Simple Variations on Arithmetic Math word Problems that conduct question sensitivity variation, reasoning ability variation, and structural variation on existing math datasets.There is a total of 1000 examples and all of them are used for evaluation in our settings.</p>
<p>• MultiArith (Roy and Roth, 2015) is a collection of multi-step arithmetic problems with 600 examples and all of them are used for evaluation in our settings.</p>
<p>• AddSub (Hosseini et al., 2014) is a dataset consisting of addition and subtraction problems with 395 examples and all of them are used for evaluation in our settings.</p>
<p>• SingleEq (Koncel-Kedziorski et al., 2015) consists grade-school algebra word problems that map to single equations with varying length.Every equation may involve multiple math operations including multiplication, division, subtraction, and addition over non-negative rational numbers and only one variable.There are 508 problems, 1117 sentences, and 15292 words in the dataset.</p>
<p>C.5 Rule-based prompt reconstruction</p>
<p>To make sure the input prompt for inference remain structurally intact, we apply a rule-based prompt reconstruction on the input.For example, "\n [question]" will be reconstructed to "\nQ: [question]" and "A: Let's step by step" will be repaired to "A: Let's think step by step".While our pruner has been trained to learn the importance of the structure integrity and consistency, there are still few cases when important tokens are pruned, leading to incorrect reasoning results.The rule-based reconstruction can effectively alleviate the influence of sub-optimal pruning strategy.The important tokens that should be reconstructed include 'Q:', 'A:', '\n', "Let's think step by step", and "The answer is".</p>
<p>D Additional Related Works</p>
<p>LLM In-Context Learning In-context learning (ICL) are one of the emerging abilities of LLMs that conduct various downstream tasks with provided few-shot demonstrations.To fully understand optimize the ICL paradigm, previous research mainly focus on the underlying mechanism of ICL or the proper application of ICL.Pioneering research (Von Oswald et al., 2023;Dai et al., 2023) empirically find the similarity between gradient-descent (GD) and ICL, which interprets the trained LLMs are meta-optimizer that can learn the examples in context in forward pass.More recently, Wang et al. (2023a) propose a hypothesis that label words in examples serve as anchors in ICL, and the anchors can help aggregate and distribute the task-relevant information flow.To better utilize ICL, previous research also research on the input format (Yoo et al., 2022) and order of examples (Min et al., 2022).Our work falls in the second category that shows the compressed examples are an optimal choice for the input of ICL.</p>
<p>LLM Context Window Extension</p>
<p>Recently, there has been rising interests in extending the context window of existing pre-trained LLMs.Common approaches include augmenting external memory modules (Tworkowski et al., 2023;Wang et al., 2023c), which add extra modules to memorize long past contexts but requires complex training, manipulating attention mechanisms (Han et al., 2023;Xiao et al., 2023) or the positional encoding (Chen et al., 2023a;Peng et al., 2023b).However, these require LLM modifications.Our method, applicable to black-box LLMs and extendable context windows, is orthogonal to this direction.</p>
<p>Comparison of CoT-Influx with Previous Methods We summarize the advantage of our CoT-Influx compared with previous prompting strategies in Table 15.Gradient-free indicates the methods do not need to backward through LLMs.Unlimited-token represents the original input prompt for these methods are not limited by the context window length of LLMs.Difficulty-aware refers to whether the method take the difficulty of problems into the consideration of their prompt design.Dynamic #Shots means we do not need to setup a target shot number and the pruned input shot numbers are different across various questions.Our CoT-Influx demonstrate significant advantage over all previous methods.(Shin et al., 2020) RLPrompt (Deng et al., 2022) Context Extension LLMLingua (Jiang et al., 2023) CoT-Influx(Ours)</p>
<p>E Prompt Settings</p>
<p>In this section, we show the prompt we used in this work for reproducibility.The prompt for evaluating the difficulty and reasoning steps of each examples are:</p>
<p>Prompt for difficulty and reasoning steps estimation:</p>
<p>We would like you to evaluate and rate the difficulty and complexity of the following question.You should first give an overall score on a scale of 1 to 10, where a higher score indicates higher difficulty and complexity.You should then evaluate the answer and give how many reasoning steps are in the answer.You must just give the score and the number of reasoning steps without any other reasons.The reply format should be 'Score': [score], 'Steps: [#steps]' ## Question: {Given question} ## Answer: {Given answer} ## Evaluation:</p>
<p>The prompt for GPT-4 Compression on prompts are shown as follow.Note that we encode the restriction of token limits in both the prompt and API by setting the max_new_token.However, the prompt compression results still fail to follow the restrictions for most cases.This disadvantages of uncontrollable token length is also discussed in previous work (Jiang et al., 2023).</p>
<p>Prompt for GPT-4-based compression:</p>
<p>Please compress the following examplars for few-shot in-context learning on math reasoning.The complete examplars could be removed if they are redundant and the tokens within each examplars can also be pruned.'The answer is' in each examplar should be retained and please keep less than {Given token} tokens in total: {Given examplars}</p>
<p>Figure 3 :
3
Figure 3: Above: The overview procedure of CoT-Influx; Below: an example illustrating the use of CoT-Influx to first prune entire CoT examples and then prune tokens.</p>
<p>Algorithm 1
1
Pruner Training and Inference Input: target LLM, dataset D, number of CoTs k, token limit T , manual few-shot cot x few-shot , repeat trepeat 1: ▶ Phase 1: MRD 3 preperation 2: Perform evolution and difficulty evaluation to get D; 3: Use the difficulty filter and split D into questions set Dquestion and CoT candidates set Dcot 4: ▶ Phase 2: Training the two-stage pruner (1 epoch) 5: for (x question , y answer ) in Dquestion do 6: Retrieve Top-k examples D = {x cot } k i=1 from Dcot 7: Hs shot = BERT {x cot } k i=1 8: for j=1 to trepeat do 9:</p>
<p>, only the parameters of the policy network require training.The embedding extractor and LLM are frozen, thus, the overall training overhead is lightweight.Difficulty-aware data filter.Existing LLMs, particularly smaller ones, underperform in math reasoning.If the question is too challenging for LLMs, the answer will always be incorrect, regardless of the quality of compressed few-shot CoT examples, making it challenging to effectively train our pruner module.To address it, we use a difficulty filter to sample a math question set D question from D, which includes only easy questions with a difficulty score below a threshold d thr .During training, each question in D question samples a batch of k CoT examples from D cot , where D cot is the CoT candidate set sampled from D. Stabilize the training.Another challenge is that pruning CoT and tokens during training introduces instability, making it difficult for effective training.</p>
<p>Figure 4 :
4
Figure 4: EM(%) accuracy on GSM8K with inputting different number of CoT examples for CoT-Influx.</p>
<p>Figure 5 :
5
Figure 5: Token length after each stage of our pruner.</p>
<p>Figure 7 :
7
Figure 7: RL training dynamics of the LLaMA2-13B pruner.</p>
<p>Number of CoT Examples Exceed token limits! LLaMA2-7B on GSM8K accuracy Standard 8-shot CoT</p>
<p>, select examples based on semantic similarity.Recently, supervised-based methods like EPR (Rubin et al., This section presents our key observations of fewshot learning in improving LLMs math reasoning, upon which the CoT-Influx design is based.Note that experiments are done with our proposed CoT dataset, MRD 3 , as introduced in Sec.4.1.Observation 1: LLMs can improve reasoning with more helpful CoT examples, but the current context window restricts the number of CoT examples.
15105002468Decreased accuracy due to 12 16 20squeezed token limits of responseFigure 1: LLaMA2-7B reasoning accuracy under anincreasing number of TopK retrieved CoT examples.2021), LLM-R (Wang et al., 2023b), and IDS (Qinet al., 2023) have been proposed, which train aretrieval model to learn better example selection.However, these methods are sub-optimal formath reasoning, as they retrieve model-agnosticexamples. In contrast, LLMs with different capabil-ities favor CoT examples of varying complexities.Moreover, they don't account for token redundancy,which restricts the number of retrieved examples.3 Pilot Study</p>
<p>Table 1 :
1
The selection of CoT examples heavily impacts LLM reasoning performance.
ModelManual 8 ShotsMethod (16 Shots)Random 1 Random 2LLaMA2-7B13.7912.3613.27LLaMA2-13B27.8223.0423.28</p>
<p>CoT-Influx Coarse-to-fine Pruner
CoT ExamplesGPT-4 EvolutionMRD 3Difficulty Filter𝐻 𝑠 shot [𝑘, 𝑁, 𝐷 𝐵𝐸𝑅𝑇 ]𝐻 𝑠 token [𝑘′, 𝑁, 𝐷 𝐵𝐸𝑅𝑇 ]Example Prompt Candidates question CoT answer question CoT answer ……. difficulty difficultyQuestion Retrieve 𝑘 shotsEmbedding Model (BERT, Frozen)[𝑘, 𝑁] Shot pruner 𝑠 shot𝑎 shot [𝑘, 2][𝑘′, 𝑁] Token pruner 𝑠 token𝑎 token [𝑘′  *  𝑁, 2]LLM (Frozen)REINFORCEReward……Q: Heloise has dogs and cats in the ratio of 10:17, with the total number of petsQ: Jason has six fish in his aquarium. He realizes that every day the number of fish doubles.being 189. If she gives 10 dogs to her friend Janet, how many dogs does sheOn the third day he takes out one-third of the fish. On the fifth day he takes out one-fourth ofremain with altogether? A: Let's think step by step. Step 1: Find the total ratio. The total ratio of dogs and cats is 10 + 17 = 27. Step 2: Find the fraction representing the number ofShot prunerthe fish. On the seventh day he adds 15 more fish. How many fish in total does he have? A: Let's think step by step. Day 1: Jason has 6 fish. Day 2: Jason has 6 x 2 = 12 fish . Day 3: Jason has 12 x 2 = 24 fish . Day 4: Jason has 24 x 2 = 48 fish . Day 5: Jason has 48 x 2 = 96dogs. The fraction representing dogs is 10/27. Step 3: Find the total number offish (he takes out 1/4 of the fish from the original 6, which is 1.5 fish, rounded up to 2 fish)dogs 10/27 * 189 = 70. Step 4: Find the remaining number of dogs after givingDay 6: Jason has 96 x 2 = 192 fish . Day 7: Jason has 192 + 15 = 207 fish . Therefore, the10 to Janet 70 -10 = 60. Therefore, the answer is 60.answer is 207.……Q: Jason has six fish in his aquarium. He realizes that every day the number offish doubles. On the third day he takes out one-third of the fish. On the fifth day he takes out one-fourth of the fish. On the seventh day he adds 15 moreToken prunerfish. How many fish in total does he have? A: Let's think step by step. Day 1: Jason has 6 fish. Day 2: Jason has 6 x 2 = 12 fish . Day 3: Jason has 12 x 2 = 24 fish . Day 4: Jason has 24 x 2 = 48 fish . Day 5: Jason has 48 x 2 = 96 fish (he takes out 1/4 of the fish from the original 6, which is 1.5 fish, rounded up to 2 fish)Day 6: Jason has 96 x 2 = 192 fish .… Q: Jason has six fish in. He every day the number of fish doubles. third day takes out one-third fish. On fifth day out one-fourth fish. On the seventh day adds 15. How many fish total have? A: Let's think step by step. Day 1: 6 fish. Day 2: Jason 6 x 2 = 12. 3: 12 x 2 = 24 fish . Day 4:Day 7: Jason has 192 + 15 = 207 fish . Therefore, the answer is 207.…
48. Day 5: Jason 48 x 2 = 96 (out 1/4 of the fish from 6 1.5 fish, rounded 2) Day 6: Jason has 96 x 2 = 192 fish.Day 7: Jason has 192 + 15 = 207 fish.Therefore, the answer is 207.…</p>
<p>Table 2 :
2
Comparison of EM (%) accuracy on GSM8K with state-of-the-art baselines.Note that the 20 CoT shots of retrieval baselines are the max number, given that the context window limit of LLaMA2 is 4096 tokens.
Method#Input CoT shots #Average tokens LLaMA2-7B LLaMA2-13B LLaMA2-70BZero-shot0-4.255.8411.45Zero-shot-CoT (Kojima et al., 2022)0-1.7412.2821.91Few-shot-CoT (Wei et al., 2022)865513.7927.8255.42Random retrieval203379.812.5122.2153.07TopK retrieval (Liu et al., 2021)203535.414.5623.6554.59BM25 retrieval (Zhenyu et al., 2023)203816.113.4225.1754.21TopK+GPT4 Compression401376.07.0811.0125.17TopK+Selective Context (Li et al., 2023b)402262.40.450.762.50TopK+LLMLingua (Jiang et al., 2023)402048.05.388.3422.74CoT-Influx482037.015.9232.3759.59</p>
<p>Table 3 :
3
Comparison of EM (%) accuracy on Addsub, Multiarith, Svamp, and Singleeq math reasoning dataset
ModelMethodAddSub Multiarith Svamp Singleeq Avg.Zero-shot58.735.5032.2 62.79 39.81Few-shot-CoT56.9643.6738.1 66.54 51.32LLaMA2-7B TopK retrieval46.0834.5038.1 46.46 41.29TopK+LLMLingua 12.9110.5019.5 19.49 15.60CoT-Influx62.2847.0040.2 72.05 55.38Zero-shot70.136.5043.8 71.07 47.88Few-shot-CoT65.8272.8342.7 77.36 64.68LLaMA2-13B TopK retrieval60.7657.0050.2 68.50 59.12TopK+LLMLingua 22.2822.3327.5 25.20 24.33CoT-Influx69.6273.8750.5 83.07 69.26</p>
<p>Table 4 :
4
Comparison of EM (%) accuracy on GSM8K with larger LLMs under the few-shot-CoT setting.
ModelParameters EM (%)Finetuned GPT-3 (Wei et al., 2022)175B34.0Chinchilla (Hoffmann et al., 2022)70B43.7Text-davinci-002 (Kojima et al., 2022)175B51.5PaLM (Chowdhery et al., 2022)540B56.5GPT-3.5 (OpenAI, 2023a)-57.1Minerva (Lewkowycz et al., 2022)540B58.8LLaMA2-70B+CoT-Influx70B59.6forms all baselines, with 2.13% to 4.55% absoluteimprovements. (2) Despite using fewer input to-kens, CoT-Influx consistently outperforms retrievalbaselines by 1.36% to 14.09% absolute improve-ments. This is because our compressed tokens in-dicate more informational CoT examples withoutredundancy. In contrast, they select entire exam-ples, which may contain redundant tokens, basedon semantic similarity between the target questionand CoT examples, without considering the differ-ent CoT preference of the target LLM. (3) CoT-Influx significantly outperforms prompt compres-sion baselines in preserving the most crucial tokensfor math reasoning, while methods like SelectiveContext and LLMLingua suffer accuracy declinesdue to difficulties in maintaining few-shot promptstructure. GPT-4 tends to prune essential reasoningsteps, which negatively impacts CoT effectiveness.We further demonstrate the effectiveness of CoT-Influx by comparing LLaMA2-70B with largersize LLMs on GSM8K. As shown in Table 4,CoT-Influx significantly boosts LLM reasoning ca-pabilities. Remarkably, without any fine-tuning,LLaMA2-70B with CoT-Influx outperform muchlarger LLMs. LLaMA2-70B surpasses GPT-3.5with an absolute improvement of 2.5%.Compatible with existing reasoning prompts.As a method to improve LLM reasoning capabil-ity, CoT-Influx is complementary with other ad-</p>
<p>Table 5 :
5
CoT-Influx is compatible with advanced prompt techniques like self-consistency (i.e., maj@20) and selfverification (i.e., verify@20).
MethodLLaMA2-13B LLaMA2-70BCoT-Influx32.3759.59CoT-Influx+maj@2033.4360.73CoT-Influx+verify@2034.0461.79Table 6: Comparison of EM(%) on GSM8K using CoT-Influx pruner across different CoT datasets.CoT datasetLLaMA2-7B LLaMA2-13B LLaMA2-70BMRD 315.9232.3759.59MRD 3 w/o evolution14.9430.5557.70GSM8K training set14.1829.6456.71</p>
<p>Table 6
6, both GPT-4generated and evolved CoT examples are vital forimproving the reasoning performance.Ablation study on coarse-to-fine pruner. Ourpruner operates at both shot and token levels tofully exploit redundancy within CoT examples. Toverify the effectiveness, we conduct experimentswith only shot or token pruner under the same set-ting. As shown in Table 7, removing any pruningstage decreases performance. Notably, removingtoken-only pruning causes a larger accuracy dropthan shot-only pruning, indicating that shot-levelredundancy is easier for the pruner to learn.</p>
<p>Table 7 :
7
Comparison of EM(%) on GSM8K with different pruning strategies.
Pruning Strategy7BLLaMA2 13B70BCoT-Influx (Prune shot and token) 15.92 32.37 59.59Prune shot only15.69 31.08 57.77Prune token only12.05 25.32 49.36</p>
<p>Table 8 :
8
The total inference costs on GSM8K.
Method#Input-shot #Token Time GPU MemoryLLaMA2-7B122108.6 2.99h19.7GBSelective Context402262.4 4.38h23.5GBLLMLingua402048.0 3.65h33.0GBCoT-Influx402037.0 3.04h21.1GB</p>
<p>Table 8
8shows the total inference latency and GPU memoryrequired to run LLaMA2-7B with different meth-ods on GSM8K, measured on a single NVIDIAA100 GPU. The results reveal that CoT-Influx in-troduces a negligible 1.4GB additional memoryand a 1.7% increase in latency. This is more effec-tive than prompt compression baselines, such asSelective Context and LLMLingua, which requiresignificantly higher latency and more GPU mem-ory, potentially hindering efficient deployment.Implications. Our analysis of retained CoT exam-ples and tokens yields the following insights: (1)More capable LLMs favor harder CoT examples,while smaller LLMs opt for simpler ones. (2) Nu-merical and format tokens are essential for mathreasoning. Function words like with, the, then, andirrelevant background context such as theater canbe pruned without affecting reasoning capability.6 ConclusionWe present CoT-Influx, a plug-and-play modulethat improves LLM math reasoning by pruningunnecessary few-shot examples at shot and tokenlevels for a more effective input context. To trainthe module, we use reinforcement learning to opti-mize a math reasoning-specific reward with GPT-4 evolved CoT dataset MRD 3 . Extensive exper-iments on various datasets and LLMs comparedwith state-of-the-art baselines demonstrate the ef-fectiveness of our method. This paper highlights</p>
<p>Table 9 :
9
Comparsion of EM(%) on GSM8K of LLaMA2-7B with CoT-Influx and LLaMA2-7B-32K with PI.
Number of input shots12162024283240Average number of tokens 2108.6 2820.6 3535.4 4217.2 4929.1 5641.2 7070.8LLaMA2-7B13.8715.0814.02----LLaMA2-7B+CoT-Influx---14.3315.0915.9215.77LLaMA2-7B-32K11.3712.8111.3711.8311.8311.5211.30</p>
<p>Table 10 :
10
The EM (%) accuracy on GSM8K with CoT-Influx and other baselines.Note that the context window limit of LLaMA2-13B-Chat and GPT-3.5-Turbo are all 4096 tokens.
Method#Input CoT shots #Average tokens LLaMA2-13B-Chat GPT-3.5-TurboFew-shot-CoT (Fu et al., 2023)865527.8272.55TopK retrieval (Liu et al., 2021)203535.431.1670.74TopK+LLMLingua (Jiang et al., 2023)402048.010.6949.96CoT-Influx482037.040.9473.31</p>
<p>Table 11 :
11
The EM (%) accuracy on GSM8K of LLaMA2-7B and LLaMA2-13B with different reward function.
Reward Function LLaMA-2-7B LLaMA-2-13BFull Reward15.9232.37w/o R Acc15.2431.46w/o R Loss14.7831.16w/o R Limit14.2529.72</p>
<p>Table 12 :
12
Sensitivity analysis on token target T and token penalty co-efficient w At the arcade Dave won 11 tickets.If he spent 5 tickets on a beanie and later won 10 more tickets, how many would he have?A: Dave initially had 11 tickets, spent 5 on a beanie, and later won 10 more, resulting in a total of: 11 -5 + 10 = 16 tickets.The answer is 16.At the arcade, Dave triumphed in a game and earned 11 tickets.He redeemed 5 of those tickets for a beanie and subsequently won an additional 10 tickets.What is his final ticket count?A: Dave started with 11 tickets, used 5 to acquire a beanie, and later added 10 more to his collection, resulting in: 11 -5 + 10 = 16 tickets.The answer is 16.Evolution result of increase_reasoning: Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.Afterward, he won 10 more tickets.Calculate his final ticket count by first finding the remaining tickets after his purchase and then adding the newly won tickets.A: Dave had 11 tickets, spent 5, leaving him with 6.Then he won 10 more, resulting in: 6 + 10 = 16 tickets.The answer is 16.Evolution result of revise_difficulty: Q: Dave got 11 tickets at the arcade.He used 5 tickets to get a beanie and then won 10 more tickets.How many tickets does he have now?A: Dave started with 11 tickets, used 5 for a beanie, and won 10 more, giving him: 11 -5 + 10 = 16 tickets.The answer is 16.Evolution result of produce_easier: Q: In a game, Sarah scored 11 points.She lost 5 points in a penalty and then scored 10 more points.What is her final score?A: Sarah initially scored 11 points, lost 5 due to a penalty, and scored 10 more, resulting in a total of: 11 -5 + 10 = 16 points.The answer is 16.Return full dataset with evolution D, questions set Dquestion, prompt candidate set Dcot
Token target T LLaMA-2-13BToken penalty co-efficient w LLaMA-2-13B204832.37(-1,1)32.37102429.57(-0.5,1)31.69307232.37(-1,0.5)32.22</p>
<p>Table 14 :
14
Detailed hyper-parameters for pruner training scheme of different LLMs.
ModelLLaMA2-7BLLaMA2-13BLLaMA2-70BEpoch333Batch Size111Pruner LLM BaseLLaMA2-13BLLaMA2-13BLLaMA2-70BInput Shot404848Input Shot (TopK)323232Input Shot (Few-shot)81616OptimizerAdamWAdamWAdamWWeight Decay1e −21e −21e −2Learning Rate1e −51e −51e −5Embedding ExtractorBERT-Large (cased) BERT-Large (cased) BERT-Large (cased)Embedding Size102410241024Tokenizer Padding512512512Difficulty Threshold d thr222Token Target T204820482048Token Penalty Coefficient w(-1,1)(-1,1)(-1,1)Selection Repeat t repeat10105Evol Subsetadd_constraintsincrease_reasoningincrease_reasoningtemperature0.80.80.8top_p0.950.950.95top_k404040num_beams111max_new_tokens111</p>
<p>Table 15 :
15
Comparison of the advantage of different prompting strategies.MethodsFrozen LLMs Automated Gradient-free Unlimited-token Transferable Interpretable Difficulty-aware Dynamic #Shots
Fine-TuningManual PromptSoft Prompt TuningPrompt RetrievalAutoPrompt
A.5 Case Study on different prompt compression methodsTo show how different prompt compression methods prune input few-shot prompts in different manners, we given an example of a 8-shot prompt selected using TopK retriever.The original full few-shot prompts are listed in the following box:Original full few-shot prompt for math reasoning (1331 tokens):Q: Dave won 11 tickets at the arcade and spent 5 on a beanie.Afterward, he won 10 more tickets.Calculate his final ticket count by first finding the remaining tickets after his purchase and then adding the newly won tickets.A: Let's think step by step.Dave had 11 tickets, spent 5, leaving him with 6.Then he won 10 more, resulting in: 6 + 10 = 16 tickets.The answer is 16.Q: At the carnival, tickets for rides cost 0.75 dollars each, or you can buy a 15-dollar armband for unlimited rides for one night.To determine the number of rides where the armband's cost equals that of individual tickets, set up and solve an equation involving x, the number of rides.A: Let's think step by step.Let x be the number of rides.Equate the cost of x rides using individual tickets, 0.75x dollars, to the 15-dollar armband cost: 0.75x = 15.Solve for x: x = 15/0.75,which gives x = 20.The answer is 20.Q: Mitch, Jam, and Jay went out for a movie.Mitch paid $7 per ticket for 3 friends, Jam purchased 2 popcorn boxes at $1.5 each, and Jay got 3 milk teas for $3 each.To equitably split the expenses, how much should each of them contribute?A: Let's think step by step.The total cost of 3 tickets at $7 each, 2 popcorn boxes at $1.5 each, and 3 milk teas at $3 each is $21 + $3 + $9 = $33.Dividing the overall expenses among 3 friends results in a contribution of $33/3 = $11 per person.The answer is $11. Q: Connor is taking his date to the movies, with tickets costing $10.00 each.They opt for the large popcorn &amp; 2 drink combo meal at $11.00, and each choose a box of candy at $2.50 per box.Determine the combined expenses for the movie tickets, combo meal, and candy to find the total amount Connor will spend on his date.A: Let's think step by step.Calculate the cost of two movie tickets (2 x $10.00 = $20.00), the combo meal ($11.00), and two boxes of candy (2 x $2.50 = $5.00), then sum them up ($20.00 + $11.00 + $5.00 = $36.00).The answer is $36.00.Q: Scott has 4 tickets.Ernest starts with 9 tickets and later discovers a stash of 72 more.Calculate the final number of tickets Ernest possesses.A: Let's think step by step.Ernest initially holds 9 tickets and acquires 72 additional ones, leading to a total of 9 + 72 = 81 tickets.The answer is 81.Q: Joseph and his friends watched two movies at his place.The first movie lasts 1 hour and 30 minutes, and the second is 30 minutes longer.They took 10 minutes for popcorn and double that for fries.Determine, in hours, the cumulative time spent cooking and watching movies by breaking down each component of time spent.A: Let's think step by step.First, find the second movie's length (1 hour and 30 minutes + 30 minutes = 2 hours).Then, sum both movies' lengths (1 hour and 30 minutes + 2 hours = 3 hours and 30 minutes).Next, calculate cooking time (10 minutes for popcorn + 20 minutes for fries = 30 minutes).Lastly, add movie and cooking times (3 hours and 30 minutes + 30 minutes = 4 hours).The answer is 4 hours.Q: The movie theater sold a number of tickets to the horror and romance movies.The horror movie ticket sales were 18 more than three times the romance movie ticket sales.If there were 25 romance movie tickets sold, how many tickets were sold for the horror movie, considering the given relationship?A: Let's think step by step.Let "h" represent the horror movie tickets sold.Given that h = 3(25) + 18, we can simplify the equation: h = 75 + 18, resulting in h = 93.The answer is 93.Q: On Saturday, Sara purchased 2 movie theater tickets at $10.62 each, rented a movie for $1.59, and bought another movie for $13.95.Determine Sara's total expenditure on movies by performing a step-by-step calculation.A: Let's think step by step.Firstly, calculate the movie tickets' cost by multiplying the ticket price ($10.62)by the quantity (2), resulting in $21.24.Secondly, combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54.Lastly, sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78.The answer is $36.78.Most of the examples above have similar background and target (tickets, movie, expense, etc.) but the difficulty and number of reasoning steps are various.In addition, the solution of most questions are highly redundant.When performing math reasoning with, it is important to select the most suitable and concise examples considering the characteristic of the current input question.In our evaluation across different methods shown in Sec.5.1, we have empirically observe the previous methods fail to retain the structural integrity of prompt.We show the pruned prompt with different methods and similar token length in the following box.We can see that Selective Context and LLMLingua frequently discard the important part including 'Q:', 'A:', '\n', "Let's think step by step", and "The answer is" in these examples.Although GPT-4 can retain majority of these tokens, the reasoning steps are systematically removed because GPT-4 cannot be instructed to utilize the redundancy in both example-level and token-level.Our proposed CoT-Influx, however, select the most representative examples and only remove the redundant function words.: Dave won11ets the and5 a be.After he.his final count by first theets after the: Lets think.Daveets5" in.: the,ets 5, or a-ollarides for one.To theidesband cost equals of, equation involving r.A: think.Let.ides using individualets, the1ollar cost5 which.:, Jam and Jay a7 ticket3 Jam2orn5 Jay3 milk.To equ the.ets boxes53 milk each1.the overallenses3 friends a.The : Connor is his,..They theorn &amp; drinkbo and0.theandy think.ofets0 theboal and two then :.Ernest and later a7.think.Ernest initially and, 9: friends at movie the minutes They and for.the spent by think, the, calculate The a the and ticket, think.:,bought.by-stepcalculation.A: Let's think step by step.Firstly, calculate the movie tickets' cost by multiplying the ticket price ($10.62)by the quantity (2), resulting in $21.24.Secondly, combine the rental ($1.59) and purchase ($13.95) costs, equaling $15.54.Lastly, sum the ticket cost and rental/purchase cost: $21.24 + $15.54 = $36.78.The answer is $36.78.GPT-4 Compression:Q: Dave won 11, spent 5 and won 10 more.Determine final count.A: The answer is 16.Q: Tickets cost 0.75 per ride, armband cost 15.Determine rides that armband's cost equals tickets.A: The answer is 20.Q: $7 per ticket for 3, 2 popcorn boxes at $1.5, 3 milk teas for $3.Determine each contribute.A: The answer is $11. Q: Tickets cost $10.00 each, meal cost $11.00, a box of candy at $2.50.Determine the expenses.A: The answer is $36.00.Q: Scott has 4. Ernest starts with 9 and discovers 72 more.Determine the final number.A: The answer is 81.Q: The first 1.5 hour, the second is 30 minutes longer.10 minutes for popcorn.Determine the time.A: The answer is 4 hours.Q: Horror movie were 18 more than 3 times romance.25 romance movie sold, Determine number of horror movie.A: The answer is 93.Q: Sara purchased 2 at $10.62 each, a movie for $1.59, and another $13.95.Determine total expenditure.A: The answer is $36.78.CoT-Influx: Q: Mitch, Jam, and went out a. Mitch paid $7 per ticket for 3, Jam purchased 2 boxes at $1.5 each, and got 3 for $3 each.To equitably split, how much should each them contribute?A: Let's think step by step.The total cost 3 tickets $7 each, 2 popcorn boxes $1.5 each, and 3 milk $3 each is $21 + $3 + $9 = $33.Dividing the overall expenses among 3 results of $33/3 = $11 per.The answer is $11. Q: The theater sold number tickets to horror and romance movies.The horror movie ticket sales were 18 more than three times romance ticket.If there 25 romance sold, how many tickets were sold horror movie, considering?A: Let's think step by step.Let "h" represent horror tickets sold.Given h = 3(25) + 18, we can simplify equation: h = 75 + 18, resulting h = .The answer is 93.Q: On, Sara purchased 2 theater tickets $10.62 each, rented movie $1.59, and bought movie $13.95.Determine Sara's total expenditure movies performing a calculation.A: Let's think step by step., calculate tickets' cost price ($10.62)by quantity (2), resulting $21.24.Secondly, combine rental ($1.59) purchase ($13.95),equaling.Lastly, sum ticket rental/purchase: $21.24 + $15.54.The answer is $36.78.B Evolution of MRD 3B.1 Prompt template for evolutionThe prompt we used for evolution of the examples in our dataset are listed as follow:Prompt for different evolution strategies I want you to act as a Prompt Rewriter.Your objective is to rewrite a given prompt into a more complex version to make those famous AI systems (e.g., LLaMA, ChatGPT and GPT4) a bit harder to handle.The prompt is made up of a math reasoning question and the corresponding answer.The rewritten prompt must be reasonable and must be understood and responded by humans.Your rewriting cannot omit or change the input and results in #Given Prompt#.Also, please retain the format of 'Question: ' and 'Answer: ' in your response.You SHOULD complicate the given prompt using the following method: {Evolution template} You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.The #Rewritten Prompt# should also follow the format that the rewritten question appears after 'Question: ' and the rewritten answer appears after 'Answer: '.The rewritten answer should end up with 'The answer is [results]'.#Given Prompt#: Question: {Given question} Answer: {Given answer} #Rewritten Prompt#:Evolution template for evolution strategy add_constraints: Please add one more constraint/requirement to the question of #Given Prompt# Evolution template for evolution strategy deepening: Please increase the depth and breadth of the question and answer of #Given Prompt# Evolution template for evolution strategy increase_reasoning: If #Given Prompt# can be solved with just a few simple thinking processes, please rewrite it to explicitly request multiple-step reasoning.Evolution template for evolution strategy revise_difficulty: Please revise the high difficulty questions to lower difficulty.Evolution template for evolution strategy produce_easier: Please produce a new and easier question with another different topic.Most part of the prompt of different evolution strategies are similar.Based on our quantitatively analysis on the difficulty and reasoning step distribution, GPT-4 can effectively follow our instruction to modify the constraints or difficulty level of input questions.B.2 Difficulty and Reasoning Steps Distribution of MRD 3Based on the GPT-4-based estimation, we are able to quantitatively look into the distribution of difficulty and reasoning step distribution in MRD 3 without evolution and MRD 3 with various evolution schemes.The results are shown in Figure6.The original distribution of both difficulty level and reasoning steps of questions centralized between 2 to 4.More questions with higher difficulty using add_constraints, deepening, and increase_reasoning.As we discuss in the reward design of our RL pruner, easy questions are important for the stabilization of RL and can help effectively identify the quality of pruned prompt, more easier questions are generated with revise_difficulty and produce_easier evolution scheme.B.3 Additional observation on difficulty distributionAs shown in Figure6In our further exploration of the optimal selection of CoT examples for improve mathematical reasoning, we observe that LLMs of different capabilities exhibit preferences for CoT examples of varying difficulty levels.As Table13shows, we categorize each CoT example in the MRD 3 -Evol dataset by difficulty level.We then select the top 16 CoT examples from different groups as few-shot examples for LLaMA2 models.Results show LLaMA2-7b prefers CoT examples with a difficulty level of 3-4, while LLaMA2-13b, more capable, prefers those with a difficulty level of 4 or above.This aligns with intuition: for instance, when assisting a middle school student with math problems, it is more beneficial to provide examples of moderate difficulty that they can comprehend, whereas for a high school student, examples with a higher level of difficulty are more useful.In our evaluation of CoT-Influx with various LLMs, we found that the shot selection results are consistent with our observation.The average difficulty score and number of reasoning steps for the examples selected by LLaMA2-70B pruner are 3.57 and 3.04, which are higher than the results of LLaMA2-13B are 3.51 and 2.98.The empirical results further support our assumption that LLMs with larger size prefers harder examples than smaller-scale LLMs.B.4 Evolution exampleWe give an example of a math questions and then show the evolved results of the questions and answers.The evolved results follow our instruction given in Sec.B.1.As can be seen from the evolution results, the answer of input questions can be modified (e.g.ground truth answer change from 16 to 12 in add_constraints).The whole background of the questions can also be replaced (e.g.computation target of question change from current tickets at the arcade to final points of a game in produce_easier).These modification and variation improve the diversity of our prompt candidate sets, which are the building block for the training and reasoning with CoT-Influx.
Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford202033Ilya Sutskever, and Dario Amodei</p>
<p>Extending context window of large language models via positional interpolation. Shouyuan Chen, Sherman Wong, Liangjian Chen, Yuandong Tian, arXiv:2306.155952023aarXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, Transactions on Machine Learning Research. 2023b</p>
<p>Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen, arXiv:2305.14788Adapting language models to compress contexts. 2023arXiv preprint</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Findings of the Association for Computational Linguistics: ACL 2023. 2021. 2023arXiv preprintTraining verifiers to solve math word problems</p>
<p>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, Zhiting Hu, arXiv:2205.12548Rlprompt: Optimizing discrete text prompts with reinforcement learning. 2022arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Everything of thoughts: Defying the law of penrose triangle for thought generation. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, Dongmei Zhang, arXiv:2311.042542023arXiv preprint</p>
<p>Chain-of-thought hub: A continuous effort to measure large language models' reasoning performance. Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot, arXiv:2305.173062023arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021. Association for Computational Linguistics2021</p>
<p>. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei, arXiv:2307.069452023arXiv preprintcontext autoencoder for context compression in a large language model</p>
<p>Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Ji Heng, Sinong Wang, arXiv:2308.16137Lm-infinite: Simple on-the-fly length generalization for large language models. 2023arXiv preprint</p>
<p>Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, Furu Wei, arXiv:2212.06713Structured prompting: Scaling in-context learning to 1,000 examples. 2022arXiv preprint</p>
<p>Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.15556Training compute-optimal large language models. 2022arXiv preprint</p>
<p>Selective annotation makes language models better fewshot learners. Jungo Su Hongjin, Chen Henry Kasai, Weijia Wu, Tianlu Shi, Jiayi Wang, Rui Xin, Mari Zhang, Luke Ostendorf, Noah A Zettlemoyer, Smith, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Learning to solve arithmetic word problems with verb categorization. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, Nate Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)2014</p>
<p>Llmlingua: Compressing prompts for accelerated inference of large language models. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Learned token pruning for transformers. Sehoon Kim, Sheng Shen, David Thorsley, Amir Gholami, Woosuk Kwon, Joseph Hassoun, Kurt Keutzer, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22Association for Computing Machinery2022</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Parsing algebraic word problems into equations. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, Siena Dumas, Ang , Transactions of the Association for Computational Linguistics. 32015</p>
<p>MAWPS: A math word problem repository. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Hannaneh Hajishirzi, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2016</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Constraint-aware and ranking-distilled token pruning for efficient transformer inference. Junyan Li, Li Lyna Zhang, Jiahang Xu, Yujing Wang, Shaoguang Yan, Yunqing Xia, Yuqing Yang, Ting Cao, Hao Sun, Weiwei Deng, Qi Zhang, Mao Yang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023a</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin, 2023b</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational Linguistics20171</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, Weizhu Chen, arXiv:2101.06804What makes good in-context examples for gpt-3?. 2021arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Jesse Mu, Xiang , Lisa Li, Noah Goodman, arXiv:2304.08467Learning to compress prompts with gist tokens. 2023arXiv preprint</p>
<p>Gpt-4 technical report. 2023aOpenAIOpenAI. 2023b. Welcome to the openai platform</p>
<p>Are nlp models really able to solve simple math word problems?. Arkil Patel, Satwik Bhattamishra, Navin Goyal, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>Yarn: Efficient context window extension of large language models. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole, arXiv:2309.000712023aarXiv preprint</p>
<p>Yarn: Efficient context window extension of large language models. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole, arXiv:2309.000712023barXiv preprint</p>
<p>Anirudh Dagar, and Wenming Ye. 2023. In-context learning with iterative demonstration selection. Chengwei Qin, Aston Zhang, arXiv:2310.09881arXiv preprint</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Learning to retrieve prompts for in-context learning. Ohad Rubin, Jonathan Herzig, Jonathan Berant, arXiv:2112.086332021arXiv preprint</p>
<p>Reticl: Sequential retrieval of in-context examples with reinforcement learning. Alexander Scarlatos, Andrew Lan, arXiv:2305.145022023arXiv preprint</p>
<p>Autoprompt: Eliciting knowledge from language models with automatically generated prompts. Taylor Shin, Yasaman Razeghi, Robert L Logan, I V , Eric Wallace, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Focused transformer: Contrastive training for context scaling. Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś, 2023</p>
<p>Transformers learn in-context by gradient descent. Johannes Von, Oswald , Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, Max Vladymyrov, International Conference on Machine Learning. PMLR2023</p>
<p>Label words are anchors: An information flow perspective for understanding in-context learning. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun, arXiv:2305.141602023aarXiv preprint</p>
<p>Learning to retrieve in-context examples for large language models. Liang Wang, Nan Yang, Furu Wei, arXiv:2307.071642023barXiv preprint</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, arXiv:2306.07174Augmenting language models with long-term memory. 2023carXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023d</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao. 2023</p>
<p>Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, 1992</p>
<p>Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. David Wingate, Mohammad Shoeybi, Taylor Sorensen, arXiv:2210.031622022arXiv preprint</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, arXiv2023</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang, arXiv:2304.12244Wizardlm: Empowering large language models to follow complex instructions. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>Ground-truth labels matter: A deeper look into input-label demonstrations. Min Kang, Junyeob Yoo, Kim, Joon Hyuhng, Hyunsoo Kim, Hwiyeol Cho, Sang-Woo Jo, Sang-Goo Lee, Taeuk Lee, Kim, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Openicl: An open-source framework for in-context learning. Wu Zhenyu, Wang Yaoxiang, Ye Jiacheng, Feng Jiangtao, Xu Jingjing, Qiao Yu, Wu Zhiyong, arXiv:2303.029132023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
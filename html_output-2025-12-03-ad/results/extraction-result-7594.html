<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7594 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7594</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7594</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-142.html">extraction-schema-142</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-270560416</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.10833v3.pdf" target="_blank">A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7594.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7594.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated statistical model discovery (Li et al., 2024a)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated statistical model discovery with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Describes an approach where LLMs iteratively propose and critique statistical models using in-context examples and internal chain-of-thought style reasoning to discover data-generating models or model families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated statistical model discovery with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative LLM used to produce candidate statistical models and rationales; leveraged in-context examples and chain-of-thought style prompting to propose and refine models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>statistics / quantitative scientific modeling (mathematics)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>in‑context learning + chain‑of‑thought iterative critique</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Provide data/problem context and exemplar model proposals in prompts; the LLM generates candidate statistical models and stepwise rationales (chain‑of‑thought), then iteratively critiques and refines candidates to produce a final model proposal.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td>automated statistical model discovery via iterative proposal-and-critique (rule/model induction from data and descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey notes only a high-level description: details (e.g., models used, dataset sizes, quantitative validation) are not reported in the survey; general limitations for LLM-driven discovery apply (hallucination, domain-tail knowledge loss, OOD generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Integrate formal model-checking or symbolic verification, combine LLM proposals with invariant learning for OOD robustness, and add retrieval/knowledge-graph grounding to reduce hallucination (suggested by the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7594.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7594.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based knowledge synthesis (Zheng et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that LLMs can synthesize knowledge from literature, infer hypotheses from data, and generate interpretable explanations in chemistry and related scientific domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for scientific synthesis, inference and explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>large language model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generic LLMs applied to literature synthesis and data-conditioned inference; methods likely include instruction tuning and retrieval-augmented generation although the survey does not specify exact architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>chemistry / general scientific literature</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>knowledge synthesis / literature‑to‑insight prompting</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Feed collections of scholarly texts (paper summaries, experimental results) to an LLM (optionally with retrieval) and prompt it to synthesize overarching findings, infer patterns, and produce interpretable explanations or candidate rules.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td>literature synthesis + inference (summarization-driven rule/hypothesis formulation); effectively a form of pattern induction from aggregated textual evidence</td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey reports this as a demonstrated capability but does not provide detailed validation; general concerns include hallucination, limited citation grounding, and the need for retrieval-augmentation and multimodal grounding for trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Develop cross-modal RAG to ground literature synthesis in experimental data/chemical structures, establish rigorous evaluation protocols (expert assessment, reproducibility checks), and build fine-grained theme-focused corpora to avoid tail-knowledge erosion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7594.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7594.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using large language models to distill qualitative scientific laws, principles, or rules from collections of scholarly papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-driven literature-to-hypothesis distillation (survey category)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based distillation of hypotheses/rules from collections of scholarly papers (surveyed capability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This survey highlights multiple works where LLMs are used to generate hypotheses, synthesize knowledge, and propose scientific directions by aggregating and reasoning over prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (e.g., GPT-4 and domain-tuned LLMs) as reported across cited works</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Range from general-purpose decoder-only LLMs (GPT-family) to domain-adapted LLaMA variants and instruction-tuned models; used with instruction tuning, retrieval augmentation, and sometimes multimodal encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>varies (from moderate to billion-parameter models)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_domain</strong></td>
                            <td>general science, chemistry, medicine, and other domains where literature collections are available</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>instruction tuning + retrieval‑augmented prompting / iterative idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Curate downstream-task datasets or retrieval indices; prompt LLMs with retrieved literature context and explicit instructions to produce hypotheses, distilled rules, or research directions; iterations may include critique/refinement or human-in-the-loop filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>law_extraction_approach</strong></td>
                            <td>knowledge synthesis and heuristic rule induction via summarization and iterative prompting (often combined with retrieval-augmented generation and human filtering)</td>
                        </tr>
                        <tr>
                            <td><strong>extracted_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>typically qualitative: expert assessment, downstream task utility (hypothesis usefulness), or case studies; the survey cites examples but does not report uniform metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey emphasizes common limitations: hallucination, lack of grounding in experimental data, vulnerability to dominant signals drowning tail knowledge, and out-of-distribution generalization challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>future_directions</strong></td>
                            <td>Curate fine-grained theme-focused corpora and knowledge graphs, integrate invariant learning for OOD robustness, and develop cross-modal RAG methods to ground distilled rules in experimental and structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated statistical model discovery with language models <em>(Rating: 2)</em></li>
                <li>Large language models for scientific synthesis, inference and explanation <em>(Rating: 2)</em></li>
                <li>Can LLMs generate novel research ideas? a large-scale human study with 100+ nlp researchers <em>(Rating: 1)</em></li>
                <li>A search engine for discovery of scientific challenges and directions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7594",
    "paper_id": "paper-270560416",
    "extraction_schema_id": "extraction-schema-142",
    "extracted_data": [
        {
            "name_short": "Automated statistical model discovery (Li et al., 2024a)",
            "name_full": "Automated statistical model discovery with language models",
            "brief_description": "Describes an approach where LLMs iteratively propose and critique statistical models using in-context examples and internal chain-of-thought style reasoning to discover data-generating models or model families.",
            "citation_title": "Automated statistical model discovery with language models",
            "mention_or_use": "mention",
            "model_name": "large language model (unspecified)",
            "model_description": "Generative LLM used to produce candidate statistical models and rationales; leveraged in-context examples and chain-of-thought style prompting to propose and refine models.",
            "model_size": null,
            "corpus_domain": "statistics / quantitative scientific modeling (mathematics)",
            "corpus_size": null,
            "method_name": "in‑context learning + chain‑of‑thought iterative critique",
            "method_description": "Provide data/problem context and exemplar model proposals in prompts; the LLM generates candidate statistical models and stepwise rationales (chain‑of‑thought), then iteratively critiques and refines candidates to produce a final model proposal.",
            "law_extraction_approach": "automated statistical model discovery via iterative proposal-and-critique (rule/model induction from data and descriptions)",
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": null,
            "evaluation_result": null,
            "baseline_comparison": null,
            "limitations": "Survey notes only a high-level description: details (e.g., models used, dataset sizes, quantitative validation) are not reported in the survey; general limitations for LLM-driven discovery apply (hallucination, domain-tail knowledge loss, OOD generalization).",
            "future_directions": "Integrate formal model-checking or symbolic verification, combine LLM proposals with invariant learning for OOD robustness, and add retrieval/knowledge-graph grounding to reduce hallucination (suggested by the survey).",
            "uuid": "e7594.0",
            "source_info": {
                "paper_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-based knowledge synthesis (Zheng et al., 2023)",
            "name_full": "Large language models for scientific synthesis, inference and explanation",
            "brief_description": "Demonstrates that LLMs can synthesize knowledge from literature, infer hypotheses from data, and generate interpretable explanations in chemistry and related scientific domains.",
            "citation_title": "Large language models for scientific synthesis, inference and explanation",
            "mention_or_use": "mention",
            "model_name": "large language model (unspecified)",
            "model_description": "Generic LLMs applied to literature synthesis and data-conditioned inference; methods likely include instruction tuning and retrieval-augmented generation although the survey does not specify exact architecture.",
            "model_size": null,
            "corpus_domain": "chemistry / general scientific literature",
            "corpus_size": null,
            "method_name": "knowledge synthesis / literature‑to‑insight prompting",
            "method_description": "Feed collections of scholarly texts (paper summaries, experimental results) to an LLM (optionally with retrieval) and prompt it to synthesize overarching findings, infer patterns, and produce interpretable explanations or candidate rules.",
            "law_extraction_approach": "literature synthesis + inference (summarization-driven rule/hypothesis formulation); effectively a form of pattern induction from aggregated textual evidence",
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": null,
            "evaluation_result": null,
            "baseline_comparison": null,
            "limitations": "Survey reports this as a demonstrated capability but does not provide detailed validation; general concerns include hallucination, limited citation grounding, and the need for retrieval-augmentation and multimodal grounding for trustworthiness.",
            "future_directions": "Develop cross-modal RAG to ground literature synthesis in experimental data/chemical structures, establish rigorous evaluation protocols (expert assessment, reproducibility checks), and build fine-grained theme-focused corpora to avoid tail-knowledge erosion.",
            "uuid": "e7594.1",
            "source_info": {
                "paper_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM-driven literature-to-hypothesis distillation (survey category)",
            "name_full": "LLM-based distillation of hypotheses/rules from collections of scholarly papers (surveyed capability)",
            "brief_description": "This survey highlights multiple works where LLMs are used to generate hypotheses, synthesize knowledge, and propose scientific directions by aggregating and reasoning over prior literature.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "various LLMs (e.g., GPT-4 and domain-tuned LLMs) as reported across cited works",
            "model_description": "Range from general-purpose decoder-only LLMs (GPT-family) to domain-adapted LLaMA variants and instruction-tuned models; used with instruction tuning, retrieval augmentation, and sometimes multimodal encoders.",
            "model_size": "varies (from moderate to billion-parameter models)",
            "corpus_domain": "general science, chemistry, medicine, and other domains where literature collections are available",
            "corpus_size": null,
            "method_name": "instruction tuning + retrieval‑augmented prompting / iterative idea generation",
            "method_description": "Curate downstream-task datasets or retrieval indices; prompt LLMs with retrieved literature context and explicit instructions to produce hypotheses, distilled rules, or research directions; iterations may include critique/refinement or human-in-the-loop filtering.",
            "law_extraction_approach": "knowledge synthesis and heuristic rule induction via summarization and iterative prompting (often combined with retrieval-augmented generation and human filtering)",
            "extracted_law": null,
            "law_type": null,
            "evaluation_method": "typically qualitative: expert assessment, downstream task utility (hypothesis usefulness), or case studies; the survey cites examples but does not report uniform metrics.",
            "evaluation_result": null,
            "baseline_comparison": false,
            "limitations": "Survey emphasizes common limitations: hallucination, lack of grounding in experimental data, vulnerability to dominant signals drowning tail knowledge, and out-of-distribution generalization challenges.",
            "future_directions": "Curate fine-grained theme-focused corpora and knowledge graphs, integrate invariant learning for OOD robustness, and develop cross-modal RAG methods to ground distilled rules in experimental and structured data.",
            "uuid": "e7594.2",
            "source_info": {
                "paper_title": "A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated statistical model discovery with language models",
            "rating": 2,
            "sanitized_title": "automated_statistical_model_discovery_with_language_models"
        },
        {
            "paper_title": "Large language models for scientific synthesis, inference and explanation",
            "rating": 2,
            "sanitized_title": "large_language_models_for_scientific_synthesis_inference_and_explanation"
        },
        {
            "paper_title": "Can LLMs generate novel research ideas? a large-scale human study with 100+ nlp researchers",
            "rating": 1,
            "sanitized_title": "can_llms_generate_novel_research_ideas_a_largescale_human_study_with_100_nlp_researchers"
        },
        {
            "paper_title": "A search engine for discovery of scientific challenges and directions",
            "rating": 1,
            "sanitized_title": "a_search_engine_for_discovery_of_scientific_challenges_and_directions"
        }
    ],
    "cost": 0.01810425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery
28 Sep 2024</p>
<p>Yu Zhang 
University of Illinois at Urbana-Champaign ♢ University of California
Los Angeles</p>
<p>University of Washington
Seattle</p>
<p>Xiusi Chen xchen@cs.ucla.edu 
University of Illinois at Urbana-Champaign ♢ University of California
Los Angeles</p>
<p>University of Washington
Seattle</p>
<p>Jin ♣ Bowen bowenj4@illinois.edu 
University of Illinois at Urbana-Champaign ♢ University of California
Los Angeles</p>
<p>University of Washington
Seattle</p>
<p>Sheng Wang swang@cs.washington.edu 
Wei Wang weiwang@cs.ucla.edu 
Texas A&amp;M University</p>
<p>Jiawei Han hanj@illinois.edu 
University of Illinois at Urbana-Champaign ♢ University of California
Los Angeles</p>
<p>University of Washington
Seattle</p>
<p>Nate Gruver 
Anuroop Sriram 
Andrea Madotto 
An- Drew Gordon Wilson 
Lawrence Zitnick 
Zachary 2024 Ulissi 
Yu Gu 
Robert Tinn 
Hao Cheng 
Michael Lucas 
Naoto Usuyama 
Xiaodong Liu 
Tristan Naumann 
Jianfeng Gao 
Hoifung 2021 Poon 
Jiang Guo 
A Santiago Ibanez-Lopez 
Hanyu Gao 
Vic- Tor Quach 
Connor W Coley 
Klavs F Jensen 
Regina Barzilay 
Jennifer Harrow 
Adam Frankish 
Jose M Gonzalez 
Electra Tapanari 
Mark Diekhans 
Dan Hendrycks 
Collin Burns 
Steven Basart 
Andy Zou 
Mantas Mazeika 
Xanh Ho 
Anh Khoa 
Duong Nguyen 
An Tuan Dao 
Jun- Feng Jiang 
Yuki Chida 
Kaito Sugimoto 
Quoc Huy 
Florian To 
Akiko Boudin 
Aizawa 
A Sur- 
Tom Hope 
Aida Amini 
David Wadden 
Madeleine Van Zuylen 
Sravanthi Parasa 
Eric Horvitz 
Daniel S Weld 
Roy Schwartz 
Hannaneh 2021 Hajishirzi 
Kaixuan Huang 
Yuanhao Qu 
Henry Cousins 
William A Johnson 
Di Yin 
Mihir Shah 
Denny Zhou 
Russ Altman 
Mengdi Wang </p>
<p>Felix Kokocinski
Daniel BarrellBronwen L Aken</p>
<p>Amonida Zadissa
2012Stephen Searle</p>
<p>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery
28 Sep 2024A46050F613A1393966C092475AC85344arXiv:2406.10833v3[cs.CL]
In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process.Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality.In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pretraining techniques.To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality.Moreover, we investigate how LLMs have been deployed to benefit scientific discovery.Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.</p>
<p>Introduction</p>
<p>The emergence of large language models (LLMs) (Zhao et al., 2023c) brings a new paradigm to natural language processing (NLP) by replacing specialized models designed for each task with unified models that are reasonably effective for a wide spectrum of problems.In the scientific domain, such a paradigm not only reshapes people's strategies to handle tasks related to natural language (e.g., scientific papers, medical records, and climate reports) but also inspires analogous ideas to deal with other types of data (e.g., molecules, proteins, tables, and metadata).In addition to understanding existing scientific data, LLMs have shown their potential to accelerate scientific discovery (Wang et al., 2023c;Zhang et al., 2023e;Wang et al., 2024d) through generation, planning, etc. * Equal contribution Given the broad and profound impact of LLMs in various scientific fields across diverse modalities, it becomes necessary to comprehensively review related work in this direction.However, existing scientific LLM surveys typically focus on either one or two fields (e.g., biomedicine (Wang et al., 2023a;He et al., 2024b;Pei et al., 2024;Zhang et al., 2024d) and chemistry (Xia et al., 2023;Pei et al., 2024;Zhang et al., 2024d)) or one modality (e.g., text (Ho et al., 2024)) only.In fact, if we take a holistic view of the research landscape, we can observe similar and interrelated techniques used to develop LLMs for different fields and modalities.</p>
<p>Figure 1 depicts three major types of scientific LLM pre-training strategies (i.e., COLUMNS 1 to 3), for each of which we give 4 examples (i.e., TYPES A to D).In COLUMN 1, following BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), existing studies use masked language modeling (MLM) to pre-train encoder language models.Here, the input can be naturally sequential (e.g., papers in each field; protein, DNA, and RNA sequences in the FASTA format (Lipman and Pearson, 1985)) or artificially linearized (e.g., molecules in the SMILES format (Weininger, 1988); sequences of venue, author, and reference nodes in citation graphs).In COLUMN 2, inspired by GPT (Brown et al., 2020) and LLaMA (Touvron et al., 2023a), previous studies adopt next token prediction to pre-train (encoder-)decoder language models, some of which further adopt instruction tuning and preference optimization (Ouyang et al., 2022).Other than plain text input (e.g., questionanswer pairs from knowledge bases or exams), we see more ways to sequentialize complex scientific data, such as flattening table cells and using particle coordinates to describe crystals.Even for images, there are studies in both mathematics (Gao et al., 2023) and biomedicine (Li et al., 2023a) that exploit a vision encoder to project an image onto several visual tokens and prepend them to text tokens as linearized LLM input.In COLUMN 3, following DPR (Karpukhin et al., 2020) and CLIP (Radford et al., 2021), two encoders are pre-trained to map relevant data pairs closer in the latent space via contrastive learning.When both modalities are sequential (e.g., text-text or text-protein), the model is built upon two LLM encoders.When we prefer to keep the non-sequential nature of one modality (e.g., molecular graphs (Edwards et al., 2021), chest X-rays (Zhang et al., 2022), and aerial views (Yan et al., 2024)), the corresponding graph or image encoder can be employed.To summarize, a cross-field, cross-modal survey will more accurately draw the connections between different scientific LLMs, demonstrate their commonalities, and potentially guide their future designs.</p>
<p>Contributions.In this paper, motivated by the discussions above, we systematically survey over 260 scientific LLMs encompassing various fields (e.g., general science, mathematics, physics, chemistry, materials science, biology, medicine, and geoscience), modalities (e.g., language, graph, vision, table, molecule, protein, genome, and climate time series), and sizes (from ∼100M to ∼100B parameters).For each field/modality, we investigate commonly adopted pre-training datasets, model architectures, and evaluation tasks of scientific LLMs.Following our motivation, when we discuss model architectures in detail, we link them back to Figure 1 to build cross-field cross-modal connections.Moreover, we provide a structured summary of these scientific LLMs in Table A1-Table A6 (Appendix A).Furthermore, for different fields, we introduce how LLMs have been deployed to benefit science by augmenting different aspects and stages of the scientific discovery process, such as hypothesis generation, theorem proving, experiment design, drug discovery, and weather forecasting.</p>
<p>2 LLMs in General Science (Table A1)</p>
<p>Language</p>
<p>The most commonly used pre-training corpora for scientific LLMs are research papers from bibliographic databases, such as AMiner (Tang et al., 2008), Microsoft Academic Graph (MAG) (Sinha et al., 2015), and Semantic Scholar (Ammar et al., 2018).Some of these sources (e.g., S2ORC (Lo et al., 2020)) contain full-text information of papers, while the others have titles and abstracts only.</p>
<p>The evolution of scientific LLMs bears similarity to that of general-domain LLMs.Specifically, pioneering models utilize paper text in a self-supervised manner during pre-training, aiming to acquire scientific knowledge from large-scale unlabeled corpora.For example, masked language modeling (MLM) is the default pre-training task for scientific LLMs with a BERT backbone (TYPE 1.A in Figure 1, e.g., SciBERT (Beltagy et al., 2019)); next token prediction is widely used for GPT-based scientific LLMs (TYPE 2.A in Figure 1, e.g., SciGPT (Luu et al., 2021)).More recently, inspired by the fact that LLMs can be trained to follow natural language instructions (Wei et al., 2022a;Ouyang et al., 2022), researchers have put more effort into tuning LLMs with instructions to solve complex scientific problems (TYPE 2.A, e.g., Galactica (Taylor et al., 2022) and SciGLM (Zhang et al., 2024a)).The instruction tuning data are often derived from datasets for downstream tasks, such as exam question answering (Welbl et al., 2017), and further filtered or augmented by humans or existing LLMs (e.g., GPT-4 (Achiam et al., 2023)).</p>
<p>General scientific LLMs are usually evaluated on common NLP tasks, such as named entity recognition (NER), relation extraction (RE) (Luan et al., 2018), question answering (QA) (Wang et al., 2024f), and classification (Cohan et al., 2019).</p>
<p>Language + Graph</p>
<p>Beyond plain text, scientific papers are associated with rich metadata including venues, authors, and references (Zhang et al., 2023g).Such metadata connect papers into a graph that complements text signals for characterizing paper semantics.To exploit metadata, some studies (TYPE 1.B, e.g., OAG-BERT (Liu et al., 2022b)) concatenate paper text with venues/authors as input and perform MLM on both text and metadata; others (TYPE 3.A, e.g., SPECTER (Cohan et al., 2020)) take citation links as supervision and train LLMs to encode linked papers closer in the embedding space.Recent approaches further modify the Transformer architec-ture in LLMs with Adapters (Singh et al., 2023), GNN-nested Transformers (Jin et al., 2023b), and Mixture-of-Experts Transformers (Zhang et al., 2023f) to better capture graph signals.</p>
<p>Graph-aware scientific LLMs are often evaluated on tasks regarding the relation between two text units (e.g., paper-paper or query-paper), including link prediction, retrieval, recommendation, and author name disambiguation.SciDocs (Cohan et al., 2020) and SciRepEval (Singh et al., 2023) are widely adopted benchmark datasets.</p>
<p>Applications in Scientific Discovery</p>
<p>Performant scientific LLMs can work alongside researchers throughout the entire scientific discovery process.Leaving field-specific applications for later sections, here we underscore LLMs' general usefulness in brainstorming and evaluation : Lahav et al. (2022) integrate LLMs into a search engine for the discovery of scientific challenges and directions; Wang et al. (2023e), Yang et al. (2024d), Baek et al. (2024), Gu and Krenn (2024), and Si et al. (2024) leverage LLMs to generate novel scientific ideas, directions, and hypotheses on the basis of prior literature and existing knowledge; Zhang et al. (2023h) rely on LLMs to find expert reviewers for each submission; Liu and Shah (2023), Liang et al. (2024c), andD'Arcy et al. (2024) explore the capacity of GPT-4 to provide useful feedback on research papers to facilitate automatic review generation; Liang et al. (2024b,a) also observe the increasing use of LLMs in writing scientific papers and conference peer reviews.</p>
<p>3 LLMs in Mathematics (Table A2)</p>
<p>Language</p>
<p>The pre-training text corpora for mathematics LLMs can be categorized into two classes: (1) multiple-choice QA, the representative datasets of which include MathQA (Amini et al., 2019), Ape210K (Zhao et al., 2020), and Math23K (Wang et al., 2017); as well as (2) generative QA, the representative datasets of which include GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), and MetaMathQA (Yu et al., 2024c).</p>
<p>Similarly to general science LLMs, the backbone model of pioneering mathematics LLMs is BERT (TYPE 1.A, e.g., GenBERT (Geva et al., 2020) and MathBERT (Shen et al., 2021)), and these models are mostly trained via MLM.For GPT-based mathematics LLMs (TYPE 2.A, e.g., GSM8K-GPT (Cobbe et al., 2021) and NaturalProver (Welleck et al., 2022)), next token prediction and instruc-tion tuning are major pre-training tasks to generate mathematical proofs and reasoning processes.The most recent models (TYPE 2.A, e.g., Rho-Math (Lin et al., 2024b) and MAmmoTH2 (Yue et al., 2024c)) are based on LLaMA and are trained to follow natural language instructions.However, when an enormous pre-training corpus is available (e.g., mathematical web pages and code), next token prediction is still favored as the mere pre-training task (Azerbayev et al., 2024;Lin et al., 2024b) or the companion task (Shao et al., 2024;Ying et al., 2024) to build base models.</p>
<p>QA and math world problems (MWP) have been the most common evaluation tasks for mathematics LLMs.In addition, quantitative reasoning contains more difficult problems, as the model has to provide a complete and self-contained solution without relying on external tools (Shao et al., 2024;Lin et al., 2024b).We see a dominance of use from GSM8K and MATH for QA, and from MathQA and Math23K for MWP.For quantitative reasoning, MMLU-STEM (Hendrycks et al., 2021a) and Big-Bench Hard (Suzgun et al., 2023) are the most widely adopted.</p>
<p>Language + Vision</p>
<p>Geometry is one of the most important branches of mathematics, and it expresses the settings jointly in text and diagrams.As such, it is mandatory to involve the vision modality for geometry LLMs.The most commonly used pre-training datasets for geometry LLMs include Geometry3K (Lu et al., 2021) and GeoQA (Chen et al., 2021), both of which contain multiple-choice geometry problems.</p>
<p>The key to incorporating the vision modality into LLMs is to encode the images and obtain linearized visual representations.Specifically, Inter-GPS (Lu et al., 2021) (TYPE 2.D) uses RetinaNet (Lin et al., 2017) to transform images into a set of relationships and then applies BART (Lewis et al., 2020a) to produce the solution; G-LLaVA (Gao et al., 2023) (TYPE 2.D) encodes visual input via a pre-trained vision Transformer (ViT), concatenates visual embeddings with textual embeddings, and then feeds the concatenation into LLaMA-2 (Touvron et al., 2023b).These models are by default pre-trained via sequence-to-sequence tasks, where the problem is the input, and the ground-truth answer with optional rationale is the output.Auxiliary loss such as masked image modeling, image construction, or text-image matching, is optionally added for better visual modeling.</p>
<p>Geometry LLMs are evaluated through geometry problem solving, where the model is asked to select the correct answer given the diagram and its caption, the question, and answer options.Renowned evaluation datasets include Geometry3K (Lu et al., 2021), GEOS (Seo et al., 2015), and MathVista (Lu et al., 2024).</p>
<p>Table</p>
<p>A large proportion of math knowledge is stored in the form of tabular data.For the "Table" modality, notable resources for pre-training include Wik-iTableQuestions (Pasupat and Liang, 2015), Wik-iSQL (Zhong et al., 2017), and WDC Web Table (Lehmberg et al., 2016).</p>
<p>The challenge in tables is similar to that in diagrams, namely to obtain linearized table representations.In most cases, tables are squeezed into linear text sequences as part of the context and are prepended with the question text as the model input.As one of the first works in this line of research, TAPAS (Herzig et al., 2020) (TYPE 1.A) adopts the MLM objective to predict the masked token in both textual and tabular contexts.Recent developments (Li et al., 2024b;Zhang et al., 2024f) resemble the design of TableLlama (Zhang et al., 2024e) (TYPE 2.B), with LLaMA-2 as the backbone and instruction tuning as the pre-training task.</p>
<p>Table LLMs are validated through table QA, where the model is asked to produce the correct answer given the table structure, data values, and a question text.Most existing studies have been evaluated on the WikiTableQuestions and WikiSQL datasets.TableInstruct (Zhang et al., 2024e) is the most recently developed comprehensive benchmark integrating 14 datasets across 11 tasks.</p>
<p>Applications in Scientific Discovery</p>
<p>Mathematics LLMs have great potential to assist humans in offering potential solutions.For instance, AlphaGeometry (Trinh et al., 2024) combines an LLM with a symbolic deduction engine, where the LLM generates useful constructs and the symbolic engine applies formal logic to find solutions.AlphaGeometry solves 25 out of 30 classical geometry problems adapted from the International Mathematical Olympiad.Sinha et al. (2024) extend AlphaGeometry by adding Wu's method (Chou, 1988), further solving 27 out of 30, surpassing human gold medalists.FunSearch (Romera-Paredes et al., 2024) integrates LLM with program search.One notable achievement of FunSearch is its ability to find a new solution to the cap set problem in combinatorial optimization.The solutions generated can be faster and more efficient than those devised by human experts.In Li et al. (2024a), LLMs iteratively propose and critique statistical models by leveraging in-context learning and chain-of-thought reasoning (Wei et al., 2022b).</p>
<p>4 LLMs in Physics (Table A3)</p>
<p>Language</p>
<p>As a derivative of BERT, astroBERT (Grezes et al., 2024) (TYPE 1.A) is further pre-trained using astronomy-related papers via MLM and next sentence prediction.It is evaluated on the NER task.Likewise, AstroLLaMA (Nguyen et al., 2023b) (TYPE 2.A) fine-tunes LLaMA-2 using over 300,000 astronomy abstracts from arXiv.It is evaluated on paper generation and recommendation tasks.AstroLLaMA-chat (Perkowski et al., 2024) (TYPE 2.A) is the chat version of AstroL-LaMA.It is continually trained on a GPT-4 generated domain-specific dialogue dataset.PhysBERT (Hellert et al., 2024) (TYPE 1.A) is the first physicsspecific model for sentence embedding trained on a curated corpus of physics literature based on 1.2 million physics papers on arXiv.It is evaluated on physics-tailored tasks, such as information retrieval, classification, and semantic similarity estimation.</p>
<p>Applications in Scientific Discovery</p>
<p>Transformer-based physics LLMs can potentially assist humans in solving differential equations and designing experiments.For instance, Cai et al. (2024) apply Transformer to predict the integer coefficients in the scattering amplitudes of Planar N = 4 Super Yang-Mills theory; RydbergGPT (Fitzek et al., 2024) uses Transformer to learn the distribution of qubit measurement outcomes that describe an array of interacting Rydberg atoms; Arlt et al. (2024) present an initial trial that applies a code-generating LLM to synthesize experimental blueprints for a whole class of quantum systems in the form of Python code.</p>
<p>LLMs in Chemistry and Materials</p>
<p>Science (Table A4)</p>
<p>Language</p>
<p>LLM pre-training corpora in chemistry and materials science typically come from research papers and databases (e.g., Materials Project (Jain et al., 2013)).Besides, recent works adopt domainspecific instruction tuning datasets (e.g., Mol-Instructions (Fang et al., 2024a) and SMolInstruct (Yu et al., 2024a)) derived from PubChem (Kim et al., 2019), MoleculeNet (Wu et al., 2018), etc.</p>
<p>Early studies on chemistry LLMs mostly adopt a moderate-sized encoder-only architecture pre-trained with MLM (TYPE 1.A, e.g., ChemBERT (Guo et al., 2022), MatSciBERT (Gupta et al., 2022), and BatteryBERT (Huang and Cole, 2022)).These models are usually evaluated on downstream tasks including reaction role labeling (Guo et al., 2022) and abstract classification (Gupta et al., 2022).Recently, researchers have focused more on large-scale decoder-only LLMs trained with next token prediction and instruction tuning (TYPE 2.A).Examples include ChemDFM (Zhao et al., 2024), ChemLLM (Zhang et al., 2024b), and LlaSMol (Yu et al., 2024a).Given the desired generalization capability of such models, they are evaluated on a diverse set of tasks such as name conversion (Kim et al., 2019), reaction prediction (Jin et al., 2017), retrosynthesis (Schneider et al., 2016), text-based molecule design (Edwards et al., 2022), and crystal generation (Antunes et al., 2023;Flam-Shepherd and Aspuru-Guzik, 2023;Gruver et al., 2024).</p>
<p>Language + Graph</p>
<p>Graphs are appropriate data structures for characterizing molecules (Jin et al., 2023a).Popular datasets containing molecular graphs include ChEBI-20 (Edwards et al., 2021(Edwards et al., , 2022)), ZINC (Sterling and Irwin, 2015), and PCDes (Zeng et al., 2022).</p>
<p>In some scenarios, molecular graphs appear simultaneously with text information, thus existing works have explored how to encode both effectively.The first type of such models adopts a GNN as the graph encoder and an LLM as the text encoder.The two modalities are connected through contrastive learning (Liu et al., 2023d) (TYPE 3.C).For example, Text2Mol (Edwards et al., 2021) uses GCN (Kipf and Welling, 2017) and SciBERT to encode a molecule and its corresponding natural language description, respectively, for text-to-molecule retrieval.The second type of such models utilizes an LLM to encode text and graphs simultaneously (Zeng et al., 2022).Graphs can be either linearized to SMILES strings (Edwards et al., 2022) (TYPE 2.C) or projected onto virtual tokens with graph encoders (Zhao et al., 2023a;Liu et al., 2023e) (TYPE 2.D).For instance, 3D-MoLM (Li et al., 2024c) uses a 3-dimensional molecular encoder to represent molecules as tokens and feeds them together with instructions into LLaMA-2 for molecule-totext retrieval and molecule captioning.</p>
<p>Language + Vision</p>
<p>Complementing text and graph modalities, molecular images form the vision modality in chemistry.Existing works adopt a similar philosophy to BLIP-2 (Li et al., 2023b), which represents each image as tokens and feeds them into an LLM (TYPE 2.D).For example, GIT-Mol (Liu et al., 2024a) projects all modalities, including graphs and images, into the latent text space and conducts encoding and decoding using T5 (Raffel et al., 2020).</p>
<p>Molecule</p>
<p>Different from subsection 5.2, this subsection introduces models dealing with molecules without associated text information.That being said, comparable approaches inspired by LLMs are utilized to develop molecular language models (Flam-Shepherd et al., 2022).To be specific, most studies adopt SMILES or SELFIES (Krenn et al., 2020) strings as the sequential representation of molecules.Similar to the trend in the "Language" modality, pioneering molecular LLMs focus on representation learning with bidirectional Transformer encoders (TYPE 1.C, e.g., SMILES-BERT (Wang et al., 2019) and MoLFormer (Ross et al., 2022)).For instance, ChemBERTa (Chithrananda et al., 2020) adopts the architecture and pre-training strategy similar to those of RoBERTa (Liu et al., 2019).These models exhibit extraordinary abilities in molecular understanding tasks such as molecular property prediction (e.g., toxicity classification (Wu et al., 2018) and atomization energy regression (Ramakrishnan et al., 2014)) as well as virtual screening (Riniker and Landrum, 2013).Later works explore the idea of representing molecules in an autoregressive fashion (TYPE 2.C, e.g., BARTSmiles (Chilingaryan et al., 2024) and ChemGPT (Frey et al., 2023)).For instance, T5Chem (Lu and Zhang, 2022) adopts the T5 backbone and a sequence-to-sequence pretraining objective.These models are evaluated in generative tasks that include molecule generation (Gaulton et al., 2017), reaction prediction, and retrosynthesis.Besides linearizing molecules, there are studies modifying the Transformer architecture to admit molecular graphs, such as MAT (Maziarka et al., 2020) and R-MAT (Maziarka et al., 2024).</p>
<p>Applications in Scientific Discovery</p>
<p>Previous studies have shown that LLMs facilitate autonomous chemical research.For example, Bran et al. ( 2024) present a chemistry LLM agent, Chem-Crow, that can integrate expert-designed tools for organic synthesis, drug discovery, and materials design; Zheng et al. (2023a) demonstrate that LLMs can perform knowledge synthesis from the scientific literature, knowledge inference from data, and interpretable explanation generation in chemistry; Boiko et al. ( 2023) develop an LLM-empowered intelligence system, Coscientist, that can design, plan, and perform chemical research.Moreover, LLMs accomplish complex tasks in chemistry, such as drug and catalyst design and molecular discovery, purely from instructions (White, 2023).For instance, Ramos et al. ( 2023) study catalyst and molecule design with in-context learning, removing the requirement for traditional training or simulation processes; ChatDrug (Liu et al., 2024b) explores drug editing using LLMs with a prompt module, a domain feedback module, and a conversation module; Jablonka et al. ( 2024) find that fine-tuned LLMs perform comparably to, or even better than, conventional techniques for many chemistry applications, spanning from the properties of molecules and materials to the yield of chemical reactions; DrugAssist (Ye et al., 2023a)  The watershed moment in the evolution biomedical LLMs is still the emergence of billionparameter architectures and instruction tuning.Before that, a wide variety of moderate-sized back-bones are explored, including both encoder-based (TYPE 1.A, e.g., BioBERT (Lee et al., 2020), Bio-ELECTRA (Ozyurt, 2020), BioRoBERTa (Lewis et al., 2020b), BioALBERT (Naseem et al., 2022), and Clinical-Longformer (Li et al., 2022a)) and (encoder-)decoder-based ones (TYPE 2.A, e.g., Sci-Five (Phan et al., 2021), BioBART (Yuan et al., 2022a), and BioGPT (Luo et al., 2022)).Evaluation tasks for these models range from biomedical NER, RE, sentence similarity estimation, document classification, and QA (i.e., the BLURB benchmark (Gu et al., 2021)) to natural language inference (NLI) (Romanov and Shivade, 2018) and entity linking (Dogan et al., 2014).After the watershed, the trend becomes instruction-tuning billion-parameter LLMs (TYPE 2.A, e.g., Med-PaLM (Singhal et al., 2023a), MedAlpaca (Han et al., 2023), and BioMistral (Labrak et al., 2024)).Accordingly, evaluation tasks now include singleround QA (Jin et al., 2021;Pal et al., 2022) and multi-round dialogue (Wang et al., 2024g).Meanwhile, there are studies proposing a Bi-Encoder architecture (TYPE 3.A, e.g., Jin et al. (2023c) and Xu et al. (2024)) that specifically targets biomedical retrieval tasks, the benchmarks of which are NFCorpus (Boteva et al., 2016), TREC-COVID (Voorhees et al., 2021), etc.</p>
<p>Language + Graph</p>
<p>Biomedical ontologies capture rich types of relations between entities.Analogously, citation links characterize connections between biomedical papers.Intuitively, jointly leveraging text and such graph information paves the way for multi-hop reasoning in QA.For instance, Yasunaga et al. (2022a) propose to use an LLM and a GNN to encode text and ontology signals, respectively, and deeply fuse them (TYPE 3.C); Yasunaga et al. (2022b) concatenate text segments from two linked papers together and feed the sequence into an LLM for pre-training, which is essentially appending a metadata neighbor (i.e., reference) as context for MLM (TYPE 1.B).Both approaches demonstrate significant improvement in QA tasks that require complex reasoning.</p>
<p>Language + Vision</p>
<p>Biomedical text-image pairs typically come from two sources: (1) medical reports, such as chest X-rays (e.g., MIMIC-CXR (Johnson et al., 2019)) and pathology reports (Huang et al., 2023); as well as (2) figure-caption pairs extracted from biomedical papers (e.g., ROCO (Pelka et al., 2018) and MedICaT (Subramanian et al., 2020)).</p>
<p>Most biomedical vision-language models exploit the CLIP architecture (Radford et al., 2021), where a text encoder and an image encoder are jointly trained to map the paired text and image closer via contrastive learning (TYPE 3.D).The choice of the text encoder evolves from BERT (Zhang et al., 2022) and GPT-2 (Huang et al., 2023) to LLaMA (Wu et al., 2023) and LLaMA-2 (Liu et al., 2023b), while the image encoder evolves from ResNet (Huang et al., 2021) to ViT (Zhang et al., 2023c) and Swin Transformer (Thawkar et al., 2024).MLM, masked image modeling, and texttext/image-image contrastive learning (i.e., by creating augmented views within the language/vision modality) are sometimes adopted as auxiliary pretraining tasks.Besides CLIP, other general-domain vision-language architectures, such as LLaVA (Li et al., 2023a), PaLM-E (Tu et al., 2024), and Gemini (Saab et al., 2024) (Ji et al., 2021), and RNABERT (Akiyama and Sakakibara, 2022), adopt BERT-like architectures and MLM as the pre-training task (i.e., predicting masked amino acids, nucleotides, k-mers, or codons); decoder-only models, such as ProGen (Madani et al., 2023) and DNAGPT (Zhang et al., 2023a), exploit GPT-like architectures and next token prediction as the pre-training task.There are also studies jointly considering text and protein modalities.For instance, ProtST (Xu et al., 2023b) matches protein sequences with their text descriptions (i.e., names and functions) via contrastive learning (TYPE 3.B); BioMedGPT (Luo et al., 2023c) first projects proteins onto tokens and then inputs these tokens together with text into LLaMA-2 for instruction tuning, bearing similarity with TYPE 2.D.</p>
<p>Existing multiomics LLMs mainly focus on single-cell transcriptomics (e.g., scRNA-seq) data, such as the expression levels of genes within a single cell (Franzén et al., 2019).Besides BERTbased (e.g., Geneformer (Theodoris et al., 2023)) and GPT-based (e.g., scGPT (Cui et al., 2024)) architectures, Performer (Yang et al., 2022a;Hao et al., 2024) is widely used due to its linear attention complexity in handling long scRNA-seq data.</p>
<p>Applications in Scientific Discovery</p>
<p>Similarly to chemistry, LLMs can automate experiments in biological and medical research.For example, CRISPR-GPT (Huang et al., 2024a) augments an LLM agent with domain knowledge to enhance the design process of CRISPR-based gene-editing experiments; TrialMind (Wang et al., 2024h) utilizes LLMs to extract results and synthesize clinical evidence from the literature for medical discovery.Moreover, LLMs can encode biological sequences to capture structural properties and guide protein design.For instance, ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2023b) (Webersinke et al., 2021), SpaBERT (Li et al., 2022b), and MGeo (Ding et al., 2023)).For instance, SpaBERT and MGeo perform MLM on a sequence of geolocations for geographic entity linking and query-POI matching, respectively.More recently, related studies concentrate on scaling up decoding-style autoregressive LLMs in geoscience (TYPE 2.A, e.g., K2 (Deng et al., 2024), OceanGPT (Bi et al., 2023b), and GeoGalactica (Lin et al., 2024c)).For instance, K2 and OceanGPT adapt LLaMA to geoscience and ocean science, respectively, via supervised fine-tuning with domain-specific instructions curated by human experts and/or augmented by general-domain LLMs.Evaluations of such models are conducted on geoscience benchmarks, such as GeoBench (Deng et al., 2024) and OceanBench (Bi et al., 2023b), which encompass a broad range of tasks including QA, classification, knowledge probing, reasoning, summarization, and generation.</p>
<p>Language + Graph</p>
<p>Some geoscience applications involve graph signals, such as heterogeneous POI networks and knowledge graphs.To handle such signals and text jointly, ERNIE-GeoL (Huang et al., 2022) introduces a Transformer-based aggregation layer to deeply fuse text and POI information within a BERT-based architecture; PK-Chat (Deng et al., 2023) combines an LLM with a pointer generation network on a knowledge graph to build a knowledge-driven dialogue system.</p>
<p>Language + Vision</p>
<p>Aerial views, together with location descriptions, profile urban regions.To address language and vision modalities jointly, UrbanCLIP (Yan et al., 2024) considers the CLIP architecture (TYPE 3.D), which is also widely adopted by biomedical visionlanguage models as mentioned in subsection 6.3, to perform text-image contrastive learning for urban indicator prediction.</p>
<p>Climate Time Series</p>
<p>The intuitions and methodologies used in LLMs also facilitate the construction of climate foundation models.Based on the ERA5 (Hersbach et al., 2020) and CMIP6 (Eyring et al., 2016) datasets of climate time series, previous studies exploit the ViT and Swin Transformer architectures to pre-train foundation models for weather forecasting.Representative models include FourCastNet (Pathak et al., 2022), Pangu-Weather (Bi et al., 2023a), etc.</p>
<p>Applications in Scientific Discovery</p>
<p>In geography, Wang et al. (2023b) and Zhou et al. (2024b) highlight the potential of LLMs in urban planning from sustainability, living, economic, disaster, and environmental perspectives.In geology, besides climate and weather forecasting, foundation models have been applied to simultaneous earthquake detection and phase picking (Mousavi et al., 2020).In environmental science, ChatClimate (Vaghefi et al., 2023) enhances GPT-4 by providing access to external, scientifically accurate knowledge on climate change to build a climate science conversational AI.</p>
<p>Challenges and Future Directions</p>
<p>In this survey, we compile literature that elucidates the data, architectures, and tasks used for scientific LLM pre-training, as well as how scientific LLMs have been applied to downstream applications in scientific discovery.In particular, we underscore analogous architectures, tasks, and trends observed during the evolution of scientific LLMs across different fields and modalities.Beyond reviewing prior research, we present several challenges to inspire further exploration of this topic.Diving into Fine-Grained Themes.Most existing scientific LLMs target a coarse-grained field (e.g., chemistry), while some tasks rely on highly specialized knowledge of a fine-grained theme (e.g., Suzuki coupling).When LLMs are pre-trained on more general corpora, frequently appeared signals may dominate the model parameter space, and domain-specific tail knowledge may be wiped out.We believe that automatically curating indepth, theme-focused knowledge graphs (Hope et al., 2021) to guide the generation process will be a promising direction to tackle this issue.Generalizing to Out-of-Distribution Scientific Data.In the scientific domain, it is common that the testing distribution shifts from the training distribution (Zhang et al., 2023e): novel scientific concepts keep emerging in newly published papers; unseen molecules with different scaffolds and unseen proteins with different numbers of peptide chains may appear during testing.Handling such out-of-distribution data remains a challenge for pretrained scientific LLMs.To our knowledge, invariant learning (Arjovsky et al., 2019) can serve as the theoretical foundation for out-of-distribution analyses, and how to integrate it into LLM pre-training is worth exploring.Facilitating Trustworthy Predictions.LLMs can generate plausible-sounding but factually incorrect output, commonly known as hallucination (Ji et al., 2023), which is particularly dangerous in high-stakes scientific domains such as chemistry and biomedicine.To mitigate this issue, retrievalaugmented generation (RAG) provides LLMs with relevant, up-to-date, and trustworthy information.However, previous RAG studies in the scientific domain mainly focus on retrieving text (Xiong et al., 2024) and knowledge (Jin et al., 2024), while scientific data are heterogeneous and multi-modal.We envision that cross-modal RAG (e.g., guiding text generation with relevant chemicals and proteins) will present additional opportunities to further enhance the trustworthiness of scientific LLMs.</p>
<p>Limitations</p>
<p>This survey primarily covers LLMs in mathematics and natural sciences.We are aware that LLMs can also significantly impact social sciences by achieving remarkable performance in representative tasks (Ziems et al., 2024) and serving as agents for social simulation experiments (Horton, 2023), but we leave the survey of these efforts as future work due to space limitations.In addition, this paper focuses on LLMs pre-trained on scientific data or augmented with domain-specific knowledge to benefit scientific discovery.There are studies (Guo et al., 2023;Wang et al., 2024f;Yue et al., 2024a;Liang et al., 2024d) proposing new benchmark datasets of scientific problems but evaluating the performance of general-purpose LLMs only, and we do not include these works in our survey.Furthermore, some LLMs may belong to more than one field or modality category given our classification criteria in the paper.For instance, BioMedGPT (Luo et al., 2023c) is pre-trained on biology and chemistry data jointly; GIT-Mol (Liu et al., 2024a) considers the language, graph, and vision modalities simultaneously.For the sake of brevity, we introduce each of them in only one subsection.</p>
<p>A Summary Tables of Scientific LLMs</p>
<p>Table A1-Table A6 summarize the modality, number of parameters, model architecture, pre-training data, pre-training task(s), and evaluation task(s) of scientific LLMs in each field.Within each field, we categorize models according to their modality; within each modality, we sort models chronologically.To be specific, if a paper has a preprint (e.g., arXiv or bioRxiv) version, its publication date is according to the preprint service.Otherwise, its publication date is according to the conference proceeding or journal.</p>
<p>Table A1: Summary of LLMs in general science."L": Language; "L+G": Language + Graph; "∼": generally adopting the architecture but with modifications; "MLM": masked language modeling; "NSP": next sentence prediction; "NER": named entity recognition; "RE": relation extraction; "QA": question answering.A5: Summary of LLMs in biology and medicine."Multi": Multiomics (e.g., single-cell); "NLI": natural language inference; "VQA": visual question answering; "EHR": electronic health record; "EMR": electronic medical record; "PPI": protein-protein interaction.Other notations have the same meaning as in previous tables.</p>
<p>Figure 1 :
1
Figure 1: Three major types of scientific LLM pre-training techniques.(COLUMN 1): Pre-training encoder LLMs with sequentialized scientific data (e.g., text, academic graphs, molecules, biological sequences) via masked language modeling.(COLUMN 2): Pre-training (encoder-)decoder LLMs with sequentialized scientific data (e.g., text, tables, crystals, images) via next token prediction (possibly with instruction tuning).(COLUMN 3): Mapping text and relevant sequences/graphs/images closer in the latent space via contrastive learning.</p>
<p>Mirza et al. (2024)23Sprueill et al. ( , 2024) )cule optimization through human-machine dialogue;Sprueill et al. (2023Sprueill et al. ( , 2024) )use LLMs as agents to search for effective catalysts through Monte Carlo Tree Search and the feedback from an atomistic neural network model;Wang et al. (2024b)re-engineer crossover and mutation operations for molecular discovery using LLMs trained on extensive chemical datasets.Meanwhile, benchmarking studies byMirza et al. (2024)demonstrate that although LLMs achieve superhuman proficiency in many chemical tasks, further research is critical to enhancing their safety and utility in chemical sciences.
6 LLMs in Biology and Medicine(Table A5)6.1 LanguageBesides research articles (e.g., titles/abstracts fromPubMed (Lu, 2011) and full text from PMC (Beckand Sequeira, 2003)), pre-training corpora forbiomedical LLMs include electronic health records(e.g., MIMIC-III (Johnson et al., 2016), MIMIC-IV (Johnson et al., 2023)), knowledge bases (e.g.,UMLS (Bodenreider, 2004)), and health-related so-cial media posts (e.g., COVID-19 tweets (Mülleret al., 2023)). Recent studies further collect su-pervised fine-tuning and preference optimizationdatasets from medical exam questions, knowledgegraphs, and doctor-patient dialogues. Examples in-clude ChiMed (Ye et al., 2023b), MedInstruct-52k(Zhang et al., 2023d), and BiMed1.3M (Acikgozet al., 2024), many of which have non-English com-
ponents (e.g., Chinese and Arabic).</p>
<p>He Cao,Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li.2023.Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery.arXiv preprint arXiv:2311.16208.
Benedikt Boecking, Naoto Usuyama, Shruthi Bannur,Daniel C Castro, Anton Schwaighofer, StephanieHyland, Maria Wetscherek, Tristan Naumann, AdityaNori, Javier Alvarez-Valle, et al. 2022. Making themost of text semantics to improve biomedical vision-Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,language processing. In ECCV'22, pages 1-21.and Sung Ju Hwang. 2024. Researchagent: Iter-Souradip Chakraborty, Ekaba Bisong, Shweta Bhatt,ative research idea generation over scientific liter-Thomas Wagner, Riley Elliott, and FrancescoDaniil A Boiko, Robert MacKnight, Ben Kline, andature with large language models. arXiv preprint Mosconi. 2020. Biomedbert: A pre-trained biomed-Gabe Gomes. 2023. Autonomous chemical researcharXiv:2404.07738. ical language model for qa and ir. In COLING'20,with large language models. Nature, 624(7992):570-pages 669-679.578.Viraj Bagal, Rishal Aggarwal, PK Vinod, and U DevaPriyakumar. 2022. Molgpt: molecular generation us-Jinho Chang and Jong Chul Ye. 2024. Bidirectional gen-Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga,ing a transformer-decoder model. Journal of Chemi-eration of structure and properties through a singleDavid Hall, Betty Xiong, Tony Lee, Roxanacal Information and Modeling, 62(9):2064-2076. molecular foundation model. Nature Communica-Daneshjou, Jonathan Frankle, Percy Liang, Michaeltions, 15(1):2323.Carbin, et al. 2024. Biomedlm: A 2.7 b parameterFan Bai, Yuxin Du, Tiejun Huang, Max Q-H Meng, andlanguage model trained on biomedical text. arXivBo Zhao. 2024. M3d: Advancing 3d medical image Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin,preprint arXiv:2403.18421.analysis with multi-modal large language models. Chongyu Chen, and Xiaodan Liang. 2022a. Unigeo:arXiv preprint arXiv:2404.00578. Unifying geometry logical reasoning via reformulat-Vera Boteva, Demian Gholipour, Artem Sokolov, anding mathematical expression. In EMNLP'22, pagesStefan Riezler. 2016. A full-text learning to rankAmos Bairoch and Rolf Apweiler. 2000. The swiss-prot 3313-3323.dataset for medical information retrieval. In ECIR'16,protein sequence database and its supplement tremblpages 716-722.in 2000. Nucleic Acids Research, 28(1):45-48. Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang,Lingbo Liu, Eric Xing, and Liang Lin. 2021. Geoqa:Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Bal-Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fer-A geometric question answering benchmark towardsdassari, Andrew D White, and Philippe Schwaller.nando Perez-Garcia, Maximilian Ilse, Daniel C Cas-multimodal numerical reasoning. In Findings of2024. Augmenting large language models with chem-tro, Benedikt Boecking, Harshita Sharma, Kenza ACL'21, pages 513-523.istry tools. Nature Machine Intelligence, 6(5):525-Bouzid, Anja Thieme, et al. 2023. Learning to535.exploit temporal structure for biomedical vision-Jiayang Chen, Zhihang Hu, Siqi Sun, Qingxiong Tan,language processing. In CVPR'23, pages 15016-Yixuan Wang, Qinze Yu, Licheng Zong, Liang Hong,Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rap-15027. Jin Xiao, Tao Shen, et al. 2022b. Interpretable rnapoport, and Michal Linial. 2022. Proteinbert: a uni-foundation model from unannotated data for highlyversal deep-learning model of protein sequence andZhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao accurate rna structure and function predictions. arXivfunction. Bioinformatics, 38(8):2102-2110.Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, and preprint arXiv:2204.00300.Zhongyu Wei. 2023. Disc-medllm: Bridging gen-Keno K Bressem, Lisa C Adams, Robert A Gaudin,eral large language models and real-world medical Junying Chen, Xidong Wang, Anningzhe Gao, FengDaniel Tröltzsch, Bernd Hamm, Marcus R Makowski,consultation. arXiv preprint arXiv:2308.14346. Jiang, Shunian Chen, Hongbo Zhang, Dingjie Song,Chan-Yong Schüle, Janis L Vahldiek, and Stefan MWenya Xie, Chuyi Kong, Jianquan Li, et al. 2023a.Niehues. 2020. Highly accurate classification ofMarco Basaldella, Fangyu Liu, Ehsan Shareghi, and Huatuogpt-ii, one-stage training for medical adaptionchest radiographic reports using a deep learning nat-Nigel Collier. 2020. Cometa: A corpus for medical of llms. arXiv preprint arXiv:2311.09774.ural language model pre-trained on 3.8 million textentity linking in the social media. In EMNLP'20,reports. Bioinformatics, 36(21):5255-5261.pages 3122-3137. Kang Chen, Tao Han, Junchao Gong, Lei Bai, FenghuaLing, Jing-Jia Luo, Xi Chen, Leiming Ma, TianningTom Brown, Benjamin Mann, Nick Ryder, MelanieJeff Beck and Ed Sequeira. 2003. Pubmed central (pmc): Zhang, Rui Su, et al. 2023b. Fengwu: Pushing theSubbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindAn archive for literature from life sciences journals. skillful global medium-range weather forecast be-Neelakantan, Pranav Shyam, Girish Sastry, AmandaThe NCBI Handbook. yond 10 days lead. arXiv preprint arXiv:2304.02948.Askell, et al. 2020. Language models are few-shotlearners. In NeurIPS'20, pages 1877-1901.Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: Ken Chen, Yue Zhou, Maolin Ding, Yu Wang, ZhixiangA pretrained language model for scientific text. In Ren, and Yuedong Yang. 2024. Self-supervised learn-Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas,EMNLP'19, pages 3615-3620. ing on millions of primary rna sequences from 72 ver-and Maria De La Iglesia-Vaya. 2020. Padchest: Atebrates improves sequence-based rna splicing pre-large chest x-ray image dataset with multi-label an-Kaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, diction. Briefings in Bioinformatics, 25(3):bbae163.notated reports. Medical Image Analysis, 66:101797.Xiaotao Gu, and Qi Tian. 2023a. Accurate medium-range global weather forecasting with 3d neural net-Lei Chen, Xiaohui Zhong, Feng Zhang, Yuan Cheng,Isabel Cachola, Kyle Lo, Arman Cohan, and Daniel Sworks. Nature, 619(7970):533-538. Yinghui Xu, Yuan Qi, and Hao Li. 2023c. Fuxi:Weld. 2020. Tldr: Extreme summarization of scien-a cascade machine learning forecasting system fortific documents. In Findings of EMNLP'20, pagesZhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Dax-15-day global weather forecast. npj Climate and4766-4777.iong Ji, Guozhou Zheng, and Huajun Chen. 2023b. Atmospheric Science, 6(1):190.Oceangpt: A large language model for ocean sciencetasks. In ACL'24, pages 3357-3372.Olivier Bodenreider. 2004. The unified medical lan-guage system (umls): integrating biomedical termi-nology. Nucleic Acids Research, 32(suppl_1):D267-D270.
Tianji Cai, Garrett W Merz, François Charton, Niklas Nolte, Matthias Wilhelm, Kyle Cranmer, and Lance J Dixon.2024.Transforming the bootstrap: Using transformers to compute scattering amplitudes in planar n= 4 super yang-mills theory.Machine Learning: Science and Technology, 5(3):035073.Yirong Chen, Zhenyu Wang, Xiaofen Xing, Zhipei Xu, Kai Fang, Junhong Wang, Sihang Li, Jieling Wu, Qi Liu, Xiangmin Xu, et al. 2023d.Bianque: Balancing the questioning and suggestion ability of health llms with multi-turn health conversations polished by chatgpt.arXiv preprint arXiv:2310.15896.</p>
<p>Table A2 :
A2
Summary of LLMs in mathematics."L+V": Language + Vision; "MWP": math word problems.Other notations have the same meaning as in previous tables.
(Mathematics, Table Continued)ModelModalitySizeArchitecturePre-training DataPre-training Task(s)Evaluation Task(s)Llemma (Azerbayev et al., 2024)L7B, 34BLLaMA-2Proof-Pile-2next token predictionQA, MWP, quantitative reasoningOVM (Yu et al., 2024b)L7BLLaMA-2GSM8Ksupervised fine-tuningQA, MWP, quantitative reasoning7BMistralDeepSeekMath (Shao et al., 2024)L7BDeepSeekmath web pages,next token prediction,QA, MWP, quantitative reasoning,instructionsinstruction tuningformal translationInternLM-Math (Ying et al., 2024)L7B, 20BInternLM2Knowledge Pile,next token prediction,QA, MWP, quantitative reasoning,Proof-Pile-2,instruction tuningformal translationinstructionsOpenMath (Toshniwal et al., 2024)L7B, 13B,LLaMA-2OpenMathInstruct-1instruction tuningQA, MWP34B, 70B7BMistralRho-Math (Lin et al., 2024b)L1B∼LLaMA-2OpenWebMath,next token predictionQA, MWP, quantitative reasoning7BMistralSlimPajama,StarCoderDataMAmmoTH2 (Yue et al., 2024c)L8BLLaMA-3WebInstructinstruction tuningQA, MWP, quantitative reasoning7BMistral8×7BMixtralTheoremLlama (Wang et al., 2024e)L8BLLaMA-3Open Bootstrappedinstruction tuningmathematical proof generationTheoremsModel Inter-GPS (Lu et al., 2021)L+VModality-SizeArchitecture ∼BART + Geometry3K, GEOS Pre-training Data RetinaNetPre-training Task(s) sequence to sequencegeometry problem solving Evaluation Task(s)SciBERT (Beltagy et al., 2019) Geoformer (Chen et al., 2022a) SciGPT2 (Luu et al., 2021) CATTS (Cachola et al., 2020) SCA-GPS (Ning et al., 2023) SciNewsBERT (Smeros et al., 2021) ScholarBERT (Hong et al., 2023) UniMath-Flan-T5 (Liang et al., 2023) AcademicRoBERTa (Yamauchi et al., 2022) L+V L+V L+V G-LLaVA (Gao et al., 2023) L+VL L L L L L110M 117M 406M 110M 340M, 770M -VL-T5 + GPT-2 BERT ResNet BART -RoBERTa + GeoQA, Geometry3K Semantic Scholar UniGeo S2ORC SciTLDR BERT news headlines ViT BERT Public.Resource.Org, masked image modeling, MLM, NSP sequence to sequence next token prediction sequence to sequence MLM, NSP sequence to sequence MLM -Flan-T5 + SVAMP, GeoQA, image reconstruction, VQ-VAE TabMWP sequence to sequence Wikipedia, BookCorpus 125M RoBERTa CiNii MLM 7B, 13B LLaVA GeoQA+, Geometry3K text-image matching, instruction tuninggeometry problem solving NER, RE, classification, parsing paper relationship explanation geometry problem solving summarization scientific claim extraction MWP, NER, RE, classification geometry problem solving geometry problem solving classification, author identificationGalactica (Taylor et al., 2022) TAPAS (Herzig et al., 2020) TaBERT (Yin et al., 2020) GraPPa (Yu et al., 2021) DARWIN (Xie et al., 2023) TUTA (Wang et al., 2021) FORGE (Yin et al., 2023) RCI (Glass et al., 2021) SciGLM (Zhang et al., 2024a) TABBIE (Iida et al., 2021)Table Table Table Table Table TableL L L L125M, 1.3B, 6.7B, 30B, 110M, 340M 120B 110M, 340M 355M RoBERTa Galactica BERT BERT 7B LLaMA 110M BERT 1.4B, 13B, GPT-NeoX WDC Web Table papers, code, reference materials, Wikipedia knowledge bases, Wikipedia, web crawl data, instructions Wikipedia papers, QA pairs, Wikipedia, WDC Web Table, instructions CORE, AMiner, MAG, SQL semantic prediction next token prediction, instruction tuning MLM MLM, cell value recovery MLM, instruction tuning MLM, cell-level cloze, next token prediction spreadsheets table context retrieval 22B 12M ALBERT WikiSQL, TabMCQ, classification SCOPUS, arXiv 6B, 32B ChatGLM SciInstruct instruction tuning WikiTableQuestions 110M ELECTRA Wikipedia, VizNet MLM,QA, link prediction, table QA knowledge probing, table QA quantitative reasoning, chemical name conversion, table QA molecule classification, protein function prediction cell type classification, QA, classification, regression table type classification QA, classification, regression table QA QA, quantitative reasoning column/row population,SPECTER (Cohan et al., 2020) TAPEX (Liu et al., 2022a) OAG-BERT (Liu et al., 2022b) FORTAP (Cheng et al., 2022) ASPIRE (Mysore et al., 2022) OmniTab (Jiang et al., 2022) SciNCL (Ostendorff et al., 2022) ReasTAP (Zhao et al., 2022) SPECTER 2.0 (Singh et al., 2023) Table-GPT (Li et al., 2024b) SciPatton (Jin et al., 2023b) SciMult (Zhang et al., 2023f) TableLlama (Zhang et al., 2024e)Table Table Table Table Table TableL+G 140M, 406M 110M L+G 110M 110M L+G 110M 406M L+G 110M 406M L+G 113M 175B -L+G -L+G 138M 7BBERT ∼BERT WikiTableQuestions Semantic Scholar AMiner, PubMed, spreadsheets OAG numerical reference prediction, link prediction replaced token detection sequence to sequence MLM MLM, numerical calculation prediction BERT S2ORC link prediction Wikipedia sequence to sequence BERT Semantic Scholar link prediction Wikipedia sequence to sequence Adapters BART BERT BART BART SciRepEval classification, regression, GPT-3.5 instructions instruction tuning link prediction, retrieval ChatGPT GraphFormers MAG MLM, link prediction MoE MAG, classification, LLaMA-2 TableInstruct instruction tuning Semantic Scholar, link prediction, retrieval SciRepEvalcolumn type classification classification, link prediction, table QA recommendation table QA, classification, link prediction, formula prediction, recommendation, retrieval, cell type classification author name disambiguation table QA paper similarity estimation table QA, table fact verification, classification, link prediction, table-to-text generation recommendation table QA, column-finding, classification, regression, missing-value identification, link prediction, retrieval, column type classification, author name disambiguation, data transformation, paper-reviewer matching table matching, data cleaning classification, link prediction table QA, RE, entity linking, classification, link prediction, column type classification, recommendation, retrieval, column/row population, patient-article/patient matching table fact verification,cell descriptionTableLLM (Zhang et al., 2024f)Table7B, 13BLLaMA-2WikiTQ, FeTaQA,instruction tuningtable QA, table updating,TAT-QA, WikiSQL,table merging, table chartingSpiderModelModalitySizeArchitecturePre-training DataPre-training Task(s)Evaluation Task(s)GenBERT (Geva et al., 2020)L110MBERTWikipediaMLM,QA, MWPsequence to sequenceMathBERT (Shen et al., 2021)L110MBERTarXiv, math curricula,MLMclassification, auto-gradingsyllabi, textbooksMWP-BERT (Liang et al., 2022)L110MBERTApe210KMLM, regression,QA, MWPclassificationBERT-TD (Li et al., 2022c)L110MBERTMath23K, MathQAsequence to sequence,QA, MWPcontrastive learningGSM8K-GPT (Cobbe et al., 2021)L6B, 175BGPT-3GSM8Ksupervised fine-tuningQA, MWPDeductReasoner (Jie et al., 2022)L125MRoBERTaMAWPS, Math23K,sequence to sequenceQA, MWPMathQA, SVAMPNaturalProver (Welleck et al., 2022)L175BGPT-3NaturalProofssupervised fine-tuningmathematical proof generationMinerva (Lewkowycz et al., 2022)L8B, 62B,PaLMarXiv, math web pagesnext token predictionQA, MWP, quantitative reasoning540BBhāskara (Mishra et al., 2022)L2.7BGPT-NeoL īlainstruction tuningQA, MWP, knowledge probingWizardMath (Luo et al., 2023a)L7B, 13B,LLaMA-2GSM8K, MATHinstruction tuningQA, MWP70BMAmmoTH (Yue et al., 2024b)L7B, 13B,LLaMA-2MathInstructinstruction tuningQA, MWP34B, 70B7BMistralMetaMath (Yu et al., 2024c)L7B, 13B,LLaMA-2MetaMathQAinstruction tuningQA, MWP70B7BMistralToRA (Gou et al., 2024)L7B, 13B,LLaMA-2ToRA-Corpusinstruction tuningQA, MWP34B, 70BMathCoder (Wang et al., 2024c)L7B, 13B,LLaMA-2MathCodeInstructinstruction tuningQA, MWP34B, 70B</p>
<p>Table A3 :
A3
Summary of LLMs in physics.Notations have the same meaning as in previous tables.
(Chemistry and Materials Science, Table Continued)ModelModalitySizeArchitecturePre-training DataPre-training Task(s)Evaluation Task(s)ChemDFM (Zhao et al., 2024)L13BLLaMAchemistry papers,next token prediction,QA, classification,textbooks, instructionsinstruction tuningname conversion,molecule captioning,text-based molecule design,reaction prediction, retrosynthesisCrystalLLM (Gruver et al., 2024)L7B, 13B,LLaMA-2Materials Projectinstruction tuningcrystal generation70BChemLLM (Zhang et al., 2024b)L7BInternLM2QA pairs, ChemDatainstruction tuningQA, classification,name conversion,molecule captioning,text-based molecule design,reaction prediction, retrosynthesisLlaSMol (Yu et al., 2024a)L6.7BGalacticaSMolInstructinstruction tuningQA, classification, regression,7BLLaMA-2name conversion,7BMistralmolecule captioning,text-based molecule design,reaction prediction, retrosynthesisText2Mol (Edwards et al., 2021)L+G-BERT +PubChem, ChEBI-20text-graph matchingtext-to-molecule retrievalGCNKV-PLM (Zeng et al., 2022)L+G110MBERTS2ORC, PubChemtext-graph matchingNER, RE, classification,text-to-molecule retrieval,molecule-to-text retrievalMolT5 (Edwards et al., 2022)L+G60M, 220M,T5C4, ZINC, ChEBI-20sequence to sequencemolecule captioning,770Mtext-based molecule designMoMu (Su et al., 2022)L+G-BERT +S2ORC, PubChemtext-graph matchingclassification,GINtext-to-molecule retrieval,molecule-to-text retrieval,molecule captioning,text-based molecule designMoleculeSTM (Liu et al., 2023d)L+G-BERT +PubChemtext-graph matchingclassification,GINtext-to-molecule retrieval,molecule-to-text retrieval,text-based molecule designText+Chem T5L+G60M, 220MT5Pistachio, ChEBI-20,sequence to sequencemolecule captioning,(Christofidellis et al., 2023)experimental procedurestext-based molecule design,reaction prediction, retrosynthesis,paragraph-to-action generationGIMLET (Zhao et al., 2023a)L+G60M∼T5ChEMBLinstruction tuningclassification, regressionMolFM (Luo et al., 2023b)L+G-∼BERT +S2ORC, PubChemMLM, KG embedding,classification,GINtext-graph matchingtext-to-molecule retrieval,molecule-to-text retrieval,molecule captioning,text-based molecule designMolCA (Liu et al., 2023e)L+G-Galactica +PubChemtext-graph matching,classification, name conversion,GINgraph-to-text generationmolecule-to-text retrieval,molecule captioning,functional group countingInstructMol (Cao et al., 2023)L+G-LLaMA +PubChem, MoleculeNet,text-graph matching,classification, regression,GINChEBI-20, USPTOinstruction tuningmolecule captioning,reaction prediction, retrosynthesis,reagent selection3D-MoLM (Li et al., 2024c)L+G-LLaMA-2 +PubChem, 3D-MoITtext-graph matching,QA, regression,Uni-Molgraph-to-text generation,molecule-to-text retrieval,instruction tuningmolecule captioningGIT-Mol (Liu et al., 2024a)L+G+V-BERT +PubChem, ChEBI-20text-graph/image/textclassification,GIN +matching,molecule captioning,Swinsupervised fine-tuningtext-based molecule design,molecule image recognitionSMILES-BERT (Wang et al., 2019)Molecule-∼BERTZINCMLMclassificationMAT (Maziarka et al., 2020)Molecule-∼BERTZINCmasked node predictionclassification, regressionChemBERTa (Chithrananda et al., 2020) Molecule125MRoBERTaPubChemMLMclassificationMolBERT (Fabian et al., 2020)Molecule110MBERTChEMBLMLM, regression,classification, regression,SMILES equivalencevirtual screeningrxnfp (Schwaller et al., 2021b)Molecule110MBERTPistachio, USPTOclassificationclassification,reaction representation learningRXNMapper (Schwaller et al., 2021a)Molecule770K∼ALBERTUSPTOMLMatom-mappingMoLFormer (Ross et al., 2022)Molecule47MlinearPubChem, ZINCMLMclassification, regressionattentionModel Chemformer (Irwin et al., 2022)Modality MoleculeSize 45M, 230MArchitecture ∼BARTPre-training Data USPTO, ChEMBL, MoleculeNetPre-training Task(s) sequence to sequence, regression reaction prediction, retrosynthesis, regression, Evaluation Task(s)astroBERT (Grezes et al., 2024) AstroLLaMA (Nguyen et al., 2023b) R-MAT (Maziarka et al., 2024) AstroLLaMA-Chat (Perkowski et al., 2024) MolGPT (Bagal et al., 2022) PhysBERT (Hellert et al., 2024) T5Chem (Lu and Zhang, 2022) ChemGPT (Frey et al., 2023)L L Molecule L Molecule L Molecule Molecule110M 7B -7B 6M 110M -4.7M, 19M,BERT LLaMA-2 ∼BERT LLaMA-2 ∼GPT-1 BERT ∼T5 ∼GPT-NeoNASA Astrophysics Data System arXiv ZINC, ChEMBL masked node prediction, MLM, NSP next token prediction regression QA pairs, LIMA, OpenOrca, UltraChat instruction tuning ZINC, ChEMBL next token prediction arXiv MLM, PubChem sequence to sequence classification, regression, molecule generation NER classification, regression paper generation, paper similarity estimation molecule generation QA classification, retrieval, contrastive learning reaction prediction, retrosynthesis clustering PubChem next token prediction -1.2BUni-Mol (Zhou et al., 2023)Molecule-SE(3)ZINC, ChEMBL,3D position recoveryclassification, regression,TransformerRCSB PDBmolecule conformation generation,binding pose predictionTransPolymer (Xu et al., 2023a) Table A4: Summary of LLMs in chemistry and materials science. "L+G+V": Language + Graph + Vision; "KG": Molecule -∼RoBERTa PI1M MLM regression polyBERT Molecule 86M DeBERTa density functional theory, MLM, regression(Kuenneth and Ramprasad, 2023) knowledge graph; "SMILES": simplified molecular-input line-entry system. Other notations have the same meaning experiments regression MFBERT Molecule -∼RoBERTa GDB-13, ZINC, MLM classification, regression, as in previous tables. (Abdel-Aty and Gould, 2022) PubChem, ChEMBL, virtual screeningUSPTOModel SPMM (Chang and Ye, 2024)Modality MoleculeSize -Architecture ∼BERTPre-training Data PubChemPre-training Task(s) next token prediction, SMILES-propertyclassification, regression, Evaluation Task(s) reaction prediction, retrosynthesis,ChemBERT (Guo et al., 2022)L110MBERTchemistry journalsMLM matchingNER SMILES-to-property generation,MatSciBERT (Gupta et al., 2022)L110MBERTScienceDirectMLMNER, RE, classification property-to-SMILES generationMatBERT (Trewartha et al., 2022) BARTSmiles (Chilingaryan et al., 2024)L Molecule110M 406MBERT BARTmaterials science journals ZINCMLM sequence to sequenceNER classification, regression,BatteryBERT (Huang and Cole, 2022)L110MBERTElsevier, Springer, RSCMLMQA, classification reaction prediction, retrosynthesisMaterialsBERT (Shetty et al., 2023) MolGen (Fang et al., 2024b)L Molecule110M 406MBERT BARTmaterials science journals ZINC, NPASSMLM, NSP sequence to sequence,NER molecule generationRecycle-BERT (Kumar et al., 2023)L110M 7BBERT LLaMAplastic recycling articlesclassification prefix tuningQA, classificationCatBERTa (Ock et al., 2023) SELFormer (Yüksel et al., 2023)L Molecule125M 58M, 87MRoBERTa ∼RoBERTaOC20 ChEMBLregression MLMregression classification, regressionLLM-Prop (Rubungo et al., 2023) PolyNC (Qiu et al., 2024a)L Molecule37M 220MT5 (encoder) T5Materials Project density functional theory,classification, regression sequence to sequenceclassification, regression classification, regressionexperiments</p>
<p>Table</p>
<p>AcknowledgmentsResearch was supported in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329.Any opinions, findings, and conclusions or recommendations expressed herein are those of the authors and do not necessarily represent the views, either expressed or implied, of DARPA or the U.S. Government.Gencode: the reference human genome annotation for the encode project.Dawn Song, and Jacob Steinhardt.  2021a. Measuring massive multitask language understanding.In ICLR'21.vey of pre-trained language models for processing scientific text.arXiv preprint arXiv:2401.17824.Zhi Hong, Aswathy Ajith, James Pauloski, Eamon Duede, Kyle Chard, and Ian Foster.2023.The diminishing returns of masked language models to science.In Findings of ACL'23, pages 1270-1283.ConVIRT(Zhang et al.,
Large-scale distributed training of transformers for chemical fingerprinting. Hisham Abdel, -Aty , Ian R Gould, Journal of Chemical Information and Modeling. 62202022</p>
<p>Prot2text: Multimodal protein's function generation with gnns and transformers. Michail Hadi Abdine, Chatzianastasis, AAAI'24. 2024Costas Bouyioukos, and Michalis Vazirgiannis</p>
<p>Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Hippocrates: An open-source framework for advancing large language models in healthcare. Osman Emre Can Acikgoz, Rayene Batur İnce, Bench, arXiv:2404.16621Arda Anıl Boz, İlker Kesen, Aykut Erdem, and Erkut Erdem2024arXiv preprint</p>
<p>Informative rna base embedding for rna structural alignment and clustering by deep representation learning. Manato Akiyama, Yasubumi Sakakibara, NAR Genomics and Bioinformatics. 41122022</p>
<p>Publicly available clinical bert embeddings. Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, Matthew Mcdermott, Proceedings of the 2nd Clinical Natural Language Processing Workshop. the 2nd Clinical Natural Language Processing Workshop2019</p>
<p>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, Hannaneh Hajishirzi, NAACL'19. 2019</p>
<p>Construction of the literature graph in semantic scholar. Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, NAACL'18. 2018</p>
<p>Keith T Luis M Antunes, Ricardo Butler, Grau-Crespo, arXiv:2307.04340Crystal structure generation with autoregressive large language modeling. 2023arXiv preprint</p>
<p>Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>Sören Arlt, Haonan Duan, Felix Li, Sang Michael Xie, Yuhuai Wu, Mario Krenn, arXiv:2406.02470Meta-designing quantum experiments with language models. 2024arXiv preprint</p>
<p>Llemma: An open language model for mathematics. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Mcaleer, Albert Q Jiang, Jia Deng, Stella Biderman, Sean Welleck, ICLR'242024</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, arXiv:2311.16079Meditron-70b: Scaling medical pretraining for large language models. 2023earXiv preprint</p>
<p>Multi-modal masked autoencoders for medical vision-and-language pre-training. Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, Tsung-Hui Chang, MICCAI'22. 2022c</p>
<p>Align, reason and learn: Enhancing medical visionand-language pre-training with knowledge. Zhihong Chen, Guanbin Li, Xiang Wan, ACM MM'22. 2022d</p>
<p>Prior: Prototype representation joint learning from medical images and reports. Pujin Cheng, Li Lin, Junyan Lyu, Yijin Huang, Wenhan Luo, Xiaoying Tang, CVPR'23. 2023</p>
<p>Fortap: Using formulas for numerical-reasoning-aware table pretraining. Zhoujun Cheng, Haoyu Dong, Ran Jia, Pengfei Wu, Shi Han, Fan Cheng, Dongmei Zhang, ACL'22. 2022</p>
<p>Bartsmiles: Generative masked language models for molecular representations. Hovhannes Gayane Chilingaryan, Ani Tamoyan, Nelly Tevosyan, Lusine Babayan, Karen Khondkaryan, Zaven Hambardzumyan, Hrant Navoyan, Armen Khachatrian, Aghajanyan, Journal of Chemical Information and Modeling. 64152024</p>
<p>Chemberta: large-scale self-supervised pretraining for molecular property prediction. Seyone Chithrananda, Gabriel Grand, Bharath Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>An introduction to wu's method for mechanical theorem proving in geometry. Shang-Ching Chou, Journal of Automated Reasoning. 431988</p>
<p>Single-sequence protein structure prediction using a language model and deep learning. Ratul Chowdhury, Nazim Bouatta, Surojit Biswas, Christina Floristean, Anant Kharkar, Koushik Roy, Charlotte Rochereau, Gustaf Ahdritz, Joanna Zhang, George M Church, Nature Biotechnology. 40112022</p>
<p>Unifying molecular and textual representations via multi-task language modelling. Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, Matteo Manica, ICML'23. 2023</p>
<p>A 5' utr language model for decoding untranslated regions of mrna and function predictions. Yanyi Chu, Dan Yu, Yupeng Li, Kaixuan Huang, Yue Shen, Le Cong, Jason Zhang, Mengdi Wang, Nature Machine Intelligence. 642024</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Structural scaffolds for citation intent classification in scientific publications. Arman Cohan, Waleed Ammar, Madeleine Van Zuylen, Field Cady, NAACL'19. 2019</p>
<p>Specter: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, The 1000 Genomes Project Consortium. 2020. 2015526A global reference for human genetic variation</p>
<p>Rnacentral: a hub of information for non-coding rna sequences. Nucleic Acids Research. 47D12019</p>
<p>scgpt: toward building a foundation model for single-cell multi-omics using generative ai. Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, Bo Wang, Nature Methods. 2182024</p>
<p>The nucleotide transformer: Building and evaluating robust foundation models for human genomics. Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P De Almeida, Hassan Sirelkhatim, bioRxiv. 2023</p>
<p>Marg: Multi-agent review generation for scientific papers. D' Mike, Tom Arcy, Larry Hope, Doug Birnbaum, Downey, arXiv:2401.042592024arXiv preprint</p>
<p>Gakg: A multimodal geoscience academic knowledge graph. Cheng Deng, Yuting Jia, Hui Xu, Chong Zhang, Jingyao Tang, Luoyi Fu, Weinan Zhang, Haisong Zhang, Xinbing Wang, Chenghu Zhou, CIKM'21. 2021</p>
<p>Cheng Deng, Bo Tong, Luoyi Fu, Jiaxin Ding, Dexing Cao, Xinbing Wang, Chenghu Zhou, arXiv:2304.00592Pk-chat: Pointer network guided knowledge driven generative dialogue model. 2023arXiv preprint</p>
<p>K2: A foundation language model for geoscience knowledge understanding and utilization. Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Yi Xu, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, WSDM'24. 2024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL'19. 2019</p>
<p>Mgeo: Multimodal geographic language model pre-training. Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang Zhang, Yao Xu, SIGIR'23. 2023</p>
<p>Ncbi disease corpus: a resource for disease name recognition and concept normalization. Rezarta Islamaj Dogan, Robert Leaman, Zhiyong Lu, Journal of Biomedical Informatics. 472014</p>
<p>Translation between molecules and natural language. Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, Heng Ji, EMNLP'22. 2022</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. Carl Edwards, Chengxiang Zhai, Heng Ji, EMNLP'21. 2021</p>
<p>Ankh: Optimized protein language model unlocks general-purpose modelling. Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost, arXiv:2301.065682023arXiv preprint</p>
<p>Prottrans: Toward understanding the language of life through self-supervised learning. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, IEEE TPAMI. 44102021</p>
<p>Overview of the coupled model intercomparison project phase 6 (cmip6) experimental design and organization. Veronika Eyring, Sandrine Bony, Gerald A Meehl, Catherine A Senior, Bjorn Stevens, Ronald J Stouffer, Karl E Taylor, Geoscientific Model Development. 20169</p>
<p>Molecular representation learning with language models and domain-relevant auxiliary tasks. Thomas Benedek Fabian, Héléna Edlich, Joshua Gaspar, Marco Meyers, Mohamed Fiscato, Ahmed, arXiv:2011.13230Marwin Segler,. 2020arXiv preprint</p>
<p>Mol-instructions: A large-scale biomolecular instruction dataset for large language models. Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Hua , ICLR'24jun Chen. 2024a</p>
<p>Domainagnostic molecular generation with chemical feedback. Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, Huajun Chen, ICLR'242024b</p>
<p>Controllable protein design with language models. Noelia Ferruz, Birte Höcker, Nature Machine Intelligence. 462022</p>
<p>Protgpt2 is a deep unsupervised language model for protein design. Noelia Ferruz, Steffen Schmidt, Birte Höcker, Nature Communications. 13143482022</p>
<p>Genalm: A family of open-source foundational dna language models for long sequences. Veniamin Fishman, Yuri Kuratov, Maxim Petrov, Aleksei Shmelev, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, Mikhail Burtsev, bioRxiv. 2023</p>
<p>. David Fitzek, Yi Hong Teoh, Pok Hin, Gebremedhin A Fung, Ejaaz Dagnew, Schuyler Merali, Benjamin Moss, Roger G Maclellan, Melko, arXiv:2405.210522024Rydberggpt. arXiv preprint</p>
<p>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as xyz, cif, and pdb files. Daniel Flam, - Shepherd, Alán Aspuru-Guzik, arXiv:2305.057082023arXiv preprint</p>
<p>Language models can learn complex molecular distributions. Daniel Flam-Shepherd, Kevin Zhu, Alán Aspuru-Guzik, Nature Communications. 13132932022</p>
<p>Panglaodb: a web server for exploration of mouse and human single-cell rna sequencing data. Oscar Franzén, Li-Ming Gan, Johan Lm Björkegren, Database. 462019. 2019</p>
<p>Neural scaling of deep chemical models. Ryan Nathan C Frey, Simon Soklaski, Siddharth Axelrod, Rafael Samsi, Connor W Gomez-Bombarelli, Vijay Coley, Gadepally, Nature Machine Intelligence. 5112023</p>
<p>G-llava: Solving geometric problem with multi-modal large language model. Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, arXiv:2312.113702023arXiv preprint</p>
<p>The chembl database in 2017. Anna Gaulton, Anne Hersey, Michał Nowotka, Patricia Bento, Jon Chambers, David Mendez, Prudence Mutowo, Francis Atkinson, Louisa J Bellis, Elena Cibrián-Uhalte, Nucleic Acids Research. 45D12017</p>
<p>Injecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, ACL'20. 2020</p>
<p>Mammo-clip: A vision language foundation model to enhance data efficiency and robustness in mammography. Shantanu Ghosh, Clare B Poynton, Shyam Visweswaran, Kayhan Batmanghelich, arXiv:2405.122552024arXiv preprint</p>
<p>Enhancing representation in radiography-reports foundation model: A granular alignment algorithm using masked contrastive learning. Weijian Huang, Cheng Li, Hong-Yu Zhou, Hao Yang, Jiarun Liu, Yong Liang, Hairong Zheng, Shaoting Zhang, Shanshan Wang, Nature Communications. 15176202024b</p>
<p>A visuallanguage foundation model for pathology image analysis using medical twitter. Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, James Zou, Nature Medicine. 2992023</p>
<p>Tabbie: Pretrained representations of tabular data. Hiroshi Iida, Dung Thai, Varun Manjunatha, Mohit Iyyer, NAACL'21. 2021</p>
<p>Quilt-1m: One million image-text pairs for histopathology. Wisdom Oluchi Ikezogwo, Mehmet Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Stefan, Chan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, Linda Shapiro, NeurIPS'23. 2023</p>
<p>Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, AAAI'19. 2019</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, Esben Jannik Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>Leveraging large language models for predictive chemistry. Kevin Maik, Jablonka , Philippe Schwaller, Andres Ortega-Guerrero, Berend Smit, Nature Machine Intelligence. 622024</p>
<p>Commentary: The materials project: A materials genome approach to accelerating materials innovation. Anubhav Jain, Ping Shyue, Geoffroy Ong, Wei Hautier, William Davidson Chen, Stephen Richards, Shreyas Dacek, Dan Cholia, David Gunter, Gerbrand Skinner, Ceder, APL materials. 112013</p>
<p>Dnabert: pre-trained bidirectional encoder representations from transformers model for dnalanguage in genome. Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri, Bioinformatics. 37152021</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Omnitab: Pretraining with natural and synthetic data for few-shot tablebased question answering. Zhengbao Jiang, Yi Mao, NAACL'22. 2022Pengcheng He, Graham Neubig, and Weizhu Chen</p>
<p>Learning to reason deductively: Math word problem solving as complex relation extraction. Zhanming Jie, Jierui Li, Wei Lu, ACL'22. 2022</p>
<p>Gang Bowen Jin, Chi Liu, Meng Han, Heng Jiang, Jiawei Ji, Han, arXiv:2312.02783Large language models on graphs: A comprehensive survey. 2023aarXiv preprint</p>
<p>Graph chain-of-thought: Augmenting large language models by reasoning on graphs. Chulin Bowen Jin, Jiawei Xie, Kashob Zhang, Yu Kumar Roy, Zheng Zhang, Ruirui Li, Xianfeng Li, Suhang Tang, Yu Wang, Jiawei Meng, Han, Findings of ACL'24. 2024</p>
<p>Patton: Language model pretraining on text-rich networks. Wentao Bowen Jin, Yu Zhang, Yu Zhang, Xinyang Meng, Qi Zhang, Jiawei Zhu, Han, ACL'23. 2023b</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, Peter Szolovits, Applied Sciences. 111464212021</p>
<p>Probing biomedical embeddings from language models. Qiao Jin, Bhuwan Dhingra, William Cohen, Xinghua Lu, Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP. the 3rd Workshop on Evaluating Vector Space Representations for NLP2019</p>
<p>Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval. Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, John Wilbur, Zhiyong Lu, Bioinformatics. 39116512023c</p>
<p>Predicting organic reaction outcomes with weisfeiler-lehman network. Wengong Jin, Connor W Coley, Regina Barzilay, Tommi Jaakkola, NIPS'17. 2017</p>
<p>Mimic-iv, a freely accessible electronic health record dataset. Lucas Alistair Ew Johnson, Lu Bulgarelli, Alvin Shen, Ayad Gayles, Steven Shammout, Tom J Horng, Sicheng Pollard, Benjamin Hao, Brian Moody, Gow, Scientific Data. 10112023</p>
<p>Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. Tom J Alistair Ew Johnson, Seth J Pollard, Nathaniel R Berkowitz, Greenbaum, Chihying Matthew P Lungren, Roger G Deng, Steven Mark, Horng, Scientific Data. 613172019</p>
<p>Mimic-iii, a freely accessible critical care database. Tom J Alistair Ew Johnson, Lu Pollard, Li-Wei H Shen, Mengling Lehman, Mohammad Feng, Benjamin Ghassemi, Peter Moody, Leo Szolovits, Roger G Anthony Celi, Mark, Scientific Data. 312016</p>
<p>Dense passage retrieval for opendomain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-Tau Yih, EMNLP'20. 2020</p>
<p>Mmbert: Multimodal bert pretraining for improved medical vqa. Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, Deva Priyakumar, Jawahar, ISBI'21. 2021</p>
<p>Transparent medical image ai via an image-text foundation model grounded in medical literature. Chanwoo Kim, U Soham, Alex J Gadgil, Jesutofunmi A Degrave, Zhuo Ran Omiye, Roxana Cai, Su-In Daneshjou, Lee, Nature Medicine. 3042024</p>
<p>Pubchem 2019 update: improved access to chemical data. Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, Nucleic Acids Research. 47D12019</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, AAAI'17. 2017</p>
<p>Selfreferencing embedded strings (selfies): A 100% robust molecular string representation. Mario Krenn, Florian Häse, Akshatkumar Nigam, Machine Learning: Science and Technology. 14450242020Pascal Friederich, and Alan Aspuru-Guzik</p>
<p>polybert: a chemical language model to enable fully machine-driven ultrafast polymer informatics. Christopher Kuenneth, Rampi Ramprasad, Nature Communications. 14140992023</p>
<p>Recycle-bert: extracting knowledge about plastic waste recycling by natural language processing. Avan Kumar, Manojkumar Bhavik R Bakshi, Hariprasad Ramteke, Kodamana, ACS Sustainable Chemistry &amp; Engineering. 11322023</p>
<p>Biomistral: A collection of opensource pretrained large language models for medical domains. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, Richard Dufour, Findings of ACL'24. 2024</p>
<p>A search engine for discovery of scientific challenges and directions. Dan Lahav, Jon Saad Falcon, Bailey Kuehl, Sophie Johnson, Sravanthi Parasa, Noam Shomron, Horng Duen, Diyi Chau, Eric Yang, Horvitz, Daniel S Weld, AAAI'22. 2022</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>A large public corpus of web tables containing time and context metadata. Oliver Lehmberg, Dominique Ritze, Robert Meusel, Christian Bizer, WWW'16. 2016</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, ACL'20. Bart2020a</p>
<p>Pretrained language models for biomedical and clinical tasks: understanding and extending the state-of-the-art. Patrick Lewis, Myle Ott, Jingfei Du, Veselin Stoyanov, Proceedings of the 3rd Clinical Natural Language Processing Workshop. the 3rd Clinical Natural Language Processing Workshop2020b</p>
<p>Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, NeurIPS'22. </p>
<p>Llava-med: Training a large language-and-vision assistant for biomedicine in one day. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, Jianfeng Gao, 2023aIn NeurIPS'23</p>
<p>Finetuning bidirectional encoder representations from transformers (bert)-based models on large-scale electronic health record notes: an empirical study. Fei Li, Yonghao Jin, Weisong Liu, Bhanu Pratap Singh Rawat, Pengshan Cai, Hong Yu, JMIR Medical Informatics. 73e148302019</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML'23. 2023b</p>
<p>Automated statistical model discovery with language models. Emily B Michael Y Li, Noah D Fox, Goodman, ICML'24. 2024a</p>
<p>Table-gpt: Table fine-tuned gpt for diverse table tasks. Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, Surajit Chaudhuri, Proceedings of the ACM on Management of Data. 232024b</p>
<p>Masked vision and language pretraining with unimodal and multimodal contrastive losses for medical visual question answering. Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong, MICCAI'23. 2023c</p>
<p>Tat-Seng Chua, and Qi Tian. 2024c. Towards 3d molecule-text interpretation in language models. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, ICLR'24</p>
<p>Codonbert: Large language models for mrna design and optimization. Sizhen Li, Saeed Moayedpour, Ruijiang Li, Michael Bailey, Saleh Riahi, Lorenzo Kogler-Anele, Milad Miladi, Jacob Miner, Dinghai Zheng, bioRxiv. Jun Wang, et al. 2023d</p>
<p>Behrt: transformer for electronic health records. Yikuan Li, Shishir Rao, José Roberto Ayala, Abdelaali Solares, Rema Hassaine, Dexter Ramakrishnan, Yajie Canoy, Kazem Zhu, Gholamreza Rahimi, Salimi-Khorshidi, Scientific Reports. 10171552020</p>
<p>Clinical-longformer and clinical-bigbird: Transformers for long clinical sequences. Yikuan Li, Ramsey M Wehbe, S Faraz, Hanyin Ahmad, Yuan Wang, Luo, arXiv:2201.118382022aarXiv preprint</p>
<p>Chatdoctor: A medical chat model fine-tuned on a large language model meta-ai (llama) using medical domain knowledge. Yunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, Steve Jiang, You Zhang, Cureus. 6152023e</p>
<p>Spabert: A pretrained language model from geographic data for geo-entity representation. Zekun Li, Jina Kim, Yao-Yi Chiang, Muhao Chen, Findings of EMNLP'22. 2022b</p>
<p>Geolm: Empowering language models for geospatially grounded language understanding. Zekun Li, Wenxuan Zhou, Yao-Yi Chiang, Muhao Chen, EMNLP'23. 2023f</p>
<p>Seeking patterns, not just memorizing procedures: Contrastive learning for solving math word problems. Zhongli Li, Wenxuan Zhang, Chao Yan, Qingyu Zhou, Chao Li, Hongzhi Liu, Yunbo Cao, Findings of ACL'22. 2022c</p>
<p>Monitoring ai-modified content at scale: A case study on the impact of chatgpt on ai conference peer reviews. Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, ICML'242024a</p>
<p>Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji, Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, arXiv:2404.01268Mapping the increasing use of llms in scientific papers. 2024barXiv preprint</p>
<p>Can large language models provide useful feedback on research papers? a large-scale empirical analysis. Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Scott Daniel, Yian Smith, Yin, NEJM AI. 1824001962024c</p>
<p>Scemqa: A scientific college entrance level multimodal question answering benchmark. Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao, Renjie Pi, Jipeng Zhang, Xiangliang Zhang, ACL'24. 2024d</p>
<p>Unimath: A foundational and multimodal mathematical reasoner. Zhenwen Liang, Tianyu Yang, Jipeng Zhang, Xiangliang Zhang, EMNLP'23. 2023</p>
<p>Mwp-bert: Numeracy-augmented pre-training for math word problem solving. Zhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin, Yunshi Lan, Jie Shao, Xiangliang Zhang, Findings of NAACL'22. 2022</p>
<p>Panacea: A foundation model for clinical trial search, summarization, design, and recruitment. Jiacheng Lin, Hanwen Xu, Zifeng Wang, Sheng Wang, Jimeng Sun, arXiv:2407.110072024aarXiv preprint</p>
<p>Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár, ICCV'17. 2017</p>
<p>Pmc-clip: Contrastive language-image pre-training using biomedical documents. Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, Weidi Xie, MICCAI'23. 2023a</p>
<p>Evolutionary-scale prediction of atomic-level protein structure with a language model. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Science. 37966372023b</p>
<p>Rho-1: Not all tokens are what you need. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, arXiv:2404.079652024barXiv preprint</p>
<p>Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, arXiv:2401.00434Geogalactica: A scientific large language model in geoscience. 2024carXiv preprint</p>
<p>Rapid and sensitive protein similarity searches. J David, Lipman, William R Pearson, Science. 22746931985</p>
<p>Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering. Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, Xiao-Ming Wu, ISBI'212021a</p>
<p>M-flag: Medical visionlanguage pre-training with frozen language models and latent space geometry optimization. Che Liu, Sibo Cheng, Chen Chen, Mengyun Qiao, Weitong Zhang, Anand Shah, Wenjia Bai, Rossella Arcucci, MIC- CAI'232023a</p>
<p>Self-alignment pretraining for biomedical entity representations. Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier, NAACL'21. 2021b</p>
<p>Qilin-med-vl: Towards chinese large vision-language model for general healthcare. Junling Liu, Ziming Wang, Qichen Ye, Dading Chong, Peilin Zhou, Yining Hua, arXiv:2310.179562023barXiv preprint</p>
<p>Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Pengfei Liu, Yiming Ren, Computers in Biology and Medicine. 171108073Jun Tao, and Zhixiang Ren. 2024a</p>
<p>Tapex: Table pre-training via learning a neural sql executor. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, Jian-Guang Lou, ICLR'222022a</p>
<p>Ryan Liu, Nihar B Shah ; Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, arXiv:2306.00622arXiv:2302.04611Reviewergpt? an exploratory study on using large language models for paper reviewing. 2023. 2023carXiv preprintA text-guided protein design framework</p>
<p>Multimodal molecule structure-text model for text-based retrieval and editing. Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Animashree Anandkumar, Nature Machine Intelligence. 5122023d</p>
<p>Conversational drug editing using retrieval and domain feedback. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, Chaowei Xiao, ICLR'242024b</p>
<p>Oag-bert: Towards a unified backbone language model for academic knowledge services. Xiao Liu, Da Yin, Jingnan Zheng, Xingjian Zhang, Peng Zhang, Hongxia Yang, Yuxiao Dong, Jie Tang, KDD'22. 2022b</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, Tat-Seng Chua, EMNLP'23. 2023e</p>
<p>S2orc: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel S Weld, ACL'20. 2020</p>
<p>Unified deep learning model for multitask reaction predictions with explanation. Jieyu Lu, Yingkai Zhang, Journal of Chemical Information and Modeling. 6262022</p>
<p>Visual language pretrained multiple instance zeroshot transfer for histopathology images. Ming Y Lu, Bowen Chen, Andrew Zhang, Richard J Drew Fk Williamson, Tong Chen, Long Ding, Yung-Sung Phi Le, Faisal Chuang, Mahmood, CVPR'23. 2023</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao, ICLR'242024</p>
<p>Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, Song-Chun Zhu, ACL'21. 2021</p>
<p>Pubmed and beyond: a survey of web tools for searching biomedical literature. Zhiyong Lu, Database. 362011. 2011</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, EMNLP'18. 2018</p>
<p>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023aarXiv preprint</p>
<p>Taiyi: a bilingual fine-tuned large language model for diverse biomedical tasks. Ling Luo, Jinzhong Ning, Yingwen Zhao, Zhijun Wang, Zeyuan Ding, Peng Chen, Weiru Fu, Qinyu Han, Guangtao Xu, Yunzhi Qiu, JAMIA. 3192024</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in Bioinformatics. 2364092022</p>
<p>Yizhen Luo, Kai Yang, Massimo Hong, Xingyi Liu, Zaiqing Nie, arXiv:2307.09484Molfm: A multimodal molecular foundation model. 2023barXiv preprint</p>
<p>Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, Zaiqing Nie, arXiv:2308.094422023carXiv preprint</p>
<p>Explaining relationships between scientific documents. Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle Lo, Isabel Cachola, Noah A Smith, ACL'21. 2021</p>
<p>Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian, arXiv:2402.16445Prollama: A protein large language model for multi-task protein language processing. 2024arXiv preprint</p>
<p>Large language models generate functional protein sequences across diverse families. Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos, Caiming Xiong, Zachary Z Sun, Richard Socher, Nature Biotechnology. 4182023</p>
<p>Xin Man, Chenghong Zhang, Jin Feng, Changyu Li, Jie Shao, arXiv:2304.08754W-mae: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. 2023arXiv preprint</p>
<p>Jacek Tabor, and Stanisław Jastrzębski. Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, arXiv:2002.08264Molecule attention transformer. 2020arXiv preprint</p>
<p>Relative molecule self-attention transformer. Łukasz Maziarka, Dawid Majchrowski, Tomasz Danel, Piotr Gaiński, Jacek Tabor, Igor Podolak, Paweł Morkisz, Stanisław Jastrzębski, Journal of Cheminformatics. 16132024</p>
<p>Language models enable zero-shot prediction of the effects of mutations on protein function. Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, Alex Rives, NeurIPS'21. 2021</p>
<p>Bidirectional representation learning from transformers using multimodal electronic health record data to predict depression. Yiwen Meng, William Speier, Michael K Ong, Corey W Arnold, IEEE Journal of Biomedical and Health Informatics. 2582021a</p>
<p>Mixture-ofpartitions: Infusing large biomedical knowledge graphs into bert. Zaiqiao Meng, Fangyu Liu, Thomas Clark, Ehsan Shareghi, Nigel Collier, EMNLP'21. 2021b</p>
<p>Electramed: a new pre-trained language representation model for biomedical nlp. Giacomo Miolo, Giulio Mantoan, Carlotta Orsenigo, arXiv:2104.095852021arXiv preprint</p>
<p>Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu, Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Mohammad Amir, Maximilian Elahi, Greiner, arXiv:2404.01475Are large language models superhuman chemists?. 2024arXiv preprint</p>
<p>Lila: A unified benchmark for mathematical reasoning. Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, EMNLP'22. 2022</p>
<p>Multimodal understanding and generation for medical images and text via vision-language pre-training. Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, Edward Choi, IEEE JBHI. 26122022</p>
<p>Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, ML4H'23. 2023Med-flamingo: a multimodal medical fewshot learner</p>
<p>Earthquake transformer-an attentive deeplearning model for simultaneous earthquake detection and phase picking. William L S Mostafa Mousavi, Weiqiang Ellsworth, Lindsay Y Zhu, Gregory C Chuang, Beroza, Nature Communications. 11139522020</p>
<p>Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter. Martin Müller, Marcel Salathé, Per E Kummervold, Frontiers in Artificial Intelligence. 610232812023</p>
<p>Joint learning of localized representations from medical images and reports. Philip Müller, Georgios Kaissis, Congyu Zou, Daniel Rueckert, ECCV'22. 2022</p>
<p>Multi-vector models with textual guidance for finegrained scientific document similarity. Sheshera Mysore, Arman Cohan, Tom Hope, NAACL'22. 2022</p>
<p>Benchmarking for biomedical natural language processing tasks with a domain specific albert. Usman Naseem, Matloob Adam G Dunn, Jinman Khushi, Kim, BMC Bioinformatics. 2311442022</p>
<p>Hyenadna: Long-range genomic sequence modeling at single nucleotide resolution. Eric Nguyen, Michael Poli, Marjan Faizi, Armin W Thomas, Michael Wornow, Callum Birch-Sykes, Stefano Massaroli, Aman Patel, Clayton M Rabideau, Yoshua Bengio, 2023aIn NeurIPS'23</p>
<p>Astrollama: Towards specialized foundation models in astronomy. Dung Tuan, Yuan-Sen Nguyen, Ioana Ting, Ciuca, O' Charles, Ze-Chang Neill, Maja Sun, Sandor Jabłońska, Ernest Kruk, Jack Perkowski, Jason Miller, Jason Jingsh Li, Proceedings of the Second Workshop on Information Extraction from Scientific Publications. the Second Workshop on Information Extraction from Scientific Publications2023b</p>
<p>Climax: A foundation model for weather and climate. Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh K Gupta, Aditya Grover, ICML'23. 2023c</p>
<p>Progen2: exploring the boundaries of protein language models. Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, Ali Madani, Cell Systems. 14112023</p>
<p>A symbolic characters aware model for solving geometry problems. Maizhen Ning, Qiu-Feng Wang, Kaizhu Huang, Xiaowei Huang, ACM MM'23. 2023</p>
<p>Catalyst energy prediction with catberta: Unveiling feature exploration strategies through large language models. Janghoon Ock, Chakradhar Guntuboina, Amir Barati, Farimani , ACS Catalysis. 13242023</p>
<p>Neighborhood contrastive learning for scientific document representations with citation embeddings. Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm, EMNLP'22. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, NeurIPS'22. 2022</p>
<p>On the effectiveness of small, discriminatively pre-trained language representation models for biomedical text mining. Ibrahim Burak, Ozyurt , Proceedings of the First Workshop on Scholarly Document Processing. the First Workshop on Scholarly Document Processing2020</p>
<p>Medmcqa: A large-scale multisubject multi-choice dataset for medical domain question answering. Ankit Pal, Logesh Kumar Umapathi, Malaikannan Sankarasubbu, CHIL'22. 2022</p>
<p>Compositional semantic parsing on semi-structured tables. Panupong Pasupat, Percy Liang, ACL'15. 2015</p>
<p>Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, arXiv:2202.11214Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators. 2022arXiv preprint</p>
<p>Qizhi Pei, Lijun Wu, Kaiyuan Gao, Jinhua Zhu, Yue Wang, Zun Wang, Tao Qin, Rui Yan, arXiv:2403.01528Leveraging biomolecule and natural language through multi-modal learning: A survey. 2024arXiv preprint</p>
<p>Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan, EMNLP'23. 2023</p>
<p>Radiology objects in context (roco): a multimodal image dataset. Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, Christoph M Friedrich, 7th Joint International Workshop, CVII-STENT and 3rd International Workshop, LABELS, Held in Conjunction with MICCAI'18. 2018</p>
<p>Xplainer: From x-ray observations to explainable zero-shot diagnosis. Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Petra Jiraskova, MICCAI'23. Springer2023Rickmer Braren, and Nassir Navab</p>
<p>Transfer learning in biomedical natural language processing: An evaluation of bert and elmo on ten benchmarking datasets. Yifan Peng, Shankai Yan, Zhiyong Lu, Proceedings of the 18th BioNLP Workshop and Shared Task. the 18th BioNLP Workshop and Shared Task2019</p>
<p>Astrollama-chat: Scaling astrollama with conversational and diverse datasets. Ernest Perkowski, Rui Pan, Dung Tuan, Yuan-Sen Nguyen, Sandor Ting, Tong Kruk, Charlie O' Zhang, Maja Neill, Zechang Jablonska, Michael J Sun, Smith, Research Notes of the AAS. 8172024</p>
<p>James T Long N Phan, Hieu Anibal, Shaurya Tran, Chanana, arXiv:2106.03598Erol Bahadroglu, Alec Peltekian, and Grégoire Altan-Bonnet. 2021. Scifive: a text-to-text transformer model for biomedical literature. arXiv preprint</p>
<p>Sara Pieri, Sahal Shaji Mullappilly, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, Timothy Baldwin, Hisham Cholakkal, arXiv:2402.13253Bimedix: Bilingual medical mixture of experts llm. 2024arXiv preprint</p>
<p>Polync: a natural and chemical language model for the prediction of unified polymer properties. Haoke Qiu, Lunyang Liu, Xuepeng Qiu, Xuemin Dai, Xiangling Ji, Zhao-Yan Sun, Chemical Science. 1522024a</p>
<p>Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2402.13963Towards building multilingual language model for medicine. 2024barXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML'21. 2021</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, JMLR. 211402020</p>
<p>Quantum chemistry structures and properties of 134 kilo molecules. Raghunathan Ramakrishnan, Matthias Pavlo O Dral, O Rupp, Von Anatole, Lilienfeld, Scientific Data. 112014</p>
<p>Bayesian optimization of catalysts with in-context learning. Shane S Mayk Caldas Ramos, Marc D Michtavy, Andrew D Porosoff, White, arXiv:2304.053412023arXiv preprint</p>
<p>Jason Roshan M Rao, Robert Liu, Joshua Verkuil, John Meier, Pieter Canny, Tom Abbeel, Alexander Sercu, Rives, Msa transformer. In ICML'21. 2021</p>
<p>Med-bert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. Laila Rasmy, Yang Xiang, Ziqian Xie, Cui Tao, Degui Zhi, Digital Medicine. 41862021</p>
<p>Opensource platform to benchmark fingerprints for ligandbased virtual screening. Sereina Riniker, Gregory A Landrum, Journal of Cheminformatics. 51262013</p>
<p>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, Lawrence Zitnick, Jerry Ma, PNAS. 11815e20162391182021</p>
<p>Mathematical discoveries from program search with large language models. Alexey Romanov, Chaitanya Shivade ; Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco Jr Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, ACL'18. Bernardino Romera-Paredes2018. 2024625Lessons from natural language inference in the clinical domain</p>
<p>Large-scale chemical language representations capture molecular structure and properties. Jerret Ross, Brian Belgodere, Inkit Vijil Chenthamarakshan, Youssef Padhi, Payel Mroueh, Das, Nature Machine Intelligence. 4122022</p>
<p>Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. Andre Niyongabo Rubungo, Craig Arnold, Barry P Rand, Adji Bousso, Dieng , arXiv:2310.140292023arXiv preprint</p>
<p>Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, arXiv:2404.18416Capabilities of gemini models in medicine. 2024arXiv preprint</p>
<p>Climatebert-netzero: Detecting and assessing net zero and reduction targets. Tobias Schimanski, Julia Bingler, Mathias Kraus, Camilla Hyslop, Markus Leippold, EMNLP'23. 2023</p>
<p>What's what: The (nearly) definitive guide to reaction role assignment. Nadine Schneider, Nikolaus Stiefl, Gregory A Landrum, Journal of Chemical Information and Modeling. 56122016</p>
<p>Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, Teodoro Laino, Science Advances. 715e41662021a</p>
<p>Mapping the space of chemical reactions using attention-based neural networks. Philippe Schwaller, Daniel Probst, Alain C Vaucher, H Vishnu, David Nair, Teodoro Kreutter, Jean-Louis Laino, Reymond, Nature Machine Intelligence. 322021b</p>
<p>Solving geometry problems: Combining text and diagram interpretation. Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, Clint Malcolm, EMNLP'15. 2015</p>
<p>Pre-training of graph augmented transformers for medication recommendation. Junyuan Shang, Tengfei Ma, Cao Xiao, Jimeng Sun, IJCAI'19. 2019</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Li, Daya Wu, Guo, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>Mathbert: A pre-trained language model for general nlp tasks in mathematics education. Jia Tracy Shen, Michiharu Yamashita, Ethan Prihar, Neil Heffernan, Xintao Wu, Ben Graff, Dongwon Lee, arXiv:2106.073402021arXiv preprint</p>
<p>A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing. Pranav Shetty, Arunkumar Chitteth Rajan, Chris Kuenneth, Sonakshi Gupta, Lakshmi Prerana Panchumarti, Lauren Holm, Chao Zhang, Rampi Ramprasad, Computational Materials. 91522023</p>
<p>Biomegatron: larger biomedical domain language model. Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, EMNLP'20. 2020</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Scirepeval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, EMNLP'23. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023a</p>
<p>Towards expert-level medical question answering with large language models. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, arXiv:2305.096172023barXiv preprint</p>
<p>An overview of microsoft academic service (mas) and applications. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, Kuansan Wang, WWW'15. 2015</p>
<p>Wu's method can boost symbolic ai to rival silver medalists and alphageometry to outperform gold medalists at imo geometry. Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat, Matthias Bethge, arXiv:2404.064052024arXiv preprint</p>
<p>Sciclops: Detecting and contextualizing scientific claims for assisting manual fact-checking. Panayiotis Smeros, Carlos Castillo, Karl Aberer, CIKM'21. 2021</p>
<p>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. Henry Sprueill, Carl Edwards, Mariefel Olarte, Udishnu Sanyal, Ji Heng, Sutanay Choudhury, Findings of EMNLP'23. 2023</p>
<p>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback. Carl Henry W Sprueill, Khushbu Edwards, Agarwal, Udishnu Mariefel V Olarte, Conrad Sanyal, Hongbin Johnston, Heng Liu, Sutanay Ji, Choudhury, ICML'24. 2024</p>
<p>Zinc 15-ligand discovery for everyone. Teague Sterling, John J Irwin, Journal of Chemical Information and Modeling. 55112015</p>
<p>Bioclip: A vision foundation model for the tree of life. Samuel Stevens, Jiaman Wu, Matthew J Thompson, Elizabeth G Campolongo, Chan Hee Song, David Edward Carlyn, Li Dong, M Wasila, Charles Dahdul, Tanya Stewart, Berger-Wolf, CVPR'24. 2024</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, Ji-Rong Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>Saprot: protein language modeling with structure-aware vocabulary. Jin Su, Chenchen Han, Yuyang Zhou, Junjie Shan, Xibin Zhou, Fajie Yuan, ICLR'242024</p>
<p>Medicat: A dataset of medical images, captions, and textual references. Sanjay Subramanian, Lucy Lu Wang, Sachin Mehta, Ben Bogin, Madeleine Van Zuylen, Sravanthi Parasa, Sameer Singh, Matt Gardner, Hannaneh Hajishirzi, Findings of EMNLP'20. 2020</p>
<p>Uniref clusters: a comprehensive and scalable alternative for improving sequence similarity searches. Yuqi Baris E Suzek, Hongzhan Wang, Peter B Huang, Cathy H Mcgarvey, Uniprot Wu, Consortium, Bioinformatics. 3162015</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Findings of ACL'23. 2023</p>
<p>Arnetminer: extraction and mining of academic social networks. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, Zhong Su, KDD'08. 2008</p>
<p>Georgios Kaissis, and Daniel Rueckert. 2023. Interactive and explainable regionguided radiology report generation. Tim Tanida, Philip Müller, CVPR'23. </p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Xraygpt: Chest radiographs summarization using medical vision-language models. Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Muhammad Rao, Salman Anwer, Jorma Khan, Fahad Laaksonen, Shahbaz Khan, Proceedings of the 23rd Workshop on Biomedical Natural Language Processing. the 23rd Workshop on Biomedical Natural Language Processing2024</p>
<p>Transfer learning enables predictions in network biology. Christina V Theodoris, Ling Xiao, Anant Chopra, Zeina R Al Mark D Chaffin, Matthew C Sayed, Helene Hill, Elizabeth M Mantineo, Zexian Brydon, X Zeng, Shirley Liu, Nature. 61879652023</p>
<p>Expertlevel detection of pathologies from unannotated chest x-ray images via self-supervised learning. Ekin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, Pranav Rajpurkar, Nature Biomedical Engineering. 6122022</p>
<p>Openmathinstruct-1: A 1.8 million math instruction tuning dataset. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, Igor Gitman, arXiv:2402.101762024arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023barXiv preprint</p>
<p>Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science. Amalie Trewartha, Nicholas Walker, Haoyan Huo, Sanghoon Lee, Kevin Cruse, John Dagdelen, Alexander Dunn, Kristin A Persson, Gerbrand Ceder, Anubhav Jain, Patterns. 432022</p>
<p>Solving olympiad geometry without human demonstrations. Yuhuai Trieu H Trinh, Wu, He Quoc V Le, Thang He, Luong, Nature. 62579952024</p>
<p>. Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, Towards generalist biomedical ai. NEJM AI. 1323001382024</p>
<p>Chatclimate: Grounding conversational ai in climate science. Saeid Ashraf Vaghefi, Dominik Stammbach, Veruska Muccione, Julia Bingler, Jingwei Ni, Mathias Kraus, Simon Allen, Chiara Colesanti-Senni, Tobias Wekhof, Tobias Schimanski, Communications Earth &amp; Environment. 414802023</p>
<p>Trec-covid: constructing a pandemic information retrieval test collection. Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, Lucy Lu, Wang , SIGIR Forum. 5412021</p>
<p>Shoya Wada, Toshihiro Takeda, Shiro Manabe, Shozo Konishi, Jun Kamohara, Yasushi Matsumura, arXiv:2005.07202Pre-training technique to localize medical bert and enhance biomedical bert. 2020arXiv preprint</p>
<p>Med-unic: Unifying cross-lingual medical vision-language pre-training by diminishing bias. Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, 2023César Quilodrán-Casas, and Rossella Arcucci. In NeurIPS'23</p>
<p>Pretrained language models in biomedical domain: A systematic survey. Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie Fu, ACM Computing Surveys. 5632023a</p>
<p>Towards automated urban planning: When generative and chatgpt-like ai meets urban planning. Dongjie Wang, Chang-Tien Lu, Yanjie Fu, arXiv:2304.038922023barXiv preprint</p>
<p>Multi-granularity cross-modal alignment for generalized medical visual representation learning. Fuying Wang, Yuyin Zhou, Shujun Wang, Varut Vardhanabhuti, Lequan Yu, NeurIPS'22. 2022a</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023c</p>
<p>Drg-llama: tuning llama model to predict diagnosis-related group for hospitalized patients. Hanyin Wang, Chufan Gao, Christopher Dantona, Bryan Hull, Jimeng Sun, Digital Medicine. 71162024a</p>
<p>Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, Ting Liu, arXiv:2304.06975Huatuo: Tuning llama model with chinese medical knowledge. 2023darXiv preprint</p>
<p>Efficient evolutionary search over chemical space with large language models. Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Streith-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, arXiv:2406.169762024barXiv preprint</p>
<p>Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li, ICLR'242024c</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, ACL'24. 2023e</p>
<p>Towards a human-computer collaborative scientific paper lifecycle: A pilot study and hands-on tutorial. Qingyun Wang, Carl Edwards, Heng Ji, Tom Hope, COLING'24. 2024d</p>
<p>Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang, arXiv:2407.03203Theoremllama: Transforming general-purpose llms into lean4 experts. 2024earXiv preprint</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang, ACM BCB'19. 2019</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, ICML'24. 2024f</p>
<p>Cmb: A comprehensive medical benchmark in chinese. Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, NAACL'24. 2024g</p>
<p>Deep neural solver for math word problems. Yan Wang, Xiaojiang Liu, Shuming Shi, EMNLP'17. 2017</p>
<p>Tuta: Treebased transformers for generally structured table pretraining. Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, Dongmei Zhang, KDD'21. 2021</p>
<p>Zifeng Wang, Lang Cao, Benjamin Danek, Yichi Zhang, Qiao Jin, Zhiyong Lu, Jimeng Sun, arXiv:2406.17755Accelerating clinical evidence synthesis with large language models. 2024harXiv preprint</p>
<p>Medclip: Contrastive learning from unpaired medical images and text. Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, Jimeng Sun, 2022bIn EMNLP'22</p>
<p>Lbert: Lexically aware transformer-based bidirectional encoder representation model for learning universal bio-entity relations. Neha Warikoo, Yung-Chun Chang, Wen-Lian Hsu, Bioinformatics. 3732021</p>
<p>Nicolas Webersinke, Mathias Kraus, arXiv:2110.12010Julia Anna Bingler, and Markus Leippold. 2021. Climatebert: A pretrained language model for climate-related text. arXiv preprint</p>
<p>2022a. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR'22M. Dai, and Quoc V. Le.</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, NeurIPS'22. 2022b</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, Journal of Chemical Information and Computer Sciences. 2811988</p>
<p>Crowdsourcing multiple choice science questions. Johannes Welbl, Nelson F Liu, Matt Gardner, Proceedings of the 3rd Workshop on Noisy Usergenerated Text. the 3rd Workshop on Noisy Usergenerated Text2017</p>
<p>Naturalprover: Grounded mathematical proof generation with language models. Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi, NeurIPS'22. 2022</p>
<p>Cellplm: Pre-training of cell language model beyond single cells. Hongzhi Wen, Wenzhuo Tang, Xinnan Dai, Jiayuan Ding, Wei Jin, Yuying Xie, Jiliang Tang, ICLR'242024</p>
<p>The future of chemistry is language. Andrew D White, Nature Reviews Chemistry. 772023</p>
<p>Pmc-llama: toward building open-source language models for medicine. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, Yanfeng Wang, JAMIA. 3192024</p>
<p>Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, Weidi Xie, arXiv:2308.02463Towards generalist foundation model for radiology. 2023arXiv preprint</p>
<p>Moleculenet: a benchmark for molecular machine learning. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, S Aneesh, Karl Pappu, Vijay Leswing, Pande, Chemical Science. 922018</p>
<p>A systematic survey of chemical pre-trained models. Jun Xia, Yanqiao Zhu, Yuanqi Du, Stan Z Li, IJCAI'23. 2023</p>
<p>Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Keloth, arXiv:2402.12749Me llama: Foundation large language models for medical applications. 2024arXiv preprint</p>
<p>Darwin series: Domain specific large language models for natural science. Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, arXiv:2308.135652023arXiv preprint</p>
<p>Benchmarking retrieval-augmented generation for medicine. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, Aidong Zhang, Findings of ACL'24. 2024</p>
<p>Doctorglm: Fine-tuning your chinese doctor is not a herculean task. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Linlin Huang, Qian Wang, Dinggang Shen, arXiv:2304.010972023arXiv preprint</p>
<p>Transpolymer: a transformer-based language model for polymer property predictions. Changwen Xu, Yuyang Wang, Amir Barati, Farimani , Computational Materials. 91642023a</p>
<p>Protst: Multi-modality learning of protein sequences and biomedical texts. Minghao Xu, Xinyu Yuan, Santiago Miret, Jian Tang, ICML'23. 2023b</p>
<p>Ran Xu, Wenqi Shi, Yue Yu, Yuchen Zhuang, Yanqiao Zhu, May D Wang, Joyce C Ho, Chao Zhang, Carl Yang, arXiv:2404.18443Bmretriever: Tuning large language models as better biomedical text retrievers. 2024arXiv preprint</p>
<p>A japanese masked language model for academic domain. Hiroki Yamauchi, Tomoyuki Kajiwara, Marie Katsurai, Ikki Ohmukai, Takashi Ninomiya, Proceedings of the Third Workshop on Scholarly Document Processing. the Third Workshop on Scholarly Document Processing2022</p>
<p>Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web. Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, Yuxuan Liang, WWW'24. 2024</p>
<p>scbert as a large-scale pretrained deep language model for cell type annotation of singlecell rna-seq data. Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, Jianhua Yao, Nature Machine Intelligence. 4102022a</p>
<p>Advancing multimodal medical capabilities of gemini. Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, arXiv:2405.031622024aarXiv preprint</p>
<p>Zhongjing: Enhancing the chinese medical capabilities of large language model through expert feedback and real-world multi-turn dialogue. Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan, AAAI'24. 2024b</p>
<p>Clinical concept extraction using transformers. Xi Yang, Jiang Bian, Yonghui William R Hogan, Wu, JAMIA. 27122020</p>
<p>A large language model for electronic health records. npj Digital Medicine. Xi Yang, Aokun Chen, Nima Pournejatian, Chang Hoo, Kaleb E Shin, Christopher Smith, Colin Parisien, Cheryl Compas, Anthony B Martin, Mona G Costa, Flores, 2022b5194</p>
<p>Soujanya Poria, and Erik Cambria. 2024d. Large language models for automated open-domain scientific hypotheses discovery. Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson ; Yang, Xinya Du, Junxian Li, Jie Zheng, arXiv:2401.01600Findings of ACL'24. 2024carXiv preprintPllama: An open-source large language model for plant science</p>
<p>Deep bidirectional language-knowledge graph pretraining. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang, Jure Leskovec, NeurIPS'22. 2022a</p>
<p>Linkbert: Pretraining language models with document links. Michihiro Yasunaga, Jure Leskovec, Percy Liang, ACL'22. 2022b</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, arXiv:2401.10334Drugassist: A large language model for molecule optimization. 2023aarXiv preprint</p>
<p>Qichen Ye, Junling Liu, Dading Chong, Peilin Zhou, Yining Hua, Andrew Liu, arXiv:2310.09089Qilinmed: Multi-stage knowledge injection advanced medical large language model. 2023barXiv preprint</p>
<p>Forge: pre-training open foundation models for science. Junqi Yin, Sajal Dash, Feiyi Wang, Mallikarjun Shankar, SC'23. 2023</p>
<p>Tabert: Pretraining for joint understanding of textual and tabular data. Pengcheng Yin, Graham Neubig, Wen-Tau Yih, Sebastian Riedel, ACL'20. 2020</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, arXiv:2402.06332Internlm-math: Open math large language models toward verifiable reasoning. 2024arXiv preprint</p>
<p>Cxr-clip: Toward large scale chest x-ray language-image pre-training. Kihyun You, Jawook Gu, Jiyeon Ham, Beomhee Park, Jiho Kim, Eun K Hong, Woonhyuk Baek, Byungseok Roh, MIC-CAI'23. 2023</p>
<p>Llasmol: Advancing large language models for chemistry with a large-scale, comprehensive, high-quality instruction tuning dataset. Botao Yu, Frazier N Baker, Ziqi Chen, Xia Ning, Huan Sun, arXiv:2402.093912024aarXiv preprint</p>
<p>Ovm, outcome-supervised value models for planning in mathematical reasoning. Fei Yu, Anningzhe Gao, Benyou Wang, Findings of NAACL'24. 2024b</p>
<p>Metamath: Bootstrap your own mathematical questions for large language models. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu, ICLR'242024c</p>
<p>Grappa: Grammar-augmented pre-training for table semantic parsing. Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, Richard Socher, Caiming Xiong, ICLR'212021</p>
<p>Biobart: Pretraining and evaluation of a biomedical generative language model. Hongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, Sheng Yu, Proceedings of the 21st Workshop on Biomedical Language Processing. the 21st Workshop on Biomedical Language Processing2022a</p>
<p>Improving biomedical pretrained language models with knowledge. Zheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang, Fei Huang, Proceedings of the 20th Workshop on Biomedical Language Processing. the 20th Workshop on Biomedical Language Processing2021</p>
<p>Coder: Knowledgeinfused cross-lingual medical term embedding for term normalization. Zheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao Li, Fei Wang, Sheng Yu, Journal of Biomedical Informatics. 1261039832022b</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, CVPR'24. 2024a</p>
<p>Mammoth: Building math generalist models through hybrid instruction tuning. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, Wenhu Chen, ICLR'242024b</p>
<p>Xiang Yue, Tuney Zheng, Ge Zhang, Wenhu Chen, arXiv:2405.03548Mammoth2: Scaling instructions from the web. 2024carXiv preprint</p>
<p>Selformer: molecular representation learning via selfies language models. Atakan Yüksel, Erva Ulusoy, Atabey Ünlü, Tunca Dogan, Machine Learning: Science and Technology. 2023425035</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun, Nature Communications. 1318622022</p>
<p>Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, Jie Tang, arXiv:2401.079502024aarXiv preprint</p>
<p>Dnagpt: A generalized pretrained tool for multiple dna sequence analysis tasks. Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, Jianhua Yao, bioRxiv. 2023a</p>
<p>Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, arXiv:2402.06852Chemllm: A chemical large language model. 2024barXiv preprint</p>
<p>Huatuogpt, towards taming language model to be a doctor. Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhang Zhiyi, Qingying Xiao, Findings of EMNLP'23. 2023b</p>
<p>A generalist vision-language foundation model for diverse biomedical tasks. Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian D Davison, Hui Ren, Nature Medicine. 2024c</p>
<p>Ningyu Zhang, Qianghuai Jia, Kangping Yin, Liang Dong, Feng Gao, Nengwei Hua, arXiv:2008.10813Conceptualized representation learning for chinese biomedical text mining. 2020arXiv preprint</p>
<p>Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, arXiv:2401.14656Scientific large language models: A survey on biological &amp; chemical domains. 2024darXiv preprint</p>
<p>Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, arXiv:2303.009152023carXiv preprint</p>
<p>Tablellama: Towards open large generalist models for tables. Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun, NAACL'24. 2024e</p>
<p>Tablellm: Enabling tabular data manipulation by llms in real office usage scenarios. Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, arXiv:2403.193182024farXiv preprint</p>
<p>Alpacare: Instruction-tuned large language models for medical application. Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold, arXiv:2310.145582023darXiv preprint</p>
<p>Artificial intelligence for science in quantum, atomistic, and continuum systems. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, arXiv:2307.084232023earXiv preprint</p>
<p>Multiple sequence alignment-based rna language model and its application to structural inference. Yikun Zhang, Mei Lang, Jiuhong Jiang, Zhiqiang Gao, Fan Xu, Thomas Litfin, Ke Chen, Jaswinder Singh, Xiansong Huang, Guoli Song, Nucleic Acids Research. 5212024g</p>
<p>Pre-training multi-task contrastive learning models for scientific literature understanding. Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, Jianfeng Gao, Findings of EMNLP'23. 2023f</p>
<p>The effect of metadata on scientific literature tagging: A cross-field cross-model study. Yu Zhang, Bowen Jin, Qi Zhu, Yu Meng, Jiawei Han, WWW'23. 2023g</p>
<p>Chain-offactors paper-reviewer matching. Yu Zhang, Yanzhen Shen, Seongku Kang, Xiusi Chen, Jin Bowen, Jiawei Han, arXiv:2310.144832023harXiv preprint</p>
<p>Contrastive learning of medical visual representations from paired images and text. Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher D Manning, Curtis P Langlotz, MLHC'22. 2022</p>
<p>Text-guided foundation model adaptation for pathological image classification. Yunkun Zhang, Jin Gao, Mu Zhou, Xiaosong Wang, Yu Qiao, Shaoting Zhang, Dequan Wang, MICCAI'23. 2023i</p>
<p>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Haiteng Zhao, Shengchao Liu, Chang Ma, Hannan Xu, Jie Fu, Zhi-Hong Deng, Lingpeng Kong, Qi Liu, 2023aIn NeurIPS'23</p>
<p>Large-scale cell representation learning via divideand-conquer contrastive learning. Suyuan Zhao, Jiahuan Zhang, Zaiqing Nie, arXiv:2306.043712023barXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023carXiv preprint</p>
<p>Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, Jingming Liu, arXiv:2009.11506Ape210k: A large-scale and template-rich dataset of math word problems. 2020arXiv preprint</p>
<p>Reastap: Injecting table reasoning skills during pre-training via synthetic reasoning examples. Yilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang, Dragomir Radev, EMNLP'22. 2022</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Chemdfm: Dialogue foundation model for chemistry. 2024arXiv preprint</p>
<p>Large language models for scientific synthesis, inference and explanation. Yizhen Zheng, Yee Huan, Jiaxin Koh, Anh Tn Ju, Lauren T Nguyen, Geoffrey I May, Shirui Webb, Pan, arXiv:2310.079842023aarXiv preprint</p>
<p>Structure-informed language models are protein designers. Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, Quanquan Gu, ICML'23. 2023b</p>
<p>Victor Zhong, Caiming Xiong, Richard Socher, arXiv:1709.00103Seq2sql: Generating structured queries from natural language using reinforcement learning. 2017arXiv preprint</p>
<p>Uni-mol: A universal 3d molecular representation learning framework. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, Guolin Ke, ICLR'232023</p>
<p>Dnabert-2: Efficient foundation model and benchmark for multispecies genome. Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, Han Liu, ICLR'242024a</p>
<p>Large language model for participatory urban planning. Zhilun Zhou, Yuming Lin, Depeng Jin, Yong Li, arXiv:2402.171612024barXiv preprint</p>
<p>Can large language models transform computational social science?. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, Computational Linguistics. 5012024</p>
<p>Genslms: Genome-scale language models reveal sars-cov-2 evolutionary dynamics. Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, The International Journal of High Performance Computing Applications. 3762023</p>            </div>
        </div>

    </div>
</body>
</html>
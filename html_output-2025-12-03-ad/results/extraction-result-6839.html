<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-109ea3b885b994a81642dd1ab60d5991c66690d5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/109ea3b885b994a81642dd1ab60d5991c66690d5" target="_blank">Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> This work presents GEMCODE, a novel pipeline for automated co-crystal screening based on the hybridization of deep generative models and evolutionary optimization for broader exploration of the target chemical space and explores the potential of language models in generating co-crystals.</p>
                <p><strong>Paper Abstract:</strong> Co-crystallization is an accessible way to control physicochemical characteristics of organic crystals, which finds many biomedical applications. In this work, we present Generative Method for Co-crystal Design (GEMCODE), a novel pipeline for automated co-crystal screening based on the hybridization of deep generative models and evolutionary optimization for broader exploration of the target chemical space. GEMCODE enables fast de novo co-crystal design with target tabletability profiles, which is crucial for the development of pharmaceuticals. With a series of experimental studies highlighting validation and discovery cases, we show that GEMCODE is effective even under realistic computational constraints. Furthermore, we explore the potential of language models in generating co-crystals. Finally, we present numerous previously unknown co-crystals predicted by GEMCODE and discuss its potential in accelerating drug development.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (reduced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reduced GPT-2 language model (chemistry fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small decoder-only transformer language model variant (reduced GPT-2) pretrained on ChEMBL and fine-tuned on a coformer dataset to generate SMILES strings of candidate coformer molecules for co-crystal design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Reduced GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, pretrained then fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈14.7M parameters (8 heads, 4 attention blocks)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained on a filtered subset of ChEMBL (~1.75M drug-like molecules selected to match coformer feature distributions) and fine-tuned on a coformer dataset (6819 two-component co-crystals; 4227 unique coformer structures).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation from the decoder LLM after fine-tuning (sequence generation). No RL or external sampling objective reported beyond the usual sampling from the model.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings (tokenized for model input/output)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>De novo generation of coformer candidates to form co-crystals with target mechanical/tabletability properties for pharmaceuticals</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Training-focussed constraints: ChEMBL filtering to coformer-like molecules (MW < 600 Da, HBD < 3, HBA < 8, rotatable bonds ≤ 9, PSA ≤ 138 Å², heavy atoms ≤ 39). Post-generation screening by pretrained property classifiers (GB models) and CCGNet ranking; expert safety filtering for pharmaceutical suitability in downstream selection.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Generated SMILES were evaluated by (1) gradient-boosting classifiers predicting three mechanical plasticity properties, (2) evolutionary optimization (GOLEM library) when applicable, and (3) CCGNet GNN for co-crystallization probability ranking. Fine-tuning was standard supervised fine-tuning; no external quantum chemistry/docking/retrosynthesis tools were used with GPT-2 in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL subset (~1.75M filtered molecules) for pretraining; coformer dataset (6819 two-component co-crystals; 4227 unique coformers) for fine-tuning; mechanical-property labels derived from CSD geometric analysis (6029 co-crystals with computed plasticity parameters) used downstream for property prediction models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity (% chemically valid SMILES), Novelty (% not in training set), Duplicates (% duplicates), Diversity (%), Percent of 'target' coformers (fraction of generated molecules predicted by pretrained classifiers to have the desired mechanical/tabletability profile), and downstream CCGNet co-crystallization score.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Per 10,000 generated molecules: GPT-2 produced significantly lower percentages of valid and novel structures compared to transformer VAE and CVAE models; achieved 3.32% of new molecules with the desired physicochemical/mechanical properties (as assessed by pretrained GB classifiers). Exact validity/novelty/duplicate percentages for GPT-2 are reported as 'significantly lower' in the text; numeric examples are provided in Appendix D.10 but not as a single table in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Produced fewer valid/novel molecules per generation than top generative architectures (transformer VAEs and GAN); authors state language-model outputs require heavy optimization to be competitive; no synthesis/experimental follow-up for GPT-2-specific outputs reported. Paper also notes that LLMs are generally less competitive for generative tasks requiring detailed SMILES syntax understanding unless specialized/optimized.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-8B (LoRA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3 (8B parameter) adapted with LoRA and fine-tuned for coformer generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large decoder-only family language model (Llama-3, 8B parameters) adapted via Low-Rank Adaptation (LoRA) and fine-tuned on ChEMBL and coformer datasets to generate SMILES coformer candidates; evaluated for validity, novelty, diversity and ability to produce molecules with desired mechanical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B (LoRA fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM, LoRA-adapted fine-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈8B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pretrained base Llama-3 model adapted using LoRA on the same ChEMBL (~1.75M filtered) pretraining and the coformer fine-tuning set (6819 two-component co-crystals; 4227 unique coformers) used across other generators.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct SMILES generation from the LoRA-adapted LLM after fine-tuning; sampling of sequences to produce molecular SMILES.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES strings</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Generation of coformer molecules to create co-crystals with improved tabletability (mechanical properties) for pharmaceuticals; broad aim of de novo coformer design.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same dataset-selection and downstream screening constraints as other generators: ChEMBL selection criteria to bias model toward coformer-like chemistry; post-generation filtering by gradient-boosting mechanical-property predictors and CCGNet ranking; expert safety selection for pharmaceutical suitability. No explicit synthetic-accessibility or toxicity automated filters reported for Llama-3 runs aside from expert selection.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Outputs evaluated with the pipeline's pretrained gradient-boosting classifiers (mechanical properties), optional evolutionary optimization, and CCGNet GNN for co-crystallization ranking. LoRA used as the adaptation technique during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ChEMBL filtered subset (~1.75M) for pretraining context and coformer dataset (6819 co-crystals / 4227 unique coformers) for fine-tuning; mechanical labels from CSD-derived dataset (6029 records) were used downstream for property classifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity %, Novelty %, Duplicates %, Diversity %, percent of generated molecules predicted to have target mechanical properties, and CCGNet co-crystallization score.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Llama-3-8B (LoRA) achieved major improvements in validity, novelty and duplicates relative to the small GPT-2 variant and produced the maximum diversity percent among tested generative approaches, but only 0.34% of generated molecules met the pretrained classifiers' target physicochemical/mechanical property profile (i.e., produced target coformers).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Although Llama-3-8B improved general generation quality (validity/novelty/diversity), its yield of molecules with the specified mechanical/tabletability target was very low (0.34%), indicating difficulty of steering large LLMs to highly specific crystallographic/mechanical objectives without further specialized objectives or optimization. Authors conclude language models need heavy optimization to be competitive with dedicated generative architectures (T-CVAE/T-VAE/GAN) in this task. No LM-specific experimental synthesis validation was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry. <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6839",
    "paper_id": "paper-109ea3b885b994a81642dd1ab60d5991c66690d5",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "GPT-2 (reduced)",
            "name_full": "Reduced GPT-2 language model (chemistry fine-tuned)",
            "brief_description": "A small decoder-only transformer language model variant (reduced GPT-2) pretrained on ChEMBL and fine-tuned on a coformer dataset to generate SMILES strings of candidate coformer molecules for co-crystal design.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Reduced GPT-2",
            "model_type": "decoder-only LLM, pretrained then fine-tuned",
            "model_size": "≈14.7M parameters (8 heads, 4 attention blocks)",
            "training_data_description": "Pretrained on a filtered subset of ChEMBL (~1.75M drug-like molecules selected to match coformer feature distributions) and fine-tuned on a coformer dataset (6819 two-component co-crystals; 4227 unique coformer structures).",
            "generation_method": "Direct SMILES generation from the decoder LLM after fine-tuning (sequence generation). No RL or external sampling objective reported beyond the usual sampling from the model.",
            "chemical_representation": "SMILES strings (tokenized for model input/output)",
            "target_application": "De novo generation of coformer candidates to form co-crystals with target mechanical/tabletability properties for pharmaceuticals",
            "constraints_used": "Training-focussed constraints: ChEMBL filtering to coformer-like molecules (MW &lt; 600 Da, HBD &lt; 3, HBA &lt; 8, rotatable bonds ≤ 9, PSA ≤ 138 Å², heavy atoms ≤ 39). Post-generation screening by pretrained property classifiers (GB models) and CCGNet ranking; expert safety filtering for pharmaceutical suitability in downstream selection.",
            "integration_with_external_tools": "Generated SMILES were evaluated by (1) gradient-boosting classifiers predicting three mechanical plasticity properties, (2) evolutionary optimization (GOLEM library) when applicable, and (3) CCGNet GNN for co-crystallization probability ranking. Fine-tuning was standard supervised fine-tuning; no external quantum chemistry/docking/retrosynthesis tools were used with GPT-2 in this paper.",
            "dataset_used": "ChEMBL subset (~1.75M filtered molecules) for pretraining; coformer dataset (6819 two-component co-crystals; 4227 unique coformers) for fine-tuning; mechanical-property labels derived from CSD geometric analysis (6029 co-crystals with computed plasticity parameters) used downstream for property prediction models.",
            "evaluation_metrics": "Validity (% chemically valid SMILES), Novelty (% not in training set), Duplicates (% duplicates), Diversity (%), Percent of 'target' coformers (fraction of generated molecules predicted by pretrained classifiers to have the desired mechanical/tabletability profile), and downstream CCGNet co-crystallization score.",
            "reported_results": "Per 10,000 generated molecules: GPT-2 produced significantly lower percentages of valid and novel structures compared to transformer VAE and CVAE models; achieved 3.32% of new molecules with the desired physicochemical/mechanical properties (as assessed by pretrained GB classifiers). Exact validity/novelty/duplicate percentages for GPT-2 are reported as 'significantly lower' in the text; numeric examples are provided in Appendix D.10 but not as a single table in main text.",
            "experimental_validation": false,
            "challenges_or_limitations": "Produced fewer valid/novel molecules per generation than top generative architectures (transformer VAEs and GAN); authors state language-model outputs require heavy optimization to be competitive; no synthesis/experimental follow-up for GPT-2-specific outputs reported. Paper also notes that LLMs are generally less competitive for generative tasks requiring detailed SMILES syntax understanding unless specialized/optimized.",
            "uuid": "e6839.0",
            "source_info": {
                "paper_title": "Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Llama-3-8B (LoRA)",
            "name_full": "Llama-3 (8B parameter) adapted with LoRA and fine-tuned for coformer generation",
            "brief_description": "A large decoder-only family language model (Llama-3, 8B parameters) adapted via Low-Rank Adaptation (LoRA) and fine-tuned on ChEMBL and coformer datasets to generate SMILES coformer candidates; evaluated for validity, novelty, diversity and ability to produce molecules with desired mechanical properties.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B (LoRA fine-tuned)",
            "model_type": "decoder-only LLM, LoRA-adapted fine-tuned model",
            "model_size": "≈8B parameters",
            "training_data_description": "Pretrained base Llama-3 model adapted using LoRA on the same ChEMBL (~1.75M filtered) pretraining and the coformer fine-tuning set (6819 two-component co-crystals; 4227 unique coformers) used across other generators.",
            "generation_method": "Direct SMILES generation from the LoRA-adapted LLM after fine-tuning; sampling of sequences to produce molecular SMILES.",
            "chemical_representation": "SMILES strings",
            "target_application": "Generation of coformer molecules to create co-crystals with improved tabletability (mechanical properties) for pharmaceuticals; broad aim of de novo coformer design.",
            "constraints_used": "Same dataset-selection and downstream screening constraints as other generators: ChEMBL selection criteria to bias model toward coformer-like chemistry; post-generation filtering by gradient-boosting mechanical-property predictors and CCGNet ranking; expert safety selection for pharmaceutical suitability. No explicit synthetic-accessibility or toxicity automated filters reported for Llama-3 runs aside from expert selection.",
            "integration_with_external_tools": "Outputs evaluated with the pipeline's pretrained gradient-boosting classifiers (mechanical properties), optional evolutionary optimization, and CCGNet GNN for co-crystallization ranking. LoRA used as the adaptation technique during fine-tuning.",
            "dataset_used": "ChEMBL filtered subset (~1.75M) for pretraining context and coformer dataset (6819 co-crystals / 4227 unique coformers) for fine-tuning; mechanical labels from CSD-derived dataset (6029 records) were used downstream for property classifiers.",
            "evaluation_metrics": "Validity %, Novelty %, Duplicates %, Diversity %, percent of generated molecules predicted to have target mechanical properties, and CCGNet co-crystallization score.",
            "reported_results": "Llama-3-8B (LoRA) achieved major improvements in validity, novelty and duplicates relative to the small GPT-2 variant and produced the maximum diversity percent among tested generative approaches, but only 0.34% of generated molecules met the pretrained classifiers' target physicochemical/mechanical property profile (i.e., produced target coformers).",
            "experimental_validation": false,
            "challenges_or_limitations": "Although Llama-3-8B improved general generation quality (validity/novelty/diversity), its yield of molecules with the specified mechanical/tabletability target was very low (0.34%), indicating difficulty of steering large LLMs to highly specific crystallographic/mechanical objectives without further specialized objectives or optimization. Authors conclude language models need heavy optimization to be competitive with dedicated generative architectures (T-CVAE/T-VAE/GAN) in this task. No LM-specific experimental synthesis validation was reported.",
            "uuid": "e6839.1",
            "source_info": {
                "paper_title": "Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What can large language models do in chemistry? a comprehensive benchmark on eight tasks.",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry.",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_for_predictive_chemistry"
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools.",
            "rating": 2,
            "sanitized_title": "augmenting_large_language_models_with_chemistry_tools"
        }
    ],
    "cost": 0.01187075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hybrid Generative AI for De Novo Design of Co-Crystals with Enhanced Tabletability</h1>
<p>Nina Gubina ${ }^{1}$ Andrei Dmitrenko ${ }^{1,2}$ Gleb Solovev ${ }^{1}$<br>Lyubov Yamshchikova ${ }^{1}$ Oleg Petrov ${ }^{1}$ Ivan Lebedev ${ }^{3}$ Nikita Serov ${ }^{1}$<br>Grigorii Kirgizov ${ }^{1}$ Nikolay Nikitin ${ }^{1}$ Vladimir Vinogradov ${ }^{1}$<br>${ }^{1}$ ITMO University, St. Petersburg, Russia<br>${ }^{2}$ D ONE AG, Zurich, Switzerland<br>${ }^{3}$ Ivanovo State University of Chemistry and Technology, Ivanovo, Russia<br>dmitrenko@scamt-itmo.ru</p>
<h4>Abstract</h4>
<p>Co-crystallization is an accessible way to control physicochemical characteristics of organic crystals, which finds many biomedical applications. In this work, we present Generative Method for Co-crystal Design (GEMCODE) ${ }^{1}$, a novel pipeline for automated co-crystal screening based on the hybridization of deep generative models and evolutionary optimization for broader exploration of the target chemical space. GEMCODE enables fast de novo co-crystal design with target tabletability profiles, which is crucial for the development of pharmaceuticals. With a series of experimental studies highlighting validation and discovery cases, we show that GEMCODE is effective even under realistic computational constraints. Furthermore, we explore the potential of language models in generating co-crystals. Finally, we present numerous previously unknown co-crystals predicted by GEMCODE and discuss its potential in accelerating drug development.</p>
<h2>1 Introduction</h2>
<p>The use of multi-component molecular crystals, specifically co-crystals, have become increasingly popular in various industries including energy [1], electronics [2, 3], optoelectronics [4, 5], food [6], and especially in pharmaceuticals [7-9]. Pharmaceutical co-crystals are defined as solids that are crystalline singlephase materials composed of a drug molecule and an additional pharmaceutically acceptable molecule (coformer) [10]. Co-crystals have a different crystal structure from the original components, leading to unique physicochemical properties. They are appealing because the resulting solid can exhibit better physicochemical properties compared to either of the pure molecules [11]. The formation of co-crystals has been shown to enhance characteristics such as bioavailability [12, 13], solubility [14-16], stability [17-19], pharmacokinetics [20, 21], and mechanical properties [14, 22, 23]. Plasticity is a mechanical property that is particularly important for the pharmaceutical industry. It is known that highly plastic materials tend to produce stronger tablets compared to those exhibiting elastic behavior [24]. In other words, it possesses improved tabletability, defined as the capacity of a powdered material to be transformed into a tablet of specified strength under the effect of compaction pressure [25]. Therefore, it is essential to control for tabletability as it allows direct pressing with minimal addition of excipients to form a stable compact tablet.
Despite all the robustness and versatility of co-crystals, determining the combination of a coformer and parent component with the desired property modification is an extremely non-trivial task, usually</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>addressed by experimental high-throughput screening [26, 27]. Due to the large amounts of time and effort required, such studies remain targeted, focusing on rather narrow classes of candidate compounds.</p>
<p>Artificial intelligence (AI) methods have recently found their way into the field of chemistry [2832]. Since then, the accumulated experimental data has become the basis for predictive models transforming the traditional way science works. With big data and machine learning (ML), it is now possible to consider a much larger set of candidate molecules for a given problem, rather than being satisfied with a limited number of experiments. Among the pioneering works in the co-crystal domain are the studies aimed at determining the probability of co-crystallization of a particular molecular pair [33, 34]. However, the sole fact of co-crystallization with no information about the properties of the resulting co-crystals is not enough to inform decision making for a specific use case. Accordingly, another direction of research investigated co-crystal properties with AI methods [35, 36]. Still, prediction of most properties has been possible only in the case of already known co-crystallising molecular pairs. De novo design of co-crystals with predefined properties leveraging big data to cover a large chemical space remains an actual task of great application value.</p>
<p>Therefore, here for the first time we develop a pipeline that generates coformer candidates based on the structure of a drug molecule to form a co-crystal with predefined mechanical properties. For that, we trained several state-of-the-art generative models on a dataset of 1.75 M chemical structures and then fine-tuned them on the state-of-the-art dataset of coformers. We then trained a classical ML model to predict plasticity parameters of the generated coformer candidates. We further employed evolutionary optimization leveraging the trained ML models to improve the tabletability profiles of the generated coformers. Finally, we applied a pretrained graph neural network (GNN) to rank the molecular pairs according to the probability of successful co-crystal formation. We systematically evaluated and optimized the aforementioned individual components to assemble GEMCODE, a practical solution achieving state-of-the-art performance even within computational constraints. The output of GEMCODE is a set of coformers forming a co-crystal with improved tabletability properties for a selected drug compound. Thus, the pipeline can serve as a tool for selecting the best molecular combination of an active pharmaceutical agent and a coformer delivering the desired properties of the co-crystal. In essence, this work makes the following novel contributions:</p>
<ul>
<li>We train a transformer-based conditional variational autoencoder (T-CVAE) setting the new state of the art for the coformer generation task, and hybridize it with multi-objective evolutionary algorithm to improve the desired properties of coformers.</li>
<li>We develop machine learning models for the prediction of mechanical properties of cocrystals for the first time in the field.</li>
<li>We present GEMCODE, a generative pipeline for de novo co-crystal design with target physicochemical properties contributing to drug tabletability.</li>
<li>In addition, we explore the capabilities of language models in the coformer generation task.</li>
<li>Finally, we predict a set of molecules forming novel tabletable co-crystals with known drugs.</li>
</ul>
<h1>2 Related Work</h1>
<h3>2.1 Generative AI for molecule generation</h3>
<p>Traditionally, the process of discovering new molecules or selecting chemical structures for a particular task relies on existing experimental evidence and subjective research experience, both limiting the number and variety of possible compounds to consider. Generative models allow efficient exploration of the molecular space, which has already caused a rapid growth of molecular generative design. Recurrent neural networks [37-40], variational autoencoders [41-44], generative adversarial networks [45-48], evolutionary algorithms [49-53] and hybrid models using reinforcement learning techniques [54-57] have been successfully applied for various problems in chemistry. In this work, we trained, evaluated and compared multiple generation approaches, such as LSTM-based GAN, transformer-based VAE and conditional VAE. The latter was inspired by a study using a conditional VAE model with an attention mechanism to generate molecules [58]. However, our approach differs significantly in that we generated a condition vector based on the predictions of the pretrained gradient-boosting model. In addition, our approach includes a fine-tuning phase on a state-of-the-art dataset of coformers.</p>
<p>DeepMind has recently presented GNoME, an AI tool for generating previously unknown inorganic crystalline materials [59]. Other similar tools exist for inorganic compounds [60, 61]. Our work also lies in the field of solid-state chemistry, but differs in the task of generating coformers, which are small organic molecules. To our knowledge, generative approaches have not yet been applied to produce coformer structures with high co-crystallization potential with drug targets. Our work effectively addresses this problem.</p>
<h1>2.2 Co-crystal property prediction</h1>
<p>Research in co-crystal property prediction is targeted at determining various parameters, such as the lattice energy, density, melting temperature, crystal density, enthalpy and entropy of melting, as well as ideal mole fraction solubility of co-crystals [62-65]. However, a limited number of samples is typically used in the training phase. For example, Gamidi and Rasmuson trained an artificial neural network on the data of 30 co-crystal systems for 8 different drugs [35]. Such models are likely to have very limited generalization power beyond the training data. The most recent model predicting the co-crystal density [36] used a large training set of 4144 molecular pairs covering a much wider chemical space of possible co-crystals. In this work, we predict several mechanical properties of co-crystals for the first time. We use an even larger amount of data for that ( 6029 samples), which makes our approach more versatile and better generalizable for different pharmaceutical applications.</p>
<h3>2.3 Applications of language models in chemistry</h3>
<p>Large language models have recently been challenged with multiple chemistry tasks, such as property prediction, yield prediction, text-based molecular design, and others [66]. The results suggest that language models are less competitive in generative tasks requiring a deeper understanding of molecular SMILES strings, but show competitive performance in classification and ranking tasks. Another study on the applicability of language models without prior specialization in the chemistry domain found that LLMs can effectively interpret chemical structures given various representations [67]. In addition, the use of language models as agents was explored in ChemCrow [68], which makes chemistry more accessible to researchers with less domain expertise. Following up on these pioneering works, we explore the applicability of language models to the creation of coformer molecules with desired properties, which has not yet been addressed in the past.</p>
<h2>3 Data</h2>
<h3>3.1 Data collection</h3>
<p>Large dataset of molecules. In order to train a generative model capable of suggesting reasonable chemical structures, a dataset of molecules from the ChEMBL database (available with CC BY-SA 3.0 license) was collected. From the large variety of molecular structures available in the database, $\sim 1.75 \mathrm{M}$ samples were selected using criteria based on the distributions of relevant parameters in the known coformers (Appendix C.1). Using these criteria ensures that the generative models are trained on molecules capable of forming co-crystals.</p>
<p>Dataset of coformers. Chemical structures in the ChEMBL database are still substantially different from the structures composing co-crystals. Coformers most often have more basic chemical structures and a smaller variety of functional groups. Therefore, we used an open dataset of 6819 two-component co-crystals [33] (available with MIT license), which contains 4227 unique chemical structures of the coformers, for fine-tuning.</p>
<p>Dataset of co-crystals mechanical properties. For the mechanical properties of co-crystals, we used the Cambridge Structural Database (CSD) [69] and a recently proposed protocol for geometric analysis of co-crystalline materials available with a CSD Python API [24]. For each of the 6819 available co-crystals, we used the API to query additional experimental data from the CSD and calculate the following binary parameters of plasticity: presence of non-overlapping Miller planes (Unobstructed planes), presence of orthogonal planes (Orthogonal planes), and presence of hydrogen bonds between the planes (H-bond bridging). Since some of the co-crystals were missing in CSD, this process yielded a total of 6029 records. This data was then used for training ML models to predict each of the three plasticity parameters.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: GEMCODE: a pipeline for generative co-crystal design consisting of models (LSTM-based GAN, T-VAE, T-CVAE) generating coformer candidates, gradient boosting (GB) classification models predicting the mechanical properties of co-crystals based on the generated coformers, an evolutionary algorithm producing additional coformer candidates with improved tabletability profiles, and a graph neural network (GNN) ranking co-crystals according to the probability of formation.</p>
<p>We analyzed the number of samples for each plasticity parameter in the collected dataset (Appendix C.2). In the case of orthogonal planes, we observed a dramatic difference between the two groups. When training the corresponding ML model, we accounted for this disproportion by adjusting a threshold probability for predicting a positive class.</p>
<h1>3.2 Data curation</h1>
<p>Cutting-edge generative models use string [70-72], 2D [73-75] and 3D [76-78] molecular graphs as molecular representations. The most common way is the SMILES (Simplified molecular-input line-entry system) notation, as the other approaches have not yet shaped the field to such an extent [79]. Therefore, we used the SMILES representations to describe the composition and structure of chemical molecules with short strings. Additionally, molecular fingerprints allowed us to represent molecules in a vectorized form and compare different structures by calculating a similarity measure (Appendix C.3).</p>
<p>We used RDKit to generate 43 molecular descriptors for each coformer with its SMILES representation. Since co-crystals consist of two coformer components, each one was described by 86 numerical features in total. Before training ML models for the prediction of mechanical properties, we applied a set of preprocessing steps. We engineered new features by aggregating the molecular features of the coformers of the same co-crystal with summation and averaging. To reduce redundancy in the feature space, we investigated the feature importances using embedded methods and the degree of linear association with target variables through correlation coefficients. After feature engineering and filtering, the datasets for the prediction of non-overlapping planes, orthogonal planes, and hydrogen bonding contained 29, 24, and 30 features, respectively.</p>
<h2>4 GEMCODE: Generative Evolution-based Method for Co-crystal Design</h2>
<p>We present GEMCODE, a novel pipeline for generative co-crystal design with improved tabletability properties. It based on the idea of hybridization of deep generative models and combinatorial optimisation. GEMCODE consists of four key components, as depicted on Figure 1.</p>
<p>First, a trained and fine-tuned generative model generates SMILES representations of coformer-like chemical structures. The generated molecules are then fed into the trained ML models along with the therapeutic compounds, where the mechanical properties of co-crystals are predicted. In addition, an evolutionary algorithm is used in combination with the ML models to further improve the tabletability of the generated coformers. Finally, co-crystals with the desired properties are selected for the next step, where a pretrained graph neural network scores and ranks molecular pairs of drugs and coformers according to the probability of co-crystallization. Thus, the pipeline outputs a list of potential coformers with the desired mechanical properties of the co-crystal, ranked according to</p>
<p>the probability of successful co-crystallization. In the following sections, we describe the individual components of the pipeline in more detail.</p>
<h1>4.1 Prediction of mechanical properties of co-crystals</h1>
<p>Since the number of training examples available for prediction of mechanical properties was only 6029, we resorted to the classical machine learning algorithms. We formulated a binary classification problem for each of the mechanical properties and implemented a number of ML models as a first screen, including logistic regression, k-nearest neighbors classifier, support vector machines, decision trees, multilayer perceptron, as well as ensemble models, such as random forest and gradient boosting. We then selected the best models and optimized their hyperparameters to achieve top performance. Those pretrained models were then integrated into the coformer generation and the evolutionary optimization frameworks. To validate this solution, we used an AutoML tool to design the modeling pipeline in an automated way (details are provided in Appendix G.5.</p>
<h3>4.2 Generation of coformers</h3>
<p>The performance of a particular deep neural network is largely determined by its architecture, as well as the strategy to learn the hidden representations [80]. In order to find the most effective solution for the coformer generation task, we implemented and systematically compared three different architectures. Our evaluation included a GAN model with recurrent neural networks for both, generator and discriminator, and two transformer-based models implementing a VAE. For more information regarding the model architectures, refer to Appendix D. 4 and D.5.
GAN-based methods consider molecule generation a minimax game, which consists of training a discriminator to distinguish between the real data and the samples produced by a generator (Appendix D.1). In this work, we employed an open-source GAN implementation ${ }^{2}$ using LSTM to address molecule generation as a sequence-to-sequence (S2S) problem, inspired by the work of d'Autume [81]. As an alternative, we opted for a transformer architecture [82] as a basis for a VAE, since it normally outperforms recurrent neural network architectures in S2S tasks [83].
Our objective was to produce co-crystals meeting specific tabletability requirements that translate to a set of target mechanical properties. We utilized a conditional variational autoencoder (CVAE) approach [84] to achieve this. By design, CVAE makes it possible to consider physicochemical characteristics of molecules and generate co-crystals with the desired properties (Appendix D. 3 offers a more detailed description of the VAE and CVAE models). We used the aforementioned mechanical properties (unobstructed planes, orthogonal planes, and H-bonds bridging) as conditions for CVAE. In the following, we refer to this model as transformer-based CVAE (T-CVAE).
Finally, we included a transformer-based VAE (T-VAE) for comparison, which does not consider any specific properties of molecules, for completeness of the analysis.</p>
<h3>4.3 Evolutionary optimization of coformers</h3>
<p>To increase the quality of coformer generation, we applied a graph-based evolutionary algorithm to structures produced by the generative models. The software implementation is obtained from the self-developed GOLEM library [85]. The fitness function was designed to reinforce the mechanical characteristics of molecules based on predictions of the classification models described above:</p>
<p>$$
f(x)=\left(1-p_{u}(x), 1-p_{o}(x), p_{h}(x)\right)^{T}
$$</p>
<p>where $x$ is an evaluated molecule of coformer, $p_{u}(x)$ is the probability of the positive class for unobstructed planes, $p_{o}(x)$ is the same probability for orthogonal planes, and $p_{h}(x)$ - for H-bond bridging. Therefore, minimization of the fitness function $f$ leads to generation of coformer molecules having an improved tabletability profile.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Accuracy and F1 score metrics for the ML models predicting three mechanical properties of co-crystals. (a) Unobstructed planes. (b) Orthogonal planes. (c) H-bonds bridging. The performance of each model is shown before ("Raw data") and after ("Processed data") the feature engineering and feature selection steps.</p>
<h3>4.4 Estimation of probability of co-crystal formation</h3>
<p>Determining the possibility of co-crystallization by molecular pairing is an important step in the co-crystal design. For this reason, many works attempted to solve this problem with AI [86, 34, 87]. Most works that are closely related to our problem do not provide code to reproduce or reuse their results [88–91]. To account for the probability of co-crystallization, we applied an existing GNN-based deep learning framework, called CCGNet [33] (available with MIT license). Unlike many of the previous works, CCGNet achieves state-of-the-art performance predicting co-crystal formation while being 100% open-source and easily reproducible. With an average balanced accuracy of 98.6%, CCGNet efficiently scores and ranks coformer candidates according to the probability of co-crystal formation. Since CCGNet was originally trained on the same database of coformers, we did not perform any fine-tuning and simply integrated the model from the open GitHub repository into the pipeline.</p>
<h2>5 Experimental studies</h2>
<h3>5.1 Prediction of mechanical properties of co-crystals</h3>
<p><strong>Implementation details.</strong> The preprocessed dataset was randomly split into train and test sets in proportion 4:1. The train set was used to optimize hyperparameters of the models with a grid search using the 10-fold cross-validation (CV). The random grid size was 500 and concerned the following parameters: learning rate, number of estimators, subsample, maximum depth of the individual estimators. The test set was used only once, to evaluate and report the performance of the optimized models. We calculated accuracy and F1 score during the CV to select the best hyperparameter set. The use of the two metrics was important given the imbalanced nature of the "Orthogonal planes" and "Unobstructed planes" target variables (Appendix C.2, Figure 3c). To account for the disproportion, we also adjusted the threshold for the probability of the positive class by calculating precision and recall metrics. Finally, we employed SHapley Additive exPlanations (SHAP) to interpret model predictions, which is based on sensitivity analysis investigating the effect of systematic changes in feature values on the model output [92].</p>
<p><strong>Results.</strong> Overall, the GB model showed the best accuracy and F1 score compared to the other models across all tasks (Figure 2). Despite the high accuracy for the orthogonal planes parameter, we obtained a moderate F1 score suggesting that the final model is more likely to predict the absence of the orthogonal planes. This is attributed to the disproportion in the training examples discussed earlier. Although we demonstrated a significant improvement in metrics by introducing the probability threshold (Appendix G.2) evaluating the model trained on the processed data, it was not enough to entirely resolve this issue.</p>
<p>We optimized the hyperparameters of the Gradient Boosting (GB) model, which resulted in the performance metrics outlined in Table 10 (Appendix G.4). Furthermore, we conducted a thorough</p>
<p>Table 1: Results of the coformer generation comparison.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GAN</th>
<th>T-VAE</th>
<th>T-CVAE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Validity $(\uparrow), \%$</td>
<td>$94.57 \pm 0.00$</td>
<td>$\mathbf{9 9 . 7 0} \pm \mathbf{0 . 0 0}$</td>
<td>$98.40 \pm 0.00$</td>
</tr>
<tr>
<td>Novelty $(\uparrow), \%$</td>
<td>$94.90 \pm 0.08$</td>
<td>$\mathbf{9 5 . 1 2} \pm \mathbf{0 . 1 1}$</td>
<td>$80.62 \pm 0.25$</td>
</tr>
<tr>
<td>Duplicates $(\downarrow), \%$</td>
<td>$42.29 \pm 0.69$</td>
<td>$\mathbf{2 4 . 3 0} \pm \mathbf{0 . 4 5}$</td>
<td>$55.70 \pm 0.19$</td>
</tr>
<tr>
<td>Target coformers $(\uparrow), \%$</td>
<td>$2.23 \pm 0.17$</td>
<td>$1.68 \pm 0.12$</td>
<td>$\mathbf{5 . 6 3} \pm \mathbf{0 . 2 2}$</td>
</tr>
<tr>
<td>Diversity of target $(\uparrow)$</td>
<td>$\mathbf{0 . 3 0} \pm \mathbf{0 . 0 0}$</td>
<td>$0.31 \pm 0.00$</td>
<td>$0.25 \pm 0.00$</td>
</tr>
</tbody>
</table>
<p>review of the existing research on the prediction of co-crystal properties to compare with our results. Notably, we are the first to develop predictive models for the plasticity parameters, so our metrics set the state of the art. In addition, our work clearly stands out by the number of data points used for training.</p>
<p>With SHAP analysis (Appendix G.3), we learned that the number of atoms among the molecular pairs forming a co-crystal is a decisive factor in the prediction of non-overlapping and orthogonal planes. In both cases, the decrease in the number of atoms in the coformer molecules significantly contributed to the presence of non-overlapping and orthogonal planes. The descriptors associated with the number of hydrogen bond donors (HBD) also had a high degree of importance. As expected, an increase in the number of HBD resulted in the hydrogen bonds forming between planes of the co-crystal.</p>
<h1>5.2 Generation of coformers</h1>
<p>Implementation details. The performance of generative models depends on hyperparameters and random restarts [93]. A grid search was implemented to select the best hyperparameters, and multiple trainings were conducted. The generative model was focused on generating coformer-like chemical structures, so it was pretrained on the ChEMBL dataset and then fine-tuned on a dataset of coformers. The importance of fine-tuning was illustrated using t-distributed stochastic neighbor embedding (t-SNE) to visualize the datasets (Appendix D.2). To evaluate the trained models, ten sets of 10,000 molecules were generated and various indicators were calculated, including validity (defined as the percentage of chemically plausible molecules to all generated), novelty (defined as the percentage of newly generated molecules that are not contained within the training set to all generated), percentage of duplicate molecules, percentage of target coformers, and diversity. More details on these indicators can be found in the Appendix F.</p>
<p>Results. Analyzing experimental results of coformer generation, we observed that T-VAE produced the highest percent of valid and novel molecules with by far the lowest percent of duplicated structures (Table 1). However, among the generated coformers, only $1.68 \%$ had the target tabletability profile, as assessed by the pretrained classification models. In contrast, when generating 10,000 candidates, T-CVAE produced $5.63 \%$ of new coformers with the required mechanical properties on average. While the diversity of target coformers ${ }^{3}$ was slightly higher for GAN, it was able to produce the intermediate $2.23 \%$ of such coformers. Therefore, we conclude that T-CVAE was the most effective approach to target coformer generation. However, the transformer architecture was also the most demanding for both, the training and the generation phases (see Appendix D. 6 for more details).</p>
<p>Ultimately, we recommend to use an ensemble of generative models whenever sufficient computational resources are available. Our findings presented in Appendix D. 7 suggest that the three models produce complementary results. Collectively, GAN, T-VAE and T-CVAE generate up to 2.47 times more unique target coformers than individually.</p>
<p>Additional experiments with language models. Inspired by the most recent applications of language models in chemistry [66-68, 94] we investigated their potential in the coformer generation task. First, we employed a reduced GPT-2 model with eight heads, four attention blocks, and 14.7M parameters. Similarly to other models, GPT-2 was pre-trained on the ChEMBL dataset and then</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Results and statistical significance of the evolutionary optimization.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Property</th>
<th style="text-align: center;">Median probability ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$p_{a d j}$</th>
<th style="text-align: center;">Novelty ( $\uparrow$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Generated</td>
<td style="text-align: center;">Optimized</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;">Unobstructed planes</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$0.82(+0.0 \%)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orthogonal planes</td>
<td style="text-align: center;">0.37</td>
<td style="text-align: center;">$0.39(+5.4 \%)$</td>
<td style="text-align: center;">2.68e-11</td>
<td style="text-align: center;">0.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H-bond bridging</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">$0.69(+11.3 \%)$</td>
<td style="text-align: center;">1.05e-66</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T-VAE</td>
<td style="text-align: center;">Unobstructed planes</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$0.82(+0.0 \%)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orthogonal planes</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$0.40(+5.3 \%)$</td>
<td style="text-align: center;">2.71e-9</td>
<td style="text-align: center;">0.72</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H-bond bridging</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$0.69(+7.8 \%)$</td>
<td style="text-align: center;">1.76e-65</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T-CVAE</td>
<td style="text-align: center;">Unobstructed planes</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">$0.83(+1.2 \%)$</td>
<td style="text-align: center;">9.52e-05</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Orthogonal planes</td>
<td style="text-align: center;">0.38</td>
<td style="text-align: center;">$0.39(+2.6 \%)$</td>
<td style="text-align: center;">1.88e-9</td>
<td style="text-align: center;">0.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">H-bond bridging</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">$0.69(+7.8 \%)$</td>
<td style="text-align: center;">1.82e-46</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>fine-tuned on the coformers dataset (see Appendix D. 9 for more details). We observed that GPT-2 produced significantly lower percent of new and valid structures per 10,000 generations (Appendix D.10). Nevertheless, the model achieved $3.32 \%$ of new molecules with the desired physicochemical properties, which is comparable to GAN and T-VAE. These results prompted us to further test a more recent and capable language model. Therefore, we trained Llama-3-8B with the low-rank adoption (LoRA) algorithm using the same ChEMBL and coformer datasets. We observed major improvements in validity, novelty and the number of duplicates among the generated molecules compared to GPT-2. Notably, Llama-3-8B produced the maximum diversity percent compared to all the tested generative approaches. However, the number of molecules with target physicochemical properties dropped to only $0.34 \%$. Analyzing these empirical results, we conclude that language models show good potential in the coformer generation task but have to be heavily optimized to achieve competitive performance with GEMCODE. We leave this endeavour for the future work.</p>
<h1>5.3 Evolutionary optimization of coformers</h1>
<p>Implementation details. The multi-objective optimization algorithm used in this work considers molecules as undirected graphs and follows the generational evolutionary scheme MOEA/D [95]. First, a population of individuals is evaluated with the fitness function. Then, MOEA/D-based selection is applied to pick individuals from the population to undergo mutation. After the variation by mutation is done, the inheritance operator is used to form the new population of individuals to proceed to the next iteration (see Appendix E.1, E. 2 for more details).
To choose an effective evolutionary scheme for the task we compared SPEA-2 [96] and MOEA/D (see Appendix E.5). Experiments have shown MOEA/D obtaining better results in some cases.
The initial population of coformer structures (obtained with the previously described generative models) were varied by the set of mutation operators, inspired by the work of Leguy [50]. The set of mutations includes simple operations (add, delete, or replace an atom, delete or replace a bond) and more complicated, multi-step actions (delete or move a functional group, insert carbon, remove an atom if it has only two neighbors). See Appendix E. 3 for more details on optimization runs.</p>
<p>Results. To evaluate results of the evolutionary search, we compared the probabilities of coformers to possess the desired mechanical properties before ("Generated") and after ("Optimized") evolutionary optimization (Table 2). For that, we used the pretrained ML models to retrieve the probabilities, calculated statistics and applied the non-parametric one-sided Mann-Whitney test (see Appendix E.4, F. 1 for more details). In most cases, we observed a significant increase in the median probability ${ }^{4}$ of the target class. Notably, evolutionary optimization equalized the performance of different generative models in their ability to produce coformers with the target tabletability profile. Moreover, this process consistently yielded new coformer structures, not present in the training set or in the initial population.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Experimentally validated coformers improving drug tabletability generated by GEMCODE. SMILES were selected based on two tabletability parameters (Unobstructed planes, H-bond bridging) and similarity metric ( $\mathrm{IT}=1$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Drug</th>
<th style="text-align: center;">Generated SMILES</th>
<th style="text-align: center;">CSD Refcode</th>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Ref.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Nicorandil</td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}=\mathrm{CC}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">WAHGEV</td>
<td style="text-align: center;">GAN / T-VAE / T-CVAE</td>
<td style="text-align: center;">$[97]$</td>
</tr>
<tr>
<td style="text-align: left;">Rivaroxaban</td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}([\mathrm{O}-]) \mathrm{CC}(=\mathrm{O})[\mathrm{O}-]$</td>
<td style="text-align: center;">YORVEJ</td>
<td style="text-align: center;">T-VAE</td>
<td style="text-align: center;">$[98]$</td>
</tr>
<tr>
<td style="text-align: left;">Paracetamol</td>
<td style="text-align: center;">$\mathrm{C} 1=\mathrm{CC}=\mathrm{C} 2 \mathrm{C}=\mathrm{CC}=\mathrm{CC} 2=\mathrm{C} 1$</td>
<td style="text-align: center;">LUJSIT</td>
<td style="text-align: center;">GAN / T-VAE / T-CVAE</td>
<td style="text-align: center;">$[99]$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\mathrm{C}<a href="\mathrm{C}">\mathrm{N}+</a>(\mathrm{C}) \mathrm{CC}(=\mathrm{O})[\mathrm{O}-]$</td>
<td style="text-align: center;">CUQKAC</td>
<td style="text-align: center;">T-CVAE</td>
<td style="text-align: center;">$[100]$</td>
</tr>
</tbody>
</table>
<h1>5.4 Validation case studies</h1>
<p>In order to test the effectiveness of GEMCODE, we generated coformers for the drugs with poor ability to form a tablet by powder pressing. Among the therapeutic molecules selected for the pipeline validation were Nicorandil, Rivaroxaban and Paracetamol. For each of the listed drugs, experimentally validated molecules were found among the GEMCODE-generated coformers improving tabletability of the co-crystals (Table 3). More details can be found in Appendix B.1.</p>
<h3>5.5 Novel coformer molecules predicted by GEMCODE</h3>
<p>To showcase the ability of GEMCODE to predict novel coformers with target tabletability profiles, we generated coformers for one of the therapeutic molecules, i.e., Nicorandil. GEMCODE enabled discovery of 23 unique coformer with improved mechanical properties and with the presence of functional groups as in experimentally validated tabletable co-crystals (see Table 4 in the Appendix B.2). This result demonstrates the potential of GEMCODE as an indispensable tool for accelerated drug development. Broader impact is further discussed in Appendix A.</p>
<h2>6 Limitations</h2>
<p>The evidence presented above looks very promising for the practical applications of our pipeline. However, a comprehensive experimental validation involving organic synthesis of coformers and co-crystal formation followed by a tablet compression experiment is required to confirm its utility. Based on our empirical results, we anticipate the following limitations of the proposed pipeline:</p>
<ul>
<li>The coformers molecular space may be too narrow for some applications due to the small sample size of the coformer dataset. Nevertheless, if computational power is available, it is possible to use an ensemble of generative models, which partially solves the problem by increasing the number of unique molecules generated.</li>
<li>Currently, the GB model is biased towards predicting the absence of orthogonal planes, leading to more false negatives in the predicted coformers. We recommend exploring an alternative set of coformers based on the other two mechanical properties only.</li>
<li>Low-scale screening may still result in some coformers failing to form co-crystals, particularly those optimized through evolution. Screening more coformers increases the chances of finding co-crystal pairs for a specific therapeutic agent.</li>
<li>While polymorphism's impact on predicting co-crystal mechanical properties is not examined here, its significance is undeniable and often understated. Despite limited reported polymorphs, their potential impact on prediction model accuracy in the co-crystal field necessitates further exploration, considering the current scarcity of polymorphism data.</li>
</ul>
<p>Most limitations of the proposed pipeline can be solved with more data available for training, which remains a major challenge for successful AI applications in co-crystallization. We are working towards collecting more data and improving its quality. Also, to date GEMCODE has been adapted mainly for pharmaceutical applications. In the future, we plan to extend GEMCODE by adding more predicted physicochemical properties and other crystal forms to be able to expand beyond the pharmaceutical field.</p>
<h1>7 Conclusion</h1>
<p>In this work, we presented GEMCODE, a novel generative pipeline for de novo co-crystal design. To make it as effective as possible, we implemented the hybrid generative approach combining positive sides of both deep learning models and combinatorial optimisation. We systematically evaluated and discussed the individual components of the pipeline achieving state-of-the-art performance in the corresponding tasks. Furthermore, we performed experiments to validate the pipeline by generating coformers for three different drugs and discovering previously unknown coformers for Nicorandil. In addition, we explored the applicability of language models in the coformer generation task and identified prospective research directions. Despite limitations associated with data availability, GEMCODE enables fast generation of unique and valid chemical structures of coformers with high probabilities of co-crystallization and target tabletability profiles. This research enhances co-crystal design for pharmaceuticals and contributes to the accelerated drug development. Thanks to data and code availability, our versatile hybrid approach might find other impactful applications in chemistry.</p>
<h2>8 Acknowledgements</h2>
<p>This research is financially supported by the Foundation for National Technology Initiative's Projects Support as a part of the roadmap implementation for the development of the high-tech field of Artificial Intelligence for the period up to 2030 (agreement 70-2021-00187)</p>
<h2>References</h2>
<p>[1] Li, H.-j., J.-c. Liu, L. Yang, et al. Theoretical predict structure and property of the novel CL-20/2,4-DNI cocrystal by systematic search approach. Defence Technology, 18(6):907-917, 2022.
[2] Wang, Z., Q. Zhang. Organic Donor-Acceptor Cocrystals for Multiferroic Applications. Asian Journal of Organic Chemistry, 9(9):1252-1261, 2020.
[3] Payne, S., I. Andrusenko, F. Papi, et al. The crystal structure and electronic properties of three novel charge transfer co-crystals tcnqf n-triphenylene ( $\mathrm{n}=0,2,4$ ). CrystEngComm, 25(5):828-834, 2023.
[4] Zou, T., J. Chang, Q. Chen, et al. Novel strategy for organic cocrystals of n-type and p-type organic semiconductors with advanced optoelectronic properties. ACS omega, 5(21):1206712072, 2020.
[5] Abou Taka, A., J. E. Reynolds, N. C. Cole-Filipiak, et al. Comparing the structures and photophysical properties of two charge transfer co-crystals. Physical Chemistry Chemical Physics, 25(40):27065-27074, 2023.
[6] Dias, J. L., M. Lanza, S. R. Ferreira. Cocrystallization: A tool to modulate physicochemical and biological properties of food-relevant polyphenols. Trends in Food Science \&amp; Technology, 110:13-27, 2021.
[7] Guo, M., X. Sun, J. Chen, et al. Pharmaceutical cocrystals: A review of preparations, physicochemical properties and applications. Acta Pharmaceutica Sinica B, 11(8):2537-2564, 2021.
[8] Shaik, A., P. U. Bhagwat, P. Palanisamy, et al. Novel pharmaceutical co-crystals of gefitinib: synthesis, dissolution, cytotoxicity, and theoretical studies. CrystEngComm, 25(17):25702581, 2023.
[9] Jia, X.-m., H. Hao, Q. Zhang, et al. The bioavailability enhancement and insight into the action mechanism of poorly soluble natural compounds from co-crystals preparation: Oridonin as an example. Phytomedicine, 122:155179, 2024.
[10] Bolla, G., B. Sarma, A. K. Nangia. Crystal Engineering of Pharmaceutical Cocrystals in the Discovery and Development of Improved Drugs. Chemical Reviews, 122(13):11514-11603, 2022.</p>
<p>[11] Karimi-Jafari, M., L. Padrela, G. M. Walker, et al. Creating cocrystals: A review of pharmaceutical cocrystal preparation routes and applications. Crystal Growth \&amp; Design, 18(10):63706387, 2018.
[12] Emami, S., M. Siahi-Shadbad, K. Adibkia, et al. Recent advances in improving oral drug bioavailability by cocrystals. BioImpacts, 8(4):305-320, 2018.
[13] Sakamoto, N., K. Miyata, T. Fukami. Quabodepistat (opc-167832), a novel antituberculosis drug candidate: Enhancing oral bioavailability via cocrystallization and mechanistic analysis of bioavailability in two cocrystals. Molecular Pharmaceutics, 2023.
[14] He, H., Q. Zhang, M. Li, et al. Modulating the Dissolution and Mechanical Properties of Resveratrol by Cocrystallization. Crystal Growth \&amp; Design, 17(7):3989-3996, 2017.
[15] Xia, M., Y. Jiang, Y. Cheng, et al. Rucaparib cocrystal: Improved solubility and bioavailability over camsylate. International Journal of Pharmaceutics, 631:122461, 2023.
[16] Imanto, T., E. R. Wikantyasning, S. Nurwaini, et al. Preparation and solid-state characterization of ketoprofen-succinic acid-saccharin co-crystal with improved solubility. Int J Appl Pharm, 16(1), 2024.
[17] Li, D., J. Li, Z. Deng, et al. Piroxicam-clonixin drug-drug cocrystal solvates with enhanced hydration stability. CrystEngComm, 21(28):4145-4149, 2019.
[18] Haneef, J., M. Amir, N. A. Sheikh, et al. Mitigating drug stability challenges through cocrystallization. AAPS PharmSciTech, 24(2):62, 2023.
[19] Shi, J., Y. Zhang, Q. An, et al. Improving the sublimation stability of ligustrazine with gallic acid by forming pharmaceutical cocrystal based on the etter's rules. Journal of Solid State Chemistry, page 124545, 2024.
[20] Xu, D., G.-Q. Zhang, T.-T. Zhang, et al. Pharmacokinetic comparisons of naringenin and naringenin-nicotinamide cocrystal in rats by lc-ms/ms. Journal of Analytical Methods in Chemistry, 2020, 2020.
[21] Haskins, M. M., O. N. Kavanagh, R. Sanii, et al. Tuning the pharmacokinetic performance of quercetin by cocrystallization. Crystal Growth \&amp; Design, 23(8):6059-6066, 2023.
[22] Bag, P. P. Introduction of plasticity to change mechanical behaviour of pharmaceutical crystals by co-crystallization: a solution of long standing problem in isoniazid. Engineered Science, $15: 129-137,2021$.
[23] Ouyang, J., L. Liu, Y. Li, et al. Cocrystals of carbamazepine: Structure, mechanical properties, fluorescence properties, solubility, and dissolution rate. Particuology, 90:20-30, 2024.
[24] Bryant, M. J., A. G. P. Maloney, R. A. Sykes. Predicting mechanical properties of crystalline materials through topological analysis. CrystEngComm, 20(19):2698-2704, 2018.
[25] Roy, P., A. Ghosh. Mechanochemical cocrystallization to improve the physicochemical properties of chlorzoxazone. CrystEngComm, 22(27):4611-4620, 2020.
[26] Rodrigues, M., J. Lopes, A. Guedes, et al. Considerations on high-throughput cocrystals screening by ultrasound assisted cocrystallization and vibrational spectroscopy. Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy, 229:117876, 2020.
[27] Khudaida, S. H., Y.-T. Yen, C.-S. Su. Cocrystal screening of anticancer drug ptoluenesulfonamide and preparation by supercritical antisolvent process. The Journal of Supercritical Fluids, 204:106106, 2024.
[28] De Almeida, A. F., R. Moreira, T. Rodrigues. Synthetic organic chemistry driven by artificial intelligence. Nature Reviews Chemistry, 3(10):589-604, 2019.
[29] Gormley, A. J., M. A. Webb. Machine learning in combinatorial polymer chemistry. Nature Reviews Materials, 6(8):642-644, 2021.</p>
<p>[30] Cerchia, C., A. Lavecchia. New avenues in artificial-intelligence-assisted drug discovery. Drug Discovery Today, 28(4):103516, 2023.
[31] Westermayr, J., J. Gilkes, R. Barrett, et al. High-throughput property-driven generative design of functional organic molecules. Nature Computational Science, 3(2):139-148, 2023.
[32] Lu, B., Y. Xia, Y. Ren, et al. When machine learning meets 2d materials: A review. Advanced Science, page 2305277, 2024.
[33] Jiang, Y., Z. Yang, J. Guo, et al. Coupling complementary strategy to flexible graph neural network for quick discovery of coformer in diverse co-crystal materials. Nature Communications, 12(1):5950, 2021.
[34] Vriza, A., I. Sovago, D. Widdowson, et al. Molecular set transformer: attending to the co-crystals in the Cambridge structural database. Digital Discovery, 1(6):834-850, 2022.
[35] Gamidi, R. K., A. C. Rasmuson. Analysis and Artificial Neural Network Prediction of Melting Properties and Ideal Mole fraction Solubility of Cocrystals. Crystal Growth \&amp; Design, 20(9):5745-5759, 2020.
[36] Guo, J., M. Sun, X. Zhao, et al. General Graph Neural Network-Based Model To Accurately Predict Cocrystal Density and Insight from Data Quality and Feature Representation. Journal of Chemical Information and Modeling, 63(4):1143-1156, 2023.
[37] Grisoni, F., M. Moret, R. Lingwood, et al. Bidirectional Molecule Generation with Recurrent Neural Networks. Journal of Chemical Information and Modeling, 60(3):1175-1183, 2020.
[38] Li, X., Y. Xu, H. Yao, et al. Chemical space exploration based on recurrent neural networks: applications in discovering kinase inhibitors. Journal of cheminformatics, 12(1):1-13, 2020.
[39] Suresh, N., N. Chinnakonda Ashok Kumar, S. Subramanian, et al. Memory augmented recurrent neural networks for de-novo drug design. Plos one, 17(6):e0269461, 2022.
[40] Dollar, O., N. Joshi, D. A. Beck, et al. Attention-based generative models for de novo molecular design. Chemical Science, 12(24):8362-8372, 2021.
[41] Gómez-Bombarelli, R., J. N. Wei, D. Duvenaud, et al. Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. ACS Central Science, 4(2):268-276, 2018.
[42] Lee, M., K. Min. Mgcvae: multi-objective inverse design via molecular graph conditional variational autoencoder. Journal of Chemical Information and Modeling, 62(12):2943-2950, 2022.
[43] Ochiai, T., T. Inukai, M. Akiyama, et al. Variational autoencoder-based chemical latent space for large molecular structures with 3d complexity. Communications Chemistry, 6(1):249, 2023.
[44] Bhadwal, A. S., K. Kumar, N. Kumar. Gmg-ncdvae: Guided de novo molecule generation using nlp techniques and constrained diverse variational autoencoder. ACM Transactions on Asian and Low-Resource Language Information Processing, 2023.
[45] Guimaraes, G. L., B. Sanchez-Lengeling, C. Outeiral, et al. Objective-Reinforced Generative Adversarial Networks (ORGAN) for Sequence Generation Models. 2017. Publisher: arXiv Version Number: 3.
[46] Prykhodko, O., S. V. Johansson, P.-C. Kotsias, et al. A de novo molecular generation method using latent vector based generative adversarial network. Journal of Cheminformatics, 11(1):74, 2019.
[47] Pang, C., J. Qiao, X. Zeng, et al. Deep generative models in de novo drug molecule generation. Journal of Chemical Information and Modeling, 2023.</p>
<p>[48] Macedo, B., I. Ribeiro Vaz, T. Taveira Gomes. Medgan: optimized generative adversarial network with graph convolutional networks for novel molecule design. Scientific Reports, 14(1):1212, 2024.
[49] Yoshikawa, N., K. Terayama, M. Sumita, et al. Population-based de novo molecule generation, using grammatical evolution. Chemistry Letters, 47(11):1431-1434, 2018.
[50] Leguy, J., T. Cauchy, M. Glavatskikh, et al. Evomol: a flexible and interpretable evolutionary algorithm for unbiased de novo molecular generation. Journal of cheminformatics, 12(1):1-19, 2020.
[51] Kerstjens, A., H. De Winter. Leadd: Lamarckian evolutionary algorithm for de novo drug design. Journal of Cheminformatics, 14(1):1-20, 2022.
[52] Jensen, J. H. A graph-based genetic algorithm and generative model/monte carlo tree search for the exploration of chemical space. Chemical science, 10(12):3567-3572, 2019.
[53] Tripp, A., J. M. Hernández-Lobato. Genetic algorithms are strong baselines for molecule generation. arXiv preprint arXiv:2310.09267, 2023.
[54] Putin, E., A. Asadulaev, Y. Ivanenkov, et al. Reinforced Adversarial Neural Computer for de Novo Molecular Design. Journal of Chemical Information and Modeling, 58(6):1194-1204, 2018.
[55] Thomas, M., N. M. O'Boyle, A. Bender, et al. Augmented hill-climb increases reinforcement learning efficiency for language-based de novo molecule generation. Journal of cheminformatics, 14(1):68, 2022.
[56] Zhavoronkov, A., Y. A. Ivanenkov, A. Aliper, et al. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nature Biotechnology, 37(9):1038-1040, 2019.
[57] Xiong, Y., Y. Wang, Y. Wang, et al. Improving drug discovery with a hybrid deep generative model using reinforcement learning trained on a bayesian docking approximation. Journal of Computer-Aided Molecular Design, 37(11):507-517, 2023.
[58] Kim, H., J. Na, W. B. Lee. Generative chemical transformer: neural machine learning of molecular geometric structures from chemical language via attention. Journal of chemical information and modeling, 61(12):5804-5814, 2021.
[59] Merchant, A., S. Batzner, S. S. Schoenholz, et al. Scaling deep learning for materials discovery. Nature, pages 1-6, 2023.
[60] Bai, J., Z. Lai, R. Yang, et al. Imitation refinement for x-ray diffraction signal processing. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3337-3341. IEEE, 2019.
[61] Bai, J., Y. Du, Y. Wang, et al. Xtal2dos: Attention-based crystal to sequence learning for density of states prediction. arXiv preprint arXiv:2302.01486, 2023.
[62] Gamidi, R. K., A. C. Rasmuson. Estimation of Melting Temperature of Molecular Cocrystals Using Artificial Neural Network Model. Crystal Growth \&amp; Design, 17(1):175-182, 2017.
[63] Rama Krishna, G., M. Ukrainczyk, J. Zeglinski, et al. Prediction of Solid State Properties of Cocrystals Using Artificial Neural Network Modeling. Crystal Growth \&amp; Design, 18(1):133144, 2018.
[64] Fathollahi, M., H. Sajady. Prediction of density of energetic cocrystals based on QSPR modeling using artificial neural network. Structural Chemistry, 29(4):1119-1128, 2018.
[65] Yue, H., J. Wang, M. Lu. Neural Network Prediction Model of Cocrystal Melting Temperature Based on Molecular Descriptors and Graphs. Crystal Growth \&amp; Design, 23(4):2540-2549, 2023.</p>
<p>[66] Guo, T., B. Nan, Z. Liang, et al. What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Advances in Neural Information Processing Systems, 36:59662-59688, 2023.
[67] Jablonka, K. M., P. Schwaller, A. Ortega-Guerrero, et al. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, pages 1-9, 2024.
[68] M. Bran, A., S. Cox, O. Schilter, et al. Augmenting large language models with chemistry tools. Nature Machine Intelligence, pages 1-11, 2024.
[69] Groom, C. R., I. J. Bruno, M. P. Lightfoot, et al. The cambridge structural database. Acta Crystallographica Section B: Structural Science, Crystal Engineering and Materials, 72(2):171179, 2016.
[70] Blaschke, T., J. Arús-Pous, H. Chen, et al. Reinvent 2.0: an ai tool for de novo drug design. Journal of chemical information and modeling, 60(12):5918-5922, 2020.
[71] Hu, F., D. Wang, Y. Hu, et al. Generating novel compounds targeting sars-cov-2 main protease based on imbalanced dataset. In 2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 432-436. IEEE, 2020.
[72] Ishida, S., T. Aasawat, M. Sumita, et al. Chemtsv2: Functional molecular design using de novo molecule generator. Wiley Interdisciplinary Reviews: Computational Molecular Science, 13(6):e1680, 2023.
[73] Li, Y., L. Zhang, Z. Liu. Multi-objective de novo drug design with conditional graph generative model. Journal of cheminformatics, 10:1-24, 2018.
[74] Luo, Y., K. Yan, S. Ji. Graphdf: A discrete flow model for molecular graph generation. In M. Meila, T. Zhang, eds., Proceedings of the 38th International Conference on Machine Learning, vol. 139 of Proceedings of Machine Learning Research, pages 7192-7203. PMLR, 2021.
[75] Fang, Y., X. Pan, H.-B. Shen. De novo drug design by iterative multiobjective deep reinforcement learning with graph-based molecular quality assessment. Bioinformatics, 39(4):btad157, 2023.
[76] Skalic, M., D. Sabbadin, B. Sattarov, et al. From target to drug: generative modeling for the multimodal structure-based ligand design. Molecular pharmaceutics, 16(10):4282-4291, 2019.
[77] Li, Y., J. Pei, L. Lai. Structure-based de novo drug design using 3d deep generative models. Chemical science, 12(41):13664-13675, 2021.
[78] Baillif, B., J. Cole, P. McCabe, et al. Deep generative models for 3d molecular structure. Current Opinion in Structural Biology, 80:102566, 2023.
[79] Martinelli, D. D. Generative machine learning for de novo drug discovery: A systematic review. Computers in Biology and Medicine, 145:105403, 2022.
[80] Bilodeau, C., W. Jin, T. Jaakkola, et al. Generative models for molecular discovery: Recent advances and challenges. WIREs Computational Molecular Science, 12(5), 2022.
[81] d'Autume, C. d. M., M. Rosca, J. Rae, et al. Training language GANs from Scratch, 2020. ArXiv:1905.09922 [cs, stat].
[82] Vaswani, A., N. Shazeer, N. Parmar, et al. Attention is all you need. Advances in neural information processing systems, 30, 2017.
[83] Karita, S., N. Chen, T. Hayashi, et al. A comparative study on transformer vs rnn in speech applications. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 449-456. 2019.
[84] Kingma, D. P., S. Mohamed, D. Jimenez Rezende, et al. Semi-supervised learning with deep generative models. Advances in neural information processing systems, 27, 2014.</p>
<p>[85] Pinchuk, M., G. Kirgizov, L. Yamshchikova, et al. Golem: Flexible evolutionary design of graph representations of physical and digital objects. In Proceedings of the Genetic and Evolutionary Computation Conference Companion, pages 1668-1675. 2024.
[86] Wicker, J. G. P., L. M. Crowley, O. Robshaw, et al. Will they co-crystallize? CrystEngComm, 19(36):5336-5340, 2017.
[87] Yang, D., L. Wang, P. Yuan, et al. Cocrystal virtual screening based on the XGBoost machine learning model. Chinese Chemical Letters, 34(8):107964, 2023.
[88] Wang, D., Z. Yang, B. Zhu, et al. Machine-Learning-Guided Cocrystal Prediction Based on Large Data Base. Crystal Growth \&amp; Design, 20(10):6610-6621, 2020.
[89] Devogelaer, J., H. Meekes, P. Tinnemans, et al. Co-crystal Prediction by Artificial Neural Networks**. Angewandte Chemie International Edition, 59(48):21711-21718, 2020.
[90] Mswahili, M. E., M.-J. Lee, G. L. Martin, et al. Cocrystal Prediction Using Machine Learning Models and Descriptors. Applied Sciences, 11(3):1323, 2021.
[91] Zheng, L., B. Zhu, Z. Wu, et al. Pharmaceutical cocrystal discovery via 3d-sminbr: A new network recommendation tool augmented by 3d molecular conformations. Journal of Chemical Information and Modeling, 63(14):4301-4311, 2023.
[92] Lundberg, S., S.-I. Lee. A Unified Approach to Interpreting Model Predictions. 2017. Publisher: arXiv Version Number: 2.
[93] Lucic, M., K. Kurach, M. Michalski, et al. Are GANs Created Equal? A Large-Scale Study, 2018. ArXiv:1711.10337 [cs, stat].
[94] Jablonka, K. M., Q. Ai, A. Al-Feghali, et al. 14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Digital Discovery, 2(5):1233-1250, 2023.
[95] Zhang, Q., H. Li. Moea/d: A multiobjective evolutionary algorithm based on decomposition. IEEE Transactions on evolutionary computation, 11(6):712-731, 2007.
[96] Zitzler, E., M. Laumanns, L. Thiele. Spea2: Improving the strength pareto evolutionary algorithm. TIK report, 103, 2001.
[97] Mannava, M. C., A. Gunnam, A. Lodagekar, et al. Enhanced solubility, permeability, and tabletability of nicorandil by salt and cocrystal formation. CrystEngComm, 23(1):227-237, 2021.
[98] Kale, D. P., V. Puri, A. Kumar, et al. The role of cocrystallization-mediated altered crystallographic properties on the tabletability of rivaroxaban and malonic acid. Pharmaceutics, 12(6):546, 2020.
[99] Karki, S., T. Friščić, L. Fábián, et al. Improving mechanical properties of crystalline solids by cocrystal formation: new compressible forms of paracetamol. Advanced materials, 21(3839):3905-3909, 2009.
[100] Maeno, Y., T. Fukami, M. Kawahata, et al. Novel pharmaceutical cocrystal consisting of paracetamol and trimethylglycine, a new promising cocrystal former. International journal of pharmaceutics, 473(1-2):179-186, 2014.
[101] Sun, C. C. Materials Science Tetrahedron—A Useful Tool for Pharmaceutical Research and Development. Journal of Pharmaceutical Sciences, 98(5):1671-1687, 2009.
[102] Krishna, G. R., L. Shi, P. P. Bag, et al. Correlation Among Crystal Structure, Mechanical Behavior, and Tabletability in the Co-Crystals of Vanillin Isomers. Crystal Growth \&amp; Design, 15(4):1827-1832, 2015.
[103] Reddy, C. M., M. T. Kirchner, R. C. Gundakaram, et al. Isostructurality, Polymorphism and Mechanical Properties of Some Hexahalogenated Benzenes: The Nature of Halogen-Halogen Interactions. Chemistry - A European Journal, 12(8):2222-2234, 2006.</p>
<p>[104] David, L., A. Thakkar, R. Mercado, et al. Molecular representations in AI-driven drug discovery: a review and practical guide. Journal of Cheminformatics, 12(1):56, 2020.
[105] Killoran, N., L. J. Lee, A. Delong, et al. Generating and designing DNA with deep generative models. 2017. Publisher: arXiv Version Number: 1.
[106] Goodfellow, I. J., J. Pouget-Abadie, M. Mirza, et al. Generative Adversarial Networks. 2014. Publisher: arXiv Version Number: 1.
[107] Schawinski, K., C. Zhang, H. Zhang, et al. Generative Adversarial Networks recover features in astrophysical images of galaxies beyond the deconvolution limit. Monthly Notices of the Royal Astronomical Society: Letters, page slx008, 2017.
[108] De Oliveira, L., M. Paganini, B. Nachman. Learning Particle Physics by Example: LocationAware Generative Adversarial Networks for Physics Synthesis. Computing and Software for Big Science, 1(1):4, 2017.
[109] Wang, C., G. Yang, G. Papanastasiou, et al. DiCyc: GAN-based deformation invariant crossdomain information fusion for medical image synthesis. Information Fusion, 67:147-160, 2021.
[110] Xiong, R., Y. Yang, D. He, et al. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020.
[111] He, J., D. Spokoyny, G. Neubig, et al. Lagging inference networks and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534, 2019.
[112] Bowman, S. R., L. Vilnis, O. Vinyals, et al. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
[113] Holtzman, A., J. Buys, L. Du, et al. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.
[114] Hu, E. J., Y. Shen, P. Wallis, et al. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
[115] Gonçalves, I., S. Silva. Experiments on controlling overfitting in genetic programming. In 15th Portuguese conference on artificial intelligence (EPIA 2011), pages 10-13. 2011.
[116] Ertl, P., A. Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Journal of cheminformatics, $1: 1-11,2009$.
[117] Ye, Z.-H., F. Guo, C.-G. Chai, et al. Searching new cocrystal structures of cl-20 and hmx via evolutionary algorithm and machine learning potential. Journal of Materials Informatics, 4(2):N-A, 2024.
[118] Brown, N., M. Fiscato, M. H. Segler, et al. Guacamol: benchmarking models for de novo molecular design. Journal of chemical information and modeling, 59(3):1096-1108, 2019.</p>
<h1>Appendix</h1>
<h2>A Impact statement</h2>
<p>This paper presents a new method for the generative design of organic co-crystals with the goals to advance application of machine learning to pharmaceutical co-crystal design and to accelerate and reduce cost of development of solid forms of active therapeutic molecules. Extensive experimental results and multiple case studies described in the paper provide hard evidence of the effectiveness of our approach. Therefore, we are confident that this work can have a broader impact on drug discovery and development, pharmaceutical industry in general and other related domains.
While we identify the aforementioned societal impacts as strongly positive, there is a risk of malicious and unintended use, as well as inaccurate predictions affecting decision-making in the drug manufacturing process. However, we deem the potential negative impacts limited due to the complexity of regulations in the corresponding fields and the laboratory experiment being the ultimate measure of success.</p>
<h2>B Results</h2>
<h2>B. 1 Validation experiment</h2>
<p>The validation experiment involved generation of 10000 coformers using ensemble generative models, namely GAN, T-VAE, T-CVAE. The generated candidates in combination with one of the three drugs (Nicorandil, Rivaroxaban, Paracetamol) were labeled into classes of three mechanical plasticity parameters. The criterion for getting into the final dataset was satisfaction of Unobstructed planes and H-bond bridging parameters. Experimentally validated coformers among the generated molecules were then searched for using the Index Tanimoto (IT), which is a metric of molecular structure similarity. Coformers with IT $=1$ were compared with molecules from literature data and added to the Table 3.</p>
<p>Nicorandil. Nicorandil, a medication that dilates blood vessels, is prescribed for treating angina pectoris, a condition characterized by chest pain caused by temporary reduced blood flow to the heart muscle. Unfortunately, during the manufacturing of Nicorandil tablets, the drug can degrade chemically due to the heat produced at high compressive pressure. Generation of a set of coformers for this drug resulted in a fumaric acid molecule among them. Experimental findings have demonstrated that co-crystallizing Nicorandil with fumaric acid not only led to the success of co-crystallization but also improved tabletability properties [97].</p>
<p>Rivaroxaban. Rivaroxaban is an anticoagulant medication that is used to prevent blood clots Rivaroxaban is often taken orally by patients, but the drug is poorly suited for direct compression tableting. The generation of coformers for rivaroxaban using GEMCODE led to the detection of malonic acid among the resulting molecules. According to Kale et al. the formation of this co-crystal leads to its excellent plastic deformation under applied compaction pressure, resulting in successful tablet formation [98].</p>
<p>Paracetamol. Paracetamol is an analgesic and antipyretic from the group of anilides, but forms an unstable tablet by direct pessation. Among the coformers generated by GEMCODE was found experimentally confirmed coformers of paracetamol are naphthalene and trimethylglycine (betaine) [99, 100]. They are not only able to form a co-crystal with paracetamol, but also improves its tabletability, as demonstrated in Karki et al. and Maeno et al..</p>
<h2>B. 2 Discovery experiment</h2>
<p>For the therapeutic molecule Nicorandil, the labeled generated candidates were selected by meeting the conditions of being recognized as safe for human use and presence of carboxyl functional group, which means finding molecules forming the same synthon as experimentally confirmed coformers. The discovered coformers are presented in the Table 4, which is divided into blocks depending on each model used for generation. The CCGNet score column indicates the values of the ranking results</p>
<p>Table 4: Previously unknown novel coformers generated using GEMCODE to improve the tabletability of the drug Nicorandil. SMILES are selected based on a similarity metric (IT $\geq 0.7$ ). Target properties abbreviated as follows: Unobstructed planes (U), Orthogonal planes (O), H-bond bridging $(\mathrm{H})$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Generated SMILES</th>
<th style="text-align: center;">Target properties</th>
<th style="text-align: center;">CCGNet score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GAN</td>
<td style="text-align: center;">$\mathrm{CC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">46.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC}(=\mathrm{O}) \mathrm{NC} 1=\mathrm{CC}=\mathrm{CC}(\mathrm{C}(=\mathrm{O}) \mathrm{O})=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">44.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1 \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">44.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">44.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CN}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">42.67</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">42.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">40.64</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{CC}(\mathrm{C}(=\mathrm{O}) \mathrm{O})=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">39.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{COC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">35.36</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC}(=\mathrm{O}) \mathrm{NC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">33.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CO}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">28.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC} 1 \mathrm{CCCCC1}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">24.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{NC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">23.40</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">21.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">21.94</td>
</tr>
<tr>
<td style="text-align: center;">VAE</td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1 \mathrm{CCCCC1}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">59.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{NC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">51.93</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">51.32</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1 \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">49.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">48.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{NC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1 \mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">48.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{COC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">47.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">46.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">44.86</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">44.45</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC}(=\mathrm{O}) \mathrm{NC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">42.07</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}=\mathrm{CC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">40.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}=\mathrm{CC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">37.96</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC} 1 \mathrm{CCCCC1}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">32.38</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CN}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">25.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC}(\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">24.27</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CO}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">22.18</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">22.01</td>
</tr>
<tr>
<td style="text-align: center;">CVAE</td>
<td style="text-align: center;">$\mathrm{COC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">46.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">42.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">35.53</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{O} / \mathrm{H}$</td>
<td style="text-align: center;">33.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{CC} 1=\mathrm{CC}=\mathrm{C}(\mathrm{C}(=\mathrm{O}) \mathrm{O}) \mathrm{C}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">32.85</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CC} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">31.68</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CNC}(=\mathrm{O}) \mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">31.41</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{CCC}(=\mathrm{O}) \mathrm{C}(=\mathrm{O}) \mathrm{O}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">28.01</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathrm{O}=\mathrm{C}(\mathrm{O}) \mathrm{C} 1 \mathrm{CCCCC1}$</td>
<td style="text-align: center;">$\mathrm{U} / \mathrm{H}$</td>
<td style="text-align: center;">24.09</td>
</tr>
</tbody>
</table>
<p>for the probability of co-crystallization with Nicorandil for the candidate molecules. The ranking was performed within the set of molecules generated by each model separately.</p>
<p>Coformer analysis. Besides carboxyl, the demonstrated chemical compounds contain functional groups such as hydroxyl and amide groups, which are characteristic of the confirmed coformers of Nicorandil from the work of Maeno et al. The generated coformers contain various structural modifications, such as changes in the length of the carbon skeleton, addition and partial substitution of functional groups, the appearance of multiple bonds and benzene rings. It is important to understand that GEMCODE is focused on the search for the best candidates from the point of view of crystallography and does not address the deep issues of interaction of the created structures with the human body. Therefore, with the help of expert evaluation, we selected molecules that should normally be safe for use in pharmaceuticals.</p>
<p>Meanwhile, many of the discovered coformer structures not only already meet all three mechanical parameters consistent with improved tabletability properties, but also have a high probability of successful co-crystalization with Nicorandil. In this way, the process of discovering new coformers using GEMCODE can be described as a smart approach to selecting candidate molecules for propertycontrolled co-crystallization.</p>
<h1>C Data</h1>
<h2>C. 1 Molecule selection criteria</h2>
<p>The ChEMBL database contains information on more than 2.4 million drug-like chemical compounds. For training generative models, we needed a large number of molecular structures that would have similar properties to the known coformers. Therefore, 1.75 million samples were selected from the molecular structures of the ChEMBL database according to the following criteria:</p>
<ul>
<li>Structural type: molecule.</li>
<li>Class: small molecules.</li>
<li>Molecular weight of each component $&lt;600 \mathrm{Da}$.</li>
<li>Number of hydrogen bond donors (HBD) less than 3 and hydrogen bond acceptors (HBA) less than 8 .</li>
<li>Number of rotatable bonds up to 9 .</li>
<li>Polar surface area up to 138 nm .</li>
<li>Number of heavy atoms in molecular structure up to 39 .</li>
</ul>
<h2>C. 2 Co-crystal data</h2>
<p>Mechanical properties of the co-crystals determine their viscoelastic nature. The presence of unobstructed planes and additional slip planes orthogonal to the stacked layers lead to the improved plasticity [101]. Also, there exists evidence that the lack of hydrogen bonding between the layers has a positive effect on the plasticity of a crystal [102, 103]. Therefore, an "ideal" co-crystal in terms of plasticity should have non-overlapping slip planes, additional orthogonal planes and no hydrogen bonding between the planes (Figure 3a).
The compaction properties of many pharmaceutical powders depend on their viscoelastic nature. The closer the material to a perfectly plastic body, the larger the bonding area after compaction of the powder and the denser (less porous) the compressed tablet is (Figure 3b). Therefore, accurate prediction of the plasticity parameters is essential for data-driven co-crystal design.</p>
<h2>C. 3 Representation of molecules</h2>
<p>Traditionally, molecules are represented as structural diagrams with bonds and atoms, but such representations are not well suited for efficient computation. Alternatively, molecules can be represented with SMILES and molecular fingerprints, which have been extensively used for various applications, including the generative models [104]. SMILES notation is often used to describe the composition</p>
<p>and structure of a chemical molecule by means of short strings (Figure 4a). Whereas molecular fingerprints is a way of representing molecules in the vectorized form (Figure 4b). Therefore, molecular fingerprints enable comparing different structures by calculating similarity measures.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: (a) Schematic representation of the mechanical properties of co-crystals. No slip plane and H -bond bridging are associated with low tabletability. The other two properties positively correlate with tabletability. (b) Schematic representation of the particle deformation during powder compression. (c) Number of coformer samples of each category per mechanical property.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Molecular representation using the chemical structure of caffeine as an example in the form of SMILES, molecular fingerprints, and molecular descriptors.</p>
<h1>D Generative models</h1>
<h2>D. 1 GAN</h2>
<p>GANs typically consist of two neural networks, a generator and a discriminator, playing an adversarial game against each other while learning the data distribution $p^{*}(x)$. The generator network receives a random input signal and generates data distribution $p_{\theta}(x)$, while the discriminator network $D_{\phi}(x)$ evaluates the generated data and tries to distinguish it from the real training examples [105]. In the original formulation, both networks are improved by competing with each other following a min-max optimisation procedure:</p>
<p>$$
\min <em _phi="\phi">{\theta} \max </em>(x)\right)\right]
$$} E_{p^{*}(x)}\left[\log D_{\phi}(x)\right]+E_{p_{\theta}(x)}\left[\log \left(1-D_{\phi</p>
<p>Goodfellow et al. proposed alternate generator losses providing better gradients for the generator [106]:</p>
<p>$$
E_{p_{\theta}(x)}\left[-\log \left(D_{\phi}(x)\right)\right]
$$</p>
<p>Since 2014, GANs have been successfully used for numerous applications, including modeling of astronomical phenomena [107], experiments in particle and high-energy physics [108], medical imaging [109], and molecule generation [45, 46]. The GAN takes SMILES representations of molecular structures as input. In the training process, the generator network creates molecular representations from the Gaussian noise and the discriminator network tries to differentiate those from the tokenized SMILES of the real chemical compounds. As a result, the generator learns to output new molecular structures similar to those in the training set.</p>
<h1>D. 2 GAN training and fine-tuning</h1>
<p>The GAN trained on the ChEMBL dataset with batch size of 512 and learning rate of 0.001 consistently produced molecules with validity $&gt;75 \%$ after 25,000 training steps. After 30,000 steps, this model was fine-tuned on the coformer dataset with a smaller batch size of 256 for additional 1,000 steps (Figure 5a). The t-SNE analysis reveals that the molecular space of coformers is considerably more constrained compared to that of ChEMBL (Figure 5b). Therefore, fine-tuning was critical to shift chemical compound generation towards the molecular space of coformers. The final model was able to produce $&gt;95 \%$ of valid and $&gt;86 \%$ of unique chemical structures molecules in the test generation of 1000 molecules at 5 times repetition.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: GAN training results on ChEMBL datasets and coformers: (a) plot of the growth of the valid chemical structures share in a batch, (b) t-SNE visualization of molecules from the ChEMBL dataset and coformers.</p>
<h2>D. 3 VAE and CVAE</h2>
<p>Variational Autoencoders (VAEs) consist of two deep neural networks, namely, an encoder and a decoder. The encoder network takes an input feature vector and converts it into a fixed-dimensional vector, while the decoder network converts this fixed-dimensional vector back to the original input feature vector. The primary objective of an autoencoder is to learn an identity function, and the fixeddimensional vector is referred to as the latent vector $z$. This latent vector $z$ serves as an information bottleneck, meaning that it is designed to capture only the most statistically salient information in the data. In VAEs, the latent vectors $z$ are sampled from a normal distribution $N(0, I)$, where I is the identity matrix. To train VAEs, the loss function, which needs to be optimized for input data $X$ and latent vector $z$, can be formulated as follows:</p>
<p>$$
E\left[\log P_{\theta}(X \mid z)\right]-D_{K L}\left[Q_{\theta^{\prime}}(z \mid X) | N(z)\right]
$$</p>
<p>where $D_{K L}$ is the Kullback-Leibler divergence, which measures the difference between two probability distributions, $Q$ and $N ; E$ is the mathematical expectation; $P$ and $Q$ are probability distributions.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Median probability score can be best described as the median probability of assigning coformers to a positive class for each of the mechanical properties. In other words, median probability score gives an idea about the central tendency of the model's confidence in predicting a particular mechanical property.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1596 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1596</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1596</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-31.html">extraction-schema-31</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <p><strong>Paper ID:</strong> paper-267617108</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.06299v1.pdf" target="_blank">A Functional Analysis Approach to Symbolic Regression</a></p>
                <p><strong>Paper Abstract:</strong> Symbolic regression (SR) poses a significant challenge for randomized search heuristics due to its reliance on the synthesis of expressions for input-output mappings. Although traditional genetic programming (GP) algorithms have achieved success in various domains, they exhibit limited performance when tree-based representations are used for SR. To address these limitations, we introduce a novel SR approach called Fourier Tree Growing (FTG) that draws insights from functional analysis. This new perspective enables us to perform optimization directly in a different space, thus avoiding intricate symbolic expressions. Our proposed algorithm exhibits significant performance improvements over traditional GP methods on a range of classical one-dimensional benchmarking problems. To identify and explain the limiting factors of GP and FTG, we perform experiments on a large-scale polynomials benchmark with high-order polynomials up to degree 100. To the best of the authors' knowledge, this work represents the pioneering application of functional analysis in addressing SR problems. The superior performance of the proposed algorithm and insights into the limitations of GP open the way for further advancing GP for SR and related areas of explainable machine learning.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1596.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1596.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FTG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier Tree Growing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mutation-inspired symbolic regression algorithm that builds an orthogonal (linearly independent) basis of functions in a Hilbert space by iteratively generating function compositions and solving a linear least-squares projection (Gram matrix) to obtain coefficients; designed to avoid manipulating large symbolic expressions directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Fourier Tree Growing (FTG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FTG generates candidate basis functions (compositions of elementary operators) using a ramped half-and-half style generator and only accepts those that are (numerically) linearly independent of previously accepted functions. On each iteration it computes the Gram matrix of accepted basis elements and solves for coefficients (via SVD-stabilized inversion) to obtain the best projection of the target function in the spanned subspace. The algorithm is presented as analogous to a (1+1)-EA in Hilbert space: new basis functions are produced (treated as mutations), checked for independence (a predicate), and when accepted strictly improve the approximation (loss). FTG does not use recombination/crossover between two parent trees.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (symbolic expressions / mathematical expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Generation of a new composition by sampling elementary operators according to a parameterized distribution (an adaptation of ramped half-and-half tree initialization). The paper frames each generated composition as the result of a mutation applied to the current approximation; the mutation is not a small local edit but a full sampled composition that is accepted only if it increases the spanned subspace (inner product predicate). Numerical checks (SVD-based inverse validity) are used to reject candidates that would cause ill-conditioned Gram inverses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>None reported (executability/functionality in the sense of runnable code/programs is not measured). The evaluated objective is approximation loss (squared error) over the training set or L2 inner-product loss in function space.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Span size of the subspace spanned by the polynomial terms (in the Large-Scale Polynomial benchmark); linear independence predicate based on inner product (i.e. adding basis vectors increases dimensionality). Also indirectly measured by tree/node counts (genotypic size).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Qualitative: FTG typically spans fewer dimensions than canonical GP on the LSP benchmark (GP spans larger subspaces more quickly), but FTG reduces loss much faster initially on conventional benchmarks. FTG produces significantly larger trees (more nodes) than GP in LSP experiments (no single-number summary provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Discussed qualitatively: FTG filters out a priori suboptimal functions (increasing accuracy and reducing wasted search) but because FTG enforces linear independence and relies on Gram-matrix inversions it becomes susceptible to numerical instability (ill-conditioned Gram matrices) which causes stagnation — a tradeoff between enforcing informative novelty (new independent basis elements) and numerical executability/stability of the projection operation.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (one-dimensional conventional benchmarks and a Large-Scale Polynomial (LSP) benchmark up to degree 100).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against canonical tree-based GP (with subtree crossover), (1+1)-GP (mutation-only), and (1+λ)-GP (mutation-only).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FTG substantially outperforms canonical GP and mutation-only GP on conventional one-dimensional SR benchmarks in terms of success rate and number of fitness evaluations to reach a given tolerance (FTG often achieved ~100% success rate and orders-of-magnitude fewer function evaluations on those benchmarks). On the LSP (high-degree polynomial) benchmarks FTG quickly reduces loss initially but then stagnates due to ill-conditioned Gram matrices (condition numbers observed up to ~3.7×10^17), while mutation/recombination-based GP spans larger subspaces and can eventually outperform FTG on very high-degree polynomials (e.g., degree 100) because GP is better at searching the space of constants and structure jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Functional Analysis Approach to Symbolic Regression', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1596.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1596.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canonical-GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonical tree-based Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard tree-based GP using subtree crossover and subtree mutation (ramped half-and-half initialization), evolving populations of parse-tree programs to minimize expression-fit loss; used as a baseline in many SR studies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Canonical tree-based Genetic Programming (Canonical-GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Population-based evolutionary algorithm operating on tree parse representations (LISP-style s-expressions). Variation includes subtree crossover (probabilistic exchange of subtrees between two parent trees) and subtree mutation (replace a subtree with a newly generated subtree). Selection via tournament selection and Pareto/fitness-driven replacement as configured; ramped half-and-half initialization used to seed population.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (symbolic expressions / mathematical expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Subtree crossover: two parent trees are selected and a random subtree is chosen from each and swapped, producing two offspring. Crossover is applied with high probability (paper reports crossover rate Cp = 0.9 in experimental configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Probabilistic subtree mutation: with configured mutation rate (paper lists Mp ≈ 0.1 for canonical-GP), a node/subtree is replaced by a freshly generated subtree (ramped half-and-half generator). The paper also references uniform subtree mutation in mutation-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>None beyond standard fitness/loss (MSE / squared error) and success rate; no executability or runtime-correctness metrics for generated programs reported.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Implicitly measured via 'span size' in the LSP benchmark (number of polynomial terms / dimensions that the population spans) and by population statistics such as number of individuals improving best-so-far and tree sizes (phenotypic/genotypic diversity proxies).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Canonical-GP tended to span greater subspaces in the LSP (closer to target polynomial span) and produced solutions with spans approximately matching the target polynomial's dimensionality, but converged slower in loss than FTG on conventional benchmarks. Canonical-GP also suffered from bloat (increased tree sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Discussed qualitatively: canonical GP's crossover and mutation can produce disruptive changes (limited locality) causing noisy search and bloat; recombination allows exploration of structural variations that help span larger subspaces (beneficial for polynomials), but can reduce selective pressure and slow convergence relative to FTG's filtered acceptance of informative basis functions.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (conventional one-dimensional benchmarks and LSP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against FTG, (1+1)-GP, and (1+λ)-GP in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Canonical-GP spans larger subspaces and finds approximate polynomial structure (degrees) but typically requires more fitness evaluations and exhibits lower success rates at strict tolerances compared to FTG on conventional benchmarks; it also shows bloat and difficulty optimizing constants precisely, limiting its efficiency on exact approximation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Functional Analysis Approach to Symbolic Regression', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1596.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1596.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mutation-only GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>(1+1)-GP and (1+λ)-GP (mutation-only Genetic Programming)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of GP that use only mutation-based variation in a (1+1) or (1+λ) evolutionary algorithm framework (no crossover), typically employing uniform subtree mutation to produce offspring from a single parent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>(1+1)-GP and (1+λ)-GP (mutation-only GP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evolutionary strategies where new offspring are generated solely by subtree mutation (uniform subtree generation). (1+1)-GP maintains a single parent and accepts an offspring if it is better; (1+λ)-GP generates λ mutated offspring and selects among them. These variants aim to reduce bloat and disruptive recombination by avoiding crossover.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (symbolic expressions / mathematical expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Uniform subtree mutation: replace a randomly selected subtree with a subtree sampled from the ramped half-and-half generator (in (1+1)-GP and (1+λ)-GP the mutation rate is effectively 1 for offspring generation as configured).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td>Measured only via approximation loss (squared error) and success rate; no separate executability or runtime validity metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Span size and population-based measures (for (1+λ)-GP where λ>1): number of individuals that improved best-so-far and spanned a larger subspace; genotypic diversity via tree sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>Mutation-only GP showed less bloat and in some polynomial cases converged faster than canonical-GP while having similar final loss and span, indicating mutation encourages effective exploration of constants and local structure. On some LSP instances (polynomials) mutation-only GP outperformed canonical-GP in spanning and convergence speed.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td>Qualitatively discussed: mutation-only variation is less disruptive in terms of bloat and can be beneficial for optimizing constants and spanning subspaces for polynomials, indicating a tradeoff where removing crossover increases locality and reduces destructive novelty while preserving useful exploratory moves.</td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Symbolic regression (conventional benchmarks and LSP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against FTG and canonical-GP in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mutation-only GP (1+λ and 1+1) suffers less from bloat and sometimes converges faster than canonical-GP; it can span subspaces as effectively as canonical-GP on polynomial targets and in some LSP cases outperforms canonical-GP, suggesting recombination is not always beneficial and mutation-only strategies can be competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Functional Analysis Approach to Symbolic Regression', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1596.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1596.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Geometric Semantic GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Geometric Semantic Genetic Programming</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GP variant that performs semantic-level variation (operators that directly manipulate program semantics/output values) rather than syntactic subtree operators, designed to improve locality in the search space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Geometric Semantic Genetic Programming (GSGP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>GSGP defines variation operators that work directly in semantic space (e.g., linear combinations of parent outputs) to produce offspring with predictable semantic effects and better locality than standard syntactic crossover/mutation. In the paper GSGP is mentioned in related work as an example of semantic-level variation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (symbolic expressions) — semantics manipulated rather than syntax</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td>Semantic recombination (not detailed in this paper) — typically combines parents' outputs to obtain offspring whose semantics are a geometric interpolation/combination of parents.</td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Semantic mutation (not detailed in this paper) — typically adds small semantic perturbations to program outputs via specific operator constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Mentioned as related work in symbolic regression literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mention only; not experimentally compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper cites GSGP as a recent model focusing on semantic variation to improve locality compared to traditional syntactic GP, but provides no experimental data for GSGP in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Functional Analysis Approach to Symbolic Regression', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1596.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1596.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of genetic/evolutionary algorithms that use crossover and mutation operations on code, programs, or literature to generate new solutions, with particular attention to measures of novelty, diversity, executability, and functionality.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RampedHalfAndHalf / generate-composition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ramped half-and-half initialization / generate-composition procedure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conventional GP tree-initialization method adapted in this work as the stochastic generator for composing new candidate functions; used both for initial populations and for FTG's generate-composition (mutation) operator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Ramped half-and-half (generate-composition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The generate-composition procedure is an adaptation of the standard ramped half-and-half tree initialization: it samples operators according to a depth-conditioned distribution parameterized by p, min and max depths, and the unary/binary selection probability, producing random compositions (trees) used either to initialize GP populations or as the generator for FTG candidate basis functions. This is the mechanism that produces new function/program candidates (treated as mutation proposals).</td>
                        </tr>
                        <tr>
                            <td><strong>input_type</strong></td>
                            <td>programs (syntactic trees / symbolic expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>crossover_operation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mutation_operation</strong></td>
                            <td>Used to generate replacement subtrees (mutation) or full candidate compositions; sampling uses a distribution E(p, d, l, u) described in the paper, where p controls the probability of sampling unary vs constant/leaf nodes at intermediate depths.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_literature</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>uses_code</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>executability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>diversity_metric</strong></td>
                            <td>Produces syntactic/genotypic diversity via random sampling across depths and operator choices.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_results</strong></td>
                            <td>No numeric diversity statistics specifically for the generator alone, but it is the source of candidate novelty for FTG and GP initial populations; parameters used in experiments: p=0.5, min depth=1, max depth=9.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_executability_tradeoff</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frontier_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_domain</strong></td>
                            <td>Used across all SR experiments in the paper (conventional and LSP).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Standard GP initialisation method; used identically across compared algorithms to avoid bias.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The authors used ramped half-and-half both for GP initialization and as FTG's candidate generator because of its simplicity and lack of operator bias; performance differences among algorithms therefore arise from selection/acceptance and projection mechanics rather than the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Functional Analysis Approach to Symbolic Regression', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Genetic Programming: On the Programming of Computers by Means of Natural Selection <em>(Rating: 2)</em></li>
                <li>Genetic Programming II: Automatic Discovery of Reusable Programs <em>(Rating: 1)</em></li>
                <li>Geometric Semantic Genetic Programming <em>(Rating: 2)</em></li>
                <li>Towards an Understanding of Locality in Genetic Programming <em>(Rating: 1)</em></li>
                <li>Ramped half-and-half <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1596",
    "paper_id": "paper-267617108",
    "extraction_schema_id": "extraction-schema-31",
    "extracted_data": [
        {
            "name_short": "FTG",
            "name_full": "Fourier Tree Growing",
            "brief_description": "A mutation-inspired symbolic regression algorithm that builds an orthogonal (linearly independent) basis of functions in a Hilbert space by iteratively generating function compositions and solving a linear least-squares projection (Gram matrix) to obtain coefficients; designed to avoid manipulating large symbolic expressions directly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Fourier Tree Growing (FTG)",
            "system_description": "FTG generates candidate basis functions (compositions of elementary operators) using a ramped half-and-half style generator and only accepts those that are (numerically) linearly independent of previously accepted functions. On each iteration it computes the Gram matrix of accepted basis elements and solves for coefficients (via SVD-stabilized inversion) to obtain the best projection of the target function in the spanned subspace. The algorithm is presented as analogous to a (1+1)-EA in Hilbert space: new basis functions are produced (treated as mutations), checked for independence (a predicate), and when accepted strictly improve the approximation (loss). FTG does not use recombination/crossover between two parent trees.",
            "input_type": "programs (symbolic expressions / mathematical expressions)",
            "crossover_operation": null,
            "mutation_operation": "Generation of a new composition by sampling elementary operators according to a parameterized distribution (an adaptation of ramped half-and-half tree initialization). The paper frames each generated composition as the result of a mutation applied to the current approximation; the mutation is not a small local edit but a full sampled composition that is accepted only if it increases the spanned subspace (inner product predicate). Numerical checks (SVD-based inverse validity) are used to reject candidates that would cause ill-conditioned Gram inverses.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "None reported (executability/functionality in the sense of runnable code/programs is not measured). The evaluated objective is approximation loss (squared error) over the training set or L2 inner-product loss in function space.",
            "executability_results": null,
            "diversity_metric": "Span size of the subspace spanned by the polynomial terms (in the Large-Scale Polynomial benchmark); linear independence predicate based on inner product (i.e. adding basis vectors increases dimensionality). Also indirectly measured by tree/node counts (genotypic size).",
            "diversity_results": "Qualitative: FTG typically spans fewer dimensions than canonical GP on the LSP benchmark (GP spans larger subspaces more quickly), but FTG reduces loss much faster initially on conventional benchmarks. FTG produces significantly larger trees (more nodes) than GP in LSP experiments (no single-number summary provided in paper).",
            "novelty_executability_tradeoff": "Discussed qualitatively: FTG filters out a priori suboptimal functions (increasing accuracy and reducing wasted search) but because FTG enforces linear independence and relies on Gram-matrix inversions it becomes susceptible to numerical instability (ill-conditioned Gram matrices) which causes stagnation — a tradeoff between enforcing informative novelty (new independent basis elements) and numerical executability/stability of the projection operation.",
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (one-dimensional conventional benchmarks and a Large-Scale Polynomial (LSP) benchmark up to degree 100).",
            "comparison_baseline": "Compared against canonical tree-based GP (with subtree crossover), (1+1)-GP (mutation-only), and (1+λ)-GP (mutation-only).",
            "key_findings": "FTG substantially outperforms canonical GP and mutation-only GP on conventional one-dimensional SR benchmarks in terms of success rate and number of fitness evaluations to reach a given tolerance (FTG often achieved ~100% success rate and orders-of-magnitude fewer function evaluations on those benchmarks). On the LSP (high-degree polynomial) benchmarks FTG quickly reduces loss initially but then stagnates due to ill-conditioned Gram matrices (condition numbers observed up to ~3.7×10^17), while mutation/recombination-based GP spans larger subspaces and can eventually outperform FTG on very high-degree polynomials (e.g., degree 100) because GP is better at searching the space of constants and structure jointly.",
            "uuid": "e1596.0",
            "source_info": {
                "paper_title": "A Functional Analysis Approach to Symbolic Regression",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Canonical-GP",
            "name_full": "Canonical tree-based Genetic Programming",
            "brief_description": "Standard tree-based GP using subtree crossover and subtree mutation (ramped half-and-half initialization), evolving populations of parse-tree programs to minimize expression-fit loss; used as a baseline in many SR studies.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Canonical tree-based Genetic Programming (Canonical-GP)",
            "system_description": "Population-based evolutionary algorithm operating on tree parse representations (LISP-style s-expressions). Variation includes subtree crossover (probabilistic exchange of subtrees between two parent trees) and subtree mutation (replace a subtree with a newly generated subtree). Selection via tournament selection and Pareto/fitness-driven replacement as configured; ramped half-and-half initialization used to seed population.",
            "input_type": "programs (symbolic expressions / mathematical expressions)",
            "crossover_operation": "Subtree crossover: two parent trees are selected and a random subtree is chosen from each and swapped, producing two offspring. Crossover is applied with high probability (paper reports crossover rate Cp = 0.9 in experimental configuration).",
            "mutation_operation": "Probabilistic subtree mutation: with configured mutation rate (paper lists Mp ≈ 0.1 for canonical-GP), a node/subtree is replaced by a freshly generated subtree (ramped half-and-half generator). The paper also references uniform subtree mutation in mutation-only variants.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "None beyond standard fitness/loss (MSE / squared error) and success rate; no executability or runtime-correctness metrics for generated programs reported.",
            "executability_results": null,
            "diversity_metric": "Implicitly measured via 'span size' in the LSP benchmark (number of polynomial terms / dimensions that the population spans) and by population statistics such as number of individuals improving best-so-far and tree sizes (phenotypic/genotypic diversity proxies).",
            "diversity_results": "Canonical-GP tended to span greater subspaces in the LSP (closer to target polynomial span) and produced solutions with spans approximately matching the target polynomial's dimensionality, but converged slower in loss than FTG on conventional benchmarks. Canonical-GP also suffered from bloat (increased tree sizes).",
            "novelty_executability_tradeoff": "Discussed qualitatively: canonical GP's crossover and mutation can produce disruptive changes (limited locality) causing noisy search and bloat; recombination allows exploration of structural variations that help span larger subspaces (beneficial for polynomials), but can reduce selective pressure and slow convergence relative to FTG's filtered acceptance of informative basis functions.",
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (conventional one-dimensional benchmarks and LSP).",
            "comparison_baseline": "Compared against FTG, (1+1)-GP, and (1+λ)-GP in experiments.",
            "key_findings": "Canonical-GP spans larger subspaces and finds approximate polynomial structure (degrees) but typically requires more fitness evaluations and exhibits lower success rates at strict tolerances compared to FTG on conventional benchmarks; it also shows bloat and difficulty optimizing constants precisely, limiting its efficiency on exact approximation tasks.",
            "uuid": "e1596.1",
            "source_info": {
                "paper_title": "A Functional Analysis Approach to Symbolic Regression",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mutation-only GP",
            "name_full": "(1+1)-GP and (1+λ)-GP (mutation-only Genetic Programming)",
            "brief_description": "Variants of GP that use only mutation-based variation in a (1+1) or (1+λ) evolutionary algorithm framework (no crossover), typically employing uniform subtree mutation to produce offspring from a single parent.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "(1+1)-GP and (1+λ)-GP (mutation-only GP)",
            "system_description": "Evolutionary strategies where new offspring are generated solely by subtree mutation (uniform subtree generation). (1+1)-GP maintains a single parent and accepts an offspring if it is better; (1+λ)-GP generates λ mutated offspring and selects among them. These variants aim to reduce bloat and disruptive recombination by avoiding crossover.",
            "input_type": "programs (symbolic expressions / mathematical expressions)",
            "crossover_operation": null,
            "mutation_operation": "Uniform subtree mutation: replace a randomly selected subtree with a subtree sampled from the ramped half-and-half generator (in (1+1)-GP and (1+λ)-GP the mutation rate is effectively 1 for offspring generation as configured).",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": "Measured only via approximation loss (squared error) and success rate; no separate executability or runtime validity metrics reported.",
            "executability_results": null,
            "diversity_metric": "Span size and population-based measures (for (1+λ)-GP where λ&gt;1): number of individuals that improved best-so-far and spanned a larger subspace; genotypic diversity via tree sizes.",
            "diversity_results": "Mutation-only GP showed less bloat and in some polynomial cases converged faster than canonical-GP while having similar final loss and span, indicating mutation encourages effective exploration of constants and local structure. On some LSP instances (polynomials) mutation-only GP outperformed canonical-GP in spanning and convergence speed.",
            "novelty_executability_tradeoff": "Qualitatively discussed: mutation-only variation is less disruptive in terms of bloat and can be beneficial for optimizing constants and spanning subspaces for polynomials, indicating a tradeoff where removing crossover increases locality and reduces destructive novelty while preserving useful exploratory moves.",
            "frontier_characterization": null,
            "benchmark_or_domain": "Symbolic regression (conventional benchmarks and LSP).",
            "comparison_baseline": "Compared against FTG and canonical-GP in experiments.",
            "key_findings": "Mutation-only GP (1+λ and 1+1) suffers less from bloat and sometimes converges faster than canonical-GP; it can span subspaces as effectively as canonical-GP on polynomial targets and in some LSP cases outperforms canonical-GP, suggesting recombination is not always beneficial and mutation-only strategies can be competitive.",
            "uuid": "e1596.2",
            "source_info": {
                "paper_title": "A Functional Analysis Approach to Symbolic Regression",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Geometric Semantic GP",
            "name_full": "Geometric Semantic Genetic Programming",
            "brief_description": "A GP variant that performs semantic-level variation (operators that directly manipulate program semantics/output values) rather than syntactic subtree operators, designed to improve locality in the search space.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Geometric Semantic Genetic Programming (GSGP)",
            "system_description": "GSGP defines variation operators that work directly in semantic space (e.g., linear combinations of parent outputs) to produce offspring with predictable semantic effects and better locality than standard syntactic crossover/mutation. In the paper GSGP is mentioned in related work as an example of semantic-level variation.",
            "input_type": "programs (symbolic expressions) — semantics manipulated rather than syntax",
            "crossover_operation": "Semantic recombination (not detailed in this paper) — typically combines parents' outputs to obtain offspring whose semantics are a geometric interpolation/combination of parents.",
            "mutation_operation": "Semantic mutation (not detailed in this paper) — typically adds small semantic perturbations to program outputs via specific operator constructs.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": null,
            "diversity_results": null,
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Mentioned as related work in symbolic regression literature.",
            "comparison_baseline": "Mention only; not experimentally compared in this paper.",
            "key_findings": "Paper cites GSGP as a recent model focusing on semantic variation to improve locality compared to traditional syntactic GP, but provides no experimental data for GSGP in this work.",
            "uuid": "e1596.3",
            "source_info": {
                "paper_title": "A Functional Analysis Approach to Symbolic Regression",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RampedHalfAndHalf / generate-composition",
            "name_full": "Ramped half-and-half initialization / generate-composition procedure",
            "brief_description": "A conventional GP tree-initialization method adapted in this work as the stochastic generator for composing new candidate functions; used both for initial populations and for FTG's generate-composition (mutation) operator.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Ramped half-and-half (generate-composition)",
            "system_description": "The generate-composition procedure is an adaptation of the standard ramped half-and-half tree initialization: it samples operators according to a depth-conditioned distribution parameterized by p, min and max depths, and the unary/binary selection probability, producing random compositions (trees) used either to initialize GP populations or as the generator for FTG candidate basis functions. This is the mechanism that produces new function/program candidates (treated as mutation proposals).",
            "input_type": "programs (syntactic trees / symbolic expressions)",
            "crossover_operation": null,
            "mutation_operation": "Used to generate replacement subtrees (mutation) or full candidate compositions; sampling uses a distribution E(p, d, l, u) described in the paper, where p controls the probability of sampling unary vs constant/leaf nodes at intermediate depths.",
            "uses_literature": false,
            "uses_code": false,
            "novelty_metric": null,
            "novelty_results": null,
            "executability_metric": null,
            "executability_results": null,
            "diversity_metric": "Produces syntactic/genotypic diversity via random sampling across depths and operator choices.",
            "diversity_results": "No numeric diversity statistics specifically for the generator alone, but it is the source of candidate novelty for FTG and GP initial populations; parameters used in experiments: p=0.5, min depth=1, max depth=9.",
            "novelty_executability_tradeoff": null,
            "frontier_characterization": null,
            "benchmark_or_domain": "Used across all SR experiments in the paper (conventional and LSP).",
            "comparison_baseline": "Standard GP initialisation method; used identically across compared algorithms to avoid bias.",
            "key_findings": "The authors used ramped half-and-half both for GP initialization and as FTG's candidate generator because of its simplicity and lack of operator bias; performance differences among algorithms therefore arise from selection/acceptance and projection mechanics rather than the generator.",
            "uuid": "e1596.4",
            "source_info": {
                "paper_title": "A Functional Analysis Approach to Symbolic Regression",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Genetic Programming: On the Programming of Computers by Means of Natural Selection",
            "rating": 2,
            "sanitized_title": "genetic_programming_on_the_programming_of_computers_by_means_of_natural_selection"
        },
        {
            "paper_title": "Genetic Programming II: Automatic Discovery of Reusable Programs",
            "rating": 1,
            "sanitized_title": "genetic_programming_ii_automatic_discovery_of_reusable_programs"
        },
        {
            "paper_title": "Geometric Semantic Genetic Programming",
            "rating": 2,
            "sanitized_title": "geometric_semantic_genetic_programming"
        },
        {
            "paper_title": "Towards an Understanding of Locality in Genetic Programming",
            "rating": 1,
            "sanitized_title": "towards_an_understanding_of_locality_in_genetic_programming"
        },
        {
            "paper_title": "Ramped half-and-half",
            "rating": 1,
            "sanitized_title": "ramped_halfandhalf"
        }
    ],
    "cost": 0.01688,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Functional Analysis Approach to Symbolic Regression
9 Feb 2024</p>
<p>Kirill Antonov k.antonov@liacs.leidenuniv.nl 
LIACS
Leiden University Leiden
The Netherlands</p>
<p>Roman Kalkreuth roman.kalkreuth@lip6.fr 
CNRS
Sorbonne Université Paris
LIP6France</p>
<p>Kaifeng Yang kaifeng.yang@fh-hagenberg.at 
University of Applied Sciences Upper Austria Hagenger
Austria</p>
<p>Thomas Bäck 
LIACS
Leiden University Leiden
The Netherlands</p>
<p>Niki Van Stein n.van.stein@liacs.leidenuniv.nl 
LIACS
Leiden University Leiden
The Netherlands</p>
<p>Anna V Kononova a.kononova@liacs.leidenuniv.nl 
LIACS
Leiden University Leiden
The Netherlands</p>
<p>A Functional Analysis Approach to Symbolic Regression
9 Feb 2024F20F25642C3BE3AC94C63E0EA6AEF566arXiv:2402.06299v1[cs.NE]
Symbolic regression (SR) poses a significant challenge for randomized search heuristics due to its reliance on the synthesis of expressions for input-output mappings.Although traditional genetic programming (GP) algorithms have achieved success in various domains, they exhibit limited performance when tree-based representations are used for SR.To address these limitations, we introduce a novel SR approach called Fourier Tree Growing (FTG) that draws insights from functional analysis.This new perspective enables us to perform optimization directly in a different space, thus avoiding intricate symbolic expressions.Our proposed algorithm exhibits significant performance improvements over traditional GP methods on a range of classical one-dimensional benchmarking problems.To identify and explain limiting factors of GP and FTG, we perform experiments on a large-scale polynomials benchmark with high-order polynomials up to degree 100.To the best of the authors' knowledge, this work represents the pioneering application of functional analysis in addressing SR problems.The superior performance of the proposed algorithm and insights into the limitations of GP open the way for further advancing GP for SR and related areas of explainable machine learning.</p>
<p>INTRODUCTION</p>
<p>Symbolic regression (SR) can be considered a major problem domain for search heuristics that focus on the synthesis of symbolic expressions.SR as a black-box optimization domain, aims at the derivation of mathematical expressions that are able to fit the inputoutput mapping of an unknown objective function to a predefined degree.In the wider domain of search heuristics, SR defines a diverse problem domain that has often been used to benchmark the performance of symbolic search algorithms but also offers a realworld application domain for such methods.Quite recently, it has been proven that SR is an NP-hard problem in view of the fact that it is not always possible to find the best-fitting mathematical expression for a given data set in polynomial time [44].GP has been applied to SR problems since its early days.J. Koza [20,22,23] reported a series of results for the synthesis of symbolic expressions that can fit the functional behavior of polynomials of lower degrees by using a parse-tree representation model inspired by LISP s-expressions, which is now known as tree-based GP in the wider family of GP representations.Based on Koza's experiments, SR evolved to a major application domain for GP and was found to be the most popular problem in the first survey on benchmarking standards in GP at that time [30].Besides the works that reported practical-oriented results by using GP, different aspects of GP have been analyzed since the early days of GP to understand its search behavior.Moreover, even if GP has been found to be a suitable method for SR, drawbacks and shortcomings that impede the effectiveness of the heuristic search in GP have been identified and studied in the past.Both, empirical and theoretical results, have been proposed to understand various properties of the evolutionary-inspired search mechanism, such as evolvability [2], locality [11], fitness landscapes &amp; problem hardness [41], neutrality [15,42] and search &amp; runtime complexity [26,27,29].Overall, works in these areas has made a significant contribution to the understanding of the heuristic search performed by GP.</p>
<p>Despite recent advancements, the challenge of disruptive mutations and recombinations persists in SR, hindering the development of more efficient randomized search heuristics (RSH) in this domain.In this paper, therefore, we take a step forward in the understanding of SR by considering the search from a general perspective of functional analysis (FA).We start by reformulating the SR problem as a classical norm-minimization problem in Hilbert Space.To solve this problem, we propose a novel method called Fourier Tree Growing (FTG) that is inspired by mutation-based GP as performed with a (1+) evolutionary algorithm (EA) and ramped half-and-half initialization that allows us to navigate in the considered Hilbert space.</p>
<p>Our experiments demonstrate that this method significantly outperforms the compared GP algorithms on classical one-dimensional benchmarking problems.We explain the observed gap in performance by considering the dynamics of the GP search process.To enable a detailed study, we propose a novel benchmark with highorder polynomials, which we call a large-scale polynomial benchmark (LSP).We apply our proposed FTG method , conventional recombination-based GP as well as mutation-only GP to solve instances of LSP to identify and analyze shortcomings and limitations in the SR domain.We address the observed shortcomings and limitations of GP and FTG and discuss ways to overcome these drawbacks.Moreover, based on our theoretical and empirical findings, we discuss how some aspects of FA in general could be used in GP to benefit its application to SR.The work presented in this paper aims at the following objectives:</p>
<p>• Reformulation of the SR problem by means of FA • Optimization in Hilbert space</p>
<p>• Evaluation of GP and FTG in the SR domain • Identification of aspects that could benefit GP in addressing SR problems</p>
<p>The results of this work can be easily adapted to canonical GP-based SR with a dimensionality reduction technique.The paper is structured as follows: In Section 2 we briefly describe GP and SR and provide the preliminaries for the proposed FTG algorithm.In Section 3 we briefly formalize the corresponding SR problem statement and establish the framework for reformulation of the SR search problem as an optimization problem in Hilbert space.</p>
<p>In Section 4 we propose FTG that performs SR search in Hibert space and rigorously study FTG's properties, and provide intuition on how it addresses SR.Section 5 is devoted to the presentation of empirical comparisons of the proposed algorithms against traditional GP search heuristics on conventional benchmarks and our proposed LSP benchmark.In Section 6 we then discuss the results of our experiments.Finally, Section 7 concludes our work and outlines our future work.</p>
<p>RELATED WORK 2.1 Genetic Programming</p>
<p>Genetic Programming (GP) is a evolutionary-inspired search heuristic originally invented to enable the synthesis of computer programs for problem-solving.The main paradigm of GP is to evolve a population of computer programs towards an algorithmic solution of a predefined problem.To accomplish this, GP transforms populations of candidate genetic programs, that are traditionally represented as parse-trees, iteratively from generation to generation into new populations of programs with (hopefully) better fitness.However, since GP is a stochastic optimization process, it can consequently not guarantee to achieve the optimal solution.The first significant work in the field of GP was done by Forsyth [10], Cramer [7], and Hicklin [13].However, GP gained significantly more popularity when Koza applied his parse tree representation model, inspired by LISP S expressions, to several types of problems, for instance, symbolic regression, algorithm construction, logic synthesis, or classification [20,22,23].Besides the traditional tree-based representation model, GP variants with linear sequence representations [33,34], graph-based [31,35] representations, or grammar-based representations [37] have been proposed.Traditional GP models variate candidate programs on a syntactical level while one of the most recently introduced GP models, Geometric Semantic GP, focuses on variation of candidate programs on a semantic level [32].However, among the different forms of GP, tree-based GP can be considered the most popular representation model since it gained significant recognition in the evolutionary computation (EC) domain due to the experiments of Koza in problem domains that were practically relevant.However, despite its reputation in the field of EC for achieving practical results, GP suffers from drawbacks and shortcomings that have been found to hinder the effectiveness of the heuristic search.In the tree-based subdomain of GP, the most well-known drawback is Bloat that is characterized by an uncontrolled growth of the average size of candidate trees in the population [38].Unlike most heuristic methods for numerical optimization in the EC domain, standard GP search operators such as subtree crossover and mutation provide only limited locality features [11], and have been found to be disruptive [3].</p>
<p>Symbolic Regression in Genetic Programming</p>
<p>Symbolic regression can be classified in the taxonomy of regression analysis, where a symbolic search on a space of mathematical functions is performed to find candidate functions that fit the ideal input-output mapping of a given dataset as close as possible, where the quality of the fit is typically measured on a given, finite set of data points.Symbolic regression in GP can therefore be considered a black-box problem that forms a major problem domain in the application scope of GP since its very early days.In general, SR by means of GP relates to the application of GP models to synthesize mathematical expressions that represent the (unknown) function's input-output mapping as closely as possible.Symbolic regression gained prominence through Koza's pioneering efforts in the 1990s; however, the problem of finding a mathematical expression to explain empirical measurements was already introduced in the previous works [9,12,25].In the early works of SR-based GP, Koza showed that GP can be used to discover SR models by encoding mathematical expressions as computational trees.Even though SR can be addressed by other algorithms (such as Monte Carlo tree search [5,39], enumeration algorithms [17], greedy algorithms [8], mixed-integer nonlinear programming [6]), GP remains a popular choice.So far, SR through GP has been applied in different areas, such as economics [43], medicine [45], engineering [24] and more [46].However, SR via GP still has some limitations, such as its gray-box property [19], model's over complexity [16], and various models (with different structural properties, utilized variables) [1].</p>
<p>Preliminaries</p>
<p>Functional Analysis is a branch of mathematical analysis that studies functions, spaces of functions, and relationships between those spaces.One of the fruitful ideas used in FA is the notion of a Banach Space of which Hilbert Space is an important special case We will briefly introduce this notion, and some relevant facts about it in this section because the notion of a Hilbert Space plays an important role in our work.</p>
<p>Definition 1 (Metric Space).</p>
<p>A metric space is a pair (, ), where  is a set and  is a real-value function on  ×  which satisfies that, for any , ,  ∈  ,</p>
<p>(1)  (, ) ≥ 0 and  (, )
= 0 ⇐⇒ 𝑥 = 𝑦, (2) 𝑑 (𝑥, 𝑦) = 𝑑 (𝑦, 𝑥), (3) 𝑑 (𝑥, 𝑧) ≤ 𝑑 (𝑥, 𝑦) + 𝑑 (𝑦, 𝑧).
The function  is called the metric on  .</p>
<p>Definition 2 (Normed Vector Space).A vector space  over field R is called a normed vector space if there is real-value function ∥•∥ on  , called the norm, such that for any ,  ∈  and any  ∈ R,
(1) ∥𝑥 ∥ ≥ 0 (2) ∥𝑥 ∥ = 0 ⇐⇒ 𝑥 = 0, (3) ∥𝛼𝑥 ∥ = |𝛼 | ∥𝑥 ∥, (4) ∥𝑥 + 𝑦 ∥ ≤ ∥𝑥 ∥ + ∥𝑦 ∥. Definition 3 (Cauchy Sequence). A sequence {𝑥 𝑛 } in a metric space (𝑋, 𝑑) is a Cauchy Sequence if ∀𝜀 &gt; 0, ∃𝑁 ∈ N : 𝑑 (𝑥 𝑛 , 𝑥 𝑚 ) &lt; 𝜀 ∀𝑛, 𝑚 &gt; 𝑁 .
Definition 4 (Complete Space).A normed space is called complete if every Cauchy sequence in the space converges to an element from this space.</p>
<p>Definition 5 (Inner Product Space).An inner product space is a vector space  over the field R together with an inner product, that is a map: ⟨•, •⟩  :  ×  → R with the following properties:</p>
<p>(1)
⟨𝛼 (𝑥 + 𝑦), 𝑧⟩ 𝑉 = 𝛼 ⟨𝑥, 𝑧⟩ 𝑉 + 𝛼 ⟨𝑦, 𝑧⟩ 𝑉 , (2) ⟨𝑥, 𝑦⟩ 𝑉 = ⟨𝑦, 𝑥⟩ 𝑉 , (3) ⟨𝑥, 𝑥⟩ 𝑉 ≥ 0, (4) ⟨𝑥, 𝑥⟩ 𝑉 = 0 ⇐⇒ 𝑥 = 0 𝑉 .
Definition 6 (Hilbert Space).A complete inner product space is a Hilbert space.</p>
<p>Theorem 1 (Projection Theorem [18]).Consider Hilbert space X and its complete linear subspace Y.</p>
<p>(1) For all elements  ∈ X, there exists a unique element  * ∈ Y such that
𝑥 − 𝑦 * = min 𝑧 ∈ Y ∥𝑥 − 𝑧 ∥
This element  * is called the closest to  in the subspace Y. (2) Given the closest element  * to  in the subspace Y, for all  ∈ X and  ∈ Y, we have ⟨ −  * , ⟩ = 0.</p>
<p>Definition 7 (Linear Independence).Elements {v 1 , v 2 , . . ., v  } of a vector space over the field R are linearly independent if for every set of constants { 1 ,  2 , . . .,   } either     v  ≠ 0 or ∀ :   = 0. Otherwise, they are linearly dependent.</p>
<p>Consider linear independent elements from Hilbert space H : ℎ 1 , ℎ 2 , . . ., ℎ  and element  ∈ H .We can formulate a linear least squares approximation problem (LLSQ) in this Hilbert space as finding such constants
𝛼 1 , 𝛼 2 , . . . , 𝛼 𝑘 that ∥𝑔 − 𝛼 1 ℎ 1 − 𝛼 2 ℎ 2 − • • • − 𝛼 𝑘 ℎ 𝑘 ∥ is minimized. Due to Theorem 1,
there exists a unique element ℎ * ∈ span {ℎ 1 , ℎ 2 , . . ., ℎ  } closest to .It is clear, that this ℎ * is the solution of the stated LLSQ.Due to the second statement of Theorem 1, all of the following equalities hold simultaneously:
𝑔 − ℎ * , ℎ 1 H = 0, 𝑔 − ℎ * , ℎ 2 H = 0, . . . , 𝑔 − ℎ * , ℎ 𝑘 H = 0
It is equivalent to the following matrix equation:
⟨𝑔, ℎ 1 ⟩ H , ⟨𝑔, ℎ 2 ⟩ H , . . . , ⟨𝑔, ℎ 𝑘 ⟩ H ⊤ = G • (𝛼 1 , 𝛼 2 , . . . , 𝛼 𝑘 ) ⊤ (1)
where 1 ≤ ,  ≤  and G = ℎ  , ℎ  H is the Gram matrix.</p>
<p>For arbitrary  ∈ R  we have
𝜶 ⊤ G𝜶 = ∑︁ 1≤𝑖,𝑗 ≤𝑘 𝛼 𝑖 𝛼 𝑗 ℎ 𝑖 , ℎ 𝑗 H = ∑︁ 1≤𝑖 ≤𝑘 𝛼 𝑖 ℎ 𝑖 , ∑︁ 1≤𝑖 ≤𝑘 𝛼 𝑖 ℎ 𝑖 H = ∑︁ 1≤𝑖 ≤𝑘 𝛼 𝑖 ℎ 𝑖 2 ≥ 0
The equality is attained for  ≠ 0 R  if and only if elements ℎ 1 , ℎ 2 , . . ., ℎ  are linearly dependent.So, the Gram matrix is positive definite for the considered linearly independent elements.It implies that all its eigenvalues  1 ,  2 , . . .,   are real positive constants, so det(G) =  1 •  2 • . . .•   &gt; 0. Then its inverse exists, and we obtain the following solution of the matrix equation Eq. ( 1):
(𝛼 1 , 𝛼 2 , . . . , 𝛼 𝑘 ) ⊤ = G −1 • ⟨𝑔, ℎ 1 ⟩ H , ⟨𝑔, ℎ 2 ⟩ H , . . . , ⟨𝑔, ℎ 𝑘 ⟩ H ⊤ (2)</p>
<p>ANALYSIS OF SYMBOLIC REGRESSION IN THE GENERAL CASE</p>
<p>Building upon the concepts introduced in Sec.2.3, this section reframes the optimization of the SR problem through functional analysis.We start by formulating the conventional SR problem more formally.</p>
<p>Problem Formalization</p>
<p>We formalize Symbolic Regression in the following form.Let us consider:</p>
<p>( ,   : X → R,   () = (0, . . ., 1, 0, . . . ) • , where 1 is in the -th position.We will refer to them as elementary operators.</p>
<p>The goal of Symbolic Regression is to obtain an estimated function  : X → R such that:</p>
<p>(1)  is a composition of operators from  ∪  ∪ Π;</p>
<p>(2)  is a minimizer of the following functional (loss function):
L 𝐹 1 𝑁 • 𝑁 ∑︁ 𝑖=1 𝑓 𝑖𝑡𝑛𝑒𝑠𝑠 𝐹 (x 𝑖 ), 𝐹 (x 𝑖 ) ,
where   (.) is the fitness function in GP.Commonly,   (.) can be choosen as MSE, NMSE, R2.In this paper, we choose   (.) as squared error (SE), that is   (, ) ( − ) 2 .(3) Among all such estimated functions that satisfy goals (1) and ( 2) ,  has the smallest length of the description.</p>
<p>GP algorithms that address the formulated problem in practice are usually limited in terms of resources and so can not afford to work infinitely long.Conventionally, the number of traverses over the dataset X is considered as an indication of time spent on solving the problem.For example, the computation of the loss function costs one time unit.In this work, the number of traverses is used as well to analyze the algorithms' performance on generating a sufficiently accurate function  .</p>
<p>Problem Reformulation</p>
<p>Let us consider a vector space of all functions F { : X → R}.Since we are working with the fixed set of training data X, two different functions in F are seen as the same function when an instance of SR is approached.This motivates the following relation for the set F .
Definition 8. Functions 𝑓 , 𝑔 : X → R are called X-identical, 𝑓 ∼ 𝑔 if ∀x ∈ X : 𝑓 (x) = 𝑔(x)
It is easy to see that this relation ∼ is an equivalence relation.Let us denote the set of all equivalence classes of functions X → R as F. By construction, we obtained the quotient space F = F /∼.To not mix up functions with classes of equivalent functions, we will use the notation [•] , which is defined as [𝑓 ] { : X → R |  ∼ }.Now we propose Theorem 2 that collects several statements about the set F and introduces an inner product, which plays a crucial role in this work.Theorem 2. F is a vector space over field R when equipped with:</p>
<p>(1) operator + :
[𝑓 ] + [𝑔] = [∀x ∈ X : x ↦ → 𝑓 (x) + 𝑔(x)] (2) scalar mult.: 𝑟 [𝑓 ] = [∀x ∈ X : x ↦ → 𝑟 𝑓 (x)] (3) neutral element: 0 = [∀x ∈ X : x ↦ → 0] Moreover, this vector space F is a 𝑁 -dimensional Hilbert space with inner product: ⟨[𝑓 ] , [𝑔]⟩ ∑︁ x∈X 𝑓 (x)𝑔(x)
See proof in the appendix.The defined inner product induces the following norm and distance in Hilbert space F:
∥ [𝑓 ] ∥ = √︄ ∑︁ x∈X 𝑓 2 (x)(3)</p>
<p>( [𝑓 ] , [𝑔]) = √︄ ∑︁ x∈X (𝑓 (x) − 𝑔(x)) 2(4)
It is convenient to consider one particular distance between an arbitrary element [ ] ∈ F and the equivalence class which contains the target function  , i.e.,  2 ([ ] , [ ]) = L ( ).Now, we can transform the goal 2 listed before to an optimization problem in Hilbert space F:
𝐹 * = arg min 𝐹 𝑑 𝐹 , <a href="5">𝐹 </a>
In the following section, we propose an approach to solve this optimization problem by generating a solution as a composition of elementary operators.However, we do not directly address the goal 3 defined before and mostly focus only on the accuracy of the obtained functions.Minimizing the length of the produced expression is left for future works.</p>
<p>OPTIMIZATION IN HILBERT SPACE</p>
<p>Depending on the chosen elementary operators, some functions in F might be not representable by a finite composition of elementary operators.This implies that achieving the global minimum value of zero for the optimization problem in Eq. ( 5) may not be reachable.In practical cases, it is relevant to support a rich set representable functions, so it is reasonable to assume that the set of elementary operators is big.Hence, we assume that basic operators are included in elementary operators.Particularly, we consider that addition and multiplication belong to the binary operators , and all real constants R belong to the set ℭ. Given these minimalistic assumptions on elementary operators, we look for a minimizer of Eq. ( 5) in the form:
𝐹 : x ↦ → 𝛼 1 𝑣 1 (x) + 𝛼 2 𝑣 2 (x) + • • • + 𝛼 𝑘 𝑣 𝑘 (x),
where functions  1 ,  2 , . . .,   are some compositions of elementary operators, that is
𝑣 𝑖 ∈ 𝔘 ∪ 𝔅 ∪ Π ∪ ℭ, 𝑖 ∈ Z 𝑘 .
Let us assume that elements ] span a complete subspace of quotient space F. From the projection theorem and Eq. ( 2), we obtain that in this case, there exists a single closest element in this subspace to the element  .However, when functions  1 ,  2 , . . .,   are linearly dependent, the determinant of the corresponding Gram matrix is zero, and so the coefficients   ,  = 1, . . ., , which gives the closest point to  , can not be computed.
[𝑣 1 ], [𝑣 2 ], . . . ,
We propose the following Algorithm 1 as a general heuristic algorithm for solving SR instances.In Theorem 2 we showed that space F is finite-dimensional, however, the following algorithm does not require this property.In this regard, we formulate the algorithm in the general case of Hilbert space, which might be infinitely dimensional.The main idea of the algorithm is to generate compositions of elementary operators and ensure that all of the generated functions are linearly independent.It allows us to apply the projection theorem and get the best possible approximation of  in the subspace spanned by the functions.</p>
<p>Algorithm 1 iteratively generates functions   and uses their linear combination as  .The algorithm uses the Gram matrix in line 8 to obtain the coefficients  for the linear combination.If the Gram matrix is the identity, then elements of  are called Fourier coefficients.In our case, the Gram matrix is almost never identity, but we still use the word Fourier to refer to those coefficients.Since GP uses trees to represent the composition of functions, the tree that represents the obtained  contains coefficients  in some nodes.This is the reason to call the algorithm Fourier Tree Growing.</p>
<p>The generation of the composition of function in line 6 is performed using a heuristic algorithm, which samples elementary operators from a parameterized probability distribution, and creates their compositions.This heuristic and the distribution are discussed later in Section 4.2.</p>
<p>We can consider the generation of the function   as the application of mutation operator to the current approximation   −1 of the target function  .Using an evolutionary metaphor, we can define   as the -th candidate solution.It turns out, that the mutation which violates the predicate in line 7 increases the quality of the produced individual   , which is shown rigorously in Theorem 3. In this case, Algorithm 1 can be seen as an adaptation of a (1 + 1)-EA for optimization in Hilbert space.  ← generate-composition (, , ℭ, Π, , , ) See proof in the appendix.Due to the unordinary definition of the time that the algorithm takes to solve an SR instance, we say explicitly where the algorithm spends the computational budget.We defined that the algorithm spends one time unit when the set of training data X is traversed one time.This traverse happens when the inner product is computed or the loss function is computed.Computation of the inner product is performed twice in the line 2. On every iteration of the loop in line 3 the FTG algorithm computes the value of loss directly in line 4 and it computes the inner product in lines 7, 8 and 9.In line 7 only one computation of the inner product is made.In line 8 algorithm makes 2 − 1 evaluations of the inner product to add the last row and last column to the Gram matrix.In line 9 algorithm makes another evaluation of the inner product ⟨[ ] , [  ]⟩.</p>
<p>Algorithm 1 General scheme of Fourier Tree Growing (FTG) algorithm in Hilbert space
H 1: 𝑣 1 ← (∀x ∈ X : x ↦ → 1) 2: 𝐹 1 ← ∀x ∈ X : x ↦ → ⟨[𝑣 1 ] , [𝑣 1 ]⟩ −1 ⟨[𝐹 ] , [𝑣 1 ]⟩ 3: for 𝑘 ← 2, 3, . . .7: while [𝐹 ] − 𝐹 𝑘 −1 , [𝑣 𝑘 ] = 0 8: G ← ⟨[𝑣 1 ] , [𝑣 1 ]⟩ ⟨[𝑣 1 ] , [𝑣 2 ]⟩ . . . ⟨[𝑣 1 ] , [𝑣 𝑘 ]⟩ ⟨[𝑣 2 ] , [𝑣 1 ]⟩ ⟨[𝑣 2 ] , [𝑣 2 ]⟩ . . . ⟨[𝑣 2 ] , [𝑣 𝑘 ]⟩ . . . . . . . . . . . . ⟨[𝑣 𝑘 ] , [𝑣 1 ]⟩ ⟨[𝑣 𝑘 ] , [𝑣 2 ]⟩ . . . ⟨[𝑣 𝑘 ] , [𝑣 𝑘 ]⟩ 9: 𝜶 ← G −1 ⟨[𝐹 ] , [𝑣 1 ]⟩ ⟨[𝐹 ] , [𝑣 2 ]⟩ . . . ⟨[𝐹 ] , [𝑣 𝑘 ]⟩ 10: 𝐹 𝑘 ← (∀x ∈ X : x ↦ → 𝛼 1 𝑣 1 (x) + 𝛼 2 𝑣 2 (x) + • • • + 𝛼 𝑘 𝑣 𝑘 (x)</p>
<p>Geometrical Interpretation of FTG</p>
<p>In this section, we provide an example of FTG execution and give a visually intuitive scheme of what FTG algorithm does.</p>
<p>Example 1 (First two steps of FTG on a one-dimensional domain).FTG starts in line 1 of Algorithm 1 with a single element, which is a constant 1.The class of equivalent functions that this constant represents is [1].This case is shown in Figure 1   Example 1 demonstrates how FTG handles randomly generated compositions of elementary operators.FTG proceeds in this fashion, spanning greater subspaces, until the class of the target function  is included in the spanned space.The crucial point of the algorithm is the linear independence of the newly generated function with the previously generated ones.Intuitive interpretation is that such a function adds new information about the target function.Following this intuition, the predicate in line 7 of Algorithm 1 checks if the generated composition adds any new information about the function.When it is added, the known information is not lost, meaning the quality of the approximation is not reduced, which is guaranteed by Theorem 3.</p>
<p>Implementation Details</p>
<p>Algorithm 2 Generate composition of elementary operators {, , ℭ, Π} given the minimal  and maximal  numbers of nested operators and a constant  ∈ [0, 1].This is our adaptation of the classical algorithm for tree initialization, called "ramped half-andhalf", see [21,28].</p>
<p>1: procedure generate-composition(, , ℭ, Π, , , )
2: 𝑝 ∼ 𝑈 ({𝑝, 1}) 3: 𝑢 ∼ 𝑈 (Z 𝑢 \ Z 𝑙 −1 ) 4: 𝑑 1 ← 0; 𝑖 ← 1; 𝑗 ← 2 5: 𝑓 𝑖 ∼ 𝐸 𝑝, 𝑑 𝑖 , 𝑙, 𝑢 6:
while  &lt;  do 7: return  1 23: end procedure In this section, we describe the practical aspects of the implementation of the proposed FTG algorithm.We start by introducing the family of probability distributions parameterized by four real values , , , .Every particular distribution in this family defines the distribution over elementary operators.They are used to sample a particular elementary operator in our procedure to generate compositions.Every sampled operator has associated depth , which denotes the number of functions in which the operator is nested.The constants ,  limit the minimal and maximal number of nested operators accordingly.The constant  defines the probability with which a unary or binary function will be sampled when this choice is possible.
𝑑 𝑗 ← 𝑑 𝑖 + 1 8: 𝑑 𝑗+1 ← 𝑑 𝑖 + 1 9: if 𝑓 𝑖 ∈ 𝔘 then 10: 𝑓 𝑗 ∼ 𝐸 𝑝, 𝑑 𝑖 + 1, 𝑙, 𝑢𝐸 (𝑝, 𝑑, 𝑙, 𝑢) =              𝑈 (𝔘 ∪ 𝔅) , if 𝑑 &lt; 𝑙 𝑈 (𝔘 ∪ 𝔅) , with prob. 𝑝 if 𝑙 ≤ 𝑑 &lt; 𝑢 𝑈 (𝑈 ({Π, ℭ})) , with prob. 1 − 𝑝 if 𝑙 ≤ 𝑑 &lt; 𝑢 𝑈 (𝑈 ({Π, ℭ})) , if 𝑑 ≥ 𝑢 (6)
Given this family of distribution , we are ready to formulate the algorithm that generates a composition of functions.FTG utilizes our adaptation of ramped half-and-half initialization as a wellestablished tree-initialization method commonly used in GP to generate new compositions [21,28].We summarize this conventional methodology in Algorithm 2.</p>
<p>Application of Algorithm 2 as function generate-composition is one of the possible ways to generate the composition of functions.We choose this implementation, because of its simplicity and unbiasedness between operators.</p>
<p>When a matrix is ill-conditioned, meaning it has a high condition number, the computation of its inverse is prone to numerical errors.However, in the area of approximation theory, it is known that arbitrary choice of linearly independent elements of Hilbert space will likely lead to a Gram matrix with a very high condition number [14,40].In our work, the functions, that specify elements of Hilbert space are produced randomly and independently from each other, so it is very likely that FTG struggles with such ill-conditioned Gram matrices.In order to address this practical limitation, we compute the inverse using Singular Value Decomposition (SVD) and check if the inverse of the Gram matrix is close to its actual inverse.If this condition is not satisfied, then we do not include the generated [  ] to the set of linearly independent elements and return to the line 7 of Algorithm 1 to generate another   .The check for closeness to the inverse is implemented as follows.When an approximation G −1 of matrix G −1 is obtained, we consider I G • G −1 and validate that every element of this product is close to the corresponding element of the identity matrix I.More precisely, for every ,  we check that |  , −  , | &lt;  1 .In this paper, we use  1 = 10 −4 .</p>
<p>Such a procedure with a generation of   adds additional computational complexity, but helps to avoid significant numerical errors with ill-conditioned matrices.In our work, FTG was dealing with condition numbers up to 3.7 • 10 17 in average across all the considered conventional benchmarks and runs.</p>
<p>In our implementation, we used static parameters  = 0.5,  = 1,  = 9 for Algorithm 2. We assumed that the inner product defined in line 7 of Algorithm 1 is zero when its absolute value is less than  2 = 10 −3 .The bigger this constant is, the more difficult it is for the algorithm to generate the composition that satisfies the defined predicate.On the other hand, the greater the constant, the more new element generated from the span of elements  1 ,  2 , . . .,   −1 , and thus the less ill-conditioned the Gram matrix is.Therefore, the trade-off between  1 and  2 exists and must be identified.In our work, we selected those constants by trying multiple values.We leave a more detailed investigation of this trade-off for future works.The full code of the Algorithm 1, Algorithm 2, conventional GP algorithms that we considered, and all experiments we performed can be found online at [4].</p>
<p>EXPERIMENTS 5.1 Experimental Setup</p>
<p>We performed experiments on classical one-dimensional symbolic regression problems.To evaluate the search performance for GP and our proposed algorithm, we measured the number evaluations of loss functionsons before the computational budget was exceeded.In addition to the mean values of the measurements, we calculated the standard deviation (SD) and the standard error of the mean (SEM).Binary tournament selection was used to select new parent individuals.The configuration for canonical GP was adopted from [36].We performed 100 independent runs with different random seeds.We used the function defined in goal 2 as the fitness function.When the difference of all absolute values becomes less than , the algorithm is classified as converged.We considered the following values of  from 10 0 to 10 −8 to evaluate different tolerance levels.In our experiments we used a function set  = {+, −, * , /, sin, cos, ln} and for each run we allowed a budget of 10 5 fitness evaluations.Besides evaluating the conventional recombination-based GP, which we refer to as canonical GP, we also considered mutation-only GP that is used with a (1 + )-EA and is referred to as (1 + )-GP.Ramped half-and-half initialization has been used for all tested algorithms.The configuration of the respective GP algorithms is shown in Table 3 (in the Appendix).We divide our experiments into two parts.We evaluate the search performance for GP and FTG on conventional benchmarks that have been proposed for SR.</p>
<p>Conventional Benchmarks</p>
<p>We selected nine well-known symbolic regression benchmarks from the work of McDermott et al. [30].The objective functions and dataset configuration of the respective problems are shown in Table 2 (in the Appendix).</p>
<p>Large-Scale Polynomial Benchmark</p>
<p>We propose a type of benchmark for SR to address the following two points that are missing in our conventional SR benchmark.</p>
<p>(1) Finite training data set leaves a chance that the obtained expression with a small value of the loss function approximate value of  (x) well only for x ∈ X, but give high   value for points in X \ X;</p>
<p>(2) GP applied to conventional benchmarking problems generates a function more complicated than the target function very quickly.The span of such candidate solutions already includes the target function.This hinders observing the capabilities of algorithms to iteratively span subspaces that are getting closer to the target function  .</p>
<p>The proposed Large-Scale Polynomial (LSP) benchmark addresses Point 1 by considering the whole domain as the training data.This makes the computation of the loss function technically more difficult.Point 2 is addressed by considering a target function as a polynomial of a high degree, for example, 100.This entails problems for the computations of the loss values for candidate solutions, because of limitations in floating point precision on the computer.Now we show how both mentioned difficulties, namely computation of the loss function over an infinite data set and possible numerical errors, are tackled in the proposed LSP benchmarks.</p>
<p>Given constants , , ,  0 ,  1 , . . .,   , we define LSP benchmark, as SR instance, where  ={+, * },  = ∅, Π = {1}, X = [, ], X = X,  =  =0     .We focus on a particular case of our general setup when sets ,  are the smallest that satisfy our assumptions.I.e.we consider  ={+, * },  = ∅, Π = {1} which implies that the dimension of the problem is  = 1.We denote such a onedimensional domain as X = [, ].Regardless of the set of constants ℭ, all the compositions of elementary operators belong to the class of polynomials.A polynomial of degree  can always be written down in the form  =0     , where   ≠ 0. We will denote this as normal form of a polynomial.</p>
<p>Up until now, we considered only finite sets X.Now, we propose to consider infinite X which equals the whole domain where the target function  is defined, i.e.X = X.In this case, we define F as space of square-integrable functions over the segment (, ), which we denote as  2 (, ).Due to the fact that all the polynomials are square-integrable functions, such F includes all compositions of the considered elementary operators. 2 (, ) is a Hilbert space with inner product ⟨ , ⟩ = ∫    ()() ∀ ,  ∈  2 (, ).It induces the following norm: ∥ ∥ = √︁ ⟨ ,  ⟩.For this benchmark, we choose the target function  as a polynomial.So the target function  ∈  2 (, ), hence the loss of SR, can be computed as
L ( 𝐹 ) = 𝐹 − 𝐹 2 = 𝐹 − 𝐹, 𝐹 − 𝐹 .
It is clear, that being able to compute the value of the inner product automatically, is sufficient for automatic computation of loss value.To implement this computation, we transform a tree to a polynomial in the form using a recursive algorithm.Then we automatically do the subtraction of polynomials in normal form, if needed, and do the multiplication of polynomials in normal form.After this, the resulting polynomial under the integral is obtained in normal form, and the computation of the inner product boils down to the computation of the following integral:
𝑏 ∫ 𝑎 𝑘 ∑︁ 𝑖=0 𝑐 𝑖 𝑥 𝑖 d𝑥 = 𝑘 ∑︁ 𝑖=0 𝑐 𝑖 𝑖 + 1 (𝑏 𝑖+1 − 𝑎 𝑖+1 )(7)</p>
<p>Results</p>
<p>Figure 2 shows the results of our experiments with conventional benchmarks.On each heatmap, the X-axis shows the tolerance value and the Y-axis shows the benchmark problem.In subfigure (a) every cell shows the difference between the success rate (in percent) of FTG and the maximal success rate that GP algorithms achieved.In subfigure (b) every cell shows a ratio between median function evaluations of FTG and minimal value across medians of function evaluations that GP algorithms made to reach the given tolerance of the solution.The more red the cell is, the better the performance of FTG relative to the considered GP algorithms.It is clearly visible that FTG performs considerably better and more robust for the tested problems when compared to conventional GP.The complete results are available in the Appendix in Table 4 and Table 5.  Figure 3 shows the results of the LSP evaluation.A column displays results for a different configuration of the benchmark, characterized by the polynomial degree .The target function is  =0   .Rows represent the tracked quantity for each generation of the algorithm.Each chart depicts the averaged tracked quantity on the Y-axis and the generation number on the X-axis.The lines represent the mean value and the shaded areas represent the standard deviation.Finally, the last two rows with numbers show the means and standard deviations of the number of individuals that simultaneously achieved better fitness than the best-so-far solution and spanned a larger subspace.For every configuration of the experiment, we used the same random seeds and made 100 independent runs to obtain statistically robust results.</p>
<p>In the further discussion, we say that a polynomial has the span of size  when a polynomial in a normal form is  =0     such that ∀ ∈ Z  :   ≠ 0. The size of the span is measured automatically by transforming a tree to a polynomial in normal form and counting the number of terms with different degrees.</p>
<p>DISCUSSION</p>
<p>Figure 2 exhibits that FTG significantly outperforms GP in terms of the number of function evaluations needed to reach the desired precision of the solution.We highlight, that FTG reaches an almost absolute success rate even for very small tolerance levels.If the tolerance of the solution is infinitely large, then all the compositions of functions are accepted as sufficient approximation  .However, when the tolerance is reduced, the set of functions with sufficiently small loss values does not increase, and, for practically relevant elementary operators, it reduces.Hence, for the minimal tolerance levels, the set of candidate solutions of sufficient quality is relatively small, which makes it harder for conventional GP to find any element from this set.This results in a small success rate of GP observed in Figure 2. At the same time, FTG manages to overcome this challenge because it has an almost perfect success rate and spends a much smaller budget to find solutions for such tolerance.We rigorously prove in Theorem 3 that, with the absence of numerical errors, FTG improves the quality of the approximation every time it manages to find a linearly independent function   .GP search takes advantage of randomness and naturally evolves functions, which could be considered as a random walk in Hilbert space F within the framework proposed in this work.Element [ ] that is encountered during the random walk may be decomposed to the sum of elements if  has addition as the upper functions in the composition.Even if the top function in the composition is already not addition, element [ ] belongs to the space span {[ ]}.We can observe that when an element [ ] is encountered during the random walk it may belong to a space that was already spanned earlier by element [].In this case, FTG filters out the function  , but GP may include it in the population.Such suboptimal choice reduces the selective pressure towards more promising candidate solutions, which reduces the speed of convergence.</p>
<p>The first row of results on the LSP benchmark in Figure 3 exhibit that FTG quickly reduces loss value at the beginning, but then stagnates.FTG appears to struggle with the ill-conditionality of the Gram matrix, which is a hindrance for the accurate computation of its inverse, and in this way, numerical errors are caused.According to our implementation of FTG as explained in Section 4.2, we loop until the inverse of the Gram matrix can be computed accurately, which causes stagnation of FTG when the Gram matrix becomes bigger.</p>
<p>In the first and second rows of Figure 3 we can see that GP spans greater subspaces and, at the same time, reduces loss value at a slower pace than FTG.For the polynomial of degree 100, GP manages to eventually overcome FTG.We propose the following explanation of the observed results:.In general, mutation and recombination with certain non-zero probability change constants are hidden inside the candidate solution function  , while preserving the structure of this function.Consider an individual as a function of the domain variable x and the vector of all constants Θ, i.e.,  (x, Θ).For changing Θ, elements [ (•, Θ)] span some subspace S, which, in the general case, can be neither convex nor complete in F. However, in S, there might be many points such that [ ] is closer to them than to the projection of  to the linear subspace span { (•, Θ 1 )} for some fixed Θ 1 .The mechanism that GP uses to generate new individuals allows it to obtain such points.Surprisingly, we can observe that (1 + )-GP, which uses only mutation to variate candidate solutions, suffers less from bloat, but converges faster than Canonical-GP, while having approximately the same loss and span of F at the end of optimization.It means that the mutation-only variation operator is beneficial for polynomials, but the study of how well it generalizes on other problems is left for future work.</p>
<p>LSP with a polynomial of degree 10 is used to see the performance of algorithms in the absence of overfitting.The proposed FTG algorithm performs better than conventional GP on this instance of LSP but does not reach a loss value of zero.At the same time, in the last row of Figure 3 we see that FTG produces trees with a significantly greater number of nodes on both considered cases of polynomials.This is due to the method that we used for the generation of compositions of functions in FTG.</p>
<p>In both considered cases of polynomials, Canonical-GP spans approximately the same number of dimensions that the target function spans.Hence, GP found the approximate shape of the target polynomial but struggled to find the right constants.</p>
<p>Our analyses of GP on this benchmark demonstrate that GP excels in generating solutions that effectively span subspaces of the considered Hilbert space.</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>In this work, we considered SR from the perspective of functional analysis and proposed a novel algorithm that is able to navigate and optimize in Hilbert space.Our consideration of SR in Hilbert space allows us to achieve insight into the principles of conventional GP.To the best of our knowledge, our work represents a pioneering application of functional analysis in the SR domain, for which we proposed first theoretical and experimental results.Our proposed FTG algorithm demonstrates significant performance gains over considered GP algorithms on conventional benchmarks.FTG manages to find solutions of minimal tolerance, which shows that it can efficiently navigate in a large search space.However, it is susceptible to numerical errors when the Gram matrix is inverted.We propose to address this issue by employing the Gram-Schmidt orthogonalization algorithm to transform linearly independent vectors into an orthonormal basis.The better performance of FTG on conventional benchmarks can be explained by the fact that FTG filters out a priory suboptimal functions, while GP does not.However, the working principles of GP allow it to be more beneficial for polynomials of high degree, which we show in the proposed LSP benchmarks.In future work, we plan to hybridize GP and FTG to obtain an algorithm that can simultaneously substantially span the subspaces of the Hilbert space and effectively optimize the constants.
(𝑝 𝑖 ) 𝑛 𝑖=1 𝑝 𝑖 : X → R ℭ Space of constants ⊆ R X Domain of variables ⊂ R 𝑛 F Function Space {𝑓 | 𝑓 : X → R} F Quotient space F /∼ span {𝑥 } Linear span of 𝑥 n.a.∼ 𝑈 (−1, 1) koza2 𝑥 5 − 2𝑥 3 + 𝑥 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) koza3 𝑥 6 − 2𝑥 4 + 𝑥 2 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) nguyen3 𝑥 5 + 𝑥 4 + 𝑥 3 + 𝑥 2 + 𝑥 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) nguyen4 𝑥 6 + 𝑥 5 + 𝑥 4 + 𝑥 3 + 𝑥 2 + 𝑥 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) nguyen5 sin(𝑥 2 ) cos(𝑥 ) − 1 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) nguyen6 sin(𝑥 ) + sin(𝑥 + 𝑥 2 ) 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (−1, 1) nguyen7 ln(𝑥 + 1) + ln(𝑥 2 + 1) 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d. ∼ 𝑈 (0, 2) nguyen8 √ 𝑥 𝑥 𝑥1, 𝑥2, . . . , 𝑥20 i.i.d.
∼  (0, 4)
(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞ 0 FTG1053(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞0(1 + 1)-GP ∞ 0 0 ∞ ∞ ∞ 0 (1 + 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞+ 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞+ 𝜆)-GP ∞ 0 0 ∞ ∞ ∞ 0 Canonical-GP ∞ 0 0 ∞ ∞ ∞ 0 FTG844</p>
<p>do</p>
<p>4 :
4
if L (   −1 ) = 0 then return   −1 end if</p>
<p>) 11: end for 12: return   Theorem 3. Consider sequence of functions (   )  =1 constructed by Algorithm 1.For every  &gt;  : L (   ) &lt; L (   ).Moreover, if H is finite dimensional with  dimensions, then algorithm returns function  such that L (  ) = 0.</p>
<p>(a), where the span {[1]} is shown with an orange line in the considered Hilbert space  .The solution of LLSQ with only one function  1 (x) = 1 gives the constant  such that ∥ −  ∥ is the smallest possible.Substitution of this  1 to Eq. (2) gives the following solution:  =  −1 y∈X  (y).The point (a) The first step of FTG for  = 1.The constant 1 is added as the function  1 and the best approximation of  with this constant is found as function  1 .(b) The second possible step of FTG for  = 2.We assume that a function  2 = sin( 2 + 1) √︁ | | is generated and the training data was such that [ 2 ] does not lie in the space spanned by [ 1 ].</p>
<p>Figure 1 :
1
Figure 1: Visualization of the first (a) and second (b) steps that FTG can make for a one-dimensional domain.</p>
<p>Difference of Success Rate (b) Ratio of Median Function Evaluations</p>
<p>Figure 2 :
2
Figure 2: Comparison of success rate (a) and median of function evaluations to reach the given tolerance (b) of FTG with three standard GP algorithms: (1 + 1)-GP, (1 + )-GP, Canonical-GP across a range of benchmark problems and tolerance levels.</p>
<p>Figure 3 :
3
Figure 3: Performance Comparison of Canonical-GP (grey) and (1+)-GP (light red) and FTG (red) on the proposed Large-Scale Polynomial Benchmarks.</p>
<p>Ordered set of points X {x 1 , x 2 , . . ., x  } ⊂ X , which we consider as training data.It is used as the set of points where the values of the function  are known; (4) Unary operators  ⊆ { |  : R → R} , binary operators  ⊆   : R 2 → R , constants ℭ ⊆ R and orthogonal
𝑛1) Domain X[𝑎 𝑖 , 𝑏 𝑖 ] ⊂ R 𝑛 ;𝑖=1(2) An unknown arbitrary function 𝐹 : X → R is to be approx-imated. We refer to this function 𝐹 as target function;(3) projection operators Π(𝑝 𝑖 ) 𝑛 𝑖=1</p>
<p>[  ] are linearly independent.The linear combination       (x) of elements from F still belongs to the space F , so we can consider element       (x) of space F. Based on the properties of quotient space,       (x) =</p>
<div class="codehilite"><pre><span></span><code><span class="p">[</span><span class="w">  </span><span class="p">(</span><span class="n">x</span><span class="p">)]</span><span class="o">.</span><span class="n">Since</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">constants</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">allowed</span><span class="p">,</span><span class="w"> </span><span class="n">elements</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">],</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="o">.</span><span class="w"> </span><span class="o">.</span><span class="p">,</span><span class="w"> </span><span class="p">[</span>
</code></pre></div>

<p>11 :
11
  ←   •     ∼  ,   + 1, ,   +1 ∼  ,   + 1, ,    ←   (  ,  +1 )
12:𝑗 ← 𝑗 + 113:end if14:if 𝑓 𝑖 ∈ 𝔅 then15:16:17:18:𝑗 ← 𝑗 + 219:end if20:𝑖 ← 𝑖 + 121:end while22:</p>
<p>Table 1 :
1
046501-046501.[46] Kaifeng Yang and Michael Affenzeller.2023.Surrogate-assisted Multi-objective Optimization via Genetic Programming Based Symbolic Regression.In Evolutionary Multi-Criterion Optimization, Michael Emmerich, André Deutz, Hao Wang, Anna V. Kononova, Boris Naujoks, Ke Li, Kaisa Miettinen, and Iryna Yevseyeva (Eds.).Springer Nature Switzerland, Cham, 176-190.[ ] in this subspace is unique.Hence,  −   −1 , [  ] = 0, which means that such [  ] would not violate the condition in line 7 of Algorithm 1, and so would not be added.Contradiction with the assumption that equality is attained.Then  [ ] ,   &lt;  [ ] ,   −1 , which implies that L   &lt; L   −1 .It proves the first statement of the theorem.If H is  -dimensional, then span {[ 1 ] , [ 2 ] , . . ., [  ]} = H . Hence, for function   :  [ ] ,   = 0 =⇒ L   = 0. Notation
The second statement is also proven.□A.2 TablesSymbolDefinitionDomain𝑓 , 𝑔A functionX → R𝑓 𝑖𝑡𝑛𝑒𝑠𝑠A fitness function for SR problemsRxFeatures in training datasetR 𝑛XTraining datasetR 𝑛𝑁FA SR functionX → R𝑈Uniform distribution over the given setn.a.𝜋Bijective mappingn.a.𝔘Unary operator space{𝑢 | 𝑢 : R → R}𝔅Binary operator space𝑏 𝑏 : R 2 → RΠOrthogonal projector operator space</p>
<p>Table 2 :
2
Conventional symbolic regression benchmark
Problem Objective FunctionVars Training Setkoza1𝑥 4 + 𝑥 3 + 𝑥 2 + 𝑥𝑥 𝑥1, 𝑥2, . . . , 𝑥20
i.i.d.</p>
<p>Table 3 :
3
Configuration of the GP algorithms
Parameters(1+1)-GP(1+𝜆)-GPCanonical GP𝑃population size11500𝜆offspring size1500500𝑀𝑡mutation typeuniform subtree uniform subtree probabilistic subtree𝑀𝑝mutation rate110.1𝐶𝑡crossover typen.a.n.a.subtree crossover𝐶𝑝crossover raten.a.n.a.0.9𝑇tournament sizen.a.n.a.</p>
<p>Table 4 :
4
Results of the algorithm comparison for the problems evaluated by the number of fitness evaluations (FE) to termination.
ToleranceProblemAlgorithmMean FESDSEM1QMedian3QSuccessrate (%)(1 + 1)-GP20535.66222551.8562570.0234941.00013087.00024691.00077koza1(1 + 𝜆)-GP21470.88018044.3481980.6248251.00015501.00029001.00083Canonical-GP25704.89815474.6151563.17216062.50021167.00031749.50098FTG210.790499.68249.968117.750143.500198.250100(1 + 1)-GP4988.4597525.069760.147802.2501975.0004934.50098koza2(1 + 𝜆)-GP7883.6538582.969867.0112501.0004751.0008376.00098Canonical-GP40612.58924258.7882488.89916934.00038846.00060260.00095FTG135.35070.2877.02996.250121.000171.000100(1 + 1)-GP12691.83318547.0452023.6492149.2506052.50012831.25084koza3(1 + 𝜆)-GP18552.28218159.1242056.1166001.00012251.00021251.00078Canonical-GP17425.92719790.6002185.5064110.5009464.00021416.00083FTG90.09046.8334.68359.00094.000118.000100&lt; 10 −2nguyen3(1 + 1)-GP (1 + 𝜆)-GP Canonical-GP24293.640 36565.706 32333.69225123.029 24355.189 14479.2652709.086 2641.691 1517.8397748.250 16001.000 21914.00014440.000 32501.000 28388.00029669.250 54001.000 40340.00086 85 922FTG187.40085.9658.596122.750168.000231.250100(x)(x) − 𝐹𝐹x∈XFTG153.590221.71022.17197.000122.500168.000100(1 + 1)-GP12280.05418832.3201952.8212204.0004458.00013355.00093nguyen7(1 + 𝜆)-GP17016.30615267.7551542.2766626.00013001.00022001.00098Canonical-GP27446.78012216.5191221.65218428.00024653.00033866.000100FTG48.76030.5943.05930.00044.00061.000100(1 + 1)-GP17749.12521108.8552250.2114811.00010081.50021978.75088nguyen8(1 + 𝜆)-GP19271.83317180.0991753.4377876.00012751.00027626.00096Canonical-GP52970.12825733.5853753.62930131.00056774.00071216.00049FTG68.52035.9653.59744.00060.00094.250100(1 + 1)-GP10031.77219916.2972076.417991.2502412.5006353.00092koza1(1 + 𝜆)-GP9141.62511727.7501196.9583501.0005501.0009126.00096Canonical-GP17713.6978298.540834.03512452.00016436.00020171.000100FTG105.92049.1174.91275.75099.000127.750100(1 + 1)-GP622.031737.29174.478216.250383.000853.50098koza2(1 + 𝜆)-GP2216.0002657.306265.7311001.0001501.0002501.000100Canonical-GP2601.5601674.910167.4911496.0002492.0003488.000100FTG78.54044.5654.45645.00077.00098.000100(1 + 1)-GP77.130102.37510.23721.00037.00067.000100koza3(1 + 𝜆)-GP621.000224.94422.494501.000501.000501.000100Canonical-GP515.24585.7888.666500.000500.000500.000100FTG3.0000.0000.0003.0003.0003.000100&lt; 10 −1nguyen3(1 + 1)-GP (1 + 𝜆)-GP Canonical-GP5233.250 13332.633 20242.1439750.193 13609.710 6818.682995.125 1374.788 688.7911696.250 5501.000 15066.5003073.500 9251.000 19424.0006071.500 14876.000 23906.00096 98 1002FTG128.33067.0336.70396.750121.000166.500100(x) (x) − 𝐹 𝐹x∈Xnguyen4 nguyen5(1 + 1)-GP (1 + 𝜆)-GP Canonical-GP FTG (1 + 1)-GP (1 + 𝜆)-GP Canonical-GP FTG9457.990 13444.299 25303.613 143.100 1278.020 2771.000 2666.300 37.08012994.070 12402.169 9637.023 74.536 1815.722 2831.448 3414.388 24.4111326.202 1259.249 999.313 7.454 182.487 283.145 341.439 2.4412262.000 6001.000 19424.000 98.500 191.000 1501.000 1371.500 19.0004374.500 9501.000 24902.000 122.500 468.000 2001.000 1994.000 30.50010826.750 17501.000 29882.000 168.500 1682.500 3501.000 2990.000 58.00096 97 96 100 99 100 100 100(1 + 1)-GP3683.3966924.684706.748657.2501480.0003616.25096nguyen6(1 + 𝜆)-GP6517.8548730.500925.4312501.0003501.0007001.00089Canonical-GP10908.2008970.776897.0786476.0008966.00013074.500100FTG73.63037.6963.77044.00060.50097.000100(1 + 1)-GP563.640579.75257.975140.250366.500774.500100nguyen7(1 + 𝜆)-GP1391.000829.39782.9401001.0001001.0001501.000100Canonical-GP2093.6001372.891137.289998.0001994.0002990.000100FTG35.29024.6982.47017.25030.50046.250100(1 + 1)-GP4221.5667441.241747.873820.0001789.0003849.00099nguyen8(1 + 𝜆)-GP5556.0005739.728573.9732001.0004001.0006501.000100Canonical-GP21288.73324501.2362582.6573986.0008966.00034986.50094FTG40.08025.2712.52720.00032.00059.000100(1 + 1)-GP1535.3967268.502741.838162.750435.500818.00096koza1(1 + 𝜆)-GP2192.9191419.039142.6191501.0002001.0002501.00099Canonical-GP4544.3642427.620243.9852741.0004484.0005978.000100FTG57.78036.2993.63031.00045.00077.000100(1 + 1)-GP16.42026.7152.6725.00011.00018.500100koza2(1 + 𝜆)-GP456.000143.09114.309501.000501.000501.000100Canonical-GP500.0000.0000.000500.000500.000500.000100FTG3.2501.9150.1923.0003.0003.000100(1 + 1)-GP13.52014.8761.4884.0008.50016.250100koza3(1 + 𝜆)-GP446.000156.44515.644501.000501.000501.000100Canonical-GP500.0000.0000.000500.000500.000500.000100FTG3.0000.0000.0003.0003.0003.000100(1 + 1)-GP862.6101099.659109.966262.000539.0001077.750100&lt; 1nguyen3(1 + 𝜆)-GP3051.0001854.050185.4051501.0002501.0004001.0001002Canonical-GP8442.5924089.553413.1075604.5008468.00010958.000100(x)FTG (1 + 1)-GP72.450 1974.77042.992 4756.8624.299 475.68643.750 409.75059.500 677.50096.250 1639.250100 100(x) − 𝐹nguyen4(1 + 𝜆)-GP Canonical-GP FTG3456.000 9780.438 81.9103029.930 5450.162 50.198302.993 556.255 5.0202001.000 5480.000 45.0003001.000 9713.000 70.0004001.000 13448.000 100.500100 100 100𝐹(1 + 1)-GP35.91035.9893.59910.75023.50049.500100x∈Xnguyen5(1 + 𝜆)-GP Canonical-GP521.000 500.000140.000 0.00014.000 0.000501.000 500.000501.000 500.000501.000 500.000100 100FTG3.0000.0000.0003.0003.0003.000100(1 + 1)-GP766.5053743.581376.24494.000180.000373.00099nguyen6(1 + 𝜆)-GP2073.1655258.273533.8971001.0001501.0001501.00097Canonical-GP2013.9201534.810153.481873.5001496.0002990.000100FTG35.36032.3443.23411.00021.00046.000100(1 + 1)-GP148.160183.45518.34543.75085.500184.500100nguyen7(1 + 𝜆)-GP756.000278.34327.834501.000501.0001001.000100Canonical-GP614.540262.14926.215500.000500.000500.000100FTG21.79014.3841.43810.00019.00031.000100(1 + 1)-GP447.690597.08459.70899.500234.000514.250100nguyen8(1 + 𝜆)-GP1351.000912.41491.2411001.0001001.0001501.000100Canonical-GP1076.632874.03489.674500.000500.0001496.000100FTG19.59013.0801.30810.00012.00030.000</p>
<p>Table 5 :
5
Results of the algorithm comparison for the problems evaluated by the number of fitness evaluations (FE) to termination.
ToleranceProblemAlgorithmMean FESDSEM1QMedian3QSuccessrate (%)(1 + 1)-GP∞00∞∞∞0koza1(1 + 𝜆)-GP14438.50025864.3559144.4302251.0003751.0007876.0008Canonical-GP22230.9099264.6951055.81015938.00019922.00025898.00077FTG1288.3302343.521237.948533.000597.000920.00097koza2&lt; 10 −82(x)(x) − 𝐹𝐹x∈X</p>
<p>0
FTG1515.3503482.339348.234542.500633.000999.250100(1 + 1)-GP∞00∞∞∞0nguyen3(1 + 𝜆)-GP7001.0000.0000.0007001.0007001.0007001.0001Canonical-GP26997.35810211.6281402.67520420.00024404.00032372.00054FTG1962.1024402.288444.698539.500660.5001235.75098nguyen4</p>
<p>0
FTG1930.2406211.331621.133552.250670.000911.500100nguyen8</p>
<p>0
FTG1408.5605689.417568.942478.000559.500718.500100(1 + 1)-GP∞00∞∞∞0koza1(1 + 𝜆)-GP14556.55622807.7047602.5682501.0004501.00015001.0009Canonical-GP22217.9749270.6881056.49315938.00019922.00025898.00077FTG971.5802028.005202.800494.750550.500693.500100koza2&lt; 10 −72(x)(x) − 𝐹𝐹x∈X</p>
<p>0
FTG950.4102804.880280.488403.250504.000609.750100(1 + 1)-GP90563.0000.0000.00090563.00090563.00090563.0001(1nguyen8</p>
<p>0
FTG959.3205207.160520.716323.000432.500509.750100(1 + 1)-GP47988.00036922.00026107.79729527.00047988.00066449.0002koza1(1 + 𝜆)-GP19151.00026725.5038451.3462626.0005001.00016126.00010Canonical-GP22166.2349258.3351055.08515938.00019922.00025400.00077FTG839.2201998.643199.864403.750499.000559.250100(1 + 1)-GP84218.00013203.9387623.29777494.50088715.00093190.0003koza2(1 + 𝜆)-GP90001.0002500.0001767.76788751.00090001.00091251.0002Canonical-GP90140.0000.0000.00090140.00090140.00090140.0001FTG802.9691206.168121.841419.750506.500593.75098(1 + 1)-GP70642.50022277.50015752.57159503.75070642.50081781.2502koza3(1 + 𝜆)-GP Canonical-GP78501.000 ∞0.000 00.000 078501.000 ∞78501.000 ∞78501.000 ∞1 0FTG885.6802248.385224.838440.500507.500559.250100&lt; 10 −6nguyen3(1 + 1)-GP (1 + 𝜆)-GP Canonical-GP∞ 7001.000 26865.8110 0.000 10088.5630 0.000 1385.771∞ 7001.000 20420.000∞ 7001.000 24404.000∞ 7001.000 32372.0000 1 542FTG1252.3542951.116296.598452.000510.000696.00099(x) (x) − 𝐹 𝐹x∈Xnguyen4 nguyen5(1 + 1)-GP (1 + 𝜆)-GP Canonical-GP FTG (1 + 1)-GP (1 + 𝜆)-GP Canonical-GP FTG∞ 61501.000 39127.478 1744.000 98966.000 98001.000 15440.000 911.4300 0.000 14573.055 4936.152 0.000 0.000 0.000 3709.3530 0.000 3038.692 498.627 0.000 0.000 0.000 370.935∞ 61501.000 29633.000 466.250 98966.000 98001.000 15440.000 338.750∞ 61501.000 36356.000 522.500 98966.000 98001.000 15440.000 426.000∞ 61501.000 46067.000 779.000 98966.000 98001.000 15440.000 525.0000 1 23 98 1 1 1 100(1 + 1)-GP20049.71432794.62612395.2041189.5003364.00019022.0007nguyen6(1 + 𝜆)-GP2701.000678.233303.3152501.0002501.0002501.0005Canonical-GP21011.03315667.2761642.37710958.00016934.00024902.00091FTG828.6601140.343114.034410.000502.500594.250100(1 + 1)-GP94875.0004763.2042750.03792492.50096616.00098128.0003nguyen7(1 + 𝜆)-GP Canonical-GP59001.000 ∞1000.000 0707.107 058501.000 ∞59001.000 ∞59501.000 ∞2 0FTG420.080769.98076.998256.250336.500431.750100(1 + 1)-GP49606.000917.000648.41749147.50049606.00050064.5002(1nguyen8
In this paper, we use notation Z 𝑚 to represent an integer set {1, • • • , 𝑚}.
Acknowledgments.We are grateful to Dr. André Deutz, Leiden University, for the insightful discussions during the writing process and for his valuable detailed feedback on the paper.We also deeply appreciate the stimulating conversation with Prof. Dr. Günter Rudolph, TU Dortmund University, whose general view on our work greatly benefited our research.This work is supported by the Austrian Science Fund (FWF -Der Wissenschaftsfonds) under the project (I 5315, 'ML Methods for Feature Identification Global Optimization).The project was financially supported by ANR project HQI ANR-22-PNCQ-0002.A APPENDIX A.1 ProofProof of Theorem 2. To prove the theorem, it is sufficient to show the existence of a bijective mapping  : F → R  such that ∀ ∈ F :  ( [ ]) ( (x  ))  =1 .By definition(8)we see that if  1 ,  2 ∈ [ ] for any  ∈ F , then ( 1 (x  ))  =1 = ( 2 (x  ))  =1 .It means that for every  ∈ F, there exists a single a ∈ R  such that  () = a.Now, to demonstrate that this  is a bijection, it is sufficient to show that for all a ∈ R  there exists a unique  ∈ F such that  () = a.For any such a (  )  =1 , let us considerIt means that such  is unique for every a. □ Proof of Theorem 3. Mathematical induction is used here to prove Theorem 3. We start by proving by induction that Algorithm 1 maintains the following invariant.For 1 ≤  ≤  , element   is the closest to the element [ ] among all the elements in the space span {[ 1 ] , . . ., [  ]}.We prove it by induction over .Assume that the statement is correct for all values of  up to value  such that 1 &lt;  ≤  &lt;  .Let us prove that it is correct for
Gaining deeper insights in symbolic regression. Michael Affenzeller, Stephan M Winkler, Gabriel Kronberger, Michael Kommenda, Bogdan Burlacu, Stefan Wagner, Genetic Programming Theory and Practice XI. 2014. 2014</p>
<p>The Evolution of Evolvability in Genetic Programming. Lee Altenberg, ; Kenneth, E KinnearJr, 10.7551/mitpress/1108.003.0009Advances in Genetic Programming. MIT Press1994</p>
<p>Comparing Subtree Crossover with Macromutation. Peter J Angeline, 10.1007/BFB0014804Evolutionary Programming VI, 6th International Conference. Lecture Notes in Computer Science. Peter J Angeline, Robert G Reynolds, John R Mcdonnell, Russell C Eberhart, EP97, Indianapolis, Indiana, USASpringer1997. April 13-16, 19971213</p>
<p>Source code of Fourier Tree Growing (FTG) and related experiments with Genetic Programming (GP). Kirill Antonov, Roman Kalkreuth, Kaifeng Yang, Thomas Bäck, Niki Van Stein, Anna V Kononova, 2024</p>
<p>Monte-carlo expression discovery. Tristan Cazenave, International Journal on Artificial Intelligence Tools. 2212500352013. 2013</p>
<p>A global MINLP approach to symbolic regression. Alison Cozad, Nikolaos V Sahinidis, Mathematical Programming. 1702018. 2018</p>
<p>A representation for the Adaptive Generation of Simple Sequential Programs. Lynn Nichael, Cramer, Proceedings of an International Conference on Genetic Algorithms and the Applications. John J Grefenstette, an International Conference on Genetic Algorithms and the ApplicationsPittsburgh, PA, USA1985Carnegie-Mellon University</p>
<p>A greedy search tree heuristic for symbolic regression. Fabrício Olivetti De França, Information Sciences. 4422018. 2018</p>
<p>Integrating quantitative and qualitative discovery: the ABACUS system. C Brian, Ryszard S Falkenhainer, Michalski, Machine Learning. 11986. 1986</p>
<p>BEAGLE A Darwinian Approach to Pattern Recognition. Richard Forsyth, 10.1108/eb005587Kybernetes. 101981. 1981</p>
<p>Towards an Understanding of Locality in Genetic Programming. Edgar Galván-López, James Mcdermott, O' Michael, Anthony Neill, Brabazon, 10.1145/1830483.1830646Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation. the 12th Annual Conference on Genetic and Evolutionary ComputationPortland, Oregon, USA; New York, NY, USAAssociation for Computing Machinery2010</p>
<p>Information processing, data inferences, and scientific generalization. Donald Gerwin, Behavioral Science. 191974. 1974</p>
<p>Application of the Genetic Algorithm to Automatic Program Generation. Joseph Hicklin, 1986University of IdahoMaster's thesis</p>
<p>On random correlation matrices. B Richard, Holmes, SIAM journal on matrix analysis and applications. 121991. 1991</p>
<p>Neutrality, Robustness, and Evolvability in Genetic Programming. Ting Hu, Wolfgang Banzhaf, 10.1007/978-3-319-97088-2_7Genetic Programming Theory and Practice XIV. Rick L Riolo, Bill Worzel, Brian Goldman, Bill Tozier, Ann Arbor, USASpringer2016. 2016. May 19-21, 2016University of Michigan ; Genetic and Evolutionary Computation</p>
<p>The identification and exploitation of dormancy in genetic programming. David Jackson, Genetic Programming and Evolvable Machines. 112010. 2010</p>
<p>Symbolic regression by exhaustive search: Reducing the search space using syntactical constraints and efficient semantic structure deduplication. Genetic programming theory and practice XVII. Lukas Kammerer, Gabriel Kronberger, Bogdan Burlacu, Stephan M Winkler, Michael Kommenda, Michael Affenzeller, 2020. 2020</p>
<p>Leonid Vital, ' Kantorovich, Gleb Pavlovich, Akilov , Functional analysis. Elsevier2016</p>
<p>Symbolic regression is not enough: it takes a village to raise a model. Ekaterina Mark E Kotanchek, Guido Vladislavleva, Smits, Genetic Programming Theory and Practice X. 2013. 2013</p>
<p>Genetic Programming: A paradigm for genetically breeding populations of computer programs to solve problems. J Koza, STAN-CS-90-13141990Dept. of Computer Science, Stanford UniversityTechnical Report</p>
<p>On the programming of computers by means of natural selection. Genetic programming. Jrgp Koza, 1992. 1992</p>
<p>Genetic Programming: On the Programming of Computers by Means of Natural Selection. R John, Koza, 1992MIT PressCambridge, MA, USA</p>
<p>Genetic Programming II: Automatic Discovery of Reusable Programs. John R Koza, 1994MIT PressCambridge Massachusetts</p>
<p>Predicting friction system performance with symbolic regression and genetic programming with factor variables. Gabriel Kronberger, Michael Kommenda, Andreas Promberger, Falk Nickel, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2018</p>
<p>Data-driven discovery of physical laws. Pat Langley, Cognitive Science. 51981. 1981</p>
<p>On the Time and Space Complexity of Genetic Programming for Evolving Boolean Conjunctions. Andrei Lissovoi, Pietro S Oliveto, 10.1609/AAAI.V32I1.11517Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). Sheila A Mcilraith, Kilian Q Weinberger, the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI Press2018. February 2-7, 2018</p>
<p>Computational Complexity Analysis of Genetic Programming. Andrei Lissovoi, Pietro S Oliveto, Theory of Evolutionary Computation -Recent Developments in Discrete Optimization. Benjamin Doerr, Frank Neumann, 2020</p>
<p>. 10.1007/978-3-030-29414-4_11Springer</p>
<p>A Survey and Comparison of Tree Generation Algorithms. Sean Luke, Liviu Panait, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceGECCO-20012001. 2001</p>
<p>On the Analysis of Simple Genetic Programming for Evolving Boolean Functions. Andrea Mambrini, Pietro S Oliveto, 10.1007/978-3-319-30668-1_7Genetic Programming -19th European Conference. Lecture Notes in Computer Science. Malcolm I Heywood, James Mcdermott, Mauro Castelli, Ernesto Costa, Kevin Sim, Porto, PortugalSpringer2016. 2016. March 30 -April 1, 20169594EuroGP</p>
<p>Genetic programming needs better benchmarks. James Mcdermott, David Robert White, Sean Luke, Luca Manzoni, Mauro Castelli, Leonardo Vanneschi, Wojciech Jaskowski, Krzysztof Krawiec, Robin Harper, Kenneth A De Jong, Una-May O' Reilly, 10.1145/2330163.2330273Genetic and Evolutionary Computation Conference, GECCO '12. Terence Soule, Jason H Moore, Philadelphia, PA, USAACM2012. July 7-11, 2012</p>
<p>An empirical study of the efficiency of learning boolean functions using a Cartesian Genetic Programming approach. Julian F Miller, Proceedings of the Genetic and Evolutionary Computation Conference. Wolfgang Banzhaf, Jason Daida, Agoston E Eiben, Max H Garzon, Vasant Honavar, Mark Jakiela, Robert E Smith, the Genetic and Evolutionary Computation ConferenceOrlando, Florida, USAMorgan Kaufmann19992</p>
<p>Geometric Semantic Genetic Programming. Alberto Moraglio, Krzysztof Krawiec, Colin G Johnson, 10.1007/978-3-642-32937-1_3Parallel Problem Solving from Nature -PPSN XII -12th International Conference. Lecture Notes in Computer Science. Kalyanmoy Cutello, Stephanie Deb, Giuseppe Forrest, Mario Nicosia, Pavone, Taormina, Italy; Carlos A. Coello CoelloSpringer2012. September 1-5, 20127491Proceedings, Part I</p>
<p>Building new spatial interaction models using genetic programming. S Openshaw, I Turton, Evolutionary Computing. Lecture Notes in Computer Science. Springer-Verlag1994</p>
<p>Stack-Based Genetic Programming. Tim Perkis, 10.1109/ICEC.1994.350025Proceedings of the 1994 IEEE World Congress on Computational Intelligence. the 1994 IEEE World Congress on Computational IntelligenceOrlando, Florida, USAIEEE Press19941</p>
<p>Parallel Distributed Genetic Programming. Riccardo Poli, CSRP-96-151996B15 2TT, UK.School of Computer Science, University of BirminghamTechnical Report</p>
<p>Pretraining Reduces Runtime in Denoising Autoencoder Genetic Programming by an Order of Magnitude. Johannes Reiter, Dirk Schweim, David Wittenberg, 10.1145/3583133.3596332Proceedings of the Companion Conference on Genetic and Evolutionary Computation. the Companion Conference on Genetic and Evolutionary ComputationLisbon, Portugal; New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Grammatical Evolution: Evolving Programs for an Arbitrary Language. J J Conor Ryan, Collins, O' Michael, Neill, 10.1007/BFb0055930Proceedings of the First European Workshop on Genetic Programming. Wolfgang Banzhaf, Riccardo Poli, Marc Schoenauer, Terence C Fogarty, the First European Workshop on Genetic ProgrammingParisSpringer-Verlag19981391</p>
<p>Code Growth in Genetic Programming. Terence Soule, James A Foster, John Dickinson, Proceedings of the 1st Annual Conference on Genetic Programming. the 1st Annual Conference on Genetic ProgrammingStanford, California; Cambridge, MA, USAMIT Press1996</p>
<p>Symbolic physics learner: Discovering governing equations via monte carlo tree search. Fangzheng Sun, Yang Liu, Jian-Xun Wang, Hao Sun, arXiv:2205.131342022. 2022arXiv preprint</p>
<p>The condition of Gram matrices and related problems. Taylor James, Proceedings of the Royal Society of Edinburgh Section A: Mathematics. 801978. 1978</p>
<p>Fitness Clouds and Problem Hardness in Genetic Programming. Leonardo Vanneschi, Manuel Clergue, Philippe Collard, Marco Tomassini, Sébastien Vérel ; Kalyanmoy Deb, Riccardo Poli, Wolfgang Banzhaf, Hans-Georg Beyer, Edmund Burke, Paul Darwen, Dipankar Dasgupta, Dario Floreano, James Foster, Mark Harman, Owen Holland, Pier Luca Lanzi, Lee Spector, 10.1007/978-3-540-24855-2_76Genetic and Evolutionary Computation -GECCO-2004, Part II. Lecture Notes in Computer Science. Andrea Tettamanzi, Dirk Thierens, Andy Tyrrell, Seattle, WA, USASpringer-Verlag20043103</p>
<p>A study of the neutrality of Boolean function landscapes in genetic programming. Leonardo Vanneschi, Yuri Pirola, Giancarlo Mauri, Marco Tomassini, Philippe Collard, Sebastien Verel, 10.1016/j.tcs.2011.03.011Theoretical Computer Science. 4252012. 30 March 2012</p>
<p>Machine learning the gravity equation for international trade. Sergiy Verstyuk, Michael R Douglas, Available at SSRN. 40537952022. 2022</p>
<p>Symbolic Regression is NP-hard. Marco Virgolin, Solon P Pissis, Transactions on Machine Learning Research. 20222022. 2022</p>
<p>Machine learning for the prediction of pseudorealistic pediatric abdominal phantoms for radiation dose reconstruction. Marco Virgolin, Ziyuan Wang, Tanja Alderliesten, A N Peter, Bosman, GP 8523.299 9957.890 1011.070 1817.000 6193.000 10616.000 97 (1 + 𝜆)-GP 13181.412 12216.936 1240.442 5001.000 9001.000 16501.000 97 Canonical-GP 29768.818 17201.884 2117.406 17556.500 25898.000 39717.500 66 FTG 87.590Journal of Medical Imaging. 71 + 152020. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
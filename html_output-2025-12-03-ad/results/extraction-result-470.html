<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-470 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-470</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-470</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-17421155</p>
                <p><strong>Paper Title:</strong> An analysis of reproducibility and non-determinism in HEP software and ROOT data</p>
                <p><strong>Paper Abstract:</strong> Reproducibility is an essential component of the scientific method. In order to validate the correctness or facilitate the extension of a computational result, it should be possible to re-run a published result and verify that the same results are produced. However, reproducing a computational result is surprisingly difficult: non-determinism and other factors may make it impossible to get the same result, even when running the same code on the same machine on the same day. We explore this problem in the context of HEP codes and data, showing three high level methods for dealing with non-determinism in general: 1) Domain specific methods; 2) Domain specific comparisons; and 3) Virtualization adjustments. Using a CMS workflow with output data stored in ROOT files, we use these methods to prevent, detect, and eliminate some sources of non-determinism. We observe improved determinism using pre-determined random seeds, a predictable progression of system timestamps, and fixed process identifiers. Unfortunately, sources of non-determinism continue to exist despite the combination of all three methods. Hierarchical data comparisons also allow us to appropriately ignore some non-determinism when it is unavoidable. We conclude that there is still room for improvement, and identify directions that can be taken in each method to make an experiment more reproducible.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e470.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e470.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HEP reproducibility study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An analysis of reproducibility and non-determinism in HEP software and ROOT data (CMS workflow evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical study of non-determinism and reproducibility in a four-step CMS simulation/reconstruction workflow, identifying sources of variability, measuring differences between repeated runs, and testing domain-specific, comparison-based, and virtualization-based mitigations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An analysis of reproducibility and non-determinism in HEP software and ROOT data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>High Energy Physics (HEP) computational workflows / scientific software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Execute and compare repeated runs of a 4-step CMS simulation/reconstruction workflow (LHE, GEN-SIM, DIGI-RECO, MiniAOD) to evaluate determinism and reproducibility of outputs (ROOT files).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Random number sources (application-level RNG seed and kernel entropy /dev/urandom), real-time clocks and timestamps, process identifiers (PIDs), concurrency (child processes, threads), file and folder names including ephemeral /tmp filenames, ordering of objects in ROOT files, I/O and network/socket calls, memory allocation behavior (differences in madvise calls), provenance/metadata intermixed with physics data, system call non-determinism (system environment dependent behavior).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>File-level checksums (md5/sha1), file sizes, hierarchical object-level equivalence levels (STRUCTURE-EQUAL, CONTENT-EQUAL, BITWISE-EQUAL) as implemented in ROOT diff, counts of unmatched objects, counts of differing system calls and referenced filenames from strace logs.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Repeated identical commands on the same machine produced bitwise-different outputs for all workflow steps; example checksums differed (step 1 sizes 6,545,067 vs 6,545,072; step2 checksums differ though sizes nearly equal; table shows multiple differing checksums across steps). Strace analysis: 16,356 filenames identical across runs, 23 additional ephemeral filenames differed; 2 bytes read from /dev/urandom; one run used 74 more madvise calls; LHE time requests ~31,637,937 calls under Parrot time-warp. With mitigations (fixed RNG seed + Parrot time-warp) full content-equality and bitwise equality were achieved for LHE outputs for 1–121 events; differences remained at >=122 events (some object mismatches persisted). Fixed-PID produced one additional object match at 122 events but did not fully resolve differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Reproducibility declared when two ROOT files are CONTENT-EQUAL (structure and data equal); stronger BITWISE-EQUAL requires identical timestamps. Additional metrics: md5/sha1 checksum equality, counts of structure-equal objects, lists of unmatched objects, and strace-identified system-call differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Without controls, workflows produced non-reproducible (bitwise-different) outputs across runs. With domain-specific fixed RNG seed plus Parrot time-warp (and later pid-fixed), the LHE stage was CONTENT-EQUAL and BITWISE-EQUAL for 1–121 events; at 122 events some objects still differed. Quantitative examples: multiple files in steps 2–4 had different checksums and slightly different sizes as reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Provenance/metadata serialized alongside physics data causing incidental differences; hidden use of system entropy (/dev/urandom); dependency on system time and unpredictable timestamps; ephemeral file/pathnames embedded in outputs; concurrency and child processes producing nondeterministic ordering; environment-dependent system calls (I/O/network); memory allocation timing differences; large, complex codebase with many authors and implicit nondeterministic behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Domain-specific: setting fixed random seeds via CMS RandomNumberServiceHelper; Domain-specific comparisons: hierarchical comparator ROOT diff with STRUCTURE-EQUAL/CONTENT-EQUAL/BITWISE-EQUAL levels to ignore incidental differences; Virtualization adjustments: use of strace to discover system-call sources and use of Parrot to intercept/modify system calls (time-stop, time-warp, pid-fixed) and translate filesystem requests (CVMFS).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Fixed RNG seed alone did not ensure determinism. Parrot time-warp combined with fixed RNG produced substantial improvement: LHE outputs became bitwise-identical for 1–121 events and all structurally equivalent objects became bitwise-equal in tested runs; pid-fixed made an additional object match at 122 events but did not eliminate all differences. Time-stop caused the task to hang (not effective).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>Primarily 2 runs per condition (each step executed twice for comparison); additional systematic tests across event counts (1..121, 122, >=122) reported for LHE stage.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CMS workflows produce non-deterministic outputs even with fixed application RNG seed due to environment-level nondeterminism (time, PIDs, ephemeral filenames, entropy). Combining domain-specific fixes (fixed seed), domain-specific comparisons (ROOT diff), and virtualization (Parrot time-warp, pid-fixed) substantially reduces variability — enabling bitwise-identical outputs for small event counts (1–121) — but residual nondeterminism remains for larger workloads, indicating further tooling and code changes are required.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e470.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e470.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROOT diff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROOT diff (hierarchical comparator for ROOT files)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A domain-specific comparator developed in this work that compares ROOT files hierarchically at object level and classifies equivalence into STRUCTURE-EQUAL, CONTENT-EQUAL, and BITWISE-EQUAL, enabling detection and isolation of incidental non-determinism embedded in ROOT files.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>High Energy Physics data formats / data validation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Compare two ROOT files from repeated workflow runs to determine structural and content equivalence while tolerating incidental differences such as ordering or timestamps.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Object ordering in ROOT files, timestamps embedded in objects, provenance/metadata objects that differ across runs, ignored structural-only objects.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Three-level equivalence: STRUCTURE-EQUAL (same object length, cycle number, class name), CONTENT-EQUAL (structure equal + identical object data), BITWISE-EQUAL (content equal + same timestamp); counts of unmatched or not-equal objects; runtime performance vs file size compared to md5/sha1.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>ROOT diff detected structural equivalence in many cases where md5/sha1 differed and could declare files CONTENT-EQUAL even when checksums differed; for files <6GB ROOT diff outperformed md5/sha1 by reading only relevant object buffers. Example outcomes: LHE outputs for 1–121 events became BITWISE-EQUAL under mitigations according to ROOT diff.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Use of STRUCTURE-EQUAL / CONTENT-EQUAL / BITWISE-EQUAL as reproducibility criteria; listing and counting of ignored objects and non-equal objects.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Using ROOT diff, some runs that were bitwise-different at file checksum level were judged CONTENT-EQUAL meaning the physics data were reproducible despite incidental metadata differences; ROOT diff enabled targeted ignoring of known provenance objects (see table of ignored objects) to focus validation on scientific content.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Provenance and metadata objects interleaved with data make naive bytewise comparisons misleading; memory limits and random reads can reduce performance for very large files; choice of which objects to ignore requires domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Hierarchical comparison allowing selective ignoring of known non-deterministic metadata objects; matching objects by structure rather than file offset to allow reordering.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Allowed declaring reproduction success (CONTENT-EQUAL) in cases where entire-file checksums differed; improved validation speed for files <6GB relative to md5/sha1 by avoiding full-file scans; enabled pragmatic acceptance of runs that are scientifically equivalent but bitwise-different.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A domain-aware hierarchical comparison (ROOT diff) is more informative and practical than bytewise checksums for assessing reproducibility of HEP outputs because it isolates incidental metadata differences from true content changes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e470.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e470.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parrot virtualization features</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parrot (time-warp, time-stop, pid-fixed) system-call interception and translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of Parrot to intercept and modify system calls to reduce environment-induced nondeterminism; features tested include time-stop (returns fixed time), time-warp (returns incremented synthetic time), and pid-fixed (returns fixed PID).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>System virtualization / computational reproducibility for scientific workflows</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Intercept and modify system calls of CMS workflow processes to produce more deterministic behavior (control time, PIDs, filesystem translation for CVMFS).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>System time/timestamps, process identifiers, CVMFS filesystem interactions, system-call-dependent behaviors (waiting on time), and any kernel-provided entropy accessed via system calls.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Observed object-level equivalence of resulting ROOT files (via ROOT diff) before and after applying Parrot features; runtime/task completion behavior (e.g., hanging with time-stop); counts of time requests under time-warp.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>time-stop caused tasks to hang (not usable); time-warp produced substantial determinism gains — with fixed RNG seed + time-warp all structurally equivalent objects became bitwise-equal for LHE and final ROOT files were bitwise-identical for 1–121 events; pid-fixed produced one additional object match in 122-event case but did not fully eliminate differences.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Use outcome of ROOT diff (CONTENT-EQUAL / BITWISE-EQUAL) on Parrot-modified runs to assess effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Parrot time-warp plus fixed RNG achieved bitwise-identical outputs for small event counts (1–121). Parrot pid-fixed improved reproducibility marginally for the 122-event case but residual differences remained.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Intervening in system calls may change program logic (time-stop caused indefinite wait); providing fixed PIDs can be dangerous if misused (risk of affecting other processes); not all nondeterminism sources are addressable solely by system-call translation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Time-warp (synthetic monotonic time increments per request), pid-fixed (return constant PID), filesystem translation for CVMFS; using Parrot to intercept and modify reads from /dev/urandom or other system resources if needed.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Time-warp was highly effective for LHE stage up to 121 events; pid-fixed had limited additional benefit for larger runs; time-stop was ineffective (caused hang).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>System-call interception (time-warp) can substantially reduce environment-driven nondeterminism, but careful tuning is required and not all sources are eliminated; some interventions (time-stop) break program behavior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e470.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e470.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fixed RNG seed (RandomNumberServiceHelper)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Application-level fixed random seed via CMS RandomNumberServiceHelper.resetSeeds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of CMS framework RandomNumberServiceHelper to set deterministic application-level random seeds prior to job execution to reduce stochastic variability from RNG usage in the simulation chain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Simulation reproducibility in High Energy Physics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>Initialize/reset all application RNG seeds at job start so repeated runs use identical seed sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>Application-level stochastic sampling (Monte Carlo RNG), potential additional kernel-level entropy reads (/dev/urandom) that can introduce unexpected randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Observation of whether file-level checksums and object-level comparisons change across runs when rng seeds are fixed versus not fixed; used in conjunction with ROOT diff and Parrot features.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Setting application-level seeds was necessary but not sufficient to obtain bitwise-identical outputs; even with fixed seeds, outputs differed due to environment-level nondeterminism (timestamps, PIDs, ephemeral filenames, kernel entropy). When combined with Parrot time-warp, fixed seed helped achieve bitwise-identical outputs for small event counts.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Effect on checksum and ROOT diff equivalence when seeds are set vs not set.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Fixed seed reduced stochastic variability from application RNG but by itself did not guarantee reproducibility; effective when combined with virtualization and object-level comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Hidden sources of entropy (kernel reads, ephemeral file naming) and environment effects undermine the control provided by application-level seed setting.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Resetting all RNGs at process start via RandomNumberServiceHelper.resetSeeds(random_seed) as shown in the paper's code snippet.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Necessary baseline mitigation; in combination with Parrot time-warp can yield bitwise-identical outputs for limited cases (LHE <=121 events).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Setting deterministic application RNG seeds is required but insufficient alone; it must be combined with environment controls (time, PIDs, filesystem behavior) and domain-aware comparisons to achieve practical reproducibility.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Wavelab and reproducible research <em>(Rating: 2)</em></li>
                <li>A tool for one-to-one comparison of edm objects <em>(Rating: 2)</em></li>
                <li>Scalable Computing: Practice and Experience <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-470",
    "paper_id": "paper-17421155",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "HEP reproducibility study",
            "name_full": "An analysis of reproducibility and non-determinism in HEP software and ROOT data (CMS workflow evaluation)",
            "brief_description": "Empirical study of non-determinism and reproducibility in a four-step CMS simulation/reconstruction workflow, identifying sources of variability, measuring differences between repeated runs, and testing domain-specific, comparison-based, and virtualization-based mitigations.",
            "citation_title": "An analysis of reproducibility and non-determinism in HEP software and ROOT data",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "High Energy Physics (HEP) computational workflows / scientific software engineering",
            "experimental_task": "Execute and compare repeated runs of a 4-step CMS simulation/reconstruction workflow (LHE, GEN-SIM, DIGI-RECO, MiniAOD) to evaluate determinism and reproducibility of outputs (ROOT files).",
            "variability_sources": "Random number sources (application-level RNG seed and kernel entropy /dev/urandom), real-time clocks and timestamps, process identifiers (PIDs), concurrency (child processes, threads), file and folder names including ephemeral /tmp filenames, ordering of objects in ROOT files, I/O and network/socket calls, memory allocation behavior (differences in madvise calls), provenance/metadata intermixed with physics data, system call non-determinism (system environment dependent behavior).",
            "variability_measured": true,
            "variability_metrics": "File-level checksums (md5/sha1), file sizes, hierarchical object-level equivalence levels (STRUCTURE-EQUAL, CONTENT-EQUAL, BITWISE-EQUAL) as implemented in ROOT diff, counts of unmatched objects, counts of differing system calls and referenced filenames from strace logs.",
            "variability_results": "Repeated identical commands on the same machine produced bitwise-different outputs for all workflow steps; example checksums differed (step 1 sizes 6,545,067 vs 6,545,072; step2 checksums differ though sizes nearly equal; table shows multiple differing checksums across steps). Strace analysis: 16,356 filenames identical across runs, 23 additional ephemeral filenames differed; 2 bytes read from /dev/urandom; one run used 74 more madvise calls; LHE time requests ~31,637,937 calls under Parrot time-warp. With mitigations (fixed RNG seed + Parrot time-warp) full content-equality and bitwise equality were achieved for LHE outputs for 1–121 events; differences remained at &gt;=122 events (some object mismatches persisted). Fixed-PID produced one additional object match at 122 events but did not fully resolve differences.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Reproducibility declared when two ROOT files are CONTENT-EQUAL (structure and data equal); stronger BITWISE-EQUAL requires identical timestamps. Additional metrics: md5/sha1 checksum equality, counts of structure-equal objects, lists of unmatched objects, and strace-identified system-call differences.",
            "reproducibility_results": "Without controls, workflows produced non-reproducible (bitwise-different) outputs across runs. With domain-specific fixed RNG seed plus Parrot time-warp (and later pid-fixed), the LHE stage was CONTENT-EQUAL and BITWISE-EQUAL for 1–121 events; at 122 events some objects still differed. Quantitative examples: multiple files in steps 2–4 had different checksums and slightly different sizes as reported in paper tables.",
            "reproducibility_challenges": "Provenance/metadata serialized alongside physics data causing incidental differences; hidden use of system entropy (/dev/urandom); dependency on system time and unpredictable timestamps; ephemeral file/pathnames embedded in outputs; concurrency and child processes producing nondeterministic ordering; environment-dependent system calls (I/O/network); memory allocation timing differences; large, complex codebase with many authors and implicit nondeterministic behaviors.",
            "mitigation_methods": "Domain-specific: setting fixed random seeds via CMS RandomNumberServiceHelper; Domain-specific comparisons: hierarchical comparator ROOT diff with STRUCTURE-EQUAL/CONTENT-EQUAL/BITWISE-EQUAL levels to ignore incidental differences; Virtualization adjustments: use of strace to discover system-call sources and use of Parrot to intercept/modify system calls (time-stop, time-warp, pid-fixed) and translate filesystem requests (CVMFS).",
            "mitigation_effectiveness": "Fixed RNG seed alone did not ensure determinism. Parrot time-warp combined with fixed RNG produced substantial improvement: LHE outputs became bitwise-identical for 1–121 events and all structurally equivalent objects became bitwise-equal in tested runs; pid-fixed made an additional object match at 122 events but did not eliminate all differences. Time-stop caused the task to hang (not effective).",
            "comparison_with_without_controls": true,
            "number_of_runs": "Primarily 2 runs per condition (each step executed twice for comparison); additional systematic tests across event counts (1..121, 122, &gt;=122) reported for LHE stage.",
            "key_findings": "CMS workflows produce non-deterministic outputs even with fixed application RNG seed due to environment-level nondeterminism (time, PIDs, ephemeral filenames, entropy). Combining domain-specific fixes (fixed seed), domain-specific comparisons (ROOT diff), and virtualization (Parrot time-warp, pid-fixed) substantially reduces variability — enabling bitwise-identical outputs for small event counts (1–121) — but residual nondeterminism remains for larger workloads, indicating further tooling and code changes are required.",
            "uuid": "e470.0"
        },
        {
            "name_short": "ROOT diff",
            "name_full": "ROOT diff (hierarchical comparator for ROOT files)",
            "brief_description": "A domain-specific comparator developed in this work that compares ROOT files hierarchically at object level and classifies equivalence into STRUCTURE-EQUAL, CONTENT-EQUAL, and BITWISE-EQUAL, enabling detection and isolation of incidental non-determinism embedded in ROOT files.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "High Energy Physics data formats / data validation",
            "experimental_task": "Compare two ROOT files from repeated workflow runs to determine structural and content equivalence while tolerating incidental differences such as ordering or timestamps.",
            "variability_sources": "Object ordering in ROOT files, timestamps embedded in objects, provenance/metadata objects that differ across runs, ignored structural-only objects.",
            "variability_measured": true,
            "variability_metrics": "Three-level equivalence: STRUCTURE-EQUAL (same object length, cycle number, class name), CONTENT-EQUAL (structure equal + identical object data), BITWISE-EQUAL (content equal + same timestamp); counts of unmatched or not-equal objects; runtime performance vs file size compared to md5/sha1.",
            "variability_results": "ROOT diff detected structural equivalence in many cases where md5/sha1 differed and could declare files CONTENT-EQUAL even when checksums differed; for files &lt;6GB ROOT diff outperformed md5/sha1 by reading only relevant object buffers. Example outcomes: LHE outputs for 1–121 events became BITWISE-EQUAL under mitigations according to ROOT diff.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Use of STRUCTURE-EQUAL / CONTENT-EQUAL / BITWISE-EQUAL as reproducibility criteria; listing and counting of ignored objects and non-equal objects.",
            "reproducibility_results": "Using ROOT diff, some runs that were bitwise-different at file checksum level were judged CONTENT-EQUAL meaning the physics data were reproducible despite incidental metadata differences; ROOT diff enabled targeted ignoring of known provenance objects (see table of ignored objects) to focus validation on scientific content.",
            "reproducibility_challenges": "Provenance and metadata objects interleaved with data make naive bytewise comparisons misleading; memory limits and random reads can reduce performance for very large files; choice of which objects to ignore requires domain knowledge.",
            "mitigation_methods": "Hierarchical comparison allowing selective ignoring of known non-deterministic metadata objects; matching objects by structure rather than file offset to allow reordering.",
            "mitigation_effectiveness": "Allowed declaring reproduction success (CONTENT-EQUAL) in cases where entire-file checksums differed; improved validation speed for files &lt;6GB relative to md5/sha1 by avoiding full-file scans; enabled pragmatic acceptance of runs that are scientifically equivalent but bitwise-different.",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "A domain-aware hierarchical comparison (ROOT diff) is more informative and practical than bytewise checksums for assessing reproducibility of HEP outputs because it isolates incidental metadata differences from true content changes.",
            "uuid": "e470.1"
        },
        {
            "name_short": "Parrot virtualization features",
            "name_full": "Parrot (time-warp, time-stop, pid-fixed) system-call interception and translation",
            "brief_description": "Use of Parrot to intercept and modify system calls to reduce environment-induced nondeterminism; features tested include time-stop (returns fixed time), time-warp (returns incremented synthetic time), and pid-fixed (returns fixed PID).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "System virtualization / computational reproducibility for scientific workflows",
            "experimental_task": "Intercept and modify system calls of CMS workflow processes to produce more deterministic behavior (control time, PIDs, filesystem translation for CVMFS).",
            "variability_sources": "System time/timestamps, process identifiers, CVMFS filesystem interactions, system-call-dependent behaviors (waiting on time), and any kernel-provided entropy accessed via system calls.",
            "variability_measured": true,
            "variability_metrics": "Observed object-level equivalence of resulting ROOT files (via ROOT diff) before and after applying Parrot features; runtime/task completion behavior (e.g., hanging with time-stop); counts of time requests under time-warp.",
            "variability_results": "time-stop caused tasks to hang (not usable); time-warp produced substantial determinism gains — with fixed RNG seed + time-warp all structurally equivalent objects became bitwise-equal for LHE and final ROOT files were bitwise-identical for 1–121 events; pid-fixed produced one additional object match in 122-event case but did not fully eliminate differences.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Use outcome of ROOT diff (CONTENT-EQUAL / BITWISE-EQUAL) on Parrot-modified runs to assess effectiveness.",
            "reproducibility_results": "Parrot time-warp plus fixed RNG achieved bitwise-identical outputs for small event counts (1–121). Parrot pid-fixed improved reproducibility marginally for the 122-event case but residual differences remained.",
            "reproducibility_challenges": "Intervening in system calls may change program logic (time-stop caused indefinite wait); providing fixed PIDs can be dangerous if misused (risk of affecting other processes); not all nondeterminism sources are addressable solely by system-call translation.",
            "mitigation_methods": "Time-warp (synthetic monotonic time increments per request), pid-fixed (return constant PID), filesystem translation for CVMFS; using Parrot to intercept and modify reads from /dev/urandom or other system resources if needed.",
            "mitigation_effectiveness": "Time-warp was highly effective for LHE stage up to 121 events; pid-fixed had limited additional benefit for larger runs; time-stop was ineffective (caused hang).",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "System-call interception (time-warp) can substantially reduce environment-driven nondeterminism, but careful tuning is required and not all sources are eliminated; some interventions (time-stop) break program behavior.",
            "uuid": "e470.2"
        },
        {
            "name_short": "Fixed RNG seed (RandomNumberServiceHelper)",
            "name_full": "Application-level fixed random seed via CMS RandomNumberServiceHelper.resetSeeds",
            "brief_description": "Use of CMS framework RandomNumberServiceHelper to set deterministic application-level random seeds prior to job execution to reduce stochastic variability from RNG usage in the simulation chain.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Simulation reproducibility in High Energy Physics",
            "experimental_task": "Initialize/reset all application RNG seeds at job start so repeated runs use identical seed sequences.",
            "variability_sources": "Application-level stochastic sampling (Monte Carlo RNG), potential additional kernel-level entropy reads (/dev/urandom) that can introduce unexpected randomness.",
            "variability_measured": true,
            "variability_metrics": "Observation of whether file-level checksums and object-level comparisons change across runs when rng seeds are fixed versus not fixed; used in conjunction with ROOT diff and Parrot features.",
            "variability_results": "Setting application-level seeds was necessary but not sufficient to obtain bitwise-identical outputs; even with fixed seeds, outputs differed due to environment-level nondeterminism (timestamps, PIDs, ephemeral filenames, kernel entropy). When combined with Parrot time-warp, fixed seed helped achieve bitwise-identical outputs for small event counts.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Effect on checksum and ROOT diff equivalence when seeds are set vs not set.",
            "reproducibility_results": "Fixed seed reduced stochastic variability from application RNG but by itself did not guarantee reproducibility; effective when combined with virtualization and object-level comparison.",
            "reproducibility_challenges": "Hidden sources of entropy (kernel reads, ephemeral file naming) and environment effects undermine the control provided by application-level seed setting.",
            "mitigation_methods": "Resetting all RNGs at process start via RandomNumberServiceHelper.resetSeeds(random_seed) as shown in the paper's code snippet.",
            "mitigation_effectiveness": "Necessary baseline mitigation; in combination with Parrot time-warp can yield bitwise-identical outputs for limited cases (LHE &lt;=121 events).",
            "comparison_with_without_controls": true,
            "number_of_runs": null,
            "key_findings": "Setting deterministic application RNG seeds is required but insufficient alone; it must be combined with environment controls (time, PIDs, filesystem behavior) and domain-aware comparisons to achieve practical reproducibility.",
            "uuid": "e470.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Wavelab and reproducible research",
            "rating": 2,
            "sanitized_title": "wavelab_and_reproducible_research"
        },
        {
            "paper_title": "A tool for one-to-one comparison of edm objects",
            "rating": 2,
            "sanitized_title": "a_tool_for_onetoone_comparison_of_edm_objects"
        },
        {
            "paper_title": "Scalable Computing: Practice and Experience",
            "rating": 1,
            "sanitized_title": "scalable_computing_practice_and_experience"
        }
    ],
    "cost": 0.012095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An analysis of reproducibility and non-determinism in HEP software and ROOT data</p>
<p>Peter Ivie 
Departments of Computer Science and Engineering, and Physics
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Charles Zheng 
Departments of Computer Science and Engineering, and Physics
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Kevin Lannon 
Departments of Computer Science and Engineering, and Physics
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>Douglas Thain 
Departments of Computer Science and Engineering, and Physics
University of Notre Dame
Notre Dame
46556INUSA</p>
<p>An analysis of reproducibility and non-determinism in HEP software and ROOT data
10.1088/1742-6596/898/10/102007
Reproducibility is an essential component of the scientific method. In order to validate the correctness or facilitate the extension of a computational result, it should be possible to re-run a published result and verify that the same results are produced. However, reproducing a computational result is surprisingly difficult: non-determinism and other factors may make it impossible to get the same result, even when running the same code on the same machine on the same day. We explore this problem in the context of HEP codes and data, showing three high level methods for dealing with non-determinism in general: 1) Domain specific methods; 2) Domain specific comparisons; and 3) Virtualization adjustments. Using a CMS workflow with output data stored in ROOT files, we use these methods to prevent, detect, and eliminate some sources of non-determinism. We observe improved determinism using pre-determined random seeds, a predictable progression of system timestamps, and fixed process identifiers. Unfortunately, sources of non-determinism continue to exist despite the combination of all three methods. Hierarchical data comparisons also allow us to appropriately ignore some non-determinism when it is unavoidable. We conclude that there is still room for improvement, and identify directions that can be taken in each method to make an experiment more reproducible.</p>
<p>Introduction</p>
<p>Scientific discovery in high energy physics is a collaborative effort. Confidence in and acceptance of the work of colleagues is vital in making new discoveries. One of the ways we gain confidence in research is by seeing that the results are not an accident. "We do not take even our own observations quite seriously, or accept them as scientific observations, until we have repeated and tested them. Only by such repetitions can we convince ourselves that we are not dealing with a mere isolated coincidence, but with events which, on account of their regularity and reproducibility, are in principle intersubjectively testable." [1] A publication is normally geared more towards communicating ideas to colleagues rather than providing precise steps to compute the results again.</p>
<p>Reproducible research can be shared with colleagues in a way that the research can both be repeated and extended as a building block for other scientists. An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions. [2] The environment and full set of instructions used by the computer make it possible to check whether multiple runs of the same software produce the same result. This may be done to validate whether a new machine produces correct results on old software, whether new software produces correct results on an old machine, or to otherwise compare repeated or extended iterations of the research. Unfortunately, replicating an environment on disparate computing resources is very challenging due to the wide variety of hardware and software choices. But even assuming that the environment is unchanging, many additional technical issues in computing still make it surprisingly hard to get the same result twice. Non-determinism in both codes and data [3] can arise unexpectedly from the use of concurrency, random number sources, real-time clocks, I/O operations, and other sources [4]. Differences might also be due to fundamentally different algorithms, or from accidents of the runtime environment. As a result, one cannot simply compare objects at the binary level.</p>
<p>We attempt to address non-determinism with the following three approaches. 1) Domain specific methods are sometimes available to alleviate some of the non-determinism. If not, then new methods might be implemented for this purpose. 2) Domain specific comparisons could be applied to results to sort through results eliminating sources of non-determinism, such as timestamps and ordering issues. 3) Virtualization adjustments allow control of environmental sources of non-determinism without the need for domain specific considerations.</p>
<p>We evaluate a typical CMS workflow used by physicists at the University of Notre Dame considering the same three ideas. 1) We tap into domain specific options such as a random seed setting in the configuration files. 2) We consider an existing tool for comparisons on CMS data and introduce a new tool called ROOT diff for comparing ROOT files in CMS but also more generally. The ROOT diff tool takes a hierarchical approach to equivalence which aids in isolating differences. 3) We search the CMS workflow for possible sources of non-determinism using strace. While running what should be an identical task multiple times, a few red flags are identified, and their possible severity is discussed. Virtualization adjustments are also employed using the Parrot tool with various options employed to eliminate non-determinism.</p>
<p>Despite applying domain specific methods, non-determinism still existed in each step of the workflow. However, domain specific comparisons showed levels of equivalence that were not detectable with bitwise or hash comparisons. In other words, non-deterministic elements were being embedded around deterministic results. Using virtualization adjustments, the Time Warp feature in Parrot seemed to produce the best results, and a fixed PID feature allowed for additional improvement. Using them in tandem with the other methods, we were able to get deterministic results for the first step in the workflow for fewer than 122 events. We continue to investigate possible causes of non-determinism when 122 events are requested.</p>
<p>All three evaluated avenues can be further pursued to make the validation of CMS workflows more successful. 1) Additional domain specific methods may be needed to separate significant results from incidental or transitory meta-data or to ensure predictable entropy. 2) Continued work on domain specific comparison tools could enable the detection of more fine grained differences, such as statistical equivalence, or to provide a framework for automating conclusions based on various equivalence metrics. 3) More options for virtualization adjustment could force the environment to ensure more deterministic behavior, such as by adjusting the algorithm that 'warps' time, or detecting other system calls that result in non-deterministic behavior. Efforts in any and all of these avenues have the potential to improve the ability to validate results and gauge reproducibility in workflows.</p>
<p>Domain specific methods</p>
<p>There are often domain specific parameters or configuration options built into tools that enable more deterministic behavior. Invoking these options can be helpful in avoiding non-determinism, but might not solve all problems. If identifiable problems are found for which configuration options are not yet available, it might be possible to add new options or otherwise modify the tools to behave more predictably. Understanding common sources of non-determinism (which will be discussed later in the Virtualization adjustments section) can be helpful in general. But clearly, changes to domain specific tools are easier when a specific source of non-determinism can be clearly identified. So we will start by observing the specifics of a CMS workflow used by physicists at the University of Notre Dame before moving on to more generic observations.</p>
<p>After describing the workflow, we describe the behavior that we observed when attempting to run the same exact task twice. In addition, we used a domain specific random seed parameter to encourage the generation of deterministic results. The two runs of a given step are compared to see whether we get bitwise identical results or not.</p>
<p>CMS workflow description</p>
<p>The following 4 steps make up a chain of tasks used to simulate possible collision events using models based on the real events observed in the Large Hadron Collider. The only difference between the workflow we used for our evaluation and the one used for real research is one of scale. We simulate relatively few events for the purposes of our evaluation, but the full complexity of the code is employed. Each of the 4 steps is described below, and the output generated from earlier steps is used as the input for later steps.</p>
<p>Physics Simulation (step #1 -LHE): This is a simulation of the hard scatter part of the collision without initial or final state radiation (ISR/FSR), hadronization, underlying event or pileup. There is no attempt to account for the detector at this stage. The acronym LHE stands for Les Houches Event [5].</p>
<p>Detector Simulation (step #2 -GEN-SIM): For very technical reasons, ISR/FSR, hadronization, and the underlying event are included in this stage, but not pileup. After this, the effects of the detector are simulated, but the data format read out is not the same as what the detector readout produces.</p>
<p>Reconstruction (step #3 -DIGI-RECO): The next step, is actually broken into two separate sub-steps that are run sequentially: The DIGI step takes the simulation file output and changes it into a format that is identical to what the detector records (hits, clusters, etc.) Pileup is also incorporated at this stage. After this step, no distinction needs to be made in the software between running on simulated and real data. The RECO step is the same reconstruction as applied to real data that reconstructs physics objects (tracks, electrons, muons, jets) from the raw hits and clusters in the (simulated) detector.</p>
<p>Data Reduction (step #4 -MiniAOD): This last step takes the output of the RECO step (which is in a data format known as AOD = Analysis Object Data), and simplifies it into a reduced data format that contains the information that almost everyone needs to do analysis. This is primarily accomplished through a combination of discarding more detailed hit and cluster level data not routinely used and by content-aware compression (e.g. discarding unnecessary precision in specific variables). Some small fraction of analyses actually need the level of detail in AOD and can't use MiniAOD, but most researchers use the MiniAOD data.</p>
<p>CMS workflow results</p>
<p>In an initial attempt to get deterministic results for each step in the workflow, the method shown in Figure 1 was used to force every execution of the step to use the exact same random seed.</p>
<p>Each step in the workflow was then executed twice on the same machine, on the same day, with the exact same command and parameters. The output of the first execution was moved to a separate folder so the exact same command (including the folder name) could be used for the second execution immediately. For steps 2-4, where the results from the previous step are used as input, the result from the first run of the previous step was used for both runs of the following step.   </p>
<p>Detector Simulation (step #2 -GEN-SIM):</p>
<p>The same is true for step2. The checksums for the two output files differ from each other.</p>
<p>Reconstruction (step #3 -DIGI-RECO):</p>
<p>Step 3 produces 3 ROOT files, but only one of them (File #1) is used as the input for step 4. File #2 is the output from the DIGI sub-step and is used as input for the RECO sub-step. File #3 is the data quality monitoring output. None of the files appears to be fully deterministic.</p>
<p>Data Reduction (step #4 -MiniAOD):</p>
<p>The final step also exhibits non-deterministic behavior. For all steps in the workflow, the exact same command produces results that are bitwise different each time it is run. Without some domain specific comparison tool or manual comparison by an expert, it is impossible to know whether the results are equivalent or if some underlying change in the environment caused the two results to diverge from each other in a significant manner. The results of a CMS workflow are serialized into ROOT objects and stored in a ROOT file. We use the ROOT framework [6] to process experiment datasets since comparing checksums is insufficient. As shown in Figure 2, a ROOT file is a sequence of data records with a well defined format. A file header contains information such as a file identifier, file version, the compression level, etc. The file also contains blocks of object data, each with header data. An object header contains the object length, header length, pointer to the object, etc., for the binary object data which is found immediately after it in the file. We can validate successful reproduction of workflow by comparing the structure and contents of each object in the ROOT file. If a workflow is reproduced correctly, each ROOT object it produced should have a matching object from the result of the original workflow. Based on this observation, we developed ROOT diff, a domain specific comparator for scientific workflows that produce ROOT files.</p>
<p>Domain specific comparisons</p>
<p>Comparison procedure</p>
<p>Simulation results of large physics workflows are often large and contain different intermediate ROOT files from multiple substages. A tool developed by CMS for one-to-one EDM object comparison is available. This is a perfect tool for domain scientists who want to observe the difference between the events, particles and variables of two ROOT files. Since the comparison is conducted at the physics object level, a more fine-grained analysis of root files is required, which consumes time and resources. For comparing a typical RECO file, it launches 180 processes, takes 30 minutes and creates many files [7]. To simplify and speed up the validation we only compare the structure and contents of each data record.  We defined three levels of equivalence: (1) STRUCTURE-EQUAL: Two root objects have same object length, cycle number and class name.</p>
<p>(2) CONTENT-EQUAL: Two root objects are structurally equal to each other and have the same object data content.</p>
<p>(3) BITWISE-EQUAL: Two root objects are content equal to each other and have same timestamp. We claim that reproduction is successful, if two ROOT files are CONTENT-EQUAL to each other.</p>
<p>The comparison procedure of ROOT diff is shown in Figure 3.</p>
<p>Step 1: Scan ROOT file 1, extract the information of each object and generate an object information list called objs info lst.</p>
<p>Step 2: Compare the structure of each object in file 2 with every object cached in objs info lst. This step ensures that if two root files have a different object order but the same object structure and contents will still be treated as equivalent.</p>
<p>Step 3: If a matched object is found in file 1, then we generate an object information pair that has the two structurally equivalent objects from each file. All these pairs are stored in a list called struc eq objs. If every object in file 1 can find a matching object in file 2, then file 1 is STRUCTURE-EQUAL to file 2. Objects with no match are stored in lists no match 1 and no match 2 respectively for reporting purposes.</p>
<p>Step 4: For each pair of structurally equivalent objects, the contents of the two objects are compared. If each object in file 1 has a matching object in file 2 and has the same data content, we say that file 1 is CONTENT-EQUAL to file 2. If all objects in file 1 have a CONTENT-EQUAL object in file 2 and share the same timestamp, then we say file 1 and file 2 BITWISE-EQUAL to each other.  We benchmark the performace of ROOT diff by conducting comparison on various sizes of ROOT files produced by Lobster [8]. The running time for comparing ROOT files from megabytes to gigabytes is shown in Figure 4. We also compare ROOT diff with md5sum and sha1sum. The growth of the running time is not always linear, because ROOT files we chosen have different structures. As shown in the figure, ROOT diff has better performance than md5sum and sha1sum, when the file size is smaller than 6GB. That is because ROOT diff does not scan the entire file and ignores objects those are not related to the simulation results. It only reads the desired object buffer blocks and will stop comparing when the first different byte is encountered. When the file size hits the memory limit, ROOT diff begins to suffer from the overhead caused by random reads, while md5sum and sha1sum read data sequentially without much of a decrease in performance.  </p>
<p>Lobster workflow comparison results</p>
<p>Virtualization Adjustments</p>
<p>The unix tool strace is an exploratory form of virtualization where system calls are captured and logged to a file. This log file can then be searched for calls to known sources of non-determinism such as random number generators provided by the kernel. While this tool is unlikely to eliminate non-determinism, we use it with the CMS workflow to identify some read flags that could be causing non-determinism. The Parrot tool [9] is a translational form of virtualization where system calls are captured and can be modified before being forwarded to the operating system. It can be used to trick a program into behaving more deterministically [10].</p>
<p>Finding sources of non-determinism</p>
<p>Executing step 1 using strace two times in a row (keeping random seed, folders, and the machine fixed, as before) produced two log files that were very similar but had notable differences. Various categories of red flags appeared in the strace log files and in comparisons between the two files.</p>
<p>File and Folder names: File and folder names can be a source of non-determinism if they are later included in any output file that includes data that should be used as a basis for validation. The system call 'getcwd' was used which means that the execution could be affected by the current directory that it executes in. Exactly 16,356 total filenames were referenced with the same name in both runs. 23 additional filenames appeared in each run, but each used (what appeared to be random) file names of the format /tmp/tmpfjAMpSC.</p>
<p>Concurrency: Both runs included the 'execve' system call, which starts up a child process. This child process could have it's own set of issues preventing determinism.</p>
<p>Entropy: A total of 2 bytes were read from '/dev/urandom'. This was one of our primary concerns initially. The read occurred after the randomly named tmp files started being created, so it was not the seed for that potential source of non-determinism, at least.</p>
<p>Available Memory: One of the runs used 74 more (about 5% more) 'madvise' system calls than the other run. Even with the same available memory, available block sizes can cause non-deterministic behavior. Additional system calls allocating memory lead to additional time spent which can also affect the entropy of the operating system. Input/Output: The 'socket' system call (and related calls) was used in the runs which indicates an implicit dependence on some external resources. Non-determinism in those resources and the connection to them can both cause unpredictable behavior in a task.</p>
<p>Time: Both the initial time and the passage of time due to possible congestion in the operating system can be an issue.</p>
<p>Eliminating non-determinism</p>
<p>Parrot was used to capture system calls made by step 1 of the CMS workflow. Parrot is often already used for high energy physics because it can translate file system requests for the CernVM File System (CVMFS) into a network request when using computing resources that can't easily mount the CVMFS file system directly (often due to Unix permission issues). Other flags in Parrot enabled us to translate additional system calls to encourage determinism.</p>
<p>Time-Stop in Parrot: A new feature in Parrot was enabled which always returns January 1st, 2000 at midnight when asked for the current system time. We hoped this would make the results for the LHE step more deterministic, however the task would never complete. Upon further exploration we learned that the LHE step checks the current time and waits for a certain amount of time to pass before continuing, so it waited indefinitely.</p>
<p>Time-Warp in Parrot: In order to overcome the issue with the time-stop feature, a more intelligent feature called time-warp was used. The first time reported is January 1st, 2000 at midnight, but for each additional request, the time is incremented by 1/100th of a second. When set to a maximum of 1649 events, an LHE task completed on January 1st, 2001 at 4:18:57 (am). In other words, with the year 2000 being a leap year the task requested the time about 31,637,937 times. This feature in Parrot effected significant improvement in the determinism of the the task. After running the task twice, the contents of all structurally equivalent objects were also bitwise equivalent. In fact, for between 1 and 121 events the final ROOT file was bitwise identical. But for 122 and more events there were still a few objects that were different, requiring a more detailed comparison. Fixed PID in Parrot: Looking more carefully at the 122 event cases, a colleague pointed out that aux.processGUID() causes differences in an identifier that gets included in the ROOT output file. So, a feature called pid-fixed was added to Parrot, so that the same PID would always be returned whenever the system was asked for the current PID. This could be a dangerous option if the provided PID is then used to terminate a process that happens to match an important running process, but no harm appeared to come from using this in the LHE stage. This feature made one additional object match between two LHE runs with 122 events, but additional differences still exist.</p>
<p>Conclusions</p>
<p>While the general causes of non-determinism in software are well known, managing them in a complex piece of software with many authors remains a challenge. Our first look at this issue highlights some of these challenges. CMS codes produce non-deterministic results, even when a random seed is fixed; and ROOT files contain provenance metadata intermixed with physics data. But, we have also shown that some of these effects can be mitigated through the use of system call interception, and various data comparison techniques. We expect that adding concurrency in the form of processes, threads, and accelerators will reveal new challenges if employed. Going forward, our aim is to modify or augment CMS codes to achieve deterministic execution, and then to create techniques and tools to assist developers with managing additional non-determinism within the development process. We aim to improve both the productivity and reliability of computational science in high energy physics.</p>
<p>Figure 1 .
1Python code for setting all random seeds up front.</p>
<p>Figure 2 .
2ROOT file Structure</p>
<p>Figure 3 .
3The ROOT diff algorithm</p>
<p>Figure 4 .
4Performance of ROOT diff</p>
<p>CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series 898 (2017) 102007 doi:10.1088/1742-6596/898/10/102007</p>
<p>CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series 898 (2017) 102007 doi:10.1088/1742-6596/898/10/102007from IOMC.RandomEngine.RandomServiceHelper import RandomNumberServiceHelper 
random_seed = sys.argv[2] 
... 
helper = RandomNumberServiceHelper(process.RandomNumberGeneratorService) 
helper.resetSeeds(random_seed) </p>
<p>Table 1 .
1File checksums &amp; sizes per stage 
Step #1 
Run checksums 
Size </p>
<p>1 
b2ed825f... 6,545,067 
2 
c35bc9c4... 6,545,072 </p>
<p>Step #2 
Run checksums 
Size </p>
<p>1 
a2f8138c... 22,773,369 
2 
40c5791a... 22,773,362 </p>
<p>Step #3 
File Run checksums 
Size </p>
<h1>1</h1>
<p>1 
b8e7810f... 21,320,833 </p>
<h1>1</h1>
<p>2 
f3fb4d9c... 21,320,560 </p>
<h1>2</h1>
<p>1 
81f04953... 38,799,122 </p>
<h1>2</h1>
<p>2 
767237d7... 38,799,120 </p>
<h1>3</h1>
<p>1 
4669be6b... 1,953,306 </p>
<h1>3</h1>
<p>2 
534891a4... 1,953,460 </p>
<p>Step #4 
Run checksums 
Size 
1 
ab202459... 3,384,806 
2 
0fa6a17a... 3,384,594 </p>
<p>CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series 898 (2017) 102007 doi:10.1088/1742-6596/898/10/102007</p>
<p>Table 2 .
2NonequalLHE objects ignored 
Class Name 
Object Name 
Ignored </p>
<p>TTree 
Metadata 
once 
StreamerInfo 
StreamerInfo 
once 
TTree 
ParameterSets 
once 
TTree 
Parentage 
once 
TTree 
Events 
once 
TTree 
LuminosityBlocks 
once 
TTree 
Runs 
once 
KeyList 
HIG-RunIIWinter15 
once 
FreeSegments 
LuminosityBlocks 
once </p>
<p>Tables 2 and 3 come from a comparison 
of Lobster runs. In table 3, we show 
the results of comparing ROOT files 
generated by two simulation runs with 
same substages and seed. ROOT diff 
ignores some of the objects that are 
only related to the structure of the file. 
Examples of ignored objects in the LHE 
stage are in table 2. Four objects in file 
1 named LHEEventProduct exter have 
no matching objects in file 2. Four 
EventAuxiliary objects in file 2 cannot be 
matched to any object in file 1. </p>
<p>Table 3 .
3Comparison results for simulation stagesStage Name 
Same Seed 
Number of 
Objects 
Ignored 
Not 
Equal </p>
<p>Structure 
Equal </p>
<p>CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series 898 (2017) 102007 doi:10.1088/1742-6596/898/10/102007
AcknowledgementsWe gratefully acknowledge Kevin Lannon and Michael Hildreth for assistance with the CMS software. This work was supported in part by the National Science Foundation under grants PHY-1247316 and OCI-1148330, and the Department of Education under grant P200A120206.
The logic of scientific discovery (Routledge). K Popper, Popper K 2005 The logic of scientific discovery (Routledge)</p>
<p>J B Buckheit, D L Donoho, Wavelab and reproducible research. SpringerBuckheit J B and Donoho D L 1995 Wavelab and reproducible research (Springer)</p>
<p>S B Davidson, J Freire, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of dataACMDavidson S B and Freire J 2008 Proceedings of the 2008 ACM SIGMOD international conference on Management of data (ACM) pp 1345-1350</p>
<p>Information and Communication Technology, Electronics and Microelectronics (MIPRO). A Bánáti, P Kacsuk, M Kozlovszky, 38th International Convention on (IEEEBánáti A, Kacsuk P and Kozlovszky M 2015 Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2015 38th International Convention on (IEEE) pp 241-244</p>
<p>. J Alwall, A Ballestrero, P Bartalini, S Belov, E Boos, A Buckley, J M Butterworth, L Dudko, S Frixione, L Garren, Computer Physics Communications. 176Alwall J, Ballestrero A, Bartalini P, Belov S, Boos E, Buckley A, Butterworth J M, Dudko L, Frixione S, Garren L et al. 2007 Computer Physics Communications 176 300-304</p>
<p>. I Antcheva, M Ballintijn, B Bellenot, M Biskup, R Brun, N Buncic, P Canal, D Casadei, O Couet, V Fine, Computer Physics Communications. 182Antcheva I, Ballintijn M, Bellenot B, Biskup M, Brun R, Buncic N, Canal P, Casadei D, Couet O, Fine V et al. 2011 Computer Physics Communications 182 1384-1385</p>
<p>A tool for one. A tool for one-to-one comparison of edm objects https://twiki.cern.ch/twiki/bin/view/CMSPublic/ SWGuidePhysicsToolsEdmOneToOneComparison accessed: September 28, 2016</p>
<p>A Woodard, M Wolf, C Mueller, N Valls, B Tovar, P Donnelly, P Ivie, K H Anampa, P Brenner, D Thain, IEEE International Conference on Cluster Computing. IEEEWoodard A, Wolf M, Mueller C, Valls N, Tovar B, Donnelly P, Ivie P, Anampa K H, Brenner P, Thain D et al. 2015 2015 IEEE International Conference on Cluster Computing (IEEE) pp 322-331</p>
<p>. D Thain, M Livny, Scalable Computing: Practice and Experience. 6Thain D and Livny M 2005 Scalable Computing: Practice and Experience 6 9-18</p>
<p>. H Meng, M Wolf, P Ivie, A Woodard, M Hildreth, D Thain, Journal of Physics: Conference Series. Meng H, Wolf M, Ivie P, Woodard A, Hildreth M and Thain D 2015 Journal of Physics: Conference Series (CHEP 2015)</p>
<p>. 10.1088/1742-6596/898/10/102007CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series. 898102007CHEP IOP Publishing IOP Conf. Series: Journal of Physics: Conf. Series 898 (2017) 102007 doi:10.1088/1742-6596/898/10/102007</p>            </div>
        </div>

    </div>
</body>
</html>
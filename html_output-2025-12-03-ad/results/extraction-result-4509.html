<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4509 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4509</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4509</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-281188f43de819c848a2f985144958ea9a4e0fcb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/281188f43de819c848a2f985144958ea9a4e0fcb" target="_blank">Automated Social Science: Language Models as Scientist and Subjects</a></p>
                <p><strong>Paper Venue:</strong> Social Science Research Network</p>
                <p><strong>Paper TL;DR:</strong> An approach for automatically generating and testing, in silico, social scientific hypotheses through the use of structural causal models, and evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation is provided.</p>
                <p><strong>Paper Abstract:</strong> We present an approach for automatically generating and testing, in silico, social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4509",
    "paper_id": "paper-281188f43de819c848a2f985144958ea9a4e0fcb",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00400325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Automated Social Science:</h1>
<h2>Language Models as Scientist and Subjects</h2>
<p>By<br>Benjamin S. Manning<br>B.A., Washington University in St. Louis, 2017<br>M.P.P., Harvard University, 2021</p>
<h2>SUBMITTED TO THE DEPARTMENT OF MANAGEMENT IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF</h2>
<p>MASTER OF SCIENCE IN MANAGEMENT RESEARCH
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
September 2024
©2024 Benjamin S. Manning. All rights reserved.
The author hereby grants to MIT a nonexclusive, worldwide, irrevocable, royalty-free license to exercise any and all rights under copyright, including to reproduce, preserve, distribute and publicly display copies of the thesis, or release the thesis under an openaccess license.</p>
<p>Authored by: Benjamin S. Manning
Department of Management
July $31^{\text {th }}, 2024$
Certified by: John J. Horton
Department of Management
Thesis supervisor
Accepted by: Eric So
Professor, Global Economics and Finance
Faculty Chair, MIT Sloan PhD Program</p>
<h1>Automated Social Science:</h1>
<h2>Language Models as Scientist and Subjects</h2>
<p>By<br>Benjamin S. Manning</p>
<p>Submitted to the Department of Management on July 30th, 2024 in partial fulfillment of the requirements for the Degree of Master of Science in Management Research</p>
<h2>ABSTRACT</h2>
<p>We present an approach for automatically generating and testing, in silico social scientific hypotheses. This automation is made possible by recent advances in large language models (LLM), but the key feature of the approach is the use of structural causal models. Structural causal models provide a language to state hypotheses, a blueprint for constructing LLM-based agents, an experimental design, and a plan for data analysis. The fitted structural causal model becomes an object available for prediction or the planning of follow-on experiments. We demonstrate the approach with several scenarios: a negotiation, a bail hearing, a job interview, and an auction. In each case, causal relationships are both proposed and tested by the system, finding evidence for some and not others. We provide evidence that the insights from these simulations of social interactions are not available to the LLM purely through direct elicitation. When given its proposed structural causal model for each scenario, the LLM is good at predicting the signs of estimated effects, but it cannot reliably predict the magnitudes of those estimates. In the auction experiment, the in silico simulation results closely match the predictions of auction theory, but elicited predictions of the clearing prices from the LLM are inaccurate. However, the LLM's predictions are dramatically improved if the model can condition on the fitted structural causal model. In short, the LLM knows more than it can (immediately) tell.</p>
<p>Thesis supervisor: John J. Horton
Title: Associate Professor, Information Technology</p>
<h1>List of Figures</h1>
<p>1 An overview of the automated system. ..... 9
2 Experimental design and fitted SCM for "two people bargaining over a mug." ..... 14
3 Experimental design and fitted SCM for "a judge is setting bail for a criminal defendant who committed 50,000 dollars in tax fraud." ..... 15
4 Experimental design and fitted SCM for "a person is interviewing for a job as a lawyer." ..... 17
5 Experimental design and fitted SCM for " 3 bidders participating in an auction for a piece of art starting at fifty dollars." ..... 19
6 Comparison of the LLM's predictions to the theoretical predictions and a subset of experimental results for the auction scenario. ..... 22
7 Comparison of the true and misspecified SCMs. ..... 26
8 Incorrect causal structure identified by the GES algorithm for the tax fraud experiment. ..... 28
A. 1 Example agents generated by the system for "two people bargaining over a mug" ..... 44
A. 2 Menu of interaction protocols for the system to choose from for a given scenario. ..... 46
A. 3 Valid graphical interpretations of the same natural language hypothesis. ..... 51
A. 4 Fitted SCM with interaction terms for "two people bargaining over a mug." ..... 53
A. 5 Fitted SCM with interaction terms for "a judge is setting bail for a criminal defendant who committed 50,000 dollars in tax fraud." ..... 54
A. 6 Fitted SCM with interaction terms for "a person is interviewing for a job as a lawyer." ..... 55
A. 7 Fitted SCM with interaction terms for " 3 bidders participating in an auction for a piece of art starting at fifty dollars." ..... 56
A. 8 Fitted SCM for auction with bidder's reservation prices and second highest bid as exogenous variables. ..... 59
A. 9 Fitted SCM for auction and second highest bid as exogenous variables. ..... 59
A. 10 Comparison of the LLM's predictions to the theoretical predictions and all experimental results for the auction scenario. ..... 60
A. 11 Prompt used to elicity LLM predictions for the Predict- $\hat{\beta}$ task. ..... 61</p>
<h1>List of Tables</h1>
<p>A. 1 GPT-4's predictions for the path estimates for the experiments in Section 3 at temperature 0. ..... 57
A. 2 GPT-4's predictions for the path estimates for the experiments in Section 3 at temperature 1. ..... 58
A. 3 Example of the information generated for each variable in an SCM. ..... 62</p>
<h1>1 Introduction</h1>
<p>There is much work on efficiently estimating econometric models of human behavior but comparatively little work on efficiently generating and testing those models to estimate. Previously, developing such models and hypotheses to test was exclusively a human task. This is changing as researchers have begun to explore automated hypothesis generation through the use of machine learning. ${ }^{1}$ But even with novel machine-generated hypotheses, there is still the problem of testing. A potential solution is simulation. Researchers have shown that Large Language Models (LLM) can simulate humans as experimental subjects with surprising degrees of realism. ${ }^{2}$ To the extent that these simulation results carry over to human subjects in out-ofsample tasks, they provide another testing option (Horton, 2023). In this paper, we combine these ideas-automated hypothesis generation and automated in silico hypothesis testing-by using LLMs for both purposes. We demonstrate that such automation is possible. We evaluate the approach by comparing results to a setting where the real-world predictions are well known and test to see if an LLM can be used to generate information that it cannot access through direct elicitation.</p>
<p>The key innovation in our approach is the use of structural causal models to organize the research process. Structural causal models are mathematical representations of cause and effect (Pearl, 2009b; Wright, 1934) and have long offered a language for expressing hypotheses. ${ }^{3}$ What is novel in our paper is the use of these models as a blueprint for the design of agents and experiments. In short, each explanatory variable describes something about a person or scenario that has to vary for the effect to be identified, so the system "knows" it needs to generate agents or scenarios that</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>vary on that dimension-a straightforward transition from stated theory to experimental design and data generation. Furthermore, the structural causal model offers a pre-specified plan for estimation (Haavelmo, 1943, 1944; Jöreskog, 1970).</p>
<p>We built an open-source computational system implementing this structural causal model-based approach. The system can automatically generate hypotheses, design experiments, run those experiments on independent LLM-powered agents, and analyze the results. We use this system to explore several social scenarios: (1) two people bargaining over a mug, (2) a bail hearing for tax fraud, (3) a lawyer interviewing for a job, and (4) an open ascending price auction with private values for a piece of art. We allow the system to propose the hypotheses for the first two scenarios and then run the experimental simulations without intervention. For (3) and (4), we demonstrate the system's ability to accommodate human input at any point by selecting the hypotheses ourselves and editing some of the agents, but otherwise, we allow the system to proceed autonomously.</p>
<p>Though yet to be optimized for novelty, the system formulates and tests multiple falsifiable hypotheses-from which it generates several findings. The probability of a deal increases as the seller's sentimental attachment to the mug decreases, and both the buyer's and the seller's reservation prices matter. A remorseful defendant is granted lower bail but is not so fortunate if his criminal history is extensive. However, the judge's case count before the hearing-which was hypothesized to matter-does not affect the final bail amount. The candidate passing the bar exam is the only important factor in her getting the job. Neither the candidate's height nor the interviewer's friendliness affect the outcome.</p>
<p>The auction scenario is particularly illuminating. An increase in the bidders' reservation prices causes an increase in the clearing price, a clearing price that is always close to the second-highest reservation amongst the bidders. These simulation results closely match the theory (Maskin and Riley, 1985) and what has been observed empirically (Athey et al., 2011).</p>
<p>None of the findings from the system's experiments are "counterintuitive," but it is important to emphasize they were the result of empiricism, not just model introspection. However, this does raise the question of whether the simulations</p>
<p>are even necessary. Instead of simulation, could an LLM simply do a "thought experiment" about the proposed in silico experiment and achieve the same insight? To test this idea, we describe the experiments that will be simulated and ask the LLM to predict the results - both the path estimates and point predictions. The path estimates being the coefficients in the linear structural causal model. To make this concrete, suppose we had the simple linear model $y=X \beta$ to describe some scenario, and we ran an experiment to estimate $\hat{\beta}$. We describe the scenario and the experiment to the LLM and ask it to predict $y_{i}$ given a particular $X_{i}$ (a "predict- $y_{i}$ " task). Separately, we ask it to predict $\hat{\beta}$ (a "predict- $\hat{\beta}$ " task). Later, we examine how the LLM does on the predict- $y_{i}$ task when it has access to the fitted structural causal model (i.e., $\hat{\beta}$ ).</p>
<p>In the predict- $y_{i}$ task, we prompt the LLM to predict the outcome $y_{i}$ given each possible combination of the $X_{i}$ 's from the auction experiment. Direct elicitation of the predictions for $y_{i}$ in the auction experiment is wildly inaccurate. The predictions are even further from the theory than the empirical results.</p>
<p>In the predict- $\hat{\beta}$ task, the LLM is asked to predict the fitted structural causal model's path estimates for all four experiments, provided with contextual information about each scenario. On average, the LLM predicts the path estimates are 13.2 times larger than the experimental results. Its predictions are overestimates for 10 out of 12 of the paths, although they are generally in the correct direction.</p>
<p>We repeat the predict- $y_{i}$ task, but this time, we provide the LLM with the experimental path estimates. For each $X_{i}$, we fit the structural causal model using all but the $i$ th observation and then ask the LLM to predict $y_{i}$ given $X_{i}$ and this fitted model. In this "predict- $y_{i} \mid \hat{\beta}<em i="i">{-i}$ " task, the predictions are far better than in the predict- $y</em>$ task without the fitted model. The mean squared error is six times lower, and the predictions are much closer to those made by the theory. The process of experimentation can improve the LLM's capacity to predict its behavior.</p>
<p>We design and implement an approach to automated social science because LLMs possess latent information about human behavior that can be systematically explored and extracted (Burns et al., 2023; Scherrer et al., 2024). These models are trained to predict the next token in a sequence of text from a massive human-generated corpus.</p>
<p>From this straightforward objective, the models develop a remarkably sophisticated model of the world, at least as captured in text (Bubeck et al., 2023; Gurnee and Tegmark, 2023; Patel and Pavlick, 2021). And while there are many situations where LLMs are imperfect proxies for humans (Cheng et al., 2023; Santurkar et al., 2023), there is also a growing body of work demonstrating that experiments with LLMs as subjects can predict human behavior in never-before-seen tasks (Binz and Schulz, 2023a; Li et al., 2024). Rapid and automated exploration of these models' behavior could be a powerful tool to efficiently generate new insights about humans. Our contribution is to demonstrate that it is possible to create such a tool: a system that can simulate the entire social scientific process without human input at any step.</p>
<p>The remainder of this paper is structured as follows: Section 2 provides an overview of the system. Section 3 provides some results generated using our system. Section 4 explores an LLM's capacity to predict the results in Section 3. Section 5 discusses the advantages of using SCMs over other methods for studying causal relationships in simulations of social interactions. The paper concludes in Section 6.</p>
<h1>2 Overview of the system</h1>
<p>To perform this automated social science, we needed to build a system. The system intentionally mirrors the experimental social scientific process. These steps are, in broad strokes:</p>
<ol>
<li>Social scientists start by selecting a topic or domain to study (e.g., misinformation, auctions, bargaining, etc).</li>
<li>Within the domain, they identify interesting outcomes and some causes that might affect the outcomes. These variables and their proposed relationships are the hypotheses.</li>
<li>They design an experiment to test these hypotheses by inducing variation in the causes and measuring the outcomes.</li>
<li>
<p>After designing the experiment, social scientists determine how they will analyze the data in a pre-analysis plan.</p>
</li>
<li>
<p>Next, they recruit participants, run the experiment, and collect the data.</p>
</li>
<li>Finally, they analyze the data per the pre-analysis plan to estimate the relationships between the proposed causes and outcomes.</li>
</ol>
<p>While any given social scientist might not follow this sequence exactly, whatever their approach may be, the first two steps should always guide the later steps - the development of the hypothesis guides the experimental design and model estimation. Of course, many social scientists must often omit steps 3-5 when a controlled experiment is impossible, but they typically have some notion of the experiment they would like to run.</p>
<p>To build our system, we formalized a sequence of these steps analogous to those listed above. The system executes them autonomously. Since the system uses AI agents instead of human subjects, it can always design and execute an experiment.</p>
<p>Structural causal models (SCM) are essential to the system's design because they make unambiguous causal statements, which allow for unambiguous estimation and experimental design. ${ }^{4}$ Algorithms can determine precisely which variables must be exogenously manipulated to identify the effect of a given cause (Pearl, 2009b). If the first two steps in the social scientific process are building the SCM, the last four can be directly determined subject to the SCM. Such precision makes automation possible as the system only relies on a few key early decisions. Otherwise, the space of possible choices for the latter steps would explode, rendering automation infeasible.</p>
<p>The system is implemented in Python and uses GPT-4 for all LLM queries. Its decisions are editable at every step. The overview in this section is a highlevel description of the system, but there are many more specific design choices and programming details in Appendix A. For the purposes of most readers, the highlevel overview should be sufficient to understand the system's process, the results we present in Section 3, and the additional analyses in Sections 4 and 5.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The system takes as input a scenario of social scientific interest: a negotiation, a bail decision, a job interview, an auction, and so on. Starting with (1) this input, the system (2) generates outcomes of interest and their potential causes, (3) creates agents that vary on the exogenous dimensions of said causes, (4) designs an experiment, (5) executes the experiment with LLM-powered agents simulating humans, (6) surveys the agents to measure the outcomes, (7) analyzes the results of the experiment to assess the hypotheses, which can be used to plan a follow-on experiment. Figure 1 illustrates these steps, and we will briefly explore each in greater depth.</p>
<p>Figure 1: An overview of the automated system.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Notes: Each step in the process corresponds to an analogous step in the social scientific process as done by humans. The development of the hypothesis guides the experimental design, execution, and model estimation. Researchers can edit the system's decisions at any step in the process.</p>
<p>The first step is to generate hypotheses as SCMs based on the social scenario, the scenario being the only necessary input to the system. This is done by querying an LLM for the relevant agents and then interesting outcomes, their potential causes, and methods to operationalize and measure both. ${ }^{5}$ We use Typewriter text to</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>indicate example output from the system. Suppose the social scenario is "two people bargaining over a mug." The LLM may generate whether a deal occurs for the mug as an outcome and operationalize it as a binary variable with a ' $1^{\prime}$ ' when a deal occurs and a ' $0^{\prime}$ ' when it does not. It then generates potential exogenous causes and their operationalizations: the buyer's budget, which is operationalized as the buyer's willingness to pay in dollars. The system takes each of these variables, constructs an SCM (see the second step in Figure 1), and stores the relevant information about the operationalizations associated with each variable. ${ }^{67}$ From this point on, the SCM serves as a blueprint for the rest of the process, namely the automatic instantiation of agents, their interaction, and the estimation of the linear paths.</p>
<p>The second step is to construct the relevant agents-the Buyer and the Seller in Figure 1, step 3. By "construct," we mean that the system prompts independent LLMs to be people with sets of attributes. These attributes are the exogenous dimensions of the SCM, dimensions that are varied in each simulation. I.e., the different experimental conditions. For the current scenario, a Budget is provided to the buyer that can take on values of ${\$ 5, \$ 10, \$ 20, \$ 40}$. By simulating interactions of agents that vary on the exogenous dimensions of the SCM, the data generated can be used to fit the SCM.</p>
<p>Next, the system generates survey questions to automatically gather data about the outcomes from the agents once each simulation is complete. An LLM can easily generate these questions when provided with information about the variables in the SCM (e.g., asking the buyer, "Did a deal happen?"). All LLM-powered agents in</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>our system have "memory." They store what happened during the simulation in text, making it easy to ask them questions about what happened.</p>
<p>Fourth, the system determines how the agents should interact. LLMs are designed to generate text in sequence. Since independent LLMs power each agent, one agent must finish speaking before the next begins. This necessitates a turn-taking protocol to simulate the conversation. We programmed a menu of six ordering protocols, from which an LLM is queried to select the most appropriate for a given scenario. We describe each protocol in Appendix A, and they are presented in Figure A.2, but in our bargaining scenario with two agents, there are only two possible ways for the agents to alternate speaking. In this case, the system selects: speaking order: (1) Buyer, (2) Seller, (step 4, Figure 1). The speaking order can be flexible in more complex simulations with more agents, such as an auction or a bail hearing.</p>
<p>Now, the system runs the experiment. The conditions are simulated in parallel (step 5 in Figure 1), each with a different value for the exogenous dimensions of the SCM - the possible budgets for the buyer.</p>
<p>The system must also determine when to stop the simulations. There is no obvious rule for when a conversation should end. Like the halting problem in computer science-it is impossible to write a universal algorithm that can determine whether a given program will complete (Turing, 1937)-such a rule for conversations does not exist. We set two stopping conditions for the simulations. After each agent speaks in a simulation, an external LLM is prompted with the transcript of the conversation and asked if the conversation should continue. If yes, the next agent speaks; otherwise, the simulation ends. Additionally, we limit the total number of agent statements to twenty. One could imagine doing something more sophisticated both with the social interactions and the stopping conditions in the future.</p>
<p>Finally, the system gathers the data for analysis. Outcomes are measured by asking the agents the survey questions (Figure 1, step 6) as determined before the experiment. The data is then used to estimate the linear SCM. For our negotiation, that would be a simple linear model with a single path estimate (i.e., linear coefficient) for the effect of the buyer's budget on the probability of a deal-the final step in Figure 1. Note that an SCM specifies, ex-ante, the exact statistical analyses</p>
<p>to be conducted after the experiment-akin to a pre-analysis plan. This step of the system's process is, therefore, mechanical.</p>
<p>The system, as outlined, is automated from start to finish-the SCM and its accompanying metadata serve as a blueprint for the rest of the process. Once there is a fitted SCM, this process can be repeated. Although we have not automated the transition from one experiment to the next, the system can generate new causal variables, induce variations, and run another experiment based on the results of the first.</p>
<h1>3 Results of experiments</h1>
<p>We present results for four social scenarios explored by the system. In the first two scenarios, our involvement was restricted to entering the scenario description. In the third and fourth scenarios, we selected the hypotheses and edited some of the agents, but the system designed and executed the experiments. We intervened in the latter scenarios not because the system is incapable of simulating these scenarios autonomously but to demonstrate the system's capacity to accommodate human input at any point while still generating exciting results.</p>
<h3>3.1 Bargaining over a mug</h3>
<p>We first use the system to simulate "two people bargaining over a mug" - this phrase being in quotes because it was the only input needed for the system to simulate the following process. The system identified the buyer and seller as the relevant agents and selected the buyer's budget, the seller's minimum acceptable price, and the seller's emotional attachment to the mug as potential causes affecting the outcome of whether a deal occurs.</p>
<p>Table 2a provides the information generated by the system about the SCM and the experimental design. The topmost row, the simulation details, provides highlevel information about the structure of the simulation. The remaining rows provide information about the variables in the SCM and how they were operationalized. The</p>
<p>system automatically generated all this information by iteratively querying an LLM.
The three exogenous variables were operationalized as the buyer's budget in dollars, the seller's minimum acceptable price in dollars, and the seller's emotional attachment as an ordinal scale from "no emotional attachment" to "extreme emotional attachment." The system chose nine values (the "Attribute Treatments" in Table 2a) to vary for each of the first two causes and five for the seller's feelings of love towards the mug (one for each level of the scale). This led to $9 \times 9 \times 5=405$ experimental runs of the simulated conversation between the buyer and seller.</p>
<p>Figure 2b provides the fitted SCM. The outcome variable is given with its mean and variance. The raw path estimates and their standard errors are shown on the arrows. For ordinal variables (e.g., the seller's feelings of love), we treat the levels as numerical values. The buyer and seller reached a deal for the mug in half of the simulations, and all three causes had a significant effect on the probability of a deal.</p>
<p>A one-dollar increase in the buyer's budget caused an average increase of 3.7 percentage points in the probability of a deal $\left(\hat{\beta}^{<em>}=0.51, p&lt;0.001\right) .{ }^{8}$ A one-dollar increase in the seller's minimum acceptable price caused an average decrease of 3.5 percentage points in the probability of a deal occurring $\left(\hat{\beta}^{</em>}=-0.49, p&lt;0.001\right)$. Finally, a one-unit increase in the ordinal scale of the seller's love for the mug, such as going from moderate emotional attachment to high emotional attachment, caused an average decrease of 2.5 percentage points in the probability of a deal $\left(\hat{\beta}^{*}=-0.07\right.$, $p=0.044)$.</p>
<h1>3.2 A bail hearing</h1>
<p>Next, we explore "a judge is setting bail for a criminal defendant who committed 50,000 dollars in tax fraud." Table 3a shows that the system selected a judge, defendant, defense attorney, and prosecutor as the relevant agents. In this scenario, the system selected a more flexible interaction protocol than the one used in the previous experiment. The judge was chosen as a center agent and, in order, the</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Figure 2: Experimental design and fitted SCM for "two people bargaining over a mug."</p>
<table>
<thead>
<tr>
<th>SIMULATION DETAILS</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Agents: Buyer, Seller</td>
<td></td>
</tr>
<tr>
<td>Simulations Run: $9 \times 9 \times 5=405$</td>
<td></td>
</tr>
<tr>
<td>Speaking Order: Buyer, Seller, Buyer, ...repeat</td>
<td></td>
</tr>
<tr>
<td>VARIABLE INFORMATION</td>
<td></td>
</tr>
<tr>
<td>Whether or not a deal occurs</td>
<td></td>
</tr>
<tr>
<td>Measurement Question: coordinator: "Did the buyer and seller explicitly agree on the price of the mug during their interaction?"</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Binary</td>
<td></td>
</tr>
<tr>
<td>Buyer's Budget</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: $\left[{ }^{\prime} 3^{\prime}, \prime 6^{\prime}, \prime 7^{\prime}, \prime 8^{\prime}, \prime 10^{\prime}, \prime 13^{\prime}\right.$,</td>
<td></td>
</tr>
<tr>
<td>$\left.{ }^{\prime} 18^{\prime}, \prime 20^{\prime}, \prime 25^{\prime}\right]$</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Your budget for the mug</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Continuous</td>
<td></td>
</tr>
<tr>
<td>Seller's minimum acceptable price</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: $\left[{ }^{\prime} 3^{\prime}, \prime 5^{\prime}, \prime 7^{\prime}, \prime 8^{\prime}, \prime 10^{\prime}, \prime 13^{\prime}\right.$,</td>
<td></td>
</tr>
<tr>
<td>$\left.{ }^{\prime} 18^{\prime}, \prime 20^{\prime}, \prime 25^{\prime}\right]$</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Your minimum acceptable price for the mug</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Continuous</td>
<td></td>
</tr>
<tr>
<td>Seller's feelings of love towards the mug</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: [no emotional attachment', 'slight emotional attachment', 'moderate emotional attachment', 'high emotional attachment', 'extreme emotional attachment']</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Your feelings of love for the mug</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Ordinal</td>
<td></td>
</tr>
</tbody>
</table>
<p>(a) Information for experimental design
<img alt="img-1.jpeg" src="img-1.jpeg" />
(b) Fitted SCM</p>
<p>Notes: Figure 2a provides the information automatically generated by the system to execute the experiment for its proposed hypothesis. This includes the high-level structure of the simulations, how the outcome is measured, and the treatment variations for each cause. The fitted SCM in Figure $2 b$ shows the experiment results. The outcome is given with its mean and variance. The edges are labeled with their unstandardized path estimate and standard error. We assume a simple linear model for the SCM, such that the above graph can also be written as DealOccurs $=0.037$ BuyerBudget 0.035 MinPrice -0.025 SellerLove.</p>
<p>prosecutor, defense attorney, and defendant as the non-center agents. This means the judge spoke first in every simulation, alternating with the other agents: judge, prosecutor, judge, defense attorney, judge, defendant, and so on. As described in Section A.3, we call this the "center-ordered" interaction protocol.</p>
<p>Figure 3: Experimental design and fitted SCM for "a judge is setting bail for a criminal defendant who committed 50,000 dollars in tax fraud."</p>
<table>
<thead>
<tr>
<th style="text-align: center;">SIMULATION DETAILS</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Agents: Judge, Defendant, Defense attorney, Prosecutor</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Simulations Run: $7 \times 7 \times 5=243$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Speaking Order: Judge, Prosecutor, Judge,</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Defense Attorney, Judge, Defendant, ... repeat</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VARIABLE INFORMATION</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Bail amount set by the judge</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Measurement Question: Judge: "What was the bail amount you set for the defendant?"</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Continuous</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Defendant's criminal history</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: ['0', '1', '2', '3', '6', '9', '12']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Number of your prior convictions</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Count</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Prior case count for judge that day</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: ['0', '2', '5', '9', '12', '18', '23']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Number of cases you have already heard today</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Count</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Defendant's level of remorse</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: ['no expressed remorse', 'low expressed remorse', 'moderate expressed remorse', 'high expressed remorse', 'extreme expressed remorse']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Your level of expressed remorse</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Ordinal</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Information for experimental design
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) Fitted SCM</p>
<p>Notes: Figure 3a provides the information automatically generated by the system to execute the experiment for its proposed hypothesis. Figure 3b shows the fitted SCM from the experiment.</p>
<p>The system chose the outcome to be the final bail amount, and the three proposed causes are the defendant's criminal history, the number of cases the judge has already heard that day, and the defendant's level of remorse. The number of cases the judge already heard that day and the defendant's level of remorse are operationalized literally, as the count of cases the judge has heard and five ordinal levels of possible outward expressions of remorsefulness. The defendant's criminal history</p>
<p>is operationalized as the number of previous convictions.
The fitted SCM in Figure 3b shows that only the defendant's criminal history had a significant effect on the final bail amount with each additional conviction causing an average increase of $\$ 521.53$ in bail $\left(\hat{\beta}^{<em>}=0.16, p=0.012\right)$. It is unclear whether the defendant's remorse affected the final bail amount. The effect size was small but non-trivial $\left(\hat{\beta}^{</em>}=-0.12\right.$, and $\left.p=0.056\right)$.</p>
<p>When we estimated the SCM with interactions, the interaction between the judge's case count and the defendant's remorse was nontrivial $\left(\hat{\beta}^{*}=-0.32, p=\right.$ 0.047). In this specification (Figure A.5), none of the other interactions or the standalone causes have a significant effect, including the defendant's criminal history.</p>
<h1>3.3 Interviewing for a job as a lawyer</h1>
<p>In our third simulated experiment, we chose the scenario "a person interviewing for a job as a lawyer." The system determined that a job applicant and an employer were the agents. Unlike the previous simulations, we manually selected the variables in the SCM. Table 4a shows that these were the employer's hiring decision as the outcome and whether the applicant passed the bar, the interviewer's friendliness, and the job applicant's height as the potential causes.</p>
<p>The system operationalized the causes as a binary variable for passing the bar, the job applicant's height in centimeters, and the interviewer's friendliness as the proposed number of friendly phrases to use during the simulation. Since one of the causes is a binary variable, the only potential cause in all our scenarios of this type, the sample size for the experimental simulations of this scenario is smaller $(n=80)$. By default, the system runs a factorial experimental design for all proposed values of each cause. With only two possible values for the job applicant passing the bar (as opposed to 5 varied treatment values for the interviewer's friendliness and 8 for the applicant's height), this limits the possible combinations of the causal variables to $2 \times 5 \times 8=80$. A researcher could run more simulations to increase the sample size.</p>
<p>We can see in Figure 4b that only the applicant passing the bar has a clear causal</p>
<p>Figure 4: Experimental design and fitted SCM for "a person is interviewing for a job as a lawyer."</p>
<table>
<thead>
<tr>
<th>SIMULATION DETAILS</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Agents: Interviewer, Job Applicant</td>
<td></td>
</tr>
<tr>
<td>Simulations Run: $2 \times 5 \times 8=405$</td>
<td></td>
</tr>
<tr>
<td>Speaking Order: Interviewer, Job Applicant,</td>
<td></td>
</tr>
<tr>
<td>Interviewer, ...repeat</td>
<td></td>
</tr>
<tr>
<td>VARIABLE INFORMATION</td>
<td></td>
</tr>
<tr>
<td>Employer's Decision</td>
<td></td>
</tr>
<tr>
<td>Measurement Question: Employer: "Have you decided to hire the job applicant?"</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Binary</td>
<td></td>
</tr>
<tr>
<td>Whether Applicant Passed Exam</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: ['Passed', 'Not']</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Your bar exam status</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Binary</td>
<td></td>
</tr>
<tr>
<td>Interviewer's level of friendliness</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: ['2', '7', '12', '17', '22']</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Number of positive phrases to use during interview</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Count</td>
<td></td>
</tr>
<tr>
<td>Job applicant's height</td>
<td></td>
</tr>
<tr>
<td>Attribute Treatments: ['160', '165', '170', '175', '180', '185', '190', '195']</td>
<td></td>
</tr>
<tr>
<td>Proxy Attribute: Your height in centimeters</td>
<td></td>
</tr>
<tr>
<td>Variable Type: Continous</td>
<td></td>
</tr>
</tbody>
</table>
<p>(a) Information for experimental design
<img alt="img-3.jpeg" src="img-3.jpeg" />
(b) Fitted SCM</p>
<p>Notes: Figure $4 a$ provides the information automatically generated by the system to execute the experiment for the proposed hypothesis. Figure $4 b$ shows the fitted SCM from the experiment.</p>
<p>effect on whether the applicant gets the job. This is the largest standardized effect we see across the simulations in the four scenarios $\left(\hat{\beta}^{*}=0.78, p&lt;0.001\right)$. On average, whether or not the applicant passes the bar increases the probability she gets the job by 75 percentage points. None are significant when we test for interactions (Figure A.6).</p>
<h1>3.4 An auction for a piece of art</h1>
<p>Finally, we explored the scenario of " 3 bidders participating in an auction for a piece of art starting at fifty dollars." Table 5a shows that the causes are each bidder's maximum budget for the piece of art, and the outcome is the final price of the piece of art-all of which we selected.</p>
<p>All four variables are operationalized in dollars. To maintain symmetry in the simulations, we manually selected the same proxy attribute for the three bidders: "your maximum budget for the piece of art." Each bidder had the same seven possible values for their attribute, leading to $7^{3}=343$ simulations of the auction. These budgets are private values, meaning that unless a bidder publicly reveals their budget, the other bidders are unaware of its value.</p>
<p>Like the tax fraud scenario, the system chose the center-ordered interaction protocol for these simulations. The auctioneer was selected as the central agent, and the other agents were bidder 1, bidder 2, and bidder 3, who alternated with the auctioneer in that order.</p>
<p>Figure 5b provides the results. All three causal variables had a positive and statistically significant effect on the final price. A one-dollar increase in any of the bidder's budgets caused a $\$ 0.35, \$ 0.29$, and $\$ 0.31$ increase in the final price for the piece of art for each respective bidder $\left(\hat{\beta}^{<em>}=0.57, p&lt;0.001 ; \hat{\beta}^{</em>}=0.47, p&lt;0.001\right.$; $\left.\hat{\beta}^{*}=0.5 p&lt;0.001\right)$. These quantities make sense as each bidder has a $\frac{1}{3}$ chance of being marginal.</p>
<p>Figure 5: Experimental design and fitted SCM for " 3 bidders participating in an auction for a piece of art starting at fifty dollars."</p>
<table>
<thead>
<tr>
<th style="text-align: center;">SIMULATION DETAILS</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Agents: Bidder 1, Bidder 2, Bidder 3, Auctioneer</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Simulations Run: $7 \times 7 \times 7=343$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Speaking Order: Auctioneer, Bidder 1, Auctioneer, Bidder 2, Auctioneer, Bidder 3, ... repeat</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">VARIABLE INFORMATION</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Final price</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Measurement Question: Auctioneer: "What was the final bid for the piece of art at the end of the auction?"</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Continuous</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Bidder 1's maximum budget</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: $\left\lceil\$ 50^{\prime},\right.$ ' $\$ 100$ ', '\$150', '\$200', ' $\$ 250$ ', '\$300', '\$350']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Your max budget for the art</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Continuous</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Bidder 2's maximum budget</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: $\left\lceil\$ 50\right.$ ', '\$100', '\$150', '\$200', ' $\$ 250$ ', '\$300', '\$350']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Your max budget for the art</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Continuous</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Bidder 3's maximum budget</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Attribute Treatments: $\left\lceil\$ 50\right.$ ', '\$100', '\$150', '\$200', ' $\$ 250$ ', '\$300', '\$350']</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Proxy Attribute: Your max budget for the art</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Variable Type: Continuous</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>(a) Information for experimental design
<img alt="img-4.jpeg" src="img-4.jpeg" />
(b) Fitted SCM</p>
<p>Notes: Figure 5a provides the information automatically generated by the system to execute the experiment for the proposed hypothesis. Figure $5 b$ shows the fitted SCM from the experiment.</p>
<h1>4 LLM predictions for paths and points</h1>
<p>It is worth reiterating that the results in the previous section were not generated by directly prompting an LLM but rather through experimentation. Although the experiments were fast and inexpensive, they were not free-in total, they took about 5 hours to run and cost over $\$ 1,000$. This raises the question of whether the simulations were even necessary. Could an LLM do a "thought experiment" (i.e., make a prediction based on a prompt) about a proposed in silico experiment and achieve the same insight? If so, we should just prompt the LLM to generate an SCM and elicit its predictions about the relationships between the variables.</p>
<p>To test this idea, we describe some of the simulations to the LLM and ask it to predict the results - path estimates and point predictions. ${ }^{9}$ Specifically, we model each scenario as $y=X \beta$, where $y$ is an $n \times 1$ vector and $X$ is a $n \times k$ matrix. Here, $n$ is the number of simulations, and $k$ is the number of proposed causes. The experiments from Section 3 provided us with estimates for $\hat{\beta}$ (a $k \times 1$ vector). We describe the scenario and the experiment to the LLM and ask it to independently predict $y_{i}$ given each $X_{i}$ (a predict- $y_{i}$ task) as well as to predict $\hat{\beta}$ (a predict- $\hat{\beta}$ task).</p>
<p>The LLM's $y_{i}$ predictions are highly inaccurate compared to those from auction theory, which predicts that the clearing price will be the second highest valuation in an open-ascending price auction with private values (Maskin and Riley, 1985). The LLM is also unable to accurately predict the path estimates $(\hat{\beta})$ of the fitted SCM. Finally, we examine how the LLM does on the predict- $y_{i}$ task when provided with an SCM fit on all of the data except for the corresponding $X_{i}$ (the predict- $y_{i} \mid \hat{\beta}_{-i}$ task). While the additional information dramatically improves the LLM's predictions, they are still less accurate than those made by auction theory.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ All predictions are made by the LLM once at temperature 0 . When we elicit these predictions many times at higher temperatures, the results are similar.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
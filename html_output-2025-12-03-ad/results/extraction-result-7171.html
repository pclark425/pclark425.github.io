<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7171 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7171</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7171</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-34e1a8a75bf6f35084ac6d714a136f39d02c649e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/34e1a8a75bf6f35084ac6d714a136f39d02c649e" target="_blank">Self-Verification Improves Few-Shot Clinical Information Extraction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work explores a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs and consistently improves accuracy for various LLMs in standard clinical information extraction tasks.</p>
                <p><strong>Paper Abstract:</strong> Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7171.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7171.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Clinical trial arm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 evaluated with the paper's self-verification pipeline; used as the primary LLM in chat mode to extract clinical trial arms from abstracts and then verify/extract evidence spans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API in chat mode; large pre-trained transformer (training/parameter details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>A chain-of-LLM-calls pipeline with four steps: (1) Original extraction, (2) Omission (find missing elements), (3) Evidence (ground each element to a text span in the input), and (4) Prune (remove inaccurate elements using the supplied evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify via separate prompts: Original -> Omission -> Evidence -> Prune)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract variable-length lists of clinical trial arms from EBM-NLP abstracts (manually annotated).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.419 ± 0.008 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.530 ± 0.010 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV improves F1 but costs more computation due to multiple LLM calls; sensitive to prompt design. Omission step increases recall at the cost of precision; Prune recovers precision. Exact string-match evaluation can underestimate true performance (acronyms / name variants). For long inputs the Omission step is repeated (see notes) which raises cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7171.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - Medication name/status</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used to extract medication names and classify medication status, with SV providing evidence spans and pruning inaccurate labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API in chat mode; paper does not report parameter count.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step verification pipeline (Original -> Omission -> Evidence -> Prune) where the model re-checks its outputs and grounds each element in a supporting text span before pruning incorrect items.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify via multiple prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract medication names and classify each as active, discontinued, or neither from clinical snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.884 ± 0.003 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.910 ± 0.001 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Omission can introduce false positives (lower precision) while improving recall; Evidence step and Prune help restore precision. SV adds compute cost; pipeline is prompt-sensitive. Evidence spans match human annotations well but evaluation via exact match may undercount partial matches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7171.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - MIMIC-III ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used to extract diagnoses and convert to ICD-9 codes from MIMIC-III notes; SV used to improve extraction and provide evidence spans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API; details not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Original extraction followed by Omission to find missed items, Evidence to ground outputs to spans, and Prune to remove inaccurate items based on evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-III ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses from free-text clinical notes (various sections) and map them to ICD-9 codes (top 50 codes restricted).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.652 ± 0.02 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.678 ± 0.007 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs struggle when ICD codes are not explicitly present and mapping is required; the paper notes high input length and complexity. SV improves recall via Omission (especially on long inputs) but increases cost. Prompt sensitivity and exact-match evaluation limitations apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7171.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - MIMIC-IV ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 applied to discharge summaries from MIMIC-IV to extract diagnoses and map to ICD-9 codes, improved by SV.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (gpt-4-0314) via Azure API; trained transformer model, parameter count not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Pipeline of Original extraction, Omission, Evidence grounding, and Prune to refine outputs and provide interpretable evidence spans.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses from discharge summaries and map to ICD-9 codes (top 50 codes).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.718 ± 0.03 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.755 ± 0.004 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements larger on long inputs where Omission finds missed evidence; SV is compute-intensive; sensitive to prompt wording. Exact matching may understate true retrieval performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7171.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (gpt-4-0314, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 used to extract diagnoses and map to ICD-10 from MIMIC-IV discharge summaries; SV improves F1 modestly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 accessed via Azure OpenAI API; model training/size details not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step self-verification: Original extraction, iterative Omission (when needed), Evidence grounding to spans, and Prune to drop unsupported items.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify; Omission repeated for long inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-10 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses and map them to ICD-10 codes from MIMIC-IV discharge summaries (restricted to top 50 codes).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.487 ± 0.02 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.533 ± 0.002 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Omission step provided much of the improvement on long inputs; SV adds computational cost. Mapping diagnoses to ICD codes remains challenging and can limit base performance. Evaluation via exact string matching may miss semantically correct outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7171.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - Clinical trial arm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo) evaluated with SV for extracting clinical trial arms from abstracts; SV increases recall and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo (ChatGPT series) accessed via Azure OpenAI API; few-shot in-context learning used for short inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Chained prompts: Original extraction, Omission (possibly repeated for long inputs), Evidence span grounding, and Prune to remove unsupported items.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract clinical trial arm names from EBM-NLP abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.342 ± 0.010 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.456 ± 0.007 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV increases computational cost and is sensitive to prompt design. Omission step trades precision for recall; Prune required to restore precision. Exact string-match evaluation limitations apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7171.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - Medication name/status</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on medication name/status extraction; SV yields a small F1 improvement and provides evidence spans for auditing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo (ChatGPT) accessed via Azure OpenAI API; few-shot prompts used for short inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Original extraction plus Omission, Evidence, and Prune steps to surface missed items, ground outputs to spans, and remove inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract medication names and categorize each as active, discontinued, or neither from clinical snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.892 ± 0.004 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.898 ± 0.002 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvement is small; SV increases compute and is prompt-sensitive. Omission step increases recall but can decrease precision until Prune and Evidence steps are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7171.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - MIMIC-III ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT applied to MIMIC-III ICD-9 extraction with SV; yields modest F1 gains compared to original extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo accessed via Azure OpenAI API; not fine-tuned for ICD mapping in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iterative multi-prompt pipeline: Original extraction -> Omission -> Evidence -> Prune; evidence spans used for pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-III ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses and map to ICD-9 codes from MIMIC-III clinical notes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.593 ± 0.003 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.619 ± 0.005 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mapping diagnoses to ICD codes is challenging; GPT-3.5 family (ChatGPT) performs worse than GPT-4 on long inputs. SV adds computation; prompt sensitivity and evaluation granularity caveats apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7171.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - MIMIC-IV ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT used on MIMIC-IV discharge summary ICD-9 extraction; SV moderately improves F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo accessed via Azure OpenAI API; used in few-shot setting for shorter inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step SV pipeline grounding outputs in evidence spans and pruning unsupported items.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses and map to ICD-9 codes from discharge summaries (MIMIC-IV).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.693 ± 0.04 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.713 ± 0.005 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV improves results but is compute-expensive; exact-match evaluation may penalize semantically equivalent outputs. Omission is particularly useful on longer inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e7171.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT - MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo, OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT evaluated on ICD-10 extraction from MIMIC-IV; SV gives small improvement in F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI gpt-3.5-turbo (ChatGPT) via Azure API; not specialized for ICD mapping in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Original extraction with follow-up Omission, Evidence, and Prune steps to find missed items, ground outputs, and remove inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-10 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses and map to ICD-10 codes from MIMIC-IV discharge summaries (restricted to top 50 codes).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.448 ± 0.04 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.464 ± 0.003 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Small F1 improvement; SV raises inference cost. Mapping to ICD-10 remains difficult; prompt sensitivity and exact-match evaluation limitations noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e7171.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003) - Clinical trial arm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 family model (text-davinci-003) evaluated on clinical trial arm extraction; SV provides a notable F1 improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI text-davinci-003 (GPT-3.5 family) accessed via Azure OpenAI API; used in few-shot setting for short inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Pipeline: Original extraction -> Omission (repeated for long inputs if needed) -> Evidence -> Prune; evidence spans used to prune incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Clinical trial arm extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Identify clinical trial arms from EBM-NLP abstracts (variable-length list).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.512 ± 0.009 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.575 ± 0.003 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV is prompt-sensitive and compute-intensive. Omission increases recall but reduces precision prior to pruning. Exact-match metrics can undercount correct but differently formatted outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e7171.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003) - Medication name/status</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 (text-davinci-003) evaluated on medication extraction; SV further improves precision and F1.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI text-davinci-003 accessed via Azure API; used with few-shot demonstrations for short inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Four-step SV pipeline that finds omissions, grounds outputs to spans, and prunes unsupported items to boost precision and maintain recall.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Medication status extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract medication names and label each as active/discontinued/neither from clinical snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.929 ± 0.002 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.935 ± 0.001 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Omission can lower precision if used alone; Prune recovers precision. SV adds inference latency and cost; sensitivity to prompt phrasing noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e7171.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003) - MIMIC-III ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 tested on MIMIC-III ICD-9 extraction; SV yields a small F1 improvement but absolute performance is lower than GPT-4 on long inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI text-davinci-003; not specialized for ICD mapping in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Original extraction then Omission/Evidence/Prune steps to surface and verify elements and provide evidence spans.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-III ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses from clinical notes (MIMIC-III) then map to ICD-9 codes (top 50 codes considered).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.431 ± 0.03 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.435 ± 0.01 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>GPT-3.5 family performs poorly on ICD-code extraction relative to GPT-4, perhaps due to needing to both extract diagnoses and map them to codes. SV helps slightly but is limited by base model capabilities and mapping difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e7171.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003) - MIMIC-IV ICD-9</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 evaluated on ICD-9 extraction from MIMIC-IV discharge summaries; SV produces small improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI text-davinci-003 accessed via Azure API; few-shot used for shorter inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Pipeline: Original extraction, Omission (repeated for long inputs when needed), Evidence grounding to spans, and Prune to remove unsupported items.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-9 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses from discharge summaries and map them to ICD-9 codes (top 50 codes).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.691 ± 0.02 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.702 ± 0.02 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SV improvements are modest; larger benefit seen with stronger base models on longer inputs. SV increases computation and is sensitive to prompt design and evidence-grounding quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7171.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e7171.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 (text-davinci-003) - MIMIC-IV ICD-10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 / text-davinci-003 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3.5 applied to ICD-10 mapping on MIMIC-IV; SV gives a small F1 gain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI text-davinci-003; mapping diagnoses to ICD-10 presented challenges for this model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Verification (SV)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Chain of prompts: initial extraction then omission search, evidence span grounding, and pruning of unsupported outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>chain-of-LLM-calls (generate then verify; Omission repeated up to 5 times for long inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MIMIC-IV ICD-10 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Extract diagnoses and convert to ICD-10 codes from discharge summaries (top 50 codes considered).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Macro F1 (case-insensitive exact string matching)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.434 ± 0.03 F1</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.442 ± 0.01 F1</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Small absolute gains; mapping diagnoses to codes remains a bottleneck. SV increases cost; prompt sensitivity and exact-match evaluation caveats apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Verification Improves Few-Shot Clinical Information Extraction', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Rarr: Researching and revising what language models say, using language models <em>(Rating: 2)</em></li>
                <li>Check your facts and try again: Improving large language models with external knowledge and automated feedback <em>(Rating: 2)</em></li>
                <li>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts <em>(Rating: 2)</em></li>
                <li>Large language model is not a good few-shot information extractor, but a good reranker for hard samples! <em>(Rating: 1)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7171",
    "paper_id": "paper-34e1a8a75bf6f35084ac6d714a136f39d02c649e",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "GPT-4 - Clinical trial arm",
            "name_full": "GPT-4 (gpt-4-0314, OpenAI)",
            "brief_description": "GPT-4 evaluated with the paper's self-verification pipeline; used as the primary LLM in chat mode to extract clinical trial arms from abstracts and then verify/extract evidence spans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API in chat mode; large pre-trained transformer (training/parameter details not provided in this paper).",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "A chain-of-LLM-calls pipeline with four steps: (1) Original extraction, (2) Omission (find missing elements), (3) Evidence (ground each element to a text span in the input), and (4) Prune (remove inaccurate elements using the supplied evidence).",
            "iteration_type": "chain-of-LLM-calls (generate then verify via separate prompts: Original -&gt; Omission -&gt; Evidence -&gt; Prune)",
            "num_iterations": null,
            "task_name": "Clinical trial arm extraction",
            "task_description": "Extract variable-length lists of clinical trial arms from EBM-NLP abstracts (manually annotated).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.419 ± 0.008 F1",
            "performance_after_reflection": "0.530 ± 0.010 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SV improves F1 but costs more computation due to multiple LLM calls; sensitive to prompt design. Omission step increases recall at the cost of precision; Prune recovers precision. Exact string-match evaluation can underestimate true performance (acronyms / name variants). For long inputs the Omission step is repeated (see notes) which raises cost.",
            "uuid": "e7171.0",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - Medication name/status",
            "name_full": "GPT-4 (gpt-4-0314, OpenAI)",
            "brief_description": "GPT-4 used to extract medication names and classify medication status, with SV providing evidence spans and pruning inaccurate labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API in chat mode; paper does not report parameter count.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Four-step verification pipeline (Original -&gt; Omission -&gt; Evidence -&gt; Prune) where the model re-checks its outputs and grounds each element in a supporting text span before pruning incorrect items.",
            "iteration_type": "chain-of-LLM-calls (generate then verify via multiple prompts)",
            "num_iterations": null,
            "task_name": "Medication status extraction",
            "task_description": "Extract medication names and classify each as active, discontinued, or neither from clinical snippets.",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.884 ± 0.003 F1",
            "performance_after_reflection": "0.910 ± 0.001 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Omission can introduce false positives (lower precision) while improving recall; Evidence step and Prune help restore precision. SV adds compute cost; pipeline is prompt-sensitive. Evidence spans match human annotations well but evaluation via exact match may undercount partial matches.",
            "uuid": "e7171.1",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - MIMIC-III ICD-9",
            "name_full": "GPT-4 (gpt-4-0314, OpenAI)",
            "brief_description": "GPT-4 used to extract diagnoses and convert to ICD-9 codes from MIMIC-III notes; SV used to improve extraction and provide evidence spans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (gpt-4-0314) accessed via Azure OpenAI API; details not provided.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Original extraction followed by Omission to find missed items, Evidence to ground outputs to spans, and Prune to remove inaccurate items based on evidence.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-III ICD-9 extraction",
            "task_description": "Extract diagnoses from free-text clinical notes (various sections) and map them to ICD-9 codes (top 50 codes restricted).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.652 ± 0.02 F1",
            "performance_after_reflection": "0.678 ± 0.007 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "LLMs struggle when ICD codes are not explicitly present and mapping is required; the paper notes high input length and complexity. SV improves recall via Omission (especially on long inputs) but increases cost. Prompt sensitivity and exact-match evaluation limitations apply.",
            "uuid": "e7171.2",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - MIMIC-IV ICD-9",
            "name_full": "GPT-4 (gpt-4-0314, OpenAI)",
            "brief_description": "GPT-4 applied to discharge summaries from MIMIC-IV to extract diagnoses and map to ICD-9 codes, improved by SV.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (gpt-4-0314) via Azure API; trained transformer model, parameter count not specified in paper.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Pipeline of Original extraction, Omission, Evidence grounding, and Prune to refine outputs and provide interpretable evidence spans.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-9 extraction",
            "task_description": "Extract diagnoses from discharge summaries and map to ICD-9 codes (top 50 codes).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.718 ± 0.03 F1",
            "performance_after_reflection": "0.755 ± 0.004 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements larger on long inputs where Omission finds missed evidence; SV is compute-intensive; sensitive to prompt wording. Exact matching may understate true retrieval performance.",
            "uuid": "e7171.3",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - MIMIC-IV ICD-10",
            "name_full": "GPT-4 (gpt-4-0314, OpenAI)",
            "brief_description": "GPT-4 used to extract diagnoses and map to ICD-10 from MIMIC-IV discharge summaries; SV improves F1 modestly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 accessed via Azure OpenAI API; model training/size details not provided in paper.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Four-step self-verification: Original extraction, iterative Omission (when needed), Evidence grounding to spans, and Prune to drop unsupported items.",
            "iteration_type": "chain-of-LLM-calls (generate then verify; Omission repeated for long inputs)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-10 extraction",
            "task_description": "Extract diagnoses and map them to ICD-10 codes from MIMIC-IV discharge summaries (restricted to top 50 codes).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.487 ± 0.02 F1",
            "performance_after_reflection": "0.533 ± 0.002 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Omission step provided much of the improvement on long inputs; SV adds computational cost. Mapping diagnoses to ICD codes remains challenging and can limit base performance. Evaluation via exact string matching may miss semantically correct outputs.",
            "uuid": "e7171.4",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - Clinical trial arm",
            "name_full": "ChatGPT (gpt-3.5-turbo, OpenAI)",
            "brief_description": "ChatGPT (gpt-3.5-turbo) evaluated with SV for extracting clinical trial arms from abstracts; SV increases recall and F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo (ChatGPT series) accessed via Azure OpenAI API; few-shot in-context learning used for short inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Chained prompts: Original extraction, Omission (possibly repeated for long inputs), Evidence span grounding, and Prune to remove unsupported items.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "Clinical trial arm extraction",
            "task_description": "Extract clinical trial arm names from EBM-NLP abstracts.",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.342 ± 0.010 F1",
            "performance_after_reflection": "0.456 ± 0.007 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SV increases computational cost and is sensitive to prompt design. Omission step trades precision for recall; Prune required to restore precision. Exact string-match evaluation limitations apply.",
            "uuid": "e7171.5",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - Medication name/status",
            "name_full": "ChatGPT (gpt-3.5-turbo, OpenAI)",
            "brief_description": "ChatGPT evaluated on medication name/status extraction; SV yields a small F1 improvement and provides evidence spans for auditing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo (ChatGPT) accessed via Azure OpenAI API; few-shot prompts used for short inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Original extraction plus Omission, Evidence, and Prune steps to surface missed items, ground outputs to spans, and remove inaccuracies.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "Medication status extraction",
            "task_description": "Extract medication names and categorize each as active, discontinued, or neither from clinical snippets.",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.892 ± 0.004 F1",
            "performance_after_reflection": "0.898 ± 0.002 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvement is small; SV increases compute and is prompt-sensitive. Omission step increases recall but can decrease precision until Prune and Evidence steps are applied.",
            "uuid": "e7171.6",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - MIMIC-III ICD-9",
            "name_full": "ChatGPT (gpt-3.5-turbo, OpenAI)",
            "brief_description": "ChatGPT applied to MIMIC-III ICD-9 extraction with SV; yields modest F1 gains compared to original extraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo accessed via Azure OpenAI API; not fine-tuned for ICD mapping in this work.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Iterative multi-prompt pipeline: Original extraction -&gt; Omission -&gt; Evidence -&gt; Prune; evidence spans used for pruning.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-III ICD-9 extraction",
            "task_description": "Extract diagnoses and map to ICD-9 codes from MIMIC-III clinical notes.",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.593 ± 0.003 F1",
            "performance_after_reflection": "0.619 ± 0.005 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Mapping diagnoses to ICD codes is challenging; GPT-3.5 family (ChatGPT) performs worse than GPT-4 on long inputs. SV adds computation; prompt sensitivity and evaluation granularity caveats apply.",
            "uuid": "e7171.7",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - MIMIC-IV ICD-9",
            "name_full": "ChatGPT (gpt-3.5-turbo, OpenAI)",
            "brief_description": "ChatGPT used on MIMIC-IV discharge summary ICD-9 extraction; SV moderately improves F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo accessed via Azure OpenAI API; used in few-shot setting for shorter inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Four-step SV pipeline grounding outputs in evidence spans and pruning unsupported items.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-9 extraction",
            "task_description": "Extract diagnoses and map to ICD-9 codes from discharge summaries (MIMIC-IV).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.693 ± 0.04 F1",
            "performance_after_reflection": "0.713 ± 0.005 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SV improves results but is compute-expensive; exact-match evaluation may penalize semantically equivalent outputs. Omission is particularly useful on longer inputs.",
            "uuid": "e7171.8",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ChatGPT - MIMIC-IV ICD-10",
            "name_full": "ChatGPT (gpt-3.5-turbo, OpenAI)",
            "brief_description": "ChatGPT evaluated on ICD-10 extraction from MIMIC-IV; SV gives small improvement in F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (gpt-3.5-turbo)",
            "model_description": "OpenAI gpt-3.5-turbo (ChatGPT) via Azure API; not specialized for ICD mapping in this paper.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Original extraction with follow-up Omission, Evidence, and Prune steps to find missed items, ground outputs, and remove inaccuracies.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-10 extraction",
            "task_description": "Extract diagnoses and map to ICD-10 codes from MIMIC-IV discharge summaries (restricted to top 50 codes).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.448 ± 0.04 F1",
            "performance_after_reflection": "0.464 ± 0.003 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Small F1 improvement; SV raises inference cost. Mapping to ICD-10 remains difficult; prompt sensitivity and exact-match evaluation limitations noted.",
            "uuid": "e7171.9",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003) - Clinical trial arm",
            "name_full": "GPT-3.5 / text-davinci-003 (OpenAI)",
            "brief_description": "GPT-3.5 family model (text-davinci-003) evaluated on clinical trial arm extraction; SV provides a notable F1 improvement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI text-davinci-003 (GPT-3.5 family) accessed via Azure OpenAI API; used in few-shot setting for short inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Pipeline: Original extraction -&gt; Omission (repeated for long inputs if needed) -&gt; Evidence -&gt; Prune; evidence spans used to prune incorrect outputs.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "Clinical trial arm extraction",
            "task_description": "Identify clinical trial arms from EBM-NLP abstracts (variable-length list).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.512 ± 0.009 F1",
            "performance_after_reflection": "0.575 ± 0.003 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SV is prompt-sensitive and compute-intensive. Omission increases recall but reduces precision prior to pruning. Exact-match metrics can undercount correct but differently formatted outputs.",
            "uuid": "e7171.10",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003) - Medication name/status",
            "name_full": "GPT-3.5 / text-davinci-003 (OpenAI)",
            "brief_description": "GPT-3.5 (text-davinci-003) evaluated on medication extraction; SV further improves precision and F1.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI text-davinci-003 accessed via Azure API; used with few-shot demonstrations for short inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Four-step SV pipeline that finds omissions, grounds outputs to spans, and prunes unsupported items to boost precision and maintain recall.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "Medication status extraction",
            "task_description": "Extract medication names and label each as active/discontinued/neither from clinical snippets.",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.929 ± 0.002 F1",
            "performance_after_reflection": "0.935 ± 0.001 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Omission can lower precision if used alone; Prune recovers precision. SV adds inference latency and cost; sensitivity to prompt phrasing noted.",
            "uuid": "e7171.11",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003) - MIMIC-III ICD-9",
            "name_full": "GPT-3.5 / text-davinci-003 (OpenAI)",
            "brief_description": "GPT-3.5 tested on MIMIC-III ICD-9 extraction; SV yields a small F1 improvement but absolute performance is lower than GPT-4 on long inputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI text-davinci-003; not specialized for ICD mapping in this study.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Original extraction then Omission/Evidence/Prune steps to surface and verify elements and provide evidence spans.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-III ICD-9 extraction",
            "task_description": "Extract diagnoses from clinical notes (MIMIC-III) then map to ICD-9 codes (top 50 codes considered).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.431 ± 0.03 F1",
            "performance_after_reflection": "0.435 ± 0.01 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "GPT-3.5 family performs poorly on ICD-code extraction relative to GPT-4, perhaps due to needing to both extract diagnoses and map them to codes. SV helps slightly but is limited by base model capabilities and mapping difficulty.",
            "uuid": "e7171.12",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003) - MIMIC-IV ICD-9",
            "name_full": "GPT-3.5 / text-davinci-003 (OpenAI)",
            "brief_description": "GPT-3.5 evaluated on ICD-9 extraction from MIMIC-IV discharge summaries; SV produces small improvements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI text-davinci-003 accessed via Azure API; few-shot used for shorter inputs.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Pipeline: Original extraction, Omission (repeated for long inputs when needed), Evidence grounding to spans, and Prune to remove unsupported items.",
            "iteration_type": "chain-of-LLM-calls (generate then verify)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-9 extraction",
            "task_description": "Extract diagnoses from discharge summaries and map them to ICD-9 codes (top 50 codes).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.691 ± 0.02 F1",
            "performance_after_reflection": "0.702 ± 0.02 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "SV improvements are modest; larger benefit seen with stronger base models on longer inputs. SV increases computation and is sensitive to prompt design and evidence-grounding quality.",
            "uuid": "e7171.13",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5 (text-davinci-003) - MIMIC-IV ICD-10",
            "name_full": "GPT-3.5 / text-davinci-003 (OpenAI)",
            "brief_description": "GPT-3.5 applied to ICD-10 mapping on MIMIC-IV; SV gives a small F1 gain.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (text-davinci-003)",
            "model_description": "OpenAI text-davinci-003; mapping diagnoses to ICD-10 presented challenges for this model.",
            "model_size": null,
            "reflection_method_name": "Self-Verification (SV)",
            "reflection_method_description": "Chain of prompts: initial extraction then omission search, evidence span grounding, and pruning of unsupported outputs.",
            "iteration_type": "chain-of-LLM-calls (generate then verify; Omission repeated up to 5 times for long inputs)",
            "num_iterations": null,
            "task_name": "MIMIC-IV ICD-10 extraction",
            "task_description": "Extract diagnoses and convert to ICD-10 codes from discharge summaries (top 50 codes considered).",
            "evaluation_metric": "Macro F1 (case-insensitive exact string matching)",
            "performance_before_reflection": "0.434 ± 0.03 F1",
            "performance_after_reflection": "0.442 ± 0.01 F1",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Small absolute gains; mapping diagnoses to codes remains a bottleneck. SV increases cost; prompt sensitivity and exact-match evaluation caveats apply.",
            "uuid": "e7171.14",
            "source_info": {
                "paper_title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Rarr: Researching and revising what language models say, using language models",
            "rating": 2
        },
        {
            "paper_title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback",
            "rating": 2
        },
        {
            "paper_title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts",
            "rating": 2
        },
        {
            "paper_title": "Large language model is not a good few-shot information extractor, but a good reranker for hard samples!",
            "rating": 1
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 1
        }
    ],
    "cost": 0.019513,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Self-Verification Improves Few-Shot Clinical Information Extraction</h1>
<p>Zelalem Gero<em> ${ }^{1}$ Chandan Singh ${ }^{</em> 1}$ Hao Cheng ${ }^{1}$ Tristan Naumann ${ }^{1}$<br>Michel Galley ${ }^{1}$ Jianfeng Gao ${ }^{1}$ Hoifung Poon ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning, which requires costly human annotations. However, despite drastic advances, modern LLMs such as GPT-4 still struggle with issues regarding accuracy and interpretability, especially in safety-critical domains such as health. We explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This framework is made possible by the asymmetry between verification and generation, where the former is often much easier than the latter. Experimental results show that our method consistently improves accuracy for various LLMs across standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts. ${ }^{1}$</p>
<h2>1. Introduction and related work</h2>
<p>Clinical information extraction plays a pivotal role in the analysis of medical records and enables healthcare practitioners to efficiently access and utilize patient data (Zweigenbaum et al., 2007; Wang et al., 2018). Few-shot learning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>approaches have emerged as a promising solution to tackle the scarcity of labeled training data in clinical information extraction tasks (Agrawal et al., 2022; Laursen et al., 2023). However, these methods continue to struggle with accuracy and interpretability, both critical concerns in the medical domain (Gutiérrez et al., 2022).</p>
<p>Here, we address these issues by using self-verification (SV) to improve few-shot clinical information extraction. SV builds on recent works that chain together large language model (LLM) calls to improve an LLM's performance (Wu et al., 2022; Wang et al., 2022; Chase, 2023). Intuitively, these chains succeed because an LLM may be able to perform individual steps in a task, e.g. evidence verification, more accurately than the LLM can perform an entire task, e.g. information extraction (Ma et al., 2023; Madaan et al., 2023; Zhang et al., 2023). Such chains have been successful in settings such as multi-hop question answering (Press et al., 2022), retrieval-augmented/tool-augmented question answering (Peng et al., 2023; Paranjape et al., 2023; Schick et al., 2023; Gao et al., 2023), and code execution (Jojic et al., 2023). Here, we analyze whether building such a chain can improve clinical information extraction.</p>
<p>Fig. 1 shows the SV pipeline we build here. We broadly define self-verification as using multiple calls to the same LLM to verify its output, and also to ground each element of its output in evidence. Our SV pipeline consists of four steps, each of which calls the same LLM with different prompts. First, the Original extraction step queries the LLM directly for the desired information. Next, the Omission step finds missing elements in the output, the Evidence step grounds each element in the output to a text span in the input, and the Prune step removes inaccurate elements in the output. Taken together, we demonstrate that these steps improve the reliability of extracted information.</p>
<p>Additionally, SV provides interpretable grounding for each output, in the form of a short text span in the input. Interpretability has taken many forms in NLP, including posthoc feature importance (Lundberg \&amp; Lee, 2017; Ribeiro et al., 2016), intrinsically interpretable models (Rudin, 2019; Singh et al., 2022a), and visualizing model intermediates, e.g. attention (Wiegreffe \&amp; Pinter, 2019). The interpretable grounding we generate comes directly from an LLM, similar</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Overview of self-verification pipeline for clinical information extraction. Each step calls the same LLM with different prompts to refine the information from the previous steps. Below each step we show abbreviated outputs for extracting a list of assigned diagnoses from a sample clinical note.
to recent works that use LLMs to generate explanations (Rajani et al., 2019; MacNeil et al., 2022; Singh et al., 2023) and ground those explanations in evidence (Rashkin et al., 2021; Gao et al., 2022).</p>
<p>Experiments on various clinical information extraction tasks and various LLMs, including GPT-4 (OpenAI, 2023) and ChatGPT (Ouyang et al., 2022), show the efficacy of SV. In addition to improving accuracy, we find that the extracted interpretations match human judgements of relevant information, enabling auditing by a human and helping to build a path towards trustworthy extraction of clinical information in resource-constrained scenarios.</p>
<h2>2. Methods and experimental setup</h2>
<h3>2.1. Methods: Self-verification</h3>
<p>Fig. 1 shows the four different steps of the introduced SV pipeline. The pipeline takes in a raw text input, e.g. a clinical note, and outputs information in a pre-specified format, e.g. a bulleted list. It consists of four steps, each of which calls the same LLM with different prompts in order to refine and ground the original output.</p>
<p>The original extraction step uses a task-specific prompt which instructs the model to output a variable-length bulleted list. In the toy example in Fig. 1, the goal is to identify the two diagnoses Hypertension and Right adrenal mass, but the original extraction step finds only Hypertension.</p>
<p>After the original LLM extraction, the Omission step finds missing elements in the output; in the Fig. 1 example it finds Right adrenal mass and Liver fibrosis. For tasks with long inputs (mean input length greater than 2,000 characters), we repeat the omission step to find more potential missed elements (we repeat five times, and continue repeating until the omission step stops finding new omissions).</p>
<p>Next, the Evidence step grounds each element in the output to a text span in the input. The grounding in this step provides interpretations that can be inspected by a human. In the Fig. 1 example, we find quotes supporting the first two diagnoses, but the quote for liver fibrosis shows that it was in fact ruled out, and is therefore an incorrect diagnosis. Finally, the Prune step uses the supplied evidence to remove inaccurate elements from the output. In Fig. 1 this results in removing liver fibrosis to return the correct final list. Taken together, these steps help to extract accurate and interpretable information.</p>
<p>We provide the exact prompts used in all steps in the Github repo. For the tasks with short inputs, we include 5 random data demonstrations in the original extraction prompt; otherwise all prompts are fixed across examples.</p>
<h3>2.2. Experimental setup</h3>
<p>Datasets Table 1 gives the details of each task we study here. Each task requires extracting a variable-length list of elements. In clinical trial arm extraction, these are names of different clinical trial arms, manually annotated from the EBM-NLP dataset (Nye et al., 2018). In the medication status extraction task, in addition to medication names the medication status must additionally be classified as active, discontinued, or neither. The text inputs for arm extraction / medication status extraction are relatively small (average length is 1,620 characters and 382 characters, respectively).</p>
<p>In the case of MIMIC-III and MIMIC-IV (Johnson et al., 2016; 2021), we predict ICD-9 or ICD-10 codes (corresponding to diagnoses and procedures). We predict ICD codes using relevant sections from all types of clinical notes for MIMIC-III (average length: 5,200 words) but only discharge summaries for MIMIC-IV (average length: 1,400 words). The ICD codes are not directly present in the text in-</p>
<p>put, and therefore the task requires translating the diagnoses to their relevant code. MIMIC data is preprocessed using a standard pipeline (see Appendix A.1) and we evaluate on a random subset of 250 inputs for each task.</p>
<p>Models We evaluate three different models: GPT3.5 (Brown et al., 2020), text-davinci-003, ChatGPT (Ouyang et al., 2022) gpt-3.5-turbo, and GPT4 (OpenAI, 2023) gpt 4-0314 (in chat mode), all accessed securely through the Azure OpenAI API. We set the sampling temperature for LLM decoding to 0.1 .</p>
<p>Evaluation Extraction is evaluated via case-insensitive exact string matching, and we report the resulting macro F1 scores, recall, and precision. In some cases, this evaluation may underestimate actual performance as a result of the presence of acronyms or different names within the output; nevertheless, the relative performance of different models/methods should still be preserved. Following common practice, we restrict ICD code evaluation to the top 50 codes appearing in the dataset.</p>
<h2>3. Results</h2>
<h3>3.1. Self-verification improves prediction performance</h3>
<p>Table 2 shows the results for clinical extraction performance with and without self-verification. Across different models and tasks, SV consistently provides a performance improvement. The performance improvement is occasionally quite large (e.g. GPT-4 shows more than a 0.1 improvement in F1 for clinical trial arm extraction and more than a 0.3 improvement for medication status extraction), and the average F1 improvement across models and tasks is 0.056 . We also compare to a baseline where we concatenate the prompts across different steps into a single large prompt which is then used to make a single LLM call for information extraction. We find that this large-prompt baseline performs slightly worse than the baseline reported in Table 2, which uses a straightforward prompt for extraction (see comparison details in Table A5).</p>
<p>For tasks with short inputs, we find that GPT-3.5 performs best, even outperforming GPT-4, as has been seen in some recent works (e.g. Patil et al. 2023). For the MIMIC tasks with larger inputs, GPT-4 performs best. In fact, GPT-3.5 performs very poorly on ICD-code extraction, perhaps because the task requires not only extracting diagnoses from the input text but also knowing the mapping between diagnoses and ICD codes.</p>
<p>Table 3 contains ablations showing how the different selfverification modules affect the results. The Omission step finds missing elements, which increases recall but at the cost of decreased precision. In contrast, the Prune step</p>
<h2>Medication status output</h2>
<ul>
<li>aspirin: discontinued</li>
<li>ibuprofen: neither</li>
<li>Naprosyn: neither</li>
<li>Tylenol: active</li>
<li>Plavix: active</li>
</ul>
<h2>Evidence highlighting</h2>
<p>Her aspirin ( 81 mg q.d.) is discontinued, and the patient is advised that she needs to avoid ibuprofen, Naprosyn, alcohol, caffeine, and chocolate. She is advised that Tylenol 325 mg or Tylenol ES (500 mg) is safe to take at 1 or $2 \mathrm{q} .4-6 \mathrm{~h}$. p.r.n. for pain or fever. Discharge activity is without restriction. DISCHARGE MEDICATIONS: 1. Plavix 75 mg p.o. q.d.</p>
<p>Figure 2. Example output and interpretation for medication status. For each element of the output list, our pipeline outputs the text span which contains evidence for that generated output (shown with highlighting).
(that incorporates the span from the Evidence step) removes extraneous elements, thereby increasing precision. Together (Full SV), the steps achieve a balance which improves F1. For tasks with longer inputs (e.g. MIMIC-IV ICD-10), the Omission step seems to provide more of the improvement in F1, likely because it is able to find evidence that was missed by a single extraction step.</p>
<h3>3.2. Self-verification yields interpretations</h3>
<p>Fig. 2 shows an example output from the self-verification pipeline for medication status (the underlying model is GPT4). In the example, the pipeline correctly identifies each medication and its corresponding status. In addition, the pipeline supplies the span of text which serves as evidence for each returned medication (shown with highlighting). This highlighting enables efficient auditing by a human for each element. In a human-in-the-loop setting, a human could also see results/highlights for elements which were pruned, to quickly check for any mistakes.</p>
<p>Table 4 evaluates the evidence spans provided by SV against human judgements collected in a prior work (Nye et al., 2018). Human reviewers annotated spans in the original text which correspond to interventions, which include clinical trial arms as a subset. Table 4 gives the fraction of generated evidence spans that overlap with a span provided by the human annotators. The fraction is quite large, e.g. $93 \%$ for GPT-4. At baseline, human annotators identify less than $3.7 \%$ of tokens as interventions, so these span overlap accuracies are much higher than expected by random chance.</p>
<p>Table 1. Tasks and associated datasets studied here.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task</th>
<th style="text-align: left;">Data</th>
<th style="text-align: left;">Example output</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ICD code extraction <br> (ICD-9 and ICD-10)</td>
<td style="text-align: left;">250 MIMIC III reports (Johnson et al., 2016), <br> 250 MIMIC IV discharge summaries (Johnson et al., 2021)</td>
<td style="text-align: left;">$[205.0,724.1,96.04]$</td>
</tr>
<tr>
<td style="text-align: left;">Clinical trial arm extraction</td>
<td style="text-align: left;">100 annotations to EBM-NLP abstracts (Nye et al., 2018)</td>
<td style="text-align: left;">{propofol, droperidol, placebo}</td>
</tr>
<tr>
<td style="text-align: left;">Medication status extraction</td>
<td style="text-align: left;">105 Annotations (Agrawal et al., 2022) to snippets from <br> CASI (Moon et al., 2012)</td>
<td style="text-align: left;">{aspirin: discontinued, plavix: active}</td>
</tr>
</tbody>
</table>
<p>Table 2. F1 scores for extraction with and without self-verification (SV). Across different models and tasks, SV consistently provides a performance improvement, although it is sometimes small. Bolding shows SV compared to original, underline shows best model for each task. Averaged over 3 random seeds; error bars show the standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical trial arm (Original / SV)</td>
<td style="text-align: left;">$0.342 \pm 0.010 / \mathbf{0 . 4 5 6} \pm \mathbf{0 . 0 0 7}$</td>
<td style="text-align: left;">$0.419 \pm 0.008 / \mathbf{0 . 5 3 0} \pm \mathbf{0 . 0 1 0}$</td>
<td style="text-align: left;">$0.512 \pm 0.009 / \mathbf{0 . 5 7 5} \pm \mathbf{0 . 0 0 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Medication name (Original / SV)</td>
<td style="text-align: left;">$0.892 \pm 0.004 / \mathbf{0 . 8 9 8} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.884 \pm 0.003 / \mathbf{0 . 9 1 0} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: left;">$0.929 \pm 0.002 / \mathbf{0 . 9 3 5} \pm \mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-III ICD-9 (Original / SV)</td>
<td style="text-align: left;">$0.593 \pm 0.003 / \mathbf{0 . 6 1 9} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.652 \pm 0.02 / \mathbf{0 . 6 7 8} \pm \mathbf{0 . 0 0 7}$</td>
<td style="text-align: left;">$0.431 \pm 0.03 / \mathbf{0 . 4 3 5} \pm \mathbf{0 . 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-IV ICD-9 (Original / SV)</td>
<td style="text-align: left;">$0.693 \pm 0.04 / \mathbf{0 . 7 1 3} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.718 \pm 0.03 / \mathbf{0 . 7 5 5} \pm \mathbf{0 . 0 0 4}$</td>
<td style="text-align: left;">$0.691 \pm 0.02 / \mathbf{0 . 7 0 2} \pm \mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td style="text-align: left;">MIMIC-IV ICD-10 (Original / SV)</td>
<td style="text-align: left;">$0.448 \pm 0.04 / \mathbf{0 . 4 6 4} \pm \mathbf{0 . 0 0 3}$</td>
<td style="text-align: left;">$0.487 \pm 0.02 / \mathbf{0 . 5 3 3} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.434 \pm 0.03 / \mathbf{0 . 4 4 2} \pm \mathbf{0 . 0 1}$</td>
</tr>
</tbody>
</table>
<p>Table 3. Ablation results when using different combinations of self-verification steps for two tasks. Omission increases Recall and Prune increases Precision. Together they increase both, improving F1. Evidence improves F1 for Medication Status. Underlying model is the best model for each task (GPT-3.5 for Medication name and GPT-4 for MIMIC-IV ICD-10). Averaged over 3 random seeds; error bars are standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Medication name</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">Recall</td>
</tr>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">$0.929 \pm 0.002$</td>
<td style="text-align: left;">$0.929 \pm 0.003$</td>
<td style="text-align: left;">$0.928 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Omission</td>
<td style="text-align: center;">$0.913 \pm 0.001$</td>
<td style="text-align: left;">$0.881 \pm 0.003$</td>
<td style="text-align: left;">$\mathbf{0 . 9 4 6} \pm \mathbf{0 . 0 0 1}$</td>
</tr>
<tr>
<td style="text-align: left;">+ Prune</td>
<td style="text-align: center;">$0.932 \pm 0.002$</td>
<td style="text-align: left;">$\mathbf{0 . 9 4 9} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.916 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Full SV</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 5} \pm \mathbf{0 . 0 0 1}$</td>
<td style="text-align: left;">$0.942 \pm 0.002$</td>
<td style="text-align: left;">$0.928 \pm 0.001$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">MIMIC-IV ICD-10</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: left;">Precision</td>
<td style="text-align: left;">Recall</td>
</tr>
<tr>
<td style="text-align: left;">Original</td>
<td style="text-align: center;">$0.487 \pm 0.002$</td>
<td style="text-align: left;">$0.544 \pm 0.003$</td>
<td style="text-align: left;">$0.448 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">+ Omission</td>
<td style="text-align: center;">$0.517 \pm 0.003$</td>
<td style="text-align: left;">$0.553 \pm 0.003$</td>
<td style="text-align: left;">$\mathbf{0 . 5 0 1} \pm \mathbf{0 . 0 0 4}$</td>
</tr>
<tr>
<td style="text-align: left;">+ Prune</td>
<td style="text-align: center;">$0.504 \pm 0.004$</td>
<td style="text-align: left;">$\mathbf{0 . 5 5 7} \pm \mathbf{0 . 0 0 5}$</td>
<td style="text-align: left;">$0.451 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">+ Full SV</td>
<td style="text-align: center;">$\mathbf{0 . 5 3 3} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$\mathbf{0 . 5 5 8} \pm \mathbf{0 . 0 0 2}$</td>
<td style="text-align: left;">$0.498 \pm 0.002$</td>
</tr>
</tbody>
</table>
<h2>4. Discussion</h2>
<p>Self-verification constitutes an important step towards unlocking the potential of LLMs in healthcare settings. As LLMs continue to generally improve in performance, clinical extraction with LLMs +SV seems likely to improve as well.</p>
<p>One limitation of SV is that it incurs a high computational cost as multiple LLM calls are chained together; however, these costs may continue to decrease as models become</p>
<p>Table 4. Evaluating evidence spans provided by the selfverification pipeline with human-annotated spans. Averaged over 3 random seeds; error bars show standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Span overlap accuracy</th>
<th style="text-align: left;">Span length</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: left;">$0.93 \pm 0.02$</td>
<td style="text-align: left;">$8.20 \pm 0.48$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: left;">$0.84 \pm 0.03$</td>
<td style="text-align: left;">$7.33 \pm 0.47$</td>
</tr>
</tbody>
</table>
<p>more efficient (Dao et al., 2022). Another limitation is that LLMs and SV continue to be sensitive to prompts, increasing the need for methods to make LLMs more amenable to prompting (Ouyang et al., 2022; Scheurer et al., 2023) and to make finding strong prompts easier (Shin et al., 2020; Xu et al., 2023; Singh et al., 2022b).</p>
<p>Finally, SV can be harnessed in a variety of ways to improve clinical NLP beyond what is studied here, e.g. for studying clinical decision rules (Kornblith et al., 2022), clinical decision support systems (Liu et al., 2023), or improving model distillation (Wu et al., 2023; Toma et al., 2023).</p>
<h2>References</h2>
<p>Agrawal, M., Hegselmann, S., Lang, H., Kim, Y., and Sontag, D. Large language models are few-shot clinical information extractors. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1998-2022, 2022.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Chase, H. Langchain: Building applications with llms through composability. https://github.com/hwchase17/ langchain, 12023.</p>
<p>Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022.</p>
<p>Edin, J., Junge, A., Havtorn, J. D., Borgholt, L., Maistro, M., Ruotsalo, T., and Maaløe, L. Automated medical coding on mimic-iii and mimic-iv: A critical review and replicability study. arXiv preprint arXiv:2304.10909, 2023.</p>
<p>Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan, Y., Zhao, V. Y., Lao, N., Lee, H., Juan, D.-C., et al. Rarr: Researching and revising what language models say, using language models. arXiv preprint arXiv:2210.08726, 2022.</p>
<p>Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and Neubig, G. Pal: Program-aided language models, 2023.</p>
<p>Gutiérrez, B. J., McNeal, N., Washington, C., Chen, Y., Li, L., Sun, H., and Su, Y. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:2203.08410, 2022.</p>
<p>Johnson, A., Bulgarelli, L., Pollard, T., Celi, L. A., Mark, R., and Horng IV, S. Mimic-iv-ed. PhysioNet, 2021.</p>
<p>Johnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi, L., and Mark, R. G. Mimic-iii, a freely accessible critical care database. Scientific data, 3(1):1-9, 2016.</p>
<p>Jojic, A., Wang, Z., and Jojic, N. Gpt is becoming a turing machine: Here are some ways to program it, 2023.</p>
<p>Kornblith, A. E., Singh, C., Devlin, G., Addo, N., Streck, C. J., Holmes, J. F., Kuppermann, N., Grupp-Phelan, J., Fineman, J., Butte, A. J., et al. Predictability and stability testing to assess clinical decision instrument performance for children after blunt torso trauma. PLOS Digital Health, 1(8):e0000076, 2022.</p>
<p>Laursen, M., Pedersen, J., Hansen, R., Savarimuthu, T. R., and Vinholt, P. Danish clinical named entity recognition and relation extraction. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pp. 655-666, 2023.</p>
<p>Liu, S., Wright, A. P., Patterson, B. L., Wanderer, J. P., Turer, R. W., Nelson, S. D., McCoy, A. B., Sittig, D. F., and Wright, A. Assessing the value of chatgpt for clinical decision support optimization. MedRxiv, pp. 2023-02, 2023.</p>
<p>Lundberg, S. M. and Lee, S.-I. A unified approach to interpreting model predictions. Advances in neural information processing systems, 30, 2017.</p>
<p>Ma, Y., Cao, Y., Hong, Y., and Sun, A. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559, 2023.</p>
<p>MacNeil, S., Tran, A., Mogil, D., Bernstein, S., Ross, E., and Huang, Z. Generating diverse code explanations using the gpt-3 large language model. In Proceedings of the 2022 ACM Conference on International Computing Education ResearchVolume 2, pp. 37-39, 2022.</p>
<p>Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Moon, S., Pakhomov, S., and Melton, G. Clinical abbreviation sense inventory. 2012.</p>
<p>Nye, B., Li, J. J., Patel, R., Yang, Y., Marshall, I. J., Nenkova, A., and Wallace, B. C. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the conference. Association for Computational Linguistics. Meeting, volume 2018, pp. 197. NIH Public Access, 2018.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.</p>
<p>Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettlemoyer, L., and Ribeiro, M. T. Art: Automatic multi-step reasoning and tool-use for large language models, 2023.</p>
<p>Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023.</p>
<p>Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L., Yu, Z., Chen, W., et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.</p>
<p>Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A., and Lewis, M. Measuring and narrowing the compositionality gap in language models, 2022.</p>
<p>Rajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.</p>
<p>Rashkin, H., Nikolaev, V., Lamm, M., Aroyo, L., Collins, M., Das, D., Petrov, S., Tomar, G. S., Turc, I., and Reitter, D. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870, 2021.</p>
<p>Ribeiro, M. T., Singh, S., and Guestrin, C. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135-1144, 2016.</p>
<p>Rudin, C. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5):206-215, 2019.</p>
<p>Scheurer, J., Campos, J. A., Korbak, T., Chan, J. S., Chen, A., Cho, K., and Perez, E. Training language models with language feedback at scale. arXiv preprint arXiv:2303.16755, 2023.</p>
<p>Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools, 2023.</p>
<p>Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and Singh, S. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.</p>
<p>Singh, C., Askari, A., Caruana, R., and Gao, J. Augmenting interpretable models with llms during training. arXiv preprint arXiv:2209.11799, 2022a.</p>
<p>Singh, C., Morris, J. X., Aneja, J., Rush, A. M., and Gao, J. Explaining patterns in data with language models via interpretable autoprompting. arXiv preprint arXiv:2210.01848, 2022b.</p>
<p>Singh, C., Hsu, A. R., Antonello, R., Jain, S., Huth, A. G., Yu, B., and Gao, J. Explaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863, 2023.</p>
<p>Toma, A., Lawler, P. R., Ba, J., Krishnan, R. G., Rubin, B. B., and Wang, B. Clinical camel: An open-source expert-level medical language model with dialogue-based knowledge encoding. arXiv preprint arXiv:2305.12031, 2023.</p>
<p>Wang, B., Deng, X., and Sun, H. Iteratively prompt pre-trained language models for chain of thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2714-2730, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022. emnlp-main. 174 .</p>
<p>Wang, Y., Wang, L., Rastegar-Mojarad, M., Moon, S., Shen, F., Afzal, N., Liu, S., Zeng, Y., Mehrabi, S., Sohn, S., et al. Clinical information extraction applications: a literature review. Journal of biomedical informatics, 77:34-49, 2018.</p>
<p>Wiegreffe, S. and Pinter, Y. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019.</p>
<p>Wu, C., Zhang, X., Zhang, Y., Wang, Y., and Xie, W. Pmc-llama: Further finetuning llama on medical papers. arXiv preprint arXiv:2304.14454, 2023.</p>
<p>Wu, T., Terry, M., and Cai, C. J. Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. In CHI Conference on Human Factors in Computing Systems, pp. 1-22, 2022.</p>
<p>Xu, B., Wang, Q., Mao, Z., Lyu, Y., She, Q., and Zhang, Y. $k$ nn prompting: Beyond-context learning with calibration-free nearest neighbor inference. arXiv preprint arXiv:2303.13824, 2023.</p>
<p>Zhang, H., Liu, X., and Zhang, J. Summit: Iterative text summarization via chatgpt. arXiv preprint arXiv:2305.14835, 2023.</p>
<p>Zweigenbaum, P., Demner-Fushman, D., Yu, H., and Cohen, K. B. Frontiers of biomedical text mining: current progress. Briefings in bioinformatics, 8(5):358-375, 2007.</p>
<h1>A. Appendix</h1>
<h2>A.1. Dataset details</h2>
<p>MIMIC To preprocess MIMIC data, we follow the steps used by (Edin et al., 2023). For MIMIC IV, we use the available discharge summaries for each patient while we retrieve more relevant sections from other types of clinical notes for MIMIC III. See the code on Github for complete details.</p>
<p>During LLM extraction, we find that directly extracting ICD codes with an LLM is difficult. Instead, we use the LLM to extract diagnoses, and then postprocess them at the end by asking the LLM to convert each diagnosis to its corresponding ICD code.</p>
<p>Clinical trial arm dataset We manually annotate the clinical trial arms from the first 100 abstract in EBM-NLP (Nye et al., 2018) without the use of any LLMs. All annotations are made available on Github. The mean number of extracted clinical trial arms is 2.14 , the maximum is 5 and the minimum is 1 .</p>
<h2>A.2. Extended extraction results</h2>
<p>Table A5. F1 scores for two tasks extracted using a single prompt which concatenates all steps in the SV pipeline. Results are slightly worse than the original extraction presented in Table 2. The prompt contains a paragraph similar to the following: Before you provide your final response: $\backslash n(1)$ Find any medications in the patient note that were missed. $\backslash n(2)$ Find evidence for each medication as a text span in the input. $\backslash n n(3)$ Verify whether each extracted medication is actually a medication and that its status is correct. Averaged over 3 random seeds; error bars are standard error of the mean.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">ChatGPT</th>
<th style="text-align: left;">GPT-4</th>
<th style="text-align: left;">GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Clinical trial arm, original</td>
<td style="text-align: left;">$0.316 \pm 0.006$</td>
<td style="text-align: left;">$0.420 \pm 0.009$</td>
<td style="text-align: left;">$0.436 \pm 0.008$</td>
</tr>
<tr>
<td style="text-align: left;">Medication name, original</td>
<td style="text-align: left;">$0.758 \pm 0.003$</td>
<td style="text-align: left;">$0.850 \pm 0.016$</td>
<td style="text-align: left;">$0.913 \pm 0.002$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ All code is made available at $\bigcirc$ github.com/microsoft/clinical-self-verification.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
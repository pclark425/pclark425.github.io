<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-480 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-480</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-480</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-266725471</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.01040v1.pdf" target="_blank">Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI</a></p>
                <p><strong>Paper Abstract:</strong> The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, have significantly impacted various aspects of our lives. However, the current challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability call for the development of next-generation AI systems. Neuro-symbolic AI (NSAI) emerges as a promising paradigm, fusing neural, symbolic, and probabilistic approaches to enhance interpretability, robustness, and trustworthiness while facilitating learning from much less data. Recent NSAI systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities. In this paper, we provide a systematic review of recent progress in NSAI and analyze the performance characteristics and computational operators of NSAI models. Furthermore, we discuss the challenges and potential future directions of NSAI from both system and architectural perspectives.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e480.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e480.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NVSA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Vector-Symbolic Architecture</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline neuro-symbolic system that uses a neural perception frontend to produce vector-symbolic representations and a symbolic/probabilistic reasoner backend to perform abductive reasoning for abstract visual-relational tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A neuro-vector-symbolic architecture for solving raven's progressive matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Vector-Symbolic Architecture (NVSA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NVSA is a modular pipeline: a neural perception frontend (e.g., ResNet18) extracts features and semantic parses which are converted to vector-symbolic or discrete percepts; a symbolic/probabilistic reasoning backend performs rule detection and probabilistic abductive reasoning to solve relational and analogical problems (specifically Raven's Progressive Matrices). The symbolic stage is run after the neural frontend and lies on the inference critical path.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic/probabilistic reasoning module that performs rule detection and probabilistic abductive reasoning; operates on vector-symbolic or symbolic representations and explicit rules/queries.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception frontend (ResNet18 in the profiled instantiation) providing feature extraction, semantic parsing and vector embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular pipeline (Neuro|Symbolic): the neural frontend produces inputs for the symbolic backend; the backend executes sequential rule detection and probabilistic abductive inference on those inputs. Integration is asynchronous/modular rather than tightly differentiable end-to-end in the profiled system.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines robust perceptual generalization from deep networks with symbolic abductive reasoning to solve abstract relational puzzles; enables disentangling perception and reasoning, improving interpretability of final decisions via symbolic tracebacks and enabling complex analogical inference not achievable by perception alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Raven's Progressive Matrices (visual abstract reasoning / relational analogical reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Paper reports NVSA scales poorly in runtime with puzzle size (total runtime increases superlinearly) and claims the architecture leverages perception+reasoning to handle abstract relational generalization; explicit numeric OOD/generalization metrics are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Symbolic backend provides more interpretable reasoning traces (rule detection and abductive explanations) compared to pure neural methods; perception component remains a black-box feature extractor but outputs are symbolicized for downstream explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic reasoning dominates runtime (e.g., ~92% of NVSA runtime in some settings), suffers from sequential, computationally intensive rule detection on the critical path, and faces scalability bottlenecks as problem size increases; vector/elementwise symbolic ops have low arithmetic intensity and are inefficient on GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Motivated by complementary strengths (neural perception + symbolic reasoning) and modular division of labor; no unified formal theory provided in this paper beyond taxonomic framing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e480.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic model that encodes symbolic first-order or propositional logic rules as constraints within a neural architecture to perform reasoning tasks while retaining differentiability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logical Neural Networks (LNN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LNNs embed symbolic logic (propositional and first-order, often fuzzy-logic style) into neural computation by representing propositions and logical connectives as differentiable network components; the network performs bidirectional inference adhering to logic constraints and is used on logic programming tasks in the profiled experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Explicit propositional/first-order logic rules and fuzzy-logic style connectives represented in the model structure; symbolic knowledge is expressed as logic clauses/constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural computation implementing differentiable truth value propagation and parameterized modules for connectives; the network executes gradient-based learning and differentiable inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Symbolic rules are compiled into the NN structure as constraints and differentiable operators (a tight neuro-symbolic embedding); inference uses bidirectional dataflow in the network to respect logic constraints while remaining differentiable for learning.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables constrained learning where outputs satisfy logical constraints, supports bidirectional (deductive/inductive) inference within a differentiable framework, and yields more rule-consistent predictions and explainable logical traces compared to black-box NNs alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Logic program reasoning tasks (the paper profiles LNN on a logic program task).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Reported to improve logical consistency and sample efficiency relative to unconstrained neural learners in prior literature, but this paper focuses on runtime/operator profiling rather than quantitative generalization evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: logic rules are explicit in the model and can be used to trace and explain inference steps; the symbolic structure imposes interpretable constraints on outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Symbolic workload in LNNs can be data-movement bounded due to sparse/irregular memory access and bidirectional inference; elementwise and irregular logic computations map poorly to GPU-optimized kernels; scalability concerns for larger symbolic models.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Framed as embedding symbolic logic into neural computation to leverage differentiability and learning while preserving logical semantics; presented as a complementarity/division-of-labor approach rather than a formal unified theory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e480.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LTN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that maps logical formulas into constraints over tensor representations, using continuous-valued semantics to integrate logical reasoning with neural-style tensor computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Logic tensor networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Logic Tensor Networks (LTN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LTNs represent predicates and logical formulas as differentiable functions over tensors; logical formulas become soft constraints on tensors and are used during training as part of the objective, allowing learning that respects ontological constraints (profiled here on a binary classification task).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>First-order logical formulas and (often fuzzy) logical connectives that express domain knowledge and ontological constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Tensor-based differentiable functions (neural-style modules) that parametrize predicates and relations and are trained with gradient-based optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Differentiable encoding of logical formulas into tensor operations and loss regularizers: logic is translated into continuous constraints and optimized together with neural parameters (soft constraints / regularization).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables injecting symbolic prior knowledge/ontology into tensor learners, improving knowledge-graph completion and constraint-consistent predictions and allowing learning with less labelled data while maintaining differentiable optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Binary classification (profiled in this paper) and, in cited literature, knowledge graph completion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Claimed to improve sample efficiency and constraint-consistent generalization by incorporating ontological constraints; the present paper does not provide quantitative generalization comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Logic formulas are explicit but are implemented as soft differentiable constraints; interpretability exists at the formula level but internal tensor representations remain less transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Soft logical constraints may weaken strict interpretability; many LTN operations are elementwise and low arithmetic intensity, challenging to accelerate on standard DNN hardware; paper highlights symbolic operator inefficiencies.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Based on continuous relaxation of logical semantics (fuzzy/differentiable logic) so that symbolic constraints can be optimized with tensor methods; overall reasoning presented as complementary rather than under a single formal unified theory in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e480.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaZero</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaZero (general RL + MCTS hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid system that combines deep neural network policy/value estimators with symbolic/search-based Monte-Carlo Tree Search (MCTS) for planning in sequential decision-making games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering chess and shogi by self-play with a general reinforcement learning algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaZero (NN + MCTS hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AlphaZero integrates a deep neural network that outputs policy and value estimates with a search-based MCTS planner that uses these estimates to guide lookahead planning; the network is trained by self-play reinforcement learning and its outputs inform symbolic tree search during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Search/planning component (Monte-Carlo Tree Search) which explores discrete action sequences and enforces logical/temporal consistency of moves via search.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Deep neural networks (policy and value networks) trained via reinforcement learning/self-play to provide evaluations and priors for MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular interaction between NN and symbolic MCTS: the NN supplies priors and values to MCTS during search; training is self-play based with the search in the loop (not a single end-to-end differentiable graph).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Synergy of pattern recognition (NN) and explicit planning (MCTS) yields sample-efficient exploration and strong long-horizon decision making, producing superhuman performance in complex board games.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Sequential decision-making games (e.g., chess, shogi, Go variants as exemplars in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Combines learned evaluation with search to generalize to novel positions via planning; specific OOD/generalization metrics are not analyzed in this survey paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Search traces provide interpretable action sequences and can be inspected to explain decisions; neural evaluations remain less interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Paper notes this style (Symbolic[Neuro]) is loosely-coupled and may require large compute; the survey highlights that symbolic components (search) can be computationally intensive and not easily scalable in some contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Explained as complementary strengths (search/planning + learned function approximation); no new theoretical framework proposed in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e480.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Probabilistic Logic Programming (DeepProbLog)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic system that augments probabilistic logic programming (ProbLog) with neural predicates to combine perception with probabilistic logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural probabilistic logic programming in deepproblog.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DeepProbLog</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>DeepProbLog extends ProbLog by allowing neural networks to serve as probabilistic predicates; perception is handled by neural modules whose outputs are fed into a probabilistic logic program that performs inference under uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic logic programming (ProbLog) defining relations and probabilistic dependencies among symbols/facts.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks used as learned predicates (e.g., classifiers) whose probabilistic outputs are consumed by the logic program.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tight integration where neural predicates are embedded into the probabilistic logic program; training can be performed end-to-end via gradients through the probabilistic semantics (leveraging probabilistic inference to connect loss to neural parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables end-to-end combination of perception and probabilistic reasoning with uncertainty handling; yields structured, explainable probabilistic inferences that incorporate learned perceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Not specified in detail in this survey (presented as an example of Neuro|Symbolic approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Combines symbolic uncertainty modeling with learned perception to improve robustness under uncertainty; the survey does not provide numeric generalization comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: probabilistic logic program provides structured explanations and probabilistic reasoning traces; neural predicate internals are learned but their outputs are explicitly used by the logic program.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Probabilistic inference can be computationally expensive; integrating neural predicates into symbolic inference raises systems and performance challenges discussed in the survey (operator heterogeneity and runtime bottlenecks).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Framed around neuro-probabilistic-symbolic integration; no single unifying theory presented in the paper beyond advocating principled joint modeling as an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e480.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeurASP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Answer Set Programming (NeurASP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid approach that integrates neural networks with answer set programming (ASP), allowing neural perceptual modules to produce inputs for symbolic ASP reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embracing neural networks into answer set programming.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>NeurASP</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NeurASP allows neural networks to define probabilistic/soft predicates whose outputs are used by an ASP solver for logical reasoning, marrying perception with declarative non-monotonic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Answer Set Programming (ASP) providing rule-based, non-monotonic symbolic reasoning and constraint satisfaction.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks that provide perception or soft predicate evaluations feeding into the ASP program.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular embedding of learned neural predicates into ASP; the neural outputs condition ASP inference, enabling combined perception+reasoning pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables non-monotonic logical reasoning on perceptual inputs, allowing richer symbolic inference (including defaults and exceptions) conditioned on learned perceptual evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned as an example in the taxonomy; no specific benchmark results given in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to improve reasoning robustness by using symbolic ASP to structure conclusions from noisy perceptual inputs; quantitative generalization properties not supplied here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High interpretability from ASP traceability and rule-based derivations; neural components remain opaque but contribute explicit predicate probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Integration can induce heavy symbolic inference costs and irregular compute patterns that are inefficient on typical NN hardware; the survey notes symbolic workloads can dominate runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Presented as hybridizing neural perception with declarative ASP reasoning; no formal unified theory offered in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e480.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSCL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NSCL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline neuro-symbolic system that separates visual perception from symbolic program induction to answer compositional visual questions and learn concepts from weak supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Symbolic Concept Learner (NSCL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NSCL uses neural perception to parse scenes into object-level concepts and a symbolic program executor to perform compositional reasoning for visual question answering (e.g., CLEVR-like tasks), enabling disentangling of vision and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic program executor and discrete concept representations with explicit compositional operators (symbolic rules/programs).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules that detect objects and produce object attributes and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline (Neuro|Symbolic): perception outputs serve as inputs to symbolic program induction and execution; symbolic programs are induced/executed to answer questions.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Supports compositional generalization and data-efficient learning of concepts; disentangles perception errors from reasoning failures enabling interpretable error analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual question answering on compositional reasoning benchmarks (e.g., CLEVR and CLEVR-like datasets referenced in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed for compositional generalizationâ€”claimed to generalize better on compositional OOD splits than end-to-end neural baselines in prior work; this survey does not provide numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: symbolic programs provide interpretable reasoning chains and compositional structure for answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Pipeline modularity can cause error propagation from perception to reasoning; symbolic executor overheads contribute to runtime and scalability concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Architectural division of labor (perception vs. reasoning) advocated; no new formal theory introduced in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e480.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NSVQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-Symbolic Visual Question Answering (NSVQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic VQA system that disentangles visual perception from symbolic reasoning by using neural perception modules and symbolic reasoning/execution backends.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural-Symbolic VQA (NSVQA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NSVQA pipelines a neural vision frontend (object detectors, attribute classifiers) into a symbolic reasoning engine that executes programs derived from parsed questions, enabling explicit reasoning over objects and relations.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic program execution and rule-based relational reasoning over discrete object representations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural perception modules for vision and language parsing that produce symbols and program specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline (Neuro|Symbolic): perception and semantic parsing feed a symbolic executor which computes answers deterministically or via scripted rules.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Improved disentanglement of perception and reasoning, clearer interpretability of reasoning steps, and improved sample efficiency for compositional VQA tasks compared to monolithic neural models (as reported in the cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Visual question answering on CLEVR/CLEVRER-style datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Better compositional generalization and modular error diagnosis relative to end-to-end neural approaches in cited literature; survey does not quantify.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Highâ€”symbolic execution provides explicit chains of reasoning and program traces to justify answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Heavily dependent on accurate symbolic parsing and object detection; symbolic engine and elementwise symbolic ops can become runtime bottlenecks.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Presented under the pipeline neuro-symbolic taxonomy; no unifying formal theory presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e480.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural architecture designed to emulate symbolic relational reasoning by structuring neural computation to mimic logical operations over relations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural logic machines.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neural Logic Machines (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NLMs are neural architectures that implement relational operators and iterative message passing to perform symbolic-style relational reasoning using neural modules; they are designed to learn logical-like behaviors (e.g., recursion, variable binding) within neural computation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Implicit symbolic relational structure (relations, predicates) represented as graph-like relational inputs; logical operations are a target behavior rather than explicit symbolic rules in some formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Parameterized neural modules (message passing neural networks / structured layers) that learn to approximate logical operators and relational inference.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Neural architecture is structured to reflect logical relational computations (neural emulation of logic) rather than an explicit symbolic engine; selective attention and GNN-style mechanisms can incorporate symbolic cues.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Neuralized relational reasoning that can learn algorithms for multi-step relational inference and generalize to varying numbers of objects/relations, bridging neural learning and symbolic relational computation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Relational reasoning benchmarks and tasks requiring multi-step logic-like inference (paper cited as example).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Designed to improve systematic generalization over relational structures; the survey references them as candidates for representing symbolic expressions within neural models.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Intermediate: architectural motifs mirror logical operations, enabling some interpretability of relational processing, but internal learned operators remain neural and less interpretable than explicit rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>May require carefully structured architectures and training to recover precise symbolic behavior; elementwise and irregular operations pose hardware acceleration challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Motivated by representing symbolic relational computations within neural networks; no comprehensive theoretical unification in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e480.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abductive Learning (ABL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that bridges machine learning and logical reasoning by performing abductive inference to learn symbolic explanations that reconcile data and background knowledge, often coupled with neural perception.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging machine learning and logical reasoning by abductive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Abductive Learning (ABL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ABL integrates neural perception or feature learners with a symbolic abductive inference stage that hypothesizes missing facts or rules to explain observations, enabling joint learning of symbolic hypotheses and neural parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic rules and background knowledge expressed as logical clauses used to generate abductive explanations (hypotheses) for observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural modules that perform perception or statistical estimation, supplying observations that the abductive engine explains.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Hybrid loop where neural outputs feed the abductive reasoner, which in turn suggests symbolic corrections or hypotheses that guide further neural trainingâ€”loosely-coupled, iterative integration.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Facilitates learning human-understandable rules/hypotheses from noisy data and improves robustness by allowing symbolic hypotheses to correct or complement learned perception.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Presented as a methodological example; specific benchmark not profiled in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Promotes better interpretability and potential robustness/generalization by explicitly modelling explanations, but quantitative comparisons are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: outputs include abductive hypotheses and symbolic explanations that are inspectable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Abductive search can be computationally heavy and sequential; combined neuro-symbolic loops present systems challenges (scalability, runtime, and integration complexity).</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Conceptual framework of abductive hypothesis generation bridging perception and logic; no new formal theory provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e480.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e480.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NeuPSL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neuro-Probabilistic Soft Logic (NeuPSL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach combining neural networks with probabilistic soft logic to enable probabilistic relational reasoning informed by learned perceptual modules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neuro-Probabilistic Soft Logic (NeuPSL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>NeuPSL couples neural perception modules with a backend based on probabilistic soft logic; neural outputs are treated as soft evidence in a probabilistic logic framework to support structured uncertain reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Probabilistic Soft Logic (PSL)-style rules offering soft relational constraints and probabilistic reasoning over symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Neural networks providing perceptual or predicate probability estimates that are consumed by the probabilistic logic layer.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline/modular integration where neural predicate outputs are used as soft evidence in a probabilistic logic engine; the survey lists NeuPSL as an example of Neuro|Symbolic paradigms.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Combines uncertainty-aware symbolic inference with learned perception, enabling robust reasoning under noisy inputs and structured probabilistic explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Mentioned as a representative system in the taxonomy; not profiled quantitatively in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Intended to improve robustness to noisy perceptual inputs via probabilistic reasoning; survey provides no numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Provides probabilistic, rule-based explanations; neural modules remain less transparent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Probabilistic reasoning and soft constraints can be computationally demanding; integration heterogeneity increases systems complexity as noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Motivated by unifying probabilistic, symbolic, and neural methods; survey highlights this unification as a significant open challenge without providing a complete framework.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A neuro-vector-symbolic architecture for solving raven's progressive matrices. <em>(Rating: 2)</em></li>
                <li>Logic tensor networks. <em>(Rating: 2)</em></li>
                <li>Neural probabilistic logic programming in deepproblog. <em>(Rating: 2)</em></li>
                <li>Embracing neural networks into answer set programming. <em>(Rating: 2)</em></li>
                <li>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. <em>(Rating: 2)</em></li>
                <li>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. <em>(Rating: 2)</em></li>
                <li>Neural logic machines. <em>(Rating: 2)</em></li>
                <li>Bridging machine learning and logical reasoning by abductive learning. <em>(Rating: 2)</em></li>
                <li>Neurosymbolic ai: The 3 rd wave. <em>(Rating: 1)</em></li>
                <li>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-480",
    "paper_id": "paper-266725471",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "NVSA",
            "name_full": "Neuro-Vector-Symbolic Architecture",
            "brief_description": "A pipeline neuro-symbolic system that uses a neural perception frontend to produce vector-symbolic representations and a symbolic/probabilistic reasoner backend to perform abductive reasoning for abstract visual-relational tasks.",
            "citation_title": "A neuro-vector-symbolic architecture for solving raven's progressive matrices.",
            "mention_or_use": "use",
            "system_name": "Neuro-Vector-Symbolic Architecture (NVSA)",
            "system_description": "NVSA is a modular pipeline: a neural perception frontend (e.g., ResNet18) extracts features and semantic parses which are converted to vector-symbolic or discrete percepts; a symbolic/probabilistic reasoning backend performs rule detection and probabilistic abductive reasoning to solve relational and analogical problems (specifically Raven's Progressive Matrices). The symbolic stage is run after the neural frontend and lies on the inference critical path.",
            "declarative_component": "Symbolic/probabilistic reasoning module that performs rule detection and probabilistic abductive reasoning; operates on vector-symbolic or symbolic representations and explicit rules/queries.",
            "imperative_component": "Neural perception frontend (ResNet18 in the profiled instantiation) providing feature extraction, semantic parsing and vector embeddings.",
            "integration_method": "Modular pipeline (Neuro|Symbolic): the neural frontend produces inputs for the symbolic backend; the backend executes sequential rule detection and probabilistic abductive inference on those inputs. Integration is asynchronous/modular rather than tightly differentiable end-to-end in the profiled system.",
            "emergent_properties": "Combines robust perceptual generalization from deep networks with symbolic abductive reasoning to solve abstract relational puzzles; enables disentangling perception and reasoning, improving interpretability of final decisions via symbolic tracebacks and enabling complex analogical inference not achievable by perception alone.",
            "task_or_benchmark": "Raven's Progressive Matrices (visual abstract reasoning / relational analogical reasoning).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Paper reports NVSA scales poorly in runtime with puzzle size (total runtime increases superlinearly) and claims the architecture leverages perception+reasoning to handle abstract relational generalization; explicit numeric OOD/generalization metrics are not provided in this paper.",
            "interpretability_properties": "Symbolic backend provides more interpretable reasoning traces (rule detection and abductive explanations) compared to pure neural methods; perception component remains a black-box feature extractor but outputs are symbolicized for downstream explainability.",
            "limitations_or_failures": "Symbolic reasoning dominates runtime (e.g., ~92% of NVSA runtime in some settings), suffers from sequential, computationally intensive rule detection on the critical path, and faces scalability bottlenecks as problem size increases; vector/elementwise symbolic ops have low arithmetic intensity and are inefficient on GPUs.",
            "theoretical_framework": "Motivated by complementary strengths (neural perception + symbolic reasoning) and modular division of labor; no unified formal theory provided in this paper beyond taxonomic framing.",
            "uuid": "e480.0",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LNN",
            "name_full": "Logical Neural Networks (LNN)",
            "brief_description": "A neuro-symbolic model that encodes symbolic first-order or propositional logic rules as constraints within a neural architecture to perform reasoning tasks while retaining differentiability.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Logical Neural Networks (LNN)",
            "system_description": "LNNs embed symbolic logic (propositional and first-order, often fuzzy-logic style) into neural computation by representing propositions and logical connectives as differentiable network components; the network performs bidirectional inference adhering to logic constraints and is used on logic programming tasks in the profiled experiments.",
            "declarative_component": "Explicit propositional/first-order logic rules and fuzzy-logic style connectives represented in the model structure; symbolic knowledge is expressed as logic clauses/constraints.",
            "imperative_component": "Neural computation implementing differentiable truth value propagation and parameterized modules for connectives; the network executes gradient-based learning and differentiable inference.",
            "integration_method": "Symbolic rules are compiled into the NN structure as constraints and differentiable operators (a tight neuro-symbolic embedding); inference uses bidirectional dataflow in the network to respect logic constraints while remaining differentiable for learning.",
            "emergent_properties": "Enables constrained learning where outputs satisfy logical constraints, supports bidirectional (deductive/inductive) inference within a differentiable framework, and yields more rule-consistent predictions and explainable logical traces compared to black-box NNs alone.",
            "task_or_benchmark": "Logic program reasoning tasks (the paper profiles LNN on a logic program task).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Reported to improve logical consistency and sample efficiency relative to unconstrained neural learners in prior literature, but this paper focuses on runtime/operator profiling rather than quantitative generalization evaluation.",
            "interpretability_properties": "High: logic rules are explicit in the model and can be used to trace and explain inference steps; the symbolic structure imposes interpretable constraints on outputs.",
            "limitations_or_failures": "Symbolic workload in LNNs can be data-movement bounded due to sparse/irregular memory access and bidirectional inference; elementwise and irregular logic computations map poorly to GPU-optimized kernels; scalability concerns for larger symbolic models.",
            "theoretical_framework": "Framed as embedding symbolic logic into neural computation to leverage differentiability and learning while preserving logical semantics; presented as a complementarity/division-of-labor approach rather than a formal unified theory.",
            "uuid": "e480.1",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LTN",
            "name_full": "Logic Tensor Networks (LTN)",
            "brief_description": "A neuro-symbolic approach that maps logical formulas into constraints over tensor representations, using continuous-valued semantics to integrate logical reasoning with neural-style tensor computation.",
            "citation_title": "Logic tensor networks.",
            "mention_or_use": "use",
            "system_name": "Logic Tensor Networks (LTN)",
            "system_description": "LTNs represent predicates and logical formulas as differentiable functions over tensors; logical formulas become soft constraints on tensors and are used during training as part of the objective, allowing learning that respects ontological constraints (profiled here on a binary classification task).",
            "declarative_component": "First-order logical formulas and (often fuzzy) logical connectives that express domain knowledge and ontological constraints.",
            "imperative_component": "Tensor-based differentiable functions (neural-style modules) that parametrize predicates and relations and are trained with gradient-based optimization.",
            "integration_method": "Differentiable encoding of logical formulas into tensor operations and loss regularizers: logic is translated into continuous constraints and optimized together with neural parameters (soft constraints / regularization).",
            "emergent_properties": "Enables injecting symbolic prior knowledge/ontology into tensor learners, improving knowledge-graph completion and constraint-consistent predictions and allowing learning with less labelled data while maintaining differentiable optimization.",
            "task_or_benchmark": "Binary classification (profiled in this paper) and, in cited literature, knowledge graph completion tasks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Claimed to improve sample efficiency and constraint-consistent generalization by incorporating ontological constraints; the present paper does not provide quantitative generalization comparisons.",
            "interpretability_properties": "Logic formulas are explicit but are implemented as soft differentiable constraints; interpretability exists at the formula level but internal tensor representations remain less transparent.",
            "limitations_or_failures": "Soft logical constraints may weaken strict interpretability; many LTN operations are elementwise and low arithmetic intensity, challenging to accelerate on standard DNN hardware; paper highlights symbolic operator inefficiencies.",
            "theoretical_framework": "Based on continuous relaxation of logical semantics (fuzzy/differentiable logic) so that symbolic constraints can be optimized with tensor methods; overall reasoning presented as complementary rather than under a single formal unified theory in this paper.",
            "uuid": "e480.2",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "AlphaZero",
            "name_full": "AlphaZero (general RL + MCTS hybrid)",
            "brief_description": "A hybrid system that combines deep neural network policy/value estimators with symbolic/search-based Monte-Carlo Tree Search (MCTS) for planning in sequential decision-making games.",
            "citation_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm.",
            "mention_or_use": "mention",
            "system_name": "AlphaZero (NN + MCTS hybrid)",
            "system_description": "AlphaZero integrates a deep neural network that outputs policy and value estimates with a search-based MCTS planner that uses these estimates to guide lookahead planning; the network is trained by self-play reinforcement learning and its outputs inform symbolic tree search during inference.",
            "declarative_component": "Search/planning component (Monte-Carlo Tree Search) which explores discrete action sequences and enforces logical/temporal consistency of moves via search.",
            "imperative_component": "Deep neural networks (policy and value networks) trained via reinforcement learning/self-play to provide evaluations and priors for MCTS.",
            "integration_method": "Modular interaction between NN and symbolic MCTS: the NN supplies priors and values to MCTS during search; training is self-play based with the search in the loop (not a single end-to-end differentiable graph).",
            "emergent_properties": "Synergy of pattern recognition (NN) and explicit planning (MCTS) yields sample-efficient exploration and strong long-horizon decision making, producing superhuman performance in complex board games.",
            "task_or_benchmark": "Sequential decision-making games (e.g., chess, shogi, Go variants as exemplars in literature).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Combines learned evaluation with search to generalize to novel positions via planning; specific OOD/generalization metrics are not analyzed in this survey paper.",
            "interpretability_properties": "Search traces provide interpretable action sequences and can be inspected to explain decisions; neural evaluations remain less interpretable.",
            "limitations_or_failures": "Paper notes this style (Symbolic[Neuro]) is loosely-coupled and may require large compute; the survey highlights that symbolic components (search) can be computationally intensive and not easily scalable in some contexts.",
            "theoretical_framework": "Explained as complementary strengths (search/planning + learned function approximation); no new theoretical framework proposed in this survey.",
            "uuid": "e480.3",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "DeepProbLog",
            "name_full": "Neural Probabilistic Logic Programming (DeepProbLog)",
            "brief_description": "A neuro-symbolic system that augments probabilistic logic programming (ProbLog) with neural predicates to combine perception with probabilistic logical reasoning.",
            "citation_title": "Neural probabilistic logic programming in deepproblog.",
            "mention_or_use": "mention",
            "system_name": "DeepProbLog",
            "system_description": "DeepProbLog extends ProbLog by allowing neural networks to serve as probabilistic predicates; perception is handled by neural modules whose outputs are fed into a probabilistic logic program that performs inference under uncertainty.",
            "declarative_component": "Probabilistic logic programming (ProbLog) defining relations and probabilistic dependencies among symbols/facts.",
            "imperative_component": "Neural networks used as learned predicates (e.g., classifiers) whose probabilistic outputs are consumed by the logic program.",
            "integration_method": "Tight integration where neural predicates are embedded into the probabilistic logic program; training can be performed end-to-end via gradients through the probabilistic semantics (leveraging probabilistic inference to connect loss to neural parameters).",
            "emergent_properties": "Enables end-to-end combination of perception and probabilistic reasoning with uncertainty handling; yields structured, explainable probabilistic inferences that incorporate learned perceptual knowledge.",
            "task_or_benchmark": "Not specified in detail in this survey (presented as an example of Neuro|Symbolic approaches).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Combines symbolic uncertainty modeling with learned perception to improve robustness under uncertainty; the survey does not provide numeric generalization comparisons.",
            "interpretability_properties": "High: probabilistic logic program provides structured explanations and probabilistic reasoning traces; neural predicate internals are learned but their outputs are explicitly used by the logic program.",
            "limitations_or_failures": "Probabilistic inference can be computationally expensive; integrating neural predicates into symbolic inference raises systems and performance challenges discussed in the survey (operator heterogeneity and runtime bottlenecks).",
            "theoretical_framework": "Framed around neuro-probabilistic-symbolic integration; no single unifying theory presented in the paper beyond advocating principled joint modeling as an open challenge.",
            "uuid": "e480.4",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "NeurASP",
            "name_full": "Neuro-Answer Set Programming (NeurASP)",
            "brief_description": "A hybrid approach that integrates neural networks with answer set programming (ASP), allowing neural perceptual modules to produce inputs for symbolic ASP reasoning.",
            "citation_title": "Embracing neural networks into answer set programming.",
            "mention_or_use": "mention",
            "system_name": "NeurASP",
            "system_description": "NeurASP allows neural networks to define probabilistic/soft predicates whose outputs are used by an ASP solver for logical reasoning, marrying perception with declarative non-monotonic reasoning.",
            "declarative_component": "Answer Set Programming (ASP) providing rule-based, non-monotonic symbolic reasoning and constraint satisfaction.",
            "imperative_component": "Neural networks that provide perception or soft predicate evaluations feeding into the ASP program.",
            "integration_method": "Modular embedding of learned neural predicates into ASP; the neural outputs condition ASP inference, enabling combined perception+reasoning pipelines.",
            "emergent_properties": "Enables non-monotonic logical reasoning on perceptual inputs, allowing richer symbolic inference (including defaults and exceptions) conditioned on learned perceptual evidence.",
            "task_or_benchmark": "Mentioned as an example in the taxonomy; no specific benchmark results given in this survey.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed to improve reasoning robustness by using symbolic ASP to structure conclusions from noisy perceptual inputs; quantitative generalization properties not supplied here.",
            "interpretability_properties": "High interpretability from ASP traceability and rule-based derivations; neural components remain opaque but contribute explicit predicate probabilities.",
            "limitations_or_failures": "Integration can induce heavy symbolic inference costs and irregular compute patterns that are inefficient on typical NN hardware; the survey notes symbolic workloads can dominate runtime.",
            "theoretical_framework": "Presented as hybridizing neural perception with declarative ASP reasoning; no formal unified theory offered in the survey.",
            "uuid": "e480.5",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "NSCL",
            "name_full": "Neuro-Symbolic Concept Learner (NSCL)",
            "brief_description": "A pipeline neuro-symbolic system that separates visual perception from symbolic program induction to answer compositional visual questions and learn concepts from weak supervision.",
            "citation_title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.",
            "mention_or_use": "mention",
            "system_name": "Neuro-Symbolic Concept Learner (NSCL)",
            "system_description": "NSCL uses neural perception to parse scenes into object-level concepts and a symbolic program executor to perform compositional reasoning for visual question answering (e.g., CLEVR-like tasks), enabling disentangling of vision and reasoning.",
            "declarative_component": "Symbolic program executor and discrete concept representations with explicit compositional operators (symbolic rules/programs).",
            "imperative_component": "Neural perception modules that detect objects and produce object attributes and embeddings.",
            "integration_method": "Pipeline (Neuro|Symbolic): perception outputs serve as inputs to symbolic program induction and execution; symbolic programs are induced/executed to answer questions.",
            "emergent_properties": "Supports compositional generalization and data-efficient learning of concepts; disentangles perception errors from reasoning failures enabling interpretable error analysis.",
            "task_or_benchmark": "Visual question answering on compositional reasoning benchmarks (e.g., CLEVR and CLEVR-like datasets referenced in literature).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed for compositional generalizationâ€”claimed to generalize better on compositional OOD splits than end-to-end neural baselines in prior work; this survey does not provide numeric comparisons.",
            "interpretability_properties": "High: symbolic programs provide interpretable reasoning chains and compositional structure for answers.",
            "limitations_or_failures": "Pipeline modularity can cause error propagation from perception to reasoning; symbolic executor overheads contribute to runtime and scalability concerns.",
            "theoretical_framework": "Architectural division of labor (perception vs. reasoning) advocated; no new formal theory introduced in this survey.",
            "uuid": "e480.6",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "NSVQA",
            "name_full": "Neural-Symbolic Visual Question Answering (NSVQA)",
            "brief_description": "A neuro-symbolic VQA system that disentangles visual perception from symbolic reasoning by using neural perception modules and symbolic reasoning/execution backends.",
            "citation_title": "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.",
            "mention_or_use": "mention",
            "system_name": "Neural-Symbolic VQA (NSVQA)",
            "system_description": "NSVQA pipelines a neural vision frontend (object detectors, attribute classifiers) into a symbolic reasoning engine that executes programs derived from parsed questions, enabling explicit reasoning over objects and relations.",
            "declarative_component": "Symbolic program execution and rule-based relational reasoning over discrete object representations.",
            "imperative_component": "Neural perception modules for vision and language parsing that produce symbols and program specifications.",
            "integration_method": "Pipeline (Neuro|Symbolic): perception and semantic parsing feed a symbolic executor which computes answers deterministically or via scripted rules.",
            "emergent_properties": "Improved disentanglement of perception and reasoning, clearer interpretability of reasoning steps, and improved sample efficiency for compositional VQA tasks compared to monolithic neural models (as reported in the cited works).",
            "task_or_benchmark": "Visual question answering on CLEVR/CLEVRER-style datasets.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Better compositional generalization and modular error diagnosis relative to end-to-end neural approaches in cited literature; survey does not quantify.",
            "interpretability_properties": "Highâ€”symbolic execution provides explicit chains of reasoning and program traces to justify answers.",
            "limitations_or_failures": "Heavily dependent on accurate symbolic parsing and object detection; symbolic engine and elementwise symbolic ops can become runtime bottlenecks.",
            "theoretical_framework": "Presented under the pipeline neuro-symbolic taxonomy; no unifying formal theory presented.",
            "uuid": "e480.7",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "NLM",
            "name_full": "Neural Logic Machines",
            "brief_description": "A neural architecture designed to emulate symbolic relational reasoning by structuring neural computation to mimic logical operations over relations.",
            "citation_title": "Neural logic machines.",
            "mention_or_use": "mention",
            "system_name": "Neural Logic Machines (NLM)",
            "system_description": "NLMs are neural architectures that implement relational operators and iterative message passing to perform symbolic-style relational reasoning using neural modules; they are designed to learn logical-like behaviors (e.g., recursion, variable binding) within neural computation.",
            "declarative_component": "Implicit symbolic relational structure (relations, predicates) represented as graph-like relational inputs; logical operations are a target behavior rather than explicit symbolic rules in some formulations.",
            "imperative_component": "Parameterized neural modules (message passing neural networks / structured layers) that learn to approximate logical operators and relational inference.",
            "integration_method": "Neural architecture is structured to reflect logical relational computations (neural emulation of logic) rather than an explicit symbolic engine; selective attention and GNN-style mechanisms can incorporate symbolic cues.",
            "emergent_properties": "Neuralized relational reasoning that can learn algorithms for multi-step relational inference and generalize to varying numbers of objects/relations, bridging neural learning and symbolic relational computation.",
            "task_or_benchmark": "Relational reasoning benchmarks and tasks requiring multi-step logic-like inference (paper cited as example).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Designed to improve systematic generalization over relational structures; the survey references them as candidates for representing symbolic expressions within neural models.",
            "interpretability_properties": "Intermediate: architectural motifs mirror logical operations, enabling some interpretability of relational processing, but internal learned operators remain neural and less interpretable than explicit rules.",
            "limitations_or_failures": "May require carefully structured architectures and training to recover precise symbolic behavior; elementwise and irregular operations pose hardware acceleration challenges.",
            "theoretical_framework": "Motivated by representing symbolic relational computations within neural networks; no comprehensive theoretical unification in this survey.",
            "uuid": "e480.8",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ABL",
            "name_full": "Abductive Learning (ABL)",
            "brief_description": "A framework that bridges machine learning and logical reasoning by performing abductive inference to learn symbolic explanations that reconcile data and background knowledge, often coupled with neural perception.",
            "citation_title": "Bridging machine learning and logical reasoning by abductive learning.",
            "mention_or_use": "mention",
            "system_name": "Abductive Learning (ABL)",
            "system_description": "ABL integrates neural perception or feature learners with a symbolic abductive inference stage that hypothesizes missing facts or rules to explain observations, enabling joint learning of symbolic hypotheses and neural parameters.",
            "declarative_component": "Symbolic rules and background knowledge expressed as logical clauses used to generate abductive explanations (hypotheses) for observed data.",
            "imperative_component": "Neural modules that perform perception or statistical estimation, supplying observations that the abductive engine explains.",
            "integration_method": "Hybrid loop where neural outputs feed the abductive reasoner, which in turn suggests symbolic corrections or hypotheses that guide further neural trainingâ€”loosely-coupled, iterative integration.",
            "emergent_properties": "Facilitates learning human-understandable rules/hypotheses from noisy data and improves robustness by allowing symbolic hypotheses to correct or complement learned perception.",
            "task_or_benchmark": "Presented as a methodological example; specific benchmark not profiled in this survey.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Promotes better interpretability and potential robustness/generalization by explicitly modelling explanations, but quantitative comparisons are not provided in this paper.",
            "interpretability_properties": "High: outputs include abductive hypotheses and symbolic explanations that are inspectable.",
            "limitations_or_failures": "Abductive search can be computationally heavy and sequential; combined neuro-symbolic loops present systems challenges (scalability, runtime, and integration complexity).",
            "theoretical_framework": "Conceptual framework of abductive hypothesis generation bridging perception and logic; no new formal theory provided in the survey.",
            "uuid": "e480.9",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "NeuPSL",
            "name_full": "Neuro-Probabilistic Soft Logic (NeuPSL)",
            "brief_description": "An approach combining neural networks with probabilistic soft logic to enable probabilistic relational reasoning informed by learned perceptual modules.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Neuro-Probabilistic Soft Logic (NeuPSL)",
            "system_description": "NeuPSL couples neural perception modules with a backend based on probabilistic soft logic; neural outputs are treated as soft evidence in a probabilistic logic framework to support structured uncertain reasoning.",
            "declarative_component": "Probabilistic Soft Logic (PSL)-style rules offering soft relational constraints and probabilistic reasoning over symbols.",
            "imperative_component": "Neural networks providing perceptual or predicate probability estimates that are consumed by the probabilistic logic layer.",
            "integration_method": "Pipeline/modular integration where neural predicate outputs are used as soft evidence in a probabilistic logic engine; the survey lists NeuPSL as an example of Neuro|Symbolic paradigms.",
            "emergent_properties": "Combines uncertainty-aware symbolic inference with learned perception, enabling robust reasoning under noisy inputs and structured probabilistic explanations.",
            "task_or_benchmark": "Mentioned as a representative system in the taxonomy; not profiled quantitatively in this paper.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Intended to improve robustness to noisy perceptual inputs via probabilistic reasoning; survey provides no numeric evaluation.",
            "interpretability_properties": "Provides probabilistic, rule-based explanations; neural modules remain less transparent.",
            "limitations_or_failures": "Probabilistic reasoning and soft constraints can be computationally demanding; integration heterogeneity increases systems complexity as noted in the survey.",
            "theoretical_framework": "Motivated by unifying probabilistic, symbolic, and neural methods; survey highlights this unification as a significant open challenge without providing a complete framework.",
            "uuid": "e480.10",
            "source_info": {
                "paper_title": "Towards Cognitive AI Systems: a Survey and Prospective on Neuro-Symbolic AI",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A neuro-vector-symbolic architecture for solving raven's progressive matrices.",
            "rating": 2,
            "sanitized_title": "a_neurovectorsymbolic_architecture_for_solving_ravens_progressive_matrices"
        },
        {
            "paper_title": "Logic tensor networks.",
            "rating": 2,
            "sanitized_title": "logic_tensor_networks"
        },
        {
            "paper_title": "Neural probabilistic logic programming in deepproblog.",
            "rating": 2,
            "sanitized_title": "neural_probabilistic_logic_programming_in_deepproblog"
        },
        {
            "paper_title": "Embracing neural networks into answer set programming.",
            "rating": 2,
            "sanitized_title": "embracing_neural_networks_into_answer_set_programming"
        },
        {
            "paper_title": "The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision.",
            "rating": 2,
            "sanitized_title": "the_neurosymbolic_concept_learner_interpreting_scenes_words_and_sentences_from_natural_supervision"
        },
        {
            "paper_title": "Neural-symbolic vqa: Disentangling reasoning from vision and language understanding.",
            "rating": 2,
            "sanitized_title": "neuralsymbolic_vqa_disentangling_reasoning_from_vision_and_language_understanding"
        },
        {
            "paper_title": "Neural logic machines.",
            "rating": 2,
            "sanitized_title": "neural_logic_machines"
        },
        {
            "paper_title": "Bridging machine learning and logical reasoning by abductive learning.",
            "rating": 2,
            "sanitized_title": "bridging_machine_learning_and_logical_reasoning_by_abductive_learning"
        },
        {
            "paper_title": "Neurosymbolic ai: The 3 rd wave.",
            "rating": 1,
            "sanitized_title": "neurosymbolic_ai_the_3_rd_wave"
        },
        {
            "paper_title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm.",
            "rating": 1,
            "sanitized_title": "mastering_chess_and_shogi_by_selfplay_with_a_general_reinforcement_learning_algorithm"
        }
    ],
    "cost": 0.0200335,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TOWARDS COGNITIVE AI SYSTEMS: A SURVEY AND PROSPECTIVE ON NEURO-SYMBOLIC AI
2 Jan 2024</p>
<p>Zishen Wan 
Georgia Institute of Technology</p>
<p>Che-Kai Liu 
Equal contribution</p>
<p>Georgia Institute of Technology</p>
<p>Hanchen Yang 
Equal contribution</p>
<p>Georgia Institute of Technology</p>
<p>Chaojian Li 
Haoran You 
Equal contribution</p>
<p>Georgia Institute of Technology</p>
<p>Yonggan Fu 
Equal contribution</p>
<p>Georgia Institute of Technology</p>
<p>Cheng Wan 
Georgia Institute of Technology</p>
<p>Tushar Krishna 
Georgia Institute of Technology</p>
<p>Georgia Institute of Technology</p>
<p>Celine Lin 
Georgia Institute of Technology</p>
<p>Arijit Raychowdhury <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#114;&#105;&#106;&#105;&#116;&#46;&#114;&#97;&#121;&#99;&#104;&#111;&#119;&#100;&#104;&#117;&#114;&#121;&#64;&#101;&#99;&#101;&#46;&#103;&#97;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;">&#97;&#114;&#105;&#106;&#105;&#116;&#46;&#114;&#97;&#121;&#99;&#104;&#111;&#119;&#100;&#104;&#117;&#114;&#121;&#64;&#101;&#99;&#101;&#46;&#103;&#97;&#116;&#101;&#99;&#104;&#46;&#101;&#100;&#117;</a>. 
Georgia Institute of Technology</p>
<p>TOWARDS COGNITIVE AI SYSTEMS: A SURVEY AND PROSPECTIVE ON NEURO-SYMBOLIC AI
2 Jan 2024BA52387BA1DCB15EFDB54F38A3D80A21arXiv:2401.01040v1[cs.AI]
The remarkable advancements in artificial intelligence (AI), primarily driven by deep neural networks, have significantly impacted various aspects of our lives.However, the current challenges surrounding unsustainable computational trajectories, limited robustness, and a lack of explainability call for the development of nextgeneration AI systems.Neuro-symbolic AI (NSAI) emerges as a promising paradigm, fusing neural, symbolic, and probabilistic approaches to enhance interpretability, robustness, and trustworthiness while facilitating learning from much less data.Recent NSAI systems have demonstrated great potential in collaborative human-AI scenarios with reasoning and cognitive capabilities.In this paper, we provide a systematic review of recent progress in NSAI and analyze the performance characteristics and computational operators of NSAI models.Furthermore, we discuss the challenges and potential future directions of NSAI from both system and architectural perspectives.</p>
<p>INTRODUCTION</p>
<p>Remarkable advancements in artificial intelligence (AI) have had a profound impact on our lives and numerous industries.These advancements are primarily driven by deep neural networks (DNN) and a virtuous cycle involving large networks, extensive datasets, and augmented computing power.As we reap the benefits of this success, there is growing evidence that continuing our current trajectory may not be viable for realizing AI's full potential.First, the escalating computational requirements and energy consumption associated with AI are on an unsustainable trajectory (Wu et al., 2022), threatening to reach a level that could stifle innovation by restricting it to a select few organizations.Second, the lack of robustness and explainability remains a significant challenge, likely due to inherent limitations in current learning methodologies (Wan et al., 2021;Dwivedi et al., 2023).Third, contemporary AI systems mostly operate in isolation, with limited collaboration between humans and AI agents.Hence, it is imperative to develop nextgeneration AI paradigms that address the growing demand for efficiency, explainability, and trust in AI systems.</p>
<p>Neuro-symbolic AI (NSAI) represents an emerging AI paradigm that integrates neural, symbolic, and probabilistic approaches to enhance explainability, robustness, and enable learning from much less data in AI (Fig. 1).Neural methods have proven highly effective in extracting complex features from data for tasks such as natural language processing and object detection.On the other hand, symbolic methods enhance explainability and reduce the dependence on extensive training data by incorporating established models of the physical world, and probabilistic methods enable cognitive systems to more effectively handle uncertainty, resulting in improved robustness under unstructured conditions.The synergistic fusion of neural, symbolic, and probabilistic methods positions NSAI as a promising paradigm capable of ushering in the third wave of AI (Garcez &amp; Lamb, 2023).NSAI promises many possibilities for AI systems that acquire human-like communication and reasoning capabilities, enabling them to recognize, classify, and adapt to new situations autonomously.In addition to its superior performance  compared to traditional AI models in tasks such as image and video question answering (Mao et al., 2019;Yang et al., 2020), NSAI holds significant potential for enhancing realtime responses, energy efficiency, explainability, and trustworthiness of collaborative human-AI applications.These applications include collaborative robotics, mixed-reality systems, and human-AI interactions in the metaverse, where robots can seamlessly interact with humans in complex environments, AI agents can reason and make decisions in a robust and explainable manner, and intelligence is pervasively embedded and untethered from the cloud.</p>
<p>In this paper, we provide a systematic survey, evaluation, and analysis of NSAI systems, promising a next-generation AI paradigm.First, we review and categorize the state-ofthe-art NSAI systems from a structured perspective (Sec.2).Second, we analyze various NSAI workloads on hardware platforms, examining their runtime characteristics and underlying compute operators (Sec.3).Lastly, we discuss the challenges and opportunities for NSAI system research and our outlook on the road ahead (Sec.4).To the best of our knowledge, this is the first paper to assess NSAI from both a system and architecture perspective, aiming to inspire the design of next-generation cognitive computing systems through synergistic advancements in NSAI algorithms, systems, architecture, and algorithm-hardware co-design.</p>
<p>NEURO-SYMBOLIC AI ALGORITHMS</p>
<p>In this section, we systematically review and categorize the recent research progress in NSAI algorithms.</p>
<p>Overview.NSAI represents an interdisciplinary approach that synergistically combines symbolic reasoning with neural network (NN) learning to create intelligent systems, leveraging the complementary strengths of both to enhance the accuracy and interpretability of the resulting models.</p>
<p>Given that NSAI algorithms typically incorporate both symbolic and neural components, various NSAI paradigms can be categorized based on how these components are integrated into a cohesive system.Inspired by Henry Kautz's NSAI taxonomy (Kaut, 2020), we systematically categorize these NSAI algorithms into five paradigms, as summarized in Tab. 1.We elaborate each of these paradigms below.Additionally, Tab. 2 provides examples of several underlying operations based on the categorization in Tab. 1.</p>
<p>Symbolic[Neuro]</p>
<p>refers to an intelligent system that empowers symbolic reasoning with the statistical learning capabilities of NNs.These systems typically consist of a comprehensive symbolic problem solver that includes looselycoupled neural subroutines for statistical learning.Examples include DeepMind's AlphaGo (Silver et al., 2017) and AlphaZero (Zhang &amp; Yu, 2020), which use Monte-Carlo Tree Search (MCTS) as the symbolic solver and NN state estimators for learning statistical patterns.(Pryor et al., 2022) NN, fuzzy logic Vector NSCL (Mao et al., 2019) NN, add, mul, div, log Vector NeurASP (Yang et al., 2020) NN, logic rules Non-Vector ABL (Dai et al., 2019) NN, logic rules Non-Vector NSVQA (Yi et al., 2018) NN, pre-defined objects Non-Vector Neuro|Symbolic refers to a hybrid system that combines a neural system and a symbolic system in a pipeline, where each component typically specializes in complementary tasks within the pipeline.To the best of our knowledge, the majority of NSAI algorithms fall into this category.For example, IBM's neuro-vector-symbolic architecture (NVSA) (Hersche et al., 2023) uses an NN as the frontend for perception and semantic parsing, and a symbolic reasoner as the backend for probabilistic abductive reasoning on the RAVEN (Zhang et al., 2019) and I-RAVEN (Hu et al., 2021) datasets.Other examples include neuro-probabilistic soft logic (NeuPSL) (Pryor et al., 2022), neural probabilistic logic programming (DeepProbLog) (Manhaeve et al., 2021), neuro-answer set programming (NeurASP) (Yang et al., 2020), nerual symbolic dynamic reasoning (Yi et al., 2020), neural symbolic concept learner (NSCL) (Mao et al., 2019), abductive learning (ABL) (Dai et al., 2019), and neurosymbolic visual question answering (NSVQA) (Yi et al., 2018) on the CLEVRER dataset (Yi et al., 2020).
Neuro
Neuro:Symbolicâ†’Neuro approach incorporates symbolic rules into NNs to guide the learning process, where symbolic knowledge is compiled into the structure of neural models for enhancing the model interpretability.For instance, logical NNs (LNNs) (Riegel et al., 2020) encode knowledge or domain expertise as symbolic rules (first-order logic or fuzzy logic) that act as constraints on the NN output.Other examples include deep learning for symbolic mathematics (Lample &amp; Charton, 2019) and differentiable inductive logic programming (ILP) (Evans &amp; Grefenstette, 2018).</p>
<p>Neuro Symbolic is a type of hybrid approach that combines symbolic logic rules with NNs.It involves mapping symbolic logic rules onto embeddings that serve as soft constraints or regularizers on the NN's loss function.Logical tensor networks (LTNs) (Badreddine et al., 2022), for instance, use logical formulas to define constraints on the tensor representations, which has proven successful in knowledge graph completion tasks.These tasks aim to predict
F = âˆ€x(isCarnivor(s)) â†’ (isM ammal(x)) {isCarnivor(s):[0, 1], isM ammal(x):[1, 0]} â†’ F = [1, 0] Mul and Add (NVSA) Xi âˆˆ {+1, âˆ’1} d â†’ (Xi â€¢ Xj)/(Xi + Xj)
Logic rules (ABL) Domain: animal(dog).carnivore(dog).mammal(dog)Logical formula: mammal(x) âˆ§ carnivore(x) ABL: hypos(x) : âˆ’animal(x), mammal(x), carnivore(x) Pre-defined objects (NSVQA)</p>
<p>equal color: (entry, entry) â†’ Boolean equal integer: (number, number) â†’ Boolean missing facts or relationships between entities.Other examples of this approach include deep ontology networks (Hohenecker &amp; Lukas, 2020) and tensorization methods (Garcez et al., 2019).As the inference is still governed by NNs, it remains a research question whether this approach will compromise the interpretability.</p>
<p>Neuro[Symbolic] refers to a system that empowers NNs with the explainability and robustness of symbolic reasoning.Unlike Symbolic [Neuro], where symbolic reasoning is used to guide the neural model learning process, in Neuro[Symbolic], the neural model incorporates symbolic reasoning by paying attention to specific symbolics at certain conditions.For instance, graph neural networks (GNNs) are often adopted as strong candidates for representing symbolic expressions when endowed with attention mechanisms (Lamb et al., 2020).In particular, this attention mechanism can be leveraged to incorporate symbolic rules into GNN models, enabling selective attention to pertinent symbolic information in the graph.Other examples include neural logic machines (NLM) (Dong et al., 2019).</p>
<p>NSAI SYSTEM PROFILING</p>
<p>In this section, we analyze the performance characteristics of three recent NSAI models and discuss their system bottleneck, workload operators, and optimization opportunities.</p>
<p>NSAI Model and Experimental Methodology</p>
<p>Model Overview.We select three NSAI models for profiling analysis: an LNN on a logic program task (Riegel et al., 2020), an LTN on a binary classification task (Badreddine et al., 2022), and an NVSA (Hersche et al., 2023) on the Raven's Progressive Matrices task (Zhang et al., 2019), representing Neuro:Symbolicâ†’Neuro, Neuro Symbolic , and Neuro|Symbolic NSAI systems (Sec.2), respectively.The readers are referred to their references for more details.</p>
<p>Runtime Profiling Method.We first conduct functionlevel profiling to capture runtime statistics, and then use the PyTorch Profiler (Maxim Lukiyanov, Guoliang Hua, Geeta Chauhan, and Gisle Dankel, 2021) to measure the CPU and GPU runtimes of each model at a per-function granularity.The experiments are conducted on a system with an Intel Xeon Silver 4114 CPU and an Nvidia RTX 2080 Ti GPU.</p>
<p>Compute Operator Analysis Method.On top of the above profiling, we perform compute operator-level profiling for further analysis.We classify each neuro and symbolic workload of the LNN, LTN, and NVSA models into six operator categories: convolution, matrix multiplication (MatMul), vector/element-wise operation (e.g., tensor add, div, and norm), data transformation (e.g., reshape and transpose), data movement (e.g., inter-device transfer), and others (e.g., fuzzy logic, and logic rule) (Susskind et al., 2021).</p>
<p>NSAI Model Profiling Results</p>
<p>Runtime Breakdown.Compared to neuro workloads, symbolic workloads are not negligible in computing latency and may become a system bottleneck.Fig. 2(a) shows the runtime breakdown for each model, where the neuro (symbolic) workloads account for 54.6% (45.4%), 48.0% (52.0%), 7.9% (92.1%) runtime of the LNN, LTN, and NVSA models, respectively.Notably, the symbolic workload dominates the NVSA's runtime, predominately due to the sequential and computational-intensive rule detection during the involved reasoning procedure.This reasoning computation depends on the result of the frontend neuro workload and thus lies on the critical path during inference; Nevertheless, there are still opportunities for leveraging data pre-processing and parallel rule query to reduce this bottleneck.</p>
<p>Runtime Scalability.We observe that the neuro vs. symbolic runtime proportion shown in Fig. 2(a) remains relatively stable across various test sets under the same size, whereas the total runtime increases quadratically with the test set size.For example, when the test set size increases from 2Ã—2 to 3Ã—3, the symbolic workload runtime percentage only increases from 92.06% to 94.71%, but the total runtime of the NVSA model increases by 5.02Ã—, indicating the potential scalability bottleneck of NSAI models.0.00% LNN 0.51% 43.6% 16.4% 39.5% 0.00% 0.00% 0.00% 19.3% 17.3% 39.4% 24.0% 0.00% 62.5% 26.8% 7.20% 3.48% 0.00% 0.00% 0.00% 73.1% 2.40% 6.36% 18.1% 30.7% 34.8% 22.0% 3.11% 9.40% 0.00% 35.7% 0.52% 49.9% 6.82% 7.12% 0.00% backbone for feature extraction.By contrast, a large portion of LNN's (neuro) runtime is on element-wise operations due to the sparse syntax tree structure composed of proposition logic.Notably, data movement also takes up a significant amount of LNN's (neuro) runtime because of its unique bidirectional dataflow during reasoning inference.</p>
<p>Symbolic Workload Analysis.The symbolic workload is dominated by vector and scalar operations exhibiting low operational intensities and complex control flows.Both LNN (symbolic) and LTN (symbolic) have a large number of logic operations, posing parallelism optimization opportunities in their database queries and arithmetic operations, especially for larger symbolic models.Meanwhile, LNN (symbolic) is severally data movement-bounded due to its sparse and irregular memory accesses and bidirectional inference, where model-aware dataflow architecture would likely be beneficial for alleviating this bottleneck.Notably, the elementwise operations usually stem from high-dimensional distributed vector computations (e.g., binding, bundling, and permutation) for symbolic representation, which is difficult to process efficiently on GPUs.Therefore, the challenges of accelerating these computations will become increasingly important as the task and feature complexities further grow.</p>
<p>CHALLENGES AND OPPORTUNITIES</p>
<p>In this section, we discuss the challenges and opportunities for NSAI systems, and outlines our vision for the future, focusing on the system and architecture perspectives.</p>
<p>Building ImageNet-like NSAI datasets.NSAI systems hold great potential in achieving human-like AI (Booch et al., 2021).However, their current applications are limited to basic decision-making and reasoning problems (Garcez et al., 2022), falling short of the broader vision of human cognitive abilities, such as interpretability, deductive reasoning, systematicity, productivity, compositionality, inferential coherence of mental thought, and causal and counterfactual thinking.To significantly advance the metacognitive capabilities of NSAI systems, more challenging and suitable datasets are highly desirable to unleash NSAI's potential.</p>
<p>Unifying neuro-symbolic-probabilistic models.Integrating neural, symbolic, and probabilistic approaches offers promise to improve AI models' explainability and robustness.However, the current attempts to combine these complementary approaches are still in a nascent manner (Wang &amp; Yang, 2022) -how to integrate them in a principled manner remains a fundamental and open challenge.We envision a unified framework to design algorithms that opportunistically combine neural, symbolic, and probabilistic components, and for quantifying scaling laws for neuroprobabilistic-symbolic inference versus large neural models.</p>
<p>Developing efficient software frameworks.NSAI systems typically utilize underlying logic, such as fuzzy logic, parameterization, and differentiable structures, to support learning and reasoning capabilities.However, most NSAI system implementations create custom software for deduction for the particular logic used, which limits modularity and extensibility (Aditya et al., 2023).Therefore, new software frameworks are needed that can encompass a broad set of reasoning logical capabilities and provide practical syntactic and semantic extensions while being fast and memoryefficient.Moreover, new programming models, compilers, and runtimes that can facilitate the ease and efficient realization of the neuro-symbolic-probabilistic models are of significance to realize the full promise of NSAI paradigms.</p>
<p>Benchmarking diverse NSAI workloads.Given the proliferation of NSAI algorithms and the rapid advancements in hardware platforms, it is crucial to benchmark NSAI systems in a comparable, quantitative, and validatable manner.</p>
<p>To achieve this, from the system aspect, we need representative benchmarks that capture the essential workload characteristics (e.g., compute kernels, access patterns, and sparsity) of neural, symbolic, and probabilistic models, and that can be quantitatively tested in human-AI applications.Additionally, from an architectural and hardware perspective, we need modeling-simulation-characterization frameworks to enable the development of novel architectures for these workloads and build optimized modular blocks as libraries by leveraging workload characteristics, as other emerging domains (Krishnan et al., 2022a;Wan et al., 2022).Benchmarking NSAI will guide ML researchers and system architects in investigating the trade-offs in accuracy, performance, and efficiency of various NSAI algorithms, and in implementing systems in a performance-portable way.</p>
<p>Designing cognitive hardware architectures.NSAI workloads that combine neural, symbolic, and probabilistic methods feature much greater heterogeneity in compute kernels, sparsity, irregularity in access patterns, and higher memory intensity than current DNN workloads.This leads to an increasing divergence with the current hardware roadmap that largely focuses on matrix multiplication or nearest neighbor search, and regular dataflows, e.g., systolic arrays (Krishnan et al., 2022b) or compute-in-memory crossbars (Crafton et al., 2022).Therefore, we need novel architectures with dedicated processing units, memory hierarchies, and on-chip interconnects that can handle the additional complexities in computations and communications.Additionally, the architecture needs to provide flexibility with both configurable interconnects and full addressable memories to keep pace with NSAI algorithmic innovations.</p>
<p>CONCLUSION</p>
<p>NSAI is an emerging paradigm for next-generation efficient, robust, explainable, and cognitive AI systems.This paper systematically reviews recent NSAI algorithms, char-acterizes their system performance, analyzes their workload operators, and identifies the challenges and opportunities towards fullfiling next-generation NSAI systems.</p>
<p>Figure 1 .
1
Figure 1.Overview of neuro-symbolic AI systems, and challenges and opportunities in advancing next-generation cognitive AI.</p>
<p>Fig. 2
2
Fig. 2(b) partitions the neuro and symbolic workloads of the LNN, LTN, and NVSA models into six operator categories with runtime breakdown.Below is the workload analysis: Neuro Workload Analysis.The neuro workload is dominated by the MatMul and activation operations.LTN (neuro) is dominated by MatMul due to its heavy MLP components, while NVSA's (neuro) majority runtime is on MatMul and convolution because it adopts ResNet18 as the perception</p>
<p>Figure 2 .
2
Figure 2. (a) Runtime breakdown and (b) compute operation analysis for three neuro-symbolic AI models -LNN, LTN and NVSA.</p>
<p>Table 1 .
1
Survey of recent NSAI algorithms into five groups, with a summary of their underlying operations and vector formats.
CategoryNSAI AlgorithmUnderlying OperationIf VectorSymbolic[Neuro] AlphaGo (Silver et al., 2017)NN, MCTSVectorNVSA (Hersche et al., 2023)NN, mul, addVectorNeuPSLNeuro|Symbolic</p>
<p>Table 2 .
2
Enumeration of the underlying operations based on Tab. 1.
Underlying OperationsExamplesFuzzy logic(LTN)
ACKNOWLEDGEMENTSWe thank Sixu Li and Yang (Katie) Zhao for their technical support.This work was supported in part by CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.
D Aditya, K Mukherji, S Balasubramanian, A Chaudhary, P Shakarian, Pyreason, arXiv:2302.13482Software for open world temporal logic. 2023arXiv preprint</p>
<p>Logic tensor networks. S Badreddine, A D Garcez, L Serafini, M Spranger, Artificial Intelligence. 3031036492022</p>
<p>Thinking fast and slow in ai. G Booch, F Fabiano, L Horesh, K Kate, J Lenchner, N Linck, A Loreggia, K Murgesan, N Mattei, F Rossi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Improving compute in-memory ecc reliability with successive correction. B Crafton, Z Wan, S Spetalnick, J.-H Yoon, W Wu, C Tokunaga, V De, A Raychowdhury, Proceedings of the 59th ACM/IEEE Design Automation Conference. the 59th ACM/IEEE Design Automation Conference2022</p>
<p>Bridging machine learning and logical reasoning by abductive learning. W.-Z Dai, Q Xu, Y Yu, Z.-H Zhou, Advances in Neural Information Processing Systems. 201932</p>
<p>Neural logic machines. H Dong, J Mao, T Lin, C Wang, L Li, D Zhou, International Conference on Learning Representations. 2019</p>
<p>Explainable ai (xai): Core ideas, techniques, and solutions. R Dwivedi, D Dave, H Naik, S Singhal, R Omer, P Patel, B Qian, Z Wen, T Shah, G Morgan, 2023ACM Computing Surveys55</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 612018</p>
<p>Neurosymbolic ai: The 3 rd wave. A D Garcez, L C Lamb, Artificial Intelligence Review. 2023</p>
<p>Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. A D Garcez, M Gori, L C Lamb, L Serafini, M Spranger, S N Tran, arXiv:1905.060882019arXiv preprint</p>
<p>Neural-symbolic learning and reasoning: a survey and interpretation. Neuro-Symbolic Artificial Intelligence: The State of the Art. A D Garcez, S Bader, H Bowman, L C Lamb, L De Penning, B Illuminoo, H Poon, C G Zaverucha, 3422022</p>
<p>A neuro-vector-symbolic architecture for solving raven's progressive matrices. M Hersche, M Zeqiri, L Benini, A Sebastian, A Rahimi, Nature Machine Intelligence. 2023</p>
<p>Ontology reasoning with deep neural networks. P Hohenecker, T Lukas, Journal of Artificial Intelligence Research. 682020</p>
<p>Stratified ruleaware network for abstract visual reasoning. S Hu, Y Ma, X Liu, Y Wei, S Bai, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202135</p>
<p>Robert s. engelmore memorial lecture at aaai. H Kaut, 2020. 2020</p>
<p>Roofline model for uavs: A bottleneck analysis tool for onboard compute characterization of autonomous unmanned aerial vehicles. S Krishnan, Z Wan, K Bhardwaj, N Jadhav, A Faust, V J Reddi, 2022 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE2022a</p>
<p>Automatic domain-specific soc design for autonomous unmanned aerial vehicles. S Krishnan, Z Wan, K Bhardwaj, P Whatmough, A Faust, S Neuman, G.-Y Wei, D Brooks, V J Reddi, IEEE/ACM International Symposium on Microarchitecture. 2022 55th. 2022bIEEE</p>
<p>Graph neural networks meet neural-symbolic computing: A survey and perspective. L C Lamb, A Garcez, M Gori, M Prates, P Avelar, M Vardi, IJCAI-PRICAI 2020-29th International Joint Conference on Artificial Intelligence-Pacific Rim International Conference on Artificial Intelligence. 2020</p>
<p>Deep learning for symbolic mathematics. G Lample, F Charton, International Conference on Learning Representations. 2019</p>
<p>Neural probabilistic logic programming in deepproblog. R Manhaeve, S DumanÄiÄ‡, A Kimmig, T Demeester, L De Raedt, Artificial Intelligence. 2981035042021</p>
<p>The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. J Mao, C Gan, P Kohli, J B Tenenbaum, J Wu, International Conference on Learning Representations. 2019</p>
<p>Introducing PyTorch Profiler -the new and improved performance tool. Maxim Lukiyanov, Guoliang Hua, Geeta Chauhan, Gisle Dankel, 2021</p>
<p>C Pryor, C Dickens, E Augustine, A Albalak, W Wang, L Getoor, Neupsl, arXiv:2205.14268Neural probabilistic soft logic. 2022arXiv preprint</p>
<p>. R Riegel, A Gray, F Luus, N Khan, N Makondo, I Y Akhalwaya, H Qian, R Fagin, F Barahona, U Sharma, arXiv:2006.131552020arXiv preprint</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, arXiv:1712.018152017arXiv preprint</p>
<p>Neuro-symbolic ai: An emerging class of ai workloads and their characterization. Z Susskind, B Arden, L K John, P Stockton, E B John, arXiv:2109.061332021arXiv preprint</p>
<p>Analyzing and improving fault tolerance of learning-based navigation systems. Z Wan, A Anwar, Y.-S Hsiao, T Jia, V J Reddi, A Raychowdhury, 2021 58th ACM/IEEE Design Automation Conference (DAC). IEEE2021</p>
<p>Robotic computing on fpgas: Current progress, research challenges, and opportunities. Z Wan, A Lele, B Yu, S Liu, Y Wang, V J Reddi, C Hao, A Raychowdhury, 2022 IEEE 4th International Conference on Artificial Intelligence Circuits and Systems (AICAS). IEEE2022</p>
<p>Towards data-and knowledge-driven artificial intelligence: A survey on neuro-symbolic computing. W Wang, Y Yang, arXiv:2210.158892022arXiv preprint</p>
<p>Sustainable ai: Environmental implications, challenges and opportunities. C.-J Wu, R Raghavendra, U Gupta, B Acun, N Ardalani, K Maeng, G Chang, F Aga, J Huang, C Bai, Proceedings of Machine Learning and Systems. Machine Learning and Systems20224</p>
<p>Embracing neural networks into answer set programming. Z Yang, A Ishay, J Lee, Neurasp, 29th International Joint Conference on Artificial Intelligence (IJCAI 2020). 2020</p>
<p>Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. K Yi, J Wu, C Gan, A Torralba, P Kohli, J Tenenbaum, Advances in neural information processing systems. 312018</p>
<p>Clevrer: Collision events for video representation and reasoning. K Yi, C Gan, Y Li, P Kohli, J Wu, A Torralba, J B Tenenbaum, International Conference on Learning Representations. 2020</p>
<p>Raven: A dataset for relational and analogical visual reasoning. C Zhang, F Gao, B Jia, Y Zhu, S.-C Zhu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>H Zhang, T Yu, Alphazero, Deep Reinforcement Learning: Fundamentals, Research and Applications. 2020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7194 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7194</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7194</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-259077070</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2306.13089v3.pdf" target="_blank">GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning</a></p>
                <p><strong>Paper Abstract:</strong> Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7194.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7194.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GIMLET-graph-token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GIMLET unified graph-as-text tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph nodes are represented as tokens interleaved with text tokens and fed into a single transformer encoder; relative-position embeddings encode graph shortest-path distances and pooled edge features, with an attention mask to decouple graph encoding from instruction text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Unified graph-token sequence (GIMLET)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each graph node is represented as a token and concatenated with instruction text tokens to form a single token sequence. Relative position embeddings combine (a) sequential offsets for text tokens, (b) graph shortest-path distances for node-node positions, (c) a reserved CROSS token for graph↔text relative positions, and (d) mean-pooled edge-feature embeddings along shortest paths. A unidirectional attention mask prevents graph tokens from attending to text tokens (graph→text blocked) while allowing text tokens to attend to graph tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based sequentialization with structured relative-position bias</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Graph nodes emitted as tokens in a single sequence; relative-position embedding uses graph shortest-path distances and mean-pooled edge features on shortest paths; special CROSS token for cross-domain positions; attention masking (unidirectional) to decouple encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Pretraining: ChEMBL bioassay + ChEMBL-property; Downstream evaluation: PCBA, ChEMBL Zero-Shot, MoleculeNet tasks (BACE, HIV, MUV, Tox21, ToxCast, BBBP, CYP450), ESOL, FreeSolv, Lipo</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-based molecule property prediction (zero-shot and few-shot), regression and classification</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GIMLET (T5 backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder–decoder transformer (T5 backbone) extended to accept node tokens and text tokens together via generalized relative position embeddings; model sized ~64M parameters in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Classification: ROC-AUC; Regression: RMSE; also correct-format generation rate for regression outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Zero-shot ROC-AUC examples: BACE 0.6957, HIV 0.6624, MUV 0.6439 (Avg bio 0.6673); Tox21 0.6119, ToxCast 0.5904 (Avg tox 0.6011); BBBP 0.5939, CYP450 0.7125 (Avg pha 0.6532). Large-scale: Chembl Zero-Shot ROC-AUC 0.7860, PCBA 0.6211. Regression RMSE examples: ESOL 1.1321, Lipophilicity 1.3455, FreeSolv 5.1032 (Avg phy 2.527). GIMLET outputs correctly formatted numbers for >98% regression test samples.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables instruction-based supervised pretraining that transfers to strong zero-shot and few-shot performance across many molecular property tasks; produces correctly formatted regression numbers; improves generalization to unseen tasks and is robust to instruction rephrasings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper does not report per-graph token lengths or formal guarantees of lossless graph reconstruction; approach increases input sequence length (node tokens) which may raise token costs for large graphs; authors note they did not address tasks requiring structured outputs (e.g., molecule generation) in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to SMILES-based LMs and models that use an independent GNN encoder, GIMLET outperforms molecule-text baselines in instruction-based zero-shot learning and approaches supervised GNN performance on some tasks; ablation shows unified tokenization (no separate encoder) and decoupled attention give clear gains versus (a) 'w.o. unifying' variant (GNN as a single token) and (b) 'w.o. decoupling' (global attention). Authors claim GIMLET provides stronger capacity for graph structure by introducing graph inductive bias via relative positions and avoids increased parameters and training difficulty of separate encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7194.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7194.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SMILES-linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SMILES string linearization of molecular graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>SMILES linearizes a molecular graph into a textual string representation (atom tokens, bond symbols, branch/parens and ring numerals) which can be input to language models for joint molecule-text training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>SMILES linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Molecular graphs are serialized into SMILES strings (linear token sequence encoding atoms, bonds, branches, stereochemistry and ring closures), which are then tokenized and processed by language models as normal text sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential token-based linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>SMILES grammar linearization (standard SMILES encoding of molecular graph connectivity and stereochemistry) — used directly as input token sequence to language models</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used by baselines; evaluated on same downstream sets as GIMLET (PCBA, Chembl Zero-Shot, MoleculeNet tasks, ESOL, FreeSolv, Lipo, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Instruction-based zero-shot molecule property prediction (as baseline input representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>KVPLM, Galactica, MolT5 (SMILES-based language models cited in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Examples: KVPLM (~110M parameters) uses SMILES + text with masked language modeling; Galactica evaluated at 125M and 1.3B sizes and trained autoregressively on SMILES and text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ROC-AUC for classification; RMSE for regression</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Examples from baselines in paper (zero-shot): KVPLM: BACE 0.5126, HIV 0.6120, MUV 0.6172 (Avg bio 0.5806); Chembl Zero-Shot ROC-AUC 0.4155, PCBA 0.4811. Galactica-125M reported low/variable zero-shot performance; Galactica-1.3B had mixed results (see paper tables). Baselines struggled on many zero-shot tasks and often failed on regression outputs (unable to format numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>SMILES enables direct use of standard language models for molecules but (as reported) these SMILES-based language models underperform in instruction-based zero-shot molecular property tasks relative to GIMLET; some SMILES LMs also struggle to correctly produce formatted numeric regression outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper cites that SMILES or SMILES-based LM approaches have limited capacity to represent graph structural details important for property prediction and are insufficient for instruction-following zero-shot tasks; baselines using SMILES often fail on regression and instruction robustness compared to GIMLET.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>SMILES-based LMs (e.g., KVPLM, Galactica) perform worse than GIMLET in instruction-based zero-shot benchmarks; some SMILES models achieve decent performance on a few tasks but generally are less effective at following complex instructions and representing detailed graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7194.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7194.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GNN-as-token</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Individual encoding module (GNN-encoded dense vector) as language-model token</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach where a separate GNN or graph encoder computes a dense representation of the molecule, which is then inserted into a language model as a token/embedding to enable multimodal interaction with text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>GNN dense-vector token (individual encoding module)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph is encoded by a standalone GNN (message-passing or graph transformer) into a dense vector representation (optionally one vector per graph or per substructure) which is provided to the language model as an embedding token or tokens; downstream text and label interaction occurs in the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>dense-vector token-based (modal embedding), not pure textual sequence</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>GNN message-passing / graph transformer encodes graph to a fixed-size vector or a small set of vectors; these embeddings are injected into the language model input (commonly as special tokens or additional embeddings).</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Used by baselines and related works (MoMu, CLAMP) and evaluated on datasets in paper: PCBA, ChEMBL, MoleculeNet tasks, ESOL, FreeSolv, Lipo, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Contrastive pretraining, retrieval, captioning, and instruction-based zero-shot evaluation (baselines adapted to instruction framework in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MoMu (GNN-language contrastive model), CLAMP (contrastive multimodal model), other prior GNN+LM architectures</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MoMu (~113M) uses a GNN to encode graphs and contrastively align with text; CLAMP ensembles encoders with large sT5 LM variants (very large parameter counts reported for ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ROC-AUC for classification; RMSE for regression where applicable (MoMu cannot handle regression in its contrastive setup in reported experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>MoMu zero-shot examples: BACE 0.6656, HIV 0.5026, MUV 0.6051 (Avg bio 0.5911); MoMu showed reasonable performance on some tasks (e.g., tox21, bace) but failed on regression tasks and was outperformed by GIMLET on average.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides modality-specific inductive bias via a dedicated graph encoder, which can improve certain downstream tasks and retrieval but complicates end-to-end language-model instruction pretraining; in this study, these models underperformed GIMLET in instruction-following zero-shot tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper lists several drawbacks: (1) dense GNN vectors have limited capacity to convey detailed graph structure to a language model, (2) adding an extra encoder increases parameters and training cost, and (3) training deeper combined stacks can suffer vanishing-gradient issues, making end-to-end optimization harder. Also, contrastive GNN+text approaches (e.g., MoMu, CLAMP) cannot directly produce numeric regression outputs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to GIMLET's unified tokenization, individual-encoder approaches often underperform in instruction-based zero-shot settings because they separate modalities early and provide dense vectors that may lose fine-grained structural detail; however, individual encoders can be beneficial where modality-specific inductive bias is crucial and for tasks like retrieval where contrastive objectives are suitable.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text2mol: Cross-modal molecule retrieval with natural language queries. <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language. <em>(Rating: 2)</em></li>
                <li>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. <em>(Rating: 2)</em></li>
                <li>A molecular multimodal foundation model associating molecule graphs with natural language. <em>(Rating: 2)</em></li>
                <li>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. <em>(Rating: 1)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7194",
    "paper_id": "paper-259077070",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "GIMLET-graph-token",
            "name_full": "GIMLET unified graph-as-text tokenization",
            "brief_description": "Graph nodes are represented as tokens interleaved with text tokens and fed into a single transformer encoder; relative-position embeddings encode graph shortest-path distances and pooled edge features, with an attention mask to decouple graph encoding from instruction text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Unified graph-token sequence (GIMLET)",
            "representation_description": "Each graph node is represented as a token and concatenated with instruction text tokens to form a single token sequence. Relative position embeddings combine (a) sequential offsets for text tokens, (b) graph shortest-path distances for node-node positions, (c) a reserved CROSS token for graph↔text relative positions, and (d) mean-pooled edge-feature embeddings along shortest paths. A unidirectional attention mask prevents graph tokens from attending to text tokens (graph→text blocked) while allowing text tokens to attend to graph tokens.",
            "representation_type": "token-based sequentialization with structured relative-position bias",
            "encoding_method": "Graph nodes emitted as tokens in a single sequence; relative-position embedding uses graph shortest-path distances and mean-pooled edge features on shortest paths; special CROSS token for cross-domain positions; attention masking (unidirectional) to decouple encodings.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Pretraining: ChEMBL bioassay + ChEMBL-property; Downstream evaluation: PCBA, ChEMBL Zero-Shot, MoleculeNet tasks (BACE, HIV, MUV, Tox21, ToxCast, BBBP, CYP450), ESOL, FreeSolv, Lipo",
            "task_name": "Instruction-based molecule property prediction (zero-shot and few-shot), regression and classification",
            "model_name": "GIMLET (T5 backbone)",
            "model_description": "Encoder–decoder transformer (T5 backbone) extended to accept node tokens and text tokens together via generalized relative position embeddings; model sized ~64M parameters in experiments.",
            "performance_metric": "Classification: ROC-AUC; Regression: RMSE; also correct-format generation rate for regression outputs",
            "performance_value": "Zero-shot ROC-AUC examples: BACE 0.6957, HIV 0.6624, MUV 0.6439 (Avg bio 0.6673); Tox21 0.6119, ToxCast 0.5904 (Avg tox 0.6011); BBBP 0.5939, CYP450 0.7125 (Avg pha 0.6532). Large-scale: Chembl Zero-Shot ROC-AUC 0.7860, PCBA 0.6211. Regression RMSE examples: ESOL 1.1321, Lipophilicity 1.3455, FreeSolv 5.1032 (Avg phy 2.527). GIMLET outputs correctly formatted numbers for &gt;98% regression test samples.",
            "impact_on_training": "Enables instruction-based supervised pretraining that transfers to strong zero-shot and few-shot performance across many molecular property tasks; produces correctly formatted regression numbers; improves generalization to unseen tasks and is robust to instruction rephrasings.",
            "limitations": "Paper does not report per-graph token lengths or formal guarantees of lossless graph reconstruction; approach increases input sequence length (node tokens) which may raise token costs for large graphs; authors note they did not address tasks requiring structured outputs (e.g., molecule generation) in this work.",
            "comparison_with_other": "Compared to SMILES-based LMs and models that use an independent GNN encoder, GIMLET outperforms molecule-text baselines in instruction-based zero-shot learning and approaches supervised GNN performance on some tasks; ablation shows unified tokenization (no separate encoder) and decoupled attention give clear gains versus (a) 'w.o. unifying' variant (GNN as a single token) and (b) 'w.o. decoupling' (global attention). Authors claim GIMLET provides stronger capacity for graph structure by introducing graph inductive bias via relative positions and avoids increased parameters and training difficulty of separate encoders.",
            "uuid": "e7194.0",
            "source_info": {
                "paper_title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SMILES-linearization",
            "name_full": "SMILES string linearization of molecular graphs",
            "brief_description": "SMILES linearizes a molecular graph into a textual string representation (atom tokens, bond symbols, branch/parens and ring numerals) which can be input to language models for joint molecule-text training.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "SMILES linearization",
            "representation_description": "Molecular graphs are serialized into SMILES strings (linear token sequence encoding atoms, bonds, branches, stereochemistry and ring closures), which are then tokenized and processed by language models as normal text sequences.",
            "representation_type": "sequential token-based linearization",
            "encoding_method": "SMILES grammar linearization (standard SMILES encoding of molecular graph connectivity and stereochemistry) — used directly as input token sequence to language models",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used by baselines; evaluated on same downstream sets as GIMLET (PCBA, Chembl Zero-Shot, MoleculeNet tasks, ESOL, FreeSolv, Lipo, etc.)",
            "task_name": "Instruction-based zero-shot molecule property prediction (as baseline input representation)",
            "model_name": "KVPLM, Galactica, MolT5 (SMILES-based language models cited in paper)",
            "model_description": "Examples: KVPLM (~110M parameters) uses SMILES + text with masked language modeling; Galactica evaluated at 125M and 1.3B sizes and trained autoregressively on SMILES and text.",
            "performance_metric": "ROC-AUC for classification; RMSE for regression",
            "performance_value": "Examples from baselines in paper (zero-shot): KVPLM: BACE 0.5126, HIV 0.6120, MUV 0.6172 (Avg bio 0.5806); Chembl Zero-Shot ROC-AUC 0.4155, PCBA 0.4811. Galactica-125M reported low/variable zero-shot performance; Galactica-1.3B had mixed results (see paper tables). Baselines struggled on many zero-shot tasks and often failed on regression outputs (unable to format numbers).",
            "impact_on_training": "SMILES enables direct use of standard language models for molecules but (as reported) these SMILES-based language models underperform in instruction-based zero-shot molecular property tasks relative to GIMLET; some SMILES LMs also struggle to correctly produce formatted numeric regression outputs.",
            "limitations": "Paper cites that SMILES or SMILES-based LM approaches have limited capacity to represent graph structural details important for property prediction and are insufficient for instruction-following zero-shot tasks; baselines using SMILES often fail on regression and instruction robustness compared to GIMLET.",
            "comparison_with_other": "SMILES-based LMs (e.g., KVPLM, Galactica) perform worse than GIMLET in instruction-based zero-shot benchmarks; some SMILES models achieve decent performance on a few tasks but generally are less effective at following complex instructions and representing detailed graph structure.",
            "uuid": "e7194.1",
            "source_info": {
                "paper_title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GNN-as-token",
            "name_full": "Individual encoding module (GNN-encoded dense vector) as language-model token",
            "brief_description": "An approach where a separate GNN or graph encoder computes a dense representation of the molecule, which is then inserted into a language model as a token/embedding to enable multimodal interaction with text.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "GNN dense-vector token (individual encoding module)",
            "representation_description": "Graph is encoded by a standalone GNN (message-passing or graph transformer) into a dense vector representation (optionally one vector per graph or per substructure) which is provided to the language model as an embedding token or tokens; downstream text and label interaction occurs in the LM.",
            "representation_type": "dense-vector token-based (modal embedding), not pure textual sequence",
            "encoding_method": "GNN message-passing / graph transformer encodes graph to a fixed-size vector or a small set of vectors; these embeddings are injected into the language model input (commonly as special tokens or additional embeddings).",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Used by baselines and related works (MoMu, CLAMP) and evaluated on datasets in paper: PCBA, ChEMBL, MoleculeNet tasks, ESOL, FreeSolv, Lipo, etc.",
            "task_name": "Contrastive pretraining, retrieval, captioning, and instruction-based zero-shot evaluation (baselines adapted to instruction framework in this paper)",
            "model_name": "MoMu (GNN-language contrastive model), CLAMP (contrastive multimodal model), other prior GNN+LM architectures",
            "model_description": "MoMu (~113M) uses a GNN to encode graphs and contrastively align with text; CLAMP ensembles encoders with large sT5 LM variants (very large parameter counts reported for ensemble).",
            "performance_metric": "ROC-AUC for classification; RMSE for regression where applicable (MoMu cannot handle regression in its contrastive setup in reported experiments)",
            "performance_value": "MoMu zero-shot examples: BACE 0.6656, HIV 0.5026, MUV 0.6051 (Avg bio 0.5911); MoMu showed reasonable performance on some tasks (e.g., tox21, bace) but failed on regression tasks and was outperformed by GIMLET on average.",
            "impact_on_training": "Provides modality-specific inductive bias via a dedicated graph encoder, which can improve certain downstream tasks and retrieval but complicates end-to-end language-model instruction pretraining; in this study, these models underperformed GIMLET in instruction-following zero-shot tasks.",
            "limitations": "Paper lists several drawbacks: (1) dense GNN vectors have limited capacity to convey detailed graph structure to a language model, (2) adding an extra encoder increases parameters and training cost, and (3) training deeper combined stacks can suffer vanishing-gradient issues, making end-to-end optimization harder. Also, contrastive GNN+text approaches (e.g., MoMu, CLAMP) cannot directly produce numeric regression outputs in this study.",
            "comparison_with_other": "Compared to GIMLET's unified tokenization, individual-encoder approaches often underperform in instruction-based zero-shot settings because they separate modalities early and provide dense vectors that may lose fine-grained structural detail; however, individual encoders can be beneficial where modality-specific inductive bias is crucial and for tasks like retrieval where contrastive objectives are suitable.",
            "uuid": "e7194.2",
            "source_info": {
                "paper_title": "GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text2mol: Cross-modal molecule retrieval with natural language queries.",
            "rating": 2
        },
        {
            "paper_title": "Translation between molecules and natural language.",
            "rating": 2
        },
        {
            "paper_title": "Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery.",
            "rating": 2
        },
        {
            "paper_title": "A molecular multimodal foundation model associating molecule graphs with natural language.",
            "rating": 2
        },
        {
            "paper_title": "Smiles-bert: large scale unsupervised pre-training for molecular property prediction.",
            "rating": 1
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry.",
            "rating": 1
        }
    ],
    "cost": 0.015453499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning
22 Oct 2023</p>
<p>Haiteng Zhao 
Peking University</p>
<p>Shengchao Liu 
Mila</p>
<p>Chang Ma 
The University of Hong Kong</p>
<p>Hannan Xu 
University of Oxford</p>
<p>Jie Fu 
Hong Kong University of Science and Technology</p>
<p>Zhi-Hong Deng 
Peking University</p>
<p>Lingpeng Kong 
The University of Hong Kong</p>
<p>Qi Liu 
The University of Hong Kong</p>
<p>GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning
22 Oct 2023A3D669B70BB909FFCDE8085F879CEC83arXiv:2306.13089v3[cs.LG]
Molecule property prediction has gained significant attention in recent years.The main bottleneck is the label insufficiency caused by expensive lab experiments.In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting.We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs.To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data.By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules.GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks.We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions.We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks.Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv. 1</p>
<p>Introduction</p>
<p>Molecule machine learning has gained significant attention in recent years [10,52,85,77], including tasks like property prediction [14,21], molecule design [81,35,28,27], and others, which have broad applications in biomedical research.The primary method of molecule tasks is graph machine learning [21], where graphs are employed to represent molecule topology.Presently, graph machine learning approaches for molecules mainly follow the pretraining and finetuning paradigm [26,60,40,84].After the pretraining on large molecule corpus, models are able to encode informative molecule representations and generalize to downstream tasks by supervised finetuning.</p>
<p>One limitation of the supervised finetuning approach is the requirement on labeled data to acquire task-specific features for prediction on downstream tasks.This requirement poses a challenge in scenarios where obtaining labeled data is arduous or costly, especially in the field of molecules, and the training cost also restricts the efficiency of downstream tasks.Additionally, the supervised method exclusively relies on labeled data for acquiring task information, and is therefore unable to incorporate other additional information about the task.For instance, there exists a wealth of information regarding molecule assay tasks often provided in textual descriptions.Unfortunately, the supervised method fails to leverage such information, thereby restricting its flexibility.</p>
<p>In this work, we propose to investigate the feasibility of employing instructions to accomplish molecule-related tasks in a zero-shot setting.The instructions, also referred to as prompts [50,6], constitute natural language descriptions of the task to be executed.This is inspired by recent progress in natural language processing, where the large language models possess strong generalization performance to unseen tasks by following instructions [71,47].This approach eliminates the need for labeled data and leverages the textual knowledge available for downstream tasks.</p>
<p>Several studies have explored the molecule-text language models, employing either autoregressive pretraining using language models on both the text and molecule strings (SMILES) [16,85,64], or contrastive pretraining of molecule and text where molecules are encoded by Graph Neural Networks (GNN) [17,59,39,56].However, these works lack the investigation into instruct-based zero-shot for downstream tasks, and our experiments have empirically demonstrated their limited performance.</p>
<p>We conjecture that the constraints of current molecule-text language models are mainly imposed by their inadequate treatment of instructions during pretraining and their limited capacity to represent molecule structures.First, the pretraining corpora of these models lack task description about descriptions for abundant molecule tasks as well as the supervised signal.They mainly address the capacity of general molecule-related text understanding, which may be insufficient to capture details of complex instructions to molecule properties.Second, these models' capacity is limited in representing graph structures of molecules.Molecular tasks heavily rely on understanding the structure information, while existing multimodal methods either encode SMILES sequence representation of molecules by language model or encode molecular graphs into dense vector representation using GNN, which are not conducive for language models to gain a deep understanding of graph features.</p>
<p>To facilitate natural language instruction-based graph learning for zero-shot molecule tasks, we propose an innovative structured language model called GIMLET (Graph Instruction based MolecuLe zEro-shoT learning).First, GIMLET extends large language models for both graph and text data by applying transformer mechanisms with generalized position embedding, ensuring strong capacity for both the learning of graph structure representations and the executing of instruction texts without introducing additional graph encoding modules.We further enhance the model by decoupling the encoding of the graph from specific tasks via employing attention masks.This allows for improved generalization of graph features across novel tasks.</p>
<p>In addition to the advanced model architecture, our approach incorporates instruction-based pretraining on an extensive dataset of molecule tasks with textual instructions.We construct a dataset comprises of 2K tasks accompanied by corresponding instructions tailored for instruction-based molecule zero-shot learning framework.Throughout the pretraining process, molecule graphs are paired with natural language instructions for molecule-related tasks, and the supervision signal is provided to train the model in executing various molecule tasks.This methodology empowers GIMLET to comprehend specific instructions for downstream tasks expressed in natural language and transfer acquired knowledge to a broad range of tasks effectively.</p>
<p>Our experimental results show that GIMLET outperforms molecule-text baselines by a substantial margin in instruction-based zero-shot learning, which is closed to supervised GNN on tasks like muv and toxcast.Additionally, GIMLET also exhibits impressive performance in few-shot learning and demonstrates robustness to instruction.In summary, our contributions are as follows:</p>
<p>• We present comprehensive investigations of natural language instruction-based graph zeroshot learning for molecule tasks, to reduce reliance on labeled data and leverage task textual knowledge.To accomplish this framework, we construct a molecule dataset consisting of two thousand tasks with instructions derived from task descriptions, which is open-sourced.</p>
<p>• We propose GIMLET, which extends language models to handle graph and text data.By applying the transformer mechanism with generalized position embedding and decoupled attention, our model learns graph structure representations and executes instruction texts without additional graph encoding modules.Instruction-based pretraining is applied for GIMLET to comprehend specific instructions expressed in natural language and transfer to a broad range of zero-shot tasks.</p>
<p>• Our experiment results outperform other molecule-text models by a substantial margin and demonstrate promising results in instruction-based zero-shot molecule tasks, demonstrating the viability of this framework.Additionally, GIMLET exhibits impressive performance in few-shot learning and demonstrates robustness to instruction.</p>
<p>Related Work</p>
<p>Molecule Representation Learning One approach for molecular representation learning is utilizing language models to acquire molecular representations based on Simplified Molecular Input Line Entry System (SMILES) strings [68,10].Toward stronger capability to incorporate pertinent substructure information, Graph Neural Networks (GNNs) are proposed to model molecules as graphs [21,83,26,36].Existing GNNs follow the message-passing paradigm and suffer from problems like long-range dependency vanishing and over-smoothing.Recently, graph transformer [52,80] has been proposed to better encode structures of graphs, illustrating effectiveness in molecule tasks [42,31,8,88].</p>
<p>Molecule Pretraining To fully explore the inherent structural information of molecules on a large scale, significant efforts have been made to molecular pretraining.Supervised pretraining is commonly used for learning useful representations [26,80,62].As for unsupervised pretraining, one approach involved the generative strategy on molecular SMILES strings [68,25,10,3,53] and graph [26,37,38,52,87], which was followed by recent works adopting the contrastive paradigm to distinguish augmented views of the same graph and other graphs [67,60,24,83,82,63,78,19,61,70,76,69,40].</p>
<p>Besides the structure-only pretraining, a few recent works incorporate natural language into molecule pretraining.One class of method is the SMILES-based language model, including KVPLM [85] and MolT5 [16], which use SMILES strings and text for joint representation and translation.Another work Galactica [64] explored the multi-task molecule task learning with instruction.Some other works acquire advanced representations for molecules by GNN, such as Text2Mol [17], MoMu [59], MoleculeSTM [39], and CLAMP [56], trained by contrastive learning between molecule graph and text description for molecule retrieval and caption tasks.MoleculeSTM and CLAMP explored molecule editing and property prediction with text.However, previous works lack the investigation into instruct-based zero-shot scenarios for complex molecule tasks like property prediction.</p>
<p>Instruction-based Zero-Shot Learning Instruction-based zero-shot learning is an innovative approach that leverages natural language instructions to enable neural models to solve a variety of tasks [50,6,55,18,89,44,45,49].To enhance the model's ability to follow instructions, some researchers have employed instruction-based pretraining techniques [54,71,12,47], which explicitly train language models to solve tasks with instructions.Besides natural language processing, instruction-based zero-shot learning is also studied in multimodal domains like images [4,9,1,32].</p>
<p>Method</p>
<p>Problem Formulation and Framework Overview</p>
<p>The molecule task is denoted as τ and consists of a set of graph data and their corresponding labels
(G i , y τ i ).
A molecule graph G = (N, E, v, e) includes nodes N = {1, . . ., n} and edges E ⊂ N ×N , while v and e represent node features and edge features, respectively.The label y τ can take various forms, such as class labels or numerical values, depending on the type of task.The instruction of τ is denoted as T τ , which is a sentence [o 1 , . . ., o m ] describing the task.</p>
<p>In the supervised approach, a model F θ predicts the label ŷ given a graph G, i.e., ŷ = F θ τ (G), by supervised finetuning individually for each downstream task τ .The limitation is that it relies on labeled data to learn the corresponding parameters and output modules, and finetuning does not enable the model to effectively utilize extra task-related knowledge.</p>
<p>To overcome these limitations, our framework leverages natural language instructions T τ to provide the model with task information, as illustrated in Figure 1.Our zero-shot graph learning framework incorporates molecule graphs G and task instructions T τ into a graph-text language model GIMLET and decodes the output as text uniformly for different tasks, i.e. ŷstr = GIMLET(G, T τ ), where ŷstr is the label string.This description-based instruction framework empowers the model to handle a wide range of molecule tasks as long as they can be described in natural language, and the uniform text decoding approach accommodates various types of tasks including classification or regression.</p>
<p>Previous pretrained molecule-text models perform poorly in our framework due to the inadequate treatment of instructions and limited capacity for graphs.Our method addresses these from two aspects: First, we propose a unified language model GIMLET for both graph and text, towards the stronger capacity to represent molecules and instructions; Second, we adopt instruction-based pretraining to GIMLET, enabling generalization to new tasks based only on the instructions.</p>
<p>Unified Graph-Text Transformer</p>
<p>The common method for multimodal language models is obtaining the feature of the other modality by applying an additional encoding module, then integrating it into the language model, as in other molecule language models with GNN encoder [17,59,56].The individual encoding module benefits for decoupling the graph feature encoding from instructions, i.e., the features of graph data can be independent of task instructions in the early encoding stage, helping for generalization when the distribution of tasks changes.</p>
<p>However, for the molecule-text model, individual pre-encoding modules present problems.First, graph learning relies on structure information, but the dense vectors encoded by GNN have a limited capacity to carry structure information.Furthermore, training the additional module is difficult due to the increased layers, since deep transformers have vanishing gradients in early layers [34,2].Lastly, the additional modules increase parameters and training costs.</p>
<p>To overcome these issues, we propose a novel approach GIMLET which not only directly unifies the standard language model for graph and text without introducing additional graph encoder module, but also remains the decoupled graph encoding for better generalization.</p>
<p>Given graph G and text input T , to utilize GIMLET, we represent the graph nodes and text tokens as tokens.The resulting hidden states are denoted as H = [h 1 , . . ., h n , h n+1 , . . ., h n+m ] T ∈ R (n+m)×d h for corresponding n graph nodes and m text tokens.In this study, we choose T5 [50] as the backbone language model, due to the encoder-decoder architecture suitable for non-sequential encoding and text output.It utilizes the relative position embedding method [57] to represent sequential structure.In attention layer Attn with parameter
W V , W Q , W K ∈ R d h ×d k and W O ∈ R d k ×d h ,
relative position embedding for i-th and j-th token is formalized as
Âij = hiW Q hjW K T √ d k + b (i, j) , A = softmax( Â), Attn(H) = AHW V W O ,(1)
where b(i, j) is embedding of the relative distance between i and j, i.e. i − j for sequence.For graph-text joint data, we construct the position embedding b(i, j) in E.q. 1 by the conjunction of different types of distances.For the relative position of graph nodes, we adopt the graph shortest distance, which has been widely used in the literature of graph transformer [80,11,48].We also use unidirectional constraint in attention to decouple the graph encoding from the instruction.The overall form of position embedding for GIMLET is:
b(i, j) = b D POS(i,j) + b M i,j + Mean k∈SP(i,j) b E e k ,(2)
where b D , b M , and b E are position bias, masking, and path embedding, individually.The relative position POS in b D POS(i,j) is defined as the conjunction of different types of distances between tokens, which allows for the effective encoding of both graph and text data, as well as their interaction:
POS (i, j) =      i − j if n + 1 ≤ i, j ≤ n + m GRAPH SHORTEST DISTANCE(i, j) if 1 ≤ i, j ≤ n &lt; CROSS&gt; otherwise ,(3)
where <CROSS> is a special distance token held out for cross distance between graph and text tokens.b M i,j aims to represent the cross mask used to decompose graph encoding from text instructions.It imposes a unidirectional constraint from the graph to the text:
b M i,j = −∞ if i ≤ n and j &gt; n otherwise 0(4)
With the unidirectional constraint, graph tokens are limited to attending only to other graph tokens.</p>
<p>On the other hand, instruction tokens have the ability to receive information from both instructions and graphs.This approach allows us to separate the encoding of graphs from instructions, enabling instructions to selectively utilize graph features for various downstream tasks.</p>
<p>Finally, Mean k∈SP(i,j) b E e k is the mean pooling of the edge features b E e k in the shortest path SP(i, j) between node i and j, which is only defined between graph node tokens, as used along with graph shortest distance in [80].</p>
<p>The generalized position embedding is applied to the encoder transformer.During decoding, the decoder generates the answer based on the text features outputted by the encoder.</p>
<p>GIMLET unifies graph and text data by a single language model, which has the following merits: (i) In comparison to additional encoding module methods, GIMLET not only avoids the challenges of training additional front layers but also provides a stronger capacity for handling graph data by introducing graph inductive bias to the whole transformer.(ii) Our method leverages both the existing knowledge within the language model and facilitates learning to execute instruction-based graph tasks through standard instruction-based learning on the language model.This approach eliminates the need for additional modeling costs.(iii) The decomposed encoding of graph data retains the advantages of the individual encoding module, reducing task disturbance and ensuring that data features remain independent of task instructions.This enhances generalization for novel instructions.We validate these claims in experiments Subsection 4.3.Pretraining Our approach leverages the generalization capabilities acquired through learning from the instructions provided during pretraining, where comprehensive linguistic information and knowledge related to molecular tasks are learned from the provided instructions.The pretraining process is conducted in a supervised manner using task labels.The loss function for supervised pretraining can be formalized as follows:</p>
<p>Pretraining and Datasets
L = 1 τ |τ | τ (G i ,y τ str,i )∈τ − 1 |y τ str,i | log PGIMLET(y τ str,i |Gi, T τ ),(5)
where P GIMLET is the model likelihood for the label string, |y τ str,i | represents the label string token number aiming to normalize the loss of long sequences, and |τ | is the task sample number.The details of pretraining and downstream zero-shot testing are in Appendix.</p>
<p>Pretraining Dataset To effectively pretrain GIMLET for downstream tasks, it is crucial to include a large number of tasks to provide the model with an extensive task corpus.To this end, we select Chembl [20] as the pretraining dataset, which is widely used for supervised graph pretraining [26,62].It consists of 1,310 prediction target labels from biological assays for drug discovery.We divided 80% of the tasks and 80% of the molecules in random for pretraining, while the remaining non-overlapping tasks were reserved for zero-shot testing.Additionally, we constructed the Chembl-property dataset, which encompasses various physical and chemical properties available in the Chembl database [20] like molecule weights, log p values, hydrogen bound acceptor numbers, etc. Full details are in Appendix.We validate the effect of Chembl biological tasks and Chembl-property physico-chemical tasks in pretraining in Subsection 4.3.</p>
<p>Downstream Dataset</p>
<p>We target a large range of molecule tasks, as shown in Figure 2. First, we include large-scale datasets PCBA [72], which contains 128 biological assay tasks with abundant molecules.We also include the hold-out zero-shot set of Chembl, noted as Chembl Zero-Shot.These two datasets form a large-scale dataset.We also target tasks from MoleculeNet [75], a popular benchmark for molecule properties prediction.We adopt the dataset categorization method in [58] which classifies MoleculeNet datasets into four categories: Physico-chemical tasks, Bio-activity tasks, Toxicity tasks, and Pharmacokinetic tasks.Additional dataset CYP450 [33] is also included in the Pharmacokinetic tasks.These tasks cover diverse aspects of molecule properties, posing a significant challenge for a unified model to simultaneously handle them in a zero-shot fashion.</p>
<p>Instructions To provide essential background information and context for each task, we include task explanations and descriptions in our instructions.The description covers a wide range of aspects, including the family, function, and mechanism of the assay target, the assay experiment setting, the approximation method used for determining the property, and others.See examples in Figure 2. Instructions for all tasks are available in the code file.The task explanation is primarily sourced from websites and databases that introduce and compile the respective datasets, or relevant papers.Details are in Appendix.The descriptions are then concatenated with relevant questions as instructions.These instructions are subsequently reviewed and validated by a professional biology Ph.D. student.</p>
<p>Experiment</p>
<p>In the experiments, we investigate the following inquiries: (i) Can GIMLET effectively handle zeroshot molecule property tasks by instructions?(ii) Can GIMLET performs better by few-shot learning?(iii) What impact does model architecture have on the performance of GIMLET?(iv) How does pretraining affect the performance of GIMLET?(v) How does the form of instruction influence GIMLET for molecule zero-shot learning?</p>
<p>Instruction-Based Zero-Shot Learning</p>
<p>Baselines In the zero-shot setting, we compare GIMLET with three molecule-text models: SMILESbase language model KVPLM [85], Galactica [64], and GNN-language model MoMu [59].Other molecule-text models [16,17,39] either haven't released parameters or are difficult to handle zeroshot setting.Notably, the pretraining of Galactica includes the MoleculeNet datasets, which is thus not strictly zero-shot on some of our tasks.We report the result of all the baselines with our zero-shot learning framework and instructions.The details of baseline evaluation are in Appendix.</p>
<p>To establish upper bounds for task performance, we also illustrate the supervised results of popular graph models.For GNNs, we includes GCN [30], GAT [66], and GIN [79].We also include the graph transformer Graphormer [80].To mitigate the impact of our pretraining, we additionally perform supervised pretraining on Graphormer using our pretraining datasets, referred to as Graphormer-p.</p>
<p>Settings Following the standard supervised setting in previous studies [26], we adopt the Scaffold split [51] with a ratio of 0.8, 0.1, 0.1 for all the datasets, and report results on the testing sets, ensuring the comparability of our results to previous works.For classification tasks, we employ ROC-AUC as the evaluation metric, while for regression tasks, we utilize RMSE.</p>
<p>Results</p>
<p>We report the result of different types of downstream tasks in Table 1.The result of GIN, GCN and GAT for MoleculeNet are from [26] which we mark by italic.We observe that in the zero-shot setting, GIMLET outperforms most baselines on the majority of datasets, except for bbbp where GIMLET also performs comparably to the baselines.In terms of the average performance across task categories, GIMLET outperforms all baselines, demonstrating the effectiveness of our method for instruction-based zero-shot molecule tasks.It is worth noting that some of the baselines also achieve results on certain tasks.For example, KVPLM works on hiv, muv and bbbp, and MoMu works on tox21 and bace, showing our instruction-based molecule zero-shot learning method is a general framework to probe knowledge in molecule-text models.</p>
<p>Comparing our zero-shot performance to the supervised results, we observe that GIMLET achieves performance close to those of GNNs on several datasets, like bace, muv, toxcast, and bbbp.This demonstrates that GIMLET is able to solve molecule tasks in the zero-shot fashion nicely.</p>
<p>The results for large-scale molecule tasks are presented in Table 2.As depicted, the baselines struggle to handle these tasks.Our GIMLET not only successfully transfers to the Chembl Zero-Shot splits, where both the tasks and graphs were unseen during pretraining, but also demonstrates strong generalization performance on the PCBA benchmark.</p>
<p>The results of regression tasks are shown in Table 3.The scatter plots of the regression are in Appendix.It is worth noting that regression tasks pose greater challenges in the zero-shot setting than classification tasks, because it is difficult to determine unseen physico-chemical properties using only natural language, and the output space is also vast.Notably, the zero-shot baselines fail to perform the regression tasks due to their inability to output correctly formatted numbers.In contrast, GIMLET generates correctly formatted numbers for over 98% regression testing samples in all the tasks and showcases the potential of zero-shot regression tasks.</p>
<p>Instructions-Based Few-Shot Finetuning</p>
<p>We apply few-shot instruction-based tuning on the downstream tasks, to examine whether GIMLET exhibits improved performance in the presence of low-resource data.Notably, for datasets with more than one task, we do few-shot learning for every single task individually.We first split datasets into training, validation, and testing sets in the same as the zero-shot setting.Then K samples for each class are randomly sampled from the training set as the few-shot examples, where K is the few-shot number.We report the result of the best validation model on the testing set.The input is the same as the zero-shot setting, including molecule data and instructions.We only tune the last linear layer, to inspect whether the feature is discriminative and linear separable.Linear tuning is also low resource costing and avoids overfitting.The last linear mapping to vocabulary is tuned for GIMLET and KVPLM.For MoMu, we tune the last linear layer of the projection head for features.</p>
<p>The result is shown in Figure 3, and plots for each dataset are in Appendix.GIMLET outperforms other baselines consistently, exhibiting improved performance with an increasing number of few-shot samples.The performance of GIMLET also approaches the supervised GIN with only limited samples.</p>
<p>Baselines also achieve some performance gain in the few-shot training but remain beaten by GIMLET, and the improvements are not stable, fluctuating with the few-shot number.The few-shot results demonstrate the high-quality representation learned by GIMLET, which is well-suited for specific tasks, highlighting the potential of GIMLET in scenarios beyond zero-shot learning.</p>
<p>Figure 3: Few shot performance.Higher is better for bio, tox, and pha, and lower is better for phy.</p>
<p>Ablation and Exploration Studies</p>
<p>Effect of Model Design We investigate components of GIMLET to highlight their benefits.Specifically, we focus on two aspects: (a) To measure the effectiveness of the unified transformer method, we compare it to the variant version which obtains graph embedding by an individual GIN as a token in text, a common method in multimodal transformers.(b) We ablate graph decoupling encoding by complete global attention.We conduct pretraining and downstream zero-shot testing in the same setting as our method.The comparison is shown in Table 4. Compared to GIMLET, w.o.unifying perform worse on all the datasets, especially on Bio-activity and Pharmacokinetic Tasks.Next, the w.o.decoupling variation performs worse than GIMLET on most datasets.The decoupling significantly improves performance on Bio-activity and Pharmacokinetic tasks and is also comparable on Toxicity tasks.This proves our claims that the unified graph-text transformer not only avoids additional modules but also has a strong capacity for encoding graph data and generalizing across tasks.</p>
<p>Effect of Pretraining Our pretraining includes two large types of pretraining tasks, including Chembl bioactivity tasks and Chembl Property physico-chemical tasks.To validate the influence of each task type, we performed ablation experiments where each of them was excluded separately.The results are shown in Table 5, and the detailed result is in Appendix.Unsurprisingly, Chembl is essential for downstream molecule assay tasks, and Chembl property plays a crucial role in Physico-chemical tasks.However, the results also reveal that Chembl positively affects downstream Physico-chemical tasks, and Chembl property benefits Bio-activity tasks, Toxicity tasks, and Pharmacokinetic tasks.The results demonstrate the positive transfer of pretraining tasks on a diverse range of downstream tasks, spanning various types and domains.</p>
<p>Robustness to Instruction</p>
<p>We explore whether GIMLET is robust to the instructions.We rephrase our instructions by GPT-3.5-turbo for testing.Each instruction is rephrased using four types of requests: We plot the performance for each type of augmentation, as well as the average standard variation for each task in Figure 4.As shown, GIMLET is more robust than baselines on most tasks.This shows that our instruction-based pretaining enables GIMLET to focus on the task rather than the specific language form.Instruction Interpretability We ablate the explanation of the instructions in downstream tasks, to validate whether the explanation in instructions helps GIMLET perform downstream tasks.Without explanation, only the task name and question are provided to the model.The examples of ablated instructions are available in Appendix.The results presented in Table 6 demonstrate a significant drop in performance when task explanations are not included.This finding supports the effectiveness of the explanation and highlights the model's ability to comprehend the explanation of tasks.</p>
<p>Conclusion and Discussion</p>
<p>In this work, we propose nature language instruction-based graph zero-shot learning for molecule tasks, and construct a molecule dataset consisting of two thousand tasks with instructions derived from task descriptions.We propose GIMLET, which extends large language models to handle graph and text data by applying the transformer mechanism with generalized position embedding and decoupled attention.Instruction-based pretraining is applied for GIMLET.Experiments demonstrate promising results in zero-shot learning, exhibit strong robustness to the instruction, and can be further improved by few-shot tuning.We do not consider the tasks with structured output like molecule generation in this study, which is left for further work.</p>
<p>A Framework</p>
<p>A.1 Details of Datasets and Instructions The datasets used in our study are presented in Table 7.These datasets consist of different types of tasks related to molecule property prediction.It should be noted that during the pretraining phase, the loss function is not specific to the task types, but rather encompasses the generative loss of the language model.</p>
<p>We have chosen not to include certain datasets, namely SIDER and ClinTox, in our collection of datasets.The decision was based on the fact that the tasks associated with these datasets are not clearly defined and involve complex systemic phenomena, making it challenging to describe them through instructional texts.For instance, the ClinTox dataset involves determining whether drugs have passed the FDA approval, which is not an objective problem but rather a dynamic and intricate social phenomenon.The SIDER dataset focuses on describing the side effects of drugs on system organ classes, which have intricate mechanisms and a wide range of possible causes, making them difficult to be effectively conveyed through instructions.</p>
<p>For the Chembl property dataset that we have constructed, detailed information can be found in Table 8.These properties are sourced from the Chembl database [20] through the web API.The task explanation is primarily sourced from relevant papers, websites, or databases that introduce and compile the respective datasets.The specific sources utilized depend on the particular datasets under consideration.For Chembl tasks, we obtain task descriptions from the Chembl website.Descriptions for MoleculeNet tasks and PCBA are primarily sourced from the PubChem website.</p>
<p>Certain datasets, such as Toxcast, include task descriptions within the dataset files.In the case of other tasks, like Chembl property and Physical-Chemical tasks, instructions are derived from Wiki or other papers.We list the instruction source in Table 9.</p>
<p>The description covers a wide range of aspects, including the family, function, and mechanism of the assay target, the assay experiment setting, the approximation method used for determining the property, and others.We describe regression tasks by introducing the relasionship between the The instructions for each task are generated automatically by conducting searches on the databases and summarizing the descriptions.We use a mixture strategy of summarizing, combining template-based summarizing and GPT-3.5-turbo-basedsummarizing methods.The GPT-3.5-turbo-based summarizing method is applied by the prompt 'Summarize the assay: \n {Descriptions to be summarized}'.</p>
<p>The resulting instructions are then concatenated with relevant questions.These instructions are subsequently reviewed and validated by a professional biology Ph.D. student and slightly modified if necessary.</p>
<p>We then list the instructions of each dataset.For datasets with more than one task, we only list the instruction of one task as an illustration.</p>
<p>Chembl
"</p>
<p>Chembl property</p>
<p>" The partition coefficient , abbreviated P , is defined as a particular ratio of the concentrations of a solute between the two solvents ( a biphase of liquid phases ) , specifically for un -ionized solutes , and the logarithm of the ratio is thus Log P .When one of the solvents is water and the other is a non -polar solvent , then the log P value is a measure of lipophilicity or hydrophobicity .The defined precedent is for the lipophilic and hydrophilic phase types to always be in the numerator and denominator respectively .What is the logarithm of the partition coefficient of this molecule ?"</p>
<p>PCBA " The assay tests the inhibition of ALDH1A1 activity using propionaldehyde as an electron donor and NAD + as an electron acceptor .FreeSolv " The free energy of hydration can be approximated by \ u0394G_hyd = \ u0394G_solv , soln -\ u0394G_solv , gas + RT ln (10^( -pKa ) ) .Can you tell me the free energy of hydration ( by using the negative pka ) of this molecule , predicted by using \ u0394G_solv and negative pka ?"</p>
<p>Lipo " Lipophilicity is an important feature of drug molecules that affects both membrane permeability and solubility , measured by octanol / water distribution coefficient ( logD at pH 7.4) .What ' s the octanol / water distribution coefficient ( logD at pH 7.4) of this molecule ?"</p>
<p>A.2 Details of Framework Application</p>
<p>In our framework, we represent the labels of various tasks as strings.For assay tasks involving classification, the labels are converted to either "Yes" or "No" based on whether the molecule has an effect on the assay.In regression tasks, the labels are transformed into numerical strings.Integer values remain unchanged, while decimal numbers are rounded to two decimal places.</p>
<p>To conduct zero-shot testing on our model, we generate output sequences and extract the answer from the results.For assay classification, we consider the first token generated as the answer and use the scores for the 'Yes' and 'No' tokens to compute the ROC-AUC score for classification.In regression tasks, we extract the number from the generated sequence by performing string matching using a regular expression template: r"-?\d+.?\d<em>e??\d</em>?".Notably, we discovered that GIMLET consistently generates results in the correct format for all classification tasks and accurately formatted numbers for over 98% of regression testing samples, without any augmentation of restriction in the vocabulary.</p>
<p>A.3 Baselines Evaluation</p>
<p>For the baselines, we apply our instruction-based molecule zero-shot learning to their respective settings.KVPLM employs SMILES for molecule representation and utilizes masked language modeling for molecule-text data.Galactica also represents molecules using SMILES but generates the next sentence in an autoregressive manner.MoMu employs contrastive learning between the GNN-encoded molecule and the corresponding text, allowing it to score each candidate sentence for the target molecule and retrieve the best matching one.Our application of each baseline model aligns with their intended use.</p>
<p>It is important to note that for the baseline models, to avoid baselines generating answers in classification not in our parsing method ('Yes' and 'No'), we limit the vocabulary during generation to only include 'Yes' and 'No' in classification tasks.This restriction is achieved by utilizing the bias term in huggingface to prevent the generation of other words.However, it is worth mentioning that our model, GIMLET, does not require this augmentation and is able to generate the desired outputs without any additional constraints.</p>
<p>For KVPLM, we mask the answer position in the whole sentence for the model to predict.For example, for molecule CCOc1ccccc1-n1nnnc1SCC(=O)NC(=O)NCc1ccco1 and classification tasks ARE inhibitor, input to KVPLM is:
" CCOc1ccccc1 -n1nnnc1SCC (= O ) NC (= O ) NCc1ccco1
Oxidative stress has been implicated in the pathogenesis of a variety of diseases ranging from cancer to neurode generatio n .The antioxidant response element ( ARE ) signaling pathway is important in the amelioration of oxidative stress .Is this molecule agonists of antioxidant response element ( ARE ) signaling pathway ?[ MASK ]"</p>
<p>For Galactica, the answer is expected to be generated after reading the question.The input example is
"[ START_I_SMILES ] CCOc1ccccc1 -n1nnnc1SCC (= O ) NC (= O )</p>
<p>B Method B.1 Discussion of Individual Encoding Module Method</p>
<p>The individual encoding module-based multimodal language model can be formalized as LLM(M (G), T ), where M is the individual encoding module for graph data G.For example, the visual module is applied to pre-encode the image data to get the dense representation, then put into the language model as tokens embedding [4,9,1,32].Current works on molecule language models also use a GNN to get the representation of molecules to interact with the language models [17,59,56].</p>
<p>This method can be considered as decomposition of the conditional probability P (ŷ|G, T )
P (ŷ|G, T ) = P M (z|G)P LLM (ŷ|z, T )dz,(6)
based on the assumption that the feature distributions P (z|G) should be modeled by modality-specific modules to introduce inductive bias, and be independent of text information to help with adaptation to novel text data.</p>
<p>However, for the molecule-text model, individual pre-encoding modules present problems.First, graph learning relies on structure information, but the dense vectors encoded by GNN have a limited capacity to carry structure information, and language models don't have inductive bias toward graph structure.Furthermore, training the additional module is difficult due to the increased layers, since deep transformers have vanishing gradients in early layers [34,2], which is a well-known problem of transformer.Lastly, the additional modules increase parameters and training costs.</p>
<p>Our method GIMLET not only overcome these issues, our approach GIMLET not only directly unifies the standard language model for graph and text without introducing additional graph encoder module, but also remains the decoupled graph encoding for better generalization.</p>
<p>B.2 Model Theoretical Capacity</p>
<p>In this section, we analyze the theoretical capacity of our modeling method.</p>
<p>Theorem 1 Assume for different input features and position embeddings, the transformer layers can output different output features.The transformer with distance-based relative position embedding has a stronger capacity than the 1-WL test for the graph isomorphism problem.</p>
<p>Proof 1 The 1-WL test is defined as the following iteration:
χ 0 G (i) = hash(v i ) χ t G (i) := hash χ t−1 G (i), χ t−1 G (j) : j ∈ N G (i) (∀i ∈ N ),(7)
where χ G is the label in WL test, hash is the hash function, N G (i) is the neighbor of node i.</p>
<p>The transformer with distance-based relative position embedding can be considered as the following mapping:
χ t G (i) := hash d G (i, j), χ t−1 G (j) : j ∈ N = hash({(0, χ t−1 G (i))} ∪ {(1, χ t−1 G (j)) : j ∈ N G (i)} ∪ {(d G (i, k), χ t−1 G (k)) : k ∈ N − N G (i) − {i}})(8)
It can be seen that the iteration of the transformer with distance-based relative position embedding includes both the node i and its neighbors N G (i), marked by distance 0 and 1, respectively, ensuring the capacity is at least as strong as 1-WL test.It further includes other nodes far away, along with their distance, which constitutes a stronger capacity than 1-WL test.Figure 5 are two example graphs that cannot be distinguished by 1-WL test, but can be distinguished by transformer with distance-based relative position embedding.Proof 2 Because GIMLET decomposes the attention from graph nodes to text, the graph nodes can only attend to other graph nodes.Thus the encoding capacity of graph data is the same as a single transformer with distance-based relative position embedding for graph data.</p>
<p>Along with the assumption of transformer layers, GIMLET is able to distinguish graph-instruction pairs if graphs can be distinguished by transformer with distance-based relative position embedding, or instructions are different.</p>
<p>B.3 Detalied Related Work</p>
<p>We present a detailed related work here, due to the space limitation of paper.</p>
<p>Molecule Representation learning In recent years, there has been a growing interest in developing molecular representation learning for downstream tasks like drug discovery and other applications.</p>
<p>One approach that has received considerable attention is utilizing language modeling techniques to acquire molecular representations based on Simplified Molecular Input Line Entry System (SMILES) strings [68,10].Although sequence-based representations have demonstrated success in some applications, concerns have been raised about their capability to incorporate all pertinent substructure information.To address this limitation, some researchers have proposed the use of Graph Neural Networks (GNNs) to model molecules as graphs [21,83,26], potentially providing a more comprehensive and accurate representation of the molecular structure.</p>
<p>Existing GNNs follow the message-passing paradigm and suffer from problems like long-range dependency vanishing and over-smoothing.Recently, Graph Transformer [52,80] has been proposed to better encode structures of graphs.The Graph Transformer is inspired by the Transformer architecture, which has shown remarkable performance in natural language processing [65,13,41].The Graph Transformer extends the Transformer architecture to the graph domain, allowing the model to capture the global structure and long-range dependencies of the graph [86,15,31,29,74,48,42,80,8,43,11,5,23,88].</p>
<p>Molecule Pretraining To fully explore the inherent structural information of molecules on a large scale and transfer useful information to downstream tasks, significant efforts have been made to address the inadequacies in molecular pre-training.Supervised pretraining is commonly used for learning useful representations [26,80,62].As for unsupervised pretraining, one approach involved using an generative pre-training strategy on molecular SMILES strings [68,25,10,3,53] and Graph [26,37,52,87], which was followed by recent works adopting the contrastive paradigm that aligns representation of augmented views of the same graph but keeping views from other graphs away [67,60,24,83,82,63,78,19,61,70,76,69,40].</p>
<p>The pretraining methods mentioned focus on obtaining representations for supervised training.However, for natural language instruction-based zero-shot graph learning, it's necessary to incorporate natural language into the pretraining process.Several studies have explored molecule structure-text multimodal pretraining.One class of method is the SMILES based language model, including KVPLM [85] and MolT5 [16], which use SMILES strings and text for joint representation and translation.Another work Galactica [64] explored the multi-task molecule task learning with instruction.Some other works acquire advanced representations for molecules by GNN, such as Text2Mol [17], MoMu [59], MoleculeSTM [39], and CLAMP [56], trained by contrastive learning between molecule graph and text description for molecule retrieval and caption tasks.MoleculeSTM and CLAMP explored molecule editing and property prediction with instructions.However, none of these works address the zero-shot fashion on complex molecule tasks like property prediction, due to constraints imposed by the pretraining methodology that not addressing the instruction-following ability, and their model capacity for representing molecule graphs.</p>
<p>Instruction-based zero-shot learning Instruction-based zero-shot learning is an innovative approach that leverages natural language instructions and definitions to enable neural models to solve a variety of tasks [50,6,55,18,89,44,45,49].By providing a human-readable prompt, this method enables easier and more efficient specification of the learning task by utilizing knowledge about the task without data.To enhance the model's ability to follow instructions, some researchers have employed instruction-based pretraining techniques [54,71,12,47], which explicitly train language models to solve tasks with instructions.Besides natural language processing, instruction-based zero-shot learning is also studied in multimodal domains like images [4,9,1,32].</p>
<p>C Experiments C.1 Experiment setting</p>
<p>Our model only utilizes the basic features [26,62] of molecule graphs, which do not include additional features like ring markers.Specifically, it utilizes the first two dimensions of node features and the first two dimensions of edge features processed by ogb.smiles2graph.Therefore, the effectiveness of GIMLET predominantly stems from its architectural design and pretraining rather than the graph features it incorporates.</p>
<p>Following the standard supervised setting in previous studies [26], we utilize the scaffold strategy [51] to partition datasets into three subsets: the training set, validation set, and testing set with a ratio of 0.8, 0.1, 0.1.The scaffold strategy is a deterministic approach that involves sorting the data based on the scaffold, which represents the molecular structure.While this strategy aids in dataset partitioning, it can introduce a significant domain gap between the training and testing sets, thereby increasing the challenge of generalization.</p>
<p>For zero-shot, we report the results on the testing sets, ensuring the comparability of our results to previous works.For few-shot, we report the result of the best validation model on the testing set, the same as previous works and other supervised baselines [51].</p>
<p>Many datasets encompass multiple tasks.To evaluate these datasets, we conduct separate testing for each task, accompanied by their respective instructions.For datasets with multiple tasks, we report the average ROC-AUC score for each task, following the methodology established in previous works [26].The disparity between the multitask result and the tested result with our instructions is due to the gap between their instructions and ours, which indicates that Galactica relies on specific task instructions for task recognition, without a true understanding of the instructions.As a result, it exhibits poor generalization to other instruction forms.Note that Galactica even do not surpass KVPLM and MoMu which are also zero-shot learning methods.</p>
<p>GIMLET exhibits superior performance compared to the larger model CLAMP on the majority of datasets, with the exception of HIV.It is important to highlight that our model is significantly smaller in size than CLAMP, underscoring the effectiveness of our unified graph-text language model.Additionally, it should be noted that CLAMP lacks the capability to handle regression tasks due to its contrastive model architecture, whereas our encoder-decoder architecture enables us to successfully tackle a wide range of task types.</p>
<p>Significantly, the supervised results shed light on the task difficulties associated with each dataset.This showcases GIMLET's capability to effectively solve molecule tasks in a zero-shot manner, approaching the performance of supervised results.Furthermore, our pretraining tasks yield an average performance improvement of 3 percent for Graphormer, with the largest gains observed in Bioactivity tasks and the smallest in Toxicity tasks.This suggests that there still exist gaps between the pretraining data and our downstream tasks, addressing the zero-shot setting of our dataset.</p>
<p>In Figure 6, we present scatter plots comparing GIMLET with KVPLM and MoMu across all tasks.The diagonal line represents the equality line where x=y indicates our method outperforms the baseline.Notably, it is evident that GIMLET consistently performs significantly better than random guessing and surpasses the baselines on all tasks.</p>
<p>We plot the scatter of regression tasks in Figure 7.The plot clearly demonstrates a strong correlation between the predicted and actual values for ESOL and Lipo.</p>
<p>C.3 Detailed Few-Shot Results</p>
<p>In both classification tasks and regression tasks, we fine-tune the last linear layer of all models using their respective modeling loss.</p>
<p>It is important to note that the instruction-based few-shot approach is trained on each task individually, while supervised baselines are trained on multiple tasks from the dataset.Therefore, comparing these two approaches may not be strictly fair, as the multitask learning of the supervised baseline can contribute to improved task performance.</p>
<p>The results for few-shot learning on each dataset are presented in Figure 8.It is evident that, across the majority of datasets, GIMLET demonstrates improvement as the number of few-shot examples increases.In fact, it even outperforms or matches the performance of the supervised GIN on several datasets, such as bace, bbbp, and esol.There is also observable enhancement in performance across various datasets when employing few-shot learning, including tox21, toxcast, lipo, and freesolv.</p>
<p>There is not result of MoMu on regression tasks, because MoMu is a contrastive model between graph and text, which cannot handle regression tasks.</p>
<p>C.4 Detailed Ablation Results of Pretraining</p>
<p>The results of pretraining ablation for each dataset are presented in Table 13, 14, 15, and 16.The findings indicate that both bioactivity assay and physico-chemical properties offer significant benefits for all the downstream tasks, demonstrating positive transfer across different domains.</p>
<p>C.5 Instruction Robustness</p>
<p>To test the robustness of GIMLET, the Instructions are rephrased by GPT-3.5-turbo.There are four types of rephrasing, realized by the following prompts:   The C E E TO X _ H 2 9 5 R _ A N D R _ d n assay endpoint was analyzed in the positive fitting direction in relation to DMSO as the negative control and activity baseline .To understand the synthesis of Androstenedione in the H295R cell line after 48 hours of chemical exposure , loss -of -signal activity was used with HPLC -MS -MS technology .This endpoint is annotated to the steroid hormone intended target family to help other related targets , where the subfamily is androgens .Can it be determined if this particular molecule exhibits desirable efficacy to be utilized in this particular assay ?" detail " The CE ETOX_H29 5R_ANDR is an assay component that is one of the 23 components that are measured or calculated from the CEETOX_H295R assay .It is intended to measure hormone induction , which is a form of inducible reporter , and the measurement is done with the help of absorbance signals using HPLC -MS -MS technology .The data obtained from the measurement of assay component CEET OX_H295R_ ANDR is analyzed into two assay endpoints .One of these endpoints , CEETOX_H295R_ANDR_dn , is analyzed in the positive fitting direction , relative to DMSO , which is used as the negative control and baseline for activity .The HPLC -MS -MS technology is used to detect the loss -of -signal activity , which helps in understanding the synthesis of Androstenedione in H295R cell line after 48 hours of chemical exposure .To make the intended target more comprehensive and relatable to other targets , the assay endpoint is annotated to the steroid hormone intended target family , where the subfamily is androgens .Can this molecule be used for this assay ?"</p>
<p>C.6 Instruction Ablation</p>
<p>To ablate the explanation-based instruction, we remove the explanation and only keep the assay name.The ablated instruction for the instruction above is:</p>
<p>" The assay name is C EETOX_H29 5R_ANDR .Is this molecule effective to this assay ?"</p>
<p>C.7 Attention Visualization</p>
<p>We present visualizations of the attention of text tokens to molecule graphs, demonstrating how our unified transformer incorporates molecule information using various instructions.We randomly sample molecules and attention heads for visualization.To emphasize high-level features, we focus on visualizing the attention patterns of the last layer.The redder means the larger attention value.</p>
<p>For BACE instruction, we visualize the attention of several keywords marked in red to molecules: "BACE1 is an aspartic-acid protease important in the pathogenesis of Alzheimer's disease, and in the formation of myelin sheaths.BACE1 is a member of family of aspartic proteases.Same as other aspartic proteases, BACE1 is a bilobal enzyme, each lobe contributing a catalytic Asp residue, with an extended active site cleft localized between the two lobes of the molecule.The assay tests whether the molecule can bind to the BACE1 protein.Is this molecule effective to the assay?"</p>
<p>Figure 1 :
1
Figure 1: Our framework handles molecule tasks in the zero-shot fashion by natural language instruction.Within GIMLET, we employ distance-based joint position embedding to encode graphs and instruction texts.Additionally, we utilize attention masks to decouple the graph encoding process.</p>
<p>Figure 2 :
2
Figure 2: (Left) Illustration of datasets.Circle size corresponds to task number.Tasks are organized by category.Tasks on the top are more related to biological assay, on the bottom need more chemical and physical properties.GIMLET is trained on pretraining tasks, then tested on downstream tasks in the zero-shot setting.(Right) Our task instructions contain task explanations and questions.Pretraining Our approach leverages the generalization capabilities acquired through learning from the instructions provided during pretraining, where comprehensive linguistic information and knowledge related to molecular tasks are learned from the provided instructions.The pretraining process is</p>
<p>Figure 4 :
4
Figure 4: Robustness to instruction.</p>
<p>Figure 5 :Theorem 2
52
Figure 5: Two example graphs that cannot be distinguished by 1-WL test, but can be distinguished by transformer with distance-based relative position embedding.</p>
<p>Figure 6 :
6
Figure 6: Scatter of GIMLET over baselines.Below the diagonal line x=y means our method performs better.</p>
<p>Figure 7 :
7
Figure 7: Scatter of GIMLET on generative tasks.</p>
<p>rewrite'Figure 8 :
8
Figure 8: Few-shot performance on each dataset</p>
<p>short"</p>
<p>CEETOX_H 295R_AND R is one of 23 components in the CEETOX_H295R assay , measuring hormone induction detected with absorbance signals by HPLC -MS -MS .It ' s analyzed into 2 endpoints , with C E E T O X _ H 2 9 5 R_ A N D R _ d n being the positive fitting direction relative to the negative control .It analyzes the loss -of -signal activity to understand Androstenedione synthesis in H295R cell line after 48 hr chemical exposure .It ' s annotated as a steroid hormone intended target in androgens sub -family .Is molecule suitable for assay ?"</p>
<p>Figure 10 :(
10
Figure 10: Visualization of attention for BACE on molecule O=C(NCC1CCCCC1)CCc1cc2c(nc1N)cccc2</p>
<p>Figure 11 :
11
Figure 11: Visualization of attention for BACE on molecule O=C1NC(CN1Cc1ccccc1)(Cc1ccccc1)C(=O)NC(Cc1ccccc1)C(O)C[NH2+]Cc1cc(N(C)C)ccc1</p>
<p>Figure 12 :(
12
Figure 12: Visualization of attention for BACE on molecule O=C1N(C)C(=[NH2+])NC1(c1ccccc1)c1ccccc1</p>
<p>Figure 13 :
13
Figure 13: Visualization of attention for BACE on molecule O(c1cc2CN(C(CCC(=O)N(C)C3CCCCC3)C3CCCCC3)C(=[NH+]c2cc1)N)c1ccccc1</p>
<p>Figure 14 :
14
Figure 14: Visualization of attention for BBBP on molecule [NH]C(CC(C)C(<a href="C(C)(C)C">N@@</a>C(N)(C)N)(C)C)c1c(c(c[nH+][o+]1)C)[O-]</p>
<p>Figure 15 :
15
Figure 15: Visualization of attention for BBBP on molecule Cc1nccc2c1[nH]c3ccccc23</p>
<p>Figure 16 :
16
Figure 16: Visualization of attention for BBBP on molecule CC1=C2NC3=CC(=O)C=CC3=C2C=CN1</p>
<p>Figure 17 :
17
Figure 17: Visualization of attention for BBBP on molecule COc1cc2CCN(C)C3CC4(C=CC(=O)C=C4)c(c1O)c23</p>
<p>Figure 18 :
18
Figure 18: Visualization of attention for BBBP on molecule CCC1(C)CC(=O)NC1=O</p>
<p>Table 1 :
1
Zero-shot performance (ROC-AUC) over Bio-activity, Toxicity, and Pharmacokinetic tasks.
Method#ParamTypebacehivmuv Avg. bio tox21 toxcast Avg. tox bbbp cyp450 Avg. phaKVPLM110M0.5126 0.6120 0.6172 0.5806 0.4917 0.5096 0.5007 0.6020 0.5922 0.5971MoMu113M0.6656 0.5026 0.6051 0.5911 0.5757 0.5238 0.5498 0.4981 0.5798 0.5390Galactica-125M 125MZero Shot0.4451 0.3671 0.4986 0.4369 0.4964 0.5106 0.5035 0.6052 0.5369 0.5711Galactica-1.3B 1.3B0.5648 0.3385 0.5715 0.4916 0.4946 0.5123 0.5035 0.5394 0.4686 0.5040GIMLET (Ours) 64M0.6957 0.6624 0.6439 0.6673 0.6119 0.5904 0.6011 0.5939 0.7125 0.6532GCN0.5M0.736 0.757 0.732 0.742 0.749 0.633 0.691 0.649 0.8041 0.7266GAT1.0M0.697 0.729 0.666 0.697 0.754 0.646 0.700 0.662 0.8281 0.7451GIN1.8MSupervised0.701 0.753 0.718 0.724 0.740 0.634 0.687 0.658 0.8205 0.7392Graphormer48M0.7760 0.7452 0.7061 0.7424 0.7589 0.6470 0.7029 0.7015 0.8436 0.7725Graphormer-p 48M0.8575 0.7788 0.7480 0.7948 0.7729 0.6649 0.7189 0.7163 0.8877 0.8020</p>
<p>Table 2 :
2
Zero-shot performance (ROC-AUC) over large scale molecule tasks.
MethodChembl Zero-Shot PCBAKVPLM0.41550.4811MoMu0.50020.5150Galactica-125M0.64610.4800Galactica-1.3B0.48180.5202GIMLET (Ours)0.78600.6211</p>
<p>Table 3 :
3
Zero-Shot performance (RMSE) on Physicalchemical datasets.
MethodTypeESOL Lipophilicity FreeSolv Avg. phyKVPLM----MoMuZero Shot----GIMLET (Ours)1.1321.3455.1032.527GCN1.3310.7602.1191.403GAT1.2530.7702.4931.505GINSupervised1.2430.7812.8711.632Graphormer0.9010.7402.2101.284Graphormer-p0.8040.6751.8501.110</p>
<p>Table 4 :
4
Ablation study on GIMLET module.
Methodbacehivmuv Avg. bio tox21 toxcast Avg. tox bbbp cyp450 Avg. phaw.o. unifying0.4319 0.6133 0.6067 0.5506 0.5922 0.5537 0.5730 0.5309 0.6206 0.5758w.o. decoupling 0.6458 0.6406 0.5421 0.6095 0.6306 0.5954 0.6130 0.5666 0.6320 0.5993GIMLET0.6957 0.6624 0.6439 0.6673 0.6119 0.5904 0.6011 0.5939 0.7125 0.6532</p>
<p>Table 5 :
5
Ablation study on GIMLET pretraining.
bioactivity assay only0.64020.60710.5676-physico-chemical only0.48940.54540.47482.6178both0.66730.65320.60112.5266
MethodAvg.bio ↑ Avg.pha ↑ Avg.tox ↑ Avg.phy ↓ rewriting, detailing, expanding, and shortening.The prompts and examples are provided in Appendix.</p>
<p>Table 6 :
6
Ablation study on GIMLET instructions.
Methodbacehivmuv Avg. bio tox21 toxcast Avg. tox bbbp cyp450 Avg. phaname only0.5416 0.6132 0.6441 0.5996 0.5809 0.5279 0.5544 0.4871 0.6669 0.5770+ explanation 0.6957 0.6624 0.6439 0.6673 0.6119 0.5904 0.6011 0.5939 0.7125 0.6532</p>
<p>Table 7 :
7
Data Overview
SplittingData ClassDatasetNo. of Molecules No. of Tasks Task Metric Task TypePretrainingBioactivity assay ChEMBL bioassay activity dataset Physico-chemical CHEMBL Property365065 3650651048 13ROC_AUC Classification RMSE RegressionLarge ScalePCBA PubChem HTS bioAssay ChEMBL Zero-Shot437929 91266128 262ROC-AUC ROC_AUC Classification ClassificationPharmacokineticCYP inhibition BBBP Blood-brain barrier penetration16896 20395 1ROC_AUC Classification ROC_AUC ClassificationMUV PubChem bioAssay9308717ROC_AUC ClassificationDownstream Zero-ShotBio-activityBACE-1 benchmark set HIV replication inhibition1513 411271 1ROC_AUC Classification ROC_AUC ClassificationToxicityTox21Toxicology in the 21st century Toxcast7831 859812 617ROC_AUC Classification ROC_AUC ClassificationESOL Water solubility11281RMSERegressionPhysico-chemicalFreeSolv Solvation free energy6421RMSERegressionLipo Lipophilicity42001RMSERegression</p>
<p>Table 8 :
8
Chembl property tasks and labels
PropertyLabel typeAromatic rings numberIntegercx_logd distribution coefficientRealcx_logp partition coefficientRealcx_most_apka − log 10 dissociation constantRealMolecular massesRealHydrogen bond donor numberIntegerHeavy atom numberIntegerLipinski's rule of five violation numberIntegerPolar surface area (PSA)RealQuantitative Estimate of Druglikeness (QED) RealRule of three passesBoolRotatable bond numberInteger</p>
<p>Table 9 :
9
Data sources and classes for different stages of the model
Dataset
task property and other properties, i.e. how to estimate these properties by other ones.However, this method is still challenging due to the model's capacity to understand complex mathematical relationships.</p>
<p>Table 13 :
13
Pretraining ablation study on Bio-activity tasks
bacehivmuvAverage_biobioactivity assay only0.63900.67720.60440.6402physico-chemical only0.46480.54610.45720.4894both0.69570.66240.64390.6673</p>
<p>Table 14 :
14
Pretraining ablation study on Toxicity tasks
tox21toxcastAverage_toxbioactivity assay only0.57260.56250.5676physico-chemical only0.44780.50170.4748both0.61190.59040.6011</p>
<p>Table 15 :
15
Pretraining ablation study on Pharmacokinetic tasks
bbbpcyp450Average_phabioactivity assay only0.53130.68290.6071physico-chemical only0.59320.49760.5454both0.59390.71250.6532</p>
<p>Table 16 :
16
Pretraining ablation study on Physical-chemical tasks The assay is specifically designed to measure hormone induction , which is a form of inducible reporter , and is detected using absorbance signals by means of HPLC -MS -MS technology .The data obtained from the CEETO X_H295R_ ANDR assay component was analyzed into two assay endpoints .
esolfreesolvlipoAverage_phybioactivity assay only---physico-chemical only1.18225.29351.37782.6178both1.13205.10271.34502.5266' Rephrase the text of the following prompt shorter : \n 'Given a task instruction, we rephrase the instruction by the prompts above. Here is an example offour types of rephrased task instruction from Toxcast:origin" CEETOX_H295R_ANDR , is one of 23 assay component ( s ) measuredor calculated from the CEETOX_H295R assay . It is designed tomake measurements of hormone induction , a form of induciblereporter , as detected with absorbance signals by HPLC -MS -MStechnology . Data from the assay component CE ETOX_H29 5R_ANDR wasanalyzed into 2 assay endpoints . This assay endpoint ,CEETOX_H295R_ANDR_dn , was analyzed in the positive fittingdirection relative to DMSO as the negative control andbaseline of activity . Using a type of inducible reporter ,loss -of -signal activity using HPLC -MS -MS was used tounderstand synthesis of Androstenedione in H295R cell line at48 hr of chemical exposure . To generalize the intended targetto other relatable targets , this assay endpoint is annotatedto the steroid hormone intended target family , where thesubfamily is androgens . Is this molecule effective to thisassay ?"rewrite" The CE ETOX_H29 5R_ANDR is a component of the CEETOX_H295Rassay , which measures hormone induction through absorbancesignals detected by HPLC -MS -MS technology . The data fromCEETO X_H295R_ ANDR has been analyzed into two endpoints , withC E E T O X _ H 2 9 5 R_ A N D R _ d n being analyzed in relation to DMSO as thenegative control and baseline of activity . HPLC -MS -MS was usedto detect loss -of -signal activity and understand the synthesisof Androstenedione in H295R cell line after 48 hours ofchemical exposure . This assay endpoint is related to thesteroid hormone intended target family , specifically thesubfamily of androgens , and can be generalized to othersimilar targets . Can this assay be effectively performed usingthis molecule ?"expand" The CE ETOX_H29 5R_ANDR assay component is just one of the 23assay components that are measured or calculated from theCEETOX_H295R assay .
The code, model, and data are available at https://github.com/zhao-ht/GIMLET. 37th Conference on Neural Information Processing Systems (NeurIPS
).
Ethical ConsiderationThe work presented here is centered on a paradigm shift in molecule property prediction tasks, transitioning from traditional supervised learning to instruction-based zero-shot learning.Due to the fact that molecule property prediction has been widely explored in prior research, the direct societal impacts may appear limited.However, indirect negative impacts could be caused by excessive reliance on algorithms.We contend that a combination of model predictions and experimental validation is essential for practical implementation.C.2 Detailed Zero-Shot ResultWe list the full zero-shot result of GIMLET and baselines inTable 10,11,and 12.The standard deviation for supervised results are denoted after ±, and the multi-task setting results of Galactica are denoted in parentheses with italic.We also include the instruction-based zero-shot result reported in recent baseline CLAMP[56]which is tested by their instruction, denoted by italics too.CLAMP is a contrastive pretrained model with ensembled encoders for molecule and text.The parameter number for CLAMP's result is not clearly stated in their paper but should be larger than 10B as they use sT5 language model[50]XXL variant (11B) as one of the ensembled language models.The result in parentheses represents the outcome of the multitask setting, also referred to as weakly supervised in the original paper, where the same instructions are used for both pretraining and testing.While Galactica has been exposed to the same task instructions, it actually employs multitask learning with instructions serving as task identity.Even in comparison to Galactica's multitask result, GIMLET demonstrates comparable or superior performance on most datasets.This highlights the ability of GIMLET to perform zero-shot tasks with high quality.For BBBP instruction:'In general, molecules that passively diffuse across the brain blood barrier have the molecular weight less than 500, with a LogP of 2-4, and no more than five hydrogen bond donors or acceptors.Does the molecule adhere to the three rules or not?'
Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>Rezero is all you need: Fast convergence at large depth. T Bachlechner, B P Majumder, H Mao, G Cottrell, J Mcauley, Uncertainty in Artificial Intelligence. PMLR2021</p>
<p>Molgpt: molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P Vinod, U D Priyakumar, Journal of Chemical Information and Modeling. 6292021</p>
<p>Beit: Bert pre-training of image transformers. H Bao, L Dong, S Piao, F Wei, arXiv:2106.082542021arXiv preprint</p>
<p>Investigating expressiveness of transformer in spectral domain for graphs. A Bastos, A Nadgeri, K Singh, H Kanezashi, T Suzumura, I O Mulang, arXiv:2201.093322022arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Theoretical pka calculations with continuum model solvents, alternative protocols to thermodynamic cycles. R Casasnovas, J Ortega-Castro, J Frau, J Donoso, F Munoz, International Journal of Quantum Chemistry. 114202014</p>
<p>Structure-aware transformer for graph representation learning. D Chen, L O'bray, K Borgwardt, International Conference on Machine Learning. PMLR2022a</p>
<p>X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, arXiv:2209.06794Pali: A jointly-scaled multilingual language-image model. 2022barXiv preprint</p>
<p>Chemberta: Large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, arXiv:2010.098852020arXiv preprint</p>
<p>From block-toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked transformers. K Choromanski, H Lin, H Chen, T Zhang, A Sehanobish, V Likhosherstov, J Parker-Holder, T Sarlos, A Weller, T Weingarten, International Conference on Machine Learning. PMLR2022</p>
<p>H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. D K Duvenaud, D Maclaurin, J Iparraguirre, R Bombarell, T Hirzel, A Aspuru-Guzik, R P Adams, Advances in neural information processing systems. 201528</p>
<p>A generalization of transformer networks to graphs. V P Dwivedi, X Bresson, arXiv:2012.096992020arXiv preprint</p>
<p>C Edwards, T Lai, K Ros, G Honke, Ji , H , arXiv:2204.11817Translation between molecules and natural language. 2022arXiv preprint</p>
<p>Text2mol: Cross-modal molecule retrieval with natural language queries. C Edwards, C Zhai, Ji , H , Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>The turking test: Can language models understand instructions?. A Efrat, O Levy, arXiv:2010.119822020arXiv preprint</p>
<p>Molecular contrastive learning with chemical element knowledge graph. Y Fang, Q Zhang, H Yang, X Zhuang, S Deng, W Zhang, M Qin, Z Chen, X Fan, H Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Chembl: a large-scale bioactivity database for drug discovery. A Gaulton, L J Bellis, A P Bento, J Chambers, M Davies, A Hersey, Y Light, S Mcglinchey, D Michalovich, B Al-Lazikani, Nucleic acids research. 40D12012</p>
<p>Neural message passing for quantum chemistry. J Gilmer, S S Schoenholz, P F Riley, O Vinyals, G E Dahl, International conference on machine learning. PMLR2017</p>
<p>Central nervous system delivery of molecules across the blood-brain barrier. F Gosselet, R A Loiola, A Roig, A Rosell, M Culot, Neurochemistry International. 1441049522021</p>
<p>Unleashing the power of transformer for graphs. L Guo, Q Zhang, H Chen, arXiv:2202.105812022arXiv preprint</p>
<p>Contrastive multi-view representation learning on graphs. K Hassani, A H Khasahmadi, International conference on machine learning. PMLR2020</p>
<p>Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. S Honda, S Shi, H R Ueda, arXiv:1911.047382019arXiv preprint</p>
<p>W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, J Leskovec, arXiv:1905.12265Strategies for pre-training graph neural networks. 2019arXiv preprint</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Machine Learning: Science and Technology. 31150222022</p>
<p>Hierarchical generation of molecular graphs using structural motifs. W Jin, R Barzilay, T Jaakkola, International conference on machine learning. PMLR2020</p>
<p>J Kim, T D Nguyen, S Min, S Cho, M Lee, H Lee, S Hong, arXiv:2207.02505Pure transformers are powerful graph learners. 2022arXiv preprint</p>
<p>Semi-supervised classification with graph convolutional networks. T N Kipf, M Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Rethinking graph transformers with spectral attention. D Kreuzer, D Beaini, W Hamilton, V Létourneau, P Tossou, Advances in Neural Information Processing Systems. 342021</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, D Li, S Savarese, S Hoi, arXiv:2301.125972023arXiv preprint</p>
<p>Prediction of human cytochrome p450 inhibition using a multitask deep autoencoder neural network. X Li, Y Xu, L Lai, J Pei, Molecular Pharmaceutics. 15102018</p>
<p>Understanding the difficulty of training transformers. L Liu, X Liu, J Gao, W Chen, J Han, 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics2020. 2020</p>
<p>Constrained graph variational autoencoders for molecule design. Q Liu, M Allamanis, M Brockschmidt, A Gaunt, Advances in neural information processing systems. 201831</p>
<p>Hyperbolic graph neural networks. Q Liu, M Nickel, D Kiela, Advances in neural information processing systems. 2019a32</p>
<p>N-gram graph: Simple unsupervised representation for graphs, with applications to molecules. S Liu, M F Demirel, Y Liang, Advances in neural information processing systems. 2019b32</p>
<p>Molecular geometry pretraining with SE(3)-invariant denoising distance matching. S Liu, H Guo, J Tang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>S Liu, W Nie, C Wang, J Lu, Z Qiao, L Liu, J Tang, C Xiao, A Anandkumar, arXiv:2212.10789Multimodal molecule structure-text model for text-based retrieval and editing. 2022arXiv preprint</p>
<p>Pre-training molecular graph representation with 3d geometry. S Liu, H Wang, W Liu, J Lasenby, H Guo, J Tang, International Conference on Learning Representations. 2021</p>
<p>Y Liu, M Ott, N Goyal, J Du, M Joshi, D Chen, O Levy, M Lewis, L Zettlemoyer, V Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019carXiv preprint</p>
<p>Ł Maziarka, T Danel, S Mucha, K Rataj, J Tabor, S Jastrzkebski, arXiv:2002.08264Molecule attention transformer. 2020arXiv preprint</p>
<p>G Mialon, D Chen, M Selosse, J Mairal, arXiv:2106.05667Graphit: Encoding graph structure in transformers. 2021arXiv preprint</p>
<p>S Mishra, D Khashabi, C Baral, Y Choi, H Hajishirzi, arXiv:2109.07830Reframing instructional prompts to gptk's language. 2021arXiv preprint</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Molecular strategies to inhibit hiv-1 replication. M H Nielsen, F S Pedersen, J Kjems, Retrovirology. 22005</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Grpe: Relative positional encoding for graph transformer. W Park, W.-G Chang, D Lee, J Kim, ICLR2022 Machine Learning for Drug Discovery. 2022</p>
<p>In-boxbart: Get instructions into biomedical multi-task learning. M Parmar, S Mishra, M Purohit, M Luo, M Mohammad, C Baral, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Deep learning for the life sciences: applying deep learning to genomics, microscopy, drug discovery, and more. B Ramsundar, P Eastman, P Walters, V Pande, 2019O'Reilly Media</p>
<p>Self-supervised graph transformer on large-scale molecular data. Y Rong, Y Bian, T Xu, W Xie, Y Wei, W Huang, J Huang, Advances in Neural Information Processing Systems. 202033</p>
<p>Molformer: Large scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, 2022</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, arXiv:2110.082072021arXiv preprint</p>
<p>Exploiting cloze-questions for few-shot text classification and natural language inference. T Schick, H Schütze, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Enhancing activity prediction models in drug discovery with the ability to understand human language. P Seidl, A Vall, S Hochreiter, G Klambauer, arXiv:2303.033632023arXiv preprint</p>
<p>Self-attention with relative position representations. P Shaw, J Uszkoreit, A Vaswani, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterHuman Language Technologies20182</p>
<p>Out-of-the-box deep learning prediction of pharmaceutical properties by broadly learned knowledge-based molecular representations. W X Shen, X Zeng, F Zhu, Y L Wang, C Qin, Y Tan, Y Y Jiang, Y Z Chen, Nature Machine Intelligence. 342021</p>
<p>B Su, D Du, Z Yang, Y Zhou, J Li, A Rao, H Sun, Z Lu, J.-R Wen, arXiv:2209.05481A molecular multimodal foundation model associating molecule graphs with natural language. 2022arXiv preprint</p>
<p>Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. F.-Y Sun, J Hoffman, V Verma, J Tang, International Conference on Learning Representations. 2020</p>
<p>Mocl: Data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. M Sun, J Xing, H Wang, B Chen, J Zhou, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining. the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining2021</p>
<p>Does gnn pretraining help molecular representation?. R Sun, H Dai, A W Yu, Advances in Neural Information Processing Systems. 202235</p>
<p>Adversarial graph augmentation to improve graph contrastive learning. S Suresh, P Li, C Hao, J Neville, Advances in Neural Information Processing Systems. 202134</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, Advances in neural information processing systems. 201730</p>
<p>P Veličković, G Cucurull, A Casanova, A Romero, P Lio, Y Bengio, arXiv:1710.10903Graph attention networks. 2017arXiv preprint</p>
<p>P Velickovic, W Fedus, W L Hamilton, P Liò, Y Bengio, R D Hjelm, Deep graph infomax. ICLR (Poster). 201924</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics. the 10th ACM international conference on bioinformatics, computational biology and health informatics2019</p>
<p>Improving molecular contrastive learning via faulty negative mitigation and decomposed fragment contrast. Y Wang, R Magar, C Liang, A Barati Farimani, Journal of Chemical Information and Modeling. 62112022a</p>
<p>Molecular graph contrastive learning with parameterized explainable augmentations. Y Wang, Y Min, E Shao, J Wu, 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE2021</p>
<p>Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Naik, A Ashok, A S Dhanasekaran, A Arunkumar, D Stap, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022b</p>
<p>Pubchem's bioassay database. Y Wang, J Xiao, T O Suzek, J Zhang, J Wang, Z Zhou, L Han, K Karapetyan, S Dracheva, B A Shoemaker, Nucleic acids research. 40D12012</p>
<p>Matched molecular pair analysis on large melting point datasets: a big data perspective. M Withnall, H Chen, I V Tetko, ChemMedChem. 1362018</p>
<p>Representing long-range context for graph neural networks with global attention. Z Wu, P Jain, M Wright, A Mirhoseini, J E Gonzalez, I Stoica, Advances in Neural Information Processing Systems. 342021</p>
<p>Moleculenet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, K Leswing, V Pande, Chemical science. 922018</p>
<p>Simgrace: A simple framework for graph contrastive learning without data augmentation. J Xia, L Wu, J Chen, B Hu, S Z Li, Proceedings of the ACM Web Conference 2022. the ACM Web Conference 20222022a</p>
<p>Mole-bert: Rethinking pre-training graph neural networks for molecules. J Xia, C Zhao, B Hu, Z Gao, C Tan, Y Liu, S Li, S Z Li, 2022b</p>
<p>Infogcl: Information-aware graph contrastive learning. D Xu, W Cheng, D Luo, H Chen, X Zhang, Advances in Neural Information Processing Systems. 202134</p>
<p>K Xu, W Hu, J Leskovec, S Jegelka, arXiv:1810.00826How powerful are graph neural networks?. 2018arXiv preprint</p>
<p>Do transformers really perform badly for graph representation?. C Ying, T Cai, S Luo, S Zheng, G Ke, D He, Y Shen, T.-Y Liu, Advances in Neural Information Processing Systems. 342021</p>
<p>Graph convolutional policy network for goal-directed molecular graph generation. J You, B Liu, Z Ying, V Pande, J Leskovec, Advances in neural information processing systems. 201831</p>
<p>Graph contrastive learning automated. Y You, T Chen, Y Shen, Z Wang, International Conference on Machine Learning. PMLR2021</p>
<p>Graph contrastive learning with augmentations. Y You, T Chen, Y Sui, T Chen, Z Wang, Y Shen, Advances in neural information processing systems. 202033</p>
<p>Pre-training via denoising for molecular property prediction. S Zaidi, M Schaarschmidt, J Martens, H Kim, Y W Teh, A Sanchez-Gonzalez, P Battaglia, R Pascanu, J Godwin, arXiv:2206.001332022arXiv preprint</p>
<p>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Z Zeng, Y Yao, Z Liu, M Sun, Nature communications. 1318622022</p>
<p>Graph-bert: Only attention is needed for learning graph representations. J Zhang, H Zhang, C Xia, L Sun, arXiv:2001.051402020arXiv preprint</p>
<p>Motif-based graph self-supervised learning for molecular property prediction. Z Zhang, Q Liu, H Wang, C Lu, C.-K Lee, Advances in Neural Information Processing Systems. 202134</p>
<p>Are more layers beneficial to graph transformers?. H Zhao, S Ma, D Zhang, Z.-H Deng, F Wei, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. R Zhong, K Lee, Z Zhang, D Klein, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
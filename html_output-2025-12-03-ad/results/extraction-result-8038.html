<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8038 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8038</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8038</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-145.html">extraction-schema-145</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <p><strong>Paper ID:</strong> paper-f13b251c8346bc3be19b71b840449831e9716999</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f13b251c8346bc3be19b71b840449831e9716999" target="_blank">SciFact-Open: Towards open-domain scientific claim verification</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> SciFact-Open is presented, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts, and it is found that systems developed on smaller corpora struggle to generalize to SciFact- open, exhibiting performance drops of at least 15 F1.</p>
                <p><strong>Paper Abstract:</strong> While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFact-Open, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-Open, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-Open reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8038.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8038.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-domain test collection introduced in this paper for scientific claim verification: 279 claims verified against a 500K-abstract corpus with pooled human-annotated evidence labels (SUPPORTS / REFUTES / NEI).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / scientific claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Open-domain claim verification test collection (pooled annotation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Evaluate systems by retrieving candidate abstracts from a 500K research-abstract corpus and comparing system-predicted evidentiary claim/abstract pairs (CAPs) to a pooled set of human-annotated ECAPs collected via TREC-style pooling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 (abstract-level label-only), Average Precision (area under precision-recall curve)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = TP / (TP+FP); Recall = TP / (TP+FN); F1 = 2*(Precision*Recall)/(Precision+Recall). Average precision = area under precision-recall curve across ranked CAPs. (Abstract-level label-only F1 follows Wadden et al. (2020) convention.)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Evidence annotations collected by three professional annotators (biology-related undergraduate backgrounds); each CAP randomly assigned to one annotator; potential-evidence CAPs labeled and then checked by a second annotator; disagreements resolved via discussion; annotators trained with an author.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>System F1 drops of 15–30 points relative to SCIFACT-ORIG; example system F1s on SCIFACT-OPEN: VERT5ERINI 36.4, PARAGRAPHJOINT 50.3, MULTiVERS 52.4 (see Table 5). Average precision values reported per model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Pooling uses a limited set of systems and finite pool depth (d=250), so dataset may miss evidence found by unseen models; single retrieval pipeline used; exhaustive annotation infeasible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8038.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCIFACT-ORIG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCIFACT (original dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The original SciFact dataset (Wadden et al., 2020) of claims verified against roughly 5K abstracts; used here as the starting point and to supply existing evidence annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fact or Fiction: Verifying Scientific Claims</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical / scientific claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Small-corpus claim verification benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Verify claims against a small (~5K) corpus of abstracts with labeled evidentiary CAPs; used to train and evaluate models in prior work and as baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1 (abstract-level), rationale-level metrics in original dataset (rationales not required in SCIFACT-OPEN)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As above: Precision, Recall, F1 computed over identified evidentiary abstracts; original dataset also required sentence-level rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCIFACT-ORIG</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Original SCIFACT collected sentence-level rationales; referenced here but SCIFACT-OPEN omits rationale requirement due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Models often approach human agreement on SCIFACT-ORIG; specific baseline metrics reported in prior work (e.g., ~68–72 F1 for top systems on SCIFACT-ORIG).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Small corpus size (5K abstracts) does not reflect open-domain retrieval; may bias models to high-performance that does not generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8038.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pooling (TREC-style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TREC-style pooling (pooled data collection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pooling methodology adapted from TREC: union the top-d most-confident CAP predictions from multiple systems, annotate that pool, and treat un-annotated CAPs as NEI for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TREC: Experiment and Evaluation in Information Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pooled relevance annotation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Collect candidate items by pooling top-ranked outputs from multiple systems and have humans annotate those; assume un-pooled items are non-relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Coverage of relevant items; stability of evaluation metrics (F1 / average precision) as pool depth or system count increases</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Pool depth d = number of top predictions per system included; system count n = number of systems pooled. Metrics measured as changes in discovered ECAPs and model F1 as d or n increases.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>applied to SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotators labeled pooled CAPs; double-annotation for potential-evidence CAPs; pool depth set to d=250 and n=4 systems for collection.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Pool depth d=250 and n=4 produced diminishing returns: further increases yield <2% changes in F1 and few new ECAPs; total annotated CAPs = 732.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Pooling with few systems may miss evidence found by other models; assumption that un-annotated CAPs are NEI may not fully hold.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8038.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 (Probabilistic Relevance Framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lexical information retrieval ranking function used to retrieve candidate abstracts per claim prior to re-ranking; used as first-stage retrieval for all models in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Probabilistic Relevance Framework: BM25 and Beyond</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bag-of-words lexical retrieval (BM25)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrieve top-k documents per claim by lexical matching score (BM25), then feed retrieved set to re-ranker and labeler.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Retrieval rank distribution of ECAPs (counts by retrieval rank)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>k = number of abstracts retrieved per claim (k=50); analysis reports fraction of ECAPs having retrieval rank ≤ 20 or ≤ 40.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>used with SCIFACT-OPEN retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Majority of evidentiary abstracts ranked among top 20 retrievals; choosing k=50 kept most ECAPs while limiting irrelevant candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Lexical retrieval may miss semantically relevant abstracts; dense retrieval was evaluated but performed worse in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8038.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural re-ranker (VERT5ERINI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VerT5erini (BM25 + neural re-ranker + T5 labeling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-and-generation pipeline: BM25 first-stage retrieval, neural reranker trained on MS-MARCO, then T5-based rationale selection and label prediction; used as both a pooling and evaluation system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific Claim Verification with VerT5erini</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VERT5ERINI</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific claim verification / biomedical NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>claim verification model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pipeline retrieval + T5-based labeler</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Retrieve with BM25, rerank retrieved abstracts using a neural reranker trained on MS-MARCO, select rationales and predict labels via T5-3B models.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1, Average Precision (area under PR curve)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As above: precision/recall/F1 on abstract-level labeled ECAPs; average precision from model confidence-ranked CAPs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on SCIFACT-ORIG and SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Predictions from this model were pooled and human-annotated as part of SCIFACT-OPEN construction.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On SCIFACT-OPEN: P=25.0, R=67.2, F1=36.4 (std devs reported via bootstrap). Inclusion in pooling raised its measured F1 by ~16% compared to excluded-pool evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>High recall relative to precision on SCIFACT-OPEN; performance sensitive to negative sampling differences across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8038.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PARAGRAPHJOINT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ParagraphJoint</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-based model that jointly predicts sentence-level rationales and abstract-level SUPPORTS/REFUTES/NEI labels, used for pooling and evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PARAGRAPHJOINT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific claim verification / biomedical NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>claim verification model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>RoBERTa-based joint rationale+label model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Encode claim + full abstract via RoBERTa (512-token truncation), select rationales via attention, predict label from selected sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1, Average Precision</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on SCIFACT-ORIG and SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used as one of the n=4 systems for pooling; its top-d predictions annotated.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On SCIFACT-OPEN: P=54.7, R=46.5, F1=50.3; exclusion from pooling reduced measured F1 by ~15.9%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Performance sensitive to training negative-sampling ratio; truncation to 512 tokens may limit full-abstract context.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8038.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MULTiVERS / MULTiVERS10</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MultiVerS (and MultiVerS_10)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Longformer-based multitask model that encodes full abstracts and claims to jointly predict rationales and claim labels; MULTiVERS_10 is variant trained with fewer negative samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MultiVerS: Improving scientific claim verification with weak supervision and full-document context</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MULTiVERS</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific claim verification / biomedical NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>claim verification model</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Longformer-based full-document joint labeling</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Encode claim and full abstract with Longformer to accommodate long inputs; multitask objective predicts abstract-level label and sentence-level rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1, Average Precision</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Evaluated on SCIFACT-ORIG and SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Used in pooling; variant MULTiVERS_10 included to diversify negative-sampling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>On SCIFACT-OPEN: MULTiVERS P=73.6, R=40.7, F1=52.4; MULTiVERS_10 P=49.6, R=53.0, F1=51.3. MULTiVERS excluded-from-pool F1 changed little, indicating overfit to SCIFACT-ORIG.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Overfitting to SCIFACT-ORIG noted; negative sampling ratio strongly affects precision/recall trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8038.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Negative sampling (r)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Negative sampling ratio (r)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training procedure where for each positive evidentiary CAP, r irrelevant (NEI) CAPs are included as negatives; used to control model precision/recall behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / information retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Negative sampling hyperparameter</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>During training, include r NEI examples per positive to shape classifier behavior (can use random or hard negatives). Affects precision/recall trade-off and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Effect on Precision / Recall / F1 on held-out corpora</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Compare resulting precision/recall/F1 on SCIFACT-OPEN vs SCIFACT-ORIG as a function of r; higher r tends to increase precision at cost of recall.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to models trained on SCIFACT-ORIG and evaluated on SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Negative sampling rate had large impact on open-domain generalization; systems with different r showed >15 F1 differences. Example: MULTiVERS (r=20) vs MULTiVERS_10 (r=10) behavior differed markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Increases training dataset size and time by factor r; choice of hard vs random negatives matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8038.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation metrics (P/R/F1 / Avg. Precision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Precision, Recall, F1, Average Precision (area under PR curve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Primary evaluation metrics used to quantify system ability to identify and correctly label evidentiary abstracts per claim in SCIFACT-OPEN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fact or Fiction: Verifying Scientific Claims</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval / classification evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Precision/Recall/F1 and Average Precision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute precision, recall, and F1 over predicted ECAPs vs. human-annotated ECAPs; average precision summarizes performance across confidence thresholds (area under precision-recall curve).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision, Recall, F1, Average Precision</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Precision = TP/(TP+FP); Recall = TP/(TP+FN); F1 = harmonic mean of precision and recall; Average Precision = integral under precision-recall curve (scalar).</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>SCIFACT-OPEN (and SCIFACT-ORIG for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Confidence-calibrated predictions required for average precision; human annotations supply ground-truth ECAPs for metric computation.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Models drop 15–30 F1 when moving from SCIFACT-ORIG to SCIFACT-OPEN; per-model AP values reported (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Metrics sensitive to pooling coverage; assumption that un-pooled CAPs are NEI can bias recall estimates; negative sampling affects precision/recall trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8038.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Average precision (AP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Average precision (area under precision-recall curve)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ranking-oriented metric reported to summarize model performance across confidence thresholds for CAP ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Average precision</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute area under the precision-recall curve across model confidence-ranked CAPs to summarize trade-offs without fixing a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average Precision (scalar)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Sum over precision at each relevant retrieved instance weighted by change in recall; equivalent to area under the precision-recall curve.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Computed on SCIFACT-OPEN predictions (requires model confidence scores)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Average precision varies by system; used alongside F1 to rank models. VERT5ERINI AP reported low (e.g., ~27.5), others higher (see Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Cannot be computed for systems without calibrated confidence outputs (ARSJOINT excluded).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8038.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bootstrap resampling (uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrap-resampled uncertainty estimates</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uncertainty estimation via bootstrap resampling of claims (1,000 iterations) used to compute standard deviations for reported metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>statistical evaluation / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Bootstrap resampling over claims</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Resample claims with replacement 1,000 times; for each resampled dataset evaluate model metrics and compute standard deviation across iterations as uncertainty estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Standard deviation (uncertainty) for Precision / Recall / F1 / AP</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Compute metric per bootstrap sample; reported uncertainty = standard deviation across bootstrap samples.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to SCIFACT-OPEN evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Standard deviations reported for per-model metrics (subscripts in Table 5) computed over 1,000 bootstrap-resampled claim sets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Bootstrap reflects variance due to claim sampling but not necessarily labeling noise or missing pooled evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8038.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jaccard similarity (overlap)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jaccard similarity for ECAP overlap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Measure used to quantify overlap among ECAPs predicted by different systems (intersection over union of predicted ECAP sets).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>set-overlap analysis / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Jaccard similarity of predicted ECAP sets</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Compute Jaccard index between pairs of systems' predicted evidentiary abstracts to assess agreement, separately for SCIFACT-ORIG and SCIFACT-OPEN additions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Jaccard similarity (0–1)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>J(A,B) = |A ∩ B| / |A ∪ B| where A and B are sets of predicted ECAPs by two systems.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to model predictions on SCIFACT-ORIG and SCIFACT-OPEN</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>High overlap (≥0.5) on SCIFACT-ORIG abstracts, low overlap (≤0.2) on SCIFACT-OPEN added abstracts (Figure 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Low overlap indicates models identify different information in novel corpora; overlap measures do not capture correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8038.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>S2ORC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>S2ORC: The Semantic Scholar Open Research Corpus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large heterogeneous corpus of research articles used as the source for SCIFACT-OPEN's 500K-abstract corpus after filtering for medicine/biology and citation connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>S2ORC: The Semantic Scholar Open Research Corpus</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>scientific literature corpus (biomedicine broadly)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Corpus source for open-domain evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Filter S2ORC for medicine/biology and samples with at least one inbound and outbound citation, randomly sample 500K abstracts (including SCIFACT-ORIG's 5K).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Corpus size and coverage; retrieval rank distribution of ECAPs within corpus</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Corpus = 500,000 abstracts sampled from ~6.5M filtered S2ORC documents; used for retrieval/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>S2ORC (source); SCIFACT-OPEN (derived)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>SCIFACT-OPEN corpus built from S2ORC; no numeric evaluation metric beyond counts reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Random sampling chosen to allow annotation coverage; does not further filter for perceived article quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8038.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MS-MARCO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large retrieval/ranking dataset used to train the neural reranker employed in the VERT5ERINI pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>information retrieval / QA</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Neural reranker training dataset</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Use MS-MARCO labelled training examples to train a neural reranker that reorders BM25 retrieval outputs for better downstream verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Reranker effectiveness measured indirectly via downstream retrieval rank of ECAPs and final verification metrics</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Improved positions of ECAPs among top-k retrieved documents and improved precision/recall/F1 of final system.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>MS-MARCO (used to train reranker)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>VERT5ERINI used a reranker trained on MS-MARCO; no standalone reranker metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>MS-MARCO is not biomedical-specific; transfer of reranker may be imperfect for scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8038.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human annotation protocol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert annotation and double-checking protocol</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Protocol used to label pooled CAPs: initial annotator marks NEI vs potential evidence; potential-evidence CAPs labeled then checked by second annotator; disagreements resolved by discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>human evaluation / annotation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Two-step human annotation with adjudication</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Random assignment of CAPs to one annotator; if unclear, first annotator assigns label and second annotator checks; in disagreement, pair discuss to decide final label.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Annotation coverage, ECAP identification rate, qualitative categories (specificity relationship)</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>732 CAPs annotated; ~34.3% of pooled CAPs judged as ECAPs; annotators also labeled specificity relationships for 206 CAPs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to SCIFACT-OPEN pooling</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Three professional annotators with undergraduate biology-related degrees; training session with an author; second-check for potential-evidence CAPs; adjudication via discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>34.3% of pooled CAPs judged ECAPs; annotator workflow led to 251 pooled ECAPs discovered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>No inter-annotator agreement statistics (e.g., kappa) reported; rationale-level annotation omitted due to cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8038.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pool depth / system count (d / n)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pool depth (d) and system count (n) parameters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Design criteria controlling pooled annotation: d = number of top predictions per system; n = number of systems pooled; chosen here as d=250 and n=4 to balance annotation yield and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>evaluation design / IR</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>Pool depth and system count sensitivity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>Analyze how discovered ECAP count and model F1 change as d and n vary to determine sufficiency of pooling parameters for reliable evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Number of ECAPs discovered vs d/n; change in model F1 / average precision as d/n increase</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Measure absolute and percentage changes in F1 and AP between incremental increases in d or n; acceptable stability defined as <2% for pool depth increases near chosen d.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>Applied to SCIFACT-OPEN pooling</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Increasing d from 225 to 250 changed F1 by <2%; adding a fourth system still produced up to ~10% change for some models, indicating diminishing returns but potential benefit from more systems.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>System count limited by available state-of-the-art systems; pooling decisions affect evaluation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8038.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8038.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, metrics, frameworks, datasets, or criteria used to evaluate scientific theories or hypotheses generated by large language models.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FEVER (conventions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FEVER dataset labeling conventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Labeling conventions from FEVER used to decide overall SUPPORTS/REFUTES/NEI when claim and evidence differ in specificity (e.g., special case vs generalization rules).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FEVER: a large-scale dataset for Fact Extraction and VERification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>fact verification datasets / annotation conventions</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_name</strong></td>
                            <td>FEVER specificity-labeling convention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method_description</strong></td>
                            <td>When evidence is a special case of a claim, assign SUPPORTS; when evidence is a generalization and supports the general claim, assign NEI, etc. (rules enumerated in Appendix B.2).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Categorical assignment of y(c,a) given specificity relations</td>
                        </tr>
                        <tr>
                            <td><strong>metric_definition</strong></td>
                            <td>Rule-based mapping from four specificity cases to final labels, used during annotation to maintain consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_benchmark</strong></td>
                            <td>FEVER conventions applied to SCIFACT-OPEN annotations</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_details</strong></td>
                            <td>Annotators applied these conventions when labeling CAPs with mismatched specificity; also produced claim revisions for some CAPs.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_falsifiability_check</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessment</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Using these conventions, 44% of examined CAPs exhibited specificity mismatch; rules guided final label assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_generated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>These conventions can collapse nuance (e.g., special-case evidence may mislead overall claim labeling); motivates claim-revision or more expressive outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciFact-Open: Towards open-domain scientific claim verification', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TREC: Experiment and Evaluation in Information Retrieval <em>(Rating: 2)</em></li>
                <li>Fact or Fiction: Verifying Scientific Claims <em>(Rating: 2)</em></li>
                <li>Scientific Claim Verification with VerT5erini <em>(Rating: 2)</em></li>
                <li>A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification <em>(Rating: 2)</em></li>
                <li>MultiVerS: Improving scientific claim verification with weak supervision and full-document context <em>(Rating: 2)</em></li>
                <li>S2ORC: The Semantic Scholar Open Research Corpus <em>(Rating: 2)</em></li>
                <li>MS MARCO: A Human Generated MAchine Reading COmprehension Dataset <em>(Rating: 2)</em></li>
                <li>The Probabilistic Relevance Framework: BM25 and Beyond <em>(Rating: 2)</em></li>
                <li>The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing <em>(Rating: 2)</em></li>
                <li>FEVER: a large-scale dataset for Fact Extraction and VERification <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8038",
    "paper_id": "paper-f13b251c8346bc3be19b71b840449831e9716999",
    "extraction_schema_id": "extraction-schema-145",
    "extracted_data": [
        {
            "name_short": "SCIFACT-OPEN",
            "name_full": "SCIFACT-OPEN",
            "brief_description": "An open-domain test collection introduced in this paper for scientific claim verification: 279 claims verified against a 500K-abstract corpus with pooled human-annotated evidence labels (SUPPORTS / REFUTES / NEI).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "biomedical / scientific claim verification",
            "theory_type": null,
            "evaluation_method_name": "Open-domain claim verification test collection (pooled annotation)",
            "evaluation_method_description": "Evaluate systems by retrieving candidate abstracts from a 500K research-abstract corpus and comparing system-predicted evidentiary claim/abstract pairs (CAPs) to a pooled set of human-annotated ECAPs collected via TREC-style pooling.",
            "evaluation_metric": "Precision, Recall, F1 (abstract-level label-only), Average Precision (area under precision-recall curve)",
            "metric_definition": "Precision = TP / (TP+FP); Recall = TP / (TP+FN); F1 = 2*(Precision*Recall)/(Precision+Recall). Average precision = area under precision-recall curve across ranked CAPs. (Abstract-level label-only F1 follows Wadden et al. (2020) convention.)",
            "dataset_or_benchmark": "SCIFACT-OPEN",
            "human_evaluation_details": "Evidence annotations collected by three professional annotators (biology-related undergraduate backgrounds); each CAP randomly assigned to one annotator; potential-evidence CAPs labeled and then checked by a second annotator; disagreements resolved via discussion; annotators trained with an author.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "System F1 drops of 15–30 points relative to SCIFACT-ORIG; example system F1s on SCIFACT-OPEN: VERT5ERINI 36.4, PARAGRAPHJOINT 50.3, MULTiVERS 52.4 (see Table 5). Average precision values reported per model.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Pooling uses a limited set of systems and finite pool depth (d=250), so dataset may miss evidence found by unseen models; single retrieval pipeline used; exhaustive annotation infeasible.",
            "uuid": "e8038.0",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "SCIFACT-ORIG",
            "name_full": "SCIFACT (original dataset)",
            "brief_description": "The original SciFact dataset (Wadden et al., 2020) of claims verified against roughly 5K abstracts; used here as the starting point and to supply existing evidence annotations.",
            "citation_title": "Fact or Fiction: Verifying Scientific Claims",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "biomedical / scientific claim verification",
            "theory_type": null,
            "evaluation_method_name": "Small-corpus claim verification benchmark",
            "evaluation_method_description": "Verify claims against a small (~5K) corpus of abstracts with labeled evidentiary CAPs; used to train and evaluate models in prior work and as baseline comparison.",
            "evaluation_metric": "Precision, Recall, F1 (abstract-level), rationale-level metrics in original dataset (rationales not required in SCIFACT-OPEN)",
            "metric_definition": "As above: Precision, Recall, F1 computed over identified evidentiary abstracts; original dataset also required sentence-level rationales.",
            "dataset_or_benchmark": "SCIFACT-ORIG",
            "human_evaluation_details": "Original SCIFACT collected sentence-level rationales; referenced here but SCIFACT-OPEN omits rationale requirement due to cost.",
            "automated_falsifiability_check": null,
            "reproducibility_assessment": null,
            "reported_results": "Models often approach human agreement on SCIFACT-ORIG; specific baseline metrics reported in prior work (e.g., ~68–72 F1 for top systems on SCIFACT-ORIG).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Small corpus size (5K abstracts) does not reflect open-domain retrieval; may bias models to high-performance that does not generalize.",
            "uuid": "e8038.1",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Pooling (TREC-style)",
            "name_full": "TREC-style pooling (pooled data collection)",
            "brief_description": "A pooling methodology adapted from TREC: union the top-d most-confident CAP predictions from multiple systems, annotate that pool, and treat un-annotated CAPs as NEI for evaluation.",
            "citation_title": "TREC: Experiment and Evaluation in Information Retrieval",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval / evaluation",
            "theory_type": null,
            "evaluation_method_name": "Pooled relevance annotation",
            "evaluation_method_description": "Collect candidate items by pooling top-ranked outputs from multiple systems and have humans annotate those; assume un-pooled items are non-relevant.",
            "evaluation_metric": "Coverage of relevant items; stability of evaluation metrics (F1 / average precision) as pool depth or system count increases",
            "metric_definition": "Pool depth d = number of top predictions per system included; system count n = number of systems pooled. Metrics measured as changes in discovered ECAPs and model F1 as d or n increases.",
            "dataset_or_benchmark": "applied to SCIFACT-OPEN",
            "human_evaluation_details": "Annotators labeled pooled CAPs; double-annotation for potential-evidence CAPs; pool depth set to d=250 and n=4 systems for collection.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Pool depth d=250 and n=4 produced diminishing returns: further increases yield &lt;2% changes in F1 and few new ECAPs; total annotated CAPs = 732.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Pooling with few systems may miss evidence found by other models; assumption that un-annotated CAPs are NEI may not fully hold.",
            "uuid": "e8038.2",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "BM25",
            "name_full": "BM25 (Probabilistic Relevance Framework)",
            "brief_description": "A lexical information retrieval ranking function used to retrieve candidate abstracts per claim prior to re-ranking; used as first-stage retrieval for all models in this work.",
            "citation_title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval",
            "theory_type": null,
            "evaluation_method_name": "Bag-of-words lexical retrieval (BM25)",
            "evaluation_method_description": "Retrieve top-k documents per claim by lexical matching score (BM25), then feed retrieved set to re-ranker and labeler.",
            "evaluation_metric": "Retrieval rank distribution of ECAPs (counts by retrieval rank)",
            "metric_definition": "k = number of abstracts retrieved per claim (k=50); analysis reports fraction of ECAPs having retrieval rank ≤ 20 or ≤ 40.",
            "dataset_or_benchmark": "used with SCIFACT-OPEN retrieval",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Majority of evidentiary abstracts ranked among top 20 retrievals; choosing k=50 kept most ECAPs while limiting irrelevant candidates.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Lexical retrieval may miss semantically relevant abstracts; dense retrieval was evaluated but performed worse in this setting.",
            "uuid": "e8038.3",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Neural re-ranker (VERT5ERINI)",
            "name_full": "VerT5erini (BM25 + neural re-ranker + T5 labeling)",
            "brief_description": "A retrieval-and-generation pipeline: BM25 first-stage retrieval, neural reranker trained on MS-MARCO, then T5-based rationale selection and label prediction; used as both a pooling and evaluation system.",
            "citation_title": "Scientific Claim Verification with VerT5erini",
            "mention_or_use": "use",
            "model_name": "VERT5ERINI",
            "model_size": null,
            "scientific_domain": "scientific claim verification / biomedical NLP",
            "theory_type": "claim verification model",
            "evaluation_method_name": "Pipeline retrieval + T5-based labeler",
            "evaluation_method_description": "Retrieve with BM25, rerank retrieved abstracts using a neural reranker trained on MS-MARCO, select rationales and predict labels via T5-3B models.",
            "evaluation_metric": "Precision, Recall, F1, Average Precision (area under PR curve)",
            "metric_definition": "As above: precision/recall/F1 on abstract-level labeled ECAPs; average precision from model confidence-ranked CAPs.",
            "dataset_or_benchmark": "Evaluated on SCIFACT-ORIG and SCIFACT-OPEN",
            "human_evaluation_details": "Predictions from this model were pooled and human-annotated as part of SCIFACT-OPEN construction.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "On SCIFACT-OPEN: P=25.0, R=67.2, F1=36.4 (std devs reported via bootstrap). Inclusion in pooling raised its measured F1 by ~16% compared to excluded-pool evaluation.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "High recall relative to precision on SCIFACT-OPEN; performance sensitive to negative sampling differences across systems.",
            "uuid": "e8038.4",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "PARAGRAPHJOINT",
            "name_full": "ParagraphJoint",
            "brief_description": "A RoBERTa-based model that jointly predicts sentence-level rationales and abstract-level SUPPORTS/REFUTES/NEI labels, used for pooling and evaluation.",
            "citation_title": "A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification",
            "mention_or_use": "use",
            "model_name": "PARAGRAPHJOINT",
            "model_size": null,
            "scientific_domain": "scientific claim verification / biomedical NLP",
            "theory_type": "claim verification model",
            "evaluation_method_name": "RoBERTa-based joint rationale+label model",
            "evaluation_method_description": "Encode claim + full abstract via RoBERTa (512-token truncation), select rationales via attention, predict label from selected sentences.",
            "evaluation_metric": "Precision, Recall, F1, Average Precision",
            "metric_definition": "As above.",
            "dataset_or_benchmark": "Evaluated on SCIFACT-ORIG and SCIFACT-OPEN",
            "human_evaluation_details": "Used as one of the n=4 systems for pooling; its top-d predictions annotated.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "On SCIFACT-OPEN: P=54.7, R=46.5, F1=50.3; exclusion from pooling reduced measured F1 by ~15.9%.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Performance sensitive to training negative-sampling ratio; truncation to 512 tokens may limit full-abstract context.",
            "uuid": "e8038.5",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "MULTiVERS / MULTiVERS10",
            "name_full": "MultiVerS (and MultiVerS_10)",
            "brief_description": "Longformer-based multitask model that encodes full abstracts and claims to jointly predict rationales and claim labels; MULTiVERS_10 is variant trained with fewer negative samples.",
            "citation_title": "MultiVerS: Improving scientific claim verification with weak supervision and full-document context",
            "mention_or_use": "use",
            "model_name": "MULTiVERS",
            "model_size": null,
            "scientific_domain": "scientific claim verification / biomedical NLP",
            "theory_type": "claim verification model",
            "evaluation_method_name": "Longformer-based full-document joint labeling",
            "evaluation_method_description": "Encode claim and full abstract with Longformer to accommodate long inputs; multitask objective predicts abstract-level label and sentence-level rationales.",
            "evaluation_metric": "Precision, Recall, F1, Average Precision",
            "metric_definition": "As above.",
            "dataset_or_benchmark": "Evaluated on SCIFACT-ORIG and SCIFACT-OPEN",
            "human_evaluation_details": "Used in pooling; variant MULTiVERS_10 included to diversify negative-sampling behavior.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "On SCIFACT-OPEN: MULTiVERS P=73.6, R=40.7, F1=52.4; MULTiVERS_10 P=49.6, R=53.0, F1=51.3. MULTiVERS excluded-from-pool F1 changed little, indicating overfit to SCIFACT-ORIG.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Overfitting to SCIFACT-ORIG noted; negative sampling ratio strongly affects precision/recall trade-off.",
            "uuid": "e8038.6",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Negative sampling (r)",
            "name_full": "Negative sampling ratio (r)",
            "brief_description": "Training procedure where for each positive evidentiary CAP, r irrelevant (NEI) CAPs are included as negatives; used to control model precision/recall behavior.",
            "citation_title": "A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "machine learning / information retrieval evaluation",
            "theory_type": null,
            "evaluation_method_name": "Negative sampling hyperparameter",
            "evaluation_method_description": "During training, include r NEI examples per positive to shape classifier behavior (can use random or hard negatives). Affects precision/recall trade-off and generalization.",
            "evaluation_metric": "Effect on Precision / Recall / F1 on held-out corpora",
            "metric_definition": "Compare resulting precision/recall/F1 on SCIFACT-OPEN vs SCIFACT-ORIG as a function of r; higher r tends to increase precision at cost of recall.",
            "dataset_or_benchmark": "Applied to models trained on SCIFACT-ORIG and evaluated on SCIFACT-OPEN",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Negative sampling rate had large impact on open-domain generalization; systems with different r showed &gt;15 F1 differences. Example: MULTiVERS (r=20) vs MULTiVERS_10 (r=10) behavior differed markedly.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Increases training dataset size and time by factor r; choice of hard vs random negatives matters.",
            "uuid": "e8038.7",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Evaluation metrics (P/R/F1 / Avg. Precision)",
            "name_full": "Precision, Recall, F1, Average Precision (area under PR curve)",
            "brief_description": "Primary evaluation metrics used to quantify system ability to identify and correctly label evidentiary abstracts per claim in SCIFACT-OPEN.",
            "citation_title": "Fact or Fiction: Verifying Scientific Claims",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval / classification evaluation",
            "theory_type": null,
            "evaluation_method_name": "Precision/Recall/F1 and Average Precision",
            "evaluation_method_description": "Compute precision, recall, and F1 over predicted ECAPs vs. human-annotated ECAPs; average precision summarizes performance across confidence thresholds (area under precision-recall curve).",
            "evaluation_metric": "Precision, Recall, F1, Average Precision",
            "metric_definition": "Precision = TP/(TP+FP); Recall = TP/(TP+FN); F1 = harmonic mean of precision and recall; Average Precision = integral under precision-recall curve (scalar).",
            "dataset_or_benchmark": "SCIFACT-OPEN (and SCIFACT-ORIG for comparison)",
            "human_evaluation_details": "Confidence-calibrated predictions required for average precision; human annotations supply ground-truth ECAPs for metric computation.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Models drop 15–30 F1 when moving from SCIFACT-ORIG to SCIFACT-OPEN; per-model AP values reported (Table 5).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Metrics sensitive to pooling coverage; assumption that un-pooled CAPs are NEI can bias recall estimates; negative sampling affects precision/recall trade-offs.",
            "uuid": "e8038.8",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Average precision (AP)",
            "name_full": "Average precision (area under precision-recall curve)",
            "brief_description": "A ranking-oriented metric reported to summarize model performance across confidence thresholds for CAP ranking.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval / evaluation",
            "theory_type": null,
            "evaluation_method_name": "Average precision",
            "evaluation_method_description": "Compute area under the precision-recall curve across model confidence-ranked CAPs to summarize trade-offs without fixing a threshold.",
            "evaluation_metric": "Average Precision (scalar)",
            "metric_definition": "Sum over precision at each relevant retrieved instance weighted by change in recall; equivalent to area under the precision-recall curve.",
            "dataset_or_benchmark": "Computed on SCIFACT-OPEN predictions (requires model confidence scores)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Average precision varies by system; used alongside F1 to rank models. VERT5ERINI AP reported low (e.g., ~27.5), others higher (see Table 5).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Cannot be computed for systems without calibrated confidence outputs (ARSJOINT excluded).",
            "uuid": "e8038.9",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Bootstrap resampling (uncertainty)",
            "name_full": "Bootstrap-resampled uncertainty estimates",
            "brief_description": "Uncertainty estimation via bootstrap resampling of claims (1,000 iterations) used to compute standard deviations for reported metrics.",
            "citation_title": "The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "statistical evaluation / NLP",
            "theory_type": null,
            "evaluation_method_name": "Bootstrap resampling over claims",
            "evaluation_method_description": "Resample claims with replacement 1,000 times; for each resampled dataset evaluate model metrics and compute standard deviation across iterations as uncertainty estimate.",
            "evaluation_metric": "Standard deviation (uncertainty) for Precision / Recall / F1 / AP",
            "metric_definition": "Compute metric per bootstrap sample; reported uncertainty = standard deviation across bootstrap samples.",
            "dataset_or_benchmark": "Applied to SCIFACT-OPEN evaluations",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Standard deviations reported for per-model metrics (subscripts in Table 5) computed over 1,000 bootstrap-resampled claim sets.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Bootstrap reflects variance due to claim sampling but not necessarily labeling noise or missing pooled evidence.",
            "uuid": "e8038.10",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Jaccard similarity (overlap)",
            "name_full": "Jaccard similarity for ECAP overlap",
            "brief_description": "Measure used to quantify overlap among ECAPs predicted by different systems (intersection over union of predicted ECAP sets).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "set-overlap analysis / evaluation",
            "theory_type": null,
            "evaluation_method_name": "Jaccard similarity of predicted ECAP sets",
            "evaluation_method_description": "Compute Jaccard index between pairs of systems' predicted evidentiary abstracts to assess agreement, separately for SCIFACT-ORIG and SCIFACT-OPEN additions.",
            "evaluation_metric": "Jaccard similarity (0–1)",
            "metric_definition": "J(A,B) = |A ∩ B| / |A ∪ B| where A and B are sets of predicted ECAPs by two systems.",
            "dataset_or_benchmark": "Applied to model predictions on SCIFACT-ORIG and SCIFACT-OPEN",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "High overlap (≥0.5) on SCIFACT-ORIG abstracts, low overlap (≤0.2) on SCIFACT-OPEN added abstracts (Figure 4).",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Low overlap indicates models identify different information in novel corpora; overlap measures do not capture correctness.",
            "uuid": "e8038.11",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "S2ORC",
            "name_full": "S2ORC: The Semantic Scholar Open Research Corpus",
            "brief_description": "A large heterogeneous corpus of research articles used as the source for SCIFACT-OPEN's 500K-abstract corpus after filtering for medicine/biology and citation connectivity.",
            "citation_title": "S2ORC: The Semantic Scholar Open Research Corpus",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "scientific literature corpus (biomedicine broadly)",
            "theory_type": null,
            "evaluation_method_name": "Corpus source for open-domain evaluation",
            "evaluation_method_description": "Filter S2ORC for medicine/biology and samples with at least one inbound and outbound citation, randomly sample 500K abstracts (including SCIFACT-ORIG's 5K).",
            "evaluation_metric": "Corpus size and coverage; retrieval rank distribution of ECAPs within corpus",
            "metric_definition": "Corpus = 500,000 abstracts sampled from ~6.5M filtered S2ORC documents; used for retrieval/evaluation.",
            "dataset_or_benchmark": "S2ORC (source); SCIFACT-OPEN (derived)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "SCIFACT-OPEN corpus built from S2ORC; no numeric evaluation metric beyond counts reported.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "Random sampling chosen to allow annotation coverage; does not further filter for perceived article quality.",
            "uuid": "e8038.12",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "MS-MARCO",
            "name_full": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
            "brief_description": "A large retrieval/ranking dataset used to train the neural reranker employed in the VERT5ERINI pipeline.",
            "citation_title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "information retrieval / QA",
            "theory_type": null,
            "evaluation_method_name": "Neural reranker training dataset",
            "evaluation_method_description": "Use MS-MARCO labelled training examples to train a neural reranker that reorders BM25 retrieval outputs for better downstream verification.",
            "evaluation_metric": "Reranker effectiveness measured indirectly via downstream retrieval rank of ECAPs and final verification metrics",
            "metric_definition": "Improved positions of ECAPs among top-k retrieved documents and improved precision/recall/F1 of final system.",
            "dataset_or_benchmark": "MS-MARCO (used to train reranker)",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "VERT5ERINI used a reranker trained on MS-MARCO; no standalone reranker metrics reported in this paper.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "MS-MARCO is not biomedical-specific; transfer of reranker may be imperfect for scientific text.",
            "uuid": "e8038.13",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Human annotation protocol",
            "name_full": "Human expert annotation and double-checking protocol",
            "brief_description": "Protocol used to label pooled CAPs: initial annotator marks NEI vs potential evidence; potential-evidence CAPs labeled then checked by second annotator; disagreements resolved by discussion.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "human evaluation / annotation",
            "theory_type": null,
            "evaluation_method_name": "Two-step human annotation with adjudication",
            "evaluation_method_description": "Random assignment of CAPs to one annotator; if unclear, first annotator assigns label and second annotator checks; in disagreement, pair discuss to decide final label.",
            "evaluation_metric": "Annotation coverage, ECAP identification rate, qualitative categories (specificity relationship)",
            "metric_definition": "732 CAPs annotated; ~34.3% of pooled CAPs judged as ECAPs; annotators also labeled specificity relationships for 206 CAPs.",
            "dataset_or_benchmark": "Applied to SCIFACT-OPEN pooling",
            "human_evaluation_details": "Three professional annotators with undergraduate biology-related degrees; training session with an author; second-check for potential-evidence CAPs; adjudication via discussion.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "34.3% of pooled CAPs judged ECAPs; annotator workflow led to 251 pooled ECAPs discovered.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "No inter-annotator agreement statistics (e.g., kappa) reported; rationale-level annotation omitted due to cost.",
            "uuid": "e8038.14",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Pool depth / system count (d / n)",
            "name_full": "Pool depth (d) and system count (n) parameters",
            "brief_description": "Design criteria controlling pooled annotation: d = number of top predictions per system; n = number of systems pooled; chosen here as d=250 and n=4 to balance annotation yield and cost.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "evaluation design / IR",
            "theory_type": null,
            "evaluation_method_name": "Pool depth and system count sensitivity analysis",
            "evaluation_method_description": "Analyze how discovered ECAP count and model F1 change as d and n vary to determine sufficiency of pooling parameters for reliable evaluation.",
            "evaluation_metric": "Number of ECAPs discovered vs d/n; change in model F1 / average precision as d/n increase",
            "metric_definition": "Measure absolute and percentage changes in F1 and AP between incremental increases in d or n; acceptable stability defined as &lt;2% for pool depth increases near chosen d.",
            "dataset_or_benchmark": "Applied to SCIFACT-OPEN pooling",
            "human_evaluation_details": null,
            "automated_falsifiability_check": false,
            "reproducibility_assessment": false,
            "reported_results": "Increasing d from 225 to 250 changed F1 by &lt;2%; adding a fourth system still produced up to ~10% change for some models, indicating diminishing returns but potential benefit from more systems.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "System count limited by available state-of-the-art systems; pooling decisions affect evaluation coverage.",
            "uuid": "e8038.15",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "FEVER (conventions)",
            "name_full": "FEVER dataset labeling conventions",
            "brief_description": "Labeling conventions from FEVER used to decide overall SUPPORTS/REFUTES/NEI when claim and evidence differ in specificity (e.g., special case vs generalization rules).",
            "citation_title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "fact verification datasets / annotation conventions",
            "theory_type": null,
            "evaluation_method_name": "FEVER specificity-labeling convention",
            "evaluation_method_description": "When evidence is a special case of a claim, assign SUPPORTS; when evidence is a generalization and supports the general claim, assign NEI, etc. (rules enumerated in Appendix B.2).",
            "evaluation_metric": "Categorical assignment of y(c,a) given specificity relations",
            "metric_definition": "Rule-based mapping from four specificity cases to final labels, used during annotation to maintain consistency.",
            "dataset_or_benchmark": "FEVER conventions applied to SCIFACT-OPEN annotations",
            "human_evaluation_details": "Annotators applied these conventions when labeling CAPs with mismatched specificity; also produced claim revisions for some CAPs.",
            "automated_falsifiability_check": false,
            "reproducibility_assessment": true,
            "reported_results": "Using these conventions, 44% of examined CAPs exhibited specificity mismatch; rules guided final label assignment.",
            "comparison_to_human_generated": null,
            "comparison_results": "",
            "limitations_noted": "These conventions can collapse nuance (e.g., special-case evidence may mislead overall claim labeling); motivates claim-revision or more expressive outputs.",
            "uuid": "e8038.16",
            "source_info": {
                "paper_title": "SciFact-Open: Towards open-domain scientific claim verification",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TREC: Experiment and Evaluation in Information Retrieval",
            "rating": 2
        },
        {
            "paper_title": "Fact or Fiction: Verifying Scientific Claims",
            "rating": 2
        },
        {
            "paper_title": "Scientific Claim Verification with VerT5erini",
            "rating": 2
        },
        {
            "paper_title": "A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification",
            "rating": 2
        },
        {
            "paper_title": "MultiVerS: Improving scientific claim verification with weak supervision and full-document context",
            "rating": 2
        },
        {
            "paper_title": "S2ORC: The Semantic Scholar Open Research Corpus",
            "rating": 2
        },
        {
            "paper_title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset",
            "rating": 2
        },
        {
            "paper_title": "The Probabilistic Relevance Framework: BM25 and Beyond",
            "rating": 2
        },
        {
            "paper_title": "The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing",
            "rating": 2
        },
        {
            "paper_title": "FEVER: a large-scale dataset for Fact Extraction and VERification",
            "rating": 2
        }
    ],
    "cost": 0.023193,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SciFact-Open: Towards open-domain scientific claim verification</h1>
<p>David Wadden ${ }^{\dagger}$ Kyle Lo ${ }^{\ddagger}$ Bailey Kuehl ${ }^{\ddagger}$ Arman Cohan ${ }^{\ddagger}$<br>Iz Beltagy ${ }^{\ddagger}$ Lucy Lu Wang ${ }^{\dagger \ddagger}$ Hannaneh Hajishirzi ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ University of Washington, Seattle, WA, USA<br>${ }^{\ddagger}$ Allen Institute for Artificial Intelligence, Seattle, WA, USA<br>{dwadden, hannaneh}@cs.washington.edu, lucylw@uw.edu,<br>{kylel,baileyk, armanc, beltagy}@allenai.org</p>
<h4>Abstract</h4>
<p>While research on scientific claim verification has led to the development of powerful systems that appear to approach human performance, these approaches have yet to be tested in a realistic setting against large corpora of scientific literature. Moving to this open-domain evaluation setting, however, poses unique challenges; in particular, it is infeasible to exhaustively annotate all evidence documents. In this work, we present SciFactOPEN, a new test collection designed to evaluate the performance of scientific claim verification systems on a corpus of 500 K research abstracts. Drawing upon pooling techniques from information retrieval, we collect evidence for scientific claims by pooling and annotating the top predictions of four state-of-the-art scientific claim verification models. We find that systems developed on smaller corpora struggle to generalize to SciFact-OPEN, exhibiting performance drops of at least 15 F1. In addition, analysis of the evidence in SciFact-OPEN reveals interesting phenomena likely to appear when claim verification systems are deployed in practice, e.g., cases where the evidence supports only a special case of the claim. Our dataset is available at https://github.com/dwadden/scifact-open.</p>
<h2>1 Introduction</h2>
<p>The task of scientific claim verification (Wadden et al., 2020; Kotonya and Toni, 2020) aims to help system users assess the veracity of a scientific claim relative to a corpus of research literature. Most existing work and available datasets focus on verifying claims against a much more limited context-for instance, a single article or text snippet (Saakyan et al., 2021; Sarrouti et al., 2021; Kotonya and Toni, 2020) or a small, artificiallyconstructed collection of documents (Wadden et al., 2020). Current state-of-the-art models are able to achieve very strong performance on these datasets,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SciFact-OPEN, a new test collection for scientific claim verification that expands beyond the 5 K abstract retrieval setting in the original SciFact dataset (Wadden et al., 2020) to a corpus of 500 K abstracts. Each claim in SciFact-OPEN is annotated with evidence that Supports or Refutes the claim. In the example shown, the majority of evidence REFUTES the claim that alcohol consumption reduces cancer risk, although one abstract indicates that alcohol consumption may reduce thyroid cancer risk specifically.
in some cases approaching human agreement (Wadden et al., 2022).</p>
<p>This gives rise to the question of the scalability of scientific claim verification systems to realistic, open-domain settings that involve verifying claims against corpora containing hundreds of thousands of documents. In these cases, claim verification systems should assist users by identifying and categorizing all available documents that contain evidence supporting or refuting each claim (Fig. 1). However, evaluating system performance in this setting is difficult because exhaustive evidence annotation is infeasible, an issue analogous to evaluation challenges in information retrieval (IR).</p>
<p>In this paper, we construct a new test collection for open-domain scientific claim verification,</p>
<p>called SCIFACT-OPEN, which requires models to verify claims against evidence from both the SCIFACT (Wadden et al., 2020) collection, as well as additional evidence from a corpus of 500K scientific research abstracts. To avoid the burden of exhaustive annotation, we take inspiration from the pooling strategy (Sparck Jones and van Rijsbergen, 1975) popularized by the TREC competitions (Voorhees and Harman, 2005) and combine the predictions of several state-of-the-art scientific claim verification models-for each claim, abstracts that the models identify as likely to SUPPORT or REFUTE the claim are included as candidates for human annotation.</p>
<p>Our main contributions and findings are as follows. (1) We introduce SCIFACT-OPEN, a new test collection for open-domain scientific claim verification, including 279 claims verified against evidence retrieved from a corpus of 500K abstracts. (2) We find that state-of-the-art models developed for SCIFACT perform substantially worse (at least 15 F1) in the open-domain setting, highlighting the need to improve upon the generalization capabilities of existing systems. (3) We identify and characterize new dataset phenomena that are likely to occur in real-world claim verification settings. These include mismatches between the specificity of a claim and a piece of evidence, and the presence of conflicting evidence (Fig. 1).</p>
<p>With SCIFACT-OPEN, we introduce a challenging new test set for scientific claim verification that more closely approximates how the task might be performed in real-word settings. This dataset will allow for further study of claim-evidence phenomena and model generalizability as encountered in open-domain scientific claim verification.</p>
<h2>2 Background and Task Overview</h2>
<p>We review the scientific claim verification task, and summarize the data collection process and modeling approaches for SCIFACT, which we build upon in this work. We elect to use the SCIFACT dataset as our starting point because of the diversity of claims in the dataset and the availability of a number of state-of-the-art models that can be used for pooled data collection. In the following, we refer to the original SCIFACT dataset as SCIFACT-ORIG.</p>
<h3>2.1 Task definition</h3>
<p>Given a claim $c$ and a corpus of research abstracts $\mathcal{A}$, the scientific claim verification task is to identify all abstracts in $\mathcal{A}$ which contain
evidence relevant to $c$, and to predict a label $y(c, a) \in{\operatorname{SuPPORTS}$, Refutes $}$ for each evidence abstract. All other abstracts are labeled $y(c, a)=$ NEI (Not Enough Info). We will refer to a single $(c, a)$ pair as a claim / abstract pair, or CAP. Any CAP where the abstract $a$ provides evidence for the claim $c$ (either SUPPORTS or REFUTES) will be called an evidentiary CAP, or ECAP. Models are evaluated on their precision, recall, and F1 in identifying and correctly labeling the evidence abstracts associated with each claim in the dataset (or equivalently, in identifying ECAPs). ${ }^{1}$</p>
<h3>2.2 SCIFACT-ORIG</h3>
<p>Each claim in SCIFACT-ORIG was created by rewriting a citation sentence occurring in a scientific article, and verifying the claim against the abstracts of the cited articles. The resulting claims are diverse both in terms of their subject matter-ranging from molecular biology to public health-as well as their level of specificity (see §3.3). Models are required to retrieve and label evidence from a small (roughly 5 K abstract) corpus.</p>
<p>Models for SCIFACT-ORIG generally follow a two-stage approach to verify a given claim. First, a small collection of candidate abstracts is retrieved from the corpus using a retrieval technique like BM25 (Robertson and Zaragoza, 2009); then, a transformer-based language model (Devlin et al., 2019; Raffel et al., 2020) is trained to predict whether each retrieved document SUPPORTS, REFUTES, or contains no relevant evidence (NEI) with respect to the claim.</p>
<p>As we show in $\S 4$ and $\S 5$, a key determinant of system generalization is the negative sampling ratio. A negative sampling ratio of $r$ indicates that the model is trained on $r$ irrelevant CAPs for every relevant ECAP. Negative sampling has been shown to improve performance (particularly precision) on SCIFACT-ORIG (Li et al., 2021). See Appendix A. 4 for additional details.</p>
<h2>3 The SCIFACT-OPEN dataset</h2>
<p>In this section, we describe the construction of SCIFACT-OPEN. We report the performance of claim verification models on SCIFACT-OPEN in $\S 4$, and perform reliability checks on the results in $\S 5$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Pooling methodology used to collect evidence for SCIFACT-OPEN. We construct the pool by combining the d most-confident predictions of n different systems. A single CAP is represented as a colored box; the number in the box indicates a hypothetical confidence score. In this example, the annotation pool contains 3 CAPs from Claim 1, 2 for Claim 2, and 1 for Claim 3. Annotators found evidence for 4 / 6 of these CAPS.</p>
<p>Our goal is to construct a test collection which can be used to assess the performance of claim verification systems deployed on a large corpus of scientific literature. This requires a collection of claims, a corpus of abstracts against which to verify them, and evidence annotations with which to evaluate system predictions. We use the claims from the SCIFACT-ORIG test set as our claims for SCIFACT-OPEN.<sup>2</sup> To obtain evidence annotations, we use all evidence from SCIFACT-ORIG as evidence in our new dataset and collect additional evidence from the SCIFACT-OPEN corpus.</p>
<p>For our corpus, we filter the S2ORC dataset (Lo et al., 2020) for all articles which (1) cover topics related to medicine or biology and (2) have at least one inbound and one outbound citation. From the roughly 6.5 million articles that pass these filters, we randomly sample 500K articles to form the corpus for SCIFACT-OPEN, making sure to include the 5K abstracts from SCIFACT-ORIG. We choose to limit the corpus to 500K abstracts to ensure that we can achieve sufficient annotation coverage of the available evidence. Additional details on corpus construction can be found in Appendix A.</p>
<p>Unlike SCIFACT-ORIG (which is skewed toward highly-cited articles from "high-impact" journals), we do not impose any additional quality filters on articles included in SCIFACT-OPEN; thus, our corpus captures the full diversity of information likely to be encountered when scientific fact-checking systems are deployed on real-world resources like S2ORC, arXiv,<sup>3</sup> or PubMed Central.<sup>4</sup></p>
<h3>3.1 Pooling for evidence collection</h3>
<p>To collect evidence from the SCIFACT-OPEN corpus, we adopt a pooling approach popularized by the TREC competitions: use a collection of state-of-the-art models to select CAPs for human annotation, and assume that all un-annotated CAPs have y(c, a) = NEI. We will examine the degree to which this assumption holds in §5.</p>
<p><strong>Pooling approach</strong> We annotate the d most-confident predicted CAPS from each of n claim verification systems. An overview of the process is in shown in Fig. 2; we number the annotation steps below to match the figure.</p>
<p>We select the most confident predictions for a single model as follows. (1) For each claim in SCIFACT-OPEN, we use an information retrieval system consisting of BM25 followed by a neural re-ranker (Pradeep et al., 2021) to retrieve k abstracts from the SCIFACT-OPEN corpus. (2) For each CAP, we compute the softmax scores associated with the three possible output labels, denoted s(SUPPORTS), s(REFUTES), s(NEI). We use max(s(SUPPORTS), s(REFUTES)) as a measure of the model's confidence that the CAP contains evidence. (3) We rank all CAPs by model confidence, and add the d top-ranked predictions to the annotation pool. The final pool (4) is the union of the top-d CAPs identified by each system. Since some CAPs are identified by multiple systems, the size of the final annotation pool is less than n×d; we provide statistics in §3.2. Finally, (5) all CAPs in the pool are annotated for evidence and assigned a final label by an expert annotator, and the label is double-checked by a second annotator (see Appendix A for details).</p>
<p>We choose to prioritize CAPS for annotation based on model confidence, rather than annotating</p>
<p><sup>2</sup>We remove 21 claims (out of 300 total) whose source citations lack important metadata; see Appendix A for details.</p>
<p><sup>3</sup>https://arxiv.org</p>
<p><sup>4</sup>https://www.ncbi.nlm.nih.gov/pmc</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Source</th>
<th>Negative sampling</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pooling and Eval</td>
<td></td>
<td></td>
</tr>
<tr>
<td>VERT5ERINI</td>
<td><em>Pradeep et al. (2021)</em></td>
<td>0</td>
</tr>
<tr>
<td>PARAGRAPHJOINT</td>
<td><em>Li et al. (2021)</em></td>
<td>10</td>
</tr>
<tr>
<td>MULTIVERS</td>
<td><em>Wadden et al. (2022)</em></td>
<td>20</td>
</tr>
<tr>
<td>MULTIVERS${}_{10}$</td>
<td><em>Wadden et al. (2022)</em></td>
<td>10</td>
</tr>
<tr>
<td>Eval only</td>
<td></td>
<td></td>
</tr>
<tr>
<td>ARSJOINT</td>
<td><em>Zhang et al. (2021)</em></td>
<td>12</td>
</tr>
</tbody>
</table>
<p>Table 1: Models used for pooled data collection and evaluation (top), and for evaluation only (bottom). “Negative sampling” indicates the negative sampling ratio. MULTIVERS${}_{10}$ shares the same architecture as MultiVERS, but trains on fewer negative samples.
a fixed number of CAPs per claim, in order to maximize the amount of evidence likely to be discovered during pooling. In $\S 3.3$, we confirm that our procedure identifies more evidence for claims that we would expect to be more extensively-studied.</p>
<p>Models and parameter settings We set $k=50$ for abstract retrieval. In practice, we found that the great majority of evidentiary abstracts were ranked among the top 20 retrievals for their respective claims (Appendix A.3), and thus using a larger $k$ would serve mainly to increase the number of irrelevant results. We set $d=250$; in $\S 5.1$, we show that this is sufficient to ensure that our dataset can be used for reliable model evaluation.</p>
<p>For our models, we utilized all state-of-the-art models developed for SCIFACT-ORIG for which modeling code and checkpoints were available (to our knowledge). We used $n=4$ systems for pooled data collection. During evaluation, we included a fifth system — ARSJOINT— which became available after the dataset had been collected. Model names, source publications, and negative sampling ratios are listed in Table 1; see Appendix A for additional details.</p>
<h3>3.2 Dataset statistics</h3>
<p>We summarize key properties of SCIFACT-OPEN. Table 2a provides an overview of the claims, corpus, and evidence in the dataset. Table 2b shows the fraction of CAPs annotated during pooling which were judged to be ECAPs (i.e. to contain evidence). Overall, roughly a third of predicted CAPs were judged as relevant; this indicates that existing systems achieve relatively low precision when used in an open-domain setting. Relevance is somewhat higher (roughly 50\%) for CAPs predicted by more</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: right;">ECAPs</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Claims</td>
<td style="text-align: left;">Corpus</td>
<td style="text-align: right;">SCIFACT-ORIG</td>
<td style="text-align: right;">Pooling</td>
<td style="text-align: right;">Total</td>
</tr>
<tr>
<td style="text-align: left;">279</td>
<td style="text-align: left;">500K</td>
<td style="text-align: right;">209</td>
<td style="text-align: right;">251</td>
<td style="text-align: right;">460</td>
</tr>
</tbody>
</table>
<p>(a) Summary of the SCIFACT-OPEN dataset, including the number of claims, abstracts, and ECAPs (evidentiary claim / evidence pairs). ECAPs come from two sources: those from SCIFACT-ORIG, and those discovered via pooling.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Num. systems</th>
<th style="text-align: right;">Annotated</th>
<th style="text-align: right;">Evidence</th>
<th style="text-align: right;">\% Evidence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">528</td>
<td style="text-align: right;">154</td>
<td style="text-align: right;">29.2</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">150</td>
<td style="text-align: right;">71</td>
<td style="text-align: right;">47.3</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">20</td>
<td style="text-align: right;">45.5</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">60.0</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">732</td>
<td style="text-align: right;">251</td>
<td style="text-align: right;">34.3</td>
</tr>
</tbody>
</table>
<p>(b) Relevance of CAPs annotated during the pooling process. The first row indicates that 528 CAPs were identified for pooling by one system only; of those CAPs, 154 were judged by annotators as containing evidence. The more systems identified a given CAP, the more likely it is to contain evidence.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Total</th>
<th style="text-align: right;">Retrieved</th>
<th style="text-align: right;">Annotated</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ECAPs</td>
<td style="text-align: right;">209</td>
<td style="text-align: right;">187 (89\%)</td>
<td style="text-align: right;">171 (82\%)</td>
</tr>
</tbody>
</table>
<p>(c) Count of how many ECAPs from SCIFACT-ORIG would have been identified during pooled data collection. “Retrieved" indicates the number of ECAPs that would have been retrieved among the top $k$, and “Annotated" indicates the number that would further have been included in the annotation pool.</p>
<p>Table 2: Annotation results and dataset statistics for SCIFACT-OPEN.
than one system. The majority of CAPs are selected by a single system only, indicating high diversity in model predictions. As mentioned in $\S 3.1$, the total number of annotated CAPs is 732 (rather than 4 models $\times 250$ CAPs / model $=1000$ ) due to overlap in system predictions.</p>
<p>Table 2c shows how many of the ECAPs from SCIFACT-ORIG would have been annotated by our pooling procedure. The fact that the great majority of the original ECAPs would have been included in the annotation pool suggests that our approach achieves reasonable evidence coverage.</p>
<h3>3.3 Evidence phenomena in SCIFACT-OPEN</h3>
<p>We observe three properties of evidence in SCIFACT-OPEN that have received less attention in the study of scientific claim verification, and that can inform future work on this task.</p>
<p>Unequal allocation of evidence Fig. 3 shows the distribution of evidence amongst claims in SCIFACT-OPEN. We find that evidence is distributed unequally; half of all ECAPs are allocated to 34 highly-studied claims ( $12 \%$ of all claims in the dataset). We investigated the characteristics of</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Evidence allocation among claims in SCIFACT-OPEN. The x-axis indicates the number of ECAPs (evidentiary claim / abstract pairs) associated with a given claim, and the y-axis is the number of claims with the corresponding number of ECAPS. For instance, 125 claims are associated with a single evidence-containing abstract.</p>
<table>
<thead>
<tr>
<th>Claim</th>
<th>ECAPs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Obesity is determined in part by genetic factors.</td>
<td>19</td>
</tr>
<tr>
<td>Inhibiting HDAC6 decreases survival of mice with ARID1A mutated tumors.</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Table 3: Example of a claim with a number of ECAPs annotated during pooled data collection (top), and another with no new ECAPs (bottom). Well-studied ECAPs tend to be shorter and mention a small number of common entities.</p>
<p>highly-studied claims, and found that they tend to be short and mention a small number of common, well-studied scientific entities. For instance, entities mentioned in well-studied claims (≥ 4 ECAPs) return, on average, 4 times as many documents when entered into a PubMed search, compared to claims with no evidence (detailed results in Appendix B). Table 3 shows an example.</p>
<h3>Mismatch in claim and evidence specificity</h3>
<p>During evidence collection for SCIFACT-OPEN, annotators reported situations where a claim and abstract exhibited a relationship, but where the claim applied at a different level of <em>specificity</em> from the evidence. For instance, in Fig. 1, the claim and refuting evidence discuss the effects of alcohol consumption on <em>overall</em> cancer risk, while the supporting evidence indicates that alcohol consumption lowers thyroid cancer risk <em>in particular</em>; the supporting evidence is <em>more specific</em> than the claim. We also saw cases where the abstract was <em>more general</em> than the claim (e.g. claim discusses thyroid cancer, abstract discusses cancer in general), and where the abstract was <em>closely related</em> to the claim (e.g. claim discusses thyroid cancer, abstract</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>ECAPs</th>
</tr>
</thead>
<tbody>
<tr>
<td>Evidence matches claim</td>
<td>115</td>
</tr>
<tr>
<td>Evidence more specific than claim</td>
<td>53</td>
</tr>
<tr>
<td>Evidence more general than claim</td>
<td>18</td>
</tr>
<tr>
<td>Evidence closely related to claim</td>
<td>20</td>
</tr>
</tbody>
</table>
<p>(a) Specificity relationship between claim and evidence, for 206 ECAPs. Specificity mismatches are common, comprising 44% of annotated examples.</p>
<p><strong>Claim</strong>: Teaching hospitals provide better care than non-teaching hospitals.</p>
<p><strong>Evidence</strong>: Teaching centers ... prolong survival <em>in women with any gynecological cancer</em> compared to community or general hospitals.</p>
<p><strong>Revised claim</strong>: Teaching hospitals provide better <em>gynecological cancer</em> care than non-teaching hospitals</p>
<p>(b) A CAP where the evidence SUPPORTS a special case of the claim, paired with a <em>revised</em> version of the claim that matches the evidence. The claim discusses medical care overall, while the evidence discusses gynecological cancer care specifically.</p>
<p>Table 4: Claim / evidence specificity mismatch in SCIFACT-OPEN.</p>
<h3>discusses throat cancer).</h3>
<p>Based on this observation, we attempted to quantify the frequency of specificity mismatches. For 206 CAPs in the SCIFACT-OPEN annotation pool, in addition to collecting a SUPPORTS / REFUTES / NEI label, annotators indicated the specificity relationship between claim and abstract, and wrote a <em>revision</em> of the claim such that the revised claim matched the specificity of the abstract. These annotations will be released as part of SCIFACT-OPEN.</p>
<p>Table 4a shows counts for different specificity relationships. We find that 91 / 206 (44%) of the examined CAPs exhibit some form of specificity mismatch. Table 4b shows an example where the evidence is more specific than the claim, along with a revised version of the claim that matches the specificity of the evidence. Examples for all specificity relation types — along with analysis showing that mismatches occur in both well-studied and less-studied claims — are included in Appendix B.2. We discuss possible implications of specificity mismatch for future work on scientific claim verification in §7.</p>
<h3>Conflicting evidence</h3>
<p>Conflicting evidence occurs when a single claim is SUPPORTED by at least one ECAP in SCIFACT-OPEN, and REFUTED by another (see Fig. 1). Of the 81 claims in SCIFACT-OPEN with at least 2 ECAPs, 16 of them (20%)</p>
<p>^{5}In cases of mismatching evidence, we follow the convention used in Thorne et al. (2018) to assign an overall SUPPORTS / REFUTES / NEI label; see Appendix B.2 for details.</p>
<p>| Model | SciFact-Orig | | | SciFact-Open | | | |
| | P | R | F1 | P | R | F1 | Average Precision |
| --- | --- | --- | --- | --- | --- | --- | --- |
| VERT5ERINI | 64.0 | 73.0 | 68.2 | $25.0_{1.9}$ | $67.2_{2.9}$ | $36.4_{2.2}$ | $27.5_{3.2}$ |
| PARAGRAPHJOINT | 75.8 | 63.5 | 69.1 | $54.7_{3.2}$ | $46.5_{3.5}$ | $50.3_{2.7}$ | $40.5_{3.1}$ |
| MULTiVERS | 73.8 | 71.2 | 72.5 | $73.6_{2.9}$ | $40.7_{3.3}$ | $52.4_{3.0}$ | $44.9_{3.7}$ |
| MultiVERS ${ }<em 3.0="3.0">{10}$ | 63.0 | 73.0 | 67.6 | $49.6</em>$ |
| ARSJOINT ${ }^{*}$ | 72.2 | 70.3 | 71.2 | $46.1_{2.9}$ | $37.6_{3.4}$ | $41.4_{2.7}$ | - |}$ | $53.0_{3.7}$ | $51.3_{2.4}$ | $43.4_{3.4</p>
<p>Table 5: System performance on SciFact-OPEN. For comparison, metrics on SciFact-Orig are also reported. Performance is substantially lower on SciFact-OPEN relative to SciFact-Orig. Precision, recall, and F1 vary widely by system, based on the negative sampling rate used during training. Subscripts indicate standard deviations over 1,000 bootstrap-resampled versions of the claims in SciFact-OPEN (see Appendix C).
*The results for ARSJOINT are not comparable with the other systems, since ARSJOINT was not used for data collection. We did not compute model confidence scores for ARSJOINT; therefore average precision is not reported.
have conflicting evidence. In examining these conflicts, we found that they were often a result of specificity mismatches as shown in Fig. 1 (see Appendix B for additional examples), indicating that modeling evidence specificity represents an important area for future work.</p>
<h2>4 Model performance on SciFact-OPEN</h2>
<p>We evaluate all models from Table 1 on SciFactOPEN. These models represent the state-of-the-art on SciFact-Orig, making them strong baselines to assess the difficulty of our new test collection.</p>
<p>SciFact-OPEN is challenging Table 5 shows the performance of all models on SciFact-OPEN, as well as on SciFact-Orig for comparison. Due to the wide variation in the precision and recall of different models on SciFact-OPEN, we also report average precision, which summarizes performance via the area under the precision / recall curve. We find that models rank similarly on F1 and average precision. Model performance drops by 15 to 30 F1 on SciFact-Open relative to SciFact-Orig, indicating that all models have trouble generalizing to large corpora unseen during training. ParAGRAPHJOINT, MultiVERS, and MultiVERS ${ }_{10}$ all exhibit similar performance (within one standard deviation of each other), while VERT5ERINI performs worse due to low precision.</p>
<p>In Appendix C, we examine model performance on well-studied and less-studied claims separately. We find that higher-recall models tend to perform better on well-studied claims (for which more evidence is available), while higher-precision models perform better on less-studied claims.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Overlap between the ECAPs predicted by different systems, as measured by Jaccard similarity. Cells below the diagonal show the similarity for abstracts contained in SciFact-Orig, while cells above the diagonal show similarity for abstracts that were added in SciFact-Open. Overlap is high on abstracts from SciFact-Orig, but much lower when models generalize to documents not seen during training.</p>
<p>Negative sampling affects generalization As mentioned in $\S 2.2$, all models except VERT5ERINI were trained with negative sampling. We observe that negative sampling rate has a much larger impact on precision and recall in the open setting than was observed for SciFact-Orig. VERT5ERINI has recall more than double its precision; for MulTIVERS, the situation is reversed. The behavior of MultiVERS ${ }<em 10="10">{10}$ is much more similar to ParAGRAPHJOINT than MultiVERS, indicating that negative sampling has a larger impact on model generalization behavior than does model architecture. ARSJOINT is qualitatively similar to ParAGRAPHJOINT and MultiVERS ${ }</em>$, but with lower overall performance since its top predictions are not annotated for evidence (see §5.3).</p>
<p>Models have low agreement on SciFact-OPEN Fig. 4 shows the overlap among the ECAPs predicted by different systems, measured using Jac-</p>
<p>card similarity. Overlap is relatively high ( $\geq 0.5$ ) for predictions involving abstracts that were found in SciFact-ORIG, and is much lower ( $\leq 0.2$ ) on abstracts added in SciFact-Open. From a data collection standpoint, low agreement on SciFactOPEN is a benefit, as it ensures that a diverse set of documents was included in the annotation pool. From a modeling standpoint, it suggests that agreement between existing models when deployed on novel corpora is lower than what has previously been observed. Understanding the differences in the information being identified by each model represents an important direction for future work.</p>
<h2>5 Dataset reliability</h2>
<p>The total number of annotations collected during pooling (§3.1) is determined by two parameters: the number of annotations per system $d$, and the number of systems $n$ for which we collect annotations. These parameters must be large enough that increasing them further is unlikely to (1) lead to the discovery of a large number of additional ECAPs or (2) alter the performance metrics of models evaluated on the dataset. Following Zobel (1998), we conduct checks to ensure that conditions (1) and (2) hold for our choices of $d$ and $n$.</p>
<h3>5.1 Annotations per system</h3>
<p>To ensure that the number of annotations per system $d=250$ (also called the pool depth ${ }^{6}$ ) is large enough to ensure reliable evaluation, we examine how much additional evidence is discovered, and how our evaluation metrics change, as $d$ increases from 0 to its final value.</p>
<p>Fig. 5a shows the total number of ECAPS discovered as a function of pool depth. Annotating the 50 most-confident CAPs per system leads to the discovery of 83 ECAPs, while increasing pool depth from 200 to 250 yields 24 new ECAPs-a more than three-fold decrease. This indicates that condition (1) approximately holds; the majority of the evidence in the corpus has been annotated by $d=250$.</p>
<p>Fig. 5b shows the F1 score of each model as a function of pool depth. While F1 scores change initially, increasing the pool depth from $d=225$ to $d=250$ changes the F1 score of each model by less than $2 \%$ (see Appendix D for plots). This</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Total number of ECAPs discovered as a function of pool depth. For instance, annotating to a depth $d=100$ would have resulted in the discovery of roughly 120 ECAPs.
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) F1 score as a function of pool depth. The blue dot at pool depth 100 indicates that VERT5ERINI would have achieved an F1 score of roughly 30, if annotation had stopped at a depth of 100. Results are ARSJOINT are shown as a dashed line to indicate that this system was not used for data collection.</p>
<p>Figure 5: Effect of pool depth on evidence discovery and evaluation metrics. As pool depth increases, fewer new ECAPs are discovered and F1 score stabilizes.
indicates that condition (2) also holds: further increases to pool depth are unlikely to affect performance metrics. We also find that generalization behavior is influenced more by negative sampling rate than by model architecture. Performance of MultiVerS decreases with depth, indicating that it was over-fit to the documents in SciFact-ORIG, while VERT5ERINI improves with depth. These observations hold if we use average precision rather than F1 to measure performance (Appendix D).</p>
<h3>5.2 System count</h3>
<p>We repeat the analysis from $\S 5.1$, but this time varying the number of systems used for data collection (the system count). ${ }^{7}$ As was the case for pool depth, Fig. 6a shows that fewer new ECAPs are discovered as more systems' predictions are annotated. Fig. 6b shows that F1 scores stabilize as system count increases, but not as completely as for pool depth; adding a fourth system still leads a $10 \%$ change in F1 score for VERT5ERINI and MuLtiVerS (Appendix D). Thus, while conditions (1) and (2) are increasingly satisfied as the system</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" />
(a) Total ECAPs discovered as a function of system count.
<img alt="img-7.jpeg" src="img-7.jpeg" />
(b) F1 score as a function of system count.</p>
<p>Figure 6: Effect of system count (i.e. number of systems used during pooling) on evidence discovery and evaluation metrics. As in Fig. 5, we see diminishing returns to increasing system count.
count increases, SCIFACT-OPEN would likely benefit from the collection of additional data identified by new models. Unfortunately, unlike pool depth, the system count that we can achieve is limited by the number of available systems for this task.</p>
<h3>5.3 System inclusion</h3>
<p>To measure the effect on measured performance of including a given system in the annotation pool, we evaluate each system on the evidence that would have been collected if that system's predictions had not been included. Results are shown in Table 6. All systems except MulTiVerS suffer a roughly $15 \%$ drop. When excluded from data collection, ParagraphJoint and MultiVerS ${ }_{10}$ both have performance comparable to ARSJoint. MulTiVerS does not benefit from having its own predictions included, since it was over-fit to SciFactOrig and struggles to identify new evidence not seen during training. Overall, for fair model comparisons, the performance of new models should be compared against the "Excluded" performance of models used for data collection.</p>
<h2>6 Related work</h2>
<p>TREC and pooled data collection Pooling for IR evaluation was popularized by the TREC information retrieval competitions [Voorhees and Harman, 2005], with a number of recent competitions focusing on retrieval in the biomedical domain</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Included</th>
<th style="text-align: center;">Excluded</th>
<th style="text-align: center;">\% Change</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VERT5ERINI</td>
<td style="text-align: center;">36.4</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">-16.3</td>
</tr>
<tr>
<td style="text-align: left;">ParagraphJoint</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">-15.9</td>
</tr>
<tr>
<td style="text-align: left;">MultiVerS</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">-1.5</td>
</tr>
<tr>
<td style="text-align: left;">MultiVerS ${ }_{10}$</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">-14.7</td>
</tr>
<tr>
<td style="text-align: left;">ARSJoint</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 6: Change in F1 score when each model is included in the annotation pool, vs. excluded. Omission leads to a performance decrease of roughly $15 \%$ for all models except MultiVerS.
(Roberts et al., 2020a,b, 2016). Relative to this work, TREC datasets are characterized by a smaller number of queries (or "topics"), a larger number of models available for annotation, and a fixed number of annotations per topic (often around 50)although previous works have proposed strategies to prioritize topics or models for annotation [Zobel, 1998; Cormack et al., 1998]. In contrast, to maximize our annotation yield, we collect a variable number of annotations per claim based on model confidence.</p>
<p>Claim verification and revision Stammbach et al. (2021) studied scientific claim verification against a large research corpus, but simplified the task by evaluating accuracy at predicting a single global truth label per claim, rather than identifying all relevant documents. The Climate-FEVER dataset [Diggelmann et al., 2020] is also opendomain, but assumes a global truth label and verifies claims against Wikipedia, not research papers.</p>
<p>In $\S 3.3$, we proposed claim revisions as a solution to claim / evidence specificity mismatch. Claim revision has previously been studied for fact verification over Wikipedia, with the goal of changing the claim from Refuted to Supported or vice versa [Thorne and Vlachos, 2021; Schuster et al., 2021; Shah et al., 2020]. Previous work has also examined the related task of generating claims based on citation contexts [Wright et al., 2022] and revising questions to match the specificity of answers found in Wikipedia [Min et al., 2020].</p>
<h2>7 Discussion \&amp; Conclusion</h2>
<p>In this work, we introduced a new test collection, SciFact-OPEN, to support performance evaluation for open-domain scientific claim verification. The construction of SciFact-OPEN was enabled by our adaptation of the pooling strategy from IR for identification and annotation of evidence from a corpus of 500 K documents. We hope such method-</p>
<p>ology can see further usage on other NLP tasks for which exhaustive annotation is infeasible.</p>
<p>In analyzing the evidence in SciFact-Open (§3.3), we found that some claims possess a large amount of conflicting evidence, and that evidence may not always match the specificity of the claims as written. We consider two future directions to improve the expressiveness of scientific claim verification. (1) As discussed in $\S 3.3$, one could still require systems to label each ECAP, but also to generate a revised claim matching the specificity of each evidence abstract. This output would provide users with fine-grained information indicating the conditions under which an input claim is likely to hold. We release 91 claim revisions, which can be used to facilitate exploratory research in this direction. (2) One could use the evidence identified by a claim verification system as input into a summarization system (DeYoung et al., 2021; Wallace et al., 2021) - potentially using additional quality criteria (e.g. citation count, publication venue) to filter or re-weight the articles included in the summary. This approach has the benefit of providing a concise summary to the user, but there is a greater risk of hallucination (Maynez et al., 2020).</p>
<p>Overall, our analysis indicates that evaluations using SciFact-OPEN can provide key insights into modeling challenges associated with scientific claim verification. In particular, the performance of existing models declines substantially when evaluated on SciFact-OPEN, suggesting that current claim verification systems are not yet ready for deployment at scale. It is our hope that the dataset and analyses presented in this work will facilitate future modeling improvements, and lead to substantial new understanding of the scientific claim verification task.</p>
<h2>8 Limitations</h2>
<p>A major challenge in information retrieval is the infeasibility of exhaustive relevance annotation. By introducing an open-domain claim verification task, we are faced with similar challenges around annotation. We adopt TREC-style pooling in our setting with substantially fewer systems than what is typically pooled in TREC competitions, which may lead to greater uncertainty in our test collection. We perform substantial analysis (§5) to better understand the sensitivity of our test collection to annotation depth and system count, and our results suggest that though further improvements are pos-
sible, SciFact-OPEN is still useful as a test collection and is able to discern substantive performance differences across models. As other models are developed for claim verification, we may indeed incorporate their predictions in pooling to produce a better test collection.</p>
<p>Through analysis of claim-evidence pairs in SciFact-OPEN, we identified the phenomenon of unequal allocation of evidence (§3.3). Some claims are associated with substantially higher numbers of relevant evidence documents; we call these highlystudied claims. In this work, we do not treat these claims any differently than those associated with limited evidence. It could be that highly-studied claims are more representative of the types of claims that users want to verify, in which case we may want to distinguish between these and other types of claims in our dataset, or develop annotation pipelines that would allow us to identify and verify more of these highly-studied claims. In the context of this paper, we derive all claims from the original SciFact test collection, and do not provide additional claims.</p>
<p>Finally, we rely on a single retrieval system to identify candidate abstracts. While our analysis indicates that this system identifies the great majority of relevant abstracts (Appendix A.3), future work could extend the dataset collected here by retrieving documents using a wider variety of IR approaches.</p>
<h2>Acknowledgments</h2>
<p>This research was supported by NSF IIS-2044660, ONR N00014-18-1-2826, ONR MURI N00014-18-1-2670, a Sloan fellowship and gifts from AI2. We thank the Semantic Scholar team at AI2, UWNLP, and the H2lab at UW for helpful comments and feedback. Thanks to Xiangci Li and Ronak Pradeep for help with ParagraphJoint and VERT5ERINI, respectively.</p>
<h2>References</h2>
<p>Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. ArXiv, abs/2004.05150.</p>
<p>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical Investigation of Statistical Significance in NLP. In EMNLP.</p>
<p>Daniel Fernando Campos, T. Nguyen, M. Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, L. Deng, and Bhaskar Mitra. 2016. MS</p>
<p>MARCO: A Human Generated MAchine Reading COmprehension Dataset. In NeurIPS.</p>
<p>Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. 1998. Efficient construction of large test collections. In SIGIR.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL.</p>
<p>Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. MS^2: MultiDocument Summarization of Medical Studies. In EMNLP.
T. Diggelmann, Jordan L. Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2020. CLIMATE-FEVER: A Dataset for Verification of Real-World Climate Claims. In Tackling Climate Change with ML workshop @ NeurIPS.</p>
<p>Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing. In $A C L$.
V. Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP.</p>
<p>Neema Kotonya and F. Toni. 2020. Explainable Automated Fact-Checking for Public Health Claims. In EMNLP.</p>
<p>Xiangci Li, G. Burns, and Nanyun Peng. 2021. A Paragraph-level Multi-task Learning Model for Scientific Fact-Verification. In Workshop on Scientific Document Understanding @ AAAI.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv, abs/1907.11692.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Michael Kinney, and Daniel S. Weld. 2020. S2ORC: The Semantic Scholar Open Research Corpus. In $A C L$.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan T. McDonald. 2020. On Faithfulness and Factuality in Abstractive Summarization. In ACL.</p>
<p>Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020. AmbigQA: Answering Ambiguous Open-domain Questions. In EMNLP.</p>
<p>Ronak Pradeep, Xueguang Ma, Rodrigo Nogueira, and Jimmy Lin. 2021. Scientific Claim Verification with VerT5erini. In Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis @EACL.</p>
<p>Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. JMLR.</p>
<p>Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle Lo, Ian Soboroff, Ellen M. Voorhees, Lucy Lu Wang, and William R. Hersh. 2020a. TREC-COVID: rationale and structure of an information retrieval shared task for COVID-19. Journal of the American Medical Informatics Association.</p>
<p>Kirk Roberts, Dina Demner-Fushman, Ellen M. Voorhees, and William R. Hersh. 2016. Overview of the TREC 2016 clinical decision support track. TREC.</p>
<p>Kirk Roberts, Dina Demner-Fushman, Ellen M. Voorhees, William R. Hersh, Steven Bedrick, Alexander J. Lazar, and Shubham Pant. 2020b. Overview of the TREC 2020 Precision Medicine Track. TREC.
S. Robertson and H. Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval.</p>
<p>Arkadiy Saakyan, Tuhin Chakrabarty, and Smaranda Muresan. 2021. COVID-Fact: Fact Extraction and Verification of Real-World Claims on COVID-19 Pandemic. In $A C L$.</p>
<p>Mourad Sarrouti, Asma Ben Abacha, Yassine Mrabet, and Dina Demner-Fushman. 2021. Evidence-based Fact-Checking of Health-related Claims. In EMNLP Findings.</p>
<p>Tal Schuster, Adam Fisch, and R. Barzilay. 2021. Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence. In NAACL.</p>
<p>Darsh J. Shah, Tal Schuster, and R. Barzilay. 2020. Automatic Fact-guided Sentence Modification. In AAAI.</p>
<p>Karen Sparck Jones and C. J. van Rijsbergen. 1975. Report on the need for and provision of an "ideal" information retrieval test collection. In British Library Research and Development Report 5266, Computer Laboratory, University of Cambridge.</p>
<p>Dominik Stammbach, Boyang Zhang, and Elliott Ash. 2021. The Choice of Knowledge Base in Automated Claim Checking. ArXiv, abs/2111.07795.</p>
<p>Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, and Jaewoo Kang. 2022. BERN2: an advanced neural biomedical named entity recognition and normalization tool. ArXiv, abs/2201.02080.</p>
<p>Nandan Thakur, N. Reimers, Andreas Ruckl'e, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. In NeurIPS.</p>
<p>James Thorne and Andreas Vlachos. 2021. Evidencebased factual error correction. In ACL.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for Fact Extraction and VERification. In NAACL.</p>
<p>Ellen M. Voorhees and Donna Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. MIT press.</p>
<p>David Wadden and Kyle Lo. 2021. Overview and Insights from the SciVer Shared Task on Scientific Claim Verification. In Proceedings of the Second Workshop on Scholarly Document Processing @ NAACL.</p>
<p>David Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan, Iz Beltagy, and Hannaneh Hajishirzi. 2022. MultiVerS: Improving scientific claim verification with weak supervision and full-document context. In NAACL Findings.</p>
<p>David Wadden, Kyle Lo, Lucy Lu Wang, Shanchuan Lin, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or Fiction: Verifying Scientific Claims. In EMNLP.</p>
<p>Byron C. Wallace, Sayantani Saha, Frank Soboczenski, and Iain James Marshall. 2021. Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural Multi-Document Summarization. In AMIA Joint Summits on Translational Science proceedings.</p>
<p>Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Lu Wang. 2022. Generating Scientific Claims for ZeroShot Scientific Fact Checking. In ACL.</p>
<p>Zhiwei Zhang, Jiyi Li, Fumiyo Fukumoto, and Yanming Ye. 2021. Abstract, Rationale, Stance: A Joint Model for Scientific Claim Verification. In EMNLP.</p>
<p>Justin Zobel. 1998. How reliable are the results of large-scale information retrieval experiments? In SIGIR.</p>
<h2>A Dataset construction</h2>
<h3>A. 1 Corpus</h3>
<p>We use 2019-09-28 release of the the S2ORC corpus (Lo et al., 2020) as our source of abstracts. We filter for documents whose mag_field_of_study field includes at least one of Medicine or Biology, and which have at least one inbound and one outbound citation; the latter check serves as a basic quality filter to make sure that the article is related to other articles found in the literature. This filtering leaves us with roughly 6.5 million documents, from which we randomly sample our SCIFACTOPEN corpus of 500 K documents (making sure to
include the 5 K documents from SCIFACT-ORIG). For more information on S2ORC, see https:// github.com/allenai/s2orc.</p>
<h2>A. 2 Claims and evidence</h2>
<p>Claims As mentioned in $\S 3$, we removed 21 claims from the SCIFACT-ORIG test set when constructing SCIFACT-OPEN, leaving 279 claims. Each claim in the dataset is based on a source citation. We removed claims for which we could not find metadata in S2ORC providing information about the source citation - in particular, the article it came from or the year the article was published. No analyses based on this information were included in the final version of this work, but the dataset had already been collected when we realized that this claim filtering step had been unnecessary. Regardless, there is no reason that the availability (or lack thereof) of source metadata would correlate with any linguistic properties of the claims; thus, the omission of these 21 claims should not bias our findings in any way.</p>
<p>Evidence annotation Evidence annotations were performed by three professional annotators with undergraduate degrees in fields related to biology. Before beginning annotations, they participated in a training session with one of the authors to ensure that they understood the task. Annotations are performed as follows. First, every CAP in the annotation pool is assigned randomly to one of the three annotators. The assigned annotator makes a decision on whether the instance clearly does not contain evidence, or whether it might. If it clearly does not, it is marked NEI and annotation stops. If it might contain evidence, the first annotator assigns a label, and a second annotator checks the label to confirm. In the case of disagreement, the two annotators discuss the instance and collectively decide on a final label.</p>
<h2>A. 3 Retrieval</h2>
<p>In $\S 3.1$, we chose to retrieve $k=50$ abstracts per claim; these abstracts were then rank-ordered by model confidence, and the most-confident predictions were annotated for evidence. Using $k=50$ is justified if worse-ranked retrievals are unlikely to be annotated as ECAPs during the pooling process. Figure 7 confirms that this is indeed the case; the great majority of evidentiary abstracts have retrieval ranks of 20 or better, and very few have ranks worse than 40.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 7: Number of ECAPs discovered as a function of <em>k</em>, the number of abstracts retrieved per claim. The great majority of abstracts judged as ECAPs were ranked among the top 20 retrievals for their respective claims.</p>
<h3>A.4 Models</h3>
<p>For pooled annotation collection, we used all models achieving state-of-the-art or competitive performance on the SCIFACT leaderboard<sup>8</sup> for which modeling code and checkpoints were available as of early summer 2021, when annotation collection began. The available systems were VERT5ERINI (Pradeep et al., 2021) and PARAGRAPHJOINT (Li et al., 2021) — the two leaders on the SciVer shared task (Wadden and Lo, 2021) — and MULTiVERS (Wadden et al., 2022), formerly called LongChecker. Early in annotation, we noticed that the systems exhibited different precision and recall behavior, and hypothesized that this was due to differences in negative sampling rate. To test this, we also collected annotations with a version of MULTiVERS trained with a negative sampling ratio of 10 (negative sampling ratio is defined in §2.2), referred to as MULTiVERS<sub>10</sub>, and found that this model indeed behaved more like PARAGRAPHJOINT than MULTiVERS in terms of precision and recall. We decided to include MULTiVERS<sub>10</sub> in the annotation process to increase the diversity of annotation pool. Subsequently, ARSJOINT (Zhang et al., 2021) was released and achieved comparable performance with the four systems used for data collection. We conduct evaluations on this system as well.</p>
<p><strong>System descriptions</strong> Given a claim <em>c</em>, all models first retrieve a collection of candidate abstracts <em>a</em>, and then predict labels for each retrieved candidate. In this work, we used the VERT5ERINI retrieval system for all models, since it outperformed the performance of the techniques used with ARSJOINT and PARAGRAPHJOINT. VERT5ERINI first retrieves documents using BM25, then reranks the retrieved documents using a neural re-</p>
<p><sup>8</sup>https://leaderboard.allenai.org/scifact</p>
<p>ranker trained on MS-MARCO (Campos et al., 2016). We experimented with using dense retrieval instead (Karpukhin et al., 2020), but found that this did not perform well; similar results were reported in Thakur et al. (2021).</p>
<p>Given a claim <em>c</em> and abstract <em>a</em>, VERT5ERINI selects rationales (evidentiary sentences) from <em>a</em> using a T5-3B model trained on SCIFACT, and then makes label predictions based on the selected rationales using a separate T5-3B model.</p>
<p>PARAGRAPHJOINT and ARSJOINT both encode the claim and full abstract using RoBERTa (Liu et al., 2019), truncating to 512 tokens, and use these representations as the basis for both rationale selection and label prediction. Rationales are predicted based on self-attention over the encodings of the tokens in each sentence, and then a final label is predicted based on self-attention over the representations of the sentences that were selected as rationales.</p>
<p>MULTiVERS encodes the claim and full abstracts in the same fashion as PARAGRAPHJOINT and ARSJOINT, using Longformer (Beltagy et al., 2020) to accommodate long abstracts, and then predicts the label and rationales in a multitask fashion, based on encodings of the leading [SEP] token and sentence separator tokens, respectively.</p>
<p><strong>Negative sampling</strong> For scientific claim verification, negative sampling has been performed as follows: for every (<em>c, a</em>) instance in the training data where <em>y</em>(<em>c, a</em>) ∈ {SUPPORTS, REFUTES}, include <em>r</em> additional (<em>c, a</em><sub><em>i</em></sub>)<sub><em>i</em>=3</sub> instances where <em>y</em>(<em>c, a</em><sub><em>i</em></sub>) = NEI for all <em>i</em>. The irrelevant abstracts <em>a</em><sub><em>i</em></sub> can be sampled randomly from the corpus, or can be chosen to be "hard" negatives; for instance, abstracts <em>a</em><sub><em>i</em></sub> could be chosen which have high lexical overlap with claim <em>c</em>, but which are not annotated as SUPPORTS or REFUTES. Negative sampling has been shown to increase the precision of fact verification models (Li et al., 2021), but comes at the cost of increasing the size of the training dataset (and thus the training time) by a factor of <em>r</em>.</p>
<h3>B Additional evidence properties</h3>
<h4>B.1 Unequal allocation of evidence</h4>
<p>Fig. 8a shows the distribution of evidence amongst claims in SCIFACT-OPEN, showing evidence from SCIFACT-ORIG and evidence collected during pooling separately. The majority of claims in SCIFACT-ORIG have one ECAP. Pooling discovered no new ECAPs for the majority of claims</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" />
(a) Evidence allocation among claims, showing evidence from SCIFACT-ORIG and evidence collected during pooling separately. The x -axis indicates the number of ECAPs associated with a given claim, and the y -axis is the number of claims with that number of ECAPS.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(b) Cumulative distribution of ECAPs across claims.</p>
<p>Figure 8: Distribution of evidence from SCIFACTORIG, and from the evidence collected via pooling.
in the dataset, and discovered a large amount of evidence for a small handful of claims. Fig. 8b shows the cumulative distribution of evidence. 14 claims account for $50 \%$ of the ECAPs discovered via pooling.</p>
<p>In $\S 3.3$, we also observed that well-studied claims tend to be short, and mention a small number of well-studied entities. Table 7 shows these results quantitatively by examining the characteristics of claims for which pooling discovered at least 4 new ECAPs, vs. claims for which it discovered none. Entities mentioned in well-studied claims return, on average, 4 times as many documents when entered into a PubMed search compared with entities mentioned in claims with no new evidence. We use BERN2 (Sung et al., 2022) to identify the entities for this analysis.</p>
<h2>B. 2 Claim / evidence specificity mismatch</h2>
<p>Annotation conventions for mismatched evidence In situations where the evidence in abstract $a$ is more specific or more general than claim $c$, we follow the convention established in the FEVER dataset (Thorne et al., 2018) to assign a final label $y(c, a)$ :</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">0 ECAPs</th>
<th style="text-align: right;">$\geq 4$ ECAPs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Avg. claim length (tokens)</td>
<td style="text-align: right;">14.1</td>
<td style="text-align: right;">10.5</td>
</tr>
<tr>
<td style="text-align: left;">Avg. entities / claim</td>
<td style="text-align: right;">2.3</td>
<td style="text-align: right;">1.7</td>
</tr>
<tr>
<td style="text-align: left;">Median PubMed results</td>
<td style="text-align: right;">49 K</td>
<td style="text-align: right;">198 K</td>
</tr>
</tbody>
</table>
<p>Table 7: Characteristics of claims for which 0 ECAPs were annotated during pooled data collection, compared to claims with $\geq 4$ ECAPs annotated. All differences are significant at the 0.05 level.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Category</th>
<th style="text-align: right;">ECAPs</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Well- <br> studied</td>
<td style="text-align: right;">Less- <br> studied</td>
</tr>
<tr>
<td style="text-align: left;">Evidence matches claim</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">34</td>
</tr>
<tr>
<td style="text-align: left;">Evidence more specific than claim</td>
<td style="text-align: right;">35</td>
<td style="text-align: right;">18</td>
</tr>
<tr>
<td style="text-align: left;">Evidence more general than claim</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">15</td>
</tr>
<tr>
<td style="text-align: left;">Evidence closely related to claim</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">14</td>
</tr>
</tbody>
</table>
<p>Table 8: Rates of claim / evidence specificity mismatch for well-studied and less-studied claims.</p>
<ul>
<li>If $a$ SUPPORTS a special case of $c$, then assign $y(c, a)=$ SUPPORTS.</li>
<li>If $a$ SUPPORTS a generalization of $c$, then assign $y(c, a)=$ NEI.</li>
<li>If $a$ REFUTES a special case of $c$, then assign $y(c, a)=$ NEI.</li>
<li>If $a$ REFUTES a generalization of $c$, then assign $y(c, a)=$ REFUTES.</li>
</ul>
<p>Occurrence for well-studied and less-studied claims Table 8 shows rates of claim / evidence specificity mismatch for well-studied and lessstudied claims, respectively. Specificity mismatches occur for both types of claims. Interestingly, CAPs where the evidence is more general than the claim occur more frequently for lessstudied claims; this likely occurs because lessstudied claims are themselves likely to be very specific and cover narrower topics.</p>
<p>Examples In $\S 3.3$, we described how the claim and evidence in an ECAP may not have matching levels of specificity. Table 9 provides examples of the different forms of specificity mismatch shown in Table 4.</p>
<h2>B. 3 Conflicting evidence</h2>
<p>Table 10 shows examples of two claims for which conflicting evidence was found in SCIFACT-OPEN.</p>
<h2>C Model performance</h2>
<p>Uncertainty estimates Table 5 includes uncertainty estimates for performance on SCIFACT-</p>
<table>
<thead>
<tr>
<th>Category</th>
<th>Evidence matches claim</th>
</tr>
</thead>
<tbody>
<tr>
<td>Claim</td>
<td>Mitochondria play a major role in calcium homeostasis.</td>
</tr>
<tr>
<td>Evidence</td>
<td>Mitochondria . . . are essential organelles responsible for ... calcium homeostasis.</td>
</tr>
<tr>
<td>Revision</td>
<td></td>
</tr>
<tr>
<td>Explanation</td>
<td>No revision necessary; claim and evidence are paraphrases</td>
</tr>
<tr>
<td>Category</td>
<td>Evidence more specific than claim</td>
</tr>
<tr>
<td>Claim</td>
<td>Teaching hospitals provide better care than non-teaching hospitals</td>
</tr>
<tr>
<td>Evidence</td>
<td>Teaching centres ... prolong survival in women with any gynecological cancer compared to community <br> or general hospitals.</td>
</tr>
<tr>
<td>Revision</td>
<td>Teaching hospitals provide better gynecological cancer care than non-teaching hospitals.</td>
</tr>
<tr>
<td>Explanation</td>
<td>The evidence refers to gynecological cancer care specifically, not care care in general.</td>
</tr>
<tr>
<td>Category</td>
<td>Evidence more general than claim</td>
</tr>
<tr>
<td>Claim</td>
<td>Somatic missense mutations in NT5C2 are associated with relapse of acute lymphoblastic leukemia.</td>
</tr>
<tr>
<td>Evidence</td>
<td>T5C2 mutant proteins show . . . resistance to chemotherapy</td>
</tr>
<tr>
<td>Revision</td>
<td>Mutations in NT5C2 are associated with relapse of cancer.</td>
</tr>
<tr>
<td>Explanation</td>
<td>Evidence mentions T5C2 mutations in general, while the claim mentions somatic missense mutations <br> specifically. The evidence discusses chemotherapy resistance generally, while the claim discusses <br> relapse of acute lymphoblastic leukemia specifically.</td>
</tr>
<tr>
<td>Category</td>
<td>Evidence closely related to claim</td>
</tr>
<tr>
<td>Claim</td>
<td>Near-infrared wavelengths increase penetration depth in fiberoptic confocal microscopy</td>
</tr>
<tr>
<td>Evidence</td>
<td>Longer wavelength can . . . increase the effective penetration depth of OCT (optical coherence-domain <br> tomography) imaging</td>
</tr>
<tr>
<td>Revision</td>
<td>Near-infrared wavelengths increase penetration depth in optical coherence-domain tomography.</td>
</tr>
<tr>
<td>Explanation</td>
<td>The claim discusses fiberoptic confocal microscopy. The evidence discusses a different imaging <br> technique, optical coherence-domain tomography.</td>
</tr>
</tbody>
</table>
<p>Table 9: Examples of different forms of claim-evidence specificity mismatch. In each example, information specific to claim or evidence is shown in italics. The revision re-writes the claim to match the specificity of the evidence.</p>
<p>OPEN. We obtain these estimates by computing the standard deviation over 1,000 bootstrap-sampled versions of the dataset (Dror et al., 2018; BergKirkpatrick et al., 2012). For a single bootstrap iteration, we resample the claims from the dataset with replacement, and evaluate against the evidence for the sampled claims, weighting the evidence by the number of times each claim was sampled.</p>
<p>Performance for well-studied and less-studied claims Table 11 shows model performance on well-studied vs. less-studied claims. Higher-recall models tend to perform better on the well-studied claims, since these are the claims where evidence is available in the corpus. Higher-precision models perform better on less-studied claims.</p>
<p>Confusion matrices Figure 9 shows confusion matrices for all systems. Models rarely confuse SUPPORTS with REFUTES; much more commonly, they either mistake irrelevant abstracts for evidence or fail to identify relevant abstracts.</p>
<h2>D Dataset reliability: Additional experiments</h2>
<p>Percentage changes in evaluation metrics In $\S 5$, we examined the effect of pool depth and system count on F1 score. Here, we show the same plots from $\S 5$, together with plots showing the percentage changes in the F1 score. Results for pool depth are shown in Fig. 10. Results for system count are shown in Fig. 11.</p>
<p>Evaluation using average precision In $\S 5$, we examined the effect of pool depth and system count on F1 score. We perform the same analysis using average precision. Fig. 12 shows the effect of pool depth, and Fig. 13 shows the effect of model count. The qualitative conclusions are the same as for F1. The fact that using F1 and average precision leads to the same conclusions indicates that simply re-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Claim</th>
<th style="text-align: left;">Bariatric surgery has a deleterious impact on mental health.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SUPPORTS</td>
<td style="text-align: left;">Our study shows that undergoing bariatric surgery is associated with increases in self-harm, psychiatric <br> service use and occurrence of mental disorders.</td>
</tr>
<tr>
<td style="text-align: left;">REFUTES</td>
<td style="text-align: left;">Statistical analysis revealed significant improvements in depressive symptoms, physical dimension of <br> quality of life, and self-esteem $\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Claim</td>
<td style="text-align: left;">Teaching hospitals provide better care than non-teaching hospitals</td>
</tr>
<tr>
<td style="text-align: left;">SUPPORTS</td>
<td style="text-align: left;">Teaching centres or regional cancer centres may prolong survival in women with any gynaecological <br> cancer compared to community or general hospitals</td>
</tr>
<tr>
<td style="text-align: left;">REFUTES</td>
<td style="text-align: left;">Overall [the results] do not suggest that a healthcare facility's teaching status on its own markedly <br> improves or worsens patient outcomes.</td>
</tr>
</tbody>
</table>
<p>Table 10: Examples of two claims with conflicting evidence.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Well-studied</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Less-studied</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Avg. Precision</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">R</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Avg. Precision</td>
</tr>
<tr>
<td style="text-align: center;">VERT5ERINI</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">42.7</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">20.9</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">25.5</td>
</tr>
<tr>
<td style="text-align: center;">ParagraphJoint</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">40.1</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;">MultiVERS</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">34.7</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">64.8</td>
<td style="text-align: center;">56.9</td>
</tr>
<tr>
<td style="text-align: center;">MultiVERS $_{10}$</td>
<td style="text-align: center;">59.3</td>
<td style="text-align: center;">42.3</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">44.8</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">52.6</td>
<td style="text-align: center;">52.7</td>
</tr>
<tr>
<td style="text-align: center;">ARSJoint *</td>
<td style="text-align: center;">37.5</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">28.1</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">51.0</td>
<td style="text-align: center;">52.4</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">27.0</td>
</tr>
</tbody>
</table>
<p>Table 11: Performance on SciFact-OPEN, for well-studied and less-studied claims. Higher-recall models generally perform better on well-studied claims, while high-precision models perform better on less-studied claims.
*The results for ARSJoint are not comparable with the other systems, since ARSJoint was not used for data collection.
calibrating each model's classification threshold to adjust for the negative sampling rate used during training would not change the results.</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 9: Confusion matrices for model predictions on SciFact-Open. The ${$ NEI, NEI $}$ cell is 0 because SciFact-Open is a retrieval task; it's not informative to compute agreement on unlabeled and unpredicted documents in the corpus.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" />
(a) F1 score as a function of pool depth. This is the same plot as shown in $\S 5.1$
<img alt="img-13.jpeg" src="img-13.jpeg" />
(b) Absolute value of percentage change in F1 score, between adjacent points in the series shown in Fig. 10a. The blue dot at pool depth 100 indicates that the F1 score for VERT5ERINI is increased by roughly $6 \%$ when the pool depth increases from 75 to 100. Values close to 0 indicate that collecting additional data does not have an appreciable effect on F1 score.</p>
<p>Figure 10: Percentage change in F1 as a function of pool depth.
<img alt="img-14.jpeg" src="img-14.jpeg" />
(a) F1 score as a function of system count.
<img alt="img-15.jpeg" src="img-15.jpeg" />
(b) Absolute value of percentage change in F1 score, between adjacent points in Fig. 11a.</p>
<p>Figure 11: Percentage change in F1 as a function of system count.
<img alt="img-16.jpeg" src="img-16.jpeg" />
(a) Average precision as a function of pool depth.
<img alt="img-17.jpeg" src="img-17.jpeg" />
(b) Absolute value of percentage change in average precision.</p>
<p>Figure 12: Effect of pool depth on model performance, as measured by average precision. We see the same trends as for F1 score. ARSJoint is not included because computing average precision requires model confidence scores.
<img alt="img-18.jpeg" src="img-18.jpeg" />
(a) Average precision as a function of system count.
<img alt="img-19.jpeg" src="img-19.jpeg" />
(b) Absolute value of percentage change in average precision.</p>
<p>Figure 13: Effect of system count on average precision. Again, the results here mirror the conclusions drawn using F1 score.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ In TREC, the pool depth refers to the number of annotations collected per system for a single query. We use it to refer to the number of annotations collected per system.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ There are $4!$ possible system orderings. We compute metrics using all orderings, and display the mean and standard deviation (as error bars) in Fig. 6.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7556 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7556</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7556</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-5a212bb4307008f27018edc0b8d873f4e9657d17</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5a212bb4307008f27018edc0b8d873f4e9657d17" target="_blank">ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions</a></p>
                <p><strong>Paper Venue:</strong> IEEE Transactions on Knowledge and Data Engineering</p>
                <p><strong>Paper TL;DR:</strong> A novel outlier detection method called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is both parameter-free and easy to interpret, and an easy-to-use and scalable Python implementation for accessibility and reproducibility is proposed.</p>
                <p><strong>Paper Abstract:</strong> Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the “rare events” that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7556",
    "paper_id": "paper-5a212bb4307008f27018edc0b8d873f4e9657d17",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00798875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions</h1>
<p>Zheng Li<em>, Yue Zhao</em>, Student Member Xiyang Hu, Nicola Botta, Cezar Ionescu, and George H. Chen</p>
<h4>Abstract</h4>
<p>Outlier detection refers to the identification of data points that deviate from a general data distribution. Existing unsupervised approaches often suffer from high computational cost, complex hyperparameter tuning, and limited interpretability, especially when working with large, high-dimensional datasets. To address these issues, we present a simple yet effective algorithm called ECOD (Empirical-Cumulative-distribution-based Outlier Detection), which is inspired by the fact that outliers are often the "rare events" that appear in the tails of a distribution. In a nutshell, ECOD first estimates the underlying distribution of the input data in a nonparametric fashion by computing the empirical cumulative distribution per dimension of the data. ECOD then uses these empirical distributions to estimate tail probabilities per dimension for each data point. Finally, ECOD computes an outlier score of each data point by aggregating estimated tail probabilities across dimensions. Our contributions are as follows: (1) we propose a novel outlier detection method called ECOD, which is both parameter-free and easy to interpret; (2) we perform extensive experiments on 30 benchmark datasets, where we find that ECOD outperforms 11 state-of-the-art baselines in terms of accuracy, efficiency, and scalability; and (3) we release an easy-to-use and scalable (with distributed support) Python implementation for accessibility and reproducibility.</p>
<p>Index Terms-outlier detection, anomaly detection, distributed learning, scalability, empirical cumulative distribution function.</p>
<h2>1 INTRODUCTION</h2>
<p>OUTLIERS, also sometimes referred to as anomalies, are data points with different data characteristics from "normal" observations. Since outliers can markedly impact the results of statistical analyses, removing outliers is often a crucial preprocessing step in data analysis models ([1], [2]). However, to remove outliers, we need to identify them first, which is the goal of outlier detection (OD). OD has many applications, such as fraud detection ([3], [4], [5]), network intrusion detection ([6], [7]), social media analysis ([8], [9]), video analysis [10], intelligent transportation [11], [12], [13], and data generation [14]. These applications often demand OD algorithms with high detection accuracy and fast execution while being easy to interpret.</p>
<p>Numerous unsupervised OD algorithms have been proposed over the years (e.g., [15], [16], [17], [18], [19], [20]; we provide a more detailed overview in Section 2). These existing approaches have a few limitations. First of all, many of these methods, especially the ones requiring density estimation and pairwise distance calculation, suffer from the curse of dimensionality-both detection accuracy and runtime efficiency worsen rapidly as the number of data points and their dimensionality increase ([21], [22]). Second, most methods require hyperparameter tuning, which is</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>difficult in the unsupervised setting ([23], [24]).
To address these limitations, we propose a simple yet effective method using empirical cumulative distribution functions for outlier detection, abbreviated as ECOD. ECOD is motivated by the definition of outliers, which may be viewed as the rare events in the data ([25], [26]). Rare events are often the ones that appear in one of the tails of a distribution. As a concrete example, if the data are sampled from a one-dimensional Gaussian, then points in the left or right tails are often considered rare events or outliers. This has motivated commonly used heuristic OD approaches such as the "three-sigma" rule that declares points more than three standard deviations from the mean to be outliers ([27], [28]; there's also a robust variant called the " 1.5 IQR" rule [29]). However, the three-sigma rule only uses the mean and standard deviation of a distribution. Instead, one could capture more information of the distribution by building a histogram of the data and using bins with low counts to determine where outliers are [30]. However, such an approach requires tuning over different ways to bin the data for histogram construction. Our approach ECOD avoids this problem of tuning altogether by estimating the empirical cumulative distribution function (ECDF) of the data, which has no parameters to tune and approximates the entire distribution without making any parametric assumptions.</p>
<p>The technical difficulty in using ECDFs arises when working with high-dimensional data: the joint ECDF over all variables converges more slowly to the true joint CDF as the number of dimensions increases [31]. Our approach ECOD sidesteps this issue in a straightforward manner: we compute a univariate ECDF for each dimension separately. Then to measure the outlyingness of a data point, we compute its tail probability across all dimensions via an independence assumption, which amounts to multiplying all the estimated tail probabilities from the different uni-</p>
<p>variate ECDFs. We do this calculation in log space and in a manner that accounts for both the left and right tails of each of the dimensions. Despite this independence assumption appearing to be quite strong, ECOD turns out to work very well in practice.</p>
<p>In summary, we propose a novel OD approach ECOD that has the following key advantages:</p>
<ul>
<li>Effectiveness: Through extensive evaluation, we show that ECOD outperforms 11 popular baseline OD methods on 30 benchmarks. Specifically, ECOD ranks the highest, and scores $2 \%$ higher in the area under the receiver operating characteristic curve and $5 \%$ higher in average precision than the second best detector.</li>
<li>Efficiency and scalability: ECOD has time complexity $\mathcal{O}(n d)$, where $n$ is the number of data points and $d$ is the number of dimensions, and can trivially be parallelized across dimensions. Moreover, since ECOD has no hyperparameters, there is no time spent on hyperparameter tuning. With a single thread, ECOD can handle datasets with 1,000,000 observations and 10,000 features on a standard personal laptop in 2 hours.</li>
<li>Interpretability: ECOD is easy to interpret. For any data point, we can look at its left or right estimated tail probability per dimension. This tells us how each dimension contributes to the overall outlier score we use. This information guides practitioners regarding which dimensions to focus on for improving data quality.
The rest of this paper is organized as follows: we review some existing OD techniques and their strengths and limitations in Section 2. We describe the algorithmic design of ECOD and its properties in Section 3. We carry out experiments to compare ECOD with state-of-the-art OD methods and demonstrate that it is one of the most accurate, efficient, and scalable methods in Section 4. Finally, we conclude the paper in Section 5 with future directions, including a discussion on how to remove the independence assumption in how we aggregate tail probability information across dimensions. To facilitate reproducibility and accessibility, we open-source ECOD with distributed learning support as part of the popular PyOD library ${ }^{1}$.</li>
</ul>
<h2>2 Related Work</h2>
<p>Over the years, different types of unsupervised outlier detection algorithms have been proposed ([32], [33]). In this section, we give an overview of key methods for tabular data. Among them, eleven state-of-the-art algorithms are selected as baselines for our experiments in Section 4.</p>
<h3>2.1 Proximity-based Algorithms</h3>
<p>First, proximity-based algorithms, as their name suggests, are based on local neighborhood information around each point. These algorithms can be categorized into density- and distance-based algorithms [32].</p>
<p>The core principle behind density-based outlier detection methods is that an outlier can be found in a low-density region, whereas non-outliers (also called inliers) are assumed to appear in dense neighborhoods. For example, the local</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>outlier factor, proposed by Breunig et al. [34], is based on the concept of local density, where locality is given by the $k$ nearest neighbors. Distances of these nearest neighbors are then used to help estimate local densities. We can declare a data point with substantially lower local density than its neighbors to be an outlier. A few improvements were later introduced, including the Connective-based Outlier Factor (COF) by Tang et al. [35], and LOcal Correlation Integral (LOCI) by Papadimitriou et al. [36].</p>
<p>Distance-based methods compare objects to their neighbors, and those considerably far from their neighbors are deemed outliers. This procedure does not estimate local densities. In determining which points are considered neighbors, commonly the $k$ nearest neighbors are used [15].</p>
<p>Proximity-based methods are mostly nonparametric as they do not assume any parametric distribution for the data. This can be a key advantage in outlier detection as minimal prior knowledge is required on the probability distribution of the dataset. This makes them intuitive and easy to understand. However, this family of methods is typically computationally expensive, sensitive to hyperparameters such as how to define the neighbors (e.g., which distance/similarity function to use, how many neighbors to consider), and vulnerable to the curse of dimensionality. Our proposed algorithm ECOD address both drawbacks-it has no hyperparameters to tune, and has time complexity that scales linearly in the dataset size and dimensions.</p>
<h3>2.2 Statistical Models</h3>
<p>In the context of OD, approaches based on statistical models first fit probability distributions to data points. They then determine whether points are outliers based on the fitted models. These approaches are usually categorized into two main groups-parametric and nonparametric methods. The key difference is that parametric methods assume that the data come from a parametric distribution, so fitting such a distribution amounts to learning the parameters of the assumed parametric distribution. Common parametric methods for OD include using Gaussian mixture models (GMM) [37] and linear regression [38]. In contrast, nonparametric methods do not assume a parametric model for the data. Some examples include Kernel Density Estimation (KDE) [39], histogram-based methods (HBOS) [30], and a few other variants. Typically, parametric models for OD are fast to use after the model fitting step, whereas nonparametric models for OD can be more expensive to work (for example, depending on the kernel used for KDE, the fitted model could-for an exact solution-require comparing a candidate test data point with all training data in deciding whether the test point is an outlier or not).</p>
<p>Note that our proposed algorithm ECOD uses a statistical model that is nonparametric although this model cannot represent all possible multivariate distributions. In particular, we model each dimension of the data in a fully nonparametric fashion (with a univariate ECDF) but aggregate information across dimensions. We assume that the different dimensions are independent. Thus, even though our approach has no parameters, it does make a strong structural assumption on the underlying joint probability distribution over the different dimensions/features.</p>
<h3>2.3 Learning-based Models</h3>
<p>Learning-based approaches involve training machine learning models to predict which points are outliers. Based on a training set, a classification model is trained to classify outliers and inliers. The same model predicts which points in the test set are outliers. A classical method in this category is the one-class SVM (OCSVM) [16]. Another popular method leverages clustering to model the behavior of data points [40], and identifies outliers based on cluster assignments.</p>
<p>Using neural networks for detecting outliers has gained more attention in the last decade. Some representative examples from this line of work include the use of generative adversarial networks (GANs) [20], autoencoders [41] (including variational autoencoders), and reinforcement learning [42]. Recent developments in this direction can be found in the survey by Pang et al. [33].</p>
<p>Learning-based methods work well in practice on large datasets. However, most learning-based methods are computationally expensive. Moreover, they tend to involve nontrivial hyperparameter tuning, especially in the unsupervised setting. Lastly, state-of-the-art learning-based methods that use deep nets are often difficult to interpret.</p>
<h3>2.4 Ensemble-based models</h3>
<p>Ensemble-based approaches in OD combine results from various base outlier detectors to produce more robust OD results. The intuition is similar to how ensembling works in standard classification/regression tasks (e.g., bagging, boosting, random forests). Notable works of ensemblebased OD include feature bagging [25] that uses various sub-feature spaces, isolation forests [17] that aggregate the information from multiple base trees, LSCP [18] that dynamically picks the best base estimator for each data point, and SUOD [21] that uses many heterogeneous estimators.</p>
<p>In general, ensemble-based methods for OD often work well in practice even for high-dimensional datasets. However, these methods also can involve nontrivial tuning, such as in selecting the right meta-detectors [23]. Additionally, ensemble-based methods are often less interpretable.</p>
<p>Our proposed algorithm ECOD could be considered an ensemble model in that we are learning a nonparametric statistical model per dimension of the data, and then we aggregate/ensemble the models across dimensions to detect outliers. Because each "base" model uses only a single dimension of the data and we straightforwardly aggregate the base models, ECOD is easy to interpret.</p>
<h2>3 Proposed Algorithm: ECOD</h2>
<p>We now present the details of ECOD in three subsections. First, we provide a problem statement of unsupervised outlier detection and the associated challenges in Section 3.1. We then provide the motivation and technical details of ECOD in Section 3.2. We discuss properties of ECOD in Section 3.3, including interpretability and scalability.</p>
<h3>3.1 Problem Formulation and Challenges</h3>
<p>We consider the following standard unsupervised outlier detection setup. We assume that we have $n$ data points $X_{1}, X_{2}, \ldots, X_{n} \in \mathbb{R}^{d}$ that are sampled i.i.d. We collectively
refer to this entire dataset by the matrix $\mathbf{X} \in \mathbb{R}^{n \times d}$, which is formed by stacking the different data points' vectors as rows. Given $\mathbf{X}$, an OD model $M$ assigns, for each data point $X_{i}$, an outlier score $O_{i} \in \mathbb{R}$ (higher means more likely an outlier).</p>
<p>There are a few notable challenges for unsupervised outlier detection:</p>
<ul>
<li>Curse of dimensionality: many OD algorithms described in Section 2 become less accurate or have computational scalability issues when the input dataset $\mathbf{X}$ is highdimensional ( $d$ is either larger than or of a similar order of magnitude as $n$ ), or the number of data points $n$ is large. Specifically for proximity-based methods, density estimation and (pairwise) distance calculation become more computationally expensive [21]. Moreover, in general, density estimation in the high-dimensional setting can require the number of data points to scale exponentially in the number of dimensions [43].</li>
<li>Limited interpretability: OD applications often require interpretability/explainability to a certain extent. For instance, in detecting whether a financial transaction is an outlier and considered "fraudulent", it could be helpful to provide some sort of evidence as to why a transaction is considered fraudulent or not. This capacity is often absent in most existing OD algorithms we are aware of.</li>
<li>Complexity in hyperparameter tuning: without access to any ground truth labels for which data points are outliers, model selection and hyperparameter tuning are challenging in existing OD algorithms [23].</li>
</ul>
<h3>3.2 The Proposed ECOD</h3>
<h3>3.2.1 Motivation and High-level Idea</h3>
<p>A natural way to characterize outliers is to take them to correspond to rare events that occur in low-density parts of probability distribution ([25], [26]). If the distribution is unimodal, then these rare events occur in the tails of the distribution. With this motivation, for each observation $X_{i}$, our method ECOD is based on computing the probability of observing a point at least as "extreme" as $X_{i}$ in terms of tail probabilities.</p>
<p>Specifically, let $F: \mathbb{R}^{d} \rightarrow[0,1]$ denote the joint cumulative distribution function (CDF) across all $d$ dimensions/features. In particular, $X_{1}, X_{2}, \ldots, X_{n}$ are assumed to be sampled i.i.d. from a distribution with joint CDF $F$. For a vector $z \in \mathbb{R}^{d}$, we denote its $j$-th entry as $z^{(j)}$, e.g., we write the $j$-th entry of $X_{i}$ as $X_{i}^{(j)}$. We use the random variable $X$ to denote a generic random variable with the same distribution as each $X_{i}$. Then by the definition of a joint CDF, for any $x \in \mathbb{R}^{d}$,</p>
<p>$$
F(x)=\mathbb{P}(\underbrace{X^{(1)} \leq x^{(1)}, X^{(2)} \leq x^{(2)}, \ldots, X^{(d)} \leq x^{(d)}}_{\text {abbreviated as " } X \leq x "})
$$</p>
<p>This probability is a measure of how "extreme" $X_{i}$ is in terms of left tails: the smaller $F\left(X_{i}\right)$ is, then the less likely a point $X$ sampled from the same distribution as $X_{i}$ will satisfy the inequality $X \leq X_{i}$ (again, this inequality needs to hold across all $d$ dimensions). Similarly, $1-F\left(X_{i}\right)$ is also a measure of how "extreme" $X_{i}$ is, however, looking</p>
<p>Algorithm 1 Unsupervised OD Using ECDF (ECOD)
Inputs: input data $\mathbf{X}=\left{X_{i}\right}<em i="i">{i=1}^{n} \in \mathbb{R}^{n \times d}$ with $n$ samples and $d$ features; $X</em>$ refers to the value of $j$-th feature of the $i$-th sample
Outputs: outlier scores $\mathbf{O}:=\operatorname{ECOD}(\mathbf{X}) \in \mathbb{R}^{n}$
1: for each dimension $j$ in $1, \ldots, d$ do
2: Estimate left and right tail ECDFs (using equations (1) and 2, which we reproduce below):
left tail ECDF: $\widehat{F}}^{(j)<em i="1">{\text {left }}^{(j)}(z)=\frac{1}{n} \sum</em>}^{n} \mathbb{1}\left{X_{i}^{(j)} \leq z\right}$ for $z \in \mathbb{R}$, right tail ECDF: $\widehat{F<em i="1">{\text {right }}^{(j)}(z)=\frac{1}{n} \sum</em>$.
3: Compute the sample skewness coefficient for the $j$-th feature's distribution:}^{n} \mathbb{1}\left{X_{i}^{(j)} \geq z\right}$ for $z \in \mathbb{R</p>
<p>$$
\gamma_{j}=\frac{\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}^{(j)}-\overline{X^{(j)}}\right)^{3}}{\left[\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}^{(j)}-\overline{X^{(j)}}\right)^{2}\right]^{3 / 2}}
$$</p>
<p>where $\overline{X^{(j)}}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{(j)}$ is the sample mean of the $j$-th feature.
4: end for
5: for each sample $i$ in $1, \ldots, n$ do
6: Aggregate tail probabilities of $X_{i}$ to obtain outlier score $O_{i}$ :</p>
<p>$$
/ / \$ 3.2 .2
$$</p>
<p>$$
\begin{aligned}
O_{\text {left-only }}\left(X_{i}\right)= &amp; -\sum_{j=1}^{d} \log \left(\widehat{F}<em i="i">{\text {left }}^{(j)}\left(X</em>\right)\right) \
O_{\text {right-only }}\left(X_{i}\right)= &amp; -\sum_{j=1}^{d} \log \left(\widehat{F}}^{(j)<em i="i">{\text {right }}^{(j)}\left(X</em>\right)\right) \
O_{\text {auto }}\left(X_{i}\right)= &amp; -\sum_{j=1}^{d}\left[\mathbb{1}\left{\gamma_{j}&lt;0\right} \log \left(\widehat{F}}^{(j)<em i="i">{\text {left }}^{(j)}\left(X</em>\right)\right)\right. \
&amp; \left.+\mathbb{1}\left{\gamma_{j} \geq 0\right} \log \left(\widehat{F}}^{(j)<em i="i">{\text {right }}^{(j)}\left(X</em>\right)\right)\right]
\end{aligned}
$$}^{(j)</p>
<p>7: Set the final outlier score for point $X_{i}$ to be</p>
<p>$$
O_{i}=\max \left{O_{\text {left-only }}\left(X_{i}\right), O_{\text {right-only }}\left(X_{i}\right), O_{\text {auto }}\left(X_{i}\right)\right}
$$</p>
<p>8: end for
9: Return outlier scores $\mathbf{O}=\left(O_{1}, \ldots, O_{n}\right)$
at the right tails of every dimension instead of the left tails. Therefore, if either $F_{\mathbf{X}}\left(X_{i}\right)$ or $1-F_{\mathbf{X}}\left(X_{i}\right)$ is extremely small, then this suggests that $X_{i}$ corresponds to a rare event and is, therefore, likely to be an outlier.</p>
<p>The challenge is that in practice, we do not know the true joint CDF and have to estimate it from data. The rate of convergence in estimating a joint CDF using a joint ECDF slows down as the number of dimensions increases [31]. As a simplifying assumption, we assume that the different dimensions/features are independent so that joint CDF has the factorization</p>
<p>$$
F(x)=\prod_{j=1}^{d} F^{(j)}\left(x^{(j)}\right) \quad \text { for } x \in \mathbb{R}^{d}
$$</p>
<p>where $F^{(j)}: \mathbb{R} \rightarrow[0,1]$ denotes the univariate CDF of the $j$-th dimension: $F^{(j)}(z)=\mathbb{P}\left(X^{(j)} \leq z\right)$ for $z \in \mathbb{R}$.</p>
<p>Now it suffices to note that univariate CDF's can be accurately estimated simply by using the empirical CDF (ECDF), namely:</p>
<p>$$
\widehat{F}<em i="1">{\text {left }}^{(j)}(z):=\frac{1}{n} \sum</em>
$$}^{n} \mathbb{1}\left{X_{i}^{(j)} \leq z\right} \quad \text { for } z \in \mathbb{R</p>
<p>where $\mathbb{1}{\cdot}$ is the indicator function that is 1 when its argument is true and is 0 otherwise.</p>
<p>Previously, we mentioned that the right tail could be obtained by looking at 1 minus a CDF. However, we remark that there is a slight asymmetry in doing this since</p>
<p>$$
1-F^{(j)}(z)=1-\mathbb{P}\left(X^{(j)} \leq z\right)=\mathbb{P}\left(\underbrace{X^{(j)}&gt;z}_{\text {note the strict inequality }}\right)
$$</p>
<p>Basically $F^{(j)}(z)$ does not use a strict inequality whereas $1-F^{(j)}(z)$ does. In how we aggregate tail probabilities, we will combine information from both left and right tails, and for symmetry, we actually separately also compute the "right-tail" ECDF:</p>
<p>$$
\widehat{F}<em i="1">{\text {right }}^{(j)}(z):=\frac{1}{n} \sum</em>
$$}^{n} \mathbb{1}\left{X_{i}^{(j)} \geq z\right} \quad \text { for } z \in \mathbb{R</p>
<p>Thus, we can estimate the joint left and right-tail ECDFs across all $d$ dimensions under an independence assumption via the estimates</p>
<p>$$
\begin{gathered}
\widehat{F}<em j="1">{\text {left }}(x)=\prod</em>}^{d} \widehat{F<em _right="{right" _text="\text">{\text {left }}^{(j)}\left(x^{(j)}\right) \text { and } \widehat{F}</em>\right) \
\text { for } x \in \mathbb{R}^{d}
\end{gathered}
$$}}(x)=\prod_{j=1}^{d} \widehat{F}_{\text {right }}^{(j)}\left(x^{(j)</p>
<p>Our proposed method ECOD builds on the ideas we just presented and consists of two main steps: first, we compute each dimension's left- and right-tail ECDFs as given in equations (1) and (2). Next, for every point $X_{i}$, we aggregate its tail probabilities $\widehat{F}<em i="i">{\text {left }}^{(j)}\left(X</em>}^{(j)}\right)$ and $\widehat{F<em i="i">{\text {right }}^{(j)}\left(X</em>$ possible combinations of whether to use the left or right tail probability per dimension. Our aggregation step uses the skewness of a dimension's distribution to automatically select whether we use the left or the right tail probability for a dimension. The pseudocode of ECOD is given in Algorithm 1.}^{(j)}\right)$ to come up with a final outlier score $O_{i} \in[0, \infty)$; higher means more likely to be an outlier. Note that these outlier scores are not probabilities and are meant to be used to compare the data points. We explain in more detail how we aggregate tail probabilities in Section 3.2.2. An important idea we use in aggregating tail probabilities is that it does not always make sense to only consider the left tail probability for every dimension and then separately only consider the right tail probability for every dimension. Meanwhile, especially when $d$ is large, it is impractical to consider all $2^{d</p>
<h3>3.2.2 Aggregating Outlier Scores</h3>
<p>Using skewness to decide on whether to use the left or the right tail probability for a specific dimension. Using only the left tail probability for every dimension or only the right tail probability for every dimension can be</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: An illustration of how using different tail probabilities affects the results. The leftmost column is plots of the ground truth; the second column is the result if left tail probabilities are used. The middle column corresponds to outlier detection if right tail probabilities are used, followed by the average of both tail probabilities in the fourth column and skewness corrected tail probabilities (SC) in the rightmost column (which shows the best result).</p>
<p>limiting in finding outliers it could be that in a specific dimension, being in the left tail could be considered more of an outlier whereas in another dimension, we should instead consider the right tail. To automatically decide which tail we should consider for a particular dimension, we consider the skewness of that dimension's distribution.</p>
<p>Consider Fig. 1, where we demonstrate the effect of using different tail probabilities in catching outliers. The subfigure on the left-most column is the ground truth of the synthetic dataset, where outliers are labeled in red and the normal points are labeled in blue. The dataset is generated by taking 180 samples from a normal distribution centered at the top right corner of a unit square as inliers, combined with 20 samples from a uniform distribution on the square as outliers. In the next three columns, we show the results of using left tail probabilities, right tail probabilities, and averages of both tail probabilities to denote outlyingness, respectively. For each situation, we consider the same number of outliers (i.e., 20).</p>
<p>Because of the nature of this dataset (i.e., all outliers fall on the left end of the 2-dimensional distribution), using the left tail probabilities works surprising well (Fig. 1, column 2), because all outliers, by the construction of the dataset, are "smaller" than inliers. On the other hand, using the right tail worked extremely poorly (Fig. 1, column 3), because there are no extremely large outliers. Thus, it would have wrongfully captured the relatively large points as outliers. Taking an average of the two tail probabilities (Fig. 1, column 4) compromises the situation, and identifies half of the left tail outliers and misidentifies half of the right tail. We should point out that if the dataset is flipped, and all outliers appear on the top right corner while the inliers appear on the bottom left, then using the right tail probabilities for identifying outliers would have worked perfectly, while left tail probabilities would perform poorly. Averaging, on the other hand, works if both large and small outliers occur.</p>
<p>To overcome this, we note that the skewness of the dataset plays a major role in whether left tail probabilities or right tail probabilities should be used. In this example, both marginals of $X_1$ and $X_2$ skew negatively (i.e., the left tail is longer and the mass of the distribution is concentrated on the right). In this case, it makes sense to use the left tail probabilities. For dimensions that skew positively, using the right tail probabilities would become a better choice. The right-most column of Fig. 1 demonstrates the outcome of ECOD of tail selection is based on the skewness of each dimension, and we can see that it is able to capture the correct outliers. We called this the skewness corrected (SC) version of ECOD and we compare the performance of different variants of ECOD using different tail probabilities in Section 4.2.</p>
<p>We, therefore, propose to decide which tail to use in ECOD based on the skewness of the underlying distribution, where the sample skewness coefficient of dimension $j$ can be calculated as below [44]:</p>
<p>$$
\gamma_j = \frac{\frac{1}{n} \sum_{i=1}^{n} (X_i^{(j)} - \overline{X^{(j)}})^3}{\left[\frac{1}{n-1} \sum_{i=1}^{n} (X_i^{(j)} - \overline{X^{(j)}})^2\right]^{3/2}}
$$</p>
<p>where $\overline{X^{(j)}} = \frac{1}{n} \sum_{i=1}^{n} X_i^{(j)}$. When $\gamma_j &lt; 0$, we can thus consider points in the left tail to be more outlying. When $\gamma &gt; 0$, we instead consider points in the right tail to be more outlying.</p>
<p><strong>Final aggregation step.</strong> To compute the final outlier score per data point, we simply work in the negative log probability space. In particular, we compute equation (3) as:</p>
<p>$$
O_{\text{left-only}}(X_i) := -\log \widehat{F}<em j="1">{\text{left}}(X_i) = -\sum</em>
$$}^{d} \log (\widehat{F}_{\text{left}}^{(j)}(X_i^{(j)})), \tag{4</p>
<p>$$
O_{\text{right-only}}(X_i) := -\log \widehat{F}<em j="1">{\text{right}}(X_i) = -\sum</em>
$$}^{d} \log (\widehat{F}_{\text{right}}^{(j)}(X_i^{(j)})). \tag{5</p>
<p>Meanwhile, we also compute the "automatic" version that decides whether to use the left or the right tail of the $j$-th dimension based on whether $\gamma_j &lt; 0$ or $\gamma_j &gt; 0$. Note that if the data are sampled from a continuous random variable, then $\gamma = 0$ with probability 0, so for simplicity, we just break the tie in favor of one of the two directions. We specifically</p>
<p>use the automatic outlier score:</p>
<p>$$
\begin{aligned}
O_{\text {auto }}\left(X_{i}\right)=-\sum_{j=1}^{d}[ &amp; \mathbb{1}\left{\gamma_{j}&lt;0\right} \log \left(\widetilde{F}<em i="i">{\text {left }}^{(j)}\left(X</em>\right)\right) \
&amp; +\mathbb{1}\left{\gamma_{j} \geq 0\right} \log \left(\widetilde{F}}^{(j)<em i="i">{\text {right }}^{(j)}\left(X</em>\right)\right)
\end{aligned}
$$}^{(j)</p>
<p>Since we are operating in negative log probability space, lower probability (which suggests a more rare occurrence) corresponds to higher negative log probability. We use whichever negative log probability score is highest as the final outlier score $O_{i}$ for point $X_{i}$, i.e.,</p>
<p>$$
O_{i}=\max \left{O_{\text {left-only }}\left(X_{i}\right), O_{\text {right-only }}\left(X_{i}\right), O_{\text {auto }}\left(X_{i}\right)\right}
$$</p>
<p>Essentially we are using the most extreme of the three scores computed.</p>
<h3>3.3 Properties of ECOD</h3>
<h3>3.3.1 ECOD as an Interpretable Outlier Detector</h3>
<p>Interpretability in machine learning is an important concept, as it provides domain experts some insights into how algorithms make their decisions [45], [46]. Interpretable algorithms provide both transparency and reliability. Having a transparent model means that humans can learn from the thought process of models, and try to discover the "whys" behind why a particular data point is classified. Clearly, interpretability is also critical in outlier detection applications [47]. For instance, explaining why a transaction is fraudulent is equally important to identifying it.</p>
<p>As ECOD evaluates the outlying behaviors on a dimensional basis, we could use ECOD as an explainable detector for dimensional contribution. From equation 6, we know that if a sample has a large outlier score, then at least one of the three tail probabilities is large. Thus, let $O_{i}^{(j)}$ be the dimensional outlier score for dimension $j$ of $X_{i}$, and using the fact that the $\log$ function is monotonic, we can see that it represents the degree of outlyingness of dimension $j$. This can be compared against some preset thresholds, such as $-\log (0.01)=4.61$, or top $\alpha$ percent of $O\left(X_{i}^{(j)}\right) \forall i$ to give practitioners some indications why particular points are considered outliers. By plotting its corresponding graph (i.e., Dimensional Outlier Graph), a more direct understanding of features' contribution can be acquired.
Illustration with Breast Cancer Wisconsin Dataset. We demonstrate the interpretability aspect of ECOD with the Breast Cancer Wisconsin (Diagnostic) Data Set (Breastw, Table 1) as an example. Cell samples are provided by Dr. William H. Wolberg from the University of Wisconsin as part of his reports on clinical cases [48]. The data used has 369 samples, with 9 features listed below, and two outcome classes (benign and malignant). Features are scored on a scale of 1-10, and are (1) Clump Thickness, (2) Uniformity of Cell Size, (3) Uniformity of Cell Shape, (4) Marginal Adhesion, (5) Single Epithelial Cell Size, (6) Bare Nuclei, (7) Bland Chromatin, (8) Normal Nucleoli and (9) Mitoses.</p>
<p>We are particularly interested in providing an explanation, in addition to giving the right classification of outliers (malignant samples), as this can provide useful guidance for physicians to further investigate why certain cells are
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Dimensional Outlier Graph for sample 70 (classified as an outlier) of BreastW dataset; dimension 2, 5, 9 are identified with most contribution to its outlyingness.
potentially harmful. We illustrate how ECOD attempts to explain its outlier detection process. For each dimension $j$ ( 9 dimensions in total) of the $i$-th data point, we plot $O\left(X_{i}^{(j)}\right)$, the dimensional outlier score in black. Clearly, different dimensions show different contributions. We also plot the $99^{\text {th }}$ percentile band in green as a reference line (which corresponds to $1-\alpha$ percentage of outliers). In the analysis below, we plot the Dimensional Outlier Graph for $70-t h$ sample $(i=70)$, which is malignant (outlier) and has been successfully classified by ECOD.</p>
<p>We see that for dimension/feature 2, 5, and 9, the dimensional outlier scores have "touched" the $99^{\text {th }}$ percentile band, while all other features remain below the contamination rate band. This suggests that this point is likely an outlier because it is extremely non-uniform in size (dim. 2), has a large single epithelial cell size (dim. 5), and is more likely to reproduce via mitosis rather than meiosis (dim. 9).</p>
<h3>3.3.2 Complexity Analysis</h3>
<p>ECOD has $\mathcal{O}(n d)$ time and space complexity, and we break down the analysis by steps. In the estimation steps (Section 3.2.1), computing ECDF for all $d$ dimensions using $n$ samples leads to $\mathcal{O}(n d)$ time and space complexity. In the aggregation steps (Section 3.2.2), tail probability calculation and aggregation also lead to $\mathcal{O}(n d)$ time and space complexity.</p>
<h3>3.3.3 Acceleration by Distributed Learning</h3>
<p>Since ECOD estimates each dimension independently, it is suited for distributed learning acceleration with multiple workers. Given there are $t$ available workers for distributed computing, e.g., $t$ cores on a single machine. This constructs the worker pool as $\mathcal{W}=\left{W_{1}, \ldots, W_{t}\right}$. Without distributed learning, ECOD will estimate each dimension $j \in{1, \ldots, d}$ of $\mathbf{X}$ iteratively, e.g., with a for loop. With multiple workers available $(t&gt;1)$, a generic scheduling system equally splits $d$ ECDF estimation and tail probability computation tasks into $t$ groups, so each available worker in $\mathcal{W}$ will process roughly $\left\lceil\frac{d}{t}\right\rceil$ estimation and computation task. Similar to the non-distributed setting, the results from each dimension will then be aggregated to generate the final outlier scores. We provide both single-thread and distributed implementations in PyOD [49].</p>
<p>TABLE 1: 30 real-world benchmark datasets used in this study for evaluation. The datasets from the ODDS repo. are appended with "(mat)", and the datasets from the DAMI repo. are appended with "(arff)". For the Shuttle dataset in both databases, we randomly subsample 10,000 samples.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># Samp. (o)</th>
<th style="text-align: center;"># Dims. (d)</th>
<th style="text-align: center;">\% Outlier</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Arrhythmia (mat)</td>
<td style="text-align: center;">452</td>
<td style="text-align: center;">274</td>
<td style="text-align: center;">14.601</td>
</tr>
<tr>
<td style="text-align: center;">Breastw (mat)</td>
<td style="text-align: center;">683</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">34.992</td>
</tr>
<tr>
<td style="text-align: center;">Cardio (mat)</td>
<td style="text-align: center;">1831</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">9.612</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere (mat)</td>
<td style="text-align: center;">351</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">35.897</td>
</tr>
<tr>
<td style="text-align: center;">Lympho (mat)</td>
<td style="text-align: center;">148</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">4.054</td>
</tr>
<tr>
<td style="text-align: center;">Mammography (mat)</td>
<td style="text-align: center;">11183</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2.325</td>
</tr>
<tr>
<td style="text-align: center;">Optdigits (mat)</td>
<td style="text-align: center;">5216</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">2.875</td>
</tr>
<tr>
<td style="text-align: center;">Pima (mat)</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">34.895</td>
</tr>
<tr>
<td style="text-align: center;">Satellite (mat)</td>
<td style="text-align: center;">6435</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">31.639</td>
</tr>
<tr>
<td style="text-align: center;">Satimage-2 (mat)</td>
<td style="text-align: center;">5803</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">1.223</td>
</tr>
<tr>
<td style="text-align: center;">Shuttle (mat) (*)</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">7.120</td>
</tr>
<tr>
<td style="text-align: center;">Speech (mat)</td>
<td style="text-align: center;">3686</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">1.654</td>
</tr>
<tr>
<td style="text-align: center;">WBC (mat)</td>
<td style="text-align: center;">278</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">5.555</td>
</tr>
<tr>
<td style="text-align: center;">Wine (mat)</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">7.751</td>
</tr>
<tr>
<td style="text-align: center;">Arrhythmia (arff)</td>
<td style="text-align: center;">450</td>
<td style="text-align: center;">259</td>
<td style="text-align: center;">45.777</td>
</tr>
<tr>
<td style="text-align: center;">Cardiotocography (arff)</td>
<td style="text-align: center;">2114</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">22.043</td>
</tr>
<tr>
<td style="text-align: center;">HeartDisease (arff)</td>
<td style="text-align: center;">270</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">44.444</td>
</tr>
<tr>
<td style="text-align: center;">Hepatitis (arff)</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">16.250</td>
</tr>
<tr>
<td style="text-align: center;">InternetAds (arff)</td>
<td style="text-align: center;">1966</td>
<td style="text-align: center;">1555</td>
<td style="text-align: center;">18.718</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere (arff)</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">34.895</td>
</tr>
<tr>
<td style="text-align: center;">KDDCup99 (arff)</td>
<td style="text-align: center;">4207</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">39.909</td>
</tr>
<tr>
<td style="text-align: center;">Lymphography (arff)</td>
<td style="text-align: center;">148</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">4.05</td>
</tr>
<tr>
<td style="text-align: center;">Pima (arff)</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">34.90</td>
</tr>
<tr>
<td style="text-align: center;">Shuttle (arff) (*)</td>
<td style="text-align: center;">10000</td>
<td style="text-align: center;">41</td>
<td style="text-align: center;">0.156</td>
</tr>
<tr>
<td style="text-align: center;">SpamBase (arff)</td>
<td style="text-align: center;">4207</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">39.91</td>
</tr>
<tr>
<td style="text-align: center;">Stamps (arff)</td>
<td style="text-align: center;">340</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">9.12</td>
</tr>
<tr>
<td style="text-align: center;">Waveform (arff)</td>
<td style="text-align: center;">3443</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">2.904</td>
</tr>
<tr>
<td style="text-align: center;">WBC (arff)</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">4.484</td>
</tr>
<tr>
<td style="text-align: center;">WDBC (arff)</td>
<td style="text-align: center;">367</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">2.724</td>
</tr>
<tr>
<td style="text-align: center;">WPBC (arff)</td>
<td style="text-align: center;">198</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">23.737</td>
</tr>
</tbody>
</table>
<p>Our experiments answer the following questions:</p>
<p>1) Among the variants of ECOD using different tail probabilities, which one yields the best performance? (\$4.2)
2) How effective is ECOD, in comparison to state-of-theart (SOTA) outlier detectors? (\$4.3.1)
3) Under which conditions, the performance of ECOD may degrade? (\$4.3.2)
4) How efficient and scalable is ECOD regarding highdimensional, large datasets? (\$4.4)</p>
<h3>4.1 Experiment Setup</h3>
<p>Datasets. Table 1 summarises 30 public outlier detection benchmark datasets used in this study from ODDS ${ }^{1}$ [50] and DAMI ${ }^{2}$ [51] data repositories. Two special notes should be made about the datasets. First, multiple versions of the same dataset exist in different literature. For example, two versions of the Pima dataset exist: the .mat version has 8 dimensions and 768 observations, while the .arff version has 32 dimensions and 351 observations. This is because different subsets of the original data were used by different authors, and they keep different dimensions and/or observations. For clarity, we specify the source of a dataset by</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>appending "(mat)" (from ODDS) or "(arff)" (from DAMI) to its name. Second, because some baselines fail to converge for a large number of observations, we truncate both versions of Shuttle down to 10,000 observations. The truncation is done by taking 10,000 random rows from the original datasets (which have slightly over 60,000 observations each).
Evaluation metrics. In each experiment, $60 \%$ of the data is used for training and the remaining $40 \%$ is set aside for testing. Performance is evaluated by taking the average score of 10 independent trials using the area under the receiver operating characteristic (ROC) and average precision (AP). Both metrics are widely used in outlier mining research [52], [53], [54], [55]. We report both the raw comparison results and the critical difference (CD) plots to show statistical difference [56], [57], where it visualizes the statistical comparison by Wilcoxon signed-rank test with Holm's correction.
Baselines, Implementation, and Environment. We compare the performance of ECOD with 11 leading outlier detectors. We aim to include a variety of detectors to make the comparison robust. Specifically, the 11 competitors are AngleBased Outlier Detection (ABOD) [58], Clustering-Based Local Outlier Factor (CBLOF) [40], Histogram-based Outlier Score (HBOS) [30], Isolation Forest (IForest) [17], k Nearest Neighbors (KNN) [15], Lightweight On-line Detector of Anomalies (LODA) [59], Local Outlier Factor(LOF) [34], Locally Selective Combination in Parallel Outlier Ensembles (LSCP) [18], One-Class Support Vector Machines (OCSVM) [16], PCA-based outlier detector (PCA) [60], and Scalable Unsupervised Outlier Detection (SUOD) [21]. Their technical strength and limitations are discussed in Section 2.</p>
<p>Notably, PyOD is a popular open-source Python toolbox for performing scalable outlier detection on multivariate data [49]. A wide range of outlier detection algorithms are included under a single, well-documented API. This allows us to easily compare ECOD with other baseline detectors. Our implementation of ECOD is also under the framework of PyOD, and both single-process and distributed versions are readily available in $\mathrm{PyOD}^{3}$. For a fair comparison, we use the single-process version in this study. In the subsequent experiments, a Windows laptop with Intel i5-8265U @ 1.60 GHz quad-core CPU and 8GB of memory are used.</p>
<h3>4.2 The Effect of Using Different Tails with ECOD</h3>
<p>How does the usage of different tail probabilities affect the performance of ECOD? In the first experiment, we compare the performances of ECOD while using different tail probabilities for measuring sample outlyingness. Specifically, the three variants and ECOD are compared: (1) only using left tail probability (ECOD-L) (2) only using right tail probability (ECOD-R) (3) using the average of both tail probabilities (ECOD-B) and (4) using automatically selected tail probability (ECOD) as outlined in Section 3.2.2. Note that with the three variants, the outlier score of the $i$-th sample is equal to $O_{\text {left-only }}\left(X_{i}\right), O_{\text {right }}\left(X_{i}\right)$, and $\frac{1}{2}\left(O_{\text {left-only }}\left(X_{i}\right)+O_{\text {right-only }}\left(X_{i}\right)\right)$ respectively, where $O_{\text {left-only }}\left(X_{i}\right)$ and $O_{\text {right-only }}\left(X_{i}\right)$ are the sum of negative log probabilities defined in Eq. (5). Differently, ECOD uses both tails automatically as described in</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: 2-D embedding of selected datasets. In the first two datasets (<em>Breastw</em> and <em>Shuttle</em>), the outliers locate at the tail for at least one dimension, and ECOD is the best algorithm in comparison to the baselines. In the last two datasets (<em>Ionosphere</em> and <em>Speech</em>), where outliers do not locate at the tails in any dimension but mingle with inliers, ECOD's performance degrades.</p>
<p>TABLE 2: ROC score of 3 ECOD variants and ECOD (average of 10 independent trials, highest score highlighted in bold). Clearly, ECOD (the rightmost column) outperforms (ranking 1st in 18 out of 30 datasets).</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>ECOD-L</th>
<th>ECOD-R</th>
<th>ECOD-B</th>
<th>ECOD</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia (mat)</td>
<td>0.716</td>
<td>0.659</td>
<td>0.760</td>
<td>0.802</td>
</tr>
<tr>
<td>Breastw (mat)</td>
<td>0.665</td>
<td>0.641</td>
<td>0.774</td>
<td>0.993</td>
</tr>
<tr>
<td>Cardio (mat)</td>
<td>0.286</td>
<td>0.725</td>
<td>0.589</td>
<td>0.897</td>
</tr>
<tr>
<td>Ionosphere (mat)</td>
<td>0.765</td>
<td>0.523</td>
<td>0.810</td>
<td>0.830</td>
</tr>
<tr>
<td>Lympho (mat)</td>
<td>0.406</td>
<td>0.677</td>
<td>0.677</td>
<td>0.993</td>
</tr>
<tr>
<td>Mammography (mat)</td>
<td>0.718</td>
<td>0.384</td>
<td>0.706</td>
<td>0.894</td>
</tr>
<tr>
<td>Optdigits (mat)</td>
<td>0.993</td>
<td>0.997</td>
<td>0.997</td>
<td>0.732</td>
</tr>
<tr>
<td>Pima (mat)</td>
<td>0.550</td>
<td>0.990</td>
<td>1.000</td>
<td>0.663</td>
</tr>
<tr>
<td>Satellite (mat)</td>
<td>0.279</td>
<td>0.736</td>
<td>0.604</td>
<td>0.661</td>
</tr>
<tr>
<td>Satimage-2 (mat)</td>
<td>0.490</td>
<td>0.819</td>
<td>0.792</td>
<td>0.985</td>
</tr>
<tr>
<td>Shuttle (mat)</td>
<td>0.151</td>
<td>0.707</td>
<td>0.664</td>
<td>0.990</td>
</tr>
<tr>
<td>Speech (mat)</td>
<td>0.688</td>
<td>0.746</td>
<td>0.873</td>
<td>0.484</td>
</tr>
<tr>
<td>WBC (mat)</td>
<td>0.578</td>
<td>0.532</td>
<td>0.588</td>
<td>0.974</td>
</tr>
<tr>
<td>Wine (mat)</td>
<td>0.025</td>
<td>0.990</td>
<td>0.988</td>
<td>0.949</td>
</tr>
<tr>
<td>Arrhythmia (arff)</td>
<td>0.012</td>
<td>0.984</td>
<td>0.924</td>
<td>0.761</td>
</tr>
<tr>
<td>Cardiotocography (arff)</td>
<td>0.432</td>
<td>0.576</td>
<td>0.493</td>
<td>0.637</td>
</tr>
<tr>
<td>HeartDisease (arff)</td>
<td>0.805</td>
<td>0.687</td>
<td>0.821</td>
<td>0.672</td>
</tr>
<tr>
<td>Hepatitis (arff)</td>
<td>0.008</td>
<td>0.993</td>
<td>0.991</td>
<td>0.843</td>
</tr>
<tr>
<td>InternetAds (arff)</td>
<td>0.887</td>
<td>0.687</td>
<td>0.936</td>
<td>0.676</td>
</tr>
<tr>
<td>Ionosphere (arff)</td>
<td>0.970</td>
<td>0.316</td>
<td>0.933</td>
<td>0.821</td>
</tr>
<tr>
<td>KDDCup99 (arff)</td>
<td>0.730</td>
<td>0.383</td>
<td>0.715</td>
<td>0.997</td>
</tr>
<tr>
<td>Lymphography (arff)</td>
<td>0.884</td>
<td>0.732</td>
<td>0.995</td>
<td>0.998</td>
</tr>
<tr>
<td>Pima (arff)</td>
<td>0.241</td>
<td>0.892</td>
<td>0.898</td>
<td>0.646</td>
</tr>
<tr>
<td>Shuttle (arff)</td>
<td>0.460</td>
<td>0.666</td>
<td>0.693</td>
<td>0.879</td>
</tr>
<tr>
<td>SpamBase (arff)</td>
<td>0.225</td>
<td>0.755</td>
<td>0.584</td>
<td>0.700</td>
</tr>
<tr>
<td>Stamps (arff)</td>
<td>0.749</td>
<td>0.414</td>
<td>0.581</td>
<td>0.938</td>
</tr>
<tr>
<td>Waveform (arff)</td>
<td>0.926</td>
<td>0.813</td>
<td>0.973</td>
<td>0.786</td>
</tr>
<tr>
<td>WBC (arff)</td>
<td>0.461</td>
<td>0.458</td>
<td>0.450</td>
<td>0.990</td>
</tr>
<tr>
<td>WDBC (arff)</td>
<td>0.031</td>
<td>0.975</td>
<td>0.884</td>
<td>0.982</td>
</tr>
<tr>
<td>WPBC (arff)</td>
<td>0.040</td>
<td>0.965</td>
<td>0.748</td>
<td>0.546</td>
</tr>
<tr>
<td>AVG</td>
<td>0.560</td>
<td>0.714</td>
<td>0.781</td>
<td>0.824</td>
</tr>
</tbody>
</table>
<p>TABLE 3: Average precision (AP) of 3 ECOD variants and ECOD (average of 10 independent trials, highest score highlighted in bold). Clearly, ECOD (the rightmost column) outperforms (ranking 1st in 18 out of 30 datasets).</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>ECOD-L</th>
<th>ECOD-R</th>
<th>ECOD-B</th>
<th>ECOD</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia (mat)</td>
<td>0.503</td>
<td>0.318</td>
<td>0.497</td>
<td>0.472</td>
</tr>
<tr>
<td>Breastw (mat)</td>
<td>0.209</td>
<td>0.987</td>
<td>0.984</td>
<td>0.987</td>
</tr>
<tr>
<td>Cardio (mat)</td>
<td>0.450</td>
<td>0.163</td>
<td>0.587</td>
<td>0.579</td>
</tr>
<tr>
<td>Ionosphere (mat)</td>
<td>0.363</td>
<td>0.006</td>
<td>0.151</td>
<td>0.719</td>
</tr>
<tr>
<td>Lympho (mat)</td>
<td>0.592</td>
<td>0.334</td>
<td>0.622</td>
<td>0.893</td>
</tr>
<tr>
<td>Mammography (mat)</td>
<td>0.528</td>
<td>0.431</td>
<td>0.912</td>
<td>0.429</td>
</tr>
<tr>
<td>Optdigits (mat)</td>
<td>0.013</td>
<td>0.392</td>
<td>0.426</td>
<td>0.053</td>
</tr>
<tr>
<td>Pima (mat)</td>
<td>0.026</td>
<td>0.049</td>
<td>0.045</td>
<td>0.540</td>
</tr>
<tr>
<td>Satellite (mat)</td>
<td>0.229</td>
<td>0.611</td>
<td>0.460</td>
<td>0.585</td>
</tr>
<tr>
<td>Satimage-2 (mat)</td>
<td>0.550</td>
<td>0.259</td>
<td>0.523</td>
<td>0.859</td>
</tr>
<tr>
<td>Shuttle (mat)</td>
<td>0.113</td>
<td>0.044</td>
<td>0.670</td>
<td>0.980</td>
</tr>
<tr>
<td>Speech (mat)</td>
<td>0.015</td>
<td>0.022</td>
<td>0.022</td>
<td>0.019</td>
</tr>
<tr>
<td>WBC (mat)</td>
<td>0.035</td>
<td>0.772</td>
<td>0.464</td>
<td>0.782</td>
</tr>
<tr>
<td>Wine (mat)</td>
<td>0.044</td>
<td>0.666</td>
<td>0.218</td>
<td>0.608</td>
</tr>
<tr>
<td>Arrhythmia (arff)</td>
<td>0.710</td>
<td>0.636</td>
<td>0.751</td>
<td>0.751</td>
</tr>
<tr>
<td>Cardiotocography (arff)</td>
<td>0.370</td>
<td>0.294</td>
<td>0.493</td>
<td>0.377</td>
</tr>
<tr>
<td>HeartDisease (arff)</td>
<td>0.319</td>
<td>0.663</td>
<td>0.508</td>
<td>0.640</td>
</tr>
<tr>
<td>Hepatitis (arff)</td>
<td>0.444</td>
<td>0.221</td>
<td>0.434</td>
<td>0.584</td>
</tr>
<tr>
<td>InternetAds (arff)</td>
<td>0.156</td>
<td>0.511</td>
<td>0.512</td>
<td>0.510</td>
</tr>
<tr>
<td>Ionosphere (arff)</td>
<td>0.572</td>
<td>0.334</td>
<td>0.607</td>
<td>0.530</td>
</tr>
<tr>
<td>KDDCup99 (arff)</td>
<td>0.166</td>
<td>0.219</td>
<td>0.278</td>
<td>0.566</td>
</tr>
<tr>
<td>Lymphography (arff)</td>
<td>0.120</td>
<td>0.986</td>
<td>1.000</td>
<td>0.463</td>
</tr>
<tr>
<td>Pima (arff)</td>
<td>0.244</td>
<td>0.590</td>
<td>0.477</td>
<td>0.703</td>
</tr>
<tr>
<td>Shuttle (arff)</td>
<td>0.014</td>
<td>0.068</td>
<td>0.073</td>
<td>0.238</td>
</tr>
<tr>
<td>SpamBase (arff)</td>
<td>0.254</td>
<td>0.568</td>
<td>0.527</td>
<td>0.981</td>
</tr>
<tr>
<td>Stamps (arff)</td>
<td>0.199</td>
<td>0.220</td>
<td>0.352</td>
<td>0.085</td>
</tr>
<tr>
<td>Waveform (arff)</td>
<td>0.049</td>
<td>0.031</td>
<td>0.042</td>
<td>0.078</td>
</tr>
<tr>
<td>WBC (arff)</td>
<td>0.028</td>
<td>0.838</td>
<td>0.820</td>
<td>0.838</td>
</tr>
<tr>
<td>WDBC (arff)</td>
<td>0.015</td>
<td>0.842</td>
<td>0.622</td>
<td>0.840</td>
</tr>
<tr>
<td>WPBC (arff)</td>
<td>0.199</td>
<td>0.266</td>
<td>0.211</td>
<td>0.243</td>
</tr>
<tr>
<td>AVG</td>
<td>0.251</td>
<td>0.411</td>
<td>0.476</td>
<td>0.564</td>
</tr>
</tbody>
</table>
<p>Eq. (6) Table 2 and 3 show the results comparison regarding both ROC and AP on the benchmark datasets.</p>
<p><strong>The proposed ECOD achieves the best performance</strong>, with an average ROC of 0.824 and average precision of 0.564. It is better than ECOD-B using the average of two tails (avg. ROC: 0.781, avg. AP: 0.476), ECOD-R using right tails (avg. ROC: 0.714, avg. AP: 0.411), and ECOD-L using left tails (avg. ROC: 0.506, avg. AP: 0.251). Its superiority can be credited to the automatic selection of tail probabilities by the skewness of the underlying datasets. Clearly, the 30 benchmark datasets would have outliers on the different tails of the distribution, and none of the variants can capture this, although taking the average in ECOD-B alleviates the problem and therefore ranks at the second position. Consequently, we use the automatic version in ECOD by default, which is more carefully analyzed throughout the paper.</p>
<p>TABLE 4: ROC scores of detector performance (average of 10 independent trials, highest score highlighted in bold); rank is shown in parenthesis (lower is better). ECOD outperforms all baselines with the highest avg. score and ranks the highest in 13 out of 30 datasets.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>ABOD</th>
<th>CBLOF</th>
<th>HBOS</th>
<th>IForest</th>
<th>KNN</th>
<th>LODA</th>
<th>LOF</th>
<th>LSCP</th>
<th>OCSVM</th>
<th>PCA</th>
<th>SUOD</th>
<th>ECOD</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia (mat)</td>
<td>0.769 (11)</td>
<td>0.784 (7)</td>
<td>0.822 (1)</td>
<td>0.802 (3)</td>
<td>0.786 (6)</td>
<td>0.75 (12)</td>
<td>0.779 (10)</td>
<td>0.793 (5)</td>
<td>0.781 (9)</td>
<td>0.782 (8)</td>
<td>0.811 (2)</td>
<td>0.802 (3)</td>
</tr>
<tr>
<td>Breastw (mat)</td>
<td>0.396 (12)</td>
<td>0.965 (6)</td>
<td>0.983 (4)</td>
<td>0.987 (2)</td>
<td>0.976 (5)</td>
<td>0.987 (2)</td>
<td>0.47 (11)</td>
<td>0.933 (9)</td>
<td>0.961 (7)</td>
<td>0.959 (8)</td>
<td>0.929 (10)</td>
<td>0.994 (1)</td>
</tr>
<tr>
<td>Cardio (mat)</td>
<td>0.569 (12)</td>
<td>0.81 (9)</td>
<td>0.835 (8)</td>
<td>0.923 (3)</td>
<td>0.724 (10)</td>
<td>0.856 (7)</td>
<td>0.574 (11)</td>
<td>0.88 (6)</td>
<td>0.935 (2)</td>
<td>0.95 (1)</td>
<td>0.9 (4)</td>
<td>0.897 (5)</td>
</tr>
<tr>
<td>Ionosphere (mat)</td>
<td>0.925 (2)</td>
<td>0.897 (3)</td>
<td>0.561 (12)</td>
<td>0.847 (7)</td>
<td>0.927 (1)</td>
<td>0.798 (10)</td>
<td>0.875 (4)</td>
<td>0.857 (6)</td>
<td>0.842 (8)</td>
<td>0.796 (11)</td>
<td>0.862 (5)</td>
<td>0.831 (9)</td>
</tr>
<tr>
<td>Lympho (mat)</td>
<td>0.911 (11)</td>
<td>0.967 (10)</td>
<td>0.996 (1)</td>
<td>0.993 (3)</td>
<td>0.975 (9)</td>
<td>0.73 (12)</td>
<td>0.977 (7)</td>
<td>0.984 (5)</td>
<td>0.976 (8)</td>
<td>0.985 (4)</td>
<td>0.979 (6)</td>
<td>0.994 (2)</td>
</tr>
<tr>
<td>Mammography (mat)</td>
<td>0.549 (12)</td>
<td>0.821 (10)</td>
<td>0.834 (9)</td>
<td>0.86 (5)</td>
<td>0.841 (8)</td>
<td>0.881 (3)</td>
<td>0.729 (11)</td>
<td>0.855 (6)</td>
<td>0.872 (4)</td>
<td>0.882 (2)</td>
<td>0.851 (7)</td>
<td>0.894 (1)</td>
</tr>
<tr>
<td>Optdigits (mat)</td>
<td>0.467 (9)</td>
<td>0.769 (2)</td>
<td>0.873 (1)</td>
<td>0.721 (4)</td>
<td>0.371 (12)</td>
<td>0.398 (11)</td>
<td>0.45 (10)</td>
<td>0.658 (6)</td>
<td>0.5 (8)</td>
<td>0.509 (7)</td>
<td>0.68 (5)</td>
<td>0.733 (3)</td>
</tr>
<tr>
<td>Pima (mat)</td>
<td>0.679 (3)</td>
<td>0.658 (8)</td>
<td>0.7 (2)</td>
<td>0.678 (4)</td>
<td>0.708 (1)</td>
<td>0.592 (12)</td>
<td>0.627 (10)</td>
<td>0.664 (6)</td>
<td>0.622 (11)</td>
<td>0.648 (9)</td>
<td>0.666 (5)</td>
<td>0.664 (6)</td>
</tr>
<tr>
<td>Satellite (mat)</td>
<td>0.571 (11)</td>
<td>0.749 (2)</td>
<td>0.758 (1)</td>
<td>0.702 (4)</td>
<td>0.684 (5)</td>
<td>0.597 (10)</td>
<td>0.557 (12)</td>
<td>0.665 (6)</td>
<td>0.662 (7)</td>
<td>0.599 (9)</td>
<td>0.722 (3)</td>
<td>0.661 (8)</td>
</tr>
<tr>
<td>Satimage-2 (mat)</td>
<td>0.819 (11)</td>
<td>0.999 (1)</td>
<td>0.98 (9)</td>
<td>0.995 (3)</td>
<td>0.954 (10)</td>
<td>0.987 (5)</td>
<td>0.458 (12)</td>
<td>0.981 (8)</td>
<td>0.998 (2)</td>
<td>0.982 (7)</td>
<td>0.993 (4)</td>
<td>0.985 (6)</td>
</tr>
<tr>
<td>Shuttle (mat)</td>
<td>0.659 (9)</td>
<td>0.608 (10)</td>
<td>0.995 (4)</td>
<td>0.998 (1)</td>
<td>0.738 (8)</td>
<td>0.589 (11)</td>
<td>0.524 (12)</td>
<td>0.939 (7)</td>
<td>0.995 (4)</td>
<td>0.994 (6)</td>
<td>0.996 (3)</td>
<td>0.998 (1)</td>
</tr>
<tr>
<td>Speech (mat)</td>
<td>0.627 (1)</td>
<td>0.452 (9)</td>
<td>0.454 (8)</td>
<td>0.447 (11)</td>
<td>0.458 (6)</td>
<td>0.455 (7)</td>
<td>0.471 (3)</td>
<td>0.465 (5)</td>
<td>0.447 (11)</td>
<td>0.45 (10)</td>
<td>0.469 (4)</td>
<td>0.485 (2)</td>
</tr>
<tr>
<td>Wbc (mat)</td>
<td>0.905 (12)</td>
<td>0.92 (9)</td>
<td>0.952 (2)</td>
<td>0.931 (7)</td>
<td>0.937 (3)</td>
<td>0.913 (11)</td>
<td>0.935 (4)</td>
<td>0.931 (7)</td>
<td>0.932 (6)</td>
<td>0.916 (10)</td>
<td>0.935 (4)</td>
<td>0.975 (1)</td>
</tr>
<tr>
<td>Wine (mat)</td>
<td>0.431 (11)</td>
<td>0.284 (12)</td>
<td>0.9 (3)</td>
<td>0.816 (6)</td>
<td>0.518 (10)</td>
<td>0.787 (8)</td>
<td>0.905 (2)</td>
<td>0.862 (5)</td>
<td>0.636 (9)</td>
<td>0.801 (7)</td>
<td>0.868 (4)</td>
<td>0.949 (1)</td>
</tr>
<tr>
<td>Arrhythmia (arff)</td>
<td>0.74 (11)</td>
<td>0.751 (6)</td>
<td>0.749 (9)</td>
<td>0.757 (3)</td>
<td>0.751 (6)</td>
<td>0.703 (12)</td>
<td>0.749 (9)</td>
<td>0.757 (3)</td>
<td>0.756 (5)</td>
<td>0.751 (6)</td>
<td>0.773 (1)</td>
<td>0.762 (2)</td>
</tr>
<tr>
<td>Cardiotocography (arff)</td>
<td>0.459 (12)</td>
<td>0.567 (9)</td>
<td>0.594 (8)</td>
<td>0.683 (3)</td>
<td>0.49 (11)</td>
<td>0.674 (4)</td>
<td>0.505 (10)</td>
<td>0.645 (6)</td>
<td>0.698 (2)</td>
<td>0.755 (1)</td>
<td>0.66 (5)</td>
<td>0.637 (7)</td>
</tr>
<tr>
<td>HeartDisease (arff)</td>
<td>0.597 (6)</td>
<td>0.593 (7)</td>
<td>0.689 (1)</td>
<td>0.602 (4)</td>
<td>0.614 (3)</td>
<td>0.482 (12)</td>
<td>0.563 (10)</td>
<td>0.599 (5)</td>
<td>0.555 (11)</td>
<td>0.576 (9)</td>
<td>0.587 (8)</td>
<td>0.673 (2)</td>
</tr>
<tr>
<td>Hepatitis (arff)</td>
<td>0.716 (11)</td>
<td>0.759 (10)</td>
<td>0.796 (6)</td>
<td>0.794 (7)</td>
<td>0.811 (4)</td>
<td>0.608 (12)</td>
<td>0.831 (2)</td>
<td>0.784 (8)</td>
<td>0.764 (9)</td>
<td>0.804 (5)</td>
<td>0.814 (3)</td>
<td>0.843 (1)</td>
</tr>
<tr>
<td>InternetAds (arff)</td>
<td>0.643 (6)</td>
<td>0.617 (10)</td>
<td>0.699 (1)</td>
<td>0.685 (2)</td>
<td>0.622 (8)</td>
<td>0.528 (12)</td>
<td>0.606 (11)</td>
<td>0.675 (4)</td>
<td>0.623 (7)</td>
<td>0.622 (8)</td>
<td>0.673 (5)</td>
<td>0.676 (3)</td>
</tr>
<tr>
<td>Ionosphere (arff)</td>
<td>0.925 (1)</td>
<td>0.886 (3)</td>
<td>0.552 (12)</td>
<td>0.836 (8)</td>
<td>0.919 (2)</td>
<td>0.822 (9)</td>
<td>0.86 (5)</td>
<td>0.855 (6)</td>
<td>0.838 (7)</td>
<td>0.787 (11)</td>
<td>0.862 (4)</td>
<td>0.822 (9)</td>
</tr>
<tr>
<td>KDDCup99 (arff)</td>
<td>0.693 (11)</td>
<td>0.997 (1)</td>
<td>0.995 (5)</td>
<td>0.997 (1)</td>
<td>0.761 (9)</td>
<td>0.728 (10)</td>
<td>0.577 (12)</td>
<td>0.96 (8)</td>
<td>0.995 (5)</td>
<td>0.997 (1)</td>
<td>0.995 (5)</td>
<td>0.997 (1)</td>
</tr>
<tr>
<td>Lymphography (arff)</td>
<td>0.986 (11)</td>
<td>0.996 (7)</td>
<td>0.998 (3)</td>
<td>0.999 (1)</td>
<td>0.997 (4)</td>
<td>0.839 (12)</td>
<td>0.995 (8)</td>
<td>0.995 (8)</td>
<td>0.992 (10)</td>
<td>0.997 (4)</td>
<td>0.997 (4)</td>
<td>0.999 (1)</td>
</tr>
<tr>
<td>Pima (arff)</td>
<td>0.667 (3)</td>
<td>0.666 (4)</td>
<td>0.659 (6)</td>
<td>0.676 (2)</td>
<td>0.706 (1)</td>
<td>0.597 (12)</td>
<td>0.658 (7)</td>
<td>0.661 (5)</td>
<td>0.64 (11)</td>
<td>0.655 (8)</td>
<td>0.65 (9)</td>
<td>0.647 (10)</td>
</tr>
<tr>
<td>Shuttle (arff)</td>
<td>0.834 (10)</td>
<td>0.98 (2)</td>
<td>0.792 (11)</td>
<td>0.866 (9)</td>
<td>0.961 (4)</td>
<td>0.707 (12)</td>
<td>0.986 (1)</td>
<td>0.941 (6)</td>
<td>0.966 (3)</td>
<td>0.929 (7)</td>
<td>0.955 (5)</td>
<td>0.88 (8)</td>
</tr>
<tr>
<td>SpamBase (arff)</td>
<td>0.435 (11)</td>
<td>0.548 (7)</td>
<td>0.67 (2)</td>
<td>0.633 (3)</td>
<td>0.54 (8)</td>
<td>0.445 (10)</td>
<td>0.426 (12)</td>
<td>0.588 (4)</td>
<td>0.539 (9)</td>
<td>0.555 (6)</td>
<td>0.562 (5)</td>
<td>0.701 (1)</td>
</tr>
<tr>
<td>Stamps (arff)</td>
<td>0.762 (10)</td>
<td>0.73 (12)</td>
<td>0.918 (2)</td>
<td>0.913 (4)</td>
<td>0.874 (9)</td>
<td>0.901 (6)</td>
<td>0.739 (11)</td>
<td>0.906 (5)</td>
<td>0.88 (8)</td>
<td>0.917 (3)</td>
<td>0.898 (7)</td>
<td>0.939 (1)</td>
</tr>
<tr>
<td>Waveform (arff)</td>
<td>0.652 (10)</td>
<td>0.714 (5)</td>
<td>0.683 (7)</td>
<td>0.681 (8)</td>
<td>0.74 (3)</td>
<td>0.622 (12)</td>
<td>0.735 (4)</td>
<td>0.76 (2)</td>
<td>0.665 (9)</td>
<td>0.634 (11)</td>
<td>0.706 (6)</td>
<td>0.787 (1)</td>
</tr>
<tr>
<td>WBC (arff)</td>
<td>0.956 (11)</td>
<td>0.966 (10)</td>
<td>0.982 (3)</td>
<td>0.991 (1)</td>
<td>0.972 (8)</td>
<td>0.981 (4)</td>
<td>0.932 (12)</td>
<td>0.967 (9)</td>
<td>0.975 (6)</td>
<td>0.974 (7)</td>
<td>0.981 (4)</td>
<td>0.99 (2)</td>
</tr>
<tr>
<td>WDRC (arff)</td>
<td>0.903 (12)</td>
<td>0.909 (11)</td>
<td>0.967 (2)</td>
<td>0.946 (3)</td>
<td>0.927 (9)</td>
<td>0.946 (3)</td>
<td>0.92 (10)</td>
<td>0.937 (6)</td>
<td>0.938 (5)</td>
<td>0.929 (8)</td>
<td>0.932 (7)</td>
<td>0.983 (1)</td>
</tr>
<tr>
<td>WPBC (arff)</td>
<td>0.457 (12)</td>
<td>0.492 (11)</td>
<td>0.536 (2)</td>
<td>0.516 (6)</td>
<td>0.526 (3)</td>
<td>0.503 (8)</td>
<td>0.511 (7)</td>
<td>0.524 (5)</td>
<td>0.498 (9)</td>
<td>0.496 (10)</td>
<td>0.526 (3)</td>
<td>0.547 (1)</td>
</tr>
<tr>
<td>AVG</td>
<td>0.69 (12)</td>
<td>0.762 (8)</td>
<td>0.797 (5)</td>
<td>0.809 (2)</td>
<td>0.76 (9)</td>
<td>0.713 (10)</td>
<td>0.697 (11)</td>
<td>0.801 (4)</td>
<td>0.783 (7)</td>
<td>0.788 (6)</td>
<td>0.808 (3)</td>
<td>0.825 (1)</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Critical difference diagram showing pairwise statistical difference comparison of ECOD and the baselines regarding both ROC and AP. ECOD outperforms all baselines, and the result is statistically significant regarding ROC.</p>
<h3>4.3 Performance Comparison with Baselines</h3>
<h4>4.3.1 Overall Results</h4>
<p>ECOD consistently outperforms regarding both ROC and AP. Of the 30 datasets in Table 1, ECOD scores the highest in terms of ROC (see Table 4). It achieves an average ROC score of 0.825, which is 2% higher than the second best alternative—iForest. Notably, iForest has been the SOTA method in many large-scale outlier analyses [55], [61]. Moreover, ECOD ranks first in 13 out of 30 occasions and ranks in the top three places in 21 out of 30 occasions. The critical difference plot in Fig. 4 corroborates the findings—it shows that ECOD outperforms with a statistical significance.</p>
<p>Similarly, Table 5 shows that ECOD is also superior to the baselines regarding AP. Of the 12 OD methods, ECOD achieves an average AP score of 0.565, which bring 5% improvement over the second place—iForest. Additionally, ECOD ranks first in 12 out of 30 datasets, and ranks in the top three places on 20 datasets. The further analysis with the critical difference plot in Fig. 4 corroborates that ECOD outperforms the baselines.</p>
<p>Additionally, it is worthy to note that ECOD (avg. ROC: 0.825, avg. AP: 0.565) is better than HBOS (avg. ROC: 0.795, avg. AP: 0.509) with a similar mechanism but estimates histogram per dimension instead. We credit the edge to: (1) ECOD uses more information than HBOS—the latter does not capture the difference for the samples in the same bin and (2) HBOS needs to decide the number of bins, which is hard to tune under unsupervised settings.</p>
<h4>4.3.2 Case Study</h4>
<p>Although ECOD outperforms on most of the datasets, we notice that its performance degrades markedly on some of the datasets, as shown in Table 4 and 5. In the case study, we further investigate by visualizing the selected datasets in 2-D by t-SNE [62].</p>
<p>In Fig. 3, we first present two datasets (<em>Breastw (mat)</em> and <em>Shuttle (mat)</em>) where ECOD outperforms all baselines, and then two datasets (<em>Ionosphere (mat)</em> and <em>Speech (mat)</em>) where it does not achieve top performance. The visualization suggests that when outliers locate at the tails in at least some dimensions, ECOD could accurately capture them. However, its performance degrades when outliers are well</p>
<p>TABLE 5: Average precision (AP) of detector performance (average of 10 independent trials, highest score highlighted in bold); rank is shown in parenthesis (lower is better). ECOD outperforms all baselines with the highest avg. performance, and ranks highest in 11 out of 30 datasets.</p>
<table>
<thead>
<tr>
<th>Data</th>
<th>ABOD</th>
<th>CBLOF</th>
<th>HBOS</th>
<th>IForest</th>
<th>KNN</th>
<th>LODA</th>
<th>LOF</th>
<th>LSCP</th>
<th>OCSVM</th>
<th>PCA</th>
<th>SUOD</th>
<th>ECOD</th>
</tr>
</thead>
<tbody>
<tr>
<td>Arrhythmia (mat)</td>
<td>0.359 (12)</td>
<td>0.399 (9)</td>
<td>0.493 (3)</td>
<td>0.506 (1)</td>
<td>0.397 (10)</td>
<td>0.436 (5)</td>
<td>0.374 (11)</td>
<td>0.411 (6)</td>
<td>0.405 (7)</td>
<td>0.402 (8)</td>
<td>0.495 (2)</td>
<td>0.473 (4)</td>
</tr>
<tr>
<td>Breastw (mat)</td>
<td>0.295 (12)</td>
<td>0.914 (8)</td>
<td>0.953 (5)</td>
<td>0.972 (3)</td>
<td>0.927 (7)</td>
<td>0.978 (2)</td>
<td>0.322 (11)</td>
<td>0.826 (9)</td>
<td>0.934 (6)</td>
<td>0.96 (4)</td>
<td>0.791 (10)</td>
<td>0.988 (1)</td>
</tr>
<tr>
<td>Cardio (mat)</td>
<td>0.194 (11)</td>
<td>0.414 (8)</td>
<td>0.46 (6)</td>
<td>0.576 (3)</td>
<td>0.345 (10)</td>
<td>0.424 (7)</td>
<td>0.163 (12)</td>
<td>0.396 (9)</td>
<td>0.532 (4)</td>
<td>0.611 (1)</td>
<td>0.462 (5)</td>
<td>0.579 (2)</td>
</tr>
<tr>
<td>Ionosphere (mat)</td>
<td>0.914 (2)</td>
<td>0.871 (3)</td>
<td>0.366 (12)</td>
<td>0.789 (8)</td>
<td>0.924 (1)</td>
<td>0.716 (11)</td>
<td>0.821 (6)</td>
<td>0.818 (7)</td>
<td>0.823 (5)</td>
<td>0.736 (9)</td>
<td>0.824 (4)</td>
<td>0.719 (10)</td>
</tr>
<tr>
<td>Lympho (mat)</td>
<td>0.517 (11)</td>
<td>0.808 (9)</td>
<td>0.925 (2)</td>
<td>0.952 (1)</td>
<td>0.82 (7)</td>
<td>0.417 (12)</td>
<td>0.825 (6)</td>
<td>0.861 (4)</td>
<td>0.814 (8)</td>
<td>0.852 (5)</td>
<td>0.799 (10)</td>
<td>0.894 (3)</td>
</tr>
<tr>
<td>Mammography (mat)</td>
<td>0.023 (12)</td>
<td>0.137 (9)</td>
<td>0.122 (10)</td>
<td>0.233 (3)</td>
<td>0.17 (6)</td>
<td>0.281 (2)</td>
<td>0.118 (11)</td>
<td>0.169 (7)</td>
<td>0.188 (5)</td>
<td>0.199 (4)</td>
<td>0.164 (8)</td>
<td>0.429 (1)</td>
</tr>
<tr>
<td>Optdigits (mat)</td>
<td>0.028 (8)</td>
<td>0.062 (2)</td>
<td>0.196 (1)</td>
<td>0.055 (3)</td>
<td>0.022 (12)</td>
<td>0.024 (11)</td>
<td>0.029 (7)</td>
<td>0.043 (6)</td>
<td>0.028 (8)</td>
<td>0.028 (8)</td>
<td>0.046 (5)</td>
<td>0.053 (4)</td>
</tr>
<tr>
<td>Pima (mat)</td>
<td>0.511 (4)</td>
<td>0.477 (7)</td>
<td>0.569 (1)</td>
<td>0.503 (5)</td>
<td>0.515 (3)</td>
<td>0.408 (12)</td>
<td>0.43 (11)</td>
<td>0.47 (9)</td>
<td>0.461 (10)</td>
<td>0.478 (6)</td>
<td>0.473 (8)</td>
<td>0.541 (2)</td>
</tr>
<tr>
<td>Satellite (mat)</td>
<td>0.397 (11)</td>
<td>0.69 (1)</td>
<td>0.687 (2)</td>
<td>0.654 (3)</td>
<td>0.543 (9)</td>
<td>0.581 (8)</td>
<td>0.39 (12)</td>
<td>0.51 (10)</td>
<td>0.653 (4)</td>
<td>0.603 (6)</td>
<td>0.608 (5)</td>
<td>0.585 (7)</td>
</tr>
<tr>
<td>Satimage-2 (mat)</td>
<td>0.187 (11)</td>
<td>0.978 (1)</td>
<td>0.758 (7)</td>
<td>0.929 (3)</td>
<td>0.419 (9)</td>
<td>0.87 (5)</td>
<td>0.027 (12)</td>
<td>0.333 (10)</td>
<td>0.975 (2)</td>
<td>0.874 (4)</td>
<td>0.623 (8)</td>
<td>0.86 (6)</td>
</tr>
<tr>
<td>Shuttle (mat)</td>
<td>0.171 (11)</td>
<td>0.195 (10)</td>
<td>0.98 (3)</td>
<td>0.986 (1)</td>
<td>0.204 (9)</td>
<td>0.379 (8)</td>
<td>0.142 (12)</td>
<td>0.715 (7)</td>
<td>0.902 (6)</td>
<td>0.926 (4)</td>
<td>0.913 (5)</td>
<td>0.981 (2)</td>
</tr>
<tr>
<td>Speech (mat)</td>
<td>0.04 (1)</td>
<td>0.022 (6)</td>
<td>0.027 (3)</td>
<td>0.018 (11)</td>
<td>0.022 (6)</td>
<td>0.017 (12)</td>
<td>0.024 (5)</td>
<td>0.027 (3)</td>
<td>0.021 (9)</td>
<td>0.022 (6)</td>
<td>0.029 (2)</td>
<td>0.02 (10)</td>
</tr>
<tr>
<td>Wbc (mat)</td>
<td>0.355 (12)</td>
<td>0.5 (11)</td>
<td>0.663 (2)</td>
<td>0.59 (4)</td>
<td>0.529 (9)</td>
<td>0.564 (5)</td>
<td>0.558 (6)</td>
<td>0.554 (7)</td>
<td>0.514 (10)</td>
<td>0.534 (8)</td>
<td>0.602 (3)</td>
<td>0.783 (1)</td>
</tr>
<tr>
<td>Wine (mat)</td>
<td>0.084 (11)</td>
<td>0.06 (12)</td>
<td>0.405 (2)</td>
<td>0.279 (6)</td>
<td>0.095 (10)</td>
<td>0.278 (7)</td>
<td>0.361 (4)</td>
<td>0.299 (5)</td>
<td>0.141 (9)</td>
<td>0.254 (8)</td>
<td>0.364 (3)</td>
<td>0.608 (1)</td>
</tr>
<tr>
<td>Arrhythmia (arff)</td>
<td>0.699 (11)</td>
<td>0.712 (8)</td>
<td>0.75 (3)</td>
<td>0.746 (4)</td>
<td>0.712 (8)</td>
<td>0.697 (12)</td>
<td>0.704 (10)</td>
<td>0.718 (5)</td>
<td>0.716 (6)</td>
<td>0.714 (7)</td>
<td>0.751 (2)</td>
<td>0.752 (1)</td>
</tr>
<tr>
<td>Cardiotocography (arff)</td>
<td>0.247 (12)</td>
<td>0.363 (9)</td>
<td>0.366 (8)</td>
<td>0.434 (2)</td>
<td>0.311 (10)</td>
<td>0.432 (3)</td>
<td>0.258 (11)</td>
<td>0.374 (7)</td>
<td>0.419 (4)</td>
<td>0.478 (1)</td>
<td>0.398 (5)</td>
<td>0.378 (6)</td>
</tr>
<tr>
<td>HeartDisease (arff)</td>
<td>0.534 (4)</td>
<td>0.521 (9)</td>
<td>0.625 (2)</td>
<td>0.534 (4)</td>
<td>0.538 (3)</td>
<td>0.445 (12)</td>
<td>0.478 (11)</td>
<td>0.525 (8)</td>
<td>0.513 (10)</td>
<td>0.53 (6)</td>
<td>0.528 (7)</td>
<td>0.64 (1)</td>
</tr>
<tr>
<td>Hepatitis (arff)</td>
<td>0.33 (12)</td>
<td>0.374 (11)</td>
<td>0.473 (6)</td>
<td>0.442 (8)</td>
<td>0.475 (5)</td>
<td>0.396 (10)</td>
<td>0.499 (4)</td>
<td>0.472 (7)</td>
<td>0.427 (9)</td>
<td>0.543 (3)</td>
<td>0.557 (2)</td>
<td>0.585 (1)</td>
</tr>
<tr>
<td>InternetAds (arff)</td>
<td>0.276 (10)</td>
<td>0.315 (8)</td>
<td>0.535 (1)</td>
<td>0.49 (3)</td>
<td>0.281 (9)</td>
<td>0.242 (12)</td>
<td>0.262 (11)</td>
<td>0.387 (5)</td>
<td>0.316 (7)</td>
<td>0.32 (6)</td>
<td>0.475 (4)</td>
<td>0.51 (2)</td>
</tr>
<tr>
<td>Ionosphere (arff)</td>
<td>0.915 (1)</td>
<td>0.862 (3)</td>
<td>0.364 (12)</td>
<td>0.777 (8)</td>
<td>0.915 (1)</td>
<td>0.763 (9)</td>
<td>0.811 (7)</td>
<td>0.819 (6)</td>
<td>0.822 (5)</td>
<td>0.723 (10)</td>
<td>0.835 (4)</td>
<td>0.703 (11)</td>
</tr>
<tr>
<td>KDDCup99 (arff)</td>
<td>0.018 (12)</td>
<td>0.198 (5)</td>
<td>0.278 (1)</td>
<td>0.273 (2)</td>
<td>0.046 (10)</td>
<td>0.135 (7)</td>
<td>0.028 (11)</td>
<td>0.127 (8)</td>
<td>0.125 (9)</td>
<td>0.199 (4)</td>
<td>0.157 (6)</td>
<td>0.239 (3)</td>
</tr>
<tr>
<td>Lymphography (arff)</td>
<td>0.801 (11)</td>
<td>0.925 (7)</td>
<td>0.978 (3)</td>
<td>0.992 (1)</td>
<td>0.942 (6)</td>
<td>0.491 (12)</td>
<td>0.925 (7)</td>
<td>0.922 (9)</td>
<td>0.837 (10)</td>
<td>0.953 (4)</td>
<td>0.944 (5)</td>
<td>0.982 (2)</td>
</tr>
<tr>
<td>Pima (arff)</td>
<td>0.506 (5)</td>
<td>0.484 (7)</td>
<td>0.528 (2)</td>
<td>0.515 (4)</td>
<td>0.525 (3)</td>
<td>0.42 (12)</td>
<td>0.458 (11)</td>
<td>0.48 (9)</td>
<td>0.478 (10)</td>
<td>0.484 (7)</td>
<td>0.487 (6)</td>
<td>0.53 (1)</td>
</tr>
<tr>
<td>Shuttle (arff)</td>
<td>0.263 (5)</td>
<td>0.358 (3)</td>
<td>0.08 (12)</td>
<td>0.13 (10)</td>
<td>0.36 (2)</td>
<td>0.132 (9)</td>
<td>0.395 (1)</td>
<td>0.184 (7)</td>
<td>0.304 (4)</td>
<td>0.232 (6)</td>
<td>0.171 (8)</td>
<td>0.085 (11)</td>
</tr>
<tr>
<td>SpamBase (arff)</td>
<td>0.38 (10)</td>
<td>0.414 (8)</td>
<td>0.525 (2)</td>
<td>0.492 (3)</td>
<td>0.422 (6)</td>
<td>0.375 (11)</td>
<td>0.349 (12)</td>
<td>0.45 (4)</td>
<td>0.412 (9)</td>
<td>0.42 (7)</td>
<td>0.431 (5)</td>
<td>0.567 (1)</td>
</tr>
<tr>
<td>Stamps (arff)</td>
<td>0.247 (10)</td>
<td>0.241 (11)</td>
<td>0.398 (5)</td>
<td>0.394 (6)</td>
<td>0.341 (9)</td>
<td>0.404 (4)</td>
<td>0.229 (12)</td>
<td>0.425 (2)</td>
<td>0.346 (8)</td>
<td>0.411 (3)</td>
<td>0.392 (7)</td>
<td>0.464 (1)</td>
</tr>
<tr>
<td>Waveform (arff)</td>
<td>0.066 (8)</td>
<td>0.129 (2)</td>
<td>0.055 (10)</td>
<td>0.056 (9)</td>
<td>0.134 (1)</td>
<td>0.052 (12)</td>
<td>0.108 (4)</td>
<td>0.114 (3)</td>
<td>0.07 (6)</td>
<td>0.054 (11)</td>
<td>0.07 (6)</td>
<td>0.079 (5)</td>
</tr>
<tr>
<td>WBC (arff)</td>
<td>0.542 (11)</td>
<td>0.556 (9)</td>
<td>0.694 (6)</td>
<td>0.863 (1)</td>
<td>0.581 (8)</td>
<td>0.724 (4)</td>
<td>0.33 (12)</td>
<td>0.543 (10)</td>
<td>0.702 (5)</td>
<td>0.638 (7)</td>
<td>0.728 (3)</td>
<td>0.838 (2)</td>
</tr>
<tr>
<td>WDBC (arff)</td>
<td>0.43 (12)</td>
<td>0.667 (9)</td>
<td>0.795 (2)</td>
<td>0.718 (6)</td>
<td>0.654 (10)</td>
<td>0.794 (3)</td>
<td>0.704 (7)</td>
<td>0.773 (4)</td>
<td>0.612 (11)</td>
<td>0.688 (8)</td>
<td>0.729 (5)</td>
<td>0.841 (1)</td>
</tr>
<tr>
<td>WPBC (arff)</td>
<td>0.21 (12)</td>
<td>0.224 (8)</td>
<td>0.23 (6)</td>
<td>0.231 (5)</td>
<td>0.233 (4)</td>
<td>0.223 (9)</td>
<td>0.225 (7)</td>
<td>0.248 (1)</td>
<td>0.223 (9)</td>
<td>0.223 (9)</td>
<td>0.245 (2)</td>
<td>0.244 (3)</td>
</tr>
<tr>
<td>AVG</td>
<td>0.351 (12)</td>
<td>0.462 (8)</td>
<td>0.509 (3)</td>
<td>0.538 (2)</td>
<td>0.447 (9)</td>
<td>0.436 (10)</td>
<td>0.378 (11)</td>
<td>0.466 (7)</td>
<td>0.49 (6)</td>
<td>0.503 (4)</td>
<td>0.503 (4)</td>
<td>0.565 (1)</td>
</tr>
</tbody>
</table>
<p>mingled with normal points (Fig. 3, subfigure c) or hidden in the middle of normal points regarding all dimensions (Fig. 3, subfigure d). We want to point out that it is unlikely that an outlier resembles normal points in all dimensions, and this explains why ECOD could consistently work for most of the datasets.</p>
<p>TABLE 6: ECOD's runtime (in seconds) on a moderate laptop under different sample size ( $n$ ) and dimensions ( $d$ ). It scales well to handle high-dimensional, large datasets.</p>
<table>
<thead>
<tr>
<th></th>
<th>$\mathrm{d}=10$</th>
<th>$\mathrm{~d}=100$</th>
<th>$\mathrm{~d}=1,000$</th>
<th>$\mathrm{~d}=10,000$</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathrm{n}=1,000$</td>
<td>0.068</td>
<td>0.173</td>
<td>1.163</td>
<td>11.460</td>
</tr>
<tr>
<td>$\mathrm{n}=10,000$</td>
<td>0.171</td>
<td>0.468</td>
<td>5.244</td>
<td>55.190</td>
</tr>
<tr>
<td>$\mathrm{n}=100,000$</td>
<td>0.640</td>
<td>7.185</td>
<td>70.541</td>
<td>567.105</td>
</tr>
<tr>
<td>$\mathrm{n}=1,000,000$</td>
<td>11.403</td>
<td>130.974</td>
<td>694.405</td>
<td>5376.593</td>
</tr>
</tbody>
</table>
<h3>4.4 Runtime Efficiency and Scalability</h3>
<p>ECOD is an efficient algorithm with fast computation. Table 7 shows that of the 12 algorithms tested, ECOD ranks third in runtime-an average of 0.228 seconds to process each dataset. The fast two algorithms are HBOS and LODA, which are known to be efficient by building uni-dimensional histograms. Intuitively, ECOD's efficiency is attributed to the feature independence assumption.</p>
<p>ECOD is also a scalable algorithm that suits high dimensional settings. Unlike proximity-based models that require extensive distance calculation or density estimation, ECOD incurs very little computational overhead. We carry out the experiment by generating synthetic datasets of dimensions $10,100,1,000$, and 10,000 and the number of observations are $1,000,10,000,100,000$, and $1,000,000$. All datasets are randomly generated and used strictly for evaluating ECOD's computation time. As such, we ignore the performance metrics and only keep performance time. Fig. 5 and Table 6 illustrate ECOD's performance and its scalability across varying the number of samples ( $n$ ) and dimensions ( $d$ ). Even on the moderately equipped personal computer, ECOD can easily handle datasets with 10,000 dimensions and 1,000,000 data points in 2 hours. The result is consistent with our complexity analysis in Section 3.3.2ECOD has $O(n d)$ time complexity that scales linearly in both the number of samples and dimensions.</p>
<p>TABLE 7: Run time (in seconds) of detectors (average of 10 independent trials, the fastest is highlighted in bold); rank is shown in parenthesis (lower is better). ECOD is one of the fastest algorithms.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Data</th>
<th style="text-align: center;">ABOD</th>
<th style="text-align: center;">CBLOF</th>
<th style="text-align: center;">HBOS</th>
<th style="text-align: center;">IForest</th>
<th style="text-align: center;">KNN</th>
<th style="text-align: center;">LODA</th>
<th style="text-align: center;">LOF</th>
<th style="text-align: center;">LSCP</th>
<th style="text-align: center;">OCSVM</th>
<th style="text-align: center;">PCA</th>
<th style="text-align: center;">SUOD</th>
<th style="text-align: center;">ECOD</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Arrhythmia (mat)</td>
<td style="text-align: center;">0.236 (7)</td>
<td style="text-align: center;">0.258 (8)</td>
<td style="text-align: center;">0.201 (6)</td>
<td style="text-align: center;">0.306 (10)</td>
<td style="text-align: center;">0.077 (5)</td>
<td style="text-align: center;">0.05 (2)</td>
<td style="text-align: center;">0.066 (4)</td>
<td style="text-align: center;">0.984 (11)</td>
<td style="text-align: center;">0.042 (1)</td>
<td style="text-align: center;">0.058 (3)</td>
<td style="text-align: center;">2.004 (12)</td>
<td style="text-align: center;">0.266 (9)</td>
</tr>
<tr>
<td style="text-align: center;">Breastw (mat)</td>
<td style="text-align: center;">0.17 (9)</td>
<td style="text-align: center;">0.076 (8)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.384 (10)</td>
<td style="text-align: center;">0.038 (7)</td>
<td style="text-align: center;">0.037 (6)</td>
<td style="text-align: center;">0.007 (3)</td>
<td style="text-align: center;">0.524 (11)</td>
<td style="text-align: center;">0.01 (4)</td>
<td style="text-align: center;">0.002 (1)</td>
<td style="text-align: center;">1.431 (12)</td>
<td style="text-align: center;">0.024 (5)</td>
</tr>
<tr>
<td style="text-align: center;">Cardio (mat)</td>
<td style="text-align: center;">0.546 (10)</td>
<td style="text-align: center;">0.175 (8)</td>
<td style="text-align: center;">0.011 (2)</td>
<td style="text-align: center;">0.426 (9)</td>
<td style="text-align: center;">0.158 (7)</td>
<td style="text-align: center;">0.06 (4)</td>
<td style="text-align: center;">0.116 (6)</td>
<td style="text-align: center;">1.512 (11)</td>
<td style="text-align: center;">0.086 (5)</td>
<td style="text-align: center;">0.005 (1)</td>
<td style="text-align: center;">1.811 (12)</td>
<td style="text-align: center;">0.053 (3)</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere (mat)</td>
<td style="text-align: center;">0.083 (9)</td>
<td style="text-align: center;">0.053 (8)</td>
<td style="text-align: center;">0.012 (4)</td>
<td style="text-align: center;">0.271 (10)</td>
<td style="text-align: center;">0.015 (5)</td>
<td style="text-align: center;">0.025 (6)</td>
<td style="text-align: center;">0.006 (3)</td>
<td style="text-align: center;">0.45 (11)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.003 (1)</td>
<td style="text-align: center;">1.458 (12)</td>
<td style="text-align: center;">0.047 (7)</td>
</tr>
<tr>
<td style="text-align: center;">Lympho (mat)</td>
<td style="text-align: center;">0.036 (8)</td>
<td style="text-align: center;">0.051 (9)</td>
<td style="text-align: center;">0.009 (5)</td>
<td style="text-align: center;">0.253 (10)</td>
<td style="text-align: center;">0.008 (4)</td>
<td style="text-align: center;">0.023 (6)</td>
<td style="text-align: center;">0.003 (3)</td>
<td style="text-align: center;">0.32 (11)</td>
<td style="text-align: center;">0.001 (1)</td>
<td style="text-align: center;">0.002 (2)</td>
<td style="text-align: center;">1.399 (12)</td>
<td style="text-align: center;">0.036 (8)</td>
</tr>
<tr>
<td style="text-align: center;">Mammography (mat)</td>
<td style="text-align: center;">1.701 (10)</td>
<td style="text-align: center;">0.268 (6)</td>
<td style="text-align: center;">0.006 (1)</td>
<td style="text-align: center;">0.689 (8)</td>
<td style="text-align: center;">0.473 (7)</td>
<td style="text-align: center;">0.091 (4)</td>
<td style="text-align: center;">0.255 (5)</td>
<td style="text-align: center;">4.6 (12)</td>
<td style="text-align: center;">1.364 (9)</td>
<td style="text-align: center;">0.007 (2)</td>
<td style="text-align: center;">2.517 (11)</td>
<td style="text-align: center;">0.044 (3)</td>
</tr>
<tr>
<td style="text-align: center;">Optdigits (mat)</td>
<td style="text-align: center;">2.154 (10)</td>
<td style="text-align: center;">0.433 (5)</td>
<td style="text-align: center;">0.032 (1)</td>
<td style="text-align: center;">0.725 (6)</td>
<td style="text-align: center;">1.457 (9)</td>
<td style="text-align: center;">0.063 (3)</td>
<td style="text-align: center;">1.371 (8)</td>
<td style="text-align: center;">14.566 (12)</td>
<td style="text-align: center;">1.169 (7)</td>
<td style="text-align: center;">0.046 (2)</td>
<td style="text-align: center;">7.114 (11)</td>
<td style="text-align: center;">0.229 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Pima (mat)</td>
<td style="text-align: center;">0.176 (9)</td>
<td style="text-align: center;">0.072 (8)</td>
<td style="text-align: center;">0.001 (1)</td>
<td style="text-align: center;">0.267 (10)</td>
<td style="text-align: center;">0.03 (7)</td>
<td style="text-align: center;">0.023 (6)</td>
<td style="text-align: center;">0.009 (4)</td>
<td style="text-align: center;">0.577 (11)</td>
<td style="text-align: center;">0.007 (3)</td>
<td style="text-align: center;">0.003 (2)</td>
<td style="text-align: center;">1.452 (12)</td>
<td style="text-align: center;">0.023 (6)</td>
</tr>
<tr>
<td style="text-align: center;">Satellite (mat)</td>
<td style="text-align: center;">1.602 (10)</td>
<td style="text-align: center;">0.399 (5)</td>
<td style="text-align: center;">0.018 (1)</td>
<td style="text-align: center;">0.669 (6)</td>
<td style="text-align: center;">0.867 (8)</td>
<td style="text-align: center;">0.071 (3)</td>
<td style="text-align: center;">0.835 (7)</td>
<td style="text-align: center;">8.549 (12)</td>
<td style="text-align: center;">1.075 (9)</td>
<td style="text-align: center;">0.024 (2)</td>
<td style="text-align: center;">5.173 (11)</td>
<td style="text-align: center;">0.158 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Satimage-2 (mat)</td>
<td style="text-align: center;">1.43 (10)</td>
<td style="text-align: center;">0.354 (5)</td>
<td style="text-align: center;">0.017 (2)</td>
<td style="text-align: center;">0.546 (6)</td>
<td style="text-align: center;">0.667 (8)</td>
<td style="text-align: center;">0.063 (3)</td>
<td style="text-align: center;">0.64 (7)</td>
<td style="text-align: center;">7.699 (12)</td>
<td style="text-align: center;">0.91 (9)</td>
<td style="text-align: center;">0.017 (2)</td>
<td style="text-align: center;">4.742 (11)</td>
<td style="text-align: center;">0.146 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Shuttle (mat)</td>
<td style="text-align: center;">2.01 (10)</td>
<td style="text-align: center;">0.175 (4)</td>
<td style="text-align: center;">0.006 (2)</td>
<td style="text-align: center;">0.634 (6)</td>
<td style="text-align: center;">0.788 (8)</td>
<td style="text-align: center;">0.076 (3)</td>
<td style="text-align: center;">0.728 (7)</td>
<td style="text-align: center;">7.973 (12)</td>
<td style="text-align: center;">1.33 (9)</td>
<td style="text-align: center;">0.003 (1)</td>
<td style="text-align: center;">3.566 (11)</td>
<td style="text-align: center;">0.189 (5)</td>
</tr>
<tr>
<td style="text-align: center;">Speech (mat)</td>
<td style="text-align: center;">7.85 (10)</td>
<td style="text-align: center;">2.036 (5)</td>
<td style="text-align: center;">0.167 (2)</td>
<td style="text-align: center;">3.589 (6)</td>
<td style="text-align: center;">7.842 (9)</td>
<td style="text-align: center;">0.154 (1)</td>
<td style="text-align: center;">7.274 (8)</td>
<td style="text-align: center;">49.828 (12)</td>
<td style="text-align: center;">4.272 (7)</td>
<td style="text-align: center;">1.052 (3)</td>
<td style="text-align: center;">14.31 (11)</td>
<td style="text-align: center;">1.148 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Wbc (mat)</td>
<td style="text-align: center;">0.08 (9)</td>
<td style="text-align: center;">0.06 (8)</td>
<td style="text-align: center;">0.008 (4)</td>
<td style="text-align: center;">0.239 (10)</td>
<td style="text-align: center;">0.015 (5)</td>
<td style="text-align: center;">0.021 (6)</td>
<td style="text-align: center;">0.007 (3)</td>
<td style="text-align: center;">0.47 (11)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.002 (1)</td>
<td style="text-align: center;">1.495 (12)</td>
<td style="text-align: center;">0.055 (7)</td>
</tr>
<tr>
<td style="text-align: center;">Wine (mat)</td>
<td style="text-align: center;">0.026 (8)</td>
<td style="text-align: center;">0.046 (9)</td>
<td style="text-align: center;">0.005 (5)</td>
<td style="text-align: center;">0.243 (10)</td>
<td style="text-align: center;">0.005 (5)</td>
<td style="text-align: center;">0.02 (7)</td>
<td style="text-align: center;">0.002 (3)</td>
<td style="text-align: center;">0.313 (11)</td>
<td style="text-align: center;">0.001 (1)</td>
<td style="text-align: center;">0.002 (3)</td>
<td style="text-align: center;">1.421 (12)</td>
<td style="text-align: center;">0.019 (6)</td>
</tr>
<tr>
<td style="text-align: center;">Arrhythmia (arff)</td>
<td style="text-align: center;">0.226 (8)</td>
<td style="text-align: center;">0.23 (9)</td>
<td style="text-align: center;">0.169 (6)</td>
<td style="text-align: center;">0.249 (10)</td>
<td style="text-align: center;">0.065 (4)</td>
<td style="text-align: center;">0.043 (2)</td>
<td style="text-align: center;">0.057 (3)</td>
<td style="text-align: center;">0.947 (11)</td>
<td style="text-align: center;">0.039 (1)</td>
<td style="text-align: center;">0.095 (5)</td>
<td style="text-align: center;">2.025 (12)</td>
<td style="text-align: center;">0.226 (8)</td>
</tr>
<tr>
<td style="text-align: center;">Cardiotocography (arff)</td>
<td style="text-align: center;">0.368 (10)</td>
<td style="text-align: center;">0.124 (7)</td>
<td style="text-align: center;">0.007 (2)</td>
<td style="text-align: center;">0.286 (9)</td>
<td style="text-align: center;">0.138 (8)</td>
<td style="text-align: center;">0.049 (4)</td>
<td style="text-align: center;">0.099 (6)</td>
<td style="text-align: center;">1.769 (11)</td>
<td style="text-align: center;">0.084 (5)</td>
<td style="text-align: center;">0.007 (2)</td>
<td style="text-align: center;">1.876 (12)</td>
<td style="text-align: center;">0.037 (3)</td>
</tr>
<tr>
<td style="text-align: center;">HeartDisease (arff)</td>
<td style="text-align: center;">0.048 (9)</td>
<td style="text-align: center;">0.044 (8)</td>
<td style="text-align: center;">0.005 (4)</td>
<td style="text-align: center;">0.21 (10)</td>
<td style="text-align: center;">0.008 (5)</td>
<td style="text-align: center;">0.018 (7)</td>
<td style="text-align: center;">0.004 (3)</td>
<td style="text-align: center;">0.366 (11)</td>
<td style="text-align: center;">0.001 (1)</td>
<td style="text-align: center;">0.002 (2)</td>
<td style="text-align: center;">1.38 (12)</td>
<td style="text-align: center;">0.016 (6)</td>
</tr>
<tr>
<td style="text-align: center;">Hepatitis (arff)</td>
<td style="text-align: center;">0.013 (6)</td>
<td style="text-align: center;">0.028 (9)</td>
<td style="text-align: center;">0.004 (5)</td>
<td style="text-align: center;">0.187 (10)</td>
<td style="text-align: center;">0.003 (4)</td>
<td style="text-align: center;">0.016 (7)</td>
<td style="text-align: center;">0.002 (3)</td>
<td style="text-align: center;">0.292 (11)</td>
<td style="text-align: center;">0.001 (2)</td>
<td style="text-align: center;">0.001 (2)</td>
<td style="text-align: center;">1.378 (12)</td>
<td style="text-align: center;">0.021 (8)</td>
</tr>
<tr>
<td style="text-align: center;">InternetAds (arff)</td>
<td style="text-align: center;">5.648 (9)</td>
<td style="text-align: center;">1.36 (3)</td>
<td style="text-align: center;">0.493 (2)</td>
<td style="text-align: center;">5.605 (8)</td>
<td style="text-align: center;">5.252 (6)</td>
<td style="text-align: center;">0.227 (1)</td>
<td style="text-align: center;">6.11 (10)</td>
<td style="text-align: center;">58.435 (12)</td>
<td style="text-align: center;">4.159 (5)</td>
<td style="text-align: center;">5.496 (7)</td>
<td style="text-align: center;">21.974 (11)</td>
<td style="text-align: center;">1.855 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Ionosphere (arff)</td>
<td style="text-align: center;">0.075 (9)</td>
<td style="text-align: center;">0.049 (8)</td>
<td style="text-align: center;">0.012 (4)</td>
<td style="text-align: center;">0.236 (10)</td>
<td style="text-align: center;">0.014 (5)</td>
<td style="text-align: center;">0.02 (6)</td>
<td style="text-align: center;">0.007 (3)</td>
<td style="text-align: center;">0.447 (11)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">1.498 (12)</td>
<td style="text-align: center;">0.036 (7)</td>
</tr>
<tr>
<td style="text-align: center;">KDDCup99 (arff)</td>
<td style="text-align: center;">136.469 (9)</td>
<td style="text-align: center;">2.1 (5)</td>
<td style="text-align: center;">0.135 (1)</td>
<td style="text-align: center;">7.714 (6)</td>
<td style="text-align: center;">133.6 (8)</td>
<td style="text-align: center;">0.409 (3)</td>
<td style="text-align: center;">141.9 (10)</td>
<td style="text-align: center;">839.8 (12)</td>
<td style="text-align: center;">175.1 (11)</td>
<td style="text-align: center;">0.301 (2)</td>
<td style="text-align: center;">68.726 (7)</td>
<td style="text-align: center;">0.965 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Lymphography (arff)</td>
<td style="text-align: center;">0.039 (8)</td>
<td style="text-align: center;">0.053 (9)</td>
<td style="text-align: center;">0.007 (5)</td>
<td style="text-align: center;">0.255 (10)</td>
<td style="text-align: center;">0.007 (5)</td>
<td style="text-align: center;">0.023 (7)</td>
<td style="text-align: center;">0.003 (3)</td>
<td style="text-align: center;">0.31 (11)</td>
<td style="text-align: center;">0.001 (1)</td>
<td style="text-align: center;">0.002 (2)</td>
<td style="text-align: center;">1.503 (12)</td>
<td style="text-align: center;">0.02 (6)</td>
</tr>
<tr>
<td style="text-align: center;">Pima (arff)</td>
<td style="text-align: center;">0.201 (9)</td>
<td style="text-align: center;">0.087 (8)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.319 (10)</td>
<td style="text-align: center;">0.034 (6)</td>
<td style="text-align: center;">0.029 (5)</td>
<td style="text-align: center;">0.014 (4)</td>
<td style="text-align: center;">0.573 (11)</td>
<td style="text-align: center;">0.013 (3)</td>
<td style="text-align: center;">0.002 (1)</td>
<td style="text-align: center;">1.43 (12)</td>
<td style="text-align: center;">0.043 (7)</td>
</tr>
<tr>
<td style="text-align: center;">Shuttle (arff)</td>
<td style="text-align: center;">0.248 (8)</td>
<td style="text-align: center;">0.079 (7)</td>
<td style="text-align: center;">0.004 (2)</td>
<td style="text-align: center;">0.366 (9)</td>
<td style="text-align: center;">0.05 (6)</td>
<td style="text-align: center;">0.033 (5)</td>
<td style="text-align: center;">0.023 (4)</td>
<td style="text-align: center;">0.712 (10)</td>
<td style="text-align: center;">0.018 (3)</td>
<td style="text-align: center;">0.003 (1)</td>
<td style="text-align: center;">1.481 (12)</td>
<td style="text-align: center;">0.912 (11)</td>
</tr>
<tr>
<td style="text-align: center;">SpamBase (arff)</td>
<td style="text-align: center;">1.329 (10)</td>
<td style="text-align: center;">0.303 (5)</td>
<td style="text-align: center;">0.017 (1)</td>
<td style="text-align: center;">0.514 (6)</td>
<td style="text-align: center;">0.904 (9)</td>
<td style="text-align: center;">0.061 (3)</td>
<td style="text-align: center;">0.888 (8)</td>
<td style="text-align: center;">8.349 (12)</td>
<td style="text-align: center;">0.666 (7)</td>
<td style="text-align: center;">0.032 (2)</td>
<td style="text-align: center;">3.836 (11)</td>
<td style="text-align: center;">0.109 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Stamps (arff)</td>
<td style="text-align: center;">0.071 (9)</td>
<td style="text-align: center;">0.055 (8)</td>
<td style="text-align: center;">0.004 (4)</td>
<td style="text-align: center;">0.233 (10)</td>
<td style="text-align: center;">0.013 (5)</td>
<td style="text-align: center;">0.018 (7)</td>
<td style="text-align: center;">0.003 (3)</td>
<td style="text-align: center;">0.384 (11)</td>
<td style="text-align: center;">0.002 (2)</td>
<td style="text-align: center;">0.002 (2)</td>
<td style="text-align: center;">1.382 (12)</td>
<td style="text-align: center;">0.014 (6)</td>
</tr>
<tr>
<td style="text-align: center;">Waveform (arff)</td>
<td style="text-align: center;">1.018 (10)</td>
<td style="text-align: center;">0.244 (5)</td>
<td style="text-align: center;">0.01 (2)</td>
<td style="text-align: center;">0.478 (8)</td>
<td style="text-align: center;">0.521 (9)</td>
<td style="text-align: center;">0.072 (4)</td>
<td style="text-align: center;">0.429 (7)</td>
<td style="text-align: center;">3.479 (12)</td>
<td style="text-align: center;">0.261 (6)</td>
<td style="text-align: center;">0.008 (1)</td>
<td style="text-align: center;">2.357 (11)</td>
<td style="text-align: center;">0.047 (3)</td>
</tr>
<tr>
<td style="text-align: center;">WBC (arff)</td>
<td style="text-align: center;">0.048 (8)</td>
<td style="text-align: center;">0.05 (9)</td>
<td style="text-align: center;">0.004 (4)</td>
<td style="text-align: center;">0.243 (10)</td>
<td style="text-align: center;">0.009 (5)</td>
<td style="text-align: center;">0.021 (7)</td>
<td style="text-align: center;">0.003 (3)</td>
<td style="text-align: center;">0.335 (11)</td>
<td style="text-align: center;">0.001 (2)</td>
<td style="text-align: center;">0.001 (2)</td>
<td style="text-align: center;">1.367 (12)</td>
<td style="text-align: center;">0.013 (6)</td>
</tr>
<tr>
<td style="text-align: center;">WDBC (arff)</td>
<td style="text-align: center;">0.092 (9)</td>
<td style="text-align: center;">0.062 (8)</td>
<td style="text-align: center;">0.009 (4)</td>
<td style="text-align: center;">0.245 (10)</td>
<td style="text-align: center;">0.016 (5)</td>
<td style="text-align: center;">0.022 (6)</td>
<td style="text-align: center;">0.007 (3)</td>
<td style="text-align: center;">0.466 (11)</td>
<td style="text-align: center;">0.005 (2)</td>
<td style="text-align: center;">0.004 (1)</td>
<td style="text-align: center;">1.5 (12)</td>
<td style="text-align: center;">0.032 (7)</td>
</tr>
<tr>
<td style="text-align: center;">WPBC (arff)</td>
<td style="text-align: center;">0.046 (7)</td>
<td style="text-align: center;">0.052 (8)</td>
<td style="text-align: center;">0.01 (5)</td>
<td style="text-align: center;">0.237 (10)</td>
<td style="text-align: center;">0.008 (4)</td>
<td style="text-align: center;">0.021 (6)</td>
<td style="text-align: center;">0.004 (3)</td>
<td style="text-align: center;">0.353 (11)</td>
<td style="text-align: center;">0.002 (1)</td>
<td style="text-align: center;">0.003 (2)</td>
<td style="text-align: center;">1.383 (12)</td>
<td style="text-align: center;">0.062 (9)</td>
</tr>
<tr>
<td style="text-align: center;">AVG</td>
<td style="text-align: center;">5.468 (9)</td>
<td style="text-align: center;">0.312 (5)</td>
<td style="text-align: center;">0.046 (1)</td>
<td style="text-align: center;">0.887 (6)</td>
<td style="text-align: center;">5.105 (7)</td>
<td style="text-align: center;">0.062 (2)</td>
<td style="text-align: center;">5.362 (8)</td>
<td style="text-align: center;">33.847 (12)</td>
<td style="text-align: center;">6.355 (11)</td>
<td style="text-align: center;">0.24 (4)</td>
<td style="text-align: center;">5.5 (10)</td>
<td style="text-align: center;">0.228 (3)</td>
</tr>
</tbody>
</table>
<p>Additionally, ECOD requires no re-training to fit new data points with relatively large data samples if no data shift is assumed, and thus is highly desirable for applications where real-time predictions are needed. Within PyOD, we also provide the distributed implementation with even more efficiency and scalability.</p>
<h2>5 CONCLUSION AND FUTURE WORKS</h2>
<p>In this paper, we present a novel outlier detection method called ECOD that uses empirical cumulative distribution functions in measuring the outlyingness of data points. Specifically, it computes left- and right-tail univariate ECDFs per dimension. For every data point, ECOD uses the univariate ECDFs to estimate tail probabilities for the data point and aggregates these tail probabilities to come up with a final outlier score. The intuition of ECOD somewhat resembles how p-value works: for a specific data point, we are looking at how low its tail probability is. The proposed ECOD is parameter-free without the need for hyperparameter tuning. Our extensive evaluation on 30 benchmark datasets shows that ECOD outperforms SOTA baselines, while being a fast and scalable outlier detection algorithm. We provide an easy-to-use Python implementation of ECOD for both single- and multi-thread support. We leave accounting for how features are related for future work. For instance, we can learn vine copulas [63] or probabilistic graphical models [64] to identify blocks of features that are highly correlated, and only for these blocks (rather than across the entire dataset), we could compute a joint ECDF. We suspect that these strategies would require some amount of hyperparameter tuning though. Separately, our
approach is not designed to handle multimodal distributions for which an outlier could be in neither left nor right tails. Figuring out a way to extend ECOD to such settings while still being fast and scalable is also a possible direction worth exploring.</p>
<h2>REFERENCES</h2>
<p>[1] X. Yang, L. Tan, and L. He, "A robust least squares support vector machine for regression and classification with noise," Neurocomputing, vol. 140, pp. 41-52, 2014.
[2] M. Najafi, L. He, and S. Y. Philip, "Outlier-robust multi-aspect streaming tensor completion and factorization." in IJCAI, 2019, pp. 3187-3194.
[3] B. Cao, M. Mao, S. Viidu, and P. Yu, "Collective fraud detection capturing inter-transaction dependency," in KDD 2017 Workshop on Anomaly Detection in Finance. PMLR, 2018, pp. 66-75.
[4] J. Tao, J. Lin, S. Zhang, S. Zhao, R. Wu, C. Fan, and P. Cui, "Mvan: Multi-view attention networks for real money trading detection in online games," in KDD, 2019, pp. 2536-2546.
[5] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, "Enhancing graph neural network-based fraud detectors against camouflaged fraudsters," in Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management, 2020, pp. 315-324.
[6] V. Chandola, A. Banerjee, and V. Kumar, "Anomaly detection: A survey," CSUR, vol. 41, no. 5, p. 15, 2009.
[7] S. Zhang, B. Li, J. Li, M. Zhang, and Y. Chen, "A novel anomaly detection approach for mitigating web-based attacks against clouds," in 2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing. IEEE, 2015, pp. 289-294.
[8] W. Yu, J. Li, M. Z. A. Bhuiyan, R. Zhang, and J. Huai, "Ring: Realtime emerging anomaly monitoring system over text streams," IEEE Transactions on Big Data, vol. 5, no. 4, pp. 506-519, 2017.
[9] J. Zhao, X. Liu, Q. Yan, B. Li, M. Shao, and H. Peng, "Multiattributed heterogeneous graph convolutional network for bot detection," Information Sciences, vol. 537, pp. 380-393, 2020.
[10] P. Cui, L.-F. Sun, Z.-Q. Liu, and S.-Q. Yang, "A sequential monte carlo approach to anomaly detection in tracking visual events," in 2007 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2007, pp. 1-8.</p>
<p>[11] S. Chen, W. Wang, and H. van Zuylen, "A comparison of outlier detection algorithms for its data," Expert Systems with Applications, vol. 37, no. 2, pp. 1169-1178, 2010.
[12] R. Xu, Y. Guo, X. Han, X. Xia, H. Xiang, and J. Ma, "Opencda: an open cooperative driving automation framework integrated with co-simulation," in 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE, 2021, pp. 1155-1162.
[13] R. Xu, H. Xiang, X. Xia, X. Han, J. Liu, and J. Ma, "Opv2v: An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle communication," arXiv preprint arXiv:2109.07644, 2021.
[14] Z. Li, Y. Zhao, and J. Fu, "SynC: A copula based framework for generating synthetic data from aggregated sources," in IEEE International Conference on Data Mining Workshops (ICDMW), 2020.
[15] S. Ramaswamy, R. Rastogi, and K. Shim, "Efficient algorithms for mining outliers from large data sets," in ACM SIGMOD Record, vol. 29, 2000, pp. 427-438.
[16] B. Schölkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson, "Estimating the support of a high-dimensional distribution," Neural Computation, vol. 13, no. 7, pp. 1443-1471, 2001.
[17] F. T. Liu, K. M. Ting, and Z.-H. Zhou, "Isolation forest," in 2008 eighth ieee international conference on data mining. IEEE, 2008, pp. $413-422$.
[18] Y. Zhao, Z. Nasrullah, M. K. Hryniewicki, and Z. Li, "LSCP: locally selective combination in parallel outlier ensembles," in SDM. SIAM, May 2019, pp. 585-593.
[19] L. Ruff, N. Görnitz, L. Deecke, S. A. Siddiqui, R. A. Vandermeulen, A. Binder, E. Müller, and M. Kloft, "Deep one-class classification," in Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 1015, 2018, ser. Proceedings of Machine Learning Research, J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018, pp. 4390-4399. [Online]. Available: http://proceedings.mlr.press/v80/ruff18a.html
[20] Y. Liu, Z. Li, C. Zhou, Y. Jiang, J. Sun, M. Wang, and X. He, "Generative adversarial active learning for unsupervised outlier detection," TKDE, 2019.
[21] Y. Zhao, X. Hu, C. Cheng, C. Wang, C. Wan, W. Wang, J. Yang, H. Bai, Z. Li, C. Xiao, Y. Wang, Z. Qiao, J. Sun, and L. Akoglu, "SUOD: Accelerating large-scale unsupervised heterogeneous outlier detection," Proceedings of Machine Learning and Systems, 2021.
[22] Y. Zhao, G. H. Chen, and Z. Jia, "Tod: Tensor-based outlier detection," arXiv preprint arXiv:2110.14007, 2021.
[23] Y. Zhao, R. Rossi, and L. Akoglu, "Automatic unsupervised outlier model selection," in Advances in Neural Information Processing Systems, vol. 34, 2021.
[24] L. Akoglu, "Anomaly mining - past, present and future," in Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, Z. Zhou, Ed. ijcai.org, 2021, pp. 4932-4936. [Online]. Available: https://doi.org/10.24963/ijcai.2021/697
[25] A. Lazarevic and V. Kumar, "Feature bagging for outlier detection," in $K D D$. ACM, 2005, pp. 157-166.
[26] D. Pokrajac, A. Lazarevic, and L. J. Latecki, "Incremental local outlier detection for data streams," in 2007 IEEE symposium on computational intelligence and data mining. IEEE, 2007, pp. 504-515.
[27] C. Leys, C. Ley, O. Klein, P. Bernard, and L. Licata, "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median," Journal of experimental social psychology, vol. 49, no. 4, pp. 764-766, 2013.
[28] Z. A. Bakar, R. Mohemad, A. Ahmad, and M. M. Deris, "A comparative study for outlier detection techniques in data mining," in 2006 IEEE conference on cybernetics and intelligent systems. IEEE, 2006, pp. 1-6.
[29] F. M. Dekking, C. Kraaikamp, H. P. Lopuhaä, and L. E. Meester, A Modern Introduction to Probability and Statistics: Understanding why and how. Springer Science \&amp; Business Media, 2005.
[30] M. Goldstein and A. Dengel, "Histogram-based outlier score (hbos): A fast unsupervised anomaly detection algorithm," KI2012: Poster and Demo Track, pp. 59-63, 2012.
[31] M. Naaman, "On the tight constant in the multivariate Dvoretzky-Kiefer-Wolfowitz inequality," Statistics \&amp; Probability Letters, vol. 173, p. 109088, 2021.
[32] C. C. Aggarwal, "Outlier analysis," in Data mining. Springer, 2015, pp. 237-263.
[33] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel, "Deep learning for anomaly detection: A review," ACM Computing Surveys (CSUR), vol. 54, no. 2, pp. 1-38, 2021.
[34] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, "LOF: identifying density-based local outliers," in ACM SIGMOD Record, vol. 29, 2000, pp. 93-104.
[35] J. Tang, Z. Chen, A. Fu, and D. Cheung, LOF: Enhancing effectiveness of outlier detections for low density patterns. Springer, 2002.
[36] S. Papadimitriou, H. Kitagawa, P. B. Gibbons, and C. Faloutsos, "Loci: Fast outlier detection using the local correlation integral," in ICDE. IEEE, 2003, pp. 315-326.
[37] X. Yang, L. Latecki, and D. Pokrajac, "Outlier detection with globally optimal exemplar-based gmm," in SDM. SIAM, April 2009, pp. 145-154.
[38] M. H. Satman, "A new algorithm for detecting outliers in linear regression," Int. J. Statist. Probab., vol. 2, no. 3, pp. 101-109, 2013.
[39] M. Pavlidou and G. Zioutas, Kernel density outlier detector. Springer, 2014.
[40] Z. He, X. Xu, and S. Deng, "Discovering cluster-based local outliers," Pattern Recognition Letters, vol. 24, no. 9-10, pp. 1641-1650, 2003.
[41] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga, "Outlier detection with autoencoder ensembles," in Proceedings of the 2017 SIAM international conference on data mining. SIAM, 2017, pp. 90-98.
[42] G. Pang, A. v. d. Hengel, C. Shen, and L. Cao, "Toward deep supervised anomaly detection: Reinforcement learning from partially labeled anomaly data," arXiv preprint arXiv:2009.06847, 2020.
[43] C. J. Stone, "Optimal rates of convergence for nonparametric estimators," The Annals of Statistics, pp. 1348-1360, 1980.
[44] R. A. Groeneveld and G. Meeden, "Measuring skewness and kurtosis," Journal of the Royal Statistical Society: Series D (The Statistician), vol. 33, no. 4, pp. 391-399, 1984.
[45] X. Hu, C. Rudin, and M. Seltzer, "Optimal sparse decision trees," Advances in Neural Information Processing Systems (NeurIPS), 2019.
[46] X. Hu, Y. Huang, B. Li, and T. Lu, "Uncovering the source of machine bias," ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Machine Learning for Consumers and Markets Workshop, 2021.
[47] N. Gupta, D. Eswaran, N. Shah, L. Akoglu, and C. Faloutsos, "Beyond outlier detection: Lookout for pictorial explanation," in Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2018, pp. 122-138.
[48] O. L. Mangasarian, W. N. Street, and W. H. Wolberg, "Breast cancer diagnosis and prognosis via linear programming," Operations Research, vol. 43, no. 4, pp. 570-577, 1995.
[49] Y. Zhao, Z. Nasrullah, and Z. Li, "Pyod: A python toolbox for scalable outlier detection," Journal of Machine Learning Research, vol. 20, no. 96, pp. 1-7, 2019. [Online]. Available: http://jmlr.org/papers/v20/19-011.html
[50] S. Rayana, "Odds library," 2016. [Online]. Available: http: //odds.cs.stonybrook.edu
[51] G. O. Campos, A. Zimek, J. Sander, R. J. G. B. Campello, B. Micenková, E. Schubert, I. Assent, and M. E. Houle, "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study," Data Mining Knowledge Discovery, vol. 30, no. 9-10, pp. 891-927, 2016.
[52] C. C. Aggarwal and S. Sathe, Outlier ensembles: An introduction, 1st ed. Springer, 2017.
[53] L. Akoglu, H. Tong, J. Vreeken, and C. Faloutsos, "Fast and Reliable Anomaly Detection in Categorical Data," in CIKM, 2012.
[54] Y. Zhao and M. K. Hryniewicki, "Xgbod: improving supervised outlier detection with unsupervised representation learning," in IJCNN. IEEE, 2018.
[55] A. Emmott, S. Das, T. Dietterich, A. Fern, and W.-k. Wong, "A Meta-Analysis of the Anomaly Detection Problem," arXiv preprint, no. 2015, 2015. [Online]. Available: http: //arxiv.org/abs/1503.01158
[56] J. Demšar, "Statistical comparisons of classifiers over multiple data sets," The Journal of Machine Learning Research, vol. 7, pp. 1-30, 2006.
[57] H. I. Fawaz, G. Forestier, J. Weber, L. Idoumghar, and P.-A. Muller, "Deep learning for time series classification: a review," Data mining and knowledge discovery, vol. 33, no. 4, pp. 917-963, 2019.
[58] H.-P. Kriegel, M. Schubert, and A. Zimek, "Angle-based outlier detection in high-dimensional data," in Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 2008, pp. 444-452.
[59] T. Pevnỳ, "LODA: Lightweight on-line detector of anomalies," Machine Learning, vol. 102, no. 2, pp. 275-304, 2016.</p>
<p>[60] M.-L. Shyu, S.-C. Chen, K. Sarinnapakorn, and L. Chang, "A novel anomaly detection scheme based on principal component classifier," MIAMI UNIV CORAL GABLES FL DEPT OF ELECTRICAL AND COMPUTER ENGINEERING, Tech. Rep., 2003.
[61] M. Q. Ma, Y. Zhao, X. Zhang, and L. Akoglu, "A large-scale study on unsupervised outlier model selection: Do internal strategies suffice?" arXiv preprint arXiv:2104.01422, 2021.
[62] L. Van der Maaten and G. Hinton, "Visualizing data using t-sne." Journal of machine learning research, vol. 9, no. 11, 2008.
[63] H. Joe and D. Kurowicka, Dependence modeling: vine copula handbook. World Scientific, 2011.
[64] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical learning with sparsity: the lasso and generalizations. Chapman and Hall/CRC, 2019.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Zheng Li is the founder of Arima, a Toronto based data mining startup and an adjunct lecturer at Northeastern University Toronto.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Yue Zhao is a Ph.D. student at Carnegie Mellon University (CMU), focusing on anomaly detection (a.k.a outlier detection) algorithms, systems, and its applications in security, healthcare, and Finance, with more than 7-year professional experience and 20+ papers (in JMLR, TKDE, NeurIPS, etc.). He is a recipient of the 2022 Norton Labs Graduate Fellowship (formerly known as the Symantec Research Labs Fellowship).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Xiyang Hu is a Ph.D. student at Carnegie Mellon University. He got his M.Sc. in Statistical Science from Duke University, and B.Arch. in Architecture with a minor in Computer Science from Tsinghua University. His research interests include machine learning, deep learning, data mining, and social impacts of AI.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Nicola Botta has worked on numerical methods for conservation laws, agent-based modelling and verified decision making. He is a senior scientist at the Potsdam Institute for Climate Impact Research, Potsdam, Germany and adjunct professor in the Functional Programming division at the Computer Science and Engineering Department, Chalmers University of Technology, Sweden.</p>
<p>Cezar Ionescu is Professor of Computer Science at Technische Hochschule Deggendorf, Deggendorf, Germany.</p>
<p>George H. Chen is an assistant professor of information systems at Carnegie Mellon University. He primarily works on building trustworthy machine learning models for time-to-event prediction (survival analysis) and for time series analysis. He often uses nonparametric prediction models that work well under very few assumptions on the data. His main application area is in healthcare.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ ODDS Library: http://odds.cs.stonybrook.edu
${ }^{2}$ DAMI Datasets: http://www.dbs.ifi.lmu.de/research/ outlier-evaluation/DAMI&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ ECOD implementation in PyOD library: https://github.com/ yzhao062/pyod/blob/master/pyod/models/ecod.py&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8640 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8640</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8640</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-273101864</p>
                <p><strong>Paper Title:</strong> FragLlama: Next-fragment prediction for molecular design</p>
                <p><strong>Paper Abstract:</strong> The emergence of ChatGPT has drawn significant attention to Large Language Models (LLMs) due to their impressive performance. While LLMs primarily focus on next token/word prediction, we apply this principle to molecular design by reframing the task as predicting the next token/fragment. We present FragLlama, a large language model trained for molecular design, featuring custom tokens that represent molecular fragments and functional groups. The model is for generating molecules given one or two fragments, for application scenarios like general hit-to-lead and lead optimization stage drug design, PROTAC linker design; mapping to commonly used drug design strategies like fragment growing and scaffold hopping. In the pre-training stage, we adapted the Llama 3 architecture to create FragLlama, training it to learn conditional probabilities of these fragment-level tokens. The subsequent alignment stage employed fine-tuning to guide the model towards generating molecules with desired properties. The effectiveness of FragLlama is demonstrated through its applications in designing molecular glue libraries, PROTAC linkers and EGFR binders. FragLlama demonstrates proficiency in reproducing expert-level designs while also exploring novel and promising chemical spaces, highlighting its potential to augment the capabilities of medicinal chemists in drug design.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8640.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8640.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FragLlama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FragLlama: Next-fragment prediction for molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only fragment-level molecular language model that predicts next molecular fragments (tokens) to perform fragment growth, scaffold modification, scaffold hopping, and linker design for drug discovery tasks such as molecular glues, PROTAC linkers and EGFR binders.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FragLlama</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only Transformer (modified Llama 3 architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>779 million parameters</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pre-trained on ~70 billion tokens derived from molecules tokenized at fragment/functional-group level using a custom fragment-level BPE tokenizer (8k vocabulary). Fine-tuning datasets included curated subsets from PubChem: e.g., 8,532 EGFR inhibitors (6,822 high IC50, 1,710 low IC50) and target-specific sets for cereblon binders/PROTACs; pretraining executed on 7 NVIDIA A100 GPUs with DeepSpeed ZeRO3.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery (molecular glue design, PROTAC linker design, hit-to-lead and lead optimization; target-specific inhibitor design e.g., EGFR)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-fragment prediction (fragment-level tokens). Fragment growth from given starting fragment(s) or anchors, fragment linking for linker design; decoding with beam-search-variance and other standard LLM decoding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Generated both molecules overlapping known chemical space and novel regions: e.g., 5,834 sanitized/deduplicated molecular-glue candidates; identified four novel chemical-space regions not present in curated PubChem set; produced molecules identical or highly similar to known recent molecular glues (one identical to PT-179; others highly similar to hit/SJ3149 analogs). PROTAC linker designs reached Tanimoto similarities up to 0.93 to references.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Conditioning via one- or two-fragment inputs and fragment growth anchors; fine-tuning on target-specific inhibitor collections and inclusion of IC50-range special tokens to bias generation toward activity; scaffold constraints used (e.g., 4-anilinoquinazoline for EGFR experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Chemical sanitization and deduplication; Atom-pair descriptors for fingerprints; pairwise Tanimoto similarity for similarity scoring; UMAP visualization of chemical-space distributions; counts of generated molecules; distribution of Tanimoto similarity scores; qualitative structural comparisons to published molecules; identification of rigid linker content for PROTACs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FragLlama can reproduce expert-level designs and explore novel, plausible chemical space: it generated thousands of valid molecular-glue candidates, rediscovered or produced molecules highly similar/identical to recently reported molecular glues, and designed diverse PROTAC linkers (similarities up to 0.93). Fine-tuning (especially with IC50 special tokens) increased overlap with known EGFR inhibitor chemical space; without fine-tuning designs were more distant from PubChem-known inhibitors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Fragment-level tokenizer improved compression (shorter token sequences) and chemical-semantic token interpretability versus atom-level tokenizers and general LLM tokenizers (example: for a GLP-1R inhibitor FragLlama token length 61 vs atom-level 105; GPT2/GPT4 token lengths 86/71 respectively). Attention patterns compared favorably to GPT2 (more targeted, sparse heads). No direct benchmarking against other molecular generation pipelines reported beyond tokenizer/attention qualitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Limitations reported include: lack of consensus for molecular sequentialization; dependence on quality/quantity of labeled data for alignment/fine-tuning; evaluation requires lengthy experimental validation; prompt instability concerns for molecular language; some generated designs did not exactly reproduce rings though mimicked chain length/functionalization; model and compute scale constraints (trained at 779M params on 7 A100 GPUs) may limit emergent capabilities; challenges in generating highly complex, elaborated drug-like compounds without target-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8640.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemSpaceAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemSpaceAL (an active learning methodology applied to protein-specific molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of an existing chemistry-specific language/modeling approach applying efficient active learning to protein-specific molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemSpaceAL</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery / protein-specific molecular generation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8640.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>cMolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>cMolGPT (conditional generative pretrained transformer for target-specific de novo molecular generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of a chemistry-specific language model for target-specific de novo molecular generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cMolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>generative pretrained transformer (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation / target-specific drug design (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>conditional generation (implied by name/description)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8640.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a molecular language model enabling concurrent sequence regression and generation (prior work in molecular LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Regression Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular language modelling / property-conditioned generation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequence regression + generation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8640.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolGPT (molecular generation using a transformer-decoder model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a transformer-decoder based molecular generation model in prior literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer-decoder (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular generation / de novo design (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>autoregressive SMILES generation (implied by 'GPT'-style)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8640.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SAFE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SAFE-GPT (a framework for molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as a framework for molecular design (recent work referenced among chemistry-specific LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SAFE-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Molecular design (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8640.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Darwin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Darwin (domain-specific large language models for natural science)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned among prior domain-specific LLM efforts applied to natural sciences/chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Darwin</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Natural sciences / chemistry (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8640.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2 and GPT-4 (OpenAI Generative Pre-trained Transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>General-purpose LLMs referenced for comparison of tokenization and attention patterns; GPT2 used in attention comparisons with FragLlama.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only Transformer (general NLP LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large-scale human-language corpora (general description only; specifics not provided in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General NLP (used as baseline/comparison; not used for chemical generation experiments in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>standard autoregressive token generation (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>FragLlama's attention patterns were more focused/sparse compared to GPT-2 on molecular tokens; FragLlama tokenizer produced chemically meaningful tokens versus GPT tokenizers which lack chemical semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Qualitative comparison: FragLlama shows better targeted attention and chemical-semantic tokens versus GPT-2/GPT-4; tokenizer compression of FragLlama was better than GPT-4 for one example molecule.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8640.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8640.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3 (architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (decoder-only Transformer architecture by Meta AI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced as architectural basis/inspiration for FragLlama; FragLlama modifies Llama 3 choices (positional encoding, attention variant).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3 (architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General LLM architecture (used as basis for molecular model modifications)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>FragLlama modified Llama 3 by replacing RoPE positional encoding with standard positional encoding and replacing GQA with FlashAttention; FragLlama did NOT use Llama weights but adapted architectural ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FragLlama: Next-fragment prediction for molecular design', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>cMolGPT: A conditional generative pretrained transformer for target-specific de novo molecular generation. <em>(Rating: 2)</em></li>
                <li>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. <em>(Rating: 2)</em></li>
                <li>MolGPT: molecular generation using a transformer-decoder model. <em>(Rating: 2)</em></li>
                <li>An efficient active learning methodology applied to protein-specific molecular generation. <em>(Rating: 2)</em></li>
                <li>Gotta be SAFE: a new framework for molecular design. <em>(Rating: 2)</em></li>
                <li>A Review of Large Language Models and Autonomous Agents in Chemistry. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8640",
    "paper_id": "paper-273101864",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "FragLlama",
            "name_full": "FragLlama: Next-fragment prediction for molecular design",
            "brief_description": "A decoder-only fragment-level molecular language model that predicts next molecular fragments (tokens) to perform fragment growth, scaffold modification, scaffold hopping, and linker design for drug discovery tasks such as molecular glues, PROTAC linkers and EGFR binders.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FragLlama",
            "model_type": "decoder-only Transformer (modified Llama 3 architecture)",
            "model_size": "779 million parameters",
            "training_data": "Pre-trained on ~70 billion tokens derived from molecules tokenized at fragment/functional-group level using a custom fragment-level BPE tokenizer (8k vocabulary). Fine-tuning datasets included curated subsets from PubChem: e.g., 8,532 EGFR inhibitors (6,822 high IC50, 1,710 low IC50) and target-specific sets for cereblon binders/PROTACs; pretraining executed on 7 NVIDIA A100 GPUs with DeepSpeed ZeRO3.",
            "application_domain": "Drug discovery (molecular glue design, PROTAC linker design, hit-to-lead and lead optimization; target-specific inhibitor design e.g., EGFR)",
            "generation_method": "Autoregressive next-fragment prediction (fragment-level tokens). Fragment growth from given starting fragment(s) or anchors, fragment linking for linker design; decoding with beam-search-variance and other standard LLM decoding strategies.",
            "novelty_of_chemicals": "Generated both molecules overlapping known chemical space and novel regions: e.g., 5,834 sanitized/deduplicated molecular-glue candidates; identified four novel chemical-space regions not present in curated PubChem set; produced molecules identical or highly similar to known recent molecular glues (one identical to PT-179; others highly similar to hit/SJ3149 analogs). PROTAC linker designs reached Tanimoto similarities up to 0.93 to references.",
            "application_specificity": "Conditioning via one- or two-fragment inputs and fragment growth anchors; fine-tuning on target-specific inhibitor collections and inclusion of IC50-range special tokens to bias generation toward activity; scaffold constraints used (e.g., 4-anilinoquinazoline for EGFR experiments).",
            "evaluation_metrics": "Chemical sanitization and deduplication; Atom-pair descriptors for fingerprints; pairwise Tanimoto similarity for similarity scoring; UMAP visualization of chemical-space distributions; counts of generated molecules; distribution of Tanimoto similarity scores; qualitative structural comparisons to published molecules; identification of rigid linker content for PROTACs.",
            "results_summary": "FragLlama can reproduce expert-level designs and explore novel, plausible chemical space: it generated thousands of valid molecular-glue candidates, rediscovered or produced molecules highly similar/identical to recently reported molecular glues, and designed diverse PROTAC linkers (similarities up to 0.93). Fine-tuning (especially with IC50 special tokens) increased overlap with known EGFR inhibitor chemical space; without fine-tuning designs were more distant from PubChem-known inhibitors.",
            "comparison_to_other_methods": "Fragment-level tokenizer improved compression (shorter token sequences) and chemical-semantic token interpretability versus atom-level tokenizers and general LLM tokenizers (example: for a GLP-1R inhibitor FragLlama token length 61 vs atom-level 105; GPT2/GPT4 token lengths 86/71 respectively). Attention patterns compared favorably to GPT2 (more targeted, sparse heads). No direct benchmarking against other molecular generation pipelines reported beyond tokenizer/attention qualitative comparisons.",
            "limitations_and_challenges": "Limitations reported include: lack of consensus for molecular sequentialization; dependence on quality/quantity of labeled data for alignment/fine-tuning; evaluation requires lengthy experimental validation; prompt instability concerns for molecular language; some generated designs did not exactly reproduce rings though mimicked chain length/functionalization; model and compute scale constraints (trained at 779M params on 7 A100 GPUs) may limit emergent capabilities; challenges in generating highly complex, elaborated drug-like compounds without target-specific fine-tuning.",
            "uuid": "e8640.0",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ChemSpaceAL",
            "name_full": "ChemSpaceAL (an active learning methodology applied to protein-specific molecular generation)",
            "brief_description": "Mentioned as an example of an existing chemistry-specific language/modeling approach applying efficient active learning to protein-specific molecular generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemSpaceAL",
            "model_type": null,
            "model_size": null,
            "training_data": null,
            "application_domain": "Drug discovery / protein-specific molecular generation (mentioned)",
            "generation_method": null,
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.1",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "cMolGPT",
            "name_full": "cMolGPT (conditional generative pretrained transformer for target-specific de novo molecular generation)",
            "brief_description": "Mentioned as an example of a chemistry-specific language model for target-specific de novo molecular generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "cMolGPT",
            "model_type": "generative pretrained transformer (mentioned)",
            "model_size": null,
            "training_data": null,
            "application_domain": "De novo molecular generation / target-specific drug design (mentioned)",
            "generation_method": "conditional generation (implied by name/description)",
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.2",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Regression Transformer",
            "name_full": "Regression Transformer",
            "brief_description": "Mentioned as a molecular language model enabling concurrent sequence regression and generation (prior work in molecular LLMs).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Regression Transformer",
            "model_type": "transformer (mentioned)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecular language modelling / property-conditioned generation (mentioned)",
            "generation_method": "sequence regression + generation (mentioned)",
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.3",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "MolGPT",
            "name_full": "MolGPT (molecular generation using a transformer-decoder model)",
            "brief_description": "Mentioned as a transformer-decoder based molecular generation model in prior literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolGPT",
            "model_type": "transformer-decoder (mentioned)",
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecular generation / de novo design (mentioned)",
            "generation_method": "autoregressive SMILES generation (implied by 'GPT'-style)",
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.4",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SAFE-GPT",
            "name_full": "SAFE-GPT (a framework for molecular design)",
            "brief_description": "Mentioned as a framework for molecular design (recent work referenced among chemistry-specific LLMs).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "SAFE-GPT",
            "model_type": null,
            "model_size": null,
            "training_data": null,
            "application_domain": "Molecular design (mentioned)",
            "generation_method": null,
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.5",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Darwin",
            "name_full": "Darwin (domain-specific large language models for natural science)",
            "brief_description": "Mentioned among prior domain-specific LLM efforts applied to natural sciences/chemistry.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Darwin",
            "model_type": null,
            "model_size": null,
            "training_data": null,
            "application_domain": "Natural sciences / chemistry (mentioned)",
            "generation_method": null,
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": null,
            "limitations_and_challenges": null,
            "uuid": "e8640.6",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "GPT-2 / GPT-4",
            "name_full": "GPT-2 and GPT-4 (OpenAI Generative Pre-trained Transformers)",
            "brief_description": "General-purpose LLMs referenced for comparison of tokenization and attention patterns; GPT2 used in attention comparisons with FragLlama.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-2 / GPT-4",
            "model_type": "decoder-only Transformer (general NLP LLMs)",
            "model_size": null,
            "training_data": "Large-scale human-language corpora (general description only; specifics not provided in paper)",
            "application_domain": "General NLP (used as baseline/comparison; not used for chemical generation experiments in this paper)",
            "generation_method": "standard autoregressive token generation (mentioned)",
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": "FragLlama's attention patterns were more focused/sparse compared to GPT-2 on molecular tokens; FragLlama tokenizer produced chemically meaningful tokens versus GPT tokenizers which lack chemical semantics.",
            "comparison_to_other_methods": "Qualitative comparison: FragLlama shows better targeted attention and chemical-semantic tokens versus GPT-2/GPT-4; tokenizer compression of FragLlama was better than GPT-4 for one example molecule.",
            "limitations_and_challenges": null,
            "uuid": "e8640.7",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "Llama 3 (architecture)",
            "name_full": "Llama 3 (decoder-only Transformer architecture by Meta AI)",
            "brief_description": "Referenced as architectural basis/inspiration for FragLlama; FragLlama modifies Llama 3 choices (positional encoding, attention variant).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Llama 3 (architecture)",
            "model_type": "decoder-only Transformer",
            "model_size": null,
            "training_data": null,
            "application_domain": "General LLM architecture (used as basis for molecular model modifications)",
            "generation_method": null,
            "novelty_of_chemicals": null,
            "application_specificity": null,
            "evaluation_metrics": null,
            "results_summary": null,
            "comparison_to_other_methods": "FragLlama modified Llama 3 by replacing RoPE positional encoding with standard positional encoding and replacing GQA with FlashAttention; FragLlama did NOT use Llama weights but adapted architectural ideas.",
            "limitations_and_challenges": null,
            "uuid": "e8640.8",
            "source_info": {
                "paper_title": "FragLlama: Next-fragment prediction for molecular design",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "cMolGPT: A conditional generative pretrained transformer for target-specific de novo molecular generation.",
            "rating": 2,
            "sanitized_title": "cmolgpt_a_conditional_generative_pretrained_transformer_for_targetspecific_de_novo_molecular_generation"
        },
        {
            "paper_title": "Regression transformer enables concurrent sequence regression and generation for molecular language modelling.",
            "rating": 2,
            "sanitized_title": "regression_transformer_enables_concurrent_sequence_regression_and_generation_for_molecular_language_modelling"
        },
        {
            "paper_title": "MolGPT: molecular generation using a transformer-decoder model.",
            "rating": 2,
            "sanitized_title": "molgpt_molecular_generation_using_a_transformerdecoder_model"
        },
        {
            "paper_title": "An efficient active learning methodology applied to protein-specific molecular generation.",
            "rating": 2,
            "sanitized_title": "an_efficient_active_learning_methodology_applied_to_proteinspecific_molecular_generation"
        },
        {
            "paper_title": "Gotta be SAFE: a new framework for molecular design.",
            "rating": 2,
            "sanitized_title": "gotta_be_safe_a_new_framework_for_molecular_design"
        },
        {
            "paper_title": "A Review of Large Language Models and Autonomous Agents in Chemistry.",
            "rating": 2,
            "sanitized_title": "a_review_of_large_language_models_and_autonomous_agents_in_chemistry"
        }
    ],
    "cost": 0.0122975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>FragLlama: Next-fragment prediction for molecular design</p>
<p>Jian Shen 
YDS Pharmatech, Inc
ETEC
1220 Washington Ave12226AlbanyNY</p>
<p>Shengmin Zhou 
YDS Pharmatech, Inc
ETEC
1220 Washington Ave12226AlbanyNY</p>
<p>Xing Che xche@yds-pharmatech.com 
YDS Pharmatech, Inc
ETEC
1220 Washington Ave12226AlbanyNY</p>
<p>FragLlama: Next-fragment prediction for molecular design
8A25B5B0A7BF85BEE8F1E97396B041BF
The emergence of ChatGPT has drawn significant attention to Large Language Models (LLMs) due to their impressive performance.While LLMs primarily focus on next token/word prediction, we apply this principle to molecular design by reframing the task as predicting the next token/fragment.We present FragLlama, a large language model trained for molecular design, featuring custom tokens that represent molecular fragments and functional groups.The model is for generating molecules given one or two fragments, for application scenarios like general hit-to-lead and lead optimization stage drug design, PROTAC linker design; mapping to commonly used drug design strategies like fragment growing and scaffold hopping.In the pre-training stage, we adapted the Llama 3 architecture to create FragLlama, training it to learn conditional probabilities of these fragment-level tokens.The subsequent alignment stage employed fine-tuning to guide the model towards generating molecules with desired properties.The effectiveness of FragLlama is demonstrated through its applications in designing molecular glue libraries, PROTAC linkers and EGFR binders.FragLlama demonstrates proficiency in reproducing expert-level designs while also exploring novel and promising chemical spaces, highlighting its potential to augment the capabilities of medicinal chemists in drug design.</p>
<p>Introduction</p>
<p>What is LLM and why it's so successful Large Language Models (LLMs) represent a major breakthrough in language modeling, building upon decades of advancements since Shannon's application of information theory in the 1950s. 1 Over time, various model architectures such as Linear Models 2 and Convolutional Neural Networks (CNNs) 3 emerged and dominated the field, only to reach their performance limits, necessitating new approaches.These innovations were often propelled by advances in hardware, which enabled machine learning models to leverage more complex statistical methods and larger datasets.</p>
<p>The success of LLMs, particularly those based on deep neural networks, can be attributed to two key principles.First, the concept of Universal Function Approximation 4,5 suggests that large neural networks, given enough neurons and depth, can theoretically approximate any continuous function.This capability allows LLMs to model highly complex relationships in data, making them powerful tools for understanding and generating language.As deep learning progressed, this principle became the foundation for building increasingly large and sophisticated models that could capture nuanced patterns in high-dimensional data. 6cond, the Turing completeness of Transformer-based architectures further enhances the power of LLMs. 7,8While earlier models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are also Turing complete, the Transformer architecture introduces significant advantages.Transformers, 9 through their self-attention mechanisms and parallelizable feed-forward networks, can process and comprehend vast amounts of contextual information more efficiently than their predecessors.This architectural innovation enables LLMs to perform a wide range of tasks with remarkable accuracy and flexibility.</p>
<p>LLMs, such as those based on the Generative Pre-trained Transformer (GPT), 10,11 leverage these principles-Universal Function Approximation and Turing completeness-to excel in natural language processing tasks.Pre-training on massive datasets, followed by fine-tuning for specific applications, enables these models to autonomously discover and extract intricate patterns in language, resulting in performance and capabilities that far surpass those of smaller or earlier models.</p>
<p>Why LLM for molecular design and the challenges</p>
<p>The success of LLMs in natural language processing has sparked interest in their application to other domains, including molecular design.LLMs possess two key strengths that make them well-suited for this task: representation power and generative capabilities.Their ability to capture complex patterns and relationships is critical for understanding molecular structures and properties.Additionally, LLMs' proficiency in generating novel content aligns with the need to design new molecules.Several chemistry-specific language models have already been developed, including but not limited to ChemSpaceAL, 12 cMolGPT, 13 Regression Transformer, 14 MolGPT, 15 SAFE-GPT, 16 and Darwin. 17For a more comprehensive review, please refer to the paper A Review of Large Language Models and Autonomous Agents in Chemistry. 18mpared to molecular representation using graph-based methods, the utilization of Transformer architectures offers superior scalability and the potential to uncover latent features that may elude human perception.Like Ilya Sutskever ever commented, "Predicting the next token well means that you understand the underlying reality that led to the creation of that token.It's not just statistics." 19This principle is directly applicable to molecular design, where LLMs can be used to infer and predict molecular properties in a manner akin to how they process natural language.</p>
<p>While the application of Language Models in Chemistry presents promising avenues for research, it is imperative to acknowledge several significant challenges.This study highlights a few key issues:</p>
<ol>
<li>
<p>Sequentialization of molecular information: Human languages are inherently sequential, and observations from GPT-like Large Language Models (LLMs) indicate that the token count for expressing identical information varies considerably across languages, as does the model's logical reasoning capability.In the field of chemistry, however, there is no consensus on the optimal "molecular language" for the sequential representation of molecules.</p>
</li>
<li>
<p>Diversity and complexity of chemical systems: The vast heterogeneity among chemical systems presents a major obstacle to developing universally applicable models for designing effective drug molecules.Additionally, the limited availability of high-quality, labeled molecular data hinders the alignment stage training, reducing the model's ability to design molecules with specific desired properties.In general, the lack of quality data impedes the development of models at a sufficient scale to unlock emergent capabilities.</p>
</li>
</ol>
<p>Evaluation challenges:</p>
<p>Evaluating molecule language models is more complex than traditional LLMs.Validating the qualities of designed molecules often requires lengthy experimental procedures, resulting in extended feedback times.</p>
<p>These challenges underscore the complexity of adapting language model paradigms to the chemical domain and highlight the need for innovative approaches in molecular representation, data acquisition, and model evaluation.</p>
<p>What's special about FragLlama</p>
<p>The FragLlama model adapts the next-token prediction paradigm used in decoder-only Large Language Models (LLMs) to molecular design.In this approach, we introduced new tokens specifically trained to represent molecular fragments and functional groups.The current token vocabulary consists of 8,000 tokens, model trained on approximately 70 billion tokens, with the model size set at 779 million parameters.The model is designed to handle tasks such as fragment growth, scaffold modification, scaffold hopping, and linker design.By framing molecular design as a next-fragment prediction task, FragLlama mimics the strategies employed by medicinal chemists, who often grow molecules from a starting fragment or substitute functional groups on a core scaffold.We trained the tokens from scratch, ensuring they are tailored to better understand molecular fragments and functional groups, rather than using tokens from LLMs trained on human language or SMILES atom tokens.</p>
<p>Building upon this innovative framework, we explored FragLlama's practical applications in diverse molecular design scenarios.The model demonstrates its ability to generate diverse and chemically valid structures, exploring both known and novel chemical spaces.</p>
<p>We illustrate this through the creation of a molecular glue library, where FragLlama not only reproduces expert-designed compounds but also ventures into unexplored regions of chemical space.In PROTAC linker design, FragLlama exhibits proficiency in generating structurally diverse linkers, including rigid ones, which are particularly valuable in this field.Furthermore, we demonstrate FragLlama's adaptability through fine-tuning experiments focused on EGFR inhibitor design.The model's performance improves when provided with comprehensive input data, including known inhibitors and their bioassay results.Throughout these applications, FragLlama showcases its potential to assist medicinal chemists by generating expert-level designs while also offering novel structural ideas.</p>
<p>Methods and Discussions</p>
<p>Data preparation and tokenization to represent molecular fragments</p>
<p>Recent research has highlighted significant limitations in the SMILES (Simplified Molecular Input Line Entry System) 20 representation of molecules, particularly in the context of its application to language models.2][23] This discrepancy introduces several potential problems:</p>
<ol>
<li>Spatial coherence in token prediction: The next-token prediction paradigm fails to consider the correct spatial "order" of molecular generation, potentially leading to chemically implausible structures.</li>
</ol>
<p>Tokenization challenges:</p>
<p>The lack of spatial correspondence hinders effective tokeniza-tion, which is crucial for language model performance.</p>
<p>Constraints on generation strategies: SMILES representation necessitates complete,</p>
<p>global generation of the entire molecule, rather than allowing for fragment-based or incremental generation.This requirement significantly alters the embedding space and complexity of next-token prediction tasks.Furthermore, it imposes limitations on fragment insertion and other partial generation techniques.</p>
<p>To address these issues, we trained tokens from scratch to represent molecular fragments, intentionally incorporating chemical domain knowledge prior to the language model processing, effectively reducing the complexity of the next token prediction.</p>
<p>Additionally, we have implemented corresponding data augmentation algorithms.These algorithms ensure that high-quality datasets can be expanded while maintaining chemical validity.Our testing demonstrates that this approach allows for lossless reconstruction of the original SMILES molecule, preserving chemical integrity throughout the augmentation process.</p>
<p>We used the Byte Pair Encoding (BPE) 24 algorithm for tokenization.Its fundamental principle can be described as follows: BPE initiates at the character level and proceeds through iterative merges of the most frequently occurring adjacent character pairs or subword pairs.Each iteration generates a new subword, which is subsequently added to the vocabulary.This process continues until a predefined vocabulary size is reached or a stopping criterion is met.</p>
<p>The efficacy of BPE lies in its ability to create a vocabulary that efficiently represents the data while balancing the trade-off between vocabulary size and token length.In the context of chemical representations, BPE can capture meaningful substructures and patterns within molecular strings, potentially leading to more effective encoding of chemical information for language models.This approach allows for adaptive tokenization that can reflect the underlying structure and frequency of molecular subunits, potentially enhancing the model's ability to learn and generalize chemical patterns.Unlike traditional methods, our approach incorporates an additional preprocessing step, which segments input molecules into smaller fragments before applying BPE.This allows the algorithm to recognize common patterns at a more granular level, leading to an optimized vocabulary specifically suited for molecular structures.The strength of this method lies in its ability to more effectively tokenize new and unseen molecules, enhancing the model's generalization capabilities.</p>
<p>Figure 1: Fragment-level tokenization process for molecular design.Starting from input SMILES strings, molecules are retrieved from a molecular database and fragmented into smaller units.These fragments form a "fragment vocabulary".In the tokenization process, the input SMILES is broken down into these molecular fragments, each assigned a corresponding token ID.The process identifies the "smallest semantic unit" for molecular representation through statistical methods, facilitating the generation and manipulation of molecular structures in subsequent tasks.</p>
<p>FragLlama Model Design Next Token Prediction for Molecule Language Modeling</p>
<p>Next Token Prediction is the core task of large language models (LLMs).In this task, the model predicts the most likely next word/token based on a given context sequence.By analyzing linguistic patterns and semantic relationships in existing text, the model learns to predict the subsequent elements in a sequence.This process is autoregressive, meaning the model generates tokens sequentially, incorporating each newly generated token into the context for subsequent predictions.Next Token Prediction enables LLMs to generate coherent text, complete sentences, and excel in various natural language processing tasks.</p>
<p>The transformation of complex human language and thought into Next Token Prediction represents a remarkably ingenious concept.The loss function employed is simply crossentropy, a fundamental measure in information theory.Entropy and information are essentially manifestations of "non-uniformity."Cross-entropy can be viewed as a form of "compression assistance."In the context of Molecule Large Language Models, for example, the token following a double bond in SMILES notation (represented by an equals sign "=") is not arbitrary, but rather represents "non-uniform" information.The entire model training process can be conceptualized as the model's progression from a "uniform" next token prediction to a non-uniform state.In LLMs, the metric "perplexity" is used to characterize the confidence level of the next token prediction.We can readily observe that the decrease in loss value tends to plateau early, while internal learning within the model continues.This process reflects the model's transition from a state of "ignorance" to one of "knowledge."</p>
<p>In its initial state, the model's predictions for all tokens are uniform, representing maximum uncertainty and minimum information content.As training progresses, the model's predictions become increasingly "non-uniform," signifying greater certainty and information richness.</p>
<p>In the chemistry domain, this translates to the model learning the inherent constraints and patterns of molecular structures.For instance, the model learns that certain atomic combinations or bond types are more probable in specific contexts, mirroring the non-uniform distribution of chemical structures in reality.This transformation not only enables the model to accurately predict the next token but also allows it to generate meaningful and chemically plausible molecular structures.This process reflects a fundamental aspect of machine learning in complex domains, where the transition from a state of maximum entropy to one of structured knowledge mirrors the acquisition of domain-specific understanding.</p>
<p>The elegance of this approach lies in its ability to capture complex chemical knowledge through a conceptually simple prediction task.By minimizing cross-entropy, the model is effectively learning to compress chemical information efficiently, capturing the underlying patterns and rules of molecular structures.This framework allows for the implicit learning of chemical principles without the need for explicit rule encoding, potentially enabling the model to discover novel patterns or relationships in chemical data.The Transformer's powerful feature extraction capabilities and scalability have established it as the foundational architecture for modern natural language processing tasks and multimodal applications.In the biological and chemical domains, the most notable application is the AlphaFold series, 26,27 which has revolutionized protein structure and biomolecular interaction prediction.</p>
<p>While the original Transformer architecture introduced in the seminal paper was an encoder-decoder structure, subsequent developments have led to the widespread adoption of encoder-only and decoder-only architectures:</p>
<ol>
<li>
<p>Encoder-Only Models: These are primarily used for global sequence embedding.In biology, examples include the ESM series of protein language models. 28In NLP, the classic example is Sentence BERT.These models excel at capturing contextual representations of entire sequences.</p>
</li>
<li>
<p>Decoder-Only Models: These are more suited for generalized few-shot learning tasks.</p>
</li>
</ol>
<p>Through techniques like prompting, they can be adapted to various domains without finetuning.The most prominent example is the GPT series.These models are particularly adept at generative tasks and can be more flexible in their applications.Decoder-only Models can be treated as Encoder-Decoder Models but share the same weights across Encoder and Decoder.</p>
<p>Generally, at smaller model sizes, BERT-like encoder-only models tend to outperform GPT-like decoder-only models, as the autoregressive loss function in decoder-only models is more challenging to optimize. 29However, as model size increases, decoder-only models often demonstrate superior performance and versatility.Our choice of a decoder-only model architecture for molecule language modeling is motivated by its greater flexibility and the relative novelty of this approach in the field.To the best of our knowledge, many aspects of chemistry language modeling remain unexplored.This choice allows us to leverage the generative capabilities and adaptability of decoder-only models in tackling various chemistryrelated tasks, from molecular property prediction to drug design.</p>
<p>Main Architectural Differences between FragLlama and Llama 3</p>
<p>The Llama series of large language models, developed by Meta AI (formerly Facebook AI Research), represents a significant advancement in the field of natural language processing.The Llama architecture is based on the decoder portion of the Transformer.Compared to GPT2, Llama incorporates several crucial improvements and optimizations: 25 1.Rotary Position Encoding (RoPE): 30 This technique efficiently handles positional information in the input sequence, allowing the model to better understand the relative positions of tokens.</p>
<ol>
<li>
<p>SwiGLU Activation Function: 31 This enhanced activation function improves the model's capacity for non-linear transformations, potentially leading to better performance across various tasks.</p>
</li>
<li>
<p>Grouped Query Attention (GQA): 32 Optimizations in the attention mechanism contribute to the model's computational efficiency by reducing the KV cache.</p>
</li>
</ol>
<p>These architectural choices enable Llama to demonstrate robust performance across a wide range of natural language processing tasks while maintaining relatively modest model sizes and computational requirements.</p>
<p>The reason we chose Llama is that, unlike some closed-source models, Llama's openness promotes open research and innovation in the AI community.Although we did not utilize Llama weights directly, our model development was significantly informed by research that employed Llama weights, such as Alpaca, 33 TinyLlama, 34 and QLoRA. 35These studies provided substantial insights and inspiration for our approach to model development.The advancements made in these Llama-based models offered valuable lessons in efficient training techniques, model architecture optimization, and fine-tuning strategies.By leveraging the Llama architecture for our molecule language model, we aim to combine the power of state-of-the-art natural language processing with the specific requirements of molecular representation and prediction.This approach allows us to benefit from the extensive research and optimization that has gone into Llama's development while tailoring the model to the unique challenges of molecule language modeling.</p>
<p>Based on the specific requirements of small molecule generation and the enhanced efficiency of our tokenizer, we have implemented several modifications to the Llama 3 architecture.These adjustments are primarily driven by the following considerations:</p>
<ol>
<li>Positional Encoding:</li>
</ol>
<p>We have replaced the Rotary Position Encoding (RoPE) with a standard positional encoding scheme.This change is justified by the fact that our objective focuses on small molecule generation, where token sequences rarely exceed 100 tokens.The long-range context capabilities provided by RoPE, while beneficial for general language tasks, are not critical for our application.This modification simplifies our model architecture without compromising performance in our specific domain.</p>
<p>Attention Mechanism:</p>
<p>We have opted to use Flash Attention instead of the Grouped Query Attention (GQA) employed in Llama 3.While GQA offers computational efficiency advantages and facilitates single-card inference for larger models, it can potentially reduce model performance.The trade-off introduced by GQA is more beneficial for models with 7 billion parameters or more, where the reduction in attention parameters can be compensated by increased Feed-Forward Network (FFN) capacity.However, for our relatively smaller model size, this trade-off is suboptimal.</p>
<p>Flash Attention 36 provides an efficient attention computation without the performance drawbacks of GQA.This choice allows us to maintain the full expressiveness of the attention mechanism, which is crucial for capturing the intricate patterns and relationships in molecular structures.</p>
<p>By carefully considering the trade-offs between model complexity, performance, and computational efficiency in the context of molecular generation, we have developed a more targeted and potentially more effective architecture for molecule language models.This tailored architectural modification simplifies the model structure and improves training efficiency.</p>
<p>The use of Flash Attention ensures that we do not sacrifice model quality for computational efficiency, which is particularly important given our smaller model size.These changes reflect careful consideration of the unique characteristics of molecular data and the specific requirements of molecule language models.</p>
<p>Pre-Training</p>
<p>Our training infrastructure, supported by the Oracle Cloud Team, represents a significant computational resource tailored for the demands of large-scale language model training in the domain of chemistry.We utilized a cluster of 7 NVIDIA A100 GPUs for the pre-training phase.And using DeepSpeed with ZeRO Stage 3 optimization for distributed training.</p>
<p>Alignment-Fine Tuning</p>
<p>Alignment in LLM refers to the crucial challenge of ensuring AI systems behave in ways that are consistent with human values and intentions, especially as these systems become more advanced and autonomous.Researchers and developers employ various methods to address this challenge, including fine-tuning pre-trained models on specific datasets, Reinforcement Learning from Human Feedback (RLHF) 37 to guide models using human preferences, reward 1.The first approach involved direct utilization of the 6,822 molecules with high IC50 values.</p>
<ol>
<li>The second approach combined all molecules, incorporating an untrained special token at the beginning of each molecule to denote its IC50 range.</li>
</ol>
<p>Given the nature of fine-tuning, we employed a reduced learning rate and a shorter training duration.This strategy was aimed at preserving the model's pre-trained knowledge while adapting it to the specific EGFR binder-related chemical space.For molecular design, we selected to use the scaffold 4-Anilinoquinazoline as its the most common smallest scaffold in our curated dataset.Given the multiple growth anchors identified in this scaffold, we didn't assign specific growth anchors.Accordingly, we conducted three types of molecular design: 1) default settings without fine-tuning; 2) fine-tuned with 6,822 EGFR inhibitors, without IC50 special tokens; 3) fine-tuned with both 8,532 EGFR inhibitors and their IC50 special tokens.</p>
<p>Decoding Strategies</p>
<p>LLM decoding strategies are methods for controlling the quality and diversity of output when generating text from language models.These strategies determine how to select the next token from the model's probability distribution.The specific decoding strategy we used is beam search variance, as it maintains multiple candidate sequences and ultimately selects the sequence with the highest overall probability.Common decoding strategies include Greedy Search, Beam Search, 38 Temperature Sampling, 39 Top-k sampling, 40 Top-p (nucleus sampling), 41 frequency penalty, 42 and presence penalty. 10These strategies balance determinism and creativity in text generation to varying degrees and are suitable for different application scenarios.Choosing an appropriate decoding strategy is crucial for generating high-quality, coherent, and diverse text.</p>
<p>Results and Discussion</p>
<p>Tokenizer Compression Rate and Meaningful Tokens</p>
<p>In our study, we explored the efficiency of various tokenizers in representing chemical structures by evaluating their compression rates.The compression rate was defined as the ratio of the original molecular data size to the tokenized data size.A lower compression rate indicates that the tokenizer is more efficient in representing the molecular structures with fewer tokens, while a higher compression rate suggests the opposite.</p>
<p>Take the molecule illustrated in Figure 3, a GLP-1R inhibitor 43 developed by Gilead Science, as an example.Using the FragLlama fragment-level tokenizer with an 8k vocab size, the resulting token length is 61.The token sequence length obtained using the atomlevel tokenizer is 105 (the atom-level tokenizer is simply char-level tokens with regex match).</p>
<p>The GPT2 tokenizer with a 5k vocab size yields a token length of 86, while the GPT4 tokenizer with a 128k vocab size results in a length of 71.From the current perspective, we have achieved a better "compression ratio" than GPT4 using an 8k vocab size.Considering only the compression ratio, shorter sequences bring many benefits.For instance, for molecules similar to the GLP-1R inhibitor, the FragLlama tokenizer needs to perform 61 next token predictions, while the atom-level tokenizer requires 105.Additionally, shorter input sequences for molecules of the same length mean less memory usage and more efficient attention queries.This allows models with the same architecture and number of parameters to easily handle more complex input sequences.</p>
<p>Secondly, for a tokenizer, the "semantic representation ability" is more important compared to the compression ratio.Although GPT4's token compression capability for GLP-1R inhibitor is similar to the FragLlama Tokenizer, GPT4's training data is mainly based on human language, which means GPT4's tokens lack chemical significance.Based on prior chemical knowledge and statistical findings, our "molecular language tokens" can be visual-ized, and they are all common components of drug molecules.This is not surprising, as the "natural language tokens" obtained by LLMs like Llama through statistical patterns also usually have linguistic value.Excellent semantic representation ability is one of the core factors in making the model easy to train and reducing model complexity.In comparison, although the atom token has a smaller vocab size, the diversity of SMILES makes the model much more difficult to train than the FragLlama Tokenizer.For example, to generate a benzene ring, the atom-level tokenizer requires 6 times more memory in the attention layers, and needs to perform 6 times of next token predictions, and this is assuming the optimal decoding parameters have been found, which adds a lot of potential instability.</p>
<p>Figure 4 showcases some typical tokens used by the FragLlama model.Although FragLlama's Tokenizer is built based on the BPE (Byte Pair Encoding) algorithm, after training, we can observe that many of the generated tokens actually correspond directly to fragments of chemical molecules.This phenomenon bears an interesting similarity to the results of BPE training in human language processing.Just as in natural language processing, where tokens after BPE training often retain linguistically meaningful units (such as roots, affixes), FragLlama's Tokenizer has successfully captured meaningful fragments in chemical structures.This result suggests that our method is not merely a mechanical segmentation of SMILES strings, but has successfully learned the chemical semantics within molecular structures.This chemically informed segmentation approach helps the model better understand and generate molecular structures, as it directly operates on chemically meaningful units rather than just abstract character sequences.</p>
<p>We have repeatedly witnessed the instability of prompts in LLMs like GPT, which is because human language itself is highly variable, and a change in a single word can completely alter the expressed meaning.We believe this is also true for molecular language (SMILES).</p>
<p>A better token vocab set can effectively alleviate this issue.</p>
<p>In summary, a good tokenizer significantly reduces vocabulary size while maintaining good semantic representation ability, which helps to reduce model complexity and memory A good tokenizer can also preserve important linguistic information through reasonable segmentation methods.In the specific domain of molecular design, our tokenizer is more conducive to preserving chemical functional groups and capturing atomic connectivity patterns.This "linguistic" parsing approach to chemical structures enables the model to directly manipulate chemically meaningful units, rather than just abstract character sequences, thereby improving the model's performance and efficiency in molecular design tasks and laying a solid foundation for subsequent model designs.</p>
<p>Attention Weights Visualization</p>
<p>The attention mechanism allows the model to focus on different parts of the input sequence when generating a particular output.It computes a weighted sum of the input tokens based on their relevance to each other, allowing the model to capture dependencies between tokens regardless of their distance from one another.Multihead attention was used to capture multiple types of relationships or features across the input sequence.Each head in multihead attention learns a different perspective on the input.By having multiple heads, the model can look at various aspects of the data simultaneously.It can learn both fine-grained and high-level patterns across different parts of the input sequence.After each head processes the input, the results are concatenated and combined into a single output, providing a rich, multi-faceted representation of the input sequence.</p>
<p>For instance, in the sentence, "The scientist carefully analyzed the results of the experiment to ensure the accuracy of the findings, despite encountering several unexpected challenges along the way," one head might focus on the grammatical structure, identifying subject-verb-object relationships such as the link between "scientist," "analyzed," and "results."Another head might track long-range dependencies, connecting the main clause with the subordinate clause by linking "analyzed" with "to ensure" to understand how the action relates to ensuring accuracy.Other heads may capture semantic roles, identifying the "scientist" as the agent and the "results" as the theme, or modifiers like "carefully," which gives more detail on how the action was performed.Additional heads focus on prepositional phrases like "of the experiment" to capture spatial and causal relationships, sentiment and connotation to identify the contrast introduced by "despite encountering several unexpected challenges," and temporal or sequential relationships to follow the logical flow of events from analyzing the results to ensuring their accuracy.Some heads handle coreference resolution, linking "results" to "findings," while others focus on contrast and concession, capturing the opposition between ensuring accuracy and encountering challenges.</p>
<p>In analogy to molecules, different attention heads in the FragLlama model may specialize in tracking various molecular features.For instance, one head might focus on the structural relationships between molecular fragments, identifying how different atoms and bonds contribute to synthesizability.Another head could track long-range dependencies between functional groups to predict lipophilicity, ensuring that hydrophobic and hydrophilic regions are properly represented.Similarly, other heads may capture chemical reactivity, such as tracking electrophilicity to determine how reactive certain parts of the molecule are.Just as in language models, each attention head plays a specialized role in interpreting different aspects of the molecular structure.</p>
<p>While models like GPT are highly powerful for linguistic tasks, the tokens and attention mechanisms they learn are not well-suited for tasks like molecular design and property prediction.The token embedding space in natural language models is optimized for capturing relationships between words, phrases, and syntax, which differ significantly from the structural and chemical properties of molecules.In molecular design, precise attention to interactions, bonding patterns, and chemical properties is critical.The role of attention in molecular models like FragLlama is to rearrange and combine molecular tokens in a way that captures these complex relationships, producing more meaningful representations for molecules.As a result, models trained on language data, like GPT2, are not interchangeable with those trained on molecular data, as they do not adequately capture the nuances of molecular features necessary for accurate predictions.</p>
<p>To explore this hypothesis, we used the GLP-1R inhibitor (Figure 3), comparing attention patterns between FragLlama and GPT2.We selected two tokens present in both the FragLlama Tokenizer and the GPT2 Tokenizer.As demonstrated in Figure 5, FragLlama employs a more targeted and efficient attention mechanism compared to GPT2.For token 'F', FragLlama exhibits a highly selective attention pattern, concentrating its focus on critical heads, as evidenced by the sparse yet focused activations in panel (a).This selective attention likely reflects a refined mechanism tailored for molecular design tasks, where precision in capturing key features and relations among the tokens is paramount.In contrast, GPT2 displays a more diffuse attention pattern for the same token, which may be less optimal for molecular applications where attention needs to be sharply focused on specific molecular features.For token 'N', FragLlama again demonstrates its ability to allocate attention selectively and effectively across fewer heads, as seen in panel (b).This contrasts with GPT2's broader and more distributed attention in panels (c) and (d), which might dilute the model's ability to focus on the most relevant aspects of molecular structures.The targeted attention strategy of FragLlama suggests that it is better suited for handling the specialized demands of molecular design.Fragment Growth Design: Showcasing the Creation of a Molecular Glue Library Molecular glue degraders are small molecules that facilitate the targeted degradation of specific proteins by promoting the formation of a ternary complex between a target protein and an E3 ubiquitin ligase. 44,45Unlike traditional inhibitors that block the function of a protein, molecular glue degraders act as catalytic agents, enhancing the recruitment of an E3 ligase to the target protein, which is then ubiquitinated and directed to the proteasome for degradation.These degraders represent a novel therapeutic strategy in drug discovery, particularly valuable in targeting proteins that were previously considered "undruggable". 46lecular glues have gained significant attention due to their potential to modulate protein interactions in a highly specific manner.For example, thalidomide 47 analogs like lenalidomide 48 and pomalidomide 49 are well-known cereblon binders, acting as molecular glues by bridging interactions between cereblon (CRBN) and transcription factors like Ikaros, leading to their ubiquitination and degradation.This mechanism not only opens new pathways for drug discovery but also enables the modulation of biological processes that are otherwise difficult to influence through traditional small molecule inhibitors. 50,51e employed the fragment growth strategy provided by FragLlama to design molecular glues, using three common cereblon binders-S-Pomalidomide, S-Lenalidomide, and S-Thalidomide-as starting points.We identified 3, 3, and 4 fragment growth anchors in the Protein Data Bank (PDB) for S-Pomalidomide, S-Lenalidomide, and S-Thalidomide, respectively (Figure 6).Using our pretrained model for molecular growth from given fragments and anchors, we generated 5,834 molecules after sanitization and deduplication.To compare the distribution of these FragLlama-generated molecules in chemical space with expert designs, we curated a reference dataset of 73,443 molecules from PubChem containing substructures of these cereblon binders.</p>
<p>We calculated Atom-pair descriptors 52 for both the curated PubChem molecules and FragLlama-designed molecules.Tanimoto distance 53 was used to calculate pairwise similarities between fingerprints.To visualize and compare the distributions in chemical space, we applied UMAP (Uniform Manifold Approximation and Projection) 54 for dimension reduction, creating a two-dimensional embedding.The UMAP visualization (Figure 7) reveals that FragLlama-designed molecules partially overlap with expert-designed compounds from PubChem in chemical space.While Pub-Chem compounds cover a broader range (PROTACs were included in the curated dataset), FragLlama also explores regions not represented in the curated dataset.Our analysis identified four distinct regions of novel chemical space that FragLlama successfully explored, with representative structures listed in Table 1.This demonstrates the model's capability to both reproduce the distribution of known drug-like compounds and design innovative molecules in underexplored areas.</p>
<p>Table 1: Comparison of scaffold structures and FragLlama-designed molecules across three distinct regions of chemical space.The top row shows the original scaffolds used as starting points in each region, while the bottom row presents the novel molecules generated by FragLlama in the corresponding regions.The regions, defined by specific coordinates, demonstrate FragLlama's ability to explore new areas of chemical space and design structurally diverse molecules from the initial scaffolds.</p>
<p>To further validate FragLlama's performance in designing drug-like molecules, we compared our generated compounds to recently reported molecular glues.Notably, we observed striking similarities between FragLlama's designs and several recently disclosed molecular glues.For instance, a recent study described potent, selective, and orally bioavailable molecular glue degraders of CK1, including a hit compound and its optimized version, SJ3149 (Figure 8A-B, first row). 55Remarkably, we identified a FragLlama-designed molecule ((4.2, 2.5) in Figure 7) that is structurally similar to the hit compound, with only one nitrogen atom moved one position along the chain of the molecule (Figure 8A, second row).Moreover, we found a FragLlama-designed molecule ((3.1, 2.9) in Figure 7) that is highly similar to SJ3149.The attached heterocyclic rings in both molecules follow a very similar pattern, with both featuring a five-membered ring fused to a six-membered aromatic ring and connectivity remain consistent, preserving the linkage through the NH group.This further demonstrates our model's capacity to generate pharmaceutically relevant compounds.In another example, we compared FragLlama's output to PT-179, a novel IMiD derivative that binds CRBN without inducing degradation of off-target proteins (Figure 8C, top). 56Once again, we identified a FragLlama-designed molecule ((12.2, 4.4) in Figure 7) identical to PT-179.These findings highlight FragLlama's ability to design expert-level molecules, underscoring its value in real-world drug discovery efforts.Fragment Linking Molecular Design: Showcasing PROTAC Linker Designs PROTACs are another type of degrader that differ from molecular glue degraders in their mechanism of action.While molecular glues promote the natural interaction between a target protein and an E3 ubiquitin ligase, PROTACs are bifunctional molecules that chemically link the two together.One end of a PROTAC binds to the target protein, and the other binds to the E3 ligase, facilitating ubiquitination and subsequent degradation.[59][60] Here, we tested three PROTACs: SP27 61 (inducing ternary structure with cereblon and PLK4 protein, Figure 9A), ARV-471 62 (cereblon and estrogen receptor, Figure 9B), and CP07 63 (cereblon and CDK9 protein, Figure 9C).FragLlama designed 156, 537, and 52 linkers for these PROTACs, respectively.To compare our designed linkers with the expert designed ones, we calculated similarities using Atom-pair descriptors 52 for the whole molecular structures and Tanimoto distance 53 as the similarity metric.For SP27, we found a designed PROTAC with a similarity score of 0.93, reproducing both the carbonyl group and six-member ring, with the only structural difference being a missing nitrogen atom in the ring (Figure 10A).For ARV-471 and CP07, the most similarly designed PROTACs had scores of 0.78 and 0.74, respectively.While we didn't reproduce the six-member rings, we achieved the same carbon chain lengths and generated branch chains with methyl and carbonyl groups to mimic the ring structure.(Figure 10B-C).</p>
<p>These examples demonstrate FragLlama's capability to design drug-like linkers for PRO-TACs.The distribution of similarity scores for these three examples showed a wide range, indicating high diversity in our designed linkers (Figure 11).This suggests that FragLlama can explore a wide range of linker structures in chemical space, potentially leading to the design of novel, effective PROTAC linkers.Rigid linkers are particularly favored in linker engineering.We also examined its capability of generating rigid linkers, as illustrated in     52 The plot compares three groups of molecules: 1) fine-tuned FragLlama designed molecules with both EGFR inhibitors and IC50 special tokens (red); 2) fine tuned FragLlamadesigned molecules with only EGFR inhibitors (yellow); 3) non-fine-tuned FragLlamadesigned molecules, alongside EGFR inhibitors from PubChem with active results in bioassays and valid IC50 values (blue).</p>
<p>To compare FragLlama-designed molecules with known inhibitors, we selected 1,769 EGFR inhibitors from PubChem that have 4-Anilinoquinazoline.We visualized the results using UMAP (Figure 13).The UMAP visualization revealed that while all three sets of FragLlama-designed molecules occupied different chemical spaces than the PubChem compounds, the set designed with the model fine-tuned with EGFR binders and IC50 tokens showed greater overlap with PubChem regions.The binder-fine-tuned-only set performed better than the non-fine-tuned set in this regard.The challenge for FragLlama lay in designing drug-like compounds with greater complexity, as the scaffold we used was the smallest common structure, while real compounds in the PubChem dataset were more elaborate.This explains why FragLlama-designed molecules without fine-tuning remained largely separate from PubChem-covered regions.However, with a fine-tuned model with both binders and IC50 tokens, FragLlama successfully designed some molecules within PubChem-covered regions.Table 2 listed examples of designed molecules in these overlapping regions, showing their 2D UMAP coordinates, scaffold structures, representative compounds from PubChem, and FragLlama-designed molecules.This demonstrates that FragLlama's ability to design drug-like molecules is enhanced when provided with comprehensive input data, including knowledge of the target protein, its known inhibitors, their bioassay results, and IC50 values, especially when more active compounds are incorporated into the fine-tuning process.</p>
<p>Conclusions</p>
<p>In conclusion, the core innovation of FragLlama lies in transforming traditional text token prediction into the prediction of molecular fragments, allowing the model to capture localized chemical features and common patterns in molecular structures.By progressively predicting and adding new fragments, FragLlama generates complete molecular structures while ensuring chemical reasonability and enhancing structural diversity.This fragmentlevel tokenization method bridges the gap between chemical structural representation and</p>
<p>Figure 1
1
Figure 1 illustrates the process of our fragment-based BPE (Byte Pair Encoding) training.</p>
<p>Figure 2
2
Figure 2 illustrates our FragLlama model architecture, which is modified from the Llama 3 architecture, 25 and demonstrates how the classic "Next token prediction" task is applied to this fragment-based molecule language model.</p>
<p>Figure 2 :
2
Figure 2: FragLlama architecture and molecular fragment generation process.(a) The architecture of the FragLlama model, built on a multi-layer transformer framework with components including Embedding, Positional Encoding, Flash Attention, SwiGLU Feed Forward layers, and RMS normalization.The architecture repeats Nx times, allowing for deeper processing of molecular fragment representations.(b) The fragment-level token generation process.Starting with an initial input molecule, FragLlama predicts the next fragment token iteratively.Each step adds a new fragment or functional group, progressively generating a complete molecular structure.Token IDs corresponding to molecular fragments are shown, and new tokens predicted at each step are highlighted in red.</p>
<p>modeling to accurately reflect human values.Alignment in LLMs also equips molecular LLMs with the ability to incorporate various types of extra information, such as experimental data, modeling data, and prior data, into the molecule generation process.By integrating this additional data, molecular LLMs can align more closely with real-world constraints and objectives, enhancing their predictive accuracy and relevance in drug discovery.For instance, experimental data on compound potency or selectivity can be used to fine-tune the model, ensuring that generated molecules meet specific biological criteria.Similarly, modeling data, such as structure-activity relationships (SAR) or docking scores, can guide the generation toward favorable chemical spaces.Prior data, representing previously collected datasets or knowledge, helps further refine the model's output, ensuring it builds upon existing research to generate novel, high-potential compounds.This ability to integrate diverse sources of information is a key advantage of alignment in molecular LLMs, making them more adaptive and useful for real-world drug design challenges.To demonstrate our current model's significant fine-tuning potential, we conducted a proof-of-concept experiment.Our objective was to investigate whether the fine-tuned model could generate molecules more closely resembling human-designed EGFR binders.Here, we collected 8,532 EGFR inhibitors from PubChem with active bioassay results and valid IC50 values to fine-tune FragLlama.The dataset included approximately 6,822 molecules with high IC50 values and 1,710 molecules with low IC50 values.Two distinct fine-tuning approaches were implemented:</p>
<p>Figure 3 :
3
Figure 3: A GLP-1R inhibitor (Gilead Science)</p>
<p>Figure 4 :
4
Figure 4: FragLlama Token Visualization</p>
<p>Figure 5 :
5
Figure 5: Attention patterns of FragLlama and GPT2 across heads for tokens 'F' and 'N'.(a) FragLlama attention for token 'F' across all heads and generation steps.(b) FragLlama attention for token 'N' across all heads and generation steps.(c) GPT2 attention for token 'F' across all heads and generation steps.(d) GPT2 attention for token 'N' across all heads and generation steps.The color scale represents attention intensity on a logarithmic scale, with yellow indicating higher attention and blue indicating lower attention.</p>
<p>Figure 6 :
6
Figure 6: Fragment growth anchors for three cereblon binders: (A) S-Pomalidomide, (B) S-Lenalidomide, and (C) S-Thalidomide.The molecular structures of each cereblon binder are shown with their respective fragment growth anchors highlighted.These anchors serve as starting points for fragment growth.</p>
<p>Figure 7 :
7
Figure 7: Visualization of molecular distribution in chemical space using Uniform Manifold Approximation and Projection (UMAP).The plot compares FragLlama-designed molecular glue molecules (red) with expert designed molecules, curated from PubChem (blue).Four novel chemical space regions (Regions 1-4) explored by FragLlama are highlighted on the right, with representative molecular structures illustrating key structural features.Three recently reported molecular glues are also indicated on the left: an analog of the hit compound for CK1 degradation, an analog of SJ3149 (optimized CK1 degrader), and PT-179 (a novel IMiD derivative)</p>
<p>Figure 8 :
8
Figure 8: Comparison of recently reported molecular glues with FragLlama-designed molecules: (A) The hit compound for CK1 degradation and its structurally similar FragLlama-designed molecule.(B) SJ3149, an optimized CK1 degrader from the hit compound, alongside a structurally similar FragLlama-designed molecule.(C) PT-179, a novel IMiD derivative that binds CRBN without off-target protein degradation, and its structurally identical FragLlama designed molecule.</p>
<p>Figure 9 :
9
Figure 9: Structural representations of three PROTAC examples with their fragment growth anchors: (A) SP27 (cereblon-PLK4), (B) ARV-471 (cereblon-estrogen receptor), and (C) CP07 (cereblon-CDK9).The molecular structures highlight the E3 ligase binders, target protein binders, and linkers, with fragment growth anchors indicated for linker design by FragLlama.</p>
<p>Figure 10 :
10
Figure 10: FragLlama-designed PROTACs with high structural similarity to reference PRO-TACs: (A) SP27 (similarity score: 0.93), (B) ARV-471 (similarity score: 0.78), and (C) CP07 (similarity score: 0.74).</p>
<p>Figure 12 .
12
Figure 12.</p>
<p>Figure 11 :
11
Figure 11: Distribution of Tanimoto similarity 53 scores for FragLlama-designed PROTACs compared to three reference PROTACs: SP27 (red), ARV-471 (blue), and CP07 (green).</p>
<p>Figure 12 :
12
Figure 12: Examples of FragLlama-designed PROTACs incorporating rigid linkers, based on (A) SP27, (B) ARV-471, and (C) CP07 scaffolds.</p>
<p>Figure 13 :
13
Figure13: Chemical space distribution visualized using Uniform Manifold Approximation and Projection (UMAP), based on Tanimoto distances 53 calculated from Atom-pair fingerprints.52The plot compares three groups of molecules: 1) fine-tuned FragLlama designed molecules with both EGFR inhibitors and IC50 special tokens (red); 2) fine tuned FragLlamadesigned molecules with only EGFR inhibitors (yellow); 3) non-fine-tuned FragLlamadesigned molecules, alongside EGFR inhibitors from PubChem with active results in bioassays and valid IC50 values (blue).</p>
<p>Table 2 :
2
List of overlapping regions between fine-tuned FragLlama-designed molecules (trained with EGFR inhibitors and IC50 special tokens) and EGFR inhibitors from Pub-Chem.Each region includes 2D UMAP coordinates, scaffold structures, representative Pub-Chem compounds, and FragLlama-designed molecules.language model capabilities, improving the performance and applicability of language models in molecular design.By training tokens representing molecular fragments from scratch and leveraging a decoder-only architecture, FragLlama learns long-range dependencies that adhere to complex structural rules.The model demonstrates significant potential in drug discovery, by generating expert-level molecular designs and exploring new target-relevant chemical spaces.growth factor receptor 2-negative (HER2-) breast cancer: Phase 1b cohort (part C) of a phase 1/2 study.Journal of Clinical Oncology 2022, 40, TPS1120-TPS1120.(63) Wu, T. et al.Discovery of novel flavonoid-based CDK9 degraders for prostate cancer treatment via a PROTAC strategy.European Journal of Medicinal Chemistry 2023, 260, 115774.</p>
<p>AcknowledgementWe thank Bryan Barker and Ruzhu Chen at Oracle Cloud for their expertise in highperformance computing infrastructure and technical assistance, and Prof. Jin Wang at Baylor College of Medicine for valuable discussions on molecular design case studies.
A mathematical theory of communication. The Bell system technical journal. C E Shannon, 194827</p>
<p>Learning representations by backpropagating errors. D E Rumelhart, G E Hinton, R J Williams, 1986323</p>
<p>Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. K Fukushima, Biological cybernetics. 361980</p>
<p>Multilayer feedforward networks are universal approximators. K Hornik, M Stinchcombe, H White, Neural networks. 21989</p>
<p>Approximation by superpositions of a sigmoidal function. G Cybenko, 19892Mathematics of control, signals and systems</p>
<p>J Hoffmann, S Borgeaud, A Mensch, E Buchatskaya, T Cai, E Rutherford, D D L Casas, L A Hendricks, J Welbl, A Clark, arXiv:2203.15556others Training compute-optimal large language models. 2022arXiv preprint</p>
<p>On the computational power of neural nets. Proceedings of the fifth annual workshop on Computational learning theory. H T Siegelmann, E D Sontag, 1992</p>
<p>Mordatch, I. Frozen pretrained transformers as universal computation engines. K Lu, A Grover, P Abbeel, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence2022</p>
<p>Attention is all you need. A Vaswani, Advances in Neural Information Processing Systems. 2017</p>
<p>Sutskever, I.; others Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, OpenAI blog. 192019</p>
<p>Language models are few-shot learners. T B Brown, arXiv:2005.141652020arXiv preprint</p>
<p>An efficient active learning methodology applied to protein-specific molecular generation. G W Kyro, A Morgunov, R I Brent, V S Batista, Chemspaceal, Biophysical Journal. 1232832024</p>
<p>cMolGPT: A conditional generative pretrained transformer for target-specific de novo molecular generation. Y Wang, H Zhao, S Sciabola, W Wang, Molecules. 2844302023</p>
<p>Regression transformer enables concurrent sequence regression and generation for molecular language modelling. J Born, M Manica, Nature Machine Intelligence. 52023</p>
<p>MolGPT: molecular generation using a transformer-decoder model. V Bagal, R Aggarwal, P Vinod, U D Priyakumar, Journal of Chemical Information and Modeling. 622021</p>
<p>Gotta be SAFE: a new framework for molecular design. E Noutahi, C Gabellini, M Craig, J S Lim, P Tossou, Digital Discovery. 32024</p>
<p>. T Xie, Y Wan, W Huang, Z Yin, Y Liu, S Wang, Q Linghu, C Kit, C Grazian, W Zhang, I Razzak, B Hoex, Series, Domain Specific Large Language Models for Natural Science. 2023</p>
<p>A Review of Large Language Models and Autonomous Agents in Chemistry. M C Ramos, C J Collison, A D White, arXiv:2407.016032024arXiv preprint</p>
<p>NVIDIA Fireside Chat with Ilya Sutskever and Jensen Huang: AI Today and Vision of the Future. </p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, Journal of chemical information and computer sciences. 281988</p>
<p>DeepSMILES: an adaptation of SMILES for use in machinelearning of chemical structures. N O'boyle, A Dalke, 2018</p>
<p>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. M Krenn, F Hse, A Nigam, P Friederich, A Aspuru-Guzik, Machine Learning: Science and Technology. 1450242020</p>
<p>InChI, the IUPAC international chemical identifier. S R Heller, A Mcnaught, I Pletnev, S Stein, D Tchekhovskoi, Journal of cheminformatics. 72015</p>
<p>R Sennrich, arXiv:1508.07909Neural machine translation of rare words with subword units. 2015arXiv preprint</p>
<p>A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.21783others The llama 3 herd of models. 2024arXiv preprint</p>
<p>others Highly accurate protein structure prediction with AlphaFold. nature. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A dek, A Potapenko, 2021596</p>
<p>others Accurate structure prediction of biomolecular interactions with AlphaFold 3. J Abramson, J Adler, J Dunger, R Evans, T Green, A Pritzel, O Ronneberger, L Willmore, A J Ballard, J Bambrick, Nature. 2024</p>
<p>others Evolutionary-scale prediction of atomic-level protein structure with a language model. Z Lin, H Akin, R Rao, B Hie, Z Zhu, W Lu, N Smetanin, R Verkuil, O Kabeli, Y Shmueli, Science. 3792023</p>
<p>D Hernandez, J Kaplan, T Henighan, S Mccandlish, arXiv:2102.01293Scaling laws for transfer. 2021arXiv preprint</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, M Ahmed, Y Lu, S Pan, W Bo, Y Liu, Neurocomputing. 1270632024</p>
<p>N Shazeer, arXiv:2002.05202Glu variants improve transformer. 2020arXiv preprint</p>
<p>Training generalized multi-query transformer models from multi-head checkpoints. J Ainslie, J Lee-Thorp, M De Jong, Y Zemlyanskiy, F Lebrn, S Sanghai, Gqa, arXiv:2305.132452023arXiv preprint</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Alpaca, A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. </p>
<p>P Zhang, G Zeng, T Wang, W Lu, Tinyllama, arXiv:2401.02385An open-source small language model. 2024arXiv preprint</p>
<p>Efficient finetuning of quantized llms. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, Qlora, Advances in Neural Information Processing Systems. 202436</p>
<p>Flashattention-2: Faster attention with better parallelism and work partitioning. T Dao, arXiv:2307.086912023arXiv preprint</p>
<p>Deep reinforcement learning from human preferences. Advances in neural information processing systems. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, 201730</p>
<p>Beam search strategies for neural machine translation. M Freitag, Y Al-Onaizan, arXiv:1702.018062017arXiv preprint</p>
<p>O Vinyals, Q Le, arXiv:1506.05869A neural conversational model. 2015arXiv preprint</p>
<p>A Fan, M Lewis, Y Dauphin, arXiv:1805.04833Hierarchical neural story generation. 2018arXiv preprint</p>
<p>A Holtzman, J Buys, L Du, M Forbes, Y Choi, arXiv:1904.09751The curious case of neural text degeneration. 2019arXiv preprint</p>
<p>N S Keskar, B Mccann, L R Varshney, C Xiong, R Socher, Ctrl, arXiv:1909.05858A conditional transformer language model for controllable generation. 2019arXiv preprint</p>
<p>M K Armstrong, GLP-1R Modulating Compounds. US Patent US20240199589A1. Gilead Sciences, Inc2024</p>
<p>Small-Molecule Modulation of Protein Homeostasis. G M Burslem, C M Crews, Chemical Reviews. 1172017</p>
<p>others Catalytic in vivo protein knockdown by small-molecule PROTACs. D P Bondeson, A Mares, I E Smith, E Ko, S Campos, A H Miah, K E Mulholland, N Routly, D L Buckley, J L Gustafson, Nature chemical biology. 112015</p>
<p>The rise of molecular glues. S L Schreiber, Cell. 1842021</p>
<p>Identification of a Primary Target of Thalidomide Teratogenicity. T Ito, H Ando, T Suzuki, T Ogura, K Hotta, Y Imamura, Y Yamaguchi, H Handa, Science. 3272010</p>
<p>Lenalidomide Causes Selective Degradation of IKZF1 and IKZF3 in Multiple Myeloma Cells. J Krnke, Science. 3432014</p>
<p>The Myeloma Drug Lenalidomide Promotes the Cereblon-Dependent Destruction of Ikaros Proteins. G Lu, R E Middleton, H Sun, M Naniong, C J Ott, C S Mitsiades, K.-K Wong, J E Bradner, W G Kaelin, Science. 3432014</p>
<p>Development of targeted protein degradation therapeutics. P P Chamberlain, L G Hamann, Nature chemical biology. 152019</p>
<p>others A novel cereblon modulator recruits GSPT1 to the CRL4CRBN ubiquitin ligase. M E Matyskiela, G Lu, T Ito, B Pagarigan, C.-C Lu, K Miller, W Fang, N.-Y Wang, D Nguyen, J Houston, Nature. 5352016</p>
<p>Atom pairs as molecular features in structure-activity studies: definition and applications. R E Carhart, D H Smith, R Venkataraghavan, Journal of Chemical Information and Computer Sciences. 251985</p>
<p>Elementary mathematical theory of classification and prediction. T T Tanimoto, 1958</p>
<p>Uniform manifold approximation and projection for dimension reduction. L Mcinnes, J Healy, J Melville, Umap, arXiv:1802.034262018arXiv preprint</p>
<p>Structure-Activity Relationship of Potent, Selective, and Orally Bioavailable Molecular Glue Degraders of CK1. G Nishiguchi, ACS Medicinal Chemistry Letters. </p>
<p>Continuous evolution of compact protein degradation tags regulated by selective molecular glues. J A M Mercer, S J Decarlo, S S R Burman, V Sreekanth, A T Nelson, M Hunkeler, P J Chen, K A Donovan, P Kokkonda, P K Tiwari, V M Shoba, A Deb, A Choudhary, E S Fischer, D R Liu, Science. 44222024</p>
<p>Targeting the undruggable proteome: the small molecules of my dreams. C M Crews, Chemistry &amp; biology. 172010</p>
<p>Structural basis of PROTAC cooperative recognition for selective protein degradation. M S Gadd, A Testa, X Lucas, K.-H Chan, W Chen, D J Lamont, M Zengerle, A Ciulli, Nature chemical biology. 132017</p>
<p>Protac-Induced Protein Degradation in Drug Discovery: Breaking the Rules or Just Making New Ones. I Churcher, Journal of Medicinal Chemistry. 612018</p>
<p>Targeted intracellular protein degradation induced by a small molecule: En route to chemical proteomics. A R Schneekloth, M Pucheault, H S Tae, C M Crews, Bioorganic &amp; Medicinal Chemistry Letters. 182008</p>
<p>Discovery of the First Potent, Selective, and In Vivo Efficacious Pololike Kinase 4 Proteolysis Targeting Chimera Degrader for the Treatment of TRIM37-Amplified Breast Cancer. Y Sun, Journal of Medicinal Chemistry. 662023</p>
<p>ARV-471, an estrogen receptor (ER) PROTAC degrader. E P Hamilton, A F Schott, R Nanda, H Lu, C F Keung, R Gedrich, J Parameswaran, H S Han, S A Hurvitz, combined with palbociclib in advanced ER+/human epidermal</p>            </div>
        </div>

    </div>
</body>
</html>
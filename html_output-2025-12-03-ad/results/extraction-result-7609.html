<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7609 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7609</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7609</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-141.html">extraction-schema-141</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <p><strong>Paper ID:</strong> paper-268385471</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.08819v2.pdf" target="_blank">Thermometer: Towards Universal Calibration for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7609.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7609.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>THERMOMETER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>THERMOMETER (dataset-specific temperature recognition network)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary recognition network that predicts a dataset-specific temperature for post-hoc calibration of LLM predictive probabilities via an amortized variational inference objective; it uses LLM last-layer features and aggregates per-task evidence to produce a positive temperature scalar without task-specific labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to LLaMA-2-Chat 7B and FLAN-T5-XL in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Auxiliary MLP-like recognition network (multi-branch MLP) that maps LLM last-layer features ϕ(x) to a non-negative temperature ψ_θ(ϕ(x)); temperature for a task is the empirical average of ψ_θ over task inputs and is used to scale logits (temperature scaling) of the frozen LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>THERMOMETER is small auxiliary model; calibrated LLMs used: LLaMA-2-Chat 7B (7B params) and FLAN-T5-XL (≈3B params)</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Predict dataset-specific temperature for temperature scaling via an amortized variational recognition network (point-mass approximation of per-task Gaussian posterior over τ), i.e., learned temperature scaling without labeled data for the target task</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for question-answering predictions (next-token prediction mapped to multiple-choice or binary 'Yes/No' correctness for free-form QA)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Natural language question answering (benchmarks: MMLU, BIG-bench, MRQA) — general NLP, not forecasting scientific discoveries</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU (57 tasks), BIG-bench (23 selected multi-choice tasks), MRQA (6 out-of-domain development datasets for free-form QA evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Expected Calibration Error (ECE; 10 bins), Maximum Calibration Error (MCE), Negative Log Likelihood (NLL), Brier score, Top-Label ECE (TL-ECE)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>LLaMA-2-Chat 7B (MRQA average): THERMOMETER ECE = 0.065 ± 0.008 (Table 2) vs Vanilla 0.127 ± 0.026 and TS (lower-bound) 0.029 ± 0.005. LLaMA-2-Chat 7B (BIG-bench average): THERMOMETER ECE = 0.090 ± 0.018 (Table 3) vs Vanilla 0.233 ± 0.035 and TS 0.044 ± 0.014. FLAN-T5-XL (MMLU average): THERMOMETER ECE = 0.080 ± 0.008 (Table 4) vs Vanilla 0.181 ± 0.013 and TS 0.063 ± 0.006. (Reported metrics include MCE, NLL, Brier in same tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors report that THERMOMETER substantially improves calibration relative to the uncalibrated model (Vanilla) and most baselines, reducing ECE across datasets and tasks; THERMOMETER predictions correlate with temperatures learned with labeled data (TS) and transfer across tasks and model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Temperature Scaling (TS), Temperature Scaling with cross-validation (TS-CV), MC-Dropout, MC-Augment, Elicitation, Elicitation-Ensemble, CAPE, and other classical calibration methods (histogram binning, isotonic regression) mentioned</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not require labeled data but needs a modest number N* of unlabeled task inputs to estimate τ* accurately (Lemma 4.1); focus is on top-label (confidence) calibration rather than full multiclass probability calibration; method maps free-form outputs to binary correctness for calibration (via similarity metric like ROUGE-L), which is an approximation; only evaluated on QA benchmarks (no experiments forecasting real-world scientific discoveries); choice of prior and hyperparameters (λ_reg, batch sizes) can affect stability; THERMOMETER learns dataset-level temperatures and authors note sample-wise temperatures are suboptimal for their setting.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7609.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature Scaling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temperature Scaling (single scalar temperature post-hoc calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple post-hoc calibration method that rescales logits by a positive scalar temperature τ learned on held-out labeled data, preserving top predicted label while sharpening/softening output probabilities to improve calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to same LLMs as baseline (LLaMA-2-Chat 7B, FLAN-T5-XL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-parameter scaling of logits: p(y|x;τ) ∝ exp(logit(y)/τ), with τ > 0 fitted by minimizing NLL on labeled held-out data; does not change argmax predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Post-hoc scalar temperature scaling fitted with labeled development data (NLL minimization)</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for QA predictions (next-token prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP / question answering benchmarks in paper (MMLU, BIG-bench, MRQA)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Per-task held-out labeled splits of MMLU, BIG-bench, MRQA (TS used as a lower-bound since it uses labeled test-task data)</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Representative numbers: MRQA (LLaMA-2-Chat 7B) TS ECE = 0.029 ± 0.005 (Table 2). BIG-bench (LLaMA-2-Chat 7B) TS ECE = 0.044 ± 0.014 (Table 3). FLAN-T5-XL MMLU TS ECE = 0.063 ± 0.006 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>TS often produces the best calibration when labeled data for the target task are available (authors treat it as a lower-bound), but it requires labeled data and thus is not applicable to unlabeled new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to THERMOMETER, TS-CV, MC-Dropout, MC-Augment, Elicitation, CAPE, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires labeled held-out data from the target task to learn τ; not directly applicable to unlabeled test tasks; only scales probabilities (top-label calibration) and does not address full multiclass probability calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7609.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TS-CV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temperature Scaling with Cross-Validation (TS-CV)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant of temperature scaling where temperature is tuned using data from K−1 tasks (cross-task tuning) and applied to a held-out task, designed to mimic leave-one-out or transfer settings without target-task labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with LLaMA-2-Chat 7B and FLAN-T5-XL as a baseline</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Cross-validation style temperature scaling that uses data pooled from other tasks (not the target) to choose a temperature for the held-out task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Temperature scaling where τ is tuned across other tasks (no labeled data from the test task) — a transfer baseline</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for QA predictions</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Cross-task held-out splits of MMLU, BIG-bench, MRQA</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Example: MRQA (LLaMA-2-Chat 7B) TS-CV ECE = 0.071 ± 0.011 (Table 2). BIG-bench TS-CV ECE = 0.087 ± 0.020 (Table 3). FLAN-T5-XL MMLU TS-CV ECE = 0.093 ± 0.007 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>TS-CV provides reasonable transfer calibration but THERMOMETER outperforms TS-CV in many transfer scenarios and in limited labeled-data regimes according to the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared with THERMOMETER, TS, MC-Dropout, MC-Augment, Elicitation, CAPE</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on cross-task labeled data (K−1 tasks) and may not adapt to substantial dataset shifts; authors report THERMOMETER has stronger transferability.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7609.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-Dropout</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Dropout</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uncertainty estimation via multiple stochastic forward passes with dropout enabled at inference, averaging predictive distributions to approximate model uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied as a baseline for FLAN-T5-XL (dropout was non-functional for LLaMA-2-Chat 7B in authors' setup)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Enable dropout at inference and perform multiple forward passes, average probabilities to estimate uncertainty distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Stochastic forward passes with dropout and averaging (approximate Bayesian posterior via MC-Dropout)</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for QA</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench for FLAN-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>FLAN-T5-XL (MMLU) MC-Dropout ECE = 0.107 ± 0.015 (Table 4). MC-Dropout was not functional for LLaMA-2-Chat 7B in this paper's setup.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>MC-Dropout provided moderate calibration performance for FLAN-T5-XL but was not competitive with THERMOMETER overall.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared against TS, TS-CV, THERMOMETER, MC-Augment, CAPE, Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Multiple forward passes increase inference cost; authors found dropout inference to be non-functional for LLaMA-2-Chat 7B in their environment (no variability), limiting applicability.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7609.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MC-Augment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Uncertainty estimation via random data augmentations of prompts at inference time, averaging resulting predictive distributions to estimate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as a baseline with LLaMA-2-Chat 7B and FLAN-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Apply random augmentations to input prompts (e.g., paraphrases) at inference, run multiple forward passes, and average probabilities to estimate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Ensembled predictions over augmented input prompts (MC-style averaging)</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for QA</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench, MRQA</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>MRQA (LLaMA-2-Chat 7B) MC-Augment ECE = 0.335 ± 0.044 (Table 2). BIG-bench (LLaMA) MC-Augment ECE = 0.186 ± 0.031 (Table 3). FLAN-T5-XL MMLU MC-Augment ECE = 0.156 ± 0.015 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Often worse than THERMOMETER and TS in these experiments; can be computationally expensive due to multiple forward passes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to TS, TS-CV, THERMOMETER, MC-Dropout, CAPE, Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inference-time cost due to multiple augmented forward passes; quality depends on augmentation strategy; not competitive with THERMOMETER in reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7609.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicitation (verbalized confidence prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods that prompt an LLM to verbally produce a confidence estimate (e.g., 'I am 70% sure') using carefully crafted prompts; may be ensembled across multiple prompts (Elicitation-Ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied as baseline with LLaMA-2-Chat 7B (and other models in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting strategies that extract natural-language self-reported confidence from LLMs (elicited verbal probabilities), possibly aggregated across multiple sampled outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Verbalized confidence elicitation via prompts; optionally ensembles of multiple elicited responses</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Model's self-assessed correctness (verbal probability) of its generated answer</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench, MRQA (evaluated as baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, TL-ECE, NLL (where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>MRQA (LLaMA-2-Chat 7B) Elicitation ECE = 0.130 ± 0.031 (Table 2). Elicitation-Ensemble ECE = 0.171 ± 0.047 (Table 2). On BIG-bench elicitation methods often performed poorly for some models (Table 3 notes high variance).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Authors report elicitation methods can work but are brittle: depend on model's ability to follow nuanced prompts and may fail for some open-source or smaller models; not as reliable as THERMOMETER across evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to TS, TS-CV, THERMOMETER, MC- methods, CAPE</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on model's reliable generation of calibrated verbal probabilities (which can be weak for some models); can be fragile to prompt design; reported to be non-functional for some encoder-decoder models in authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7609.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAPE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CAPE (Prompt augmentation via option permutation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt augmentation method that generates multiple perturbed prompts (e.g., by permuting options) and aggregates outputs to improve calibration and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Calibrating language models via augmented prompt ensembles (CAPE)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used as a baseline with LLaMA-2-Chat 7B and FLAN-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt augmentation and ensembling technique that aggregates outputs across permutations/augmentations of prompt structure to reduce calibration error.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Ensemble/aggregation over augmented prompts</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Top-label confidence for multiple-choice QA</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench, MRQA</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>MRQA (LLaMA-2-Chat 7B) CAPE ECE = 0.067 ± 0.016 (Table 2). BIG-bench CAPE ECE = 0.214 ± 0.073 (Table 3). FLAN-T5-XL MMLU CAPE ECE = 0.167 ± 0.017 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>CAPE can improve calibration in some settings but is not consistently better than THERMOMETER across datasets and model types in authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Compared to TS, TS-CV, THERMOMETER, MC approaches, Elicitation</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inference-time cost due to multiple prompt augmentations; effectiveness depends on augmentation strategy; not universally superior.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7609.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-2-Chat 7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-2-Chat (7 billion parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned decoder-only LLM used in this paper as the primary model to be calibrated; produces probabilistic next-token distributions used as base forecasts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-2-Chat 7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer, instruction-tuned family (Touvron et al., 2023); outputs token-level logits used for next-token probability estimation; frozen during calibration experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈7B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Model-internal next-token softmax probabilities; subject to post-hoc calibration (TS, THERMOMETER, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Next-token completion mapped to multiple-choice letter tokens (A/B/C/D) or binary correctness for free-form QA</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks (MMLU, BIG-bench, MRQA)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench, MRQA</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>Vanilla (uncalibrated) ECE on MRQA = 0.127 ± 0.026 (Table 2); on BIG-bench Vanilla ECE = 0.233 ± 0.035 (Table 3); THERMOMETER reduces ECE to 0.065 (MRQA) and 0.090 (BIG-bench) as reported above.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Pretrained LLMs can be reasonably calibrated, but instruction-tuning/alignment interventions can harm calibration; uncalibrated (Vanilla) model often overconfident (high ECE) on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>Calibrated via THERMOMETER, TS, TS-CV, MC-Augment, Elicitation, CAPE</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Authors note dropout was non-functional for this model in their setup; calibration requires appropriate methods as instruction tuning can degrade calibration; no experiments forecasting real scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7609.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e7609.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models estimate probabilities for future real-world scientific discoveries, including model details, prediction targets, datasets, forecasting horizon, probability estimation methods, evaluation metrics, reported performance, calibration quality, baselines, limitations, and concrete probability examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5-XL (instruction-finetuned encoder-decoder, ~3B params)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder instruction-tuned LLM used to demonstrate THERMOMETER's robustness across model families; calibration baselines behave differently on this architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer (Chung et al., 2022) with instruction tuning; used to extract last-layer features and evaluate calibration methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈3B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>probability_estimation_method</strong></td>
                            <td>Decoder output token probabilities (softmax over vocabulary) with post-hoc calibration methods applied</td>
                        </tr>
                        <tr>
                            <td><strong>prediction_target</strong></td>
                            <td>Next-token completion mapped to multiple-choice or binary correctness for free-form QA</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>NLP QA benchmarks (MMLU, BIG-bench)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>MMLU, BIG-bench (selected tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>forecasting_horizon</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>ECE, MCE, NLL, Brier, TL-ECE</td>
                        </tr>
                        <tr>
                            <td><strong>reported_performance</strong></td>
                            <td>FLAN-T5-XL (MMLU average): Vanilla ECE = 0.181 ± 0.013, TS ECE = 0.063 ± 0.006, THERMOMETER ECE = 0.080 ± 0.008 (Table 4). FLAN-T5-XL (BIG-bench): THERMOMETER ECE = 0.078 ± 0.011 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>THERMOMETER improves calibration for FLAN-T5-XL and outperforms many baselines; elicitation-based methods were often non-functional on FLAN-T5-XL per the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_methods</strong></td>
                            <td>TS, TS-CV, MC-Dropout, MC-Augment, CAPE, THERMOMETER</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some elicitation methods fail on encoder-decoder models; as with other models, THERMOMETER requires unlabeled task inputs to estimate τ* and is evaluated only on QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>probability_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_world_future</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Thermometer: Towards Universal Calibration for Large Language Models', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>On calibration of modern neural networks <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>Sample-dependent adaptive temperature scaling for improved calibration <em>(Rating: 2)</em></li>
                <li>Calibrating language models via augmented prompt ensembles (CAPE) <em>(Rating: 2)</em></li>
                <li>Sample-dependent adaptive temperature scaling for improved calibration (Joy et al., 2023) <em>(Rating: 1)</em></li>
                <li>Are you using test log-likelihood correctly? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7609",
    "paper_id": "paper-268385471",
    "extraction_schema_id": "extraction-schema-141",
    "extracted_data": [
        {
            "name_short": "THERMOMETER",
            "name_full": "THERMOMETER (dataset-specific temperature recognition network)",
            "brief_description": "An auxiliary recognition network that predicts a dataset-specific temperature for post-hoc calibration of LLM predictive probabilities via an amortized variational inference objective; it uses LLM last-layer features and aggregates per-task evidence to produce a positive temperature scalar without task-specific labeled data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to LLaMA-2-Chat 7B and FLAN-T5-XL in experiments",
            "model_description": "Auxiliary MLP-like recognition network (multi-branch MLP) that maps LLM last-layer features ϕ(x) to a non-negative temperature ψ_θ(ϕ(x)); temperature for a task is the empirical average of ψ_θ over task inputs and is used to scale logits (temperature scaling) of the frozen LLM.",
            "model_size": "THERMOMETER is small auxiliary model; calibrated LLMs used: LLaMA-2-Chat 7B (7B params) and FLAN-T5-XL (≈3B params)",
            "probability_estimation_method": "Predict dataset-specific temperature for temperature scaling via an amortized variational recognition network (point-mass approximation of per-task Gaussian posterior over τ), i.e., learned temperature scaling without labeled data for the target task",
            "prediction_target": "Top-label confidence for question-answering predictions (next-token prediction mapped to multiple-choice or binary 'Yes/No' correctness for free-form QA)",
            "domain": "Natural language question answering (benchmarks: MMLU, BIG-bench, MRQA) — general NLP, not forecasting scientific discoveries",
            "dataset_used": "MMLU (57 tasks), BIG-bench (23 selected multi-choice tasks), MRQA (6 out-of-domain development datasets for free-form QA evaluation)",
            "forecasting_horizon": null,
            "evaluation_metric": "Expected Calibration Error (ECE; 10 bins), Maximum Calibration Error (MCE), Negative Log Likelihood (NLL), Brier score, Top-Label ECE (TL-ECE)",
            "reported_performance": "LLaMA-2-Chat 7B (MRQA average): THERMOMETER ECE = 0.065 ± 0.008 (Table 2) vs Vanilla 0.127 ± 0.026 and TS (lower-bound) 0.029 ± 0.005. LLaMA-2-Chat 7B (BIG-bench average): THERMOMETER ECE = 0.090 ± 0.018 (Table 3) vs Vanilla 0.233 ± 0.035 and TS 0.044 ± 0.014. FLAN-T5-XL (MMLU average): THERMOMETER ECE = 0.080 ± 0.008 (Table 4) vs Vanilla 0.181 ± 0.013 and TS 0.063 ± 0.006. (Reported metrics include MCE, NLL, Brier in same tables.)",
            "calibration_quality": "Authors report that THERMOMETER substantially improves calibration relative to the uncalibrated model (Vanilla) and most baselines, reducing ECE across datasets and tasks; THERMOMETER predictions correlate with temperatures learned with labeled data (TS) and transfer across tasks and model scales.",
            "baseline_methods": "Temperature Scaling (TS), Temperature Scaling with cross-validation (TS-CV), MC-Dropout, MC-Augment, Elicitation, Elicitation-Ensemble, CAPE, and other classical calibration methods (histogram binning, isotonic regression) mentioned",
            "limitations": "Does not require labeled data but needs a modest number N* of unlabeled task inputs to estimate τ* accurately (Lemma 4.1); focus is on top-label (confidence) calibration rather than full multiclass probability calibration; method maps free-form outputs to binary correctness for calibration (via similarity metric like ROUGE-L), which is an approximation; only evaluated on QA benchmarks (no experiments forecasting real-world scientific discoveries); choice of prior and hyperparameters (λ_reg, batch sizes) can affect stability; THERMOMETER learns dataset-level temperatures and authors note sample-wise temperatures are suboptimal for their setting.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.0",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Temperature Scaling (TS)",
            "name_full": "Temperature Scaling (single scalar temperature post-hoc calibration)",
            "brief_description": "A simple post-hoc calibration method that rescales logits by a positive scalar temperature τ learned on held-out labeled data, preserving top predicted label while sharpening/softening output probabilities to improve calibration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to same LLMs as baseline (LLaMA-2-Chat 7B, FLAN-T5-XL)",
            "model_description": "Single-parameter scaling of logits: p(y|x;τ) ∝ exp(logit(y)/τ), with τ &gt; 0 fitted by minimizing NLL on labeled held-out data; does not change argmax predictions.",
            "model_size": null,
            "probability_estimation_method": "Post-hoc scalar temperature scaling fitted with labeled development data (NLL minimization)",
            "prediction_target": "Top-label confidence for QA predictions (next-token prediction)",
            "domain": "NLP / question answering benchmarks in paper (MMLU, BIG-bench, MRQA)",
            "dataset_used": "Per-task held-out labeled splits of MMLU, BIG-bench, MRQA (TS used as a lower-bound since it uses labeled test-task data)",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "Representative numbers: MRQA (LLaMA-2-Chat 7B) TS ECE = 0.029 ± 0.005 (Table 2). BIG-bench (LLaMA-2-Chat 7B) TS ECE = 0.044 ± 0.014 (Table 3). FLAN-T5-XL MMLU TS ECE = 0.063 ± 0.006 (Table 4).",
            "calibration_quality": "TS often produces the best calibration when labeled data for the target task are available (authors treat it as a lower-bound), but it requires labeled data and thus is not applicable to unlabeled new tasks.",
            "baseline_methods": "Compared to THERMOMETER, TS-CV, MC-Dropout, MC-Augment, Elicitation, CAPE, etc.",
            "limitations": "Requires labeled held-out data from the target task to learn τ; not directly applicable to unlabeled test tasks; only scales probabilities (top-label calibration) and does not address full multiclass probability calibration.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.1",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "TS-CV",
            "name_full": "Temperature Scaling with Cross-Validation (TS-CV)",
            "brief_description": "Variant of temperature scaling where temperature is tuned using data from K−1 tasks (cross-task tuning) and applied to a held-out task, designed to mimic leave-one-out or transfer settings without target-task labeled data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used with LLaMA-2-Chat 7B and FLAN-T5-XL as a baseline",
            "model_description": "Cross-validation style temperature scaling that uses data pooled from other tasks (not the target) to choose a temperature for the held-out task.",
            "model_size": null,
            "probability_estimation_method": "Temperature scaling where τ is tuned across other tasks (no labeled data from the test task) — a transfer baseline",
            "prediction_target": "Top-label confidence for QA predictions",
            "domain": "NLP QA benchmarks",
            "dataset_used": "Cross-task held-out splits of MMLU, BIG-bench, MRQA",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "Example: MRQA (LLaMA-2-Chat 7B) TS-CV ECE = 0.071 ± 0.011 (Table 2). BIG-bench TS-CV ECE = 0.087 ± 0.020 (Table 3). FLAN-T5-XL MMLU TS-CV ECE = 0.093 ± 0.007 (Table 4).",
            "calibration_quality": "TS-CV provides reasonable transfer calibration but THERMOMETER outperforms TS-CV in many transfer scenarios and in limited labeled-data regimes according to the authors.",
            "baseline_methods": "Compared with THERMOMETER, TS, MC-Dropout, MC-Augment, Elicitation, CAPE",
            "limitations": "Relies on cross-task labeled data (K−1 tasks) and may not adapt to substantial dataset shifts; authors report THERMOMETER has stronger transferability.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.2",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MC-Dropout",
            "name_full": "Monte Carlo Dropout",
            "brief_description": "Uncertainty estimation via multiple stochastic forward passes with dropout enabled at inference, averaging predictive distributions to approximate model uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied as a baseline for FLAN-T5-XL (dropout was non-functional for LLaMA-2-Chat 7B in authors' setup)",
            "model_description": "Enable dropout at inference and perform multiple forward passes, average probabilities to estimate uncertainty distribution.",
            "model_size": null,
            "probability_estimation_method": "Stochastic forward passes with dropout and averaging (approximate Bayesian posterior via MC-Dropout)",
            "prediction_target": "Top-label confidence for QA",
            "domain": "NLP QA benchmarks",
            "dataset_used": "MMLU, BIG-bench for FLAN-T5-XL",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "FLAN-T5-XL (MMLU) MC-Dropout ECE = 0.107 ± 0.015 (Table 4). MC-Dropout was not functional for LLaMA-2-Chat 7B in this paper's setup.",
            "calibration_quality": "MC-Dropout provided moderate calibration performance for FLAN-T5-XL but was not competitive with THERMOMETER overall.",
            "baseline_methods": "Compared against TS, TS-CV, THERMOMETER, MC-Augment, CAPE, Elicitation",
            "limitations": "Multiple forward passes increase inference cost; authors found dropout inference to be non-functional for LLaMA-2-Chat 7B in their environment (no variability), limiting applicability.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.3",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "MC-Augment",
            "name_full": "Monte Carlo Augmentation",
            "brief_description": "Uncertainty estimation via random data augmentations of prompts at inference time, averaging resulting predictive distributions to estimate uncertainty.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Used as a baseline with LLaMA-2-Chat 7B and FLAN-T5-XL",
            "model_description": "Apply random augmentations to input prompts (e.g., paraphrases) at inference, run multiple forward passes, and average probabilities to estimate uncertainty.",
            "model_size": null,
            "probability_estimation_method": "Ensembled predictions over augmented input prompts (MC-style averaging)",
            "prediction_target": "Top-label confidence for QA",
            "domain": "NLP QA benchmarks",
            "dataset_used": "MMLU, BIG-bench, MRQA",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "MRQA (LLaMA-2-Chat 7B) MC-Augment ECE = 0.335 ± 0.044 (Table 2). BIG-bench (LLaMA) MC-Augment ECE = 0.186 ± 0.031 (Table 3). FLAN-T5-XL MMLU MC-Augment ECE = 0.156 ± 0.015 (Table 4).",
            "calibration_quality": "Often worse than THERMOMETER and TS in these experiments; can be computationally expensive due to multiple forward passes.",
            "baseline_methods": "Compared to TS, TS-CV, THERMOMETER, MC-Dropout, CAPE, Elicitation",
            "limitations": "Inference-time cost due to multiple augmented forward passes; quality depends on augmentation strategy; not competitive with THERMOMETER in reported results.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.4",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Elicitation",
            "name_full": "Elicitation (verbalized confidence prompts)",
            "brief_description": "Methods that prompt an LLM to verbally produce a confidence estimate (e.g., 'I am 70% sure') using carefully crafted prompts; may be ensembled across multiple prompts (Elicitation-Ensemble).",
            "citation_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "mention_or_use": "use",
            "model_name": "Applied as baseline with LLaMA-2-Chat 7B (and other models in literature)",
            "model_description": "Prompting strategies that extract natural-language self-reported confidence from LLMs (elicited verbal probabilities), possibly aggregated across multiple sampled outputs.",
            "model_size": null,
            "probability_estimation_method": "Verbalized confidence elicitation via prompts; optionally ensembles of multiple elicited responses",
            "prediction_target": "Model's self-assessed correctness (verbal probability) of its generated answer",
            "domain": "NLP QA benchmarks",
            "dataset_used": "MMLU, BIG-bench, MRQA (evaluated as baseline in this paper)",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, TL-ECE, NLL (where applicable)",
            "reported_performance": "MRQA (LLaMA-2-Chat 7B) Elicitation ECE = 0.130 ± 0.031 (Table 2). Elicitation-Ensemble ECE = 0.171 ± 0.047 (Table 2). On BIG-bench elicitation methods often performed poorly for some models (Table 3 notes high variance).",
            "calibration_quality": "Authors report elicitation methods can work but are brittle: depend on model's ability to follow nuanced prompts and may fail for some open-source or smaller models; not as reliable as THERMOMETER across evaluated models.",
            "baseline_methods": "Compared to TS, TS-CV, THERMOMETER, MC- methods, CAPE",
            "limitations": "Relies on model's reliable generation of calibrated verbal probabilities (which can be weak for some models); can be fragile to prompt design; reported to be non-functional for some encoder-decoder models in authors' experiments.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.5",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CAPE",
            "name_full": "CAPE (Prompt augmentation via option permutation)",
            "brief_description": "A prompt augmentation method that generates multiple perturbed prompts (e.g., by permuting options) and aggregates outputs to improve calibration and robustness.",
            "citation_title": "Calibrating language models via augmented prompt ensembles (CAPE)",
            "mention_or_use": "use",
            "model_name": "Used as a baseline with LLaMA-2-Chat 7B and FLAN-T5-XL",
            "model_description": "Prompt augmentation and ensembling technique that aggregates outputs across permutations/augmentations of prompt structure to reduce calibration error.",
            "model_size": null,
            "probability_estimation_method": "Ensemble/aggregation over augmented prompts",
            "prediction_target": "Top-label confidence for multiple-choice QA",
            "domain": "NLP QA benchmarks",
            "dataset_used": "MMLU, BIG-bench, MRQA",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "MRQA (LLaMA-2-Chat 7B) CAPE ECE = 0.067 ± 0.016 (Table 2). BIG-bench CAPE ECE = 0.214 ± 0.073 (Table 3). FLAN-T5-XL MMLU CAPE ECE = 0.167 ± 0.017 (Table 4).",
            "calibration_quality": "CAPE can improve calibration in some settings but is not consistently better than THERMOMETER across datasets and model types in authors' experiments.",
            "baseline_methods": "Compared to TS, TS-CV, THERMOMETER, MC approaches, Elicitation",
            "limitations": "Inference-time cost due to multiple prompt augmentations; effectiveness depends on augmentation strategy; not universally superior.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.6",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA-2-Chat 7B",
            "name_full": "LLaMA-2-Chat (7 billion parameters)",
            "brief_description": "An instruction-tuned decoder-only LLM used in this paper as the primary model to be calibrated; produces probabilistic next-token distributions used as base forecasts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-2-Chat 7B",
            "model_description": "Decoder-only transformer, instruction-tuned family (Touvron et al., 2023); outputs token-level logits used for next-token probability estimation; frozen during calibration experiments.",
            "model_size": "≈7B parameters",
            "probability_estimation_method": "Model-internal next-token softmax probabilities; subject to post-hoc calibration (TS, THERMOMETER, etc.)",
            "prediction_target": "Next-token completion mapped to multiple-choice letter tokens (A/B/C/D) or binary correctness for free-form QA",
            "domain": "NLP QA benchmarks (MMLU, BIG-bench, MRQA)",
            "dataset_used": "MMLU, BIG-bench, MRQA",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "Vanilla (uncalibrated) ECE on MRQA = 0.127 ± 0.026 (Table 2); on BIG-bench Vanilla ECE = 0.233 ± 0.035 (Table 3); THERMOMETER reduces ECE to 0.065 (MRQA) and 0.090 (BIG-bench) as reported above.",
            "calibration_quality": "Pretrained LLMs can be reasonably calibrated, but instruction-tuning/alignment interventions can harm calibration; uncalibrated (Vanilla) model often overconfident (high ECE) on evaluated tasks.",
            "baseline_methods": "Calibrated via THERMOMETER, TS, TS-CV, MC-Augment, Elicitation, CAPE",
            "limitations": "Authors note dropout was non-functional for this model in their setup; calibration requires appropriate methods as instruction tuning can degrade calibration; no experiments forecasting real scientific discoveries.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.7",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "FLAN-T5-XL",
            "name_full": "FLAN-T5-XL (instruction-finetuned encoder-decoder, ~3B params)",
            "brief_description": "An encoder-decoder instruction-tuned LLM used to demonstrate THERMOMETER's robustness across model families; calibration baselines behave differently on this architecture.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-XL",
            "model_description": "Encoder-decoder transformer (Chung et al., 2022) with instruction tuning; used to extract last-layer features and evaluate calibration methods.",
            "model_size": "≈3B parameters",
            "probability_estimation_method": "Decoder output token probabilities (softmax over vocabulary) with post-hoc calibration methods applied",
            "prediction_target": "Next-token completion mapped to multiple-choice or binary correctness for free-form QA",
            "domain": "NLP QA benchmarks (MMLU, BIG-bench)",
            "dataset_used": "MMLU, BIG-bench (selected tasks)",
            "forecasting_horizon": null,
            "evaluation_metric": "ECE, MCE, NLL, Brier, TL-ECE",
            "reported_performance": "FLAN-T5-XL (MMLU average): Vanilla ECE = 0.181 ± 0.013, TS ECE = 0.063 ± 0.006, THERMOMETER ECE = 0.080 ± 0.008 (Table 4). FLAN-T5-XL (BIG-bench): THERMOMETER ECE = 0.078 ± 0.011 (Table 5).",
            "calibration_quality": "THERMOMETER improves calibration for FLAN-T5-XL and outperforms many baselines; elicitation-based methods were often non-functional on FLAN-T5-XL per the authors' experiments.",
            "baseline_methods": "TS, TS-CV, MC-Dropout, MC-Augment, CAPE, THERMOMETER",
            "limitations": "Some elicitation methods fail on encoder-decoder models; as with other models, THERMOMETER requires unlabeled task inputs to estimate τ* and is evaluated only on QA tasks.",
            "probability_examples": null,
            "real_world_future": false,
            "uuid": "e7609.8",
            "source_info": {
                "paper_title": "Thermometer: Towards Universal Calibration for Large Language Models",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "On calibration of modern neural networks",
            "rating": 2,
            "sanitized_title": "on_calibration_of_modern_neural_networks"
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2,
            "sanitized_title": "just_ask_for_calibration_strategies_for_eliciting_calibrated_confidence_scores_from_language_models_finetuned_with_human_feedback"
        },
        {
            "paper_title": "Sample-dependent adaptive temperature scaling for improved calibration",
            "rating": 2,
            "sanitized_title": "sampledependent_adaptive_temperature_scaling_for_improved_calibration"
        },
        {
            "paper_title": "Calibrating language models via augmented prompt ensembles (CAPE)",
            "rating": 2,
            "sanitized_title": "calibrating_language_models_via_augmented_prompt_ensembles_cape"
        },
        {
            "paper_title": "Sample-dependent adaptive temperature scaling for improved calibration (Joy et al., 2023)",
            "rating": 1,
            "sanitized_title": "sampledependent_adaptive_temperature_scaling_for_improved_calibration_joy_et_al_2023"
        },
        {
            "paper_title": "Are you using test log-likelihood correctly?",
            "rating": 1,
            "sanitized_title": "are_you_using_test_loglikelihood_correctly"
        }
    ],
    "cost": 0.01904,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Thermometer: Towards Universal Calibration for Large Language Models
27 Jun 2024</p>
<p>Maohao Shen 
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
CambridgeUSA</p>
<p>Subhro Das 
MIT-IBM Watson AI Lab
IBM Research</p>
<p>Kristjan Greenewald 
MIT-IBM Watson AI Lab
IBM Research</p>
<p>Prasanna Sattigeri 
MIT-IBM Watson AI Lab
IBM Research</p>
<p>Gregory Wornell 
Department of Electrical Engineering and Computer Science
Massachusetts Institute of Technology
CambridgeUSA</p>
<p>Soumya Ghosh <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#103;&#104;&#111;&#115;&#104;&#115;&#111;&#64;&#117;&#115;&#46;&#105;&#98;&#109;&#46;&#99;&#111;&#109;">&#103;&#104;&#111;&#115;&#104;&#115;&#111;&#64;&#117;&#115;&#46;&#105;&#98;&#109;&#46;&#99;&#111;&#109;</a>. 
MIT-IBM Watson AI Lab
IBM Research</p>
<p>Thermometer: Towards Universal Calibration for Large Language Models
27 Jun 202429550269790C6B0D1EB3EC552CF7598CarXiv:2403.08819v2[cs.LG]
We consider the issue of calibration in large language models (LLM).Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs.Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging.These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks.Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs.THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM.It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks.Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method 1 .</p>
<p>Introduction</p>
<p>Well-calibrated forecasts are a desirable property of any probabilistic forecaster.They ensure that probabilities produced by the forecaster can be interpreted as accurate confidence estimates of the forecasts.Informally, this implies that the forecaster is more often wrong on predictions made with low probabilities than those made with high probabilities.Such forecasts are useful for both enabling trust in the forecaster's predictions and incorporating the forecasts as part of a larger autonomous or semi-autonomous system.</p>
<p>Large language models (LLMs) (Brown et al., 2020;Raffel et al., 2020;Touvron et al., 2023) define probability Different methods for calibrating LLaMA-2-Chat 7B compared on the PROFESSIONAL LAW task of MMLU on a A100, 40 GB GPU.The task contains 1533 questions.Our method, THERMOMETER is significantly faster than methods that require multiple forward passes (Wei &amp; Zou, 2019;Xiong et al., 2023;Jiang et al., 2023a) at inference time and achieves lower calibration error compared to methods with comparable runtime (Tian et al., 2023).Vanilla refers to the no-calibration baseline, and TS-CV is a temperature scaling variant (Section 5).Similar trends hold for other benchmarks.</p>
<p>distributions over sequences of tokens and produce probabilistic forecasts over future tokens in a sequence2 .Their ability to synthesize knowledge from large volumes of data, represented as sequences of tokens, has led to remarkable performance on diverse tasks, including question answering (Hendrycks et al., 2020), commonsense reasoning (Zhong et al., 2019), and machine translation (Zhu et al., 2020) among others.However, much like other probabilistic forecasters, before deploying LLMs in critical applications, it is important that they are well-calibrated in addition to being accurate.Unfortunately, a growing body of evidence suggests that while pre-trained LLMs are often well-calibrated, alignment interventions such as instruction tuning which make the pre-trained LLMs more usable, also harm calibration (OpenAI, 2023;Zhu et al., 2023).</p>
<p>While calibration has long been studied (Brier, 1950;Dawid, 1982;Gneiting et al., 2007) and different approaches for improving calibration properties of probabilistic forecasters exist (Platt et al., 1999;Lakshminarayanan et al., 2017;Gal &amp; Ghahramani, 2016;Guo et al., 2017), LLMs pose unique challenges.Training a LLM is expensive, and even inference typically incurs non-negligible expenses.This makes any calibration approach that requires multiple training runs prohibitively expensive.Even approaches that only require multiple inferences at test time can be unreasonably expensive for certain applications.Moreover, owing to their versatility, instruction-tuned LLMs are often applied, without further adaptation, to a diverse array of tasks.It is essential that methods for calibrating them do not affect the accuracy of the uncalibrated LLMs and that the calibration methods themselves can adapt to new tasks.Finally, measuring and alleviating calibration is challenging in cases where the LLM is required to generate free-form text.The equivalence class defined by the LLM-generated sequences that map to the same semantic content is large, making it challenging to assign meaningful confidence to the generation or even robustly assess the quality of the generation.</p>
<p>We present THERMOMETER for calibrating LLMs while alleviating the above challenges.THERMOMETER learns, from multiple tasks, a parameterized mapping to map the outputs of the LLM to better-calibrated probabilities.Our approach is (i) computationally efficient: we do not require multiple training runs, and at inference time, we are only ∼ 0.5% slower than the uncalibrated LLM, (ii) accuracy preserving: we build on temperature scaling (Guo et al., 2017) which provably guarantees that greedy-decoded predictions do not change after our calibration procedure, and (iii) takes a step towards being universal: once trained we do not require retraining when exposed to a similar but new task.Moreover, similarly to recent work (Kadavath et al., 2022;Lin et al., 2022), we circumvent the challenges posed by free-form text generation by mapping the free-form text generation task to a next-token prediction task.We empirically evaluate THERMOMETER on diverse benchmarks and models and find it consistently produce better-calibrated uncertainties than competing methods at a fraction of the computational cost.Moreover, we find THERMOMETER transfers across datasets and model scales.THERMOMETER trained for calibrating a smaller model (e.g., LLaMA-2-Chat 7B) also improves calibration of larger models (e.g., LLaMA-2-Chat 70B) from the same family of models.</p>
<p>Related Work</p>
<p>A variety of methods exist that aim to produce bettercalibrated uncertainties.Post-hoc calibration methods learn to map the outputs of a pre-trained model to wellcalibrated uncertainties.These include histogram binning (Zadrozny &amp; Elkan, 2001;Naeini et al., 2015), isotonic regression (Zadrozny &amp; Elkan, 2002), and approaches that assume parametric maps, including matrix, vector, and temperature scaling (Platt et al., 1999;Guo et al., 2017) as well as more ambitious variants (Kull et al., 2019) that aim to calibrate, in multi-class prediction problems, all class probabilities rather than just the probability of the most likely class (confidence calibration).The next-token prediction task is also a multi-class prediction problem, albeit one with a large number of classes.Here, the number of classes equals the number of tokens in the LLM's vocabulary.This high dimensionality motivates us to focus on the more modest goal of confidence calibration.</p>
<p>We learn an auxiliary model that, given an unlabeled dataset, predicts a dataset-specific temperature, allowing us to calibrate an LLM's uncertainties on a previously unseen task.Others (Joy et al., 2023;Yu et al., 2022) have also considered parameterized temperature maps but with a focus on more traditional vision models.Joy et al. (2023) predict per-data instance temperatures, do not learn from multiple datasets, and find it necessary to learn a variational auto-encoder for representation learning jointly.In contrast, we leverage data from multiple tasks to generalize to new tasks, find no need for additional representation learning, and empirically find dataset-specific temperatures to better calibrate LLMs than data-instance-specific temperatures.Yu et al. (2022) are motivated by making calibration robust to distribution shifts.Their approach involves a two-step procedure: independently learning dataset-specific temperatures for each dataset and then fitting a linear regression to the learned temperatures.In contrast, we jointly learn a non-linear auxiliary model across multiple datasets by simply maximizing a lower bound to the likelihood.Moreover, empirically, we find that THERMOMETER outperforms linear counterparts.</p>
<p>Other ab initio approaches involve training with labelsmoothing (Szegedy et al., 2016), mix-up augmentations (Zhang et al., 2017), confidence penalties (Pereyra et al., 2017), focal loss (Mukhoti et al., 2020), or approximate Bayesian procedures (Izmailov et al., 2021).The substantial changes to the training process required by these approaches make them difficult to use with LLMs.Yet other approaches include ensembling over multiple models arrived at by retraining from different random initializations, for example, deep ensembles (Lakshminarayanan et al., 2017), or from perturbations of a single model, for example, Monte-Carlo dropout (Gal &amp; Ghahramani, 2016).Training multiple LLM variants is prohibitively expensive, and while perturbations around a single model are possible, we find them to be not competitive with THERMOMETER.Jiang et al. (2021); Xiao et al. (2022); Chen et al. (2022) empirically evaluate calibration of LLMs, find evidence of miscalibration, and evaluate existing calibration interventions and combinations thereof with varying degrees of success.Park &amp; Caragea (2022) use mixup while Desai &amp; Durrett (2020) find temperature scaling and label smoothing effective for calibrating smaller encoder-only models.Others (Lin et al., 2022) employ supervised fine-tuning to produce verbalized uncertainties to be better calibrated on certain tasks.Such fine-tuning, however, is computeintensive.An alternate body of work (Zhang et al., 2021;Kadavath et al., 2022;Mielke et al., 2022) approach calibration indirectly by learning an auxiliary model for predicting whether a generation is incorrect.There is also a body of work (Abbas et al., 2024;Han et al., 2023;Jiang et al., 2023b;Zhao et al., 2021;Zhou et al., 2024) that use the term calibration to mean de-biasing the predictions of a language model to biases introduced by the choice and ordering of in-context examples.This is distinct from our notion of statistical calibration.Yet others (Tian et al., 2023;Xiong et al., 2023), similar to our approach, consider RLHF-tuned LLMs and find that they can express better-calibrated uncertainties with carefully crafted prompts.Our work is orthogonal, extending temperature scaling to tasks without labeled data, and can be combined with their approach.</p>
<p>Background</p>
<p>Setup and Notation We consider question answering tasks, both involving multiple-choice and free form answers.We pose this as a next token prediction problem by concatenating the question, any available context, and the multiplechoice options into a single prompt.In Section 4.3, we describe our treatment of free form answers within this setup.See Appendix C.1 for example prompts.Given the prompt, the next token predicted by the LLM is considered the answer to the question.We denote the n th prompt in a dataset by x n and the corresponding completion by y n .We assume that both the prompt and the completion have been tokenized, and the prompt x n = x n,tn , . . ., x n,2 , x n,1 is a sequence of t n tokens.The tokens x n,t and y n take one of V values.We use D = {(x n , y n )} N n=1 to denote a training set of prompt, completion pairs.We model
p(y n | x n ; M) = exp(w T yn ϕ(x n ; W)) V v ′ =1 exp(w T v ′ ϕ(x n ; W))
, using a large language model, M, parameterized by weights, {W, w 1 , . . ., w V }.We use p(
y n | x n ; M) as a nota- tionally convenient stand in for p(Y = y n |X = x n ; M),
where Y and X are random variables corresponding to the completion and the prompt.We also note that p(Y | X = x n ; M) ∈ ∆ V where ∆ V is a V -dimensional probability simplex.We view ϕ(x n , W) as a feature extractor that transforms the input prompt x n into a Ddimensional representation.For notational brevity, we will suppress the dependence on W and use ϕ(x n , W) and ϕ(x n ) interchangeably.</p>
<p>Confidence calibration</p>
<p>We aim to confidence calibrate M, such that among the completions whose top probability is β the accuracy is also β,
Pr(Y = Ŷ | P = β) = β, for all β ∈ [0, 1],
where Ŷ = argmax p(Y |X;M) and P = max p(Y |X; M).</p>
<p>One common notion of miscalibration is the expected calibration error (ECE) (Naeini et al., 2015),
E Pr(Y = Ŷ | P = β) − β .ECE = M m=1 |B m | N acc(B m ) − conf(B m ) .
Temperature Scaling A large ECE score indicates the need for calibration as the model's prediction is poorly calibrated on the given dataset.Temperature scaling (Guo et al., 2017;Platt et al., 1999) is a widely used post-hoc calibration technique that introduces a single positive, scalar parameter τ ∈ R + , the temperature, and defines,
p(y n | x n ; τ, M) = exp(w T yn ϕ(x n ; W)/τ ) V v ′ =1 exp(w T v ′ ϕ(x n ; W)/τ )
.</p>
<p>The parameter τ is typically learned by minimizing the negative log likelihood (NLL) on a labeled held-out dataset, τ = arg min τ ∈R+ − N * n=1 log p(y n | x n ; τ, M), freezing all other model parameters.Since temperature scaling only scales probabilities, it does not change the class with the maximum probability and preserves accuracy.Recent work (Chen et al., 2022;Xiao et al., 2022;Tian et al., 2023) has found temperature scaling effective at calibrating LLMs.Besides, it does not require additional tuning of the LLM or multiple forward passes at inference time.However, the requirement of a held-out labeled dataset can be severely limiting for LLMs.LLMs are often used on new tasks, where no labeled data is available!To alleviate this challenge, we propose to bypass dataset specific optimization and instead learn to predict the optimal temperature for a previously unseen and unlabeled dataset.</p>
<p>THERMOMETER</p>
<p>Consider a multi-task setting.Given K tasks with labeled datasets
{D k } K k=1 = {{ x k n , y k n } N k n=1 } K k=1
, our goal is to learn to infer task-specific temperature parameters, in order to accurately infer the temperature for an unseen task given only N * prompts, D * = {x * n } N * n=1 but not the corresponding completions.To this end, we propose a probabilistic model with task-specific latent variables, τ k , for modeling the conditional distribution of completions given a prompt,
p(D 1 , . . . D K | ν 0 ; M) = K k=1 p(τ k | ν 0 ) N k n=1 p(y k n | x k n , τ k ; M)dτ k ,(1)
where p(y
k n | x k n , τ k ; M) = exp(w T y k n ϕ(x k n ;W)/τ k ) V v ′ =1 exp(w T v ′ ϕ(x k n ;W)/τ k ) , p(τ k | ν 0
) is a prior on τ k for regularizing it away from zero and large positive values, and ν 0 is the prior's hyperparameters.</p>
<p>Our key idea is to use a recognition network (Dayan et al., 1995), to infer the per-task latent temperatures.We share the recognition network across all tasks, allowing us to amortize computation across tasks and sidestep the need for task specific optimization for inferring τ k .Crucially, we design our recognition network to condition only on ϕ(x k n ) and not on the completions y k n , allowing us to infer the temperature for unlabeled data D * at test time.We dub our recognition network, THERMOMETER, which, much like a real THERMOMETER, is designed to estimate temperature, albeit in the context of a dataset.We work within the framework of variational inference, which provides us with a coherent objective to optimize for learning the recognition network.We begin by forming a variational lower bound of the logarithm of the marginal likelihood, p(D 1 , . . .</p>
<p>A Variational Lower-bound
D K | ν 0 ; M), L(θ) = K k=1 E q(τ k ;θ) [log p (D k | τ k ; M)] − KL (q(τ k ; θ) || p(τ k | ν 0 )) ,(2)
where q(τ k ; θ) is a variational approximation to the posterior distribution p(τ k | D 1 , . . ., D k ).We choose a product of Gaussian PDF for the variational approximation.Each corresponding Gaussian variable has a fixed variance ϵ, and a mean parameterized by a recognition network (for example, a MLP), ψ θ : R D → R + , where θ ∈ R P represents the parameters of the recognition network that are shared across D 1 , . . ., D k .The resulting variational approximation is,
q(τ k ; θ) ∝ N k n=1 N (τ k | ψ θ (ϕ(x k n )), ϵ), N τ k | 1 N k N k n=1 ψ θ (ϕ(x k n )), ϵ N k ,(3)
where the last equality follows from standard properties of Gaussian distributions.We provide a proof in Appendix B.2.This particular choice of the variational parameterization affords us several benefits.First, it allows us to accumulate evidence (Bouchacourt et al., 2018)  Training Objective Plugging in the variational approximation Equation (3) in Equation ( 2), we have,
L(θ) = K k=1 E q(τ k ;θ) [log p (D k |τ k ; M)+log p(τ k |ν 0 )]+C,
(4) where C3 is a constant independent of θ.By further observing that when ϵ is small, for example, 10 −2 , for even moderate N k the variational approximation q(τ k | θ) is reasonably approximated by a point mass distribution 2).This leads to the objective for training THERMOMETER,
δ(τ k − 1 N k N k n=1 ψ θ (ϕ(x k n ))) (FigureL(θ) = K k=1 log p D k 1 N k N k n=1 ψ θ (ϕ(x k n )); M −λ reg log p 1 N k N k n=1 ψ θ (ϕ(x k n )) ν 0 ,(5)
where we have dropped the constant, plugged in the point mass approximation in Equation ( 4), and introduced a hyperparameter λ reg for balancing the prior and the likelihood terms.We place a Gamma prior, with a mode at one, on the temperatures
τ k , p(τ k | ν 0 ) = Gamma(τ k | α 0 = 1.25, β 0 = 4) 4 .
This prior enforces non-negative tem-peratures, and although the prior's mode at one encodes our belief that M is already reasonably calibrated, the high prior variance emphasizes that this is only a weakly held belief.Finally, we train THERMOMETER by minimizing, θ * = arg min θ∈R P − L(θ), or equivalently maximizing an approximate 5 lower bound to the log-marginal likelihood.We use stochastic optimization with mini-batched gradients to optimize L(θ).Algorithm 1 summarizes our learning procedure.The temperature estimation step in the algorithm involves averaging over the minibatch which introduces a small bias in the minibatch gradients.However, this bias decreases linearly with increasing batch size (Lemma 4.1), and disappears in the full-batch limit.Empirically, we find performance to be largely invariant to batch size (Table 8).
D dev = {D dev k } K k=1 , D val = {D val k } K k=1 ▷ Initialize THERMOMETER, θ 0 ← θ init ; Denote the number of mini-batches by B = ⌈ N Nb ⌉, where N = |D dev |. for m = 1, 2, . . . , M do ▷ Sample, uniformly at random, a task k and its dataset D dev k . ▷ Sample a batch of N b samples from D dev k , i.e., { x k n , y k n } Nb n=1 . ▷ Estimate temperature τk = 1 Nb Nb n=1 ψ θ (ϕ(x k n )). ▷ Evaluate the loss w.r.t batch of samples, i.e., − L(θ)N b = − N b n=1 log p(y k n | x k n , τk; M) − λreg B • log p(τk | ν0)
▷ Update the THERMOMETER parameter using AdamW, i.e., θm ←
AdamW(θm−1, γ, −∇θ L(θ)b). if m &gt; burnin then ▷ Checkpoint θm every m ′ iterations. end end ▷ Evaluate L(θ) on D val for each check-pointed θ and set θ * to the check-pointed θ with lowest − L(θ).
output Optimized THERMOMETER, θ * .</p>
<p>Test Time Procedure</p>
<p>Given a new test task, we assume we have a collection of N * inputs {x * n } N * i=1 with which we compute the temperature to be used for the task,
τ * = 1 N * N * n=1 ψ θ * (ϕ(x * n )).(6)
5 the approximation is increasingly accurate with larger N k .</p>
<p>Impact of test data size At test time not all the data may be immediately available when computing the temperature τ * (e.g.deploying on real-world tasks) and N * in (6).As the empirical average used to compute τ * is a proxy for
E x∼P * [ψ θ (ϕ(x))]
, where P * is the unknown data generating process for the test task, it is important that N * not be too small or this approximation will not be accurate and performance may suffer.We can control this error, however.Under an assumption that for fixed parameters, θ, 0 ≤ ψ θ (ϕ(x)) ≤ C θ for some C θ (e.g. because the output of the THERMOMETER model must be bounded if the input is), we can apply a Bernstein inequality to obtain: Lemma 4.1.Let X be the support set of the distribution of x, and assume we have 6 Further assume that we are interested in measuring calibration error using a metric CE (for example, ECE) that as a function of temperature, CE(τ ) is L-Lipschitz.Then with probability at least 1
N * i.i.d. samples {x n } N * n=1 from P * . Assume that for fixed parameters θ, sup x∈X ψ θ (ϕ(x)) ≤ C θ for some C θ , and Var[ψ θ (ϕ(x n ))] ≤ V θ .− 2 N * 2 , E 1 N * N * n=1 ψ θ (ϕ(x n )) − E x∼P * ψ θ (ϕ(x)) ≤ 4 3 C θ log N * N * + (2V θ ) 1 2 log N * N * , and, CE 1 N * n n=1 ψ θ (ϕ(x n )) − CE E x∼P * ψ θ (ϕ(x)) ≤ L 4 3 C θ log N * N * + (2V θ ) 1 2 log N * N * .
The proof is in Appendix B.1.This result guarantees that these partial sums will converge quickly as N * increases, and that any resulting loss of CE will be small.We confirm this guarantee experimentally in Appendix A.4.Our bound can be checked by practitioners as a guide to choosing N * , trading off accuracy and the need for more examples.In practice, both the Lipschitz constant and V θ can be easily estimated.Specifically, the former is simple since the temperature is a one-dimensional last-layer parameter of an otherwise pretrained model, and the latter can be bounded by using an empirical estimate of the variance.7</p>
<p>Question Answering with Free Form Answers</p>
<p>As we introduced in Section 3, given a sequence of tokens as prompts x n , we aim to calibrate the LLM's prediction on its corresponding completion token y n .In the context of a multiple-choice QA problem, y n typically represents a single token corresponding to one of the multiple-choice options, such as 'A' or 'B'.However, in practice, LLMs are more commonly tasked with generating free-form responses for arbitrary questions without predefined answer options.</p>
<p>We address this issue by converting the free-form QA problem into a multiple-choice format, by posing a 'Yes' or 'No' question to an LLM, inquiring whether its generated response is correct or incorrect.Given a training set
D = {(x n , y n )} N n=1
, where x n represents the question context, and y n denotes the true answers, we generate a new prompt xn by concatenating the original question x n with LLM's generated response z n , i.e., xn = [x n , z n ], and construct a new target ỹn by comparing generated response z n and true answer y n , i.e., ỹn is assigned 'Yes' if z n closely approximates y n based on a predefined similarity metric (for example, the Rouge-L score).The resulting calibration problem can be tackled in the same manner as multiple-choice QA, i.e., calibrate LLM's prediction of single completion token ỹn ('Yes' or 'No') given the prompts xn .</p>
<p>Experiments</p>
<p>We describe the experimental setup here, and present the main experimental results in Section 5.1, and conduct a comprehensive analysis in Section 5.2.Additional details and results are in Appendix A and Appendix C.</p>
<p>Benchmark Datasets</p>
<p>We employ two widely used benchmark datasets for multiple-choice question-and-answer (QA) experiments: MMLU (Hendrycks et al., 2020) and BIG-bench (Srivastava et al., 2022), and adopt MRQA (Fisch et al., 2019) for experiments on QA with free form answers. MMLU contains datasets of exam questions from fifty seven subjects spanning various fields, each question comprises a question context and four possible answers.BIG-bench is a massive benchmark containing more than two hundred datasets covering a wide range of NLP tasks.We extracted multiple-choice QA datasets with over thousand training instances, which resulted in twenty three datasets.The MRQA benchmark comprises of a predefined train and development split, with six training datasets and six development datasets.The style of questions vary greatly both within and across the training and development sets.More details are in Appendix C.2.</p>
<p>Models</p>
<p>We aim to enhance the calibration performance of instruction tuned language models.As decoder-only models become more ubiquitous in practice, our main experiments are conducted using decoder-only model LLaMA-2-Chat 7B with seven billion parameters (Touvron et al., 2023).To demonstrate the robust calibration performance of our proposed THERMOMETER with respect to different types of models, we also conduct a set of experiments using encoderdecoder model FLAN-T5-XL with three billion parameters (Chung et al., 2022).THERMOMETER is an auxiliary model with a Multi-Layer Perceptron (MLP)-like structure described in Appendix C.3.</p>
<p>Evaluation Metrics</p>
<p>We evaluate the calibration performance using following metrics: (1) Expected Calibration Error (ECE): measures the average error between prediction confidence and accuracy across different confidence intervals, for which we use 10 bins in our evaluation.(2) Maximum Calibration Error (MCE) (Naeini et al., 2015): identifies the largest error between prediction confidence and accuracy among all confidence bins, represents the worst-case calibration scenario.(3) Negative log likelihood (NLL): a proper scoring rule (Gneiting &amp; Raftery, 2007) that measures closeness to the data generating process in the Kullback-Leibler sense (Deshpande et al., 2024).(4) Brier Score (Brier) (Brier, 1950): another proper scoring rule that measures the mean squared error between prediction confidence and one-hot labels.(5) Top Label ECE (TL-ECE) (Gupta &amp; Ramdas, 2021): measures the average error conditioned on both confidence and top-predicted labels.TL-ECE also serves as an upper bound of ECE, and is an alternate measure of calibration error.</p>
<p>Implementation Details For multiple-choice QA tasks, we employ a leave-one-out approach to assess the THERMOMETER's calibration on unseen tasks.We train the model using K − 1 datasets, and test the trained model on the remaining single testing task, repeating this process for all the K datasets.For the QA tasks with free form answers, we use the established train and dev splits.We train THERMOMETER on MRQA's train split, and evaluate on the six held-out development datasets.In all experiments, we report results averaged over five random trials.See Appendix C.4 for details.</p>
<p>Baselines Our goal of calibrating models on tasks with no labeled data rules out many calibration approaches.Nonetheless, we compare THERMOMETER against several classical</p>
<p>Main Results</p>
<p>Multiple-choice QA Our main calibration results for multiple-choice QA tasks are presented in two formats.We offer an overview of the overall calibration performance by calculating average results across all datasets.This includes 57 tasks from MMLU and 23 from BIG-bench.The average results and 95% confidence intervals for MMLU are presented in Table 1.We provide a finer granularity view through scatter plots illustrating performance on each task in Figure 3.In this plots, points above the diagonal are instances where the calibration method fails to improve over the uncalibrated model, where THERMOMETER shows zero failure cases.Furthermore, we compare the predicted temperatures of THERMOMETER, which uses no labeled data from the test task, with temperatures learned via temperature scaling that uses all the labeled data from the task in Figure 4. We find THERMOMETER's predictions adapt to different tasks and align with learned temperatures.</p>
<p>The diverse nature of the BIG-bench tasks makes learning THERMOMETER more challenging.Nonetheless, the results for BIG-bench, included in Appendix A.1, THERMOMETER still outperforms other baselines with large margin, while stays within noise of TS-CV.Finally, we remark that the similar behavior of THERMOMETER holds for encoder-decoder model FLAN-T5-XL (see Appendix A.2). THERMOMETER shows superior performance over other baseline methods.For the challenging BIG-bench, THERMOMETER with FLAN-T5-XL outperforms all competing approaches, producing lower ECE and TL-ECE scores than the strongest competitor, TS-CV, on 20/23 tasks.</p>
<p>Thermometer: Towards Universal Calibration for LLMs  QA with Free Form Answers We use the MRQA shared task to evaluate THERMOMETER's efficacy for calibrating QA tasks with free form answers.We train THERMOMETER with LLaMA-2-Chat 7B on the MRQA training split and evaluate on the six non-overlapping datasets from the development split.The results are presented in Table 2. Similar to multiple-choice QA tasks, we find that THERMOMETER substantially improves calibration of the uncalibrated model and performs favorably against competing methods.Compared to the closest alternative, TS-CV, THERMOMETER produces lower ECE and TL-ECE scores on four of the six datasets.These results suggest broader applicability of THERMOMETER to (non-QA) free-form generation tasks.</p>
<p>Analysis</p>
<p>Here, we empirically scrutinize critical aspects of THERMOMETER using MMLU and LLaMA-2-Chat 7B.Ablations analyzing the choice of THERMOMETER architecture, and batch sizes, are in Appendix A.4. THERMOMETER transfers across model scales and benchmarks We examine how well THERMOMETER can transfer its calibration capabilities across models of the same family but of different scales.The results in Figure 5 demonstrate a significant finding, i.e., THERMOMETER trained with a smaller-scale model can effectively calibrate larger-scale models within the same family.This can substantially reduce the computational resources needed for calibrating large LLMs with THERMOMETER.</p>
<p>We also explore the ability of THERMOMETER to transfer across different benchmark datasets.</p>
<p>We train THERMOMETER on one benchmark dataset and test it on another.Figure 6 shows that THERMOMETER effectively transfers across different benchmarks, highlighting the robustness of THERMOMETER to substantial data shifts.</p>
<p>number of labeled instances</p>
<p>In green, we plot ECE (averaged over the fifty seven MMLU datasets) achieved by TS as a function of the number of labeled instances.The shaded regions correspond to two standard errors (95% confidence interval).THERMOMETER outperforms TS when labeled data is less than thirty.</p>
<p>THERMOMETER is effective in small labeled data regimes THERMOMETER demonstrates a significant advantage in calibrating new test tasks by predicting the desired temperature using only unlabeled data.In contrast, TS requires labeled data for temperature tuning.The TS results in Table 1, which utilize all available labeled data, serve as a lower-bound benchmark for our method.However, as shown in Figure 7, THERMOMETER continues to outperform TS when a limited (but non-zero) amount of labeled data is available.This suggests that when labeled data for a task is scarce or nonexistent, it is more effective to rely on THERMOMETER than employing temperature scaling.</p>
<p>THERMOMETER's performance improves with number of training tasks How many tasks do we need for training THERMOMETER?We explore this by varying the number of training tasks.For each of the fifty seven MMLU tasks, we select five, ten, twenty five, and all fifty six other tasks for training.Figure 8 presents our results aggregated over all MMLU tasks.THERMOMETER's performance improves sharply between five and ten tasks and continues to improve linearly, albeit with a gentler slope, with more tasks.sample-wise temperature scaling approach (Joy et al., 2023) that learns a unique temperature for each data point.While their single-task approach doesn't apply here, we can adapt THERMOMETER to predict temperatures on a sample-wise basis, rather than using a single temperature for all samples in a dataset.The results presented in Figure 9 show that while sample-wise temperature scaling is effective to a degree, it is sub-optimal when compared to aggregation across data samples.Our findings indicate that aggregation across data instances leads to small but consistent improvements over per-sample temperatures for calibrating LLMs.</p>
<p>Concluding Remarks</p>
<p>This work introduces THERMOMETER, a simple yet effective approach for calibrating LLMs.Through comprehensive empirical evaluations we find that it improves calibration, is robust to data shifts, and is computationally efficient.</p>
<p>While we demonstrate the effectiveness of THERMOMETER in question answering tasks with free-form answers, the potential for its application extends far beyond.Future directions include adapting THERMOMETER for other complex free-form generation tasks, such as summarization and translation, and applying THERMOMETER to larger LLMs.</p>
<p>Impact Statement</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.</p>
<p>A. Additional Results</p>
<p>A.1. THERMOMETER Consistently Performs Well on BIG-bench</p>
<p>The average calibration performance of THERMOMETER on the BIG-bench dataset is presented in Table 3.Additionally, scatter plots illustrating these results are detailed in Figure 10.The comparison between THERMOMETER's predicted temperatures and the optimal temperatures obtained through temperature scaling is depicted in Figure 11.Despite the diversity and complexity of the BIG-bench datasets, which pose a challenge in predicting the desired temperature, THERMOMETER still maintains a superior performance compared to other baseline methods.</p>
<p>ECE: THERMOMETER</p>
<p>A.2. THERMOMETER Consistently Performs Well on Encoder-decoder LLMs</p>
<p>To demonstrate THERMOMETER's robustness with respect to different types of language models, we also conduct a set of additional experiments on encoder-decoder model FLAN-T5-XL with three billion parameters.We empirically observe that elicitation-based calibration methods (Tian et al., 2023;Xiong et al., 2023) are not even functional on FLAN-T5-XL model thus omit reporting their results, i.e., by providing the crafted prompts, FLAN-T5-XL often fails to generate the corresponding verbalized confidence.This indicates that elicitation-based methods (Tian et al., 2023;Xiong et al., 2023) that rely on the ability of a model to faithfully follow nuanced instructions generally do not perform well for smaller and mid-sized open source models.In fact, the authors in (Tian et al., 2023) concede that even for Llama2-70b, "The verbal calibration of the open source model Llama-2-70b-chat is generally weaker than that of closed source models".</p>
<p>We evaluate the calibration performance of different methods on both MMLU and BIG-bench.The average calibration performance of THERMOMETER on MMLU datasets and BIG-bench datasets are shown in Table 4 and Table 5, respectively.We also provide the scatter plots of comparison for MMLU and BIG-bench in Figure 12 and Figure 13, respectively.Finally, The comparison between THERMOMETER's predicted temperatures and the optimal temperatures obtained through temperature scaling is presented in Figure 14.Similar to decode-only model LLaMA-2-Chat 7B, THERMOMETER consistently outperforms other baselines.Furthermore, we further validate that THERMOMETER also transfers well across different model scales of Flan-T5 in Figure 16.</p>
<p>A.3. THERMOMETER Shows Stronger Transfer-ability Than TS-CV</p>
<p>In Figure 5 and Figure 6, we demonstrate the strong transfer-ability of THERMOMETER across different model scales and datasets.This is the another advantage of THERMOMETER over those inference-based methods (Wei &amp; Zou, 2019;Tian et al., 2023;Xiong et al., 2023;Jiang et al., 2023a), which do have the capability to transfer.TS-CV is the only baseline that has transfer-ability, so we also conduct an additional experiment to compare the transfer-ability of THERMOMETER with the  TS-CV baseline.As illustrated Figure 16, Thermometer typically shows stronger transfer-ability than TS-CV, producing lower calibration errors.(3) MLP (two hidden layer): A deeper MLP with two hidden layers, offering more complexity.Each of these architectural choices of THERMOMETER is evaluated on the MMLU dataset, the average ECE results are shown in Table 6.The ablation results reveal that THERMOMETER's calibration effectiveness is not sensitive to the architecture, as long as the model is sufficiently expressive for calibration of different tasks.</p>
<p>Regularizer Weight To explore the sensitivity of THERMOMETER to hyper-parameter tuning, particularly the regularizer weight λ reg in the training objective, we conduct an ablation study with varying values of λ reg .The results shown in Table 7 demonstrate that while THERMOMETER shows a overall robustness to changes in the regularizer weight, it prefers smaller values of λ reg .Intuitively, a larger λ reg upweights the prior term in the training objective and more strongly discourages large temperatures.We also tried a variant with λ reg = 0, but this caused numerical instability during training and the resulting models exhibited poor performance.Recall that by construction ψ θ (ϕ(x)) ≥ 0 (desirable since we are estimating a temperature).We can apply a Bernstein inequality to obtain
Pr N * n=1 ψ θ (ϕ(x n )) − (Eψ θ (ϕ(x))) &gt; t &lt; 2exp − 3t 2 3N * V θ + 2C θ t .(7)
Let us choose t to achieve inverse squared probability concentration in N * , specifically, we set the right hand size of (7) to 2N −2 * and solve for the corresponding t:
−2 log N * := − 3t 2 * 3N * V θ + 2C θ t * −6N * (log N * )V θ − 4(log N * )C θ t * + 3t 2 * = 0
The quadratic theorem yields9
t * = 4(log N * )C θ + 16(log N * ) 2 C 2 θ + 72V θ N * log N * 6 = 2 3 C θ log N * + 4 9 (log N * ) 2 + 2V θ N * log N * ≤ 4 3 C θ log N * + (2V θ ) 1 2 N * log N * .
Substituting this in and dividing both sides by N * yields the first part of the lemma statement.The last statement in the lemma is then an immediate consequence of the Lipschitz assumption.</p>
<p>B.2. Product of Gaussians</p>
<p>From standard exponential family properties we have,  Examples of the processed data, formatted with prompts templates, are detailed in Table 13.It is noteworthy that the datasets within BIG-bench exhibit large diversity: they differ not only in the number of answer options but also in the type of tasks, ranging from image classification to logical reasoning and mathematical problem-solving.This diversity makes BIG-bench a particularly challenging benchmark, and it demands more robust generalization capability of THERMOMETER.
N k n=1 N (x | µ n , Σ n ) ∝ N (x | μ, Σ), where Σ = N K n=1 Σ −1 n −1 and μ = Σ N k n=1 Σ −1 n µ n . (8) Equation (3) follows from plugging in Σ 1 = Σ 2 = . . . = Σ N k = ϵ and µ n = ψ θ (ϕ(x k n )) in Equation (8).
MRQA MRQA (Machine Reading for Question Answering) (Fisch et al., 2019) is widely used reading comprehension task with free-form answers.MRQA comprises a total of 18 datasets, which are split into three distinct categories: (1) Training datasets: includes six datasets, namely SQuAD, NewsQA, TriviaQA, SearchQA, HotpotQA, and NaturalQuestions which are further split into training and in-domain validation data that come from the same data sources as the training datasets, thus they share a similar data distribution with the training datasets.(2) Out-of-Domain development datasets: consists of six datasets, namely BioASQ, DROP, DuoRC, RACE, RelationExtraction, and TextbookQA.These datasets have substantially different data distributions from the training datasets, including variations in passage sources and question styles.(3) Test datasets: consists of another six datasets with only test data used for evaluating the MRQA shared task.Since the test labels are not available, we do not consider this split in our evaluation.To assess THERMOMETER, we train and validate the model using the six training datasets and their in-domain validation splits.We evaluate THERMOMETER on the out-of-domain development datasets.Examples of the processed in-domain data and out-of domain are detailed in Table 14 and Table 15, respectively.Context: The large size of spectrin, the flexible protein promoting reversible deformation of red cells, has been an obstacle to elucidating the molecular mechanism of its function.By studying cloned fragments of the repeating unit domain, we have found a correspondence between positions of selected spectrin repeats in a tetramer with their stabilities of folding.Six fragments consisting of two spectrin repeats were selected for study primarily on the basis of the predicted secondary structures of their linker regions.Fragments with a putatively helical linker were more stable to urea-and heat-induced unfolding than those with a putatively nonhelical linker.Two of the less stably folded fragments, human erythroid alpha-spectrin repeats 13 and 14 (HEalpha13,14) and human erythroid betaspectrin repeats 8 and 9 (HEbeta8,9), are located opposite each other on antiparallel spectrin dimers.At least partial unfolding of these repeats under physiological conditions indicates that they may serve as a hinge.Also less stably folded, the fragment of human erythroid alpha-spectrin repeats 4 and 5 (HEalpha4,5) lies opposite the site of interaction between the partial repeats at the C-and N-terminal ends of beta-and alpha-spectrin, respectively, on the opposing dimer.More stably folded fragments, human erythroid alpha-spectrin repeats 1 and 2 (HEalpha1,2) and human erythroid alpha-spectrin repeats 2 and 3 (HEalpha2,3), lie nearly opposite each other on antiparallel spectrin dimers of a tetramer.These clusterings along the spectrin tetramer of repeats with similar stabilities of folding may have relevance for spectrin function, particularly for its well known flexibility.. Question: Alpha-spectrin and beta-spectrin subunits form parallel or antiparallel heterodimers?Answer: Antiparallel.</p>
<p>Is</p>
<p>Figure 1 .
1
Figure 1.Calibration performance against inference runtime.Different methods for calibrating LLaMA-2-Chat 7B compared on the PROFESSIONAL LAW task of MMLU on a A100, 40 GB GPU.The task contains 1533 questions.Our method, THERMOMETER is significantly faster than methods that require multiple forward passes(Wei &amp; Zou, 2019;Xiong et al., 2023;Jiang et al., 2023a) at inference time and achieves lower calibration error compared to methods with comparable runtime(Tian et al., 2023).Vanilla refers to the no-calibration baseline, and TS-CV is a temperature scaling variant (Section 5).Similar trends hold for other benchmarks.</p>
<p>Figure 2 .
2
Figure 2. Evidence accumulation.We illustrate evidence accumulation for the PROFESSIONAL LAW dataset from MMLU.It contains N k = 1533 instances.The left panel, shows twenty of the possible 1533 Gaussians, N (τ k | ψ θ (ϕ(x k n )), ϵ = 0.01).The right panel, plots the Gaussian proportional to 1533 n=1 N (τ k | ψ θ (ϕ(x k n )), ϵ = 0.01).The resulting Gaussian is nearly a point mass at 1 1533</p>
<p>across data instances in D k , with the uncertainty in the inferred τ k decreasing linearly with N k .Second, by sharing θ across datasets we share statistical strength across the training datasets D 1 , . . ., D K which allows us to use ψ θ to predict τ * for an unseen dataset D * .Finally, when ϵ is small, our parameterization leads to a particularly simple algorithm.</p>
<p>Figure 3 .
3
Figure 3. LLaMA-2-Chat 7B Scatter Plots: ECE Score of 57 MMLU Datasets.The x-axis and y-axis represent ECE score of uncalibared and calibrated model, respectively.THERMOMETER reduces calibration error on all 57 datasets, often substantially.</p>
<p>.008 0.136±0.0110.304±0.0371.220±0.0430.162±0.006calibration methods as well as those recently proposed methods: (1) Vanilla: calibration performance of LLMs without any calibration techniques applied.(2) Temperature Scaling (TS): cheats by using labeled data from the testing task to tune the task-specific temperatures and establishes a lower bound for the error.(3) Temperature Scaling with crossvalidation (TS-CV): This variant of TS is similar to the setting of our approach.Here, the temperature is tuned using data from K − 1 tasks, and used to calibrate the held-out task, without any task-specific adaptation.(4) Monte Carlo Dropout (MC-Dropout) 8(Gal &amp; Ghahramani, 2016): performs inference with random dropout multiple times, and averages the output probabilities across these inferences.(5)Monte Carlo Augmentation (MC-Augment)(Wei &amp; Zou, 2019): employs random data augmentations of the prompts rather than dropout.(6) Elicitation(Tian et al., 2023) directly generates verbalized confidence by providing the LLMs with carefully crafted prompts.(7) Elicitation-Ensemble(Xiong et al., 2023) improves the quality of verbalized confidence by aggregation of multiple samples.(8) CAPE(Jiang et al., 2023a) performs prompt augmentation using option permutation.</p>
<p>Figure 4 .
4
Figure 4. THERMOMETER against temperature scaling on MMLU.Comparison of THERMOMETER predictions and temperatures obtained by temperature scaling.Each green dot represents one MMLU task.The x-coordinate is the temperature learned via temperature scaling and the y-coordinate is the THERMOMETER predicted temperature.THERMOMETER accurately predicts the temperature for unseen task.</p>
<p>Figure 5 .
5
Figure 5. THERMOMETER transfers across different model scales of LLaMA-2-Chat.We use LLaMA-2-Chat 7B THERMOMETER predicted temperatures for calibrating LLaMA-2-Chat 7B (bold axes), LLaMA-2-Chat 13B and LLaMA-2-Chat 70B.In these plots, each dot represents a MMLU task.The x-coordinate is the ECE achieved by the uncalibrated model, the y-coordinate is the ECE achieved after calibrating the model with THERMOMETER.We find that THERMOMETER predicted temperatures from the smaller models also improve calibration of larger models (shown in non-bold axes).</p>
<p>Figure 6 .
6
Figure 6.THERMOMETER transfers across different datasets.Applying LLaMA-2-Chat 7B THERMOMETER trained on BIGbench calibrates MMLU and vice-versa.</p>
<p>Figure 7 .
7
Figure 7. Temperature scaling vs. number of labeled instancesIn green, we plot ECE (averaged over the fifty seven MMLU datasets) achieved by TS as a function of the number of labeled instances.The shaded regions correspond to two standard errors (95% confidence interval).THERMOMETER outperforms TS when labeled data is less than thirty.</p>
<p>Figure 9 .
9
Figure 9. Aggregation v.s.Sample-wise Temperature.Samplewise temperature scaling is less effective compared to THERMOMETER.</p>
<p>Figure 10 .
10
Figure 10.LLaMA-2-Chat 7B Scatter Plots: ECE Score of 23 BIG-bench Datasets.The x-axis and y-axis represent ECE score of uncalibared and calibrated model, respectively.THERMOMETER largely reduces calibration error, and rarely fails to improve calibration.</p>
<p>Figure 11 .
11
Figure11.THERMOMETER against temperature scaling on BIG-bench (LLaMA-2-Chat 7B).Comparison of THERMOMETER predicted temperature and optimal temperature obtained by temperature scaling.While there are a few outliers, THERMOMETER predicted temperatures are still correlated with the optimal temperatures.</p>
<p>Figure 12 .
12
Figure 12.FLAN-T5-XL Scatter Plots: ECE Score of 57 MMLU Datasets.The x-axis and y-axis represent ECE score of uncalibared and calibrated model, respectively.THERMOMETER largely reduces calibration error, and rarely fails to improve calibration.</p>
<p>Figure 13 .
13
Figure 13.FLAN-T5-XL Scatter Plots: ECE Score of 23 BIG-bench Datasets.The x-axis and y-axis represent ECE score of uncalibared and calibrated model, respectively.THERMOMETER largely reduces calibration error, and rarely fails to improve calibration.</p>
<p>Figure 14 .
14
Figure14.THERMOMETER against temperature scaling on MMLU and BIG-bench (FLAN-T5-XL).Comparison of THERMOMETER predicted temperature and optimal temperature obtained by temperature scaling.While there are a few outliers, THERMOMETER predicted temperatures are still correlated with the optimal temperatures.</p>
<p>Figure 15 .
15
Figure15.THERMOMETER transfers across different model scales of FLan-T5.We use FLan-T5-XL 3B THERMOMETER predicted temperatures to calibrate FLan-T5-XL 3B (bold axes), FLan-T5-XXL 11B and Flan-UL2 20B.In these plots, each dot represents a MMLU task.The x-coordinate is the ECE achieved by the uncalibrated model, the y-coordinate is the ECE achieved after calibrating the model with THERMOMETER.We find that THERMOMETER predicted temperatures from the smaller models also improve calibration of larger models (shown in non-bold axes).</p>
<p>Figure 16 .
16
Figure 16.Compare Transfer-ability of THERMOMETER with TS-CV.Top: We use FLan-T5-XL 3B THERMOMETER predicted temperatures to calibrate 20B Flan-UL2 in the left subplot, and LLaMA-2-Chat 7B THERMOMETER predicted temperatures for calibrating LLaMA-2-Chat 13B in the right subplot.Bottom: Applying LLaMA-2-Chat 7B THERMOMETER trained on BIG-bench calibrates MMLU and vice-versa.</p>
<p>Algorithm 1 THERMOMETER Training input Training datasets of K tasks {D k } K k=1 ; pre-trained LLM M with feature extractor ϕ; prior hyper-parameters ν 0 ; batch size N b ; learning rate γ; number of iterations M ; checkpoint parameters {m ′ , burnin}; initialization θ init</p>
<p>▷ Split {D k } K k=1 into a development and validation set,</p>
<p>Table 1 .
1
LLaMA-2-Chat 7B Average Calibration Performance on MMLU.The results are reported as the mean and two standard error of the calibration results over 57 datasets.TS serves as the lower-bound as it has access to the labeled data of testing task.</p>
<p>Table 2 .
2
LLaMA-2-Chat 7B Average Calibration Performance on MRQA.The results are reported as the mean and two standard error of the calibration results over the six MRQA validation datasets.
MethodsECETL-ECEMCENLLBrierTS (lower-bound)0.029±0.005 0.058±0.021 0.115±0.015 0.536±0.042 0.178±0.017Vanilla0.127±0.026 0.146±0.023 0.198±0.023 0.656±0.068 0.198±0.022TS-CV0.071±0.011 0.099±0.014 0.157±0.015 0.556±0.042 0.183±0.017MC-Augment0.335±0.044 0.411±0.034 0.679±0.052 0.997±0.081 0.370±0.026Elicitation0.130±0.031 0.207±0.036 0.522±0.169//Elicitation-Ensemble 0.171±0.047 0.237±0.032 0.552±0.206//CAPE0.067±0.016 0.098±0.028 0.156±0.060 0.556± 0.087 0.183±0.036THERMOMETER0.065±0.008 0.093±0.016 0.163±0.027 0.551±0.039 0.182±0.016</p>
<p>Table 3 .
3
LLaMA-2-Chat 7B Average Calibration Performance on BIG-bench.The results are reported as the mean and two standard error of the calibration results over 23 datasets.
MethodsECETL-ECEMCENLLBrierTS (lower-bound)0.044±0.014 0.089±0.025 0.106±0.027 1.229±0.130 0.189±0.013Vanilla0.233±0.035 0.263±0.037 0.467±0.055 1.539±0.171 0.232±0.021TS-CV0.087±0.020 0.118±0.027 0.240±0.041 1.258±0.129 0.195±0.013MC-Augment0.186±0.031 0.231±0.033 0.436±0.043 1.466±0.180 0.214±0.016Elicitation0.225±0.048 0.263±0.059 0.745±0.085//Elicitation-Ensemble 0.202±0.049 0.241±0.063 0.753±0.068//CAPE0.214±0.073 0.235±0.076 0.475±0.107 1.567±0.365 0.231±0.043Thermometer0.090±0.018 0.125±0.026 0.243±0.038 1.261±0.132 0.195±0.0130.0 0.1 0.2 0.3 0.4 0.5 Uncalibrated (Vanilla) ECE</p>
<p>Table 4 .
4
FLAN-T5-XL Average Calibration Performance on MMLU.The results are reported as the mean and two standard error of the calibration results over 57 datasets.TS serves as the lower-bound as it has access to the labeled data of testing task.
MethodsECETL-ECEMCENLLBrierTS (lower-bound) 0.063±0.006 0.128±0.010 0.249±0.037 1.141±0.054 0.153±0.008Vanilla0.181±0.013 0.215±0.014 0.448±0.049 1.286±0.074 0.167±0.010TS-CV0.093±0.007 0.146±0.010 0.314±0.053 1.160±0.054 0.155±0.009MC-Dropout0.107±0.015 0.159±0.012 0.372±0.041 1.301±0.065 0.171±0.007MC-Augment0.156±0.015 0.198±0.014 0.431±0.045 1.266±0.067 0.167±0.008CAPE0.167±0.017 0.210±0.015 0.410±0.048 1.271±0.072 0.166± 0.009THERMOMETER0.080±0.008 0.139±0.011 0.319±0.042 1.154±0.055 0.155±0.0090.4ECE: TS (lower-bound)0.4ECE: TS-CV0.4ECE: MC-Dropout0.1 0.2 0.3 Calibrated ECE0.1 0.2 0.3 Calibrated ECE0.1 0.2 0.3 Calibrated ECE0.0 0.0 0.40.1 Uncalibrated (Vanilla) ECE 0.2 0.3 ECE: MC-Augment0.40.0 0.0 0.40.1 Uncalibrated (Vanilla) ECE 0.2 0.3 ECE: CAPE0.40.0 0.0 0.40.1 Uncalibrated (Vanilla) ECE 0.2 0.30.40.1 0.2 0.3 Calibrated ECE0.1 0.2 0.3 Calibrated ECE0.1 0.2 0.3 Calibrated ECE0.0 0.00.1 Uncalibrated (Vanilla) ECE 0.2 0.30.40.0 0.00.1 Uncalibrated (Vanilla) ECE 0.2 0.30.40.0 0.00.1 Uncalibrated (Vanilla) ECE 0.2 0.30.4</p>
<p>Table 5 .
5
FLAN-T5-XL Average Calibration Performance on BIG-bench.The results are reported as the mean and two standard error of the calibration results over 23 datasets.
MethodsECETL-ECEMCENLLBrierTS (lower-bound) 0.043±0.006 0.109±0.022 0.179±0.042 1.087±0.147 0.157±0.013Vanilla0.192±0.032 0.230±0.033 0.444±0.055 1.417±0.235 0.183±0.017TS-CV0.121±0.018 0.175±0.026 0.306±0.039 1.170±0.150 0.168±0.013MC-Dropout0.130±0.015 0.184±0.023 0.360±0.056 1.311±0.184 0.180±0.014MC-Augment0.167±0.028 0.211±0.030 0.374±0.058 1.378±0.213 0.186±0.017CAPE0.176±0.057 0.213±0.061 0.432±0.122 1.387±0.242 0.181± 0.033Thermometer0.078±0.011 0.138±0.021 0.220±0.035 1.113±0.148 0.160±0.0130.00.1 Uncalibrated (Vanilla) ECE 0.2 0.30.4</p>
<p>Table 6 .
6
Ablation Study of THERMOMETER Architecture.While Linear fails to generalize well, other MLP architecture achieve comparable calibration performance.
ArchitectureECETL-ECEMCENLLBrierLinear0.102±0.006 0.151±0.007 0.322±0.015 1.241±0.0200.165±.003MLP (one layer)0.079±0.004 0.137±0.006 0.302±0.018 1.219±0.022 0.162±0.003MLP (three layers) 0.075±0.004 0.136±0.006 0.305±0.017 1.218±0.023 0.162±0.003MLP (ours)0.078±0.008 0.136±0.011 0.304±0.037 1.220±0.043 0.162±0.006</p>
<p>Table 7 .
7
Ablation study of λreg.THERMOMETER shows to be insensitive to this hyper-parameter, and it prefers relatively small λreg.
λregECETL-ECEMCENLLBrierλreg = 1.00.101±0.006 0.161±0.005 0.347±0.021 1.251±0.027 0.164±0.004λreg = 10 −1 0.087±0.004 0.145±0.005 0.318±0.015 1.225±0.023 0.163±0.003λreg = 10 −2 0.078±0.008 0.136±0.011 0.304±0.037 1.220±0.043 0.162±0.006λreg = 10 −3 0.076±0.004 0.134±0.006 0.311±0.018 1.221±0.022 0.162±0.003</p>
<p>Table 8 .
8
Ablation study of training temperature inference batch size.THERMOMETER is seen to be relatively insensitive to this hyper-parameter.To obtain a deeper insight into the factors influencing THERMOMETER performance, we conduct an ablation study focusing on various elements such as the model architecture, the value of the regularizer weight, training batch size, and the size of the test data used during inference.The results are detailed in Appendix A.4.
Training temperature batch size bECETL-ECEMCENLLBrier10.083±0.009 0.140±0.012 0.321±0.035 1.222±0.045 0.162±0.006160.074±0.008 0.135±0.012 0.291±0.036 1.217±0.045 0.162±0.006320.077±0.008 0.135±0.012 0.309±0.035 1.220±0.045 0.162±0.0061280.078±0.008 0.136±0.011 0.304±0.037 1.220±0.043 0.162±0.006A.4. Ablation StudiesTHERMOMETER Architecture To determine if the impressive calibration performance of THERMOMETER is influencedby its specific architectural choice, we conduct an ablation study exploring different model structures. The variants ofmodel architecture include: (1) Linear: a simple model consisting of a single linear layer without a nonlinear activationfunction; (2) MLP (no hidden layer): A MLP with only input and output layers, excluding any hidden layers;</p>
<p>Temperature Inference Batch Size at Train Time We next explore the sensitivity of THERMOMETER to the size of the batch used to estimate the temperature in training, i.e. b in Algorithm 1.Based on Lemma 4.1, we expect the method to be relatively insensitive to this parameter.Here we use λ reg = 0.01 and testing temperature batch size 128.Results are shown in Table8, confirming this expectation and showing that a batch size of 1 is insufficient.Temperature Inference Batch Size at Test Time Similarly, we explore the sensitivity of THERMOMETER to the size of the batch used to estimate the temperature at test time, i.e.N * .N * may need to be small in practice if only a few unlabeled examples are available before picking a temperature to run on the task.Based on Lemma 4.1, we expect the method to be</p>
<p>Table 9 .
9
Ablation study of testing temperature inference batch size.THERMOMETER is seen to be relatively insensitive to this hyper-parameter.
Testing temperature batch size N *ECETL-ECEMCENLLBrier10.081±0.009 0.138±0.012 0.299±0.034 1.220±0.045 0.162±0.006160.074±0.008 0.135±0.011 0.296±0.036 1.217±0.045 0.162±0.0061280.078±0.008 0.136±0.011 0.304±0.037 1.220±0.043 0.162±0.006relatively insensitive to this parameter. Here we use λ reg = 0.01 and training temperature batch size 128. Results are shownin Table 9, confirming this expectation.B. Derivation and ProofsB.1. Proof of Lemma 4.1</p>
<p>Table 12 .
12
(Srivastava et al., 2022)ollector was a serious antique car buyer, and was always searching for Thunderbirds in good shape.She saw a newspaper ad offering a 1964 Thunderbird sports coupe for 25, 000, The ad also gave a website address "to view a picture of the car," which Carol visited, and which showed a T-Bird with a perfect body and interior.Carol paid the 25, 000 and signed a sales contract which specified that the Thunderbird was "used and sold as is."WhenCarol went to pick up the car, she learned it was a 1968 model, which was not at all rare and worth much less than the advertised 1964 model.If Carol brings suit against the seller, the likely outcome is for A. Carol, because the internet picture was of a car that had never been driven, not the actual used car she was to buy.B.The seller, because the buyer was aware the Thunderbird was sold "as Massive Multitask Language Understanding)(Hendrycks et al., 2020)is a popular benchmark commonly used to evaluate the depth of knowledge AI models acquire during pre-training, especially under zero-shot and few-shot settings.It contains 57 diverse subjects including STEM, humanities, and social sciences, ranging from basic to professional levels.Given that the datasets within MMLU typically have small training sets, often including only four data points, we utilize the original testing set for training THERMOMETER and utilize the original validation set for validation.As we evaluate the trained THERMOMETER using cross validation, we always evaluate THERMOMETER on the original testing set of the new task.Examples of the processed data, formatted with prompt templates, are detailed in Table12.BIG-bench BIG-bench (The Beyond the Imitation Game Benchmark)(Srivastava et al., 2022), with its extensive collection of over 200 datasets, contains a wide range of NLP tasks.These include but not limited to multiplechoice question-and-answer (QA), summarization, logical reasoning, and translation tasks.Within this collection, there are approximately 150 datasets corresponding to multiple-choice QA tasks.However, many of them have only a small number of training instances.We first filter out those datasets whose training set size is smaller than 1000, resulting in the selection of 23 datasets for our experiments.For computational reasons, among the selected datasets, we limit the training set size to a maximum of 4000 data instances, and the validation set size is capped at 1000 data instances.The selected datasets are: ARITHMETIC, BBQ LITE JSON, CIFAR10 CLASSIFICATION,
Dataset NamePrompt
CONTEXTUAL PARAMETRIC KNOWLEDGE CONFLICTS, COLOR, ELEMENTARY MATH QA, EPISTEMIC REASONING, FACT CHECKER, FORMAL FALLACIES SYLLOGISMS NEGATION, GOAL STEP WIKIHOW, HYPERBATON, LOGI-CAL FALLACY DETECTION, MNIST ASCII, MOVIE DIALOG SAME OR DIFFERENT, PLAY DIALOG SAME OR DIFFERENT,</p>
<p>Table 13 .
13
BIG-bench Data Examples.Which phrase best fits the {MASK} span?Context: A: Guess what came in the mail today?B: What?A: My acceptance letter to Yale !B: Wow !Congratulation !When do classes start?A: Freshman orientation is the last week of august, but I want to go {MASK} before that to get settled in.B: You're so lucky !Do you have to do many things before you leave?A: Yes.I'll be very busy !I have to get a visa, buy a plane ticket, and pack my things.But first, I want to register for classes.B: When can you do that?A: Well, they sent me their prospectus, so I can start looking now.do you want to help me decide which classed to take?B: Sure.What can you choose from?A: Well, I have to take all the fundamental course, plus a few from my major.B: What is your major?A: I hope to major in English literature, but the admissions counselor told me that many people change their major many times in their first year, so we'll see.B: What are the fundamental course?A: In order to graduate, every student must take a certain amount of classes in history, math, English, philosophy, science and art.B: Interesting.That's very different from the Chinese education system.A: Yes, it is.It's also very different from the British education system.B: Really?A: oh, sure.In British, students don't have to take the foundation course.B: why not?A: maybe because they think they know everything already !ha ha.
Dataset NamePrompt</p>
<p>Table 15 .
15
MRQA Out-of-domain Test Data Examples.
Dataset NamePromptLLM's Response v.s. True TargetCompletion TokenBioASQChoose A, or B.Answer in as few words as possible.
and corresponding semantic entities represented by the tokens.
C = K/2 + (K/2) ln(2πϵ)
We use the shape scale parameterization, with shape = 1.25 and scale = 4.
This can be replaced with C 2 θ /4 if no additional knowledge of the variance is available.
This can be done rigorously by deriving and adding a concentration upper bound for the error of the empirical variance before plugging into the bound in Lemma 4.1 as V θ .
We found dropout to not be functional in LLaMA-2-Chat 7B, enabling dropout at inference time did not produced any variability, so we only report MC-Dropout comparisons with FLAN-T5-XL.
Recall we need t * ≥ 0.
AcknowledgementsWe are grateful to Natalia Martinez Gil, Dan Gutfreund, Farzaneh Mirzazadeh, and Veronika Thost for their feedback on the manuscript and to Inkit Padhi for help with compute infrastructureThermometer: Towards Universal Calibration for LLMs Table10.Prompt Template."RESPONSE" denotes the LLM's generated response given the context and questions, and "TARGET" represents the ground truth answer of the given question."ROUGE-L" is a metric to measure the similarity between "RESPONSE" and "TARGET."Task Prompt Completion TokenMultiple-choice QA Choose A, B, C, or D. Question: {"QUESTION"} A. {"OPTION 1"} B. {"OPTION 2"} C. {"OPTION 3"} D. {"OPTION 4"} Answer: {"MASK" } yn ∈ {"A", "B", "C", "D"}QA with Free form AnswersChoose A, or B. Answer in as few words as possible.Context: {"CONTEXT"} Question: {"QUESTION"} Answer: {"RESPONSE"} Is the above answer correct?A. Yes B. No Answer: The construction of prompt templates and completion tokens for multiple-choice QA tasks and QA with free form answers is outlined in Table10.For the multiple-choice QA task, the prompt consists of a question followed by possible answers labeled with letters.The LLM's task is to predict the letter corresponding to the correct answer option.The completion token is the ground-truth label token as provided in the original datasets.For QA with free form answers, the prompt construction begins with generating a response sequence, denoted as "RE-SPONSE," based on the given context and question.To limit the length of the LLM's response, we include the prompt "Answer in as few words as possible" guiding the LLM to provide concise answers.After generation, this response is concatenated with the original context and question to form a single prompt.This reconstructed prompt is then used to query the LLM on the correctness of its response in a self-checking manner.Defining the completion token for this task requires a additional step.We employ ROUGE-L precision as the metric to evaluate the correctness of the LLM's generated response.ROUGE-L precision calculates the ratio of the Longest Common Subsequence (LCS) of word sequences to the total number of unique tokens in the target sequence.Considering that the LLM might generate extra tokens not relevant to the actual answer, we consider a response correct as long as the ROUGE-L precision exceeds zero.Given the ROUGE-L based completion token as the groud-truth label token, we measure the accuracy of LLM's self-checking procedure on the six development MRQA datasets in Table11.Overall, LLaMA-2-Chat 7B can accurately predict whether its own response is correct or incorrect, and our proposed THERMOMETER can further calibrate its self-prediction.Thermometer: Towards Universal Calibration for LLMsC.3. THERMOMETER ArchitectureWe use a multi-branch MLP for THERMOMETER inspired from ensemble learning techniques.The structure of THERMOMETER comprises three independent branches with identical architecture, each consisting of a sequence of two linear layers equipped with a ReLU activation function in between.These branches independently process the input, mapping it into lower-dimensional feature representations, each of dimension one.Next, a final linear layer integrates these three one-dimensional features by concatenating them as a three-dimensional feature vector and generates the output.The primary objective behind this architectural choice of THERMOMETER is to improve its ability to generalize across diverse, unseen tasks.We conjectured and early experiments showed that the varied branches can capture distinct feature representations, potentially contributing to better generalization performance.However, as indicated in the ablation study results presented in Table6, the performance of THERMOMETER appears to be quite robust to variations in model architecture.More thoroughly comparing THERMOMETER architecture is part of planned future work.C.4. Implementation DetailsLLMs To efficiently train THERMOMETER, we employ a strategy of extracting and storing the last layer features from Large Language Models (LLMs) on the local device.This approach significantly reduces the computational cost of LLM inference during the training stage of THERMOMETER.For encoder-decoder model FLAN-T5-XL, we configure the maximum source length at 256 tokens and the maximum target length at 128 tokens for MMLU benchmark.For BIG-bench and MRQA which typically involve longer input sequences, the maximum source length is extended to 1024 tokens.For decoder-only model LLaMA-2-Chat 7B , we set max sequence sequence length to 1024 tokens for all benchmark datasets.THERMOMETER In the implementation of THERMOMETER, the input dimension of THERMOMETER is set to be 2048 and 4096 for FLAN-T5-XL and LLaMA-2-Chat 7B , respectively.Correspondingly, the dimensions of the hidden layers in THERMOMETER are set at 256 for FLAN-T5-XL and 512 for LLaMA-2-Chat 7B .To ensure that the output of THERMOMETER remains positive, a Softplus activation function is adopted.The optimization of THERMOMETER utilizes the AdamW optimizer, and all the hyper-parameter used for training is summarized in Table16.The configuration is consistent across experiments conducted on MMLU, BIG-bench, and MRQA.All experiments are implemented in PyTorch using Tesla V100 GPU with 32 GB memory and Tesla A100 GPU with 40 GB memory.
Enhancing in-context learning via linear probe calibration. M Abbas, Y Zhou, P Ram, N Baracaldo, H Samulowitz, T Salonidis, T Chen, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics. The 27th International Conference on Artificial Intelligence and Statistics2024</p>
<p>Multi-level variational autoencoder: Learning disentangled representations from grouped observations. D Bouchacourt, R Tomioka, S Nowozin, Proceedings of AAAI. AAAI201832</p>
<p>Verification of forecasts expressed in terms of probability. G W Brier, Monthly weather review. 78161950</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>A close look into the calibration of pre-trained language models. Y Chen, L Yuan, G Cui, Z Liu, Ji , H , arXiv:2211.00151202223arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>The well-calibrated Bayesian. A P Dawid, Journal of the American Statistical Association. 773791982</p>
<p>The Helmholtz machine. P Dayan, G E Hinton, R M Neal, R S Zemel, Neural computation. 751995</p>
<p>Calibration of pre-trained transformers. S Desai, G Durrett, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Are you using test log-likelihood correctly?. S Deshpande, S Ghosh, T D Nguyen, T Broderick, Transactions on Machine Learning Research. 2835-88562024</p>
<p>A Fisch, A Talmor, R Jia, M Seo, E Choi, D Chen, arXiv:1910.09753Mrqa 2019 shared task: Evaluating generalization in reading comprehension. 2019622arXiv preprint</p>
<p>Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. Y Gal, Z Ghahramani, International Conference on Machine Learning. PMLR201617</p>
<p>Strictly proper scoring rules, prediction, and estimation. T Gneiting, A E Raftery, Journal of the American Statistical Association. 1024772007</p>
<p>Probabilistic forecasts, calibration and sharpness. T Gneiting, F Balabdaoui, A E Raftery, Journal of the Royal Statistical Society Series B. 6922007</p>
<p>On calibration of modern neural networks. C Guo, G Pleiss, Y Sun, K Q Weinberger, International Conference on Machine Learning. 201713</p>
<p>C Gupta, A Ramdas, arXiv:2107.08353Top-label calibration and multiclass-to-binary reductions. 2021arXiv preprint</p>
<p>Prototypical calibration for few-shot learning of language models. Z Han, Y Hao, L Dong, Y Sun, F Wei, The Eleventh International Conference on Learning Representations. 2023</p>
<p>D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020620arXiv preprint</p>
<p>What are Bayesian neural network posteriors really like. P Izmailov, S Vikram, M D Hoffman, A G Wilson, International conference on machine learning. 2021</p>
<p>Calibrating language models via augmented prompt ensembles. M Jiang, Y Ruan, S Huang, S Liao, S Pitis, R B Grosse, J Ba, 2023a115</p>
<p>How can we know when language models know? on the calibration of language models for question answering. Z Jiang, J Araki, H Ding, G Neubig, Transactions of the Association for Computational Linguistics. 922021</p>
<p>Generative calibration for in-context learning. Z Jiang, Y Zhang, C Liu, J Zhao, K Liu, The 2023 Conference on Empirical Methods in Natural Language Processing. 2023b</p>
<p>Sample-dependent adaptive temperature scaling for improved calibration. T Joy, F Pinto, S.-N Lim, P H Torr, P K Dokania, Proceedings of AAAI. AAAI2023379</p>
<p>Language models (mostly) know what they know. S Kadavath, T Conerly, A Askell, T Henighan, D Drain, E Perez, N Schiefer, Z Hatfield-Dodds, N Dassarma, E Tran-Johnson, arXiv:2207.05221202223arXiv preprint</p>
<p>Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. M Kull, M Perello Nieto, M Kängsepp, T Silva Filho, H Song, P Flach, Advances in Neural Information Processing Systems. 3222019</p>
<p>Simple and scalable predictive uncertainty estimation using deep ensembles. B Lakshminarayanan, A Pritzel, C Blundell, Advances in Neural Information Processing Systems. 201730</p>
<p>Teaching models to express their uncertainty in words. S Lin, J Hilton, O Evans, Transactions on Machine Learning Research. 2835-8856. 22022</p>
<p>Reducing conversational agents' overconfidence through linguistic calibration. S J Mielke, A Szlam, E Dinan, Y.-L Boureau, Transactions of the Association for Computational Linguistics. 1032022</p>
<p>Calibrating deep neural networks using focal loss. J Mukhoti, V Kulharia, A Sanyal, S Golodetz, P Torr, P Dokania, Advances in Neural Information Processing Systems. 202033</p>
<p>Obtaining well calibrated probabilities using Bayesian binning. M P Naeini, G Cooper, M Hauskrecht, Proceedings of AAAI. AAAI2015296</p>
<p>. OpenAI. GPT-4 technical report. 12023</p>
<p>On the calibration of pre-trained language models using mixup guided by area under the margin and saliency. S Y Park, C Caragea, arXiv:2203.075592022arXiv preprint</p>
<p>Regularizing neural networks by penalizing confident output distributions. G Pereyra, G Tucker, J Chorowski, Ł Kaiser, G Hinton, arXiv:1701.065482017arXiv preprint</p>
<p>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. J Platt, Advances in large margin classifiers. 1999103</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, arXiv:2206.046152022620arXiv preprint</p>
<p>Rethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2016</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. K Tian, E Mitchell, A Zhou, A Sharma, R Rafailov, H Yao, C Finn, C Manning, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingDecember 2023. 1, 3, 7, 1415</p>
<p>Llama 2: Open foundation and finetuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, arXiv:2307.09288202316arXiv preprint</p>
<p>Easy data augmentation techniques for boosting performance on text classification tasks. J Wei, K Zou, Eda, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Association for Computational LinguisticsNovember 2019. 1715</p>
<p>Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. Y Xiao, P P Liang, U Bhatt, W Neiswanger, R Salakhutdinov, L.-P Morency, arXiv:2210.04714202223arXiv preprint</p>
<p>Can llms express their uncertainty? an empirical evaluation of confidence elicitation in llms. M Xiong, Z Hu, X Lu, Y Li, J Fu, J He, B Hooi, arXiv:2306.130632023. 1, 3, 7, 1415arXiv preprint</p>
<p>Robust calibration with multi-domain temperature scaling. Y Yu, S Bates, Y Ma, M Jordan, Advances in Neural Information Processing Systems. 202235</p>
<p>Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers. B Zadrozny, C Elkan, International Conference on Machine Learning. 20011</p>
<p>Transforming classifier scores into accurate multiclass probability estimates. B Zadrozny, C Elkan, Proceedings of the 8th ACM SIGKDD international conference on Knowledge discovery and data mining. the 8th ACM SIGKDD international conference on Knowledge discovery and data mining2002</p>
<p>H Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. 2017arXiv preprint</p>
<p>Knowing more about questions can help: Improving calibration in question answering. S Zhang, C Gong, E Choi, arXiv:2106.014942021arXiv preprint</p>
<p>Calibrate before use: Improving few-shot performance of language models. Z Zhao, E Wallace, S Feng, D Klein, S Singh, International conference on machine learning. 2021</p>
<p>Improving question answering by commonsensebased pre-training. W Zhong, D Tang, N Duan, M Zhou, J Wang, J Yin, Natural Language Processing and Chinese Computing: 8th CCF International Conference, NLPCC 2019. Dunhuang, ChinaOctober 9-14, 2019. 2019Proceedings, Part I 8</p>
<p>Batch calibration: Rethinking calibration for in-context learning and prompt engineering. H Zhou, X Wan, L Proleev, D Mincu, J Chen, K A Heller, S Roy, The Twelfth International Conference on Learning Representations. 2024</p>
<p>On the calibration of large language models and alignment. C Zhu, B Xu, Q Wang, Y Zhang, Z Mao, Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>J Zhu, Y Xia, L Wu, D He, T Qin, W Zhou, H Li, T.-Y Liu, arXiv:2002.06823Incorporating BERT into neural machine translation. 2020arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6398 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6398</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6398</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-267211725</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2401.14295v3.pdf" target="_blank">Demystifying Chains, Trees, and Graphs of Thoughts</a></p>
                <p><strong>Paper Abstract:</strong> —The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models’ (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM’s capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies , because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6398.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6398.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting paradigm that elicits multi-step intermediate reasoning chains in the model's output to improve final-answer accuracy on complex tasks, especially arithmetic and logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified large LLMs (evaluations reported across GPT/PaLM/LLaMA families in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (large LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (evaluations reported across small-to-very-large models; effectiveness increases with scale)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in detail in this paper; generally models trained on broad web, code, and text corpora (survey notes that larger models with broader data show better CoT rationales).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, SVAMP, MAWPS, SVAMP, MultiArith, DROP (surveyed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / grade-school math / discrete reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (open-answer), sometimes with few-shot exemplars containing rationales</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to harder math word problems; datasets include standard (GSM8K) and harder variants (GSM-hard, DROP)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot chain-of-thought (CoT) exemplars, occasionally multi-prompt chains or single-prompt CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact-match final answer) and rationale quality</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in-survey summary: CoT achieves ~60% on GSM8K (∼40 percentage points higher than IO baseline), ~80% on SVAMP (∼10 pp higher), ~90% on MAWPS (∼14 pp higher) — aggregated across referenced evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Survey cites analyses showing CoT success increases with model scale; theoretical work (latent-space models) suggests high-quality exemplars reduce ambiguity and make generated chain probability converge to true chain probability; evaluation work shows many chains are superficially plausible but can be incorrect, motivating verifiers and discriminative checks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Error accumulation along sequential steps (cascading mistakes), plausible-but-incorrect intermediate reasoning, sensitivity to exemplar quality and ordering, out-of-distribution generalization degradation (longer chains or longer numeric tokens degrade performance).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Performance and rationale fidelity improve with model size; CoT effectiveness increases with LLM scale (survey repeatedly notes larger models yield better rationales and higher accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6398.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot-CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot Chain-of-Thought prompting ("Let's think step by step")</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot instruction-style prompt (e.g., 'Let's think step by step') that elicits multi-step reasoning without hand-designed few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large Language Models Are Zero-Shot Reasoners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs evaluated in cited works (survey-level summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified here; same general pretraining regimes as other LLMs (broad web and text).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MultiArith, GSM8K, AQUA, SVAMP (survey cites dataset-level effects)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic / multi-step word problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions, zero-shot instruction appended (no exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to moderate complexity (datasets above)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot instruction ('Let's think step by step')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey notes zero-shot-CoT yields substantial gains over simple zero-shot IO on some arithmetic sets (e.g., reports a 60% gain on MultiArith vs IO in cited work), but does not consistently outperform few-shot CoT on arithmetic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Survey reports that zero-shot instructions can elicit chain-like outputs but tend to be weaker than few-shot exemplar CoT for arithmetic; effectiveness depends on model capability and prompt wording.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Often underperforms few-shot CoT on arithmetic; still susceptible to arithmetic mistakes and incomplete or imprecise chains.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Works better as model capacity increases, but even so typically lags best few-shot CoT performance for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6398.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (self-consistency sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sampling-based enhancement where multiple independent CoT chains are sampled and a consensus (majority vote or scoring) selects the final answer, leveraging stochasticity to improve correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs in surveyed experiments (sampling applied to base LLM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in survey beyond model-level notes</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and other arithmetic datasets (survey reports improvements when sampling multiple chains)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic word problems / multi-hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language questions; multiple sampled CoT outputs</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to multi-hop problems</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot CoT + multiple-sample self-consistency voting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (majority-vote selection of final answer)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey notes increased accuracy with more sampled chains; practical gains plateau after ~5–10 sampled chains (qualitative summary; exact numbers dataset- and model-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Self-consistency harnesses internal stochastic diversity to reduce reliance on a single chain; sampling more chains increases chance of at least one correct chain and voting/aggregation reduces effect of individual chain errors.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Increased token and compute cost; if all sampled chains share the same systematic error, voting will not help; poor verifiers or weak aggregation functions limit gains.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Gains from self-consistency are observed across model sizes but require sufficient model capacity to produce diverse valid reasoning paths; more benefit for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6398.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program of Thoughts prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that elicits code-like (programmatic) intermediate outputs from the model, then executes code to produce final numerical answers, separating computation from reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs capable of generating code (survey references code-capable models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (code-capable large LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Models with code in the pretraining mixture (survey notes code and program-aided approaches help numeric accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, AQUA, SVAMP, FinQA, ConvFinQA (surveyed results)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>numerical arithmetic and financial calculation problems (multi-step)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem → prompt to generate program (Python) then execute program for exact computation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to advanced arithmetic, financial reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot or zero-shot PoT (code-generation then execution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (exact numeric result after program execution)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports PoT (few-shot) improves GSM8K and AQUA by >8% and improves SVAMP by >4%; in financial datasets (FinQA/ConvFinQA) few-shot PoT reaches roughly ~20% improvement over CoT (as summarized in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Programmatic output decouples symbolic arithmetic computation (exact when executed) from natural-language reasoning, reducing numerical rounding and tokenization errors inherent to free-text arithmetic; survey highlights that executing generated code can correct arithmetic mistakes made in purely textual chains.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Errors arise if generated code is incorrect or malformed, or if the model fails to generate precise variable assignments; hallucinated library calls or unsafe code must be handled; reliance on tool execution introduces system integration complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>PoT benefits from models with strong code-generation capabilities and from executing code (tool integration) — improvements seen when model and execution environment are paired.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6398.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A tree-structured prompting/search scheme that explores multiple candidate 'thoughts' at each step, uses evaluation/voting to select promising branches, and supports backtracking and search strategies (BFS/DFS) for improved problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as generators and evaluators (survey-level; examples include GPT-family models in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified beyond model-level general pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, DROP, Game of 24, symbolic tasks (survey references ToT improvements on arithmetic and symbolic benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic reasoning, symbolic reasoning, planning and creative tasks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language tasks where intermediate nodes represent partial solutions; multi-prompt expansion and evaluation of nodes</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>moderate to hard (datasets like GSM8K, DROP; Game of 24; puzzles)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-prompt tree search with LLM acting as generator and often as state evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy of final answers; sometimes node-evaluation scores and search efficiency (tokens/calls)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey states ToT outperforms CoT on arithmetic datasets like GSM8K and DROP; precise gains vary by dataset and configuration (survey-level claim: ToT and GoT are winners in arithmetic reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>ToT's branching + evaluation mitigates sequential error accumulation by exploring alternative paths and backtracking; evaluation/voting inside the model can act as a lightweight verifier; trade-offs include higher token and latency cost due to multiple branches/samples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Computational cost (many prompts), difficulty selecting optimal branching factor, poor local evaluators lead to exploring many wrong branches, and risk of evaluation overfitting to surface features rather than mathematical correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>ToT benefits from stronger models that can both generate diverse high-quality child thoughts and reliably evaluate them; larger LLMs make evaluation and generation more effective but increase token cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6398.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general graph-structured reasoning paradigm that allows thoughts to have multiple parents and children (branching and aggregation), enabling dynamic programming–like aggregation of subsolutions to improve complex and multi-hop reasoning including arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph of Thoughts: Solving Elaborate Problems with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 and other LLMs cited in survey (GoT experiments reported with GPT-3.5 in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-scale and other large models (survey reports improvements using GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not explicitly described in survey for GoT; standard pretraining on web/text and possibly multi-modal in some variants</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>integer sorting, set intersections, keyword counting, GSM8K, ScienceQA, Game of 24 (surveyed GoT evaluations across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic, multi-hop retrieval/aggregation, symbolic transformations</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language tasks decomposed into graph-of-operations and Graph Reasoning State (multi-prompt graph construction and aggregation)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>moderate to hard arithmetic and compositional tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-prompt graph construction with proposer/verifier/aggregator modules; supports aggregation operations that combine sub-results</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy, token efficiency, and problem-size scaling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports GoT significantly improves over IO, CoT, and ToT on surveyed tasks (reported gains in referenced work), often achieving higher accuracy with comparable or fewer tokens; exact numeric gains are task- and implementation-dependent (survey-level claim).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Graph topologies allow aggregation of partial computations and help reduce error propagation by recombining multiple validated subresults; aggregation can implement DP-like recomposition and hedge against single-path failures.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Higher orchestration complexity, greater number of LLM calls if not carefully pruned, reliance on correct aggregation logic (errors in merging subsolutions can propagate), and need for robust verifiers to prevent adding incorrect nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>GoT advantages grow with problem size and complexity; larger models produce better node proposals and verifications, improving graph construction quality and final accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6398.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-Most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-Most Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decomposition-based multi-step prompting scheme that first decomposes a complex question into a list of sub-questions (from least to most difficult), then solves sub-questions sequentially using previously obtained answers in context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs reported in cited works</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified beyond general LLM pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DROP, GSM8K (especially multi-step subsets), SCAN and compositional tasks (surveyed examples)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and compositional reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language decomposition into sub-questions and stepwise solving</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>tasks with many inference steps (examples where >5 steps benefit noticeably)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>few-shot decomposition exemplars + sequential multi-prompt solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey notes Least-to-Most improves accuracy on datasets like DROP by ~10% and shows marked gains on problems requiring more than 5 inference steps; exact numbers vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Explicit decomposition reduces difficult global reasoning into simpler subproblems, limiting depth of reasoning per step and making verification easier; helps address error accumulation by restricting local reasoning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Quality of initial decomposition is critical — poor decompositions lead to poor final answers; increased total number of prompts/tokens; requires that subproblems be solvable in isolation.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Works well across model sizes but benefits more from stronger models that can produce reliable decompositions and solvers for subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6398.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6398.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PVS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan, Verify and Switch (PVS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid multi-method framework that plans a reasoning method (selecting among CoT, PoT, EoT, etc.), verifies computed answers (including executing checks), and if verification fails switches methods iteratively to improve arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used as planners and reasoners (survey-level summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not specified in survey; relies on tool-execution and program-aided data where applicable</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ALGEBRA, GSM-hard, MultiArith, other arithmetic benchmarks (survey cites improvements)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>arithmetic with large numbers, algebraic calculations, numerical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problem; planner selects reasoning method; verifier executes checks or computations</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>hard arithmetic and algebraic problems (e.g., GSM-hard, ALGEBRA)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>multi-method planning (CoT, PoT, EoT) + verifier module + iterative switching</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Survey reports PVS shows ~5.5% average improvement across math benchmarks and >10% improvements on ALGEBRA and GSM-hard (cited improvements in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Combining multiple reasoning paradigms plus explicit verification reduces systematic errors and compensates for weaknesses of any single method; verifier module (including tool execution) is key to detecting and correcting mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>If verifiers are weak or switch policy is poor, iterations may not converge; switching without memory may repeat prior errors; tool integration complexity and increased inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>More effective with stronger LLMs that can reliably plan and evaluate multiple reasoning methods and when paired with robust verifiers or external execution environments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Chains, Trees, and Graphs of Thoughts', 'publication_date_yy_mm': '2025-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models Are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models <em>(Rating: 2)</em></li>
                <li>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks <em>(Rating: 2)</em></li>
                <li>Tree of Thoughts: Deliberate Problem Solving with Large Language Models <em>(Rating: 2)</em></li>
                <li>Graph of Thoughts: Solving Elaborate Problems with Large Language Models <em>(Rating: 2)</em></li>
                <li>Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 1)</em></li>
                <li>Program-Aided Language Models (PAL) / related program-aided prompting works <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6398",
    "paper_id": "paper-267211725",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting paradigm that elicits multi-step intermediate reasoning chains in the model's output to improve final-answer accuracy on complex tasks, especially arithmetic and logical reasoning.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "unspecified large LLMs (evaluations reported across GPT/PaLM/LLaMA families in survey)",
            "model_family": "decoder-only transformer (large LLMs)",
            "model_size": "various (evaluations reported across small-to-very-large models; effectiveness increases with scale)",
            "training_data_description": "Not specified in detail in this paper; generally models trained on broad web, code, and text corpora (survey notes that larger models with broader data show better CoT rationales).",
            "benchmark_name": "GSM8K, SVAMP, MAWPS, SVAMP, MultiArith, DROP (surveyed)",
            "task_type": "multi-step arithmetic word problems / grade-school math / discrete reasoning",
            "problem_format": "natural-language word problems (open-answer), sometimes with few-shot exemplars containing rationales",
            "difficulty_level": "grade-school to harder math word problems; datasets include standard (GSM8K) and harder variants (GSM-hard, DROP)",
            "prompting_method": "few-shot chain-of-thought (CoT) exemplars, occasionally multi-prompt chains or single-prompt CoT",
            "performance_metric": "accuracy (exact-match final answer) and rationale quality",
            "performance_value": "Reported in-survey summary: CoT achieves ~60% on GSM8K (∼40 percentage points higher than IO baseline), ~80% on SVAMP (∼10 pp higher), ~90% on MAWPS (∼14 pp higher) — aggregated across referenced evaluations",
            "internal_analysis": "Survey cites analyses showing CoT success increases with model scale; theoretical work (latent-space models) suggests high-quality exemplars reduce ambiguity and make generated chain probability converge to true chain probability; evaluation work shows many chains are superficially plausible but can be incorrect, motivating verifiers and discriminative checks.",
            "failure_modes": "Error accumulation along sequential steps (cascading mistakes), plausible-but-incorrect intermediate reasoning, sensitivity to exemplar quality and ordering, out-of-distribution generalization degradation (longer chains or longer numeric tokens degrade performance).",
            "scaling_trend": "Performance and rationale fidelity improve with model size; CoT effectiveness increases with LLM scale (survey repeatedly notes larger models yield better rationales and higher accuracy).",
            "uuid": "e6398.0",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Zero-shot-CoT",
            "name_full": "Zero-shot Chain-of-Thought prompting (\"Let's think step by step\")",
            "brief_description": "A zero-shot instruction-style prompt (e.g., 'Let's think step by step') that elicits multi-step reasoning without hand-designed few-shot exemplars.",
            "citation_title": "Large Language Models Are Zero-Shot Reasoners",
            "mention_or_use": "use",
            "model_name": "various LLMs evaluated in cited works (survey-level summary)",
            "model_family": "decoder-only transformer",
            "model_size": "various",
            "training_data_description": "Not specified here; same general pretraining regimes as other LLMs (broad web and text).",
            "benchmark_name": "MultiArith, GSM8K, AQUA, SVAMP (survey cites dataset-level effects)",
            "task_type": "arithmetic / multi-step word problems",
            "problem_format": "natural-language questions, zero-shot instruction appended (no exemplars)",
            "difficulty_level": "grade-school to moderate complexity (datasets above)",
            "prompting_method": "zero-shot instruction ('Let's think step by step')",
            "performance_metric": "accuracy",
            "performance_value": "Survey notes zero-shot-CoT yields substantial gains over simple zero-shot IO on some arithmetic sets (e.g., reports a 60% gain on MultiArith vs IO in cited work), but does not consistently outperform few-shot CoT on arithmetic benchmarks.",
            "internal_analysis": "Survey reports that zero-shot instructions can elicit chain-like outputs but tend to be weaker than few-shot exemplar CoT for arithmetic; effectiveness depends on model capability and prompt wording.",
            "failure_modes": "Often underperforms few-shot CoT on arithmetic; still susceptible to arithmetic mistakes and incomplete or imprecise chains.",
            "scaling_trend": "Works better as model capacity increases, but even so typically lags best few-shot CoT performance for arithmetic tasks.",
            "uuid": "e6398.1",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "CoT-SC",
            "name_full": "Chain-of-Thought with Self-Consistency (self-consistency sampling)",
            "brief_description": "A sampling-based enhancement where multiple independent CoT chains are sampled and a consensus (majority vote or scoring) selects the final answer, leveraging stochasticity to improve correctness.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "mention_or_use": "use",
            "model_name": "various LLMs in surveyed experiments (sampling applied to base LLM outputs)",
            "model_family": "decoder-only transformer",
            "model_size": "various",
            "training_data_description": "Not specified in survey beyond model-level notes",
            "benchmark_name": "GSM8K and other arithmetic datasets (survey reports improvements when sampling multiple chains)",
            "task_type": "multi-step arithmetic word problems / multi-hop reasoning",
            "problem_format": "natural-language questions; multiple sampled CoT outputs",
            "difficulty_level": "grade-school to multi-hop problems",
            "prompting_method": "few-shot CoT + multiple-sample self-consistency voting",
            "performance_metric": "accuracy (majority-vote selection of final answer)",
            "performance_value": "Survey notes increased accuracy with more sampled chains; practical gains plateau after ~5–10 sampled chains (qualitative summary; exact numbers dataset- and model-dependent).",
            "internal_analysis": "Self-consistency harnesses internal stochastic diversity to reduce reliance on a single chain; sampling more chains increases chance of at least one correct chain and voting/aggregation reduces effect of individual chain errors.",
            "failure_modes": "Increased token and compute cost; if all sampled chains share the same systematic error, voting will not help; poor verifiers or weak aggregation functions limit gains.",
            "scaling_trend": "Gains from self-consistency are observed across model sizes but require sufficient model capacity to produce diverse valid reasoning paths; more benefit for larger models.",
            "uuid": "e6398.2",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PoT",
            "name_full": "Program of Thoughts prompting",
            "brief_description": "A prompting method that elicits code-like (programmatic) intermediate outputs from the model, then executes code to produce final numerical answers, separating computation from reasoning.",
            "citation_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "mention_or_use": "use",
            "model_name": "LLMs capable of generating code (survey references code-capable models)",
            "model_family": "decoder-only transformer",
            "model_size": "various (code-capable large LLMs)",
            "training_data_description": "Models with code in the pretraining mixture (survey notes code and program-aided approaches help numeric accuracy)",
            "benchmark_name": "GSM8K, AQUA, SVAMP, FinQA, ConvFinQA (surveyed results)",
            "task_type": "numerical arithmetic and financial calculation problems (multi-step)",
            "problem_format": "natural-language problem → prompt to generate program (Python) then execute program for exact computation",
            "difficulty_level": "grade-school to advanced arithmetic, financial reasoning",
            "prompting_method": "few-shot or zero-shot PoT (code-generation then execution)",
            "performance_metric": "accuracy (exact numeric result after program execution)",
            "performance_value": "Survey reports PoT (few-shot) improves GSM8K and AQUA by &gt;8% and improves SVAMP by &gt;4%; in financial datasets (FinQA/ConvFinQA) few-shot PoT reaches roughly ~20% improvement over CoT (as summarized in survey).",
            "internal_analysis": "Programmatic output decouples symbolic arithmetic computation (exact when executed) from natural-language reasoning, reducing numerical rounding and tokenization errors inherent to free-text arithmetic; survey highlights that executing generated code can correct arithmetic mistakes made in purely textual chains.",
            "failure_modes": "Errors arise if generated code is incorrect or malformed, or if the model fails to generate precise variable assignments; hallucinated library calls or unsafe code must be handled; reliance on tool execution introduces system integration complexity.",
            "scaling_trend": "PoT benefits from models with strong code-generation capabilities and from executing code (tool integration) — improvements seen when model and execution environment are paired.",
            "uuid": "e6398.3",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree of Thoughts",
            "brief_description": "A tree-structured prompting/search scheme that explores multiple candidate 'thoughts' at each step, uses evaluation/voting to select promising branches, and supports backtracking and search strategies (BFS/DFS) for improved problem solving.",
            "citation_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLMs used as generators and evaluators (survey-level; examples include GPT-family models in cited work)",
            "model_family": "decoder-only transformer",
            "model_size": "various",
            "training_data_description": "Not specified beyond model-level general pretraining",
            "benchmark_name": "GSM8K, DROP, Game of 24, symbolic tasks (survey references ToT improvements on arithmetic and symbolic benchmarks)",
            "task_type": "arithmetic reasoning, symbolic reasoning, planning and creative tasks",
            "problem_format": "natural-language tasks where intermediate nodes represent partial solutions; multi-prompt expansion and evaluation of nodes",
            "difficulty_level": "moderate to hard (datasets like GSM8K, DROP; Game of 24; puzzles)",
            "prompting_method": "multi-prompt tree search with LLM acting as generator and often as state evaluator",
            "performance_metric": "accuracy of final answers; sometimes node-evaluation scores and search efficiency (tokens/calls)",
            "performance_value": "Survey states ToT outperforms CoT on arithmetic datasets like GSM8K and DROP; precise gains vary by dataset and configuration (survey-level claim: ToT and GoT are winners in arithmetic reasoning).",
            "internal_analysis": "ToT's branching + evaluation mitigates sequential error accumulation by exploring alternative paths and backtracking; evaluation/voting inside the model can act as a lightweight verifier; trade-offs include higher token and latency cost due to multiple branches/samples.",
            "failure_modes": "Computational cost (many prompts), difficulty selecting optimal branching factor, poor local evaluators lead to exploring many wrong branches, and risk of evaluation overfitting to surface features rather than mathematical correctness.",
            "scaling_trend": "ToT benefits from stronger models that can both generate diverse high-quality child thoughts and reliably evaluate them; larger LLMs make evaluation and generation more effective but increase token cost.",
            "uuid": "e6398.4",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "GoT",
            "name_full": "Graph of Thoughts",
            "brief_description": "A general graph-structured reasoning paradigm that allows thoughts to have multiple parents and children (branching and aggregation), enabling dynamic programming–like aggregation of subsolutions to improve complex and multi-hop reasoning including arithmetic.",
            "citation_title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 and other LLMs cited in survey (GoT experiments reported with GPT-3.5 in referenced work)",
            "model_family": "decoder-only transformer",
            "model_size": "GPT-3.5-scale and other large models (survey reports improvements using GPT-3.5)",
            "training_data_description": "Not explicitly described in survey for GoT; standard pretraining on web/text and possibly multi-modal in some variants",
            "benchmark_name": "integer sorting, set intersections, keyword counting, GSM8K, ScienceQA, Game of 24 (surveyed GoT evaluations across tasks)",
            "task_type": "arithmetic, multi-hop retrieval/aggregation, symbolic transformations",
            "problem_format": "natural-language tasks decomposed into graph-of-operations and Graph Reasoning State (multi-prompt graph construction and aggregation)",
            "difficulty_level": "moderate to hard arithmetic and compositional tasks",
            "prompting_method": "multi-prompt graph construction with proposer/verifier/aggregator modules; supports aggregation operations that combine sub-results",
            "performance_metric": "accuracy, token efficiency, and problem-size scaling",
            "performance_value": "Survey reports GoT significantly improves over IO, CoT, and ToT on surveyed tasks (reported gains in referenced work), often achieving higher accuracy with comparable or fewer tokens; exact numeric gains are task- and implementation-dependent (survey-level claim).",
            "internal_analysis": "Graph topologies allow aggregation of partial computations and help reduce error propagation by recombining multiple validated subresults; aggregation can implement DP-like recomposition and hedge against single-path failures.",
            "failure_modes": "Higher orchestration complexity, greater number of LLM calls if not carefully pruned, reliance on correct aggregation logic (errors in merging subsolutions can propagate), and need for robust verifiers to prevent adding incorrect nodes.",
            "scaling_trend": "GoT advantages grow with problem size and complexity; larger models produce better node proposals and verifications, improving graph construction quality and final accuracy.",
            "uuid": "e6398.5",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "Least-to-Most",
            "name_full": "Least-to-Most Prompting",
            "brief_description": "A decomposition-based multi-step prompting scheme that first decomposes a complex question into a list of sub-questions (from least to most difficult), then solves sub-questions sequentially using previously obtained answers in context.",
            "citation_title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "mention_or_use": "use",
            "model_name": "various LLMs reported in cited works",
            "model_family": "decoder-only transformer",
            "model_size": "various",
            "training_data_description": "Not specified beyond general LLM pretraining",
            "benchmark_name": "DROP, GSM8K (especially multi-step subsets), SCAN and compositional tasks (surveyed examples)",
            "task_type": "multi-step arithmetic and compositional reasoning",
            "problem_format": "natural-language decomposition into sub-questions and stepwise solving",
            "difficulty_level": "tasks with many inference steps (examples where &gt;5 steps benefit noticeably)",
            "prompting_method": "few-shot decomposition exemplars + sequential multi-prompt solving",
            "performance_metric": "accuracy",
            "performance_value": "Survey notes Least-to-Most improves accuracy on datasets like DROP by ~10% and shows marked gains on problems requiring more than 5 inference steps; exact numbers vary by dataset.",
            "internal_analysis": "Explicit decomposition reduces difficult global reasoning into simpler subproblems, limiting depth of reasoning per step and making verification easier; helps address error accumulation by restricting local reasoning complexity.",
            "failure_modes": "Quality of initial decomposition is critical — poor decompositions lead to poor final answers; increased total number of prompts/tokens; requires that subproblems be solvable in isolation.",
            "scaling_trend": "Works well across model sizes but benefits more from stronger models that can produce reliable decompositions and solvers for subtasks.",
            "uuid": "e6398.6",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        },
        {
            "name_short": "PVS",
            "name_full": "Plan, Verify and Switch (PVS)",
            "brief_description": "A hybrid multi-method framework that plans a reasoning method (selecting among CoT, PoT, EoT, etc.), verifies computed answers (including executing checks), and if verification fails switches methods iteratively to improve arithmetic reasoning.",
            "citation_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
            "mention_or_use": "use",
            "model_name": "LLMs used as planners and reasoners (survey-level summary)",
            "model_family": "decoder-only transformer",
            "model_size": "various",
            "training_data_description": "Not specified in survey; relies on tool-execution and program-aided data where applicable",
            "benchmark_name": "ALGEBRA, GSM-hard, MultiArith, other arithmetic benchmarks (survey cites improvements)",
            "task_type": "arithmetic with large numbers, algebraic calculations, numerical reasoning",
            "problem_format": "natural-language problem; planner selects reasoning method; verifier executes checks or computations",
            "difficulty_level": "hard arithmetic and algebraic problems (e.g., GSM-hard, ALGEBRA)",
            "prompting_method": "multi-method planning (CoT, PoT, EoT) + verifier module + iterative switching",
            "performance_metric": "accuracy (final-answer correctness)",
            "performance_value": "Survey reports PVS shows ~5.5% average improvement across math benchmarks and &gt;10% improvements on ALGEBRA and GSM-hard (cited improvements in survey).",
            "internal_analysis": "Combining multiple reasoning paradigms plus explicit verification reduces systematic errors and compensates for weaknesses of any single method; verifier module (including tool execution) is key to detecting and correcting mistakes.",
            "failure_modes": "If verifiers are weak or switch policy is poor, iterations may not converge; switching without memory may repeat prior errors; tool integration complexity and increased inference cost.",
            "scaling_trend": "More effective with stronger LLMs that can reliably plan and evaluate multiple reasoning methods and when paired with robust verifiers or external execution environments.",
            "uuid": "e6398.7",
            "source_info": {
                "paper_title": "Demystifying Chains, Trees, and Graphs of Thoughts",
                "publication_date_yy_mm": "2025-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models Are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_disentangling_computation_from_reasoning_for_numerical_reasoning_tasks"
        },
        {
            "paper_title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "rating": 2,
            "sanitized_title": "graph_of_thoughts_solving_elaborate_problems_with_large_language_models"
        },
        {
            "paper_title": "Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts",
            "rating": 2,
            "sanitized_title": "plan_verify_and_switch_integrated_reasoning_with_diverse_xofthoughts"
        },
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Program-Aided Language Models (PAL) / related program-aided prompting works",
            "rating": 1,
            "sanitized_title": "programaided_language_models_pal_related_programaided_prompting_works"
        }
    ],
    "cost": 0.027444499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Demystifying Chains, Trees, and Graphs of Thoughts
5 Sep 2025</p>
<p>Maciej Besta 
Florim Memedi 
Zhenyu Zhang 
Robert Gerstenberger 
Guangyuan Piao 
Nils Blach 
Piotr Nyczyk 
Marcin Copik 
Grzegorz Kwaśniewski 
Lukas Gianinazzi 
Ales Kubicek 
Hubert Niewiadomski 
Aidan O'mahony 
Onur Mutlu 
Torsten Hoefler 
Eth Zurich 
Dell 
Cledar 
Basf Se 
Demystifying Chains, Trees, and Graphs of Thoughts
5 Sep 202578B1B2E22A2E5D5C0A1AD9ED8152EFF7arXiv:2401.14295v5[cs.CL]Prompt EngineeringPromptingPrompting TopologyChain-of-ThoughtTree of ThoughtsGraph of ThoughtsLarge Language ModelsLLMsVision-Language ModelsVLMsGenerative AIChain PromptingTree PromptingGraph Prompting
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques.Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph.As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing.To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes.For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts.We then build the first taxonomy of structure-enhanced LLM reasoning schemes.We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others.We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context.Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost.We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges.Our work will help to advance future prompt engineering techniques.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have become a dominant tool in modern machine learning (ML).Originating from simple Natural Language Processing (NLP) tasks [34], [85], [159], their far-extending potential has been quickly applied in other fields, such as logical reasoning [51], planning [197], medicine [186], and many others.Since the primary communication medium with LLMs is natural language, prompt engineering has become a new area of study that gained widespread attention and importance [158], [206].First, it democratizes the access to LLMs and to the overall generative AI landscape, by being easy to use and try by anybody.Second, it is cost-effective and does not require fine-tuning or pre-training, which are expensive and time-consuming.</p>
<p>Crafting LLM queries to increase both the accuracy of outcomes as well as cost-effectiveness in tasks such as logical or algebraic queries is challenging.Despite continuous advances in the size and cognitive power of LLMs, solving elaborate tasks with a single straightforward prompt yields imprecise or plain incorrect results due to the left-to-right, one-token-at-a-time nature of generative Transformer models [199].Therefore, recent works focus on guiding LLMs towards the final solution through intermediate steps.Examples of such schemes include Chain-of-Thought (CoT) [199], Tree of Thoughts (ToT) [217], Graph of Thoughts (GoT) [10], AutoGPT [162], ReAct [218], or LLMCompiler [111].This † Corresponding author line of works increases the performance of the LLM reasoning.</p>
<p>Yet, despite all these advancements, state-of-the-art schemes still exhibit numerous limitations.First, they are still limited to simple tasks such as Game of 24 -it is critical to further enhance prompting to enable solving complex multifaceted tasks.Moreover, state-of-the-art prompting schemes often entail high inference costs [10], [217].Third, designing, developing, maintaining, and extending these schemes is hard.On one hand, this is due to the rapid development and enrichment of the "LLM ecosystem" that must be seamlessly integrated into the prompting pipeline.This includes retrieval-augmented generation (RAG), accessing the Internet, executing Python scripts, fine-tuning, and others.On other hand, different concepts related to the LLM reasoning are not well-defined, hindering effective design of new more powerful schemes.For example, while many schemes rely on the notion of the LLM thought, it is not clear how it relates to concepts such as a prompt.</p>
<p>To address the above issues, we first identify and crystallize fundamental building blocks and concepts in the general prompt execution pipeline.Then, we analyze and clarify these blocks and concepts in the context of recent schemes such as CoT, ToT, and GoT (contribution #1).Our study is based on a broad analysis of recent works on LLM reasoning.Then, we use the gained insights to develop a general blueprint and a taxonomy of the LLM reasoning schemes, focusing on how the underlying structure of reasoning can be used to facilitate more efficient, effective, and productive prompting (contribution #2).For this, we observe that the reasoning process in many recent prompting schemes can be modeled as a graph.While the nature of interacting with the LLM is temporal, the representation of the graph structure behind the LLM reasoning is periodically merged with the LLM context, becoming -to a degree -spatial, thus forming different topologies.These topologies can be a plain path graph (as in CoT [199]), multiple parallel path graphs with a single root (as in CoT with Self-Consistency) [194], a tree (as in ToT [217]), or an arbitrary graph (as in GoT [10]).We then use our taxonomy to survey and analyze existing prompting schemes (contribution #3).</p>
<p>We dissect these schemes into fundamental aspects such as the class of graphs (i.e., the topology) used to model the reasoning process, the representation of this reasoning, or the encoding of the reasoning schedule.We focus on investigating which classes of schemes offer more performance in terms of the accuracy of predictions, the latency of execution, or the cost effectiveness (contribution #4).We finally list open challenges and potential for novel research directions (contribution #5).</p>
<p>EVOLUTION OF REASONING TOPOLOGIES</p>
<p>We first summarize the evolution of reasoning topologies; see Figure 1 for an overview.For the sake of brevity, we do not yet precisely define the used terminology, instead relying on terms broadly used in the literature.In Sections 3-4, we introduce and discuss precise naming.</p>
<p>In the basic Input-Output (IO) prompting, the LLM provides a final reply immediately upon receiving the user initial prompt.There are no intermediate steps in the LLM reasoning.Chain topologies, introduced in Chain-of-Thought by Wei et al. [199], improve upon IO prompting by incorporating explicit intermediate "steps of reasoning" in addition to the input and output.Chain-of-Thought with Self-Consistency (CoT-SC) [194] improves upon CoT by introducing several independent reasoning chains, originating from the same initial input.Then, the best outcome from the final thoughts is chosen, according to a predefined function S. The driving idea is to harness the randomness within the LLM reasoning, as it can generate different thoughts from the same prompt.</p>
<p>Tree of Thoughts (ToT) [134], [217] elevates the CoT limitations by allowing prompt branching at any point of the chain of thoughts.Therefore, different exploration paths are not fundamentally independent, like in CoT-SC, but a chain of thoughts can branch during the reasoning process to explore different options.A single tree node represents a partial solution.Based on a given node, the thought generator constructs a given number k of new nodes.Then, the state evaluator generates scores for each such new node.Depending on the use case, the evaluation could be conducted using an LLM itself, or it can harness human scores.Finally, the schedule of extending the tree is dictated by the utilized search algorithm (e.g., BFS or DFS).</p>
<p>Finally, Graph of Thoughts (GoT) [10] enables arbitrary reasoning dependencies between generated thoughts.Similarly to ToT, every thought can generate multiple child thoughts.However, each thought can also have multiple parents, which can form an aggregation operation.GoT, allowing both branching (thoughts with out-degree &gt; 1) and aggregation (thoughts with in-degree &gt; 1) operations, can express -for example -reasoning patterns resembling dynamic programming, where GoT subgraphs are responsible for solving subproblems, which are then combined to form a final solution.</p>
<p>ESSENCE OF GENERAL PROMPT EXECUTION</p>
<p>We first summarize general prompt execution by giving a detailed overview of the prompting pipeline (Section 3.1) and then establishing a functional formulation for any prompting scheme (Section 3.2).This formulation will facilitate our subsequent analysis of reasoning topologies.pre-prompt", using tools, checking for compliance, avoiding exploits, RAG, quality enhancements, ...</p>
<p>Basic Prompting Pipeline</p>
<p>Figure 2 summarizes the prompting pipeline.On the left side of the figure, it shows a high-level user-AI interaction, which consists of a series of exchanges of information between the user and the LLM infrastructure ; the user sends prompts while the LLM sends back replies .An individual i-th prompting interaction, pictured in the main central part of Figure 2, starts with the user sending a prompt p i .Before being fed to the model, the prompt can be preprocessed by the LLM provider 1 , becoming p i .This could include adding a "system pre-prompt" or some additional metadata &lt;/&gt; , checking for the compliance with some policies, enhancing the prompt quality, conducting retrieval augmentation , or including outcomes from running external tools such as Python scripts or accessing the Internet .The preprocessed prompt is added to the LLM context 2 , which then is fed to the model 3 .This results in the autoregressive output generation 4 .</p>
<p>The model is usually frozen such that its weights do not change.In some considered prompting schemes, the model can also be fine-tuned .The output o i is potentially post-processed 5 , which could involve running o i through additional neural layers (e.g., for sentiment analysis) or other forms of post-processing on the provider side, such as checking for NSFW, adding more metadata &lt;/&gt; , and other operations.The post-processed output o i is also added to the context 6 and it is sent back to the user as a reply 8</p>
<p>. Note that o i could also be potentially fed back to the model directly, for additional iterations, before getting back to the user 7 .</p>
<p>Functional Formulation &amp; Building Blocks</p>
<p>We formalize the basic prompting pipeline from Section 3.1.This allows us to crystallize its fundamental building blocks, facilitating future optimizations and propelling both efficient and effective designs.The fundamental functional building blocks are f pre (for the prompt preprocessing 1 ), f post (for the post-processing of the LLM output 5 ), LLM (for the auto-generative LLM execution 4 ), f c (for deter- mining how the context is updated in stage 2 and f ′ c (for determining how the context is updated in stage 6 ).For this, we observe that the i-th prompting interaction (for i = 1, ... and c ′ 0 = {}) can be formally described as
p i = f pre (p i ) (1) c i = f c ( p i , c ′ i−1 )(2)o i = LLM X (c i ) (3) o i = f post (o i ) (4) c ′ i = f ′ c (c i , o i )(5)
where</p>
<p>• p i is the user prompt in the i-th prompting interaction, • f pre (p i ) is a preprocessing transformation applied to p i .It may involve Retrieval Augmented Generation (RAG), executing a script, accessing the Internet, and using other tools, • p i is the preprocessed version of the i-th prompt, • c i is the context at the beginning of the i-th prompting interaction (after executing f c ), • c ′ i is the context after executing f ′ c of the i-th prompting interaction (note that in the actual implementation, c i and c ′ i would be referring to the same data structure); note that c ′ 0 = {}, • o i is the output of the auto-generative execution of a given LLM X (e.g., LLaMA), • f post (o i ) is a post-processing transformation applied to o i ; this may involve additional neural layers (e.g., for sentiment analysis), checking for compliance with guidelines, and others.While most existing schemes do not focus on this part, we expect that in the future, the post-processing transformation could also -similarly to preprocessing -involve executing a script, accessing the Internet, RAG, and others, • o i is the post-processed LLM output o i ; o i = f post (o i ),</p>
<p>• f c , f ′ c are transformations that determine the exact form of updating the context.</p>
<p>Implementing Building Blocks</p>
<p>The provided building blocks can serve as the basis for productive implementations of prompting baselines on different architectures.For example, scheduling different parts of the prompting pipeline in the cloud setting could be done using the granularity of these blocks: a lightweight post-processing f post could execute a fast function, while a longer and stateful RAG operation within f pre could be automatically placed on EC2.</p>
<p>The details of f pre , LLM X , f post , and f c depend on the specific LLM infrastructure.In general, they can be used to implement different parts of the generative AI ecosystem.For example, most of RAG-based frameworks would implement RAG in f pre .Similarly, adding a system pre-prompt can be implemented as a part of f pre .The details of how the context is updated, or how some of its parts are removed when the input length reaches its limit, are specified in f c .</p>
<p>In many cases, it is the user responsibility to specify the behavior of f pre , f post , f c ; this is the case -for example -with LLaMA or when using the OpenAI API.Contrarily, when interacting with commercial services such as ChatGPT, these transformations are defined and implemented on the LLM infrastructure side.</p>
<p>ESSENCE OF REASONING TOPOLOGIES</p>
<p>We now crystallize different concepts in the area of reasoning topologies, and relate it to the general prompting pipeline and the functional formulation from Section 3.</p>
<p>What Is a Thought and a Reasoning Topology?</p>
<p>Many works use the term "thought".Yet, its precise meaning can differ, depending on the setting.For example, in CoT, a thought refers to a statement within a paragraph that contains a part of the reasoning process aimed at solving the input task.We show this in the top part of Figure 3.In ToT, in some tasks, such as Game of 24, a thought means an intermediate or a final solution to the initial question.However, in creative writing, it could be a plan of solving the input task, or a passage of text.In GoT, a thought contains a solution of the input task (or of its subtask).For example, it can be a subset of documents to be summarized, or a sequence of numbers to be sorted.</p>
<p>To encompass all these cases, we define a thought to be a semantic unit of task resolution, i.e., a step in the process of solving a given task.All the above examples fall into this definition: a step in task resolution can be a statement, a plan, a text passage, a set of documents, or a sequence of numbers.We model thoughts with nodes; edges between nodes correspond to dependencies between these thoughts.The details of these dependencies are also use case specific.For example, when generating a paragraph of text, if a given paragraph y is a refined version of a previous version x, then x and y become nodes in the topology, and there is an edge from x to y indicating that y depends on x.If the task is to sort a sequence of numbers, and the strategy is based on splitting the sequence into sub-sequences, sorting them independently, and merging, then the initial sequence could be modeled as a node x, and the sub-sequences would form further nodes y, z, ..., with edges (x, y), (x, z), ... from x to all the nodes modeling sub-sequences.Now, a reasoning topology is a graph of these nodes and edges.</p>
<p>Formally, a topology can be defined as G = (V, E), where V is a set of nodes modeling thoughts, and E is a set of edges between these nodes, modeling reasoning dependencies between thoughts.Such a graph-theoretic approach to reason about chains, trees, and graphs of thoughts facilitates devising more efficient reasoning schemes.For example, when aiming for minimizing the latency of solving a given task, one would attempt to devise a topology with low distance between the input and output nodes.</p>
<p>Please note, that the concept of a thought is also not clearly defined in the field of psychology, but in general refers either to the mental representations of outside objects [170] or self-generated mental visualizations in, e.g., daydreaming [175].This partially aligns with our definition as a semantic unit in a reasoning process (as mental representations or visualizations usually carry certain welldefined semantic meaning).</p>
<p>Semantic Roles of Thoughts &amp; Topologies</p>
<p>Graph nodes can model different aspects of reasoning.For example, in writing tasks, some nodes model plans of writing a paragraph, while other nodes model the actual paragraphs of text.We refer to such aspects as different semantic roles.As already observed in the prompting literature [10], semantic roles can also be modeled with graph theory, namely with heterogeneous graphs.This enables harnessing a powerful machinery for novel LLM reasoning works.For example, one could consider using some of the heterogeneous graph learning methods [140], [182], [224] in future prompting approaches.</p>
<p>Fundamental Use Cases of Thoughts &amp; Topologies</p>
<p>We identify two fundamental use cases of thoughts and topologies: in-context examples and reasoning steps that bring us towards a solution.In a topology of thoughts, a node v is reachable from another node u, if there exists a path from u to v. If a node is reachable from the node modeling the input task statement, we call such a node a solution node, and the corresponding topology is a solution topology.However, certain nodes may be not reachable from the input node.For example, a user may provide (in their prompt) in-context examples that form a small topology, which are not a step in the reasoning towards solving the input task, but merely examples.We will refer to such thoughts and topologies as thoughts and topologies of in-context examples.Examples of the two use cases can be found in Figure 3 The answer is 11.</p>
<p>A:</p>
<p>The cafeteria has 23 apples originally.</p>
<p>They used 20 to make lunch.</p>
<p>So they had 23 -20 = 3.</p>
<p>They bought 6 more apples, so they have 3 + 6 = 9.</p>
<p>The answer is 9. Solution topologies, on the other hand, can span across many prompts and replies.</p>
<p>Distinguishing between solution and in-context example thoughts and topologies can enable more effective and efficient LLM reasoning schemes.For example, a graph topology has to be represented in a certain way.Now, knowing that topologies of in-context examples are usually limited to a single prompt, while solution topologies usually stretch beyond an individual prompt or LLM reply, one could use different representations for these two topology classes, in order to minimize token utilization in each of them.</p>
<p>Both topologies can collectively be modeled also as a graph, with multiple components corresponding to topologies that are not connected with one another.To further facilitate future optimizations, one could potentially harness a hypervertex model [31], in which arbitrary subgraphs can be modeled as individual nodes called hypervertices.In such a view, one could model each separate topology of in-context examples as a hypervertex, connected with other nodes or hypervertices with hyperedges.This approach could be harnessed to provide a theoretical framework for optimizing the holistic performance of a reasoning scheme, including its ingredients such as in-context examples, in relation to all other scheme ingredients.</p>
<p>Functional Formulation of Reasoning Topologies</p>
<p>The LLM reasoning that harnesses topologies is formulated in the exactly same way as described in Section 3.However, one has to take into account the fact that prompts p i , replies o i , and context c i in general, they all contain thoughts and their dependencies.Now, the exact way in which a topology is mapped to p i , o i , and c i , depends on a specific prompting scheme.For example, in the i-th prompting interaction of CoT, a reasoning topology T is a subset (potentially a proper subset) of either the LLM reply o i (when T is a solution topology), or of the user prompt p i , as an in-context example (when T is a topology of incontext examples).In many tree and graph based schemes, however, this mapping is not so simple, and a topology can span across multiple prompts and replies.We illustrate these examples in Figure 3.</p>
<p>When developing an LLM reasoning scheme that harnesses reasoning topologies, one needs to specify the details of such mappings, but also build the representation of that topology, the schedule of traversing the topology, and many others.To facilitate devising future LLM reasoning schemes, we now provide a blueprint that clearly defines all these aspects and how they can be instantiated.</p>
<p>A Blueprint for LLM Reasoning</p>
<p>We identify the following fundamental aspects of an LLM reasoning scheme that harnesses a topology: ❶ topology class (the structure of connections between intermediate steps of the LLM reasoning, Section 4.5.1),❷ topology scope (the mapping between the topology and prompts/replies/context, Section 4.5.1),❸ topology representation (how a given topology is represented within a prompt/reply/context, Section 4.5.1),❹ topology derivation (how a given topology is obtained, Section 4.5.1),❺ reasoning schedule (how is a given topology traversed to conduct the LLM reasoning, Section 4.5.2),❻ schedule representation (how is a given schedule represented within a prompt/thought, Section 4.5.2), and ❼ harnessed parts of the AI pipeline (what parts of the generative AI pipeline, beyond prompting, are used, Section 4.5.3).We picture the blueprint in Figure 4, and we analyze existing schemes with respect to this blueprint 1 , in Table 1.</p>
<p>The provided blueprint and taxonomy are applicable to both solution and in-context example topologies.For example, a topology modeling an individual in-context example can have its own representation, schedule, etc..However, for clarity, we will focus on applying the blueprint and taxonomy mostly to solution topologies.</p>
<p>Topology of Reasoning</p>
<p>A reasoning scheme can harness different topologies for the LLM reasoning process.Here, we distinguish chains, trees, and graphs.Note that both a chain and a tree are each a special case of a connected graph: a tree is an acyclic connected undirected graph, and a chain is a path graph (i.e., a tree where each node has at most one child); see Part ❶ of Figure 4. Still, we treat them separately because they differ in their effectiveness for different prompting tasks [10], [134], [217].Under this view, the plain IO prompt can be viewed as a single node graph.</p>
<p>Second, we observe that these topologies can be harnessed within an individual prompt or a reply (single-prompt topology), but also across prompts or replies (multi-prompt topology); see Part ❷ of Figure 4.</p>
<p>An important aspect is the representation of the topology, see Part ❸ of Figure 4.The representation can be implicit (the nodes and edges are not specified explicitly) or explicit (nodes and edges are stated explicitly).Explicit representations vary and include a set of triples [17] or a description of nodes and edges in natural text.Implicit representation depends on a scheme -for example, it could be a textual recipe that prescribes generating the next reasoning steps.Finally, we also identify how the topology is derived -for example, it can be constructed by the user or by the LLM itself (Part ❹ of Figure 4).Specifically, derivation of a multi-prompt topology can be manual (fixed by user before the LLM reasoning), automatic (decided dynamically by the LLM), or semi-automatic (the overall reasoning structure is predefined before the LLM reasoning starts, but the user/LLM have some control over the structure as well during the actual reasoning).</p>
<p>Reasoning Schedule</p>
<p>The reasoning topology forms the "skeleton" for the LLM reasoning, effectively prescribing the algorithm for solving a given task.However, for a given fixed topology, many prompting schemes offer different approaches for the execution of the intermediate reasoning steps.For example, ToT harnesses Breadth-First Search (BFS) or Depth-First Search (DFS).This motivates us to introduce another dimension of structure-enhanced reasoning, namely the reasoning schedule.This schedule prescribes how the topology of reasoning is going to be processed (Part ❺ of Figure 4).Whenever the schedule is specified, it can be represented in different ways.It can be a description in a natural language, a code specification, in-context examples, or others (Part ❻ of Figure 4).Finally, as with the topology, the schedule itself could also be determined using different methods, for example by the LLM on-the-fly, or pre-determined (e.g., as the fixed BFS schedule).</p>
<p>Beyond Prompting</p>
<p>Many schemes go beyond pure prompting LLMs.This may include pre-training, fine-tuning, retrieval, tools, or different modalities (Part ❼ of Figure 4).We also consider this aspect, as it provides insights into the integration of reasoning topologies with other mechanisms in the AI pipeline beyond plain prompting interactions.</p>
<p>REASONING WITH CHAINS</p>
<p>We now proceed to investigate in more detail individual schemes that use chain topologies.We analyze these works with respect to our blueprint and taxonomy in the top part of Table 1 (detailed descriptions of each individual scheme are provided in the appendix) We also illustrate fundamental concepts introduced in these works, namely multi-step reasoning, zero-shot reasoning, planning &amp; task decomposition, task preprocessing, iterative refinement, and tool utilization.We finish this section with a comparative analysis and illustrations of example topology representations.</p>
<p>Multi-Step Reasoning</p>
<p>The concept of multi-step reasoning was first introduced through the seminal Chain-of-Thought (CoT) [199], a single-prompt scheme, which uses topologies of in-context  [120] ??</p>
<p>? graph E (S)A DFS E ?text Graph-of-Thought (GoT) [219] graph
I (text) M chain E M linear E M Ȝ text,image ControlLLM [133] graph E (json) M graph E M DFS E M †
Ȝ text,image,video,audio † can be used Cumulative Reasoning [229] graph (DAG) I (text) M graph (DAG) E SA arbitrary E M text Everything of Thoughts (XoT) [57] graph ?</p>
<p>L chain E M linear E M Ȝ text ResPrompt [100] graph "Reasoning Schedule": Details of the harnessed reasoning schedule, including its specific Scheme, Representation, and Derivation."AI pipeline": Does a given scheme harness parts of the AI pipeline beyond prompting?If yes, which ones? ("P": pre-training, "F": fine-tuning, "R": retrieval, 'T": tools, "Modalities": modalities).
I (text) M - - - - - - text Hypergraph-of-Thought (HoT) [216] hypergraph ? M - - - - - - Ȝ text,image BatchPrompt [125] batch E (text) M chain E M linear E M text Memory Injections [165] - - - - - - - - - text
When describing representations, we use the following abbreviations: "E": explicit, "I": implicit.When describing derivation, we use the following abbreviations: "A": automatic, "L": learned, "M": manual, "SA": semi-automatic."Ȝ": full support (i.e., YES), "Ȟ": partially [supported], " ": no support (i.e., NO).</p>
<p>examples, also known as few-shot examples, to guide the LLM into reasoning step-by-step before providing the final answer.Different following works augment or adapt incontext examples to elicit different forms of reasoning steps, while still relying on the single-prompt chain topology.For instance, instead of only providing a step-by-step reasoning chain in the examples, SelfAsk [154] expands each step in the chain to also pose a follow-up question that is then answered in the subsequent step.Similarly, Program of Thoughts (PoT) [41] uses code examples, rather than the natural-language-based examples as in CoT, to obtain a stepby-step generated, functional Python program that can be executed to get the final result.</p>
<p>Zero-Shot Reasoning Instructions</p>
<p>Zero-shot reasoning instructions aim to elicit the same multi-step reasoning chains, but without the use of handtuned, problem-specific in-context examples, i.e., they eliminate chain topologies forming in-context examples.Zeroshot-CoT [113], an extension to CoT, achieves this by simply prompting the LLM with one sentence, "Let's think step by step", or using other similar statements.Along the same lines, PoT can also leverage zero-shot reasoning instruction, e.g., "Let's write a Python program step by step and return the result.Firstly we need to define the variables.".</p>
<p>Planning &amp; Task Decomposition</p>
<p>Both planning and task decomposition aim to break down a task into a number of manageable sub-tasks that help reaching the final solution.Plan-and-Solve (PS) Prompting [192], one of the key single-prompt schemes building on this concept, first divides the complex task into a chain of sub-tasks and then executes these step-by-step for the final solution.PS operates in a zero-shot, multi-step manner, thus also relies on the previous two concepts as well.</p>
<p>Planning and decomposition is also frequently used in multi-prompt chains.The introduction of a node for specifying the details of the decomposition at the start of a multiprompt reasoning chain, usually not only determines the chain's depth, but also facilitates more effective reasoning approaches in the subsequent sub-steps.This allows for a finer-grained resolution of sub-tasks, enriching the overall reasoning process.Here, Least-to-Most Prompting [238] grows a reasoning chain where decomposition of complex tasks or questions is conducted in the first node and the subtasks/sub-questions are solved in the subsequent nodes.</p>
<p>Specifically, the multi-prompt chain operates by first decomposing the original question into a list of sub-questions, which are each solved in individual sub-steps with the questions and answers of previous sub-steps included in the context.The chain terminates when the final answer is returned after answering all sub-questions in the list.Then, Decomposed Prompting [106] is a modular framework for a detailed decomposition of complex tasks.To generate a reasoning chain, the LLM is prompted by demonstrations comprised of sequential question-operation triplets, which form "sub-questions".In contrast to Least-to-Most prompting, this allows for the recursive breakdown of questions into simpler sub-questions until they can be solved directly, as further decomposition is a valid operation in the framework.Apart from these two above schemes, decomposition is applied in many similar works [60], [103], [221].</p>
<p>Task Preprocessing</p>
<p>The concept of task preprocessing comprises any technique that preprocesses the context of a task by updating it or rephrasing the task description itself, before taking any reasoning steps.For instance, the multi-prompt scheme Selection-Inference (SI) [51] is designed to tackle multi-step logical reasoning problems where all essential information is already present within the input context.The key functionality of SI lies in its recurring process of context pruning before each reasoning step.This means that it selectively filters the context to retain only the relevant information necessary for each specific subsequent step of reasoning, ensuring that the most pertinent data is always used for each decision-making stage.On the other hand, instead of pruning the context, the multi-prompt scheme Chain-of-Symbol (CoS) [91], specifically designed for spatial planning tasks, augments the context with condensed symbolic representations, before subsequently using those as prompts for the LLM to conduct CoT-based reasoning.</p>
<p>Iterative Refinement</p>
<p>The introduction of verification enables the reasoning frameworks to iteratively refine the generated context and intermediate results.With this strategy, the execution of chain-based reasoning is effectively extended with loops, with conditions on how many times one can loop over a node (based on the number of iterations or some terminal conditions).The concept is applied in different works [124], [127], [141], [171], [202], [236].</p>
<p>Tool Utilization</p>
<p>To better integrate multiple execution methods, more effective schemes opt to devise a plan that specifies tools for handling each sub-task, before executing the reasoning chain.Examples include AutoGPT [162], Toolformer [167], Chameleon [137], ChatCot [45], PVS [132] and others [235].</p>
<p>Analysis &amp; Comparison of Designs</p>
<p>We now broadly discuss and analyze chain designs with respect to different aspects of our blueprint.</p>
<p>Game of 24 User Prompt</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24 Sure, here's one way to obtain 24 using the numbers 14, 8, 8, and 2 ((14 -8) / 2) * 8 = 24 Fig. 5: GAME OF 24.An example showing an IO scheme (effectively an implicit single-prompt single-node chain topology).A few-shot IO prompt is used, leading to a single output directly providing the result with no intermediate reasoning steps.</p>
<p>LLM Answer</p>
<p>Topology &amp; Its Construction</p>
<p>In single-prompt schemes, the entire reasoning process is executed within a single prompting round.This approach is less common for complex tasks, as it often demands sophisticated prompt engineering to encompass the entire reasoning pathway in one go.On the other hand, most chain designs employ multi-prompt schemes, where the reasoning process is segmented into multiple rounds of prompting.This allows for a more nuanced and step-wise approach to problem-solving.Key novel architectural features of chain designs include the ability to appropriately decompose tasks, verify and refine intermediate solutions, preprocess initial prompts, and utilize external tools, such as Python scripts.This multifaceted approach enables LLMs to tackle more complex problems by breaking them down into smaller, more manageable components and iteratively refining the solutions.</p>
<p>Representations of Topology &amp; Schedule</p>
<p>We now illustrate representative prompts that show differences between single-and multi-prompt chain-based topologies (the tasks are described in detail in the prompts of the figures).We use examples based on the well-known tasks of Game of 24, Creative Writing, and Mathematical or Logical Reasoning.For this, we illustrate an example simplest IO scheme in Figure 5 (effectively a single-node CoT) and compare it to an implicit single-prompt few-shot CoT in Figure 6, an implicit single-prompt zero-shot CoT in Figure 7 and to an example few-shot explicit multi-prompt CoT (Selection-Inference) in Figure 8.</p>
<p>Performance</p>
<p>We now summarize performance patterns found in chain topologies.For a more detailed performance comparison, see Appendix E.1.</p>
<p>Overall, in Arithmetic Reasoning, CoT significantly outperforms Input-Output (IO) prompting, with notable improvements in terms of accuracy on several datasets from benchmarks such as GSM8K (Grade School Math) [48],</p>
<p>Math Reasoning</p>
<p>User Prompt Q: Alexis is applying for a new job and bought a new set of business clothes to wear to the interview.She went to a department store with a budget of $200 and spent $30 on a button-up shirt, $46 on suit pants, $38 on a suit coat, $11 on socks, and $18 on a belt.She also purchased a pair of shoes, but lost the receipt for them.She has $16 left from her budget.How much did Alexis pay for the shoes?Q: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month for the past three months.He gets a $5 allowance a month.He also mows lawns and shovels driveways.Let D be the number of driveways Tobias shoveled.</p>
<p>He earned $5 allowance per month for three months, so he has $5 * 3 = $15 from his allowance.</p>
<p>He mowed 4 lawns, each earning him $15, so he made $15 * 4 = $60 from mowing lawns.</p>
<p>He also earned $7 for each driveway shoveled, and he shoveled D driveways, so he made $7 * D from shoveling driveways.</p>
<p>In total, he has $15 (allowance) + $60 (mowing lawns) + $7D (shoveling driveways) before buying the shoes.</p>
<p>After buying the $95 shoes, he has $15 in change.</p>
<p>Therefore: $15 + $60 + $7D -$95 = $15 So, Tobias shoveled 5 driveways.</p>
<p>Creative Writing User Prompt</p>
<p>LLM Answer</p>
<p>Mastering a handstand may seem daunting at first, but the key lies in simplicity.It isn't difficult to do a handstand if you just stand on your hands.Breaking down the complex maneuver into a straightforward action can make the seemingly impossible, achievable.Sometimes, the most challenging tasks become manageable when approached with a clear and uncomplicated perspective.</p>
<p>In the vast expanse of outer space, surprises await even the most seasoned explorers.It caught him off guard that space smelled of seared steak.The unexpected aroma challenged preconceptions, reminding us that the universe is full of mysteries that extend beyond the visual and auditory senses.The interplay of senses in unfamiliar environments can reshape our understanding of the cosmos.</p>
<p>Navigating social interactions often demands creative strategies, especially when faced with unwanted advances.When she didn't like a guy who was trying to pick her up, she started using sign language.Non-verbal communication became her shield, a tool to ward off unwelcome attention without resorting to confrontation.In these moments, the power of expression transcends spoken words, allowing for a graceful escape from uncomfortable situations.Perceptions of identity are as diverse as the individuals who form them.Each person who knows you has a different perception of who you are.It's a kaleidoscope of perspectives, shaped by unique experiences and interactions.Understanding that our multifaceted nature is reflected in the eyes of those around us can foster empathy and open the door to a richer tapestry of human connections.</p>
<p>Write a coherent passage of 4 short paragraphs.The end sentence of each paragraph must be: 1.It isn't difficult to do a handstand if you just stand on your hands.2. It caught him off guard that space smelled of seared steak.3. When she didn't like a guy who was trying to pick her up, she started using sign language.4. Each person who knows you has a different perception of who you are.Let's think step by step.SVAMP (Simple Variations on Arithmetic Math word Problem) [153], and MAWPS (MAth Word ProblemS) [114], which focus on mathematical tasks.The effectiveness of CoT increases with the scale of the LLM.Variants like Zeroshot-CoT, PoT, and schemes with decomposition nodes (like</p>
<p>LLM Answer</p>
<p>All smart, blue things are kind.We know that Dave is smart and Dave is blue.</p>
<p>Selection</p>
<p>Dave is kind.</p>
<p>Inference</p>
<p>If something is kind then it is nice.We know that Dave is kind.</p>
<p>Dave is nice.</p>
<p>If something is nice and blue then it is young.We know that Dave is nice and Dave is blue.</p>
<p>Dave is young.</p>
<p>If something is nice and young then it is cold.We know that Dave is nice and Dave is young.</p>
<p>Dave is cold.</p>
<p>If something is cold then it is quiet.We know that Dave is cold.</p>
<p>Dave is quiet.Least-to-Most Prompting, PS+) show further improvements in specific datasets.Chameleon, with its table reader tools, enhances performance in tabular math questions.</p>
<p>In Commonsense Reasoning, CoT excels over IO prompting with a distinct advantage in datasets like Strat-egyQA [72].Specialized methods like SelfAsk and the Selection-Inference framework show further improvements in multi-hop questions.Decomposition-based schemes like Least-to-Most and Decomposed Prompting achieve high accuracy in tasks requiring sequential actions or multifaceted reasoning.ChatCoT, with its retrieval and external</p>
<p>Tree Topology Variants</p>
<p>Graph Topology Variants tool utilization, shows a 20% gain on specific datasets.In Symbolic Reasoning, CoT demonstrates near-perfect accuracy for in-context examples and substantial accuracy for out-of-domain cases for tasks like last letter concatenation and coin flip predictions.Schemes with decomposition nodes outperform CoT in more complex tasks and longer word scenarios.</p>
<p>Overall, across different domains, CoT and its variants show a consistent trend of outperforming basic IO prompting.The integration of additional tools, tailored prompting strategies (like few-shot or zero-shot), and the incorporation of decomposition and refinement nodes enhance the LLMs' performance significantly.</p>
<p>REASONING WITH TREES</p>
<p>We next investigate in more detail individual schemes that use tree topologies.We analyze these works with respect to our blueprint and taxonomy in the middle part of Table 1 (detailed descriptions of each individual scheme are provided in the appendix).We structure the discussion based on the harnessed topology variants, namely trees of chains, 1-level trees, and k-ary trees, see Figure 9 for details.As in chain schemes, we also discuss fundamental concepts introduced or harnessed in these works.Tree schemes, most importantly, introduce exploration (i.e., generating multiple thoughts from a given one).The purpose behind exploration is usually either task decomposition (which is similar to CoT, but it differs as decompositions are not limited to a single linear plan) or sampling (i.e., having a higher chance of obtaining a high-quality solution).Moreover, tree schemes also introduce voting (i.e., automatic selection of the best outcome of all the generated outputs) and they harnesses various architectural concepts also used in chain schemes, such as iterative refinement or task preprocessing.We finish this section with a comparative analysis and illustrations of example topology representations.</p>
<p>Trees of Chains</p>
<p>While trees as reasoning topologies have been explicitly established in the works by Long [134] and Yao [217], this idea has been present earlier.Chain-of-Thought with Self-Consistency (CoT-SC) [194] is an early scheme that harnesses the tree structure to a certain degree.Here, multiple CoTs originate from the same initial (root) prompt, forming a "tree of chains".The chain providing the best outcome to the initial question, is selected as the final answer.</p>
<p>Single-Level Trees</p>
<p>A tree-based approach has also been harnessed in Skeletonof-Thought (SoT) [150], which effectively harnesses a tree with a single level of depth.This scheme aims to reduce the end-to-end generation latency of LLMs, caused by their inherent sequential decoding.Instead of generating one long continuous answer, this scheme uses a divide-andconquer approach.In a first prompt, the LLM is instructed to generate a skeleton of the answer, i.e., a list of points that are independently answerable.Then, for each of these points, a new prompt is issued in parallel to answer just this specific part of the question.As these points are processed in parallel, the overall latency is reduced.</p>
<p>k-Ary Trees</p>
<p>Numerous schemes have harnessed more general k-ary trees.First, the Tree-of-Thought (ToT) design by Long [134] utilizes a tree structure to decompose a problem into subproblems and solve them using separate LLM prompts.After the LLM suggests possible next steps and corresponding partial solutions, a checker module decides if any of these solutions is valid, whether it can be selected as the final one, or whether it should backtrack to the previous step.All issued prompts and answers are explicitly stored as a tree structure and navigated through using a controller module.The LLM prompting is only used to generate the next individual steps (i.e., hops) in this tree, whereas the overall problem solving process is coordinated by the controller.</p>
<p>Tree of Thoughts (ToT) by Yao et al. [217] differs from the above ToT approach in using the LLM itself as a solution evaluator with access to all generated solutions, instead of using a programmed or learned evaluator module.This allows to rate states individually or vote across intermediate solutions to select the most promising one to continue with the search.Both mentioned ToT approaches are a generalization of the IO, CoT, and CoT-SC prompting schemes.</p>
<p>Other examples in this class of topologies include Thought Decomposition [210] (a multi-prompt scheme based on stochastic beam search and self-evaluation), a scheme by Creswell and Shanahan [50] (an extension of the chain-based Selection-Inference [51]), Dynamic Leastto-Most Prompting [59] (an extension of least-to-most prompting with a tree-based problem decomposition and a dynamic external tree-based few-shot example selection), Algorithm of Thoughts (AoT) [168] (a single-prompt approach that utilizes in-context examples formulated in an algorithmic tree-based fashion), Tree of Uncertain Thought (TouT) [146] (an extension of ToT with local "uncertainty scores" by incorporating the variance of multiple LLM responses into the state evaluation function), Tree-of-Mixed-Thought (TomT) [93] (a ToT-based reasoning scheme to answer questions on visual scene graphs), or Tree of Clarifications (ToC) [107] (recursive prompting of an LLM to construct a tree of disambiguations for the initial question).</p>
<p>Analysis &amp; Comparison of Designs</p>
<p>We now broadly discuss and analyze tree designs with respect to different aspects of our blueprint.</p>
<p>Topology &amp; Its Construction</p>
<p>The key novel architectural feature of tree schemes is the exploration of a thought, i.e., the ability to generate multiple new steps based on a given single one.The vast majority of tree schemes are multi-prompt.Most multi-prompt schemes use a dynamic approach to building the tree topology.The details of how the topology is exactly shaped depend on the specific question.For most multi-prompt approaches, the user can adapt the tree topology to a certain degree, i.e., by varying the branching factor (i.e., the number of thoughts generated from a given vertex) and limiting the depth of the tree.</p>
<p>Representations of Topology &amp; Schedule</p>
<p>We showcase the differences between implicit vs. explicit and single-vs.multi-prompt topologies using representative examples (the tasks are described in detail in the prompts of the figures).We continue with the driving tasks of Game of 24, Creating Writing, and Mathematical/Logical Reasoning.For this, we illustrate an implicit single-prompt tree topology elicited by AoT in Figure 10 as well as two example explicit multi-prompt tree topologies from ToT and CoT-SC in, respectively, Figure 11 and 12. Lastly, we show an example of a parallel execution schedule in Figure 13 for SoT [150].</p>
<p>Performance</p>
<p>We now summarize performance patterns found within tree topologies.A detailed analysis can be found in Appendix E.2.</p>
<p>Overall, increasing the branching factor (i.e., the number of thoughts generated from a given vertex) often leads to a higher diversity of outcomes, which can be beneficial for accuracy, but it also increases #prompts, i.e., computational cost.The most advantageous branching factor is hard to find and it often depends on the specific problem to solve.Easily decomposable problems may benefit less from more branching than complex problems.Specifically, more complicated problems profit more from decomposing them into many/diverse sub-problems (e.g., this ensures enough diversity for self-consistency to work better).In contrast, a question that has clearly only two sub-parts does not benefit from many more subdivisions, as the additional branches then can be either redundant or wrong.Singleprompt approaches can perform better on some problems than multi-prompt approaches, while using only a single prompt compared to possibly hundreds [168].</p>
<p>REASONING WITH GRAPHS</p>
<p>We also analyze schemes that harness graph topologies, see the bottom part of Table 1 (detailed descriptions of each individual scheme are provided, as for chains and trees, in the appendix).Similarly to the tree analysis, we structure the discussion based on the harnessed topology variants, see Figure 9 for details.We also discuss fundamental concepts introduced or harnessed in these works.Graph schemes, most importantly, introduce aggregation (i.e., being able to combine multiple thoughts into a single one).The purpose behind aggregation is usually synergy (i.e., being able to produce an outcome better than the individual ingredients) or an effective composition of outcomes of tasks.Graph schemes also use architectural concepts employed in chain or tree schemes, such as exploration or iterative refinement.We conclude this section with a comparative analysis and illustrations of example representations of graph topologies.</p>
<p>Special Classes of Graphs</p>
<p>Different schemes harness certain special classes of graphs.Branch-Solve-Merge (BSM) [164] employs a 1-level double tree structure to first divide a problem into independently solvable sub-problems, and then combines them into a final solution.The first prompt instructs the LLM to propose subproblems, which are then solved independently.The final prompt instructs the LLM to merge the results of the subproblems into a single output.Socratic Questioning [156] is a scheme that models recursive exploration of the thought space using a tree structure.Hereby, the original question is recursively decomposed into sub-tasks until all tasks can be solved with high confidence.These results are then aggregated and propagated back up the tree to answer the original question.This results in an overall double tree reasoning topology.</p>
<p>Game of 24 User Prompt</p>
<p>Use numbers and basic arithmetic operations (+ -* /) to obtain 24.</p>
<p>[in-context examples]</p>
<p>User: 14 8 8 2 Assistant:</p>
<p>Step by step reasoning in a CoT fashion is used for backtacking the solution.</p>
<p>The implicit tree structure is created on the fly and traversed in a DFS manner.Nodes are constructed from left to right.Answer: (14 -8) * (8 / 2) = 24.</p>
<p>Backtracking the solution:</p>
<p>Step 1: 8 / 2 = 4</p>
<p>Step 2: 14 -8 = 6</p>
<p>Step 3: 6 * 4 = 24</p>
<p>Trying another promising first operation: 8 / 2: (14, 8, 4)</p>
<p>14 + 10: (24, 8) 14 -8: (6, 4) 14 + 8: (22,4) Fig. 10: GAME OF 24.An example implicit single-prompt tree topology, encoded with text.It demonstrates a Game of 24 DFS in-context example from AoT [168].The left view shows the user prompt and the single textual answer from the LLM.The right view shows the implicit tree structure that is explored during the generation of the LLM answer.We mark text corresponding to implicit nodes as bold.encoded with text, from the Tree of Thoughts (ToT) scheme [217] for creative writing.Given the task of writing a coherent passage of four paragraphs ending in given sentences, first multiple plans (nodes) are generated and then ranked.In a next step, the best plan is used to generate multiple possible passages as outputs.Finally, the best ranked passage is the output of the ToT reasoning.</p>
<p>Creative</p>
<p>Directed Graphs</p>
<p>Some schemes embrace a general directed graph model.Graph of Thoughts (GoT) [10] uses a multi-prompt approach to improve the LLM problem solving performance by decomposing a given task into sub-tasks that form a graph.This decomposition is specified as a Graph of Operations, which coordinates how the LLM is prompted and how the results are further used in the reasoning process.The Graph of Thought [120] presents a multi-prompt approach where a graph of thoughts is constructed recursively in a DFS manner by starting at the question node that represents</p>
<p>Math Reasoning Answer 2 Answer 3</p>
<p>Select final answer based on majority vote:</p>
<p>Let D be the number of driveways Tobias shoveled.</p>
<p>He earned $5 allowance […]</p>
<p>So the answer is 5.</p>
<p>Answer 1</p>
<p>Let D be the number of driveways Tobias shoveled.</p>
<p>He has been saving up […]</p>
<p>So the answer is 1.</p>
<p>Let M be the number of lawns Tobias mows and D be the number of […]</p>
<p>So the answer is 5.</p>
<p>So the answer is 5.</p>
<p>User [in-context examples]</p>
<p>Q: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month […]?</p>
<p>Fig. 12: MATH REASONING.An example using explicit multi-prompt tree topology, encoded with text.Given a math reasoning task, CoT-SC [194] is used to generate multiple answers and pick a final one based on majority vote.Each of the generated answers contains multiple CoT reasoning steps, depicted here in a single node.</p>
<p>the question to be answered by the LLM.From this node, possible reasoning paths are generated by the LLM.For each path, new nodes, i.e., intermediate reasoning steps, are generated by the LLM and are then used to grow the graph.Graph-of-Thought [219] describes a two-stage framework to answer multi-modal questions, i.e., textual questions accompanied by images.In the first stage, the model generates natural language rationales based on the input text, which provide additional context and knowledge to support answering the given question.This rationale generation is learned as part of the overall model pipeline.</p>
<p>In the second stage, these rationales are then appended to the initial question and passed again through the model First, a skeleton node is generated containing a list of points to answer the question.Then, each point of the skeleton is expanded in parallel, and the final answer is derived by concatenating them.</p>
<ol>
<li>Active listening involves fully concentrating on ... Fig. 13: An example explicit multi-prompt tree topology, encoded with text.It demonstrates the automatically derived tree topology of Skeleton-of-Thought (SoT) [150] where the individual points are expanded in parallel.</li>
</ol>
<p>to predict an answer.Other schemes in this class include Cumulative Reasoning [229], Everything of Thoughts (XoT) [57], ControlLLM [133], and ResPrompt [100].</p>
<p>Hypergraphs</p>
<p>Finally, we also consider a hypergraph, which generalizes a graph by enabling edges to connect arbitrary subsets of nodes instead of being links between just two nodes.We include hypergraphs in the taxonomy, because preliminary works already harness them for multi-modal prompting [216].Here, Hypergraph-of-Thought (HoT) [216] is a multi-modal reasoning paradigm modeling the thought process as a hypergraph.First, a graph-of-thoughts as in [219] is constructed.Then a textual hypergraph is constructed, sharing the same nodes.The hyperedges are then defined as node triples, e.g., "(Lionel Messi, place of birth, Rosario)".Additionally, a visual hypergraph-of-thought is constructed by performing k-means clustering on image patches, where a cluster corresponds to a hyperedge.Both hypergraphs are then encoded and combined to perform graph learning.</p>
<p>Analysis &amp; Comparison of Designs</p>
<p>We now broadly discuss and analyze graph designs with respect to different aspects of our blueprint.</p>
<p>Topology &amp; Its Construction</p>
<p>Firstly, the considered schemes exhibit a blend of singleand multi-prompt aspects, allowing for a high degree of flexibility and control over the prompting process.This is evident in the diverse approaches taken by different schemes such as GoT, ControlLLM, and Cumulative Reasoning, each offering unique ways of constructing and utilizing graphs for problem-solving.Secondly, the user's control over the topology of the graph is significant in most schemes, enabling customization of the reasoning process based on specific needs, such as setting branching factors or defining the depth of the graph.Thirdly, the role of the LLM in these graph-based schemes is multifaceted, involving the generation, evaluation, and modification of nodes within the graph, as well as determining the conclusion of the reasoning process.Lastly, there is a notable variation in the degree of user and LLM influence on the topology across different schemes, with some allowing direct user control, while others rely on predefined heuristics or the LLM's decision-making capabilities.</p>
<p>Representations of Topology &amp; Schedule</p>
<p>We now illustrate a representative set of prompts that show different aspects of graph-based prompting topologies, focusing on how the respective graph topologies are encoded inside the prompts (the tasks are described in detail in the prompts of the figures).We illustrate ResPrompt and Cumulative Reasoning (as example implicit single-prompt representations), ControlLLM (as an example explicit singleprompt representation), and Branch-Solve-Merge (as an example multi-prompt representation).Figure 14 shows a prompting example using Cumulative Reasoning [229] for the Game of 24 with an explicit multiprompt graph topology.Figure 15 shows an in-context example of ResPrompt [100] with a multi-step math question, where the topology is single-prompt and implicit.Here, an (implicit) edge can be formed by repeating the same token, for example "earned from his allowance", which implies a connection between step 2 and 4 where each step indicates a node.Figure 16 illustrates an in-context example where Cumulative Reasoning [229] is applied to the same math problem with user-specified number of intermediate nodes before reaching the final solution node.Here, implicit vertices are formed using numbered positions 1 and 2 on the list, and edges connect points 1 and 2 to point 3.In contrast to the implicit representation of topology, Figure 17 shows an example from ControlLLM [133] to represent the topology explicitly with a JSON format, but also in the singleprompt setting.Finally, Figure 18 depicts a multi-prompt example from Branch-Solve-Merge (BSM) [164] for story generation.</p>
<p>Performance</p>
<p>The considered works universally show improvements in effectiveness of graph-based prompting schemes over chains and trees across various tasks, suggesting a promising direction for future research and application in the field of AI and machine learning.Starting with the user input node, one valid node is selected at random in each iteration.Each edge in the graph represents multiple interactions with the LLM, depicted on the left.First a next step is proposed.Then this step is validated and verified, judging if this can lead to the desired solution or if it is a dead end (dashed nodes).Opposed to ToT, these invalid steps are kept track of and are used in the next propose prompt as "forbidden steps", so directly influencing the next node (dashed arrows).If the validation output is "sure", the Reporter is invoked to summarise the steps and derive the final result as shown in the reporter prompt.</p>
<p>Game of 24</p>
<p>... User Input</p>
<p>Propose Prompt Verifier Prompts Fig. 14: GAME OF 24.An example explicit multi-prompt graph topology, encoded with text using Cumulative Reasoning [229].</p>
<p>Math Reasoning</p>
<p>User Prompt 6.As he charges $7 to shovel a driveway, we divide the total earned from shoveling by the charge per driveway to find out the number of driveways he shoveled: $35 / $7 = 5 driveways.</p>
<p>1.He has $15 left after buying the shoes that cost $95, which means he had $95 + $15 = $110 in total before the purchase.</p>
<ol>
<li>
<p>Next, we calculate the amount he earned from his allowance.He has been saving for three months, getting $5 each month, [...]</p>
</li>
<li>
<p>Then we know he mowed 4 lawns and he charges $15 per lawn, so from mowing lawns, he earned 4 * $15 = $60.</p>
</li>
<li>
<p>Let us sum up the money he earned from his allowance and from mowing lawns: $15 (allowance) + $60 (mowing) = $75.</p>
</li>
<li>
<p>Now, let us subtract the total amount of money earned from allowance and mowing from his total amount of money before buying the shoes to [...].</p>
</li>
</ol>
<p>6.As he charges $7 to shovel a driveway, we divide the total earned from shoveling by the charge per driveway to find out the number of driveways he shoveled: $35 / $7 = 5 driveways.</p>
<p>Math Reasoning
Validate Step Verify Step
...</p>
<p>Reporter Prompt</p>
<p>Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Read and analyze the "Premises" first, then using First-Order Logic (FOL) to answer the "Question".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.</p>
<p>[in-context examples]</p>
<p>Premises:</p>
<ol>
<li>Tobias is buying a new pair of shoes that costs $95.</li>
</ol>
<p>He has been saving up his money each month</p>
<p>[…] Question: How many driveways did he shovel?Generated Propositions:</p>
<p>1.He had $110 in total before buying the shoes.2.He had $75 from saving the last three months and mowing lawns.</p>
<ol>
<li>Using 1 and 2, we know that he earned $35 from shoveling driveways.Reasoning:</li>
</ol>
<p>We know that he earned $35 from shoveling, and he charges $7 for shoveling.Therefore, the number of driveways he shoveled is $35 / $7 = 5.</p>
<p>Each premise is a valid node at the begining.In each iteration five valid nodes are selected at random to generate a proposition.Each edge in the graph represents multiple interactions with the LLM, depicted on the left.Each proposition is validated and verified before adding it to the valid node list, judging if this is deducted from valid nodes and verified based on FOL or if it is a dead end (dashed nodes).After having a predefined number of propositions, the Reporter is invoked to answer the question with all valid nodes.</p>
<p>User Input Propose Prompt</p>
<p>Verifier Prompts</p>
<p>Premises:</p>
<ol>
<li>Tobias is buying a new pair of shoes that costs $95.</li>
</ol>
<p>[…] Question: How many driveways did he shovel?Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use First-Order Logic (FOL) to deduce a "Proposition" from two given "Premises".Please make sure that the "Proposition" is logically correct.Please make sure that the "Proposition" is not a duplicate of the "Premises".Please make sure your reasoning is directly deduced from the "Premises" and "Propositions" other than introducing unsourced common knowledge and unsourced information by common sense reasoning.</p>
<p>Judgment: False</p>
<p>The proposition is not added to the valid node list.</p>
<p>Judgment: False</p>
<p>He had $95 in total before buying the shoes.</p>
<p>He had $110 in total before buying the shoes.</p>
<p>He had $75 from saving the last three months and mowing lawns.</p>
<p>He earned $35 from shoveling driveways.</p>
<p>We know that he earned $35 from shoveling, and he charges $7 for shoveling.Therefore, the number of driveways he shoveled is $35 / $7 = 5.</p>
<p>CHAINS VS. TREES VS. GRAPHS OF THOUGHTS</p>
<p>We also broadly discuss tradeoffs, commonalities, and differences between the three fundamental classes of topologies: chains, trees, and graphs.The novelty in chain-based prompting lies in introducing explicit intermediate LLM thoughts between the input and the output.This linear sequence of thoughts guides the LLM in a step-by-step manner towards the solution, enhancing the clarity and traceability of the reasoning process.Beyond this, tree-based schemes bring the possibility to explore several next-step variants at each juncture, allowing the LLM to evaluate multiple pathways and select the most promising one.This branching structure facilitates a broader exploration of potential solutions.Graph-based schemes, however, represent the most complex structure, offering an arbitrary reasoning framework.They enable the aggregation of various reasoning steps into a synergistic solution, allowing for non-linear and multifaceted problem-solving approaches.This diversity in structures reflects the varying needs and complexities of tasks that LLMs are expected to handle.</p>
<p>In terms of cost-effectiveness, chain-based prompting schemes generally outperform their tree-based and graph-based counterparts.This efficiency largely stems from their single-prompt nature, which requires fewer computational resources and less processing time.Tree and graph-based schemes, on the other hand, often involve multiple rounds of prompting to explore the various branches or connections in their respective structures, naturally incurring higher costs.However, this is not an inherent limitation of these schemes.Future developments in tree and graph-based prompting should aim to better encode the tree and graph structures within a single prompt.Such advancements could potentially combine the cost-effectiveness of single-prompt schemes with the enhanced quality and complexity management offered by tree and graph-based structures.</p>
<p>When it comes to the quality of outcomes, tree-based and graph-based prompting schemes typically surpass chainbased ones.This superior performance is attributed to their ability to explore a wider range of potential transformations at each step, going beyond the linear refinement seen in chains.Tree-based schemes, with their branching paths, offer multiple avenues for solution exploration at each decision point, leading to a more comprehensive search for the optimal outcome.Graph-based schemes take this a step further by allowing for an even more diverse set of connections and interactions between different steps in the reasoning</p>
<p>JSON LLM Task Decomposition Answer User Task</p>
<p>Please dub the video.mp4with suitable background music and then generate an HTML web page to introduce this video.</p>
<p>[{ "description": "Dub the video with background music", "task": [ "video-editing"], "id": 1, "args": { "type": "video", "value": "video.mp4"}, "returns": { "type": "video", "value": "<GEN>-1" } }, { "description": "Generate an HTML page to introduce the video", "task": [ "HTML-generation"], "id": 2, "dep": 1, "args": { "type": "video", "value": "<GEN>-1" }, "returns": { "type": "HTML", "value": "<GEN>-2" } }] Fig. 17: JSON.An example explicit single-prompt graph topology, encoded with JSON, based on the ControlLLM scheme [133] for task decomposition.It shows two nodes describing decomposed subtasks for solving a given task.The "dep" field refers to dependent tasks, showing there is a direct edge from the first node (task 1) to the second.Fig. 18: CREATIVE WRITING.An example multi-prompt graph topology, encoded with text, from the Branch-Solve-Merge scheme [164] for story generation with branch, solve, and merge prompts.Given a list of concepts as input, the branch module generates three child nodes: two groups of concepts and one topic node.The solve module then creates two story nodes based on each group of concepts and the topic.Finally, these two story nodes are merged into the final solution node.</p>
<p>Creative Writing</p>
<p>process.This flexibility enables these schemes to handle more complex and multifaceted tasks effectively, leading to higher-quality outcomes in scenarios where simple linear reasoning is insufficient.</p>
<p>Performance vs. Task &amp; Data Categories</p>
<p>Task and Data Categories We synthesize an evaluation of considered schemes, based on a broad literature overview.In general, the evaluation spans arithmetic, commonsense, symbolic, and specialized reasoning tasks.Arithmetic reasoning datasets like GSM8K [48],</p>
<p>SVAMP [153], MAWPS [114], and DROP [61] involve problems requiring sequential numerical reasoning, where structured decomposition is beneficial.Commonsense reasoning datasets include StrategyQA [72], 2WikiMultiHopQA [89],</p>
<p>and MuSiQue [188]; they emphasize multi-hop reasoning.Symbolic reasoning, including last-letter concatenation and spatial tasks (Brick World [91], SPARTUN [144]), tests pattern recognition and out-of-distribution generalization.Specialized reasoning tasks, such as code generation (MBPP [8], HumanEval [39]) and causal inference (CauseEffect [176], SocialQA [176]), benefit from structured planning and external tool use.We now describe how different reasoning topologies compare in these categories of benchmarks.Arithmetic Reasoning Winners: ToT, GoT ToT outperforms CoT by enabling structured decomposition and backtracking, making it particularly effective in datasets like GSM8K and DROP.GoT further improves performance by integrating cumulative reasoning across steps thanks to its aggregation capabilities, reducing error propagation.CoT struggles with multi-step arithmetic reasoning problems due to error accumulation in sequential reasoning.</p>
<p>Commonsense Reasoning Winners: GoT, ToT GoT is highly effective in multi-hop and retrieval-based reasoning (e.g., 2WikiMultiHopQA, MuSiQue) by dynamically aggregating knowledge across multiple nodes.ToT excels in structured multi-hop tasks (e.g., Decomposed Prompting in StrategyQA) by breaking down inference into manageable chunks.CoT is less reliable in tasks requiring multiple layers of inference, as sequential chains cannot effectively retrieve and synthesize distant context.</p>
<p>Symbolic Reasoning Winners: ToT, GoT ToT provides superior performance in structured symbolic tasks such as last-letter concatenation, object counting, and coin flipping by systematically exploring the solution space.GoT enhances performance in logical and mathematical transformations by maintaining dependencies between reasoning steps.CoT fails to generalize well in OOD cases due to its linear reasoning constraints.</p>
<p>Specialized Reasoning Winners: GoT, ToT GoT is ideal for complex reasoning pipelines (e.g., ScienceQA [136]) by allowing nodes to interact flexibly across multiple steps, making it particularly strong in planning-based tasks.ToT enhances structured reasoning in mathematical and coding tasks (e.g., Reflexion [171] in HumanEval, MBPP) by iteratively refining intermediate outputs.CoT is limited in these types of tasks due to its inherently sequential reasoning pattern.</p>
<p>Sequential Reasoning Winners: CoT, ToT CoT performs well in structured, non-branching tasks (e.g., simple math problems, single-hop commonsense tasks) due to its interpretability and efficiency.ToT provides additional benefits in cases where intermediate verification is needed, preventing early reasoning errors.GoT is unnecessary for purely sequential problems and adds computational overhead without significant performance gains.</p>
<p>Summary Chain-based methods are effective for sequential reasoning, but struggle with complex decompositions and multi-hop inference.Tree-based approaches improve on these limitations by allowing structured problemsolving via multi-step decomposition and exploration, with user-defined constraints such as branching factor, depth lim-its, and search strategies (BFS, DFS, beam search).ToT [217] adaptively prunes search trees, while Thought Decomposition [210] dynamically expands reasoning depth for improved accuracy at the cost of increased token usage.Though computationally costly, tree-based schemes excel in problems with hierarchical structure, outperforming chainbased ones in arithmetic and symbolic reasoning.Graphbased methods further generalize tree-based approaches by enabling non-hierarchical, interconnected reasoning, allowing nodes to influence each other dynamically.Graph schemes like GoT [10] improve arithmetic and multi-hop reasoning efficiency, often using fewer tokens than treebased methods.</p>
<p>Evaluation Techniques</p>
<p>Works on evaluating methodologies for reasoning topologies has so far focused on CoTs.Chain-based reasoning requires methods that assess not just final answer correctness but also the validity of intermediate reasoning steps.Nguyen et al. [149] show that LLMs frequently generate plausible yet incorrect reasoning chains, necessitating structured verification mechanisms.The OCEAN framework [205] formalizes CoT evaluation as a Markov Decision Process (MDP), using knowledge graph-based inverse propensity scores (KG-IPS) to assess reasoning fidelity.Meanwhile, Chang et al. [228] highlight the necessity of robust verification for self-correction, showing that weak verifiers hinder accuracy improvements even when models are fine-tuned for reasoning correction.Additionally, discriminative evaluation methods assess LLMs' ability to identify logical inconsistencies in reasoning paths, while generative evaluation tests their capacity to produce structured and verifiable reasoning steps.These studies collectively suggest that effective evaluation of chain methods must integrate logical consistency checks, structured reasoning assessments, and robust verification models to ensure reasoning chains genuinely support correct answers.</p>
<p>DESIGN ARCHITECTURES</p>
<p>We also analyze design tradeoffs.We consider the following aspects: design architecture (what the architecture-level decomposition of different schemes into interacting modules is), productivity &amp; programmability (how well different schemes support efforts such as new extensions), and scalability as well as parallelizability (whether considered schemes can be parallelized and whether they scale well).</p>
<p>Design Architecture</p>
<p>Here, we analyze how the overall prompting scheme, combining the LLM and some additional logic, decomposes into different modules.The module-level design architecture is explicitly discussed to a certain degree by some treebased approaches, such as [50], [93], [134], [146], [217] as well as graph-based approaches such as [10], [120], [156], [164], [222], [229].Detailed module-level architectures are presented by Long [134], Yao et al. [217], and Besta et al. [10].</p>
<p>The general architecture of considered schemes can be summarized as consisting of the following modules: a generator, an evaluator, a halter, and a controller module.The generator prompts the LLM to produce further reasoning steps given the current reasoning step and some context.The evaluator rates the current reasoning step, depending on the implementation this rating can depend on other states, the path to the initial question node, or some other context.The halter determines if a suitable solution has been found and how it should be reported or if the search should continue.The controller module coordinates the other modules as well as the construction and exploration of the tree/graph.</p>
<p>Productivity &amp; Programmability</p>
<p>ToT by Yao et al. [217], Tree Prompting [173] and SoT [150] provide implementations that can be used directly for custom tasks; ToT [217] is the only tree approach that provides an easily usable API to solve custom problems in a multi-prompt fashion.ToT by Long [134], Thought Decomposition [210] and ToC [107] provide implementations to reproduce their results, but can not easily be used for other tasks.CoT-SC [194] is a paradigm and can easily be implemented when needed, AoT [168] is a single-prompt scheme and does not provide an implementation but lists in-context examples.The remaining approaches [50], [59], [93], [146] have no code published at this time.</p>
<p>In graph designs, Socratic Questioning [156] and Graph of Thoughts (GoT) [10] provide implementations that can be used directly for custom tasks.Cumulative Reasoning [229] provides an implementation to reproduce their results, but cannot easily be used for other tasks.Thought Propagation (TP) [222], Branch-Solve-Merge (BSM) [164], Control-LLM [133] and ResPrompt [100] do not provide implementation but list a set of prompting examples.Among these, ResPrompt [100] is a paradigm can be easily implemented when needed.The remaining approaches [57], [120], [216], [219] have no code published at this time.</p>
<p>Overall, GoT [10] offers the most general API for addressing custom tasks in a multi-prompt fashion.It offers a principled way to design a prompting structure, through thought transformations, i.e., ways to transform GoT nodes into new outcomes.Example GoT API routines, which implement thought transformations include -for example Generate(prompt, k=4) (prompt the LLM to produce k answers to a given prompt), Generate(prompt, k=1)+Repeat(k=4) (generate 4 context isolated responses of the LLM), Aggregate(thought1, thought2) (combine two thoughts), KeepBest(N=1) (query the LLM to return N best results), Improve(thought) (query the LLM to improve the result using information from another thought, e.g., input list and currently sorted list → try to fix incorrectly sorted elements).</p>
<p>Scalability &amp; Parallelizability</p>
<p>Only a few works address scalability and parallelizability improvements of LLMs.Skeleton of Thought [150] uses the ToT approach: the prompt query is a tree of depth one and all leaves can be processed in parallel.Batch Prompt [125] improves the performance by batching different data points into a single prompt.On the other hand, Lookahead Decoding [66] is focused on speculatively guessing tokens to speed up the answer generation.We now discuss in more detail how these concepts are realized.</p>
<p>Skeleton of Thought [150] tackles the problem of high inference latency in LLMs by addressing the sequential decoding problem (generating answer tokens one at a time).They proposed a new approach where a skeleton prompt leads the LLM to first generate a succinct list of points that form the skeleton of its future answer.Then, each point in the skeleton is decoded in parallel, using batching whenever possible.The method does not require changes to LLMs and can be applied to off-the-shelf models.with reported speedups of up to 2.69×.However, this method ignores dependencies between points in the skeleton answer, and the authors propose replacing the tree with a graph model as future work.</p>
<p>Lookahead Decoding [66] extends the idea of speculative decoding where a sequence of future tokens is predicted, and the LLM later verifies each one of them in parallel.This method represents the sequential generation of a chain of responses as a non-linear system of equations and then solves it with the Jacobi iteration method.In each iteration step, at least one guessed token -the next one -is verified and matched successfully.Furthermore, the method caches n-grams generated for each token by inspecting N prior Jacobi iterations, allowing it to decode multiple tokens on a positive match.By conducting the lookahead and verification in parallel, authors achieve decoding speedup by up to 2.25×, at the cost of an exponential increase in GPU FLOPs.</p>
<p>BatchPrompt [125] is a scheme that batches similar questions into a single prompt to reduce the overhead of task description and few-shot examples compared to singlequestion prompting.However, naive batching decreases the overall LLM performance, and the final accuracy depends on the position of each single prompt within the batch, which may be challenging to tune.BatchPrompt avoids this problem by performing majority voting on multiple rounds of prompting with different permutations of the question, allowing for competitive results compared to singlequestion prompting.By using large batches with few voting rounds, the scheme can process queries with significantly fewer calls to the LLM.</p>
<p>FOUNDATIONS &amp; THEORY</p>
<p>There are a few preliminary works that attempt to provide foundations for structured prompting.</p>
<p>Madaan and Yazdanbakhsh [142] focus on dissecting CoT into three basic components, namely symbols (sequences of tokens used as the basis of the LLM reasoning, for example numbers to be sorted), patterns (the structure within prompt that reinforces task understanding, for example the order of numbers to be sorted), and text (any tokens that are neither a symbol nor a pattern).Here, "patterns" is effectively a certain single-prompt topology that enhances the LLM reasoning.The authors discover that, for example, patterns are relevant for better task understanding, and they form a symbiotic relationship with text: the latter help to create more useful patterns while the former enable the LLM to generate text that helps in task solving.</p>
<p>Tutunov et al. [189] use probabilistic graphical models to provide a theoretical understanding of how and why LLMs are able to generate a coherent chain of thoughts.Their model generalizes Jiang's latent space theory of language models [99] to chains of thoughts.In the latent space theory, thoughts convey a hidden intention.Because the intention is not directly observed -only the uttered thought -there is a potential for ambiguity.This ambiguity ρ can be quantified probabilistically for the language as a whole.Tutonov et al. extend Jiang's model by adding an additional hidden context variable upon which intentions are conditioned.This context c defines a family of tasks which each define a set of coherent reasoning steps.Moreover, Tutonov et al. generalize the one-shot generation of thoughts into a chain of thoughts with an associated chain of intentions.Every intention is conditioned on the context c and the previous intentions.In turn, the i-th thought in the chain is conditioned on the i-th intention.Their main result is as follows: conditioned on the input and a sequence of N example chain of thoughts generated from a context c, the probability that an LLM assigns to a chain of thoughts approaches the true probability of the chain given the input and the hidden context c with convergence speed ρ N .This means that, with the appropriate examples, LLMs can generate a chain of thoughts that is arbitrarily close to the true output.</p>
<p>Besta et al. [10] discuss tradeoffs between latency (number of steps to reach the final thought) and volume, which they define -for a given thought t -as the number of preceding LLM thoughts that could have impacted thought t.Formally, it is the number of thoughts in the topology from which there exists a path to thought t.They assume a time of O(1) for each reasoning step, fix the total cost for each topology in their analysis to Θ(n) and also make certain assumptions for each topology.For a single chain both the latency and the volume have a high value of N , which for multiple chains (k independent chains) is reduced by k for both metrics (N/k).In their tree analysis they assume a complete k-ary tree, which has a low latency of ⌈log k N ⌉, but the volume is similarly low (O(log k N )).For graphs, they assume two complete k-ary trees, where the first tree, representing the division into sub-tasks, is joined at the leaf level with another "reversed" k-ary tree (including its edges), representing the aggregation into the final solution.They conclude that double-tree graphs provide the best tradeoff with a latency of log k N and a volume of N .</p>
<p>Several works, while not providing any theoretical underpinning for structured-enhanced prompting, investigate the capabilities for CoT to conduct formal analyses.This includes GSM8K [48], ProofWriter [183], FOLIO [84], Sim-pleLogic [225], and PrOntoQA [166].</p>
<p>There have been several works that investigate the theoretical underpinning of general in-context learning.This includes Xie et al. [209], Wies et al. [204], Hahn and Goyal [82], and Jiang [99].However, as they do not focus on the topologies of prompting, details are outside the focus of our work.</p>
<p>To align theoretical foundations with practical techniques, we illustrate how structured-enhanced prompting principles translate into real-world applications.Madaan and Yazdanbakhsh's decomposition of CoT into symbols, patterns, and text informs prompt engineering strategies, such as designing math prompts where number alignment (patterns) improves model understanding, or crafting logic puzzles where structured reasoning cues guide CoT outputs.In arithmetic reasoning (e.g., GSM8K), explicitly formatting step-by-step solutions with structured separators enhances model performance by leveraging predictable reasoning patterns.Tutunov et al.'s latent space theory extension provides a foundation for designing few-shot prompting methods, where increasing high-quality exemplars within the prompt minimizes ambiguity and maximizes coherence in reasoning, which is critical in commonsense tasks like StrategyQA.Their model also justifies iterative refinement techniques such as self-correction loops (e.g., Reflexion [171]), where LLMs re-evaluate their reasoning chain steps by conditioning future reasoning on past outputs.Besta et al.'s trade-off analysis directly supports taskspecific topology selection, showing that tree-based prompting is ideal for hierarchical problem-solving (e.g., Least-to-Most Prompting [238] in multi-step arithmetic) while graphbased approaches best suit compositional reasoning (e.g., multi-hop retrieval tasks like HotPotQA [215]).Their latency vs. volume model further informs computationally efficient prompting strategies, helping select whether sequential chain-based prompting (low compute) or graph-structured prompting (better recall at higher cost) is preferable for realworld deployments, such as chatbot-based tutoring systems or automated theorem proving.Finally, benchmark-driven studies on GSM8K, ProofWriter, and FOLIO validate these structured prompting paradigms, demonstrating how carefully designed prompt hierarchies improve logical correctness in formal reasoning tasks.</p>
<p>RESEARCH OPPORTUNITIES</p>
<p>We now review future research directions in structureenhanced prompting.</p>
<p>Exploring New Topology Classes An intriguing related approach is the study of novel topology classes, such as hypergraphs, in prompting schemes.The existing HoT scheme [216] primarily uses triples, barely scratching the surface of what hypergraphs can offer in terms of their connectivity structure, where hyperdges can connect arbitrary subsets of vertices.Future research could delve into how hypergraphs can accommodate more complex relationships in data such as motifs [18], dense subgraphs [30], cliques [25], [75], [178], and others, potentially leading to breakthroughs in LLM's understanding and reasoning capabilities.</p>
<p>Explicit Representations in Single-Prompt Settings The explicit representation of prompting topologies has been largely unexplored, especially in scenarios involving single prompts.Research in this area could focus on how different representations, such as Adjacency Lists, Adjacency Matrix, or numerous others [29], can be employed to enhance the efficiency and effectiveness of LLMs.This exploration is vital, considering the impact of different data structures on the processing and interpretation of information by LLMs.The challenge lies in integrating these complex representations into a single-prompt format without compromising the simplicity and accessibility of the models.</p>
<p>Automatic Derivation of Tree and Graph Topologies</p>
<p>The majority of current tree and graph topologies in LLM prompting are created manually or semi-automatically.A promising research direction is the development of methodologies for the automatic derivation of these topologies.</p>
<p>Automating this process could significantly reduce the time and effort required to configure LLMs for specific tasks, making them more accessible and efficient.This automation might involve leveraging machine learning algorithms to identify and implement the most effective topologies based on the nature of the task and the data involved.</p>
<p>Advancements in Single-Prompt Schemes While there are a few examples of single-prompt schemes, such as the initial Chain-of-Thought (CoT), there's much room for improvement.Future research could focus on how to encapsulate more complex and detailed tree and graph structures within a single prompt, because it could reduce computational costs and simplify user interaction with LLMs (as one does not need multiple prompting interactions to build a given topology).The challenge lies in balancing the richness of the prompt with the need to maintain clarity and avoid overwhelming the model.Here, one could harness a recent line of works related to encoding graph structures within a prompt, such as GPT4Graph [80], GraphText [234], GraphGPT [185], LLMs-as-Predictors [44], and others [36], [62], [94], [96], [128], [157], [190], [191], [220], [226], [227], [232].</p>
<p>Investigating New Scheduling Approaches Most current structure-enhanced prompting schemes rely on standard scheduling algorithms like Breadth-First Search (BFS), Depth-First Search (DFS), or are manually designed.Exploring new scheduling techniques could lead to more efficient and effective processing of prompts.This could include adaptive scheduling algorithms that adjust their approach based on the nature of the task or the responses of the LLM, potentially enhancing the model's performance in complex reasoning tasks.</p>
<p>Investigating Novel Graph Classes An interesting idea is to explore graph classes for more effective and more efficient reasoning topologies.While current approaches have harnessed, among others, general directed graphs, utilizing specialized ones could lead to better reasoning routines.One could harness, for example, structures behind low-diameter networks [19], [20], [98], [108], [109], [117], [118], [119] for potential gains in theoretical properties of LLM reasoning, such as lower latency.</p>
<p>Integration with Graph Algorithms and Paradigms Integrating graph-related algorithms and paradigms could offer more powerful representations and schedules in LLM prompting.This integration might involve the use of advanced graph algorithms to optimize the structure and flow of the prompting process [11], [13], potentially leading to more accurate and efficient outcomes.Research could explore how such different graph paradigms [26] can be adapted to the unique requirements of LLM prompting.</p>
<p>Diversifying Modalities in Prompting Different modalities in prompting, such as visual, auditory, or kinesthetic, are currently underexplored.Research in this area could involve developing multi-modal prompting systems that can understand and respond to inputs in various forms while harnessing the advantages of graphs or trees of thoughts.This diversification could lead to more interactive and inclusive LLM systems that cater to a wider range of users and use cases.</p>
<p>Enhancing Retrieval in Prompting Retrieval in prompting is another area that has received certain attention, with various recent schemes [6], [7], [40], [69], [101], [107], [195], [203], [223].Improving retrieval mechanisms could enhance the LLM's ability to access and utilize relevant information more efficiently.This might involve developing more sophisticated algorithms for data retrieval or integrating external databases and knowledge bases [4], [5], [15], [16], [53] to expand the scope and depth of the LLM's responses.</p>
<p>Parallel Design in Prompting The aspect of parallel design in runtime prompting is an area that remains mostly unaddressed, and only a few schemes such as Skeletonof-Thought address this challenge.Building upon these attempts could significantly enhance the speed and efficiency of LLMs.Research could focus on developing models that can simultaneously process multiple components of a prompt or handle various tasks in parallel, or the appropriate mapping to massively parallel architectures [19], [28], [74], [97], thereby reducing latency and improving user experience.One could also investigate effective integration of prompting with distributed-memory infrastructure and paradigms, such as remote direct memory access (RDMA) [21], [55], [56], [71] or serverless processing [49].</p>
<p>Integrating Structure-Enhanced Prompting with Graph Neural Networks A potential area of exploration is the integration of structure-enhanced prompting with Graph Neural Networks (GNNs) [22], [27], [37], [112], [208], [230], [239] and other mechanisms for Graph Machine Learning [12], [33], [83].GNNs, known for their proficiency in handling relational data and capturing dependencies in graph structures, could augment the capabilities of LLMs in processing complex, structured prompts.By embedding prompting structures into graph-based representations, GNNs can provide a more nuanced and context-aware interpretation of the prompts, potentially leading to richer and more accurate responses.Furthermore, leveraging graph-related embeddings [12], [76] can enhance the LLMs' ability to capture the subtleties in the relationships and hierarchies present in the prompts.Finally, harnessing heterogeneous GNNs [140], [182], [224] for integration with the concept of different semantic roles of vertices could be an interesting direction.</p>
<p>Integrating Structure-Enhanced Prompting with Complex System Architectures An essential and emerging area of research is the integration of prompting capabilities into the environment of complex existing system architectures, such as graph representation learning systems [64], [122], [193], graph databases [14], [17], [23], or relational databases.This integration aims to facilitate direct and nuanced interactions with complex data structures stored in these systems.By embedding LLMs into these environments, the prompting process can leverage the inherent organizational and relational capabilities of these databases.This approach would allow LLMs to access, interpret, and manipulate large and intricate datasets more efficiently and accurately.For instance, integrating with graph databases could enable LLMs to naturally understand and utilize the connections and relationships within data, while relational databases could provide a structured and queryable data format that complements the LLMs' linguistic capabilities.</p>
<p>Hardware Acceleration Understanding energy and performance bottlenecks and mitigating them with specialized techniques such as processing-in-memory [1], [24], [73], [147], [148], [169], FPGAs [28], [54], [145], or even quantum devices [9] will likely be increasingly important.Such advances can also enable much more scalable models and model execution under stringent conditions.</p>
<p>RELATED WORK</p>
<p>We also broadly discuss related analyses, taxonomies, and surveys.</p>
<p>General Prompt Engineering</p>
<p>There exist several detailed overviews of general prompt engineering.The main difference is that we provide the first taxonomy and analysis of structure-enhanced prompting methods, in which we focus on the topology of the LLM reasoning.Wang et al. [198] provide an overview of interactive NLP, in which they outline interactions of LLMs and humans, knowledge bases, models/tools, and environments.Gu et al. [78] provide a systematic survey of prompt engineering on vision-language foundation models.Liu et al. [131] describe in great detail general prompting and describe this area through the "pre-train, prompt, and predict" paradigm of building a prompting scheme.Qiao et al. [158] overview prompting schemes related to reasoning.Chen et al. [38] review the potential of prompting with LLMs.Finally, Zhang et al. [233] and Chu et al. [47] review CoT and the associated prompting schemes.</p>
<p>Graph-Related Generative AI</p>
<p>There have also been numerous works on graphs and LLMs.The key difference is that these works focus mostly on using graph structures as input data (at pre-training, finetuning, or prompting stage).We instead focus on graphs (and other structures) as mechanisms enabling structured LLM reasoning [95].</p>
<p>General overviews of using graphs together with LLMs have been outlines by Li et al. [123] and Zhang et al. [231].Pan et al. [152] very briefly mention some of the methods for integrating graphs with LLMs.Zong et al. briefly mention graph-related fusion in their work on self-supervised multimodal learning [241].Yang et al. [81] analyze data-centric graph learning.</p>
<p>Next, various works propose to enhance general generative models with knowledge graphs (KGs).The focus of these works is usually to use KGs in order to enhance the LLM answers, for example by grounding knowledge in general models to reduce effects such as hallucinations [92], [151], [200], [213], [214].Example schemes include Knowledge Graph Prompting (KGP) [196], Graph Neural Prompting (GNP) [187], Think-on-Graph (ToG) [180], Knowledge Solver (KSL) [63], KnowledGPT [195], and others [32], [139].Zhu et al. [240] discuss how LLMs can be used for enhancing KG construction and tasks.Wen et al. [201] present MindMap, a framework to perform reasoning on KG data.Pertinent triples from a KG are retrieved and the LLM is prompted to answer a question based on these triples and show the reasoning process by generating a "mind map" in the form of a textual reasoning tree.</p>
<p>Several works discuss graph foundation models [67], [130].These works discuss how to -in analogy to LLMs -devise general models pre-trained on massive amounts of graph data, that could then be fine-tuned for more specific applications, and used together with prompting for answering various graph-related tasks.</p>
<p>Recents schemes illustrate how to run analytics on graphs using prompting [102]; examples include GPT4Graph [80], GraphText [234], GraphGPT [185], LLMsas-Predictors [44], and others [36], [62], [94], [96], [128], [157], [190], [191], [220], [226], [227], [232].</p>
<p>Finally, several works [181], [207] describe techniques for graph prompt learning, which is a class of approaches for enhanced prompting with graph pre-trained models.This class of schemes is orthogonal to our work, because it focuses on prompting for graph-oriented tasks, and it does not target the LLM structure of reasoning.</p>
<p>CONCLUSION</p>
<p>In conclusion, the rise of Large Language Models (LLMs) has revolutionized machine learning, extending their applications beyond Natural Language Processing (NLP) into diverse fields like medicine, logical reasoning, and planning.Prompt engineering has emerged as a crucial area, democratizing access to LLMs and offering a cost-effective alternative to fine-tuning and pre-training.However, challenges arise in optimizing LLM queries for complex tasks due to the inherent limitations of generative Transformer models.</p>
<p>This paper addresses these challenges by introducing a blueprint and an accompanying taxonomy of prompting schemes, focusing on the underlying structure of reasoning.We propose to model a general prompting scheme as a graph topology, where different classes of graphs, such as k-ary trees or directed graphs, can be used to reflect the structure and gain insights into different prompting schemes.The taxonomy is then used to survey and analyze existing designs, dissecting them into fundamental aspects such as the representation of the reasoning topology, the derivation of the topology, or the encoding of the reasoning schedule.This taxonomy forms a blueprint that can be used to facilitate designing more effective prompting schemes.</p>
<p>We also conduct an analysis of structure-enhanced prompting methods in terms of their accuracy and quality of outcomes, latency, and cost-effectiveness.Our investigation results in different insights into the tradeoffs between prompting schemes, which facilitate selecting the best method for a given budget or workload target.</p>
<p>Furthermore, we investigate the preliminary works into foundations of structured-enhancing prompting, parallel and scalable designs, and productivity and programmability.We also provide valuable insights into open challenges and potential research directions, navigating the path for future research avenues into more advanced prompting.</p>
<p>APPENDIX A DETAILED DESCRIPTIONS OF CHAIN SCHEMES</p>
<p>We study prompting schemes that use chain topologies.We list them and determine their relationship to our blueprint in Table 1.</p>
<p>A.1 Single-Prompt Schemes</p>
<p>Chain-based reasoning can be conducted within a single prompt, potentially containing multiple in-context examples.It is used in both textual and visual reasoning tasks.</p>
<p>In the seminal Chain-of-Thought (CoT) [199] scheme, in-context examples consisting of intermediate reasoning steps for a given complex question are introduced to prompt the LLM to generate a similar multi-step reasoning chain.This chain is used to improve the quality of the LLM answer compared to basic IO prompting.</p>
<p>Zero-shot-CoT was proposed by Kojima et al. [113].It comes with multi-step reasoning without in-context examples, by simply prompting the LLM with one sentence, "Let's think step by step", or using other similar statements.This zero-shot prompting improves upon other zero-shot methods in terms of the accuracy of reasoning outcomes.</p>
<p>SelfAsk [154] is a single-prompt scheme similar to CoT which augments the in-context examples with intermediate questions.Instead of only providing a step-by-step reasoning chain in the examples, each step in this chain is expanded to pose a follow-up question and then answer it, e.g., "Superconductivity was discovered in 1911." is augmented to "Follow up: When was superconductivity discovered?Intermediate answer: Superconductivity was discovered in 1911".</p>
<p>Plan-and-Solve (PS) Prompting [192] builds a chain based on a devised plan, harnessing Zero-shot CoT.It first divides the complex task into a list of sub-tasks and then executes the stepwise plan for the solution.PS can be extended to PS+ by adding instructions in the prompt to request the LLM to extract variables and explicitly calculate the intermediate values.PS+ has no impact on the length of the chain.</p>
<p>Program of Thoughts (PoT) [41] is a single-prompt scheme similar to CoT which generates code to solve a question.Rather than the natural-language-based examples used in CoT, the LLM is prompted by code exemplars, where each statement refers to one intermediate step in the reasoning chain.Zero-shot prompting can also be applied, e.g., "Let's write a Python program step by step and return the result.Firstly we need to define the variables.".The final result is obtained by executing the generated code.</p>
<p>A.2 Multi-Prompt Schemes</p>
<p>Elicited by multiple rounds of prompting, the LLM reasoning process can be a chain consisting of several messages.</p>
<p>Selection-Inference (SI) [51] aims to address multi-step logical reasoning problems where all necessary information is provided within the input context.Each reasoning step in SI involves two distinct sub-tasks: selection and inference.Firstly, the selection sub-task is responsible for identifying and selecting relevant information needed for the subsequent reasoning step, effectively pruning the context.Following this, the inference sub-task generates a new intermediate piece of information, thereby performing the actual reasoning step.This newly generated information then becomes available for use in subsequent reasoning steps for the selection sub-task.The length of the chain is bounded to a fixed number of steps, and the prompts for both sub-tasks include relevant few-shot examples.</p>
<p>Chain-of-symbol (CoS) [91] is a multi-prompt scheme utilizing two prompts for solving spatial planning tasks in natural language.Prompted in a zero-shot manner, the LLM initially generates CoT-like in-context exemplars and modifies them by replacing the spatial relationships with symbols.For example, the navigational task "Start at bank A. There are two stores on the map, store B and store G.The road from bank A to store B goes through bank C, house H, and cinema F to store B, totaling 600 meters." is succinctly converted into the symbol sequence "bank A / bank C / house H / cinema F / store B, (200 + 100 + 100 + 200 = 600).".Exemplars simplified in this way are then provided as prompts for LLMs to conduct CoT-based reasoning.</p>
<p>A.2.1 Multi-Prompt Chains with Decomposition</p>
<p>The introduction of a node for decomposition at the start of a reasoning chain can not only determine the chain's depth, but also facilitate more detailed reasoning approaches in the subsequent sub-steps.This allows for a finer-grained resolution of sub-questions, enriching the overall reasoning process.Apart from the schemes described below, similar strategies are applied in works [60], [103], [221].</p>
<p>Least-to-Most Prompting [238] grows a reasoning chain, where the decomposition of complex tasks is conducted in the first node and the sub-tasks are solved in the subsequent nodes.Prompted with few-shot examples showcasing the decomposition of questions into lists of sub-questions, the LLM firstly generates a sequence of sub-questions given an original question.Then the solving process starts by providing the LLM with example solutions for simple questions.Subsequently during each sub-step, the LLM generates the sub-solution, given the question for this step and the list of previously solved questions with the generated answers.The chain terminates when the final answer is returned after solving all sub-problems in the list.</p>
<p>Decomposed Prompting [106] is a modular framework for a detailed decomposition of complex tasks.To generate a reasoning chain, the LLM is prompted by demonstrations comprised of sequential question-operation triplets, which form "sub-questions".The growth of the chain follows the autoregressive pattern of CoT, while each sub-step captures the input and passes the output through the contextual environment.Under the limited scope of the tasks, the types of operation functions are fixed, enabling the LLM to conduct in-context learning for operations before inference.</p>
<p>A.2.2 Multi-Prompt Chains with Verification</p>
<p>The introduction of verification enables the reasoning frameworks to iteratively refine the generated context.With this strategy, the execution of chain-based reasoning is effectively extended with loops, with conditions on how many times one can loop over a node (based on the number of iterations or some terminal conditions).Apart from the schemes described below, similar strategies are applied in works [124], [127], [202].</p>
<p>LogiCoT [236] is a zero-shot, multi-prompt framework that leverages iterative verification to minimize cumulative errors in the reasoning chain.After generating a reasoning chain through few-shot CoT, this framework systematically navigates the reasoning chain with zero-shot prompts to pinpoint the first instance where the original reasoning deviated.It preserves the accurate part of the reasoning chain up to the erroneous node and then prompts the LLM to diagnose and correct this specific misstep, thereby forming an alternative reasoning chain.This process repeats until a fully verified reasoning chain is established, with each node passing the verification.SELF-REFINE [141] adopts a distinct approach to iterative refinement compared to LogiCoT.Instead of focusing on pinpointing and correcting errors within a reasoning chain, it initially generates a complete output and then enters a cyclical process, alternating between generating specific feedback with concrete actions for enhancement and applying this feedback to refine the output.</p>
<p>Similar to SELF-REFINE, Reflexion [171] employs the concept of iterative refinement through generating and applying feedback, but introduces a modular approach to the process, segmenting the refinement into specialized stages.It starts with an initial output by the LLM, followed by an evaluation through task-specific grading functions, and then generates informative feedback, which is subsequently applied.This reasoning chain continues looping until the evaluation stage confirms that the output is correct.</p>
<p>Reasoning Graph Verifier (RGV) [35] is a framework that addresses arithmetic questions.Their approach is a variant of CoT-SC, in which they employ a trainable verifier module to decide on the best CoT solution.</p>
<p>A.2.3 Multi-Prompt Chains with External Tools</p>
<p>To better integrate multiple execution methods, some schemes opt to devise a plan that specifies tools for handling each sub-task, before executing the reasoning chain.</p>
<p>Plan, Verify and Switch (PVS) [132] iteratively engages in planning, verification, and switching among different methods until the acquisition of the final answer.Initially, the LLM is prompted with definitions of three chain-based reasoning methods, which include CoT, PoT, and a method using the solution of linear equations to represent the reasoning process (named EoT).Then, at the first stage of each iteration, the LLM selects a reasoning method according to the input question and subsequently follows in-context examples on the application of the method to complete a reasoning chain accordingly.Then, the verification module acquires conditional information and the intermediate variables from the sub-steps of the chain and executes calculations, validating the correctness of the final answer.If the verification fails, the next iteration is activated, in which the LLM selects a new method from the non-selected ones and generates a new chain without memories of the previous reasoning process.The planning module facilitates the dynamic switch of reasoning methods, thereby integrating the strengths of different paradigms.</p>
<p>Chameleon [137] is a reasoning framework that provides the LLM with access to various external tools, such as programming interpreters and table readers.Given the question, tool descriptions, and tool usage examples, the LLM generates a plan for the reasoning chain, in which each node corresponds to a tool used in the sub-step.During the execution of the chain, the output of previous steps together with the cached tool execution results are taken as the input of each current step.The nodes of the chain are executed sequentially until the final result is returned.</p>
<p>There are also tool-based chain schemes with no planning module; they dynamically select the reasoning method.</p>
<p>ChatCoT [45] realizes tool-based reasoning chains through multi-round conversations with the LLM.Initially, the LLM is shown how to decompose CoT reasoning as a multi-turn conversation, in order to learn problem-solving in a step-by-step, autoregressive conversational way.Additionally, the knowledge of tools and relevant examples are included in the context to elicit their selection and correct usage.During inference, the chain proceeds and unfolds without a premature plan until the problem is solved, while the LLM simultaneously selects and executes appropriate tools during the multi-turn conversation.The conversation is constrained to a maximum number of turns, thereby constraining the maximum reasoning depth, while ensuring all previous turns are kept within the context window.</p>
<p>APPENDIX B DETAILED DESCRIPTIONS OF TREE SCHEMES</p>
<p>We next analyze works that harness trees as the prompting topology.We structure the discussion based on the harnessed topology variants, see Figure 9.</p>
<p>B.1 Trees of Chains</p>
<p>While harnessing trees as prompting topologies has been explicitly established in the works by Long [134] and Yao [217], this idea has been present earlier.Specifically, Chain-of-Thought with Self-Consistency (CoT-SC) [194] is an early scheme that harnesses the tree structure to a certain degree.Here, multiple CoTs originate from the same initial (root) prompt, forming a "tree of chains".The chain providing the best outcome to the initial question, is selected as the final answer.</p>
<p>B.2 Single-Level Trees</p>
<p>Skeleton-of-Thought (SoT) [150] is a prompting scheme aiming to reduce the end-to-end generation latency of LLMs, caused by their inherent sequential decoding.Instead of generating one long continuous answer, this scheme uses a divide-and-conquer approach.In a first prompt, the LLM is instructed to generate a skeleton of the answer, i.e., a list of points that are independently answerable.Then, for each of these points, a new prompt is issued in parallel to answer just this specific part of the question.As these points are processed in parallel, the overall latency is reduced.One can additionally include a prompt at the beginning which lets the LLM decide on solving the question via independent points, or -whenever reasonable -by using a single continuous answer.</p>
<p>B.3 k-Ary Trees</p>
<p>First, the Tree-of-Thought (ToT) design by Long [134] utilizes a tree structure to decompose a problem into subproblems and solve them using separate LLM prompts.After the LLM suggests possible next steps and corresponding partial solutions, a checker module decides if any of these solutions is valid, whether it can be selected as the final one, or whether it should backtrack to the previous step.All issued prompts and answers are explicitly stored as a tree structure and navigated through using a controller module.The LLM prompting is only used to generate the next individual steps (i.e., hops) in this tree, whereas the overall problem solving process is coordinated by the controller.</p>
<p>Tree of Thoughts (ToT) by Yao et al. [217] differs from the above ToT approach in using the LLM itself as a solution evaluator with access to all generated solutions, instead of using a programmed or learned evaluator module.This allows to rate states individually or vote across intermediate solutions to select the most promising one to continue with the search.Both mentioned ToT approaches are a generalization of the IO, CoT, and CoT-SC prompting schemes.</p>
<p>B.3.1 Pre-ToT Schemes</p>
<p>Thought Decomposition [210] is a multi-prompt scheme based on stochastic beam search and self-evaluation.At each level (reasoning step), n new intermediate nodes are generated for each of the (usually) k input nodes.Each node of this set of nk nodes is evaluated and then the set is pruned down to k output nodes via sampling with stochastic beam search.For both the generation and the evaluation prompts, few-shot examples are used with a focus on textual or program-aided reasoning, depending on the dataset used for evaluation.</p>
<p>Creswell and Shanahan [50] describe an extension of the chain-based Selection-Inference [51], which is used to answer multiple-choice questions by chaining together correct reasoning steps over a predefined context, i.e., a set of statements (the provided statements are sufficient to derive the answer to the question).The space of all possible reasoning chains forms a tree rooted at the initial question.Each node represents a reasoning step that is derived by (1) selecting a subset of statements from the context and then (2) inferring a new statement, which is then added to the context.For both steps, the authors use separately finetuned LMs.This tree is explored using beam search, where another fine-tuned LM assesses the value of the current node, and the search ends as soon as a halter-LM decides that the question can be answered with the current context and then answers it.</p>
<p>Dynamic Least-to-Most Prompting [59] extends leastto-most prompting with a tree-based problem decomposition and a dynamic external tree-based few-shot example selection.The goal is to turn natural language questions into formal representations such as SPARQL queries.Initially the input question is decomposed into sub-problems using a series of prompts.This process yields an arbitrary decision tree for the input in contrast to a chain in traditional least-tomost prompting.This decision tree is used to dynamically select few-shot examples by externally matching that tree against a precomputed decision tree for examples.In the final step, the decision tree is linearized into a sequence of sub-problems with increasing complexity which are then translated by using prompts enhanced with the respective examples selected in the second step.</p>
<p>B.3.2 Post-ToT Schemes</p>
<p>Different schemes have extended the initial ToT designs.Algorithm of Thoughts (AoT) [168] is a single-prompt approach that utilizes in-context examples formulated in an algorithmic fashion.Instead of providing step-by-step examples as in CoT prompting, AoT harnesses algorithmic reasoning steps in the examples, that explore the solution space (tree) with either DFS or BFS.As opposed to ToT, AoT uses only a single prompt.</p>
<p>Tree of Uncertain Thought (TouT) [146] extends ToT with local "uncertainty scores" by incorporating the variance of multiple LLM responses into the state evaluation function.</p>
<p>Tree-of-Mixed-Thought (TomT) [93] uses ToT-based prompting to answer questions on visual scene graphs, e.g., "Does the red chair have the same number of legs as the table in front of it?".The LLM is tasked to generate Python code that solves the question using a set of provided functions to extract and reason on the data of the scene graph.This prompting scheme uses a DFS variant of ToT where each individual node generates up to s consecutive reasoning steps, i.e. lines of code, opposed to the original ToT where s = 1.Evaluating the correctness of the reasoning steps is done as in ToT for each node, where one checks if the generated code so far can (1) run correctly, and (2) the interactions with the scene graph are consistent with the existing elements in the scene.</p>
<p>Tree of Clarifications (ToC) [107] enables to answer ambiguous questions by first retrieving pertinent external information and then recursively prompting an LLM to construct a tree of disambiguations for the initial question.This tree is explored using BFS and after no more ambiguations exist, a long form answer is generated, by combining all the previously explored nodes.</p>
<p>Tree Prompting [173] is a high-level approach that proposes the training of a binary decision tree to classify text during inference.Several mechanisms to construct the prompts that make up the nodes of the decision tree during learning are proposed: prompts based on few-shot examples, human curated instruction prompts, dynamic prompts constructed with discrete prompt search methods like iPrompt [174], and kNN prompting [211], which constructs new nodes based on the nearest neighbors.kNN prompting can result in multiple prompts per decision tree node with the other mechanisms using only a single prompt.Additional improvements can be made by employing tree ensembles such as random forests [121] or gradient-boosted trees [65].During inference the number of prompts is bound by the depth of the decision tree, where as the training can be resource intensive.</p>
<p>B.4 Analysis &amp; Comparison of Designs</p>
<p>We now broadly discuss and analyze tree designs with respect to different aspects of our blueprint.A detailed analysis can be found in Appendix E.2.</p>
<p>B.4.1 Topology &amp; Its Construction</p>
<p>The key novel architectural feature of tree schemes is exploration of a step, i.e., the ability to generate multiple new steps based on a given single one.The vast majority of tree schemes are multi-prompt.Most multi-prompt schemes use a dynamic approach to building the tree topology.The details of how the topology is exactly shaped depend on the specific question.For most multi-prompt approaches, the user can adapt the tree topology to a certain degree, i.e., by varying the branching factor and limiting the depth of the tree.</p>
<p>B.4.2 Performance</p>
<p>Increasing the branching factor often leads to more diversity of outcomes, which can be beneficial for accuracy, but also increases the number of prompts, i.e., computational cost.The most advantageous branching factor is hard to find and often depends on the specific problem to solve.Easily decomposable problems may benefit less from more branching than complex problems.Specifically, more complicated problems profit more from decomposing them into many/diverse sub-problems (e.g., this ensures enough diversity for self-consistency to work better).In contrast, a question that has clearly only two sub-parts does not benefit from many more subdivisions, as the additional branches can either be redundant or wrong.Single-prompt approaches can perform better on some problems than multi-prompt approaches, while using only a single prompt compared to possibly hundreds.</p>
<p>APPENDIX C DETAILED DESCRIPTIONS OF GRAPH SCHEMES</p>
<p>We finally describe schemes with topologies beyond trees or chains.We observe that they can be further grouped into subclasses, based on the harnessed class of graphs.We picture different types of graph-based schemes in Figure 9.</p>
<p>C.1 Special Classes of Graphs</p>
<p>Different schemes harness certain classes of graphs.</p>
<p>Branch-Solve-Merge (BSM) [164] employs a 1-level double tree structure to first divide a problem into independently solvable sub-problems, and then combine them into a final solution.The first prompt instructs the LLM to propose sub-problems, which are then solved independently.The final prompt instructs the LLM to merge the results of the sub-problems into a single output.</p>
<p>Thought Propagation (TP) [222] employs a multi-level double tree structure for prompting the LLM.This approach follows the idea that multi-step problems are easier to solve with access to example solutions of similar problems.This is done in three steps.In the first step, given an input problem the LLM is prompted to propose a set of related problems.The main idea here is that solving these related problems can then be used as reference examples for solving the input problem instead of reasoning from scratch.Additionally, solving similar problems can lead to high-level plans for the input problem and allows TP to rectify errors during planning.The LLM is prompted using in-context examples to generate related problems for both situations.In the second step, the LLM is prompted to solve the input problem as well as the related problems.For this prompt, existing prompting techniques, e.g., CoT, ToT, etc., can be used.Even though the solutions to the related problems are not expert-level, they can be refined in the next step.In the last step, the LLM is prompted to come up with new solutions for the initial problem based on the solutions from the related problems.Additionally, the LLM is prompted to derive high-level plans to solve the input problem using the solutions of the related problems.This process can also be extended to recursively generate further related problems, which yields in the general form a double tree structure with depth k.</p>
<p>Socratic Questioning [156] is a prompting scheme that models recursive exploration of the thought space using a tree structure.Hereby, the original question is recursively decomposed into sub-tasks until all tasks can be solved with high confidence.These results are then aggregated and propagated back up the tree to answer the original question.This results in an overall double tree reasoning topology.Additionally, the approach also provides a model to generate an image caption related to the text prompt and thus allows multi-modal reasoning.</p>
<p>C.2 Directed Graphs</p>
<p>Some schemes embrace a general directed graph model.</p>
<p>Graph of Thoughts (GoT) [10] uses a multi-prompt approach to improve the LLM problem solving performance by decomposing a given task into sub-tasks that form a graph.This decomposition is specified as a Graph of Operations (GoO).The GoO coordinates how the LLM is prompted and how the results, which form a separate graph called Graph Reasoning State (GRS), are further used in the reasoning process.</p>
<p>Graph of Thought [120] presents a multi-prompt approach where a graph is constructed recursively in a DFS manner by starting at the question node that represents the question to be answered by the LLM.From this node, possible reasoning paths are generated by the LLM.For each path, new nodes, i.e., intermediate reasoning steps, are generated by the LLM and are then used to grow the graph.To limit the size of the graph, the scheme uses a depth limit and requires a set of condition nodes to be provided at the start.These nodes represent axioms for the reasoning process that form initial nodes in the reasoning paths.After the graph construction, a path from the condition nodes to the question node is searched, and a checker module validates each reasoning step along this path.This checking is done by a series of calls to the LLM and only passes with a positive score, if all calls agree on the validity of the step.If no valid path is found, the graph is updated to enable valid reasoning paths.In the first step of the update, all nodes only depending on condition nodes (via a valid step) are added to the condition set.Then, new nodes and edges are added to the existing graph in the same DFS manner as when creating the graph from scratch.These graph updates are repeated until a valid path to the question node is found and therefore a solution to the initial question.</p>
<p>Graph-of-Thought [219] describes a two-stage framework to answer multi-modal questions, i.e., textual questions accompanied by images.In the first stage, the model generates natural language rationales based on the input text, which provide additional context and knowledge to support answering the given question.This rationale generation is learned as part of the overall model pipeline.In the second stage, these rationales are then appended to the initial question and passed again through the model to predict an answer.The prediction consists of multiple steps.First, subject-verb-object triples are extracted from the input question using the Stanfords OpenIE system [3].Using these triples, a Graph-of-Thought is constructed by first converting each triple to a 3-node path and then merging similar nodes using the Stanford CoreNLP system [143].Next, the input text and the accompanying image are encoded using a Transformer encoder and a vision encoder, respectively.The Graph-of-Thought is encoded using a GAN.The resulting features from text and image are then combined using crossattention and passed through a gated fusion layer before passing them to the final Transformer decoder that predicts the rationales in the first stage and the answers in the second stage.The scheme uses the pre-trained T5 [160] model and fine-tunes it for the rationale generation.</p>
<p>ControlLLM [133] is a framework that allows answering multi-modal queries, e.g.understanding and generating images, videos, and audio.First, the LLM is prompted to decompose the query into predefined sub-tasks, e.g., "imageprocessing" or "video-generation".Hereby, the LLM also infers appropriate input and output types, e.g., text or image, from a predefined set for each sub-task.In a next step, a so called Tool Graph is constructed by using the generated subtasks and the input resources, e.g.video or text, as nodes.Connections between the nodes are drawn for all compatible input/output types.Nodes corresponding to sub-tasks or resources are called tool nodes or resource nodes respectively.In a next step, the graph is searched in a DFS manner until a solution, i.e., path from input to output resource node, is found.The authors evaluated different heuristics for the DFS, i.e., greedy search, beam search, adaptive beam search and exhaustive search.The graph traversal is guided by the LLM and in each step the LLM is prompted to rate the relevance of the connected tool nodes for solving the overall task, this rating is then used as the heuristic.In a last step, the found solution, i.e., path from input to output in the tool graph, is executed to retrieve the requested output.Finally, the LLM is prompted to summarize the result for the user.The overall approach is similar to GoT [10], with the main difference being that each node can access and use a tool and the graph traversal uses a LLM-based heuristic.</p>
<p>Cumulative Reasoning [229] describes a paradigm to solve multi-step problems by iteratively constructing a directed acyclic graph (DAG).A proposer module suggest a next deduction step based on any previous steps, a verifier module evaluates the proposed step and finally, a reporter module checks if a valid solution has been reached and otherwise a next step is proposed.In this approach, one manually writes problem specific prompts that follow this paradigm.One also manually provides the implementation on how the individual modules interact, i.e., how the DAG is formed.Conceptually, this approach is an extension of ToT as it allows each step to use all previously derived results, meaning different sub trees can be connected to form a DAG.</p>
<p>Everything of Thoughts (XoT) [57] is a two-stage framework utilizing a reinforcement learning model trained using Monte Carlo Tree Search on specific problem sets to generate graphs of thoughts as well as iteratively prompting the LLM to revise and infer solutions based on these graphs.First, the model has to be trained in advance and can be applied to solve problems with clear intermediate steps and solution states, e.g., Game of 24 or 2x2 pocket cube.This model is then used to infer a graph of thought where paths leading to a solved state of the problem are converted to text and fed to an LLM as assisting knowledge to solve the initial question.The LLM is instructed to review the steps and refine them if necessary, hereby iteratively refining the solution graph by applying the first stage to replace incorrect nodes.</p>
<p>ResPrompt [100] is a single-prompt approach extending CoT by augmenting the few-shot examples with residual links, i.e., previously derived results.If a current reasoning step depends on previous results, these results are referenced verbatim in parentheses, effectively forming an arbitrary reasoning graph.Later stages can explicitly rely on multiple previous results as opposed to a single previous result.</p>
<p>C.3 Hypergraphs</p>
<p>Finally, we also consider a hypergraph, which generalizes a graph by enabling edges to connect arbitrary subsets of nodes instead of being links between just two nodes.We include hypergraphs in the taxonomy, because preliminary works already harness them for multi-modal prompting [216].</p>
<p>Hypergraph-of-Thought (HoT) [216] is a multi-modal reasoning paradigm modeling the thought process as a hypergraph.First, a graph-of-thoughts as in [219] is constructed.Then a textual hypergraph is constructed, sharing the same nodes.The hyperedges are then defined as node triples, e.g., "(Lionel Messi, place of birth, Rosario)".Additionally, a visual hypergraph-of-thought is constructed by performing k-means clustering on image patches, where a cluster corresponds to a hyperedge.Both hypergraphs are then encoded and combined to perform graph learning.</p>
<p>C.4 Analysis &amp; Comparison of Designs</p>
<p>We now broadly discuss and analyze graph designs with respect to different aspects of our blueprint.A detailed analysis can be found in Appendix E.3.</p>
<p>C.4.1 Topology &amp; Its Construction</p>
<p>Firstly, the considered schemes exhibit a blend of singleand multi-prompt aspects, allowing for a high degree of flexibility and control over the prompting process.This is evident in the diverse approaches taken by different schemes such as GoT, ControlLLM, and Cumulative Reasoning, each offering unique ways of constructing and utilizing graphs for problem-solving.Secondly, the user's control over the topology of the graph is significant in most schemes, enabling customization of the reasoning process based on specific needs, such as setting branching factors or defining the depth of the graph.Thirdly, the role of the LLM in these graph-based schemes is multifaceted, involving the generation, evaluation, and modification of nodes within the graph, as well as determining the conclusion of the reasoning process.Lastly, there is a notable variation in the degree of user and LLM influence on the topology across different schemes, with some allowing direct user control, while others rely on predefined heuristics or the LLM's decision-making capabilities.</p>
<p>C.4.2 Performance</p>
<p>The considered works universally show improvements in effectiveness of graph-based prompting schemes over chains and trees across various tasks, suggesting a promising direction for future research and application in the field of AI and machine learning.</p>
<p>APPENDIX D BENCHMARKS</p>
<p>In this section, we introduce benchmarks designed to test the reasoning capabilities of LLMs.These benchmarks will be instrumental in the performance analysis discussion in the following section.</p>
<p>D.1 Arithmetic Reasoning</p>
<p>Arithmetic reasoning tasks, also called math word problems, focus on narrative-based mathematical questions where the model must extract and solve numerical equations from a given text-based scenario.In the datasets, each item is presented as question-answering (QA) pairs, in which the answers are formatted as direct answers or multiple choice.Frequently used datasets include GSM8K (Grade School Math) [48], SVAMP (Simple Variations on Arithmetic Math word Problems) [153], MAWPS (MAth Word ProblemS) [114], MultiArith [163], AddSub [90], DROP (Discrete Reasoning Over the content of Paragraphs) [61] and AQUA [126].</p>
<p>Based on these benchmarks, datasets enhanced for more comprehensive textual understanding and more complex calculations were developed.GSM-hard [68] extends GSM8K by introducing calculations of much larger numbers.AQUA-RAT [126] requires more detailed rationales in answering than AQUA.MathQA [2] selects the QAs from AQUA and provides operation programs.MATH [88] is built on the problem sets from math competitions and includes L A T E X representations.ALGEBRA [86] focuses on the algebraic domain.</p>
<p>Researchers take specific domains into consideration.FinQA (Financial Question Answering) [42] involves the analysis and interpretation of financial data, reports, and tables.ConvFinQA [43] presents the financial questions in a conversational manner.To test the comprehension of structured data, TabMWP [138] collects math word problems that involve tabular data.</p>
<p>D.2 Commonsense and Logical Reasoning</p>
<p>In commonsense reasoning, benchmarks test the abilities to understand text and to generate responses that align with human-like commonsense knowledge.Following that principal, group of datasets composed of multihop questions are constructed, including HotPotQA [215], StrategyQA [72], MuSiQue [188], Bamboogle [154], and CommaQA-E (Communicating with Agents for QA) [105].Apart from the complexity of the questions, more QA datasets incorporating stepwise explanations for the answers are built, such as 2WikiMultiHopQA (2Wiki stands for Wikipedia and Wikidata) [89], ProofWriter [183], En-tailmentBank [52].SocialQA [176] focuses on daily social events.</p>
<p>Logical reasoning tasks aim to test the ability to understand contextually cause-and-effect relationships.CauseEffect [176] tasks the model with determining the causal event given descriptions of two events.</p>
<p>D.3 Symbolic Reasoning and Other Domains</p>
<p>There are several tasks involving manipulations of symbols in different domains.For Last Letter Concatenation [199], the model concatenates the last the letters of two given words into an abbreviation.Coin Clipping [199] tasks the model with determining, whether the head side of a coin faces up after a group of "flip/not flip" commands.</p>
<p>In the domain of spatial planning tasks, Brick World [91] asks the model to acquire a specific brick after sequentially grabbing group of bricks in 2D/3D scenarios.SCAN (Simplified version of the CommAI Navigation tasks) [116] consists of sets of compositional navigation commands paired with the corresponding action sequences.Other textbased spatial tasks are also popular, such as NLVR-based Manipulation [179] and SPARTUN [144] Semantic parsing datasets focus on the ability of the models to generalize from knowledge about components.Examples include CFQ (Compositional Freebase Questions) [104], COGS (COmpositional Generalization Challenge based on Semantic Interpretation) [110], and Alchemy [135].</p>
<p>Other benchmarks are intended for special domains, such as coding (CodeNet [155]) or pedestrian traffic behavior (PIE (Pedestrian Intention Estimation) [161]) for autonomous driving.</p>
<p>Benchmarks that target multiple domains like MT-Bench [237] and MMLU (Massive Multitask Language Understanding) [87] measure how broad the knowledge of a model is.Sometimes just multiple areas of a single domain are targeted, for example with ScienceQA [136].ASQA (Answer Summaries for Questions which are Ambiguous) [177] focuses on ambiquous questions in the context of longform question answering.Several benchmarks such as ALF-World [172] and VQA-V2 (Visual Question Answering) [77] look into visual tasks, while others target logical reasoning such as LogiQA [129] and FOLIO [84].AutoTNLI [115] consists of counterfactual entity-based tables.</p>
<p>Other evaluation methods also include puzzles, like the Game of 24 (given four numbers, find the combination of those numbers and the four basic mathematical operations, so that the end result is 24), the 8-Puzzle (3x3 board with a missing piece, goal is to slide the pieces to their target location) or 2x2 pocket cube (smaller version of the Rubik's cube).</p>
<p>APPENDIX E DETAILED ANALYSES E.1 Chain</p>
<p>E.1.1 Performance Analysis</p>
<p>Chain-structured prompting methods have been applied to solve questions in diverse fields.We conduct a qualitative comparison of these methods in different domains.</p>
<p>E.1.1.1Arithmetic Reasoning: IO prompting exhibits very low performance on mathematical tasks.Compared with IO prompting, CoT attains an accuracy of around 60% (40% higher) on GSM8K [48], 80% (10% higher) on SVAMP [153] and 90% (14% higher) on MAWPS [114].The accuracy of the rationales significantly increases with the scale of LLMs.Zero-shot-CoT does not outperform CoT on arithmetic tasks, but obtains a 60% gain in accuracy on MultiArith [163] over IO prompting, simultaneously exceeding IO prompting on GSM8K, AQUA [70] and SVAMP tasks.Compared with CoT, PoT with few-shot examples improves on GSM8K and AQUA by more than 8% and 4% on SVAMP.For the financial datasets FinQA [42] and ConvFinQA [43], few-shot PoT reaches a roughly 20% improvement over CoT.In comparison with CoT, schemes that employ decomposition demonstrate a modest advantage on math reasoning tasks.Least-to-Most Prompting increases the accuracy on DROP [61] by 10% because of the strong decomposability of the problems in the dataset.Although Least-to-Most Prompting only shows a slight average improvement on GSM8K, it significantly enhances accuracy (by 5%) for problems within GSM8K that require more than 5 inference steps to solve.PS+ prompting yields at least 5% improvement on MultiArith, AQUA, AddSub [90], and SVAMP, but only a slight gain on GSM8K due to the high complexity and low decomposability of the problems in GSM8K.Schemes incorporating reflection nodes show limited advancement in math-related tasks, yet exhibit promising capabilities in contextual and textual comprehension.</p>
<p>Tested on all math-related benchmarks mentioned above, PVS exhibits a 5.5% improvement on average.Moreover, PVS shows an improvement of accuracy larger than 10% on ALGEBRA [86] and GSM-hard [68] that contain algebraic calculations with large numbers, due to the integration of PoT and EoT methods.With the plug-in of the table reader tools, Chameleon increases the accuracy of answering by 8% over CoT in the math questions represented in tabular context on the TabMWP [138] dataset.E.1.1.2Commonsense Reasoning: CoT outperforms IO prompting in terms of accuracy (75.6% vs 69.4%) on the StrategyQA [72] dataset, and performs 20% better on sports understanding tasks than experienced humans.Zero-shot-CoT does not provide performance gains on commonsense reasoning benchmarks over IO prompting.The evaluation of SelfAsk focuses on multi-hop questions that require composing multiple facts to solve and improves over CoT by smaller margins on 2WikiMultiHopQA [89] and MuSiQue [188], but by 11% on Bamboogle [154] without heavy token usage.Tested on bAbI QA [58] and ProofWriter OWA [183] datasets, the SI framework enabled, on average, a 7B LLM (58.75% accuracy) to outperform a 280B LLM (44.03%) within the scope of CoT frameworks.Taking CoT as the baseline, schemes with decomposition show varying levels of performance on commonsense reasoning tasks.The problems in SCAN [116] require the conversion of single textual commands into action sequences, which suits the schemes that incorporate decomposition nodes.Here, Leastto-Most achieves 99.7% accuracy compared to just 16.2% for CoT.While PS+ shows limited improvement on StrategyQA due to the simple guidance from zero-shot prompting, Decomposed Prompting attains a 20% improvement on the CommaQA-E [105] dataset and a 30% improvement on 2WikiMultiHopQA and Musique, which stem from the designed retrieval function.With the retrieval and calculation tools, ChatCoT obtains a 20% gain on the HotPotQA [215] dataset compared with CoT.</p>
<p>Refinement improves incorrect thoughts, leading to higher reasoning abilities of LLMs.Reflexion's success rate on HotPotQA progressively improves with the accumulation of refinement iterations and surpasses 70% after six attempts, while CoT maintains a steady success rate of around 30%.LogiCoT also exhibits enhancement in reasoning tasks related with contextual understanding and causality inference, achieving an improvement of around 10% on the SocialQA [176] and CauseEffect [176] datasets.E.1.1.3Symbolic Reasoning: In the task of concatenating the last letters of words, CoT achieves almost 100% accuracy on cases that were part of the in-context exemplars and around 70% accuracy on out-of-domain (OOD) cases, while IO prompting shows nearly no accuracy on both cases.For coin flipping, IO prompting with a 540B LLM achieves nearly 100% on in-domain cases, but is inferior to CoT (50% vs 90%) on OOD cases.The problem-solving rate of Zero-shot-CoT is lower than CoT, but higher than IO on coin flip and last letter concatenation tasks.In the domain of spatial reasoning, CoS exhibites an increase of accuracy ranging from 2% to 40% on Brick World, NLVR-based Manipulation [179], Natural Language Navigation [79] and the spatial QA dataset SPARTUN [144], which completely exceeds the performance of CoT while using a lower number User: Question: There are a set of bricks.Brick E is on top of brick A. Now we have to get a specific brick.The bricks must be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick A? Answer: Let us think step by step: Listing 1: An example implicit single-prompt chain topology, encoded with text.It shows the CoT example of a Brick World problem from [91] with a linear sequence of four connected nodes followed by the final solution node. of tokens.The use of decomposition enables multi-prompt chain schemes to outperform CoT on OOD cases of the last letter concatenation task, where the length of the tested words exceeds those in the exemplars.The accuracy of the LLM gradually decreases from 94% to 74% as word length increases from 4 to 12.In contrast, CoT's accuracy drops from 70% to 32%.The accuracy of the zero-shot method PS+ is still 10% higher than few-shot CoT's.Moreover, in the more complex task of concatenating the k th letter, Decomposed Prompting shows higher accuracy (more than 96%) than CoT (around 70%) and Least-to-Most (around 12%).</p>
<p>E.1.1.4Reasoning in Special Domains: The performance evaluation of the schemes in special domains is only selectively and qualitatively detailed here, due to the non-uniform, artificially designed metrics utilized in the assessment.Refinement schemes perform well in the coderelated tasks.SELF-REFINE successfully increases readability of code on the CodeNet [155] dataset, and obtains higher speedup than the IO prompting methods on code optimization with the PIE [161] dataset.Meanwhile Reflexion achieves high accuracy in code generation on MBPP [8] and HumanEval [39].</p>
<p>E.1.2 Representations of Topology &amp; Schedule</p>
<p>Listing 1 shows an example chain topology from [91].</p>
<p>E.2 Tree</p>
<p>We now broadly discuss and analyze tree designs with respect to different aspects of our blueprint.</p>
<p>E.2.1 Topology &amp; Its Construction</p>
<p>E.2.1.1Multi-Prompt Schemes: The vast majority of schemes are multi-prompt [50], [59], [93], [107], [134], [146], [150], [173], [194], [210], [217].In all these schemes, the topology is to a certain degree dynamically constructed by the LLM (within the set boundaries of the user and the defined approach).The role of the LLMs can be summarised as (1) generating new child nodes, (2) evaluating given nodes and (3) deciding when we reached a final solution and reporting the results.Then, the user to a large degree also controls the construction process.In ToT by Long [134], the user provides a step limit as well as a checker module (rule-based or as a DNN) that decides if a reasoning step is valid or if backtracking to a previous node is necessary.ToT by Yao et al. [217] allows the user to choose the number of samples to generate at each node from which the most promising b candidates are kept for BFS, here b defines the branching factor of the tree.When using DFS, the user provides a value threshold, so when a node evaluates to a lower score than the threshold, one backtracks to the parent and continues from there.For both presented exploration schemes (BFS and DFS) the user provides an upper bound on the total number of nodes.Thought Decomposition [210] explores the tree using stochastic beam search.Hereby, the tree is constructed level by level and, in each level, k candidates are kept (the beam size) and n new nodes are generated for each candidate.The randomness in the stochastic beam search is controlled by a user-defined parameter, similarly, the user sets the temperature for generating new samples from the LLM.The total number of steps in the search, i.e., the tree depth, is capped at 16.In CoT-SC [194], the user simply provides the number of CoT samples that should be generated, resulting in a depth-1 tree.Creswell and Shanahan [50] use beam search to explore the tree, letting the user choose the number of candidate nodes to generate on each level, the beam size as well as the maximum depth of the tree.In Dynamic Least-to-Most Prompting [59], the user has no direct influence on the structure of the tree topology.The topology is entirely based on the results of the LLMs decomposition of the input.TouT [146] has the same user parameters as ToT by Yao et al. [217], but additionally uses an uncertainty threshold for DFS to backtrack from nodes where the uncertainty gets too large.In TomT [93], the user can influence the tree topology by limiting the total number of nodes, setting a branching factor and selecting a block size k which indirectly limits the depth of the tree by generating k consecutive reasoning steps for each node instead of using one node per reasoning step as in the ordinary ToT [217].ToC [107] limits the tree size by an upper node limit as well as setting a maximum depth.The user can choose a branching factor k which is used to dynamically select k-shot in-context examples for the generation prompt such that the appropriate number of child nodes are generated.Finally, in SoT [150] the depth-1 tree topology is only influenced by the response of the LLM (i.e., how the LLM determines the number of child nodes).The user can only indirectly influence this topology by possibly altering the node generation prompt itself.E.2.1.2Single-Prompt Schemes: Only AoT [168] exclusively uses a single-prompt tree topology.Here, incontext examples are designed such that the reasoning follows a tree structure, i.e., the examples show algorithmic reasoning steps such as trying out different solutions with DFS on the solution space, to make the LLM search for the solution in a similar tree-structured fashion.Thus, while in-context examples are provided by the user as guidance, the topology for solving the input question is constructed dynamically on-the-fly solely by the LLM.E.2.1.3Beyond Single-and Multi-Prompt: In all the above schemes, the root node holds the initial question and remaining nodes represent intermediate or final solutions.Contrarily, Tree Prompting [173] builds its topology during an initial learning phase, by learning the structure of a binary decision tree through training samples, with the goal of text classification.During inference, the tree structure is fixed and independent of the specific input.The user can in-directly influence the tree topology by manually providing the prompt-candidates used for learning the binary decision tree at the training stage.</p>
<p>E.2.2 Performance</p>
<p>We perform a qualitative performance evaluation of different tree schemes.We consider what problems or datasets the approach is evaluated on, how the quality of answers scales with respect to the number of prompts (or tokens), and what the trade-offs in quality are with respect to different tree topologies or reasoning schedules.E.2.2.1 Considered Problems &amp; Datasets: ToT by Long [134], ToT by Yao et al. [217], AoT [168], and TouT [146] evaluate their performance on problems that can be simply decomposed in a tree-structured way, such as 5x5 Sudoku puzzles in ToT by Long, and Game of 24 or 5x5 Crossword puzzle for the other three approaches.Thought Decomposition [210] and CoT-SC [194] perform their evaluation on arithmetic reasoning, e.g, GSM8K [48], symbolic reasoning, e.g., object counting or last letter concatenation, as well as on commonsense reasoning tasks such as Com-monsenseQA [184] and StrategyQA [72].The scheme of Creswell and Shanahan [50] assumes access to an existing context to reason on and evaluate their approach on ProofWriter [183] and EntailmentBankQA [52].Dynamic Least-to-Most Prompting [59] is evaluated on semantic parsing datasets (CFQ [104] and COGS [110]), TomT [93] create a new synthetic visual question answering dataset for evaluation by combining two existing datasets, ToC [107] is evaluated on a long-form QA dataset (ASQA [177]), Tree Prompting [173] uses 13 text classification datasets and SoT [150] is evaluated on diverse questions from [46], [212].E.2.2.2 Accuracy vs. #prompts, topology variant, &amp; reasoning schedule: Long [134] does not discuss performance evaluation and the scheme is executed until a solution is found or a maximum of 100 prompts are issued.Yao et al. [217] evaluate their approach on Game of 24 using BFS on a tree of depth 3 with a branch factor of 5 and scoring each node 3 times, on Creative Writing using a tree of depth 2, branch factor of 5 and selecting the best node in each level by scoring 5 times, and finally on solving 5 × 5 Crosswords with a ToT of depth 5 to 10 using DFS, a branch factor of 5 and at most 100 DFS steps.Thought Decomposition [210] performs beam search on a ToT with branching factor 16 and beam size 5, essentially generating 5•16 = 80 samples at each tree level, while the depth is limited to 16.The authors find out that increasing the branching factor leads to more diversity and thus improves performance when using majority voting on the last level.This increase comes at the cost of using more tokens.In CoT-SC [194], the authors evaluate the number of reasoning chains to sample, i.e., using a tree of depth one with a branching factor of 1 up to 40 while reporting the average over 10 runs.Sampling more outputs improves accuracy, but also increases computational cost; most gains are realized after sampling 5 to 10 CoT-paths in practice.The approach of Creswell an Shanahan [50] assumes access to a context, i.e., a set of statements, to reason on.The individual LMs (selection, inference, two different halter modules, and correctness prediction of a step) need to be fine-tuned separately in advance, which makes this approach very specific and incurs a setup cost.Dynamic Least-to-Most Prompting [59] comes with no explicit discussion about the number of tokens used, but the number of prompts varies with the decomposition of the specific input phrase and so does the number of in-context exemplars.The authors claim a speedup of over a factor 2 compared to CoT-SC prompting as only a single result is generated and no majority voting is necessary as in CoT-SC.AoT [168] uses a single prompt for the Game of 24 (reporting 9% out of token errors) and two prompts for the 5x5 Crossword puzzle.The authors claim that ToT uses many more prompts, i.e., over 100 rsp.200, for the two problem setups.Hereby, AoT is reported to perform better than ToT for the Game of 24.The authors show that the impact of more exploration steps inside the examples leads to longer prompt generations, i.e., slower inference for the same number of games.TouT [146] sample 20 LLM responses per ToT node to compute local variances and claim improved performance to ToT for both Game of 24 and 5x5 Crossword puzzles and further increasing or decreasing the number of samples leads to degrading performance.In TomT [93], the DFS is limited to 30 steps with their ToT-One-Stop and the reference ToT implementation using a branching factor of 3. The authors report improved accuracy for ToT-One-Stop compared to ToT while using approximately half of the number of steps.ToC [107] claims comparable performance to Cot-SC with using less than 20 LLM calls.According to the published results, Tree Prompting [173] enables smaller models to outperform larger models and the accuracy can be improved at the cost of more LLM calls; the number of LLM calls for the experiments is limited to 40.Finally, SoT [150] reports end-to-end latency speedups across different task categories and LLMs.The authors compare the two step decoding process of SoT, i.e., creating short bullet point style answers which are expanded in the second step, against "normal decoding", where a continuous answer is generated in one go.There are a varying number of bullet points generated with the average being 9 bullet points.The authors assess the answer quality with the help of two LLM judges (FastChat and LLMZoo) using different metrics.SoT reports a high token overhead of roughly 60x to 90x, which can be optimized down to roughly 30x.</p>
<p>E.3 Graph</p>
<p>We now broadly discuss and analyze graph designs with respect to different aspects of our blueprint.</p>
<p>E.3.1 Topology &amp; Its Construction</p>
<p>Most graph schemes combine to a certain degree single-and multi-prompt aspects.GoT by Besta et al. [10] requires a problem specific graph of operations for solving a problem in a multi-prompting fashion.One has complete freedom over the individual prompts to the LLM and can incorporate arbitrary single-prompting schemes.GoT by Lei et al. [120] dynamically constructs the graph, at the time of this writing no code or prompt templates have been published, so it is not clear to what extend single-prompting is used.GoT by Yao et al. [219] uses a NLP approach to construct a graph of thought and uses an LM as part of the AI-pipeline to learn how to use these graphs of thought for predicting an answer.ControlLLM [133] is similar to GoT [10] in the sense that all nodes in the Tool Graph correspond to nodes in the GoT with access to tools.Cumulative Reasoning [229] is a conceptual extension of ToT with access to all previously derived results, i.e., nodes, resulting in a DAG.XoT [57] uses a problem-specific pre-trained RL-model to create a graph of thought and uses the LLM to review and summarize the results.</p>
<p>In graph schemes, the user has significant control over the topology and its creation.In BSM [164], the topology is a double tree with one intermediate level.The user can set either the number of branches, or its upper bound by specifying the number of sub-problems to generate inside a single prompt.In Thought Propagation [222], the user specifies the branching factor in a single prompt to generate analogous problems, and can predefine the number of levels of the double tree to grow the topology.The user in Socratic Questioning [156] defines the branching factor of the topology by specifying its value, lower limit, or upper limit within a single prompt.Furthermore, the user can limit the growth of the topology by defining the maximum tree depth.In Graph of Thoughts (GoT) [10], the user defines the complete topology of reasoning across multiple prompts for a given use case, including, but not limited to, the branching factor, the maximum depth of tree, and the number of child nodes to keep.The topology in Graph of Thought (GoT) [120] is influenced by a user-defined depth limit.In Cumulative Reasoning [229], the user can influence the total number of nodes as the proposer (LLM) iteratively generates a new node until a user-specified limit is reached.In ControlLLM [133], the user does not define the number of nodes during the task decomposition stage for solving a given task.However, a user may have implicit influence on the number of nodes via prompting, e.g., by adding the phrase "Parse out as few tasks as possible".The user in Everything of Thoughts (XoT) [57] can indirectly influence the topology by defining the number of times the LLM is prompted to review and refine the solution graph.In ResPrompt [100], the user simply provides incontext examples which directly dictate the basis of the topology.Differing from previous schemes, users in twostage frameworks such as Graph-of-Thought (GoT) [219] and Hypergraph-of-Thought (HoT) [216] do not influence the topology as the (hyper)graph-of-thought is constructed by predefined heuristics.</p>
<p>Similar to prompting with trees, the LLM may also influence the topology of prompting with graphs within user-defined boundaries.The role of the LLM can be summarized as (1) generating new child nodes, (2) evaluating given nodes, (3) deleting or backtracking from a new node (e.g., based on evaluation results), and (4) determining when the reasoning process should be concluded.</p>
<p>E.3.2 Performance</p>
<p>E.3.2.1 Considered Problems &amp; Datasets:</p>
<p>BSM [164] evaluates its performance on answering questions from multiple areas by using MT-Bench [237].TP [222] performs shortest-path reasoning, creative writing and LLM-Agent Planning on ALFWorld [172].Socratic Questioning [156] evaluates their method on both, textonly datasets (e.g.MMLU [87] and MATH [88]), logical</p>
<p>Fig. 1 :
1
Fig. 1: Evolution of reasoning topologies used in prompting schemes.</p>
<p>Fig. 2 :
2
Fig. 2: Overview of a general prompting pipeline.</p>
<p>2 A
2
. Solution thoughts and topologies are marked with the blue color while in-context examples are marked with the green color.Topologies of in-context examples do not span beyond a single prompt.Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has 3 tennis balls.How many tennis balls does he have now?Green color indicates a topology of in-context examples, i.e., a topology that is not a part of the topology related to the posed input task.Blue color indicates a solu�on topology, i.e., topology that is related to the input task.Different shades indicate different seman�c roles of nodes/edges In-context examples (and poten�ally other parts of the context) may include instances of topologies.For example, here, there are 1-level trees and forests within other in-context examples.Seman�c role of the dark shade: provide the next part of the solu�on Seman�c role of the light shade: evaluate Q: The cafeteria has 23 apples.If they used 20 to make lunch and bought 6 more, how many apples do they have?thought refers to a single sentence in a paragraph that contains the LLM reasoning towards solving the task A thought refers to the next part of the</p>
<p>Input 2 ( 13 +Fig. 3 :
2133
Fig. 3: Overview and examples of how reasoning topologies correspond to the user-AI prompting interactions.</p>
<p>Implicit"</p>
<p>The first preliminary solu�on should be enhanced three �mes.Each of these three enhanced solu�ons should be further augmented in two a�empts"</p>
<p>Fig. 4 :
4
Fig. 4: Taxonomy and blueprint for structure-enhanced reasoning.</p>
<p>Q: 9 6 5 3 A
3</p>
<p>He charges $15 to mow a lawn and $7 to shovel.After buying the shoes, he has $15 in change.If he mows 4 lawns, how many driveways did he shovel?Q: [more in-context examples...] Let S be the amount Alexis paid for the shoes.She used all but $16 of her budget, so S + 143 = 200 -16 = 184.Thus, Alexis paid S = 184 -143 = 41 for the shoes.So the answer is 41.She spent S + 30 + 46 + 38 + 11 + 18 = S + 143.</p>
<p>Fig. 6 :
6
Fig. 6: MATH REASONING.Another implicit single-prompt chain topology example, encoded with text.It shows how few-shot CoT solves a math question from GSM8K[48].This results in an implicit chain where individual reasoning steps can be regarded as nodes.</p>
<p>Fig. 7 :
7
Fig. 7: CREATIVE WRITING.An example implicit single-prompt chain topology, encoded with text.It shows the Creative Writing example from ToT [217] using zero-shot CoT prompting.This results in an implicit chain with a first node for the question and one node for each generated paragraph.</p>
<p>Fig. 8 :
8
Fig. 8: LOGICAL REASONING.An example explicit multi-prompt chain topology, encoded with text.It demonstrates the manually derived chain topology of Selection-Inference [51]."Selection" and "Inference" indicate independent consecutive prompts/thoughts.</p>
<p>Fig. 11 :
11
Fig.11: CREATIVE WRITING.An example explicit multi-prompt tree topology, encoded with text, from the Tree of Thoughts (ToT) scheme[217] for creative writing.Given the task of writing a coherent passage of four paragraphs ending in given sentences, first multiple plans (nodes) are generated and then ranked.In a next step, the best plan is used to generate multiple possible passages as outputs.Finally, the best ranked passage is the output of the ToT reasoning.</p>
<p>[ 3 .
3
in-context examples] Q: Tobias is buying a new pair of shoes that costs $95.He has been saving up his money each month […]?LLM Answer: Implicit Tree Structure LLM Answer Then we know he mowed 4 lawns and he charges $15 per lawn, so from mowing lawns, he earned 4 * $15 = $60.</p>
<p>2 . 4 .Fig. 15 :
2415
Fig. 15: MATH REASONING.An example implicit single-prompt graph topology, encoded with text.It shows an in-context example of a math question from ResPrompt [100], representing a linear sequence of six connected nodes with two implicit edges of the graph topology, marked with two different colors (red and blue), together with their corresponding nodes.</p>
<p>[in-context examples] Premises: [...] Question: [...]He had $110 in total before buying the shoes.Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please determine whether the "Proposition" is directly deduced from the "Premises" with certainty other than introducing unsourced information by common sense reasoning, reply with True or False.[in-context examples] Premises: [...] Proposition: He had $95 in total before buying the shoes.Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let us think step by step.Please use First-Order Logic (FOL) to determine whether the deduction of two given "Premises" to a "Proposition" is valid or not, reply with True or False.[in-context examples] Premises: [...] Proposition: He had $95 in total before buying the shoes.</p>
<p>Fig. 16 :
16
Fig. 16: MATH REASONING.An example implicit single-prompt graph topology, encoded with text, for the same question as in Figure 15, but using Cumulative Reasoning [229].It illustrates three intermediate proposition nodes with direct connections from the first two nodes to the third one.The reasoning part indicates the final solution node, with direct connections from the third proposition and input nodes.</p>
<p>2 Scope Representa�on Deriva�on Output Output 1 2 3 4 7 Reasoning Schedule 5 Schedule Representa�on 6 Class What is the connec�on structure of reasoning steps? Topology Representa�on 3 Can the topology extend beyond a single prompt? How is the topology structure represented? Topology Schedule Extensions Harnessed Parts of the Genera�ve AI Pipeline Mul�-prompt The structure is contained within a single prompt/reply Prompt or reply Prompt or reply Prompt or reply One prompt/reply can contain mul�ple reasoning nodes One prompt/reply can contain a single reasoning node 0 1 2 3 4 5 6 7 8 9
<node 0> connects to <node 1>, <node 2>, <node 3> <node 1> connects to <node 4>, <node 5><node 2> connects to <node 6>, <node 7><node 3> connects to <node 8>, <node 9></p>
<p>Topology Reasoning AI Scheme single-prompt multi-prompt Schedule Pipeline Remarks Class Rp. Dv. Class Rp. Dv. Scheme Rp. Dv. P F R T Modalities
Chain-of-Thought (CoT) [199]chainI (text)SA ------textZero-shot-CoT [113]chainI (text)SA ------textSelfAsk [154]chainI (text)SA ------ȞtextPlan-and-Solve Prompting [192]chainI (text)SA ------textProgram of Thoughts (PoT) [41]chainI (text,code) SA ------Ȝ text,code,tableSelection-Inference (SI) [51]---chainE MlinearIMȞtextChain-of-Symbol (CoS) [91]chainI (text)SA chainE SA linearIMtextLeast-to-Most Prompting [238]---chainE SA linearIMtextDecomposed Prompting [106]---chainE SA linearIMȞ Ȟ textLogiCoT [236]chainI (text)SA treeE SA linearIMtextSELF-REFINE [141]---chainE SA linearIMtextReflexion [171]---chainE SA linearIMtextReasoning Graph Verifier (RGV) [35] chainI (text)SA graphE SA linearIMtextPlan, Verify and Switch (PVS) [132]chainI (text,code) SA chainE SA linearIMȜ text,codeChameleon [137]---chainE SA linearIMȜ Ȝ text,codeChatCoT [45]chainI (text)SA chainE SA linearIMȜ Ȝ textTree-of-Thought (ToT) [134]treeI (text)M treeE SA arbitraryE MtextTree of Thoughts (ToT) [217]treeI (text)M treeE SA arbitraryE MtextThought Decomposition [210]treeI (text)M treeE SA beam  †E SAȞ text,code† stochasticSelf-Consistency with CoT [194]chainI (text)M tree (Ȟ)  †E SA ---text† depth oneCreswell and Shanahan [50]treeI (text)M treeE SA beamE AȜtextDynamic Least-to-Most Prompting [59] treeI (text)M treeE Abottom up E AȞtext,codeAlgorithm of Thoughts (AoT) [168]treeI (text)M ---DFS, (BFS) IMtextTree of Uncertain Thought (TouT) [146] treeI (text)M treeE SA BFS, DFS E MtextTree-of-Mixed-Thought [93]treeI (text)M treeE SA DFSE AȜ scene graphsTree of Clarifications (ToC) [107]tree (Ȟ)  †I (text)M treeE SA BFSE AȜtext† depth oneTree Prompting [173]---treeE Atop-down E AtextSkeleton-of-Thought (SoT) [150]tree (Ȟ)  †I (text)M tree (Ȟ)  †E AparallelE Atext† depth oneBranch-Solve-Merge (BSM) [164]tree (depth one) I (text)M graph (Ȟ)  † E SA BFSE Mtext† double tree (Ȟ)Thought Propagation (TP) [222]graph (Ȝ)arbitraryM graph (Ȟ)  † E SA BFSE Mtext† double tree (Ȝ)Socratic Questioning [156]tree (depth one) I (text)M graph (Ȟ)  † E SA DFSE Mmulti† double treeGraph of Thoughts (GoT) [10]graph (Ȝ)arbitraryM graphE MarbitraryE MtextGraph of Thought (GoT)</p>
<p>TABLE 1 : Comparison of LLM reasoning schemes with respect to the provided taxonomy (Section 4.5 and Figure 4). "
1Topology": Details of the harnessed structure.</p>
<p>Multi-prompt": Does a given scheme support single-/ multi-prompt topology? If yes, what is the supported Class, Representation, and Derivation?</p>
<p>Logical Reasoning User Prompt If</p>
<p>something is quiet and cold then it is smart.All young, cold things are kind.If something is kind then it is nice.All smart, blue things are kind.If something is cold then it is quiet.If Anne is smart and Anne is kind then Anne is cold.
If something is nice and blue thenit is young.If something is nice and youngthen it is cold.Anne is young.Gary is nice.Charlie is quiet.Anne is blue.[...]Dave is smart.Anne is quiet.Dave is blue.Gary is cold.Anne is nice.</p>
<p>Does it imply that the statement "Dave is not quiet" is true?</p>
<p>Trying a promising first operation: 1. 8 + 2: (14, 10, 8) -14 + 10: (24, 8) 32, 16, 192, 3
-14 -10: (8, 4) 12, 4, 32, 2-14 * 10: (140, 8) 148, 132, 1120, fractional-14 / 10: fractional-14 + 8: (22, 10) 32, 12, 220, fractional-14 -8: (10, 6) 16, 4, 60, fractional-14 * 8: (112, 10) 122, 102, 1120, fractional-14 / 8: fractional-10 + 8: (18, 14) 32, 4, 252, fractional-10 -8: (14, 2) 16, 12, 28, 7-10 * 8: (80, 14) 94, 66, 1120, fractional-10 / 8: fractionalTrying another promising first operation:2. 8 / 2: (14, 8, 4)-14 + 8: (22, 4) 26, 18, 88, fractional-14 -8: (6, 4) 10, 2, 24 = 6 * 4 -&gt; found it!Backtracking the solution:Step 1:8 / 2 = 4Step 2:14 -8 = 6Step 3:6 * 4</p>
<p>= 24 Considering these steps: 24 = 6 * 4 = (14 -8) * 4 = (14 -8) * (8 / 2) = 24. Answer: (14 -8) * (8 / 2) = 24. LLM Answer: Implicit Tree Structure LLM Answer ... ...
Trying a promising first operation:8 + 2: (14, 10, 8)10 / 8: fractional3216192310224 = 6 * 4 -&gt; found it!Considering these steps: 24 = 6 * 4 = (14 -8) * 4 = (14 -8) * (8 / 2) = 24.</p>
<p>Writing ... ...
Plan 1Plan 21. Introduce and explain the technique of doing a handstand. 2. … 3. … 4. ...1. Introduction to an unusual self-help book, mentioning a handstand as a metaphor for embracing challenges.Plan 2 is chosen.Passage 1Passage 2Passage 2 is chosen.
UserWrite a coherent passage of 4 short paragraphs.The end sentence of each paragraph must be: 1.It isn't difficult to do a handstand if you just stand... 2. … 3. … 4. ... 2. … 3. … 4. ...</p>
<p>You are an organizer responsible for only giving the skeleton (not the full content) for answering the question.Provide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question.Instead of writing a full sentence, each skeleton point should be very short with only 3~5 words.Generally, the skeleton should have 3~10 points.Now, please provide the skeleton for the following question.
Question AnsweringSkeleton PromptUser InputPoint-Expanding PromptQ: [...] Skeleton:1. Active listening 2. Identify issues 3. Compromise Q: What are the most effective strategies for conflict 1. Active listening involves fully concentrating on ... 2. Identify issues. Look into the root causes of ... resolution in the workplace?You are responsible for continuing the writing of one and only one point in the overall answer to the following question. Q: [...] The skeleton of the answer is 1. Active listening 2. Identify issues 3. Compromise Continue and only continue the writing of point 1. Write it <strong>very shortly</strong> in 1~2 sentence and do not continue with other points!3. Compromise. Look for a middle ground ...1. Active listening 2. Identify issues 3. Compromise</p>
<p>You need to combine the given intermediate steps step by step into a complete expression. [in-context examples]</p>
<p>Suppose you are one of the greatest AI scientists, logicians and mathematicians.You are very good at basic arithmetic operations.Use numbers and basic arithmetic operations (+ -* /) to obtain 24 with input numbers.
...Validate StepReporter PromptInput: 14, 8, 8, 2Intermediate steps:8 / 2 = 4 (left 4, 8, 14)Verify Step14 -8 = 6 (left 4, 6) 6 * 4 = 24Draft:Because 6 * 4 = 24, while 6 = 14 -8. So (14 -8) * 4 = 24.Because (14 -8) * 4 = 24, while 4 = 8 / 2. So (14 -8) * (8 / 2) = 24.Output:(14 -8) * (8 / 2) = 24</p>
<p>In each step, you are only allowed to randomly choose TWO of the input numbers to obtain a new number using arbitrarily one basic arithmetic operation (AVOID duplication of forbidden steps). Your calculation process must be correct. [in-context examples]</p>
<p>Suppose you are one of the greatest AI scientists, logicians and mathematicians.You are very good at basic arithmetic operations.Use numbers and basic arithmetic operations (+ -<em> /) to obtain 24 with input numbers.Suppose you are one of the greatest AI scientists, logicians and mathematicians.You are very good at basic arithmetic operations.Use numbers and basic arithmetic operations (+ -</em> /) to obtain 24 with input numbers.
14, 8, 8, 212, 8, 8Input: 14, 8, 8, 2Forbidden Steps : [...]Next Step:14 -2 = 12Remaining Numbers:12, 8, 8(14 -8) * (8 / 2) = 24</p>
<p>Evaluate if a given intermediate step is correct and only use two existing numbers. [in-context examples]</p>
<p>Suppose you are one of the greatest AI scientists, logicians and mathematicians.You are very good at basic arithmetic operations.Use numbers and basic arithmetic operations (+ -* /) to obtain 24 with input numbers.
Input: 14, 8, 8, 2Intermediate step: 14 -2 = 12</p>
<p>Evaluate if given numbers can reach 24 (sure/likely/impossible) [in-context examples]
4, 8, 1414, 44, 6The intermediate step is valid.Judgment:ValidInput: 12, 8, 8Draft:(12 -8) * 8 = 3212 + 8 + 8 = 288 / 8 + 12 = 13I cannot obtain 24 now, and numbers are not within a reasonable range.Output:impossible(12, 8, 8) is added to forbidden steps.
We encourage participation in this analysis. In case the reader is in possession of additional information relevant for the analyzed schemes, the authors would welcome the input.
ACKNOWLEDGEMENTSWe thank Hussein Harake, Colin McMurtrie, Mark Klein, Angelo Mangili, and the whole CSCS team granting access to the Ault and Daint machines, and for their excellent technical support.We thank Timo Schneider for help with infrastructure at SPCL.This project received funding from the European Research Council (Project PSAP, No. 101002047), and the European High-Performance Computing Joint Undertaking (JU) under grant agreement No. 955513 (MAELSTROM).This project was supported by the ETH Future Computing Laboratory (EFCL), financed by a donation from Huawei Technologies.This project received funding from the European Union's HE research and innovation programme under the grant agreement No. 101070141 (Project GLACIATION).reasoning questions (LogiQA[129]), as well as multimodal visual QA datasets such as VQA-V2[77]and two others.GoT by Besta et al.[10]performs integer sorting with duplicates, set intersections, keyword counting and document merging.GoT byLei et al. [120]measures its performance on the Game of 24, solving higher-degree polynomial equations as well as deriving formulas for recursive sequences.GoT by Yao et al.[219]evaluates on the arithmetic reasoning benchmark GSM8K[48]and on ScienceQA[136], which contains science questions with images.ControlLLM[133]provides its own benchmark containing tasks for image editing and perception, visual QA and the usage of over 20 different common tools.Cumulative Reasoning[229]evaluates its approach on FOLIO[84], AutoTNLI[115], Game of 24, as well as mathematical reasoning on the MATH[88]dataset.XoT[57]evaluates on Game of 24, 8-Puzzle and on a 2x2 pocket cube.ResPrompt[100]uses multiple math reasoning benchmarks for evaluation, i.e., GSM8K[48], AQUA-RAT[126], MathQA[2]and SVAMP[153], as well as the sequential reasoning benchmark SCONE-Alchemy[135]and commonsense reasoning questions from StrategyQA[72].HoT[216]evaluates their approach on ScienceQA[136].In summary, there is no clear set of benchmarks that is used consistently with all approaches, but most include math reasoning problems or some common reasoning tasks such as Game of 24.E.3.2.2 Accuracy vs. #prompts, topology variant, &amp; reasoning schedule: BSM[164]uses a branching factor of 5 for its experiments and outperforms zero-shot prompting and CoT-SC (sampling 5 times).BSM makes LLaMA-2-70B-chat competitive with zero-shot GPT-4 for the turn-1 questions.BSM has similar computational requirements as CoT-SC, but yields higher scores.Additionally, BSM benefits smaller models (LLaMA-2-7B), where CoT-SC is ineffective.The benefits of increasing the branching factor seems to be saturated at 4, but depends on the specific problem.TP[222]for shortest-path reasoning makes significant performance gains over all the baselines (IO, CoT, ToT) for all evaluated models, i.e., PaLM-2, GPT-3.5 and GPT-4.The authors report that there is only a marginal difference between 1-shot and 5-shot prompting without further exploration.The performance can directly be influenced by the number of layers used for TP.Hereby, 1-layer TP has competitive performance to ToT with a similar amount of tokens used.Socratic Questioning[156]is compared against IO, CoT, CoT-SC and ToT prompting.The authors report 3-4% absolute gain over other methods.The reported ToT accuracy is relatively low compared to the other baselines.GoT by Besta et al.[10]significantly improves upon IO, CoT, and ToT on all four tasks using GPT-3.5.The accuracy enhancement of GoT over the best-performing baseline (ToT) is achieved with lower or comparable number of tokens.The performance advantages of GoT over the baselines increase with the problem size.
PIM-Enabled Instructions: A Low-Overhead, Locality-Aware Processing-In-Memory Architecture. J Ahn, S Yoo, O Mutlu, K Choi, Proceedings of the 42nd Annual International Symposium on Computer Architecture, ISCA '15. the 42nd Annual International Symposium on Computer Architecture, ISCA '15Portland, OR, USAAssociation for Computing MachineryJune 2015</p>
<p>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. A Amini, S Gabriel, S Lin, R Koncel-Kedziorski, Y Choi, H Hajishirzi, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American ChapterMinneapolis, MN, USAAssociation for Computational LinguisticsJune 20191NAACL '19</p>
<p>Leveraging Linguistic Structure for Open Domain Information Extraction. G Angeli, M J Johnson Premkumar, C D Manning, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. C Zong, M Strube, the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational LinguisticsJuly 20151ACL-IJCNLP '15</p>
<p>Survey of Graph Database Models. R Angles, C Gutierrez, 1:1-1:39ACM Comput. Surv. 401Feb. 2008</p>
<p>An Introduction to Graph Data Management. R Angles, C Gutierrez, Graph Data Management, Fundamental Issues and Recent Developments, Data-Centric Systems and Applications (DCSA). G H L Fletcher, J Hidders, J L Larriba-Pey, Springer2018</p>
<p>GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval. D Arora, A Kini, S R Chowdhury, N Natarajan, G Sinha, A Sharma, arXiv:2310.20158Oct. 2023</p>
<p>Self-RAG: Learning to Retrieve, Generate, and Critique Through Self-Reflection. A Asai, Z Wu, Y Wang, A Sil, H Hajishirzi, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Program Synthesis with Large Language Models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, C Sutton, arXiv:2108.07732Aug. 2021</p>
<p>K Bertels, A Sarkar, A Krol, R Budhrani, J Samadi, E Geoffroy, J Matos, R Abreu, G Gielen, I Ashraf, arXiv:2102.02035Quantum Accelerator Stack: A Research Roadmap. May 2021</p>
<p>Graph of Thoughts: Solving Elaborate Problems with Large Language Models. M Besta, N Blach, A Kubicek, R Gerstenberger, L Gianinazzi, J Gajda, T Lehmann, M Podstawski, H Niewiadomski, P Nyczyk, T Hoefler, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality. M Besta, A Carigiet, K Janda, Z Vonarburg-Shmaria, L Gianinazzi, T Hoefler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20. the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '20Atlanta, GA, USAIEEE PressNov. 202099</p>
<p>HOT: Higher-Order Dynamic Graph Representation Learning with Efficient Transformers. M Besta, A C Catarino, L Gianinazzi, N Blach, P Nyczyk, H Niewiadomski, T Hoefler, Proceedings of the Second Learning on Graphs Conference (LOG '23). S Villar, B Chamberlain, the Second Learning on Graphs Conference (LOG '23)PMLRNov. 2023231of Proceedings of Machine Learning Research</p>
<p>Substream-Centric Maximum Matchings on FPGA. M Besta, M Fischer, T Ben-Nun, D Stanojevic, J De Fine Licht, T Hoefler, ACM Trans. Reconfigurable Technol. Syst. 132Apr. 2020</p>
<p>M Besta, M Fischer, V Kalavri, M Kapralov, T Hoefler, Practice of Streaming Processing of Dynamic Graphs: Concepts, Models, and Systems. IEEE Transactions on Parallel and Distributed Systems. June 202334</p>
<p>M Besta, R Gerstenberger, N Blach, M Fischer, T Hoefler, GDI: A Graph Database Interface Standard. Nov. 2023. Jan. 15, 2024</p>
<p>The Graph Database Interface: Scaling Online Transactional and Analytical Graph Workloads to Hundreds of Thousands of Cores. M Besta, R Gerstenberger, M Fischer, M Podstawski, N Blach, B Egeli, G Mitenkov, W Chlapek, M Michalewicz, H Niewiadomski, J , T Hoefler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '23. the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '23Denver, CO, USAAssociation for Computing MachineryNov. 202322</p>
<p>Demystifying Graph Databases: Analysis and Taxonomy of Data Organization, System Designs, and Graph Queries. M Besta, R Gerstenberger, E Peter, M Fischer, M Podstawski, C Barthels, G Alonso, T Hoefler, 31:1- 31:40ACM Comput. Surv. 562Sept. 2023</p>
<p>Motif Prediction with Graph Neural Networks. M Besta, R Grob, C Miglioli, N Bernold, G Kwaśniewski, G Gjini, R Kanakagiri, S Ashkboos, L Gianinazzi, N Dryden, T Hoefler, Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '22Washington DC, USAAssociation for Computing MachineryAug. 2022</p>
<p>M Besta, S M Hassan, S Yalamanchili, R Ausavarungnirun, O Mutlu, T Hoefler, Slim NoC: A Low-Diameter On-Chip Network Topology for High Energy Efficiency and Scalability. SIGPLAN Not. Mar. 201853</p>
<p>Slim Fly: A Cost Effective Low-Diameter Network Topology. M Besta, T Hoefler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '14. the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '14New Orleans, LA, USAIEEE PressNov. 2014</p>
<p>Active Access: A Mechanism for High-Performance Distributed Data-Centric Computations. M Besta, T Hoefler, Proceedings of the 29th ACM on International Conference on Supercomputing, ICS '15. the 29th ACM on International Conference on Supercomputing, ICS '15Newport Beach, CA, USAAssociation for Computing MachineryJune 2015</p>
<p>Parallel and Distributed Graph Neural Networks: An In-Depth Concurrency Analysis. M Besta, T Hoefler, IEEE Transactions on Pattern Analysis and Machine Intelligence. 465May 2024</p>
<p>Neural Graph Databases. M Besta, P Iff, F Scheidl, K Osawa, N Dryden, M Podstawski, T Chen, T Hoefler, Proceedings of the First Learning on Graphs Conference (LOG '22). B Rieck, R Pascanu, the First Learning on Graphs Conference (LOG '22)PMLRDec. 2022198of Proceedings of Machine Learning Research</p>
<p>SISA: Set-Centric Instruction Set Architecture for Graph Mining on Processing-in-Memory Systems. M Besta, R Kanakagiri, G Kwaśniewski, R Ausavarungnirun, J Beránek, K Kanellopoulos, K Janda, Z Vonarburg-Shmaria, L Gianinazzi, I Stefan, J G Luna, J Golinowski, M Copik, L Kapp-Schwoerer, S Di Girolamo, N Blach, M Konieczny, O Mutlu, T Hoefler, Proceedings of the 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21. the 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21Association for Computing MachineryOct. 2021</p>
<p>ProbGraph: High-Performance and High-Accuracy Graph Mining with Probabilistic Set Representations. M Besta, C Miglioli, P Labini, J Tětek, P Iff, R Kanakagiri, S Ashkboos, K Janda, M Podstawski, G Kwaśniewski, N Gleinig, F Vella, O Mutlu, T Hoefler, Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22. the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22Dallas, TX, USAIEEE PressNov. 2022</p>
<p>To Push or To Pull: On Reducing Communication and Synchronization in Graph Computations. M Besta, M Podstawski, L Groner, E Solomonik, T Hoefler, Proceedings of the 26th International Symposium on High-Performance Parallel and Distributed Computing, HPDC '17. the 26th International Symposium on High-Performance Parallel and Distributed Computing, HPDC '17Washington DC, USAAssociation for Computing MachineryJune 2017</p>
<p>High-Performance and Programmable Attentional Graph Neural Networks with Global Tensor Formulations. M Besta, P Renc, R Gerstenberger, P Labini, A Ziogas, T Chen, L Gianinazzi, F Scheidl, K Szenes, A Carigiet, P Iff, G Kwaśniewski, R Kanakagiri, C Ge, S Jaeger, J Was, F Vella, T Hoefler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '23. the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '23Denver, CO, USAAssociation for Computing MachineryNov. 2023</p>
<p>M Besta, D Stanojevic, J De Fine Licht, T Ben-Nun, T Hoefler, arXiv:1903.06697Graph Processing on FPGAs: Taxonomy, Survey, Challenges. Apr. 2019</p>
<p>Log(Graph): A Near-Optimal High-Performance Graph Representation. M Besta, D Stanojevic, T Zivic, J Singh, M Hoerold, T Hoefler, Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18. the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT '18Limassol, Cyprus, SeptAssociation for Computing Machinery20187</p>
<p>Graph-MineSuite: Enabling High-Performance and Programmable Graph Mining Algorithms with Set Algebra. M Besta, Z Vonarburg-Shmaria, Y Schaffner, L Schwarz, G Kwaśniewski, L Gianinazzi, J Beranek, K Janda, T Holenstein, S Leisinger, P Tatkowski, E Ozdemir, A Balla, M Copik, P Lindenberger, M Konieczny, O Mutlu, T Hoefler, Proc. VLDB Endow. VLDB EndowJuly 202114</p>
<p>Data Models. A Bonifati, G Fletcher, H Voigt, N Yakovets, Querying Graphs, Synthesis Lectures on Data Management (SDLM). Springer International Publishing2018</p>
<p>Improving Language Model Predictions via Prompts Enriched with Knowledge Graphs. R Brate, M.-H Dang, F Hoppe, Y He, A Mero Ño-Pe Ñuela, V Sadashivaiah, Proceedings of the Workshop on Deep Learning for Knowledge Graphs, DL4KG '22. the Workshop on Deep Learning for Knowledge Graphs, DL4KG '22Hangzhou, ChinaOct. 2022</p>
<p>Geometric Deep Learning: Going Beyond Euclidean Data. M M Bronstein, J Bruna, Y Lecun, A Szlam, P Vandergheynst, IEEE Signal Processing Magazine. 344July 2017</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proceedings of the Thirty-fourth Annual Conference on Neural Information Processing Systems (NeurIPS '20). H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, the Thirty-fourth Annual Conference on Neural Information Processing Systems (NeurIPS '20)Curran AssociatesDec. 202033</p>
<p>GraphReason: Enhancing Reasoning Capabilities of Large Language Models Through a Graph-Based Verification Approach. L Cao, Proceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations, NLRSE '24. B Dalvi Mishra, G Durrett, P Jansen, B Lipkin, D Neves Ribeiro, L Wong, X Ye, W Zhao, the 2nd Workshop on Natural Language Reasoning and Structured Explanations, NLRSE '24Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>GraphLLM: Boosting Graph Reasoning Ability of Large Language Model. Z Chai, T Zhang, L Wu, K Han, X Hu, X Huang, Y Yang, arXiv:2310.05845Oct. 2023</p>
<p>I Chami, S Abu-El-Haija, B Perozzi, C Ré, K Murphy, Machine Learning on Graphs: A Model and Comprehensive Taxonomy. 202223</p>
<p>Unleashing the Potential of Prompt Engineering in Large Language Models: A Comprehensive Review. B Chen, Z Zhang, N Langrené, S Zhu, arXiv:2310.14735Sept. 2024</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, arXiv:2107.03374July 2021Evaluating Large Language Models Trained on Code</p>
<p>MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. W Chen, H Hu, X Chen, P Verga, W Cohen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. W Chen, X Ma, X Wang, W W Cohen, Transactions on Machine Learning Research. Nov. 2023</p>
<p>FinQA: A Dataset of Numerical Reasoning over Financial Data. Z Chen, W Chen, C Smiley, S Shah, I Borova, D Langdon, R Moussa, M Beane, T.-H Huang, B Routledge, W Y Wang, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP '21. M.-F Moens, X Huang, L Specia, S W -T, Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP '21Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNov. 2021</p>
<p>ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. Z Chen, S Li, C Smiley, Z Ma, S Shah, W Y Wang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. Z Chen, H Mao, H Li, W Jin, H Wen, X Wei, S Wang, D Yin, W Fan, H Liu, J Tang, SIGKDD Explor. Newsl. 252Mar. 2024</p>
<p>ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-Based Large Language Models. Z Chen, K Zhou, B Zhang, Z Gong, W X Zhao, J.-R Wen, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. Mar. 2023. Dec. 15, 2023</p>
<p>Navigate Through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future. Z Chu, J Chen, Q Chen, W Yu, T He, H Wang, W Peng, M Liu, B Qin, T Liu, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241Long Papers), ACL '24</p>
<p>Training Verifiers to Solve Math Word Problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, arXiv:2110.14168Nov. 2021</p>
<p>SeBS: A Serverless Benchmark Suite for Function-as-a-Service Computing. M Copik, G Kwaśniewski, M Besta, M Podstawski, T Hoefler, Proceedings of the 22nd International Middleware Conference, Middleware '21. the 22nd International Middleware Conference, Middleware '21Québec City, CanadaAssociation for Computing MachineryDec. 2021</p>
<p>Faithful Reasoning Using Large Language Models. A Creswell, M Shanahan, arXiv:2208.14271Aug. 2022</p>
<p>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning. A Creswell, M Shanahan, I Higgins, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>Explaining Answers with Entailment Trees. B Dalvi, P Jansen, O Tafjord, Z Xie, H Smith, L Pipatanangkura, P Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP '21. M.-F Moens, X Huang, L Specia, S W -T, Yih, the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP '21Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNov. 2021</p>
<p>. A Davoudian, L Chen, M Liu, 40:1-40:43A Survey on NoSQL Stores. ACM Comput. Surv. 512Apr. 2018</p>
<p>Transformations of High-Level Synthesis Codes for High-Performance Computing. J De Fine Licht, M Besta, S Meierhans, T Hoefler, IEEE Transactions on Parallel and Distributed Systems. 325May 2020</p>
<p>Building Blocks for Network-Accelerated Distributed File Systems. S Di Girolamo, D De Sensi, K Taranov, M Malesevic, M Besta, T Schneider, S Kistler, T Hoefler, Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22. the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22Dallas, TX, USAIEEE PressNov. 202210</p>
<p>Network-Accelerated Non-Contiguous Memory Transfers. S Di Girolamo, K Taranov, A Kurth, M Schaffner, T Schneider, J Beránek, M Besta, L Benini, D Roweth, T Hoefler, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '19. the International Conference for High Performance Computing, Networking, Storage and Analysis, SC '19Denver, CO, USAAssociation for Computing MachineryNov. 201956</p>
<p>Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation. R Ding, C Zhang, L Wang, Y Xu, M Ma, W Zhang, S Qin, S Rajmohan, Q Lin, D Zhang, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>J Dodge, A Gane, X Zhang, A Bordes, S Chopra, A Miller, A Szlam, J Weston, arXiv:1511.06931Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems. Apr. 2016</p>
<p>Compositional Semantic Parsing with Large Language Models. A Drozdov, N Schärli, E Aky Ürek, N Scales, X Song, X Chen, O Bousquet, D Zhou, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>Successive Prompting for Decomposing Complex Questions. D Dua, S Gupta, S Singh, M Gardner, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MN, USAAssociation for Computational LinguisticsJune 20191NAACL '19</p>
<p>Talk like a Graph: Encoding Graphs for Large Language Models. B Fatemi, J Halcrow, B Perozzi, Proceedings of the Workshop on New Frontiers in Graph Learning, GLFrontiers '23. the Workshop on New Frontiers in Graph Learning, GLFrontiers '23New Orleans, LA, USAOpenReviewDec. 2023</p>
<p>Knowledge Solver: Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs. C Feng, X Zhang, Z Fei, arXiv:2309.03118Sept. 2023</p>
<p>Fast Graph Representation Learning with PyTorch Geometric. M Fey, J E Lenssen, Proceedings of the Workshop on Representation Learning on Graphs and Manifolds, RLGM '19. the Workshop on Representation Learning on Graphs and Manifolds, RLGM '19New Orleans, LA, USAMay 2019</p>
<p>Experiments with a New Boosting Algorithm. Y Freund, R E Schapire, Proceedings of the Thirteenth International Conference on International Conference on Machine Learning, ICML '96. the Thirteenth International Conference on International Conference on Machine Learning, ICML '96Bari, ItalyMorgan KaufmannJuly 1996</p>
<p>Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding. Y Fu, P Bailis, I Stoica, H Zhang, Nov. 2023. Dec. 23, 2023</p>
<p>Towards Foundation Models for Knowledge Graph Reasoning. M Galkin, X Yuan, H Mostafa, J Tang, Z Zhu, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>PAL: Program-Aided Language Models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, Proceedings of the 40th International Conference on Machine Learning (ICML '23). A Krause, E Brunskill, K Cho, B Engelhardt, S Sabato, J Scarlett, the 40th International Conference on Machine Learning (ICML '23)Honolulu, HI, USAPMLRJuly 2023202of Proceedings of Machine Learning Research</p>
<p>Retrieval-Augmented Generation for Large Language Models: A Survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, arXiv:2312.10997Mar. 2024</p>
<p>A Dataset and Baselines for Visual Question Answering on Art. N Garcia, C Ye, Z Liu, Q Hu, M Otani, C Chu, Y Nakashima, T Mitamura, Computer Vision -ECCV 2020 Workshops. Lecture Notes in Computer Science. A Bartoli, A Fusiello, Glasgow, UKSpringerAug. 202012536</p>
<p>Enabling Highly-Scalable Remote Memory Access Programming with MPI-3 One Sided. R Gerstenberger, M Besta, T Hoefler, Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '13. the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '13Denver, CO, USAAssociation for Computing MachineryNov. 201353</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. M Geva, D Khashabi, E Segal, T Khot, D Roth, J Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Processing-In-Memory: A Workload-Driven Perspective. S Ghose, A Boroumand, J S Kim, J Ómez-Luna, O Mutlu, IBM Journal of Research and Development. 636Nov. 2019</p>
<p>The Spatial Computer: A Model for Energy-Efficient Parallel Computation. L Gianinazzi, T Ben-Nun, M Besta, S Ashkboos, Y Baumann, P Luczynski, T Hoefler, arXiv:2205.04934Jan. 2023</p>
<p>Parallel Algorithms for Finding Large Cliques in Sparse Graphs. L Gianinazzi, M Besta, Y Schaffner, T Hoefler, Proceedings of the 33rd ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '21. the 33rd ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '21Association for Computing MachineryJuly 2021</p>
<p>Learning Combinatorial Node Labeling Algorithms. L Gianinazzi, M Fries, N Dryden, T Ben-Nun, M Besta, T Hoefler, arXiv:2106.03594May 2022</p>
<p>Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. Y Goyal, T Khot, A Agrawal, D Summers-Stay, D Batra, D Parikh, International Journal of Computer Vision. 1274Apr. 2019</p>
<p>A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models. J Gu, Z Han, S Chen, A Beirami, B He, G Zhang, R Liao, Y Qin, V Tresp, P Torr, arXiv:2307.12980July 2023</p>
<p>Visionand-Language Navigation: A Survey of Tasks, Methods, and Future Directions. J Gu, E Stefani, Q Wu, J Thomason, X Wang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. S Muresan, P Nakov, A Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221Long Papers), ACL '22</p>
<p>GPT4Graph: Can Large Language Models Understand Graph Structured Data? An Empirical Evaluation and Benchmarking. J Guo, L Du, H Liu, arXiv:2305.15066July 2023</p>
<p>Data-Centric Graph Learning: A Survey. Y Guo, D Bo, C Yang, Z Lu, Z Zhang, J Liu, Y Peng, C Shi, IEEE Transactions on Big Data. 111Feb. 2025</p>
<p>A Theory of Emergent In-Context Learning as Implicit Structure Induction. M Hahn, N Goyal, arXiv:2303.07971Mar. 2023</p>
<p>Representation Learning on Graphs: Methods and Applications. W L Hamilton, R Ying, J Leskovec, Bulletin of the Technical Committee on Data Engineering. 403Sept. 2017</p>
<p>FOLIO: Natural Language Reasoning with First-Order Logic. S Han, H Schoelkopf, Y Zhao, Z Qi, M Riddell, W Zhou, J Coady, D Peng, Y Qiao, L Benson, L Sun, A Wardle-Solano, H Szab Ó, E Zubova, M Burtell, J Fan, Y Liu, B Wong, M Sailor, A Ni, L Nan, J Kasai, T Yu, R Zhang, A Fabbri, W M Kryscinski, S Yavuz, Y Liu, X V Lin, S Joty, Y Zhou, C Xiong, R Ying, A Cohan, D Radev, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP '24. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP '24Miami, FL, USAAssociation for Computational LinguisticsNov. 2024</p>
<p>A Survey on Improving NLP Models with Human Explanations. M Hartmann, D Sonntag, Proceedings of the First Workshop on Learning with Natural Language Supervision, LNLS '22. J Andreas, K Narasimhan, A Nematzadeh, the First Workshop on Learning with Natural Language Supervision, LNLS '22Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Solving Math Word Problems by Combining Language Models With Symbolic Solvers. J He-Yueya, G Poesia, R E Wang, N D Goodman, arXiv:2304.09102Apr. 2023</p>
<p>Measuring Massive Multitask Language Understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, Proceedings of the Ninth International Conference on Learning Representations, ICLR '21, Virtual Event. the Ninth International Conference on Learning Representations, ICLR '21, Virtual EventOpenReviewMay 2021</p>
<p>Measuring Mathematical Problem Solving with the MATH Dataset. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems: Datasets and Benchmarks Track, NeurIPS '21, Virtual Event. J Vanschoren, S Yeung, the Thirty-fifth Conference on Neural Information Processing Systems: Datasets and Benchmarks Track, NeurIPS '21, Virtual EventDec. 2021</p>
<p>Constructing A Multi-Hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. X Ho, A.-K Duong Nguyen, S Sugawara, A Aizawa, Proceedings of the 28th International Conference on Computational Linguistics, COLING '20. D Scott, N Bel, C Zong, the 28th International Conference on Computational Linguistics, COLING '20Barcelona, SpainDec. 2020International Committee on Computational Linguistics</p>
<p>Learning to Solve Arithmetic Word Problems with Verb Categorization. M J Hosseini, H Hajishirzi, O Etzioni, N Kushman, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP '14. A Moschitti, B Pang, W Daelemans, the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP '14Doha, QatarAssociation for Computational LinguisticsOct. 2014</p>
<p>Chain-of-Symbol Prompting Elicits Planning in Large Language Models. H Hu, H Lu, H Zhang, W Lam, Y Zhang, arXiv:2305.10276Aug. 2024</p>
<p>A Survey of Knowledge Enhanced Pre-Trained Language Models. L Hu, Z Liu, Z Zhao, L Hou, L Nie, J Li, IEEE Transactions on Knowledge and Data Engineering. 364Apr. 2024</p>
<p>Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-Hop Visual Reasoning. P Hu, J Qi, X Li, H Li, X Wang, B Quan, R Wang, Y Zhou, arXiv:2308.09658Aug. 2023</p>
<p>Beyond Text: A Deep Dive into Large Language Models' Ability on Understanding Graph Data. Y Hu, Z Zhang, L Zhao, Proceedings of the Workshop on New Frontiers in Graph Learning, GLFrontiers '23. the Workshop on New Frontiers in Graph Learning, GLFrontiers '23New Orleans, LA, USAOpenReviewDec. 2023</p>
<p>Towards Reasoning in Large Language Models: A Survey. J Huang, K C , -C Chang, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Can LLMs Effectively Leverage Graph Structural Information through Prompts, and Why. J Huang, X Zhang, Q Mei, J Ma, Transactions on Machine Learning Research. June 2024</p>
<p>HexaMesh: Scaling to Hundreds of Chiplets with an Optimized Chiplet Arrangement. P Iff, M Besta, M Cavalcante, T Fischer, L Benini, T Hoefler, Proceedings of the 60th ACM/IEEE Design Automation Conference, DAC '23. the 60th ACM/IEEE Design Automation Conference, DAC '23San Francisco, CA, USAIEEE PressJuly 2023</p>
<p>Sparse Hamming Graph: A Customizable Network-on-Chip Topology. P Iff, M Besta, M Cavalcante, T Fischer, L Benini, T Hoefler, Proceedings of the 2023 60th ACM/IEEE Design Automation Conference, DAC '23. the 2023 60th ACM/IEEE Design Automation Conference, DAC '23San Francisco, CA, USAJuly 2023</p>
<p>A Latent Space Theory for Emergent Abilities in Large Language Models. H Jiang, arXiv:2304.09960Sept. 2023</p>
<p>RES-PROMPT: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models. S Jiang, Z Shakeri, A Chan, M Sanjabi, H Firooz, Y Xia, B Akyildiz, Y Sun, J Li, Q Wang, A Celikyilmaz, Proceedings of the 2024 Conference of the North American Chapter. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American ChapterMexico City, MexicoAssociation for Computational LinguisticsJune 20241Long Papers), ACL '24</p>
<p>Active Retrieval Augmented Generation. Z Jiang, F Xu, L Gao, Z Sun, Q Liu, J Dwivedi-Yu, Y Yang, J Callan, G Neubig, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 202323</p>
<p>Large Language Models on Graphs: A Comprehensive Survey. B Jin, G Liu, C Han, M Jiang, H Ji, J Han, IEEE Transactions on Knowledge and Data Engineering. 3612Dec. 2024</p>
<p>Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. J Jung, L Qin, S Welleck, F Brahman, C Bhagavatula, R Le Bras, Y Choi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Measuring Compositional Generalization: A Comprehensive Method on Realistic Data. D Keysers, N Schärli, N Scales, H Buisman, D Furrer, S Kashubin, N Momchev, D Sinopalnikov, L Stafiniak, T Tihon, D Tsarkov, X Wang, M Van Zee, O Bousquet, Proceedings of the Eighth International Conference on Learning Representations, ICLR '20, Virtual Event. the Eighth International Conference on Learning Representations, ICLR '20, Virtual EventOpenReviewApr. 2020</p>
<p>Hey AI, Can You Solve Complex Tasks by Talking to Agents?. T Khot, K Richardson, D Khashabi, A Sabharwal, Findings of the Association for Computational Linguistics: ACL 2022. S Muresan, P Nakov, A Villavicencio, Dublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>Decomposed Prompting: A Modular Approach for Solving Complex Tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models. G Kim, S Kim, B Jeon, J Park, J Kang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 202323</p>
<p>Flattened Butterfly: A Cost-Efficient Topology for High-Radix Networks. J Kim, W J Dally, D Abts, Proceedings of the 34th Annual International Symposium on Computer Architecture, ISCA '07. the 34th Annual International Symposium on Computer Architecture, ISCA '07San Diego, CA, USAAssociation for Computing MachineryJune 2007</p>
<p>Technology-Driven, Highly-Scalable Dragonfly Topology. J Kim, W J Dally, S Scott, D Abts, Proceedings of the 35th Annual International Symposium on Computer Architecture, ISCA '08. the 35th Annual International Symposium on Computer Architecture, ISCA '08Beijing, ChinaIEEE PressJune 2008</p>
<p>COGS: A Compositional Generalization Challenge Based on Semantic Interpretation. N Kim, T Linzen, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20. B Webber, T Cohn, Y He, Y Liu, the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20Association for Computational LinguisticsNov. 2020</p>
<p>An LLM Compiler for Parallel Function Calling. S Kim, S Moon, R Tabrizi, N Lee, M W Mahoney, K Keutzer, A Gholami, Proceedings of the 41st International Conference on Machine Learning (ICML '24). R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, F Berkenkamp, the 41st International Conference on Machine Learning (ICML '24)Vienna, AustriaPMLRJuly 2024235of Proceedings of Machine Learning Research</p>
<p>Semi-Supervised Classification with Graph Convolutional Network. T N Kipf, M Welling, Proceedings of the 5th International Conference on Learning Representations, ICLR '17. the 5th International Conference on Learning Representations, ICLR '17Toulon, FranceOpenReviewApr. 2017</p>
<p>Large Language Models Are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, ; S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Proceedings of the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22). the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22)New Orleans, LA, USACurran AssociatesDec. 202235</p>
<p>MAWPS: A Math Word Problem Repository. R Koncel-Kedziorski, S Roy, A Amini, N Kushman, H Hajishirzi, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '16. K Knight, A Nenkova, O Rambow, the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '16San Diego, CA, USAAssociation for Computational LinguisticsJune 2016</p>
<p>Realistic Data Augmentation Framework for Enhancing Tabular Reasoning. D Kumar, V Gupta, S Sharma, S Zhang, Findings of the Association for Computational Linguistics: EMNLP 2022. Y Goldberg, Z Kozareva, Y Zhang, Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. B Lake, M Baroni, Proceedings of the 35th International Conference on International Conference on Machine Learning (ICML '18). J Dy, A Krause, the 35th International Conference on International Conference on Machine Learning (ICML '18)Stockholm, SwedenPMLRJuly 201880of Proceedings of Machine Learning Research</p>
<p>PolarFly: A Cost-Effective and Flexible Low-Diameter Topology. K Lakhotia, M Besta, L Monroe, K Isham, P Iff, T Hoefler, F Petrini, Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22. the International Conference on High Performance Computing, Networking, Storage and Analysis, SC '22Dallas, TX, USAIEEE PressNov. 202212</p>
<p>K Lakhotia, K Isham, L Monroe, M Besta, T Hoefler, F Petrini, Proceedings of the 35th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '23. the 35th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '23Orlando, FL, USAAssociation for Computing MachineryJune 2023</p>
<p>. K Lakhotia, L Monroe, K Isham, M Besta, N Blach, T Hoefler, F Petrini, PolarStar: Expanding the Horizon of Diameter-3</p>
<p>Networks, Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '24. the 36th ACM Symposium on Parallelism in Algorithms and Architectures, SPAA '24Nantes, FranceAssociation for Computing MachineryJune 2024</p>
<p>Boosting Logical Reasoning in Large Language Models Through a New Framework: The Graph of Thought. B Lei, .-H Lin, C Liao, C Ding, arXiv:2308.08614Aug. 2023</p>
<p>Classification and Regression Trees. Leo Breiman, Jerome Friedman, Charles J Stone, R A Olshen, 1984Chapman and Hall</p>
<p>PyTorch Distributed: Experiences on Accelerating Data Parallel Training. S Li, Y Zhao, R Varma, O Salpekar, P Noordhuis, T Li, A Paszke, J Smith, B Vaughan, P Damania, S Chintala, Proc. VLDB Endow. VLDB EndowAug. 202013</p>
<p>A Survey of Graph Meets Large Language Model: Progress and Future Directions. Y Li, Z Li, P Wang, J Li, X Sun, H Cheng, J X Yu, Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '24. K Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI '24Jeju, South KoreaAug. 2024</p>
<p>Making Language Models Better Reasoners with Step-Aware Verifier. Y Li, Z Lin, S Zhang, Q Fu, B Chen, J.-G Lou, W Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231ACL '23</p>
<p>BatchPrompt: Accomplish More with Less. J Lin, M Diesendruck, L Du, R Abraham, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. W Ling, D Yogatama, C Dyer, P Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20171ACL '17</p>
<p>Deductive Verification of Chain-of-Thought Reasoning. Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. C Liu, B Wu, arXiv:2308.11224Sept. 2023</p>
<p>LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning. J Liu, L Cui, H Liu, D Huang, Y Wang, Y Zhang, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI '20. C Bessiere, the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI '20Jan. 2021Virtual Event</p>
<p>Towards Graph Foundation Models: A Survey and Beyond. J Liu, C Yang, Z Lu, J Chen, Y Li, M Zhang, T Bai, Y Fang, L Sun, P S Yu, C Shi, arXiv:2310.11829July 2024</p>
<p>Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Comput. Surv. 559Jan. 2023</p>
<p>Plan, Verify and Switch: Integrated Reasoning with Diverse X-of-Thoughts. T Liu, Q Guo, Y Yang, X Hu, Y Zhang, X Qiu, Z Zhang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 202323</p>
<p>ControlLLM: Augment Language Models with Tools by Searching on Graphs. Z Liu, Z Lai, Z Gao, E Cui, Z Li, X Zhu, L Lu, Q Chen, Y Qiao, J Dai, W Wang, Proceedings of the 18th Conference on Computer Vision (ECCV '24). Lecture Notes. A Leonardis, E Ricci, S Roth, O Russakovsky, T Sattler, G Varol, the 18th Conference on Computer Vision (ECCV '24)Milan, Italy; Nature SwitzerlandSpringerOct. 202415070Computer Science (LNCS)</p>
<p>Large Language Model Guided Tree-of-Thought. J Long, arXiv:2305.08291May 2023</p>
<p>Simpler Context-Dependent Logical Forms via Model Projections. R Long, P Pasupat, P Liang, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. K Erk, N A Smith, the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational LinguisticsAug. 20161Long Papers), ACL '16</p>
<p>Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Proceedings of the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22). S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22)New Orleans, LA, USACurran AssociatesDec. 202235</p>
<p>Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. P Lu, B Peng, H Cheng, M Galley, K.-W Chang, Y N Wu, S.-C Zhu, J Gao, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Dynamic Prompt Learning via Policy Gradient for Semi-Structured Mathematical Reasoning. P Lu, L Qiu, K.-W Chang, Y N Wu, S.-C Zhu, T Rajpurohit, P Clark, A Kalyan, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. L Luo, Y.-F Li, G Haffari, S Pan, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Single-Cell Biological Network Inference Using a Heterogeneous Graph Transformer. A Ma, X Wang, J Li, C Wang, T Xiao, Y Liu, H Cheng, J Wang, Y Li, Y Chang, J Li, D Wang, Y Jiang, L Su, G Xin, S Gu, Z Li, B Liu, D Xu, Q Ma, Nature Communications. 141182023</p>
<p>Self-Refine: Iterative Refinement with Self-Feedback. A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing SystemsNew Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango. A Madaan, A Yazdanbakhsh, arXiv:2209.07686Oct. 2022</p>
<p>The Stanford CoreNLP Natural Language Processing Toolkit. C Manning, M Surdeanu, J Bauer, J Finkel, S Bethard, D Mcclosky, Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL '14. K Bontcheva, J Zhu, 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL '14Baltimore, MD, USAAssociation for Computational LinguisticsJune 2014</p>
<p>Transfer Learning with Synthetic Corpora for Spatial Role Labeling and Reasoning. R Mirzaee, P Kordjamshidi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>A Survey of FPGA-based Accelerators for Convolutional Neural Networks. S , Neural Computing and Applications. 324Feb. 2020</p>
<p>Tree of Uncertain Thoughts Reasoning for Large Language Models. S Mo, M Xin, arXiv:2309.07694Sept. 2023</p>
<p>A Modern Primer on Processing in Memory. O Mutlu, S Ghose, J Ómez-Luna, R Ausavarungnirun, Emerging Computing: From Devices to Systems -Looking Beyond Moore and Von Neumann, Computer Architecture and Design Methodologies (CADM). M M S Aly, A Chattopadhyay, SingaporeSpringer Nature2023</p>
<p>Processing Data Where It Makes Sense: Enabling In. O Mutlu, S Ghose, J Ómez-Luna, R Ausavarungnirun, Memory Computation. Microprocessors and Microsystems. 67June 2019</p>
<p>Direct Evaluation of Chain-of-Thought in Multi-Hop Reasoning with Knowledge Graphs. M.-V Nguyen, L Luo, F Shiri, D Phung, Y.-F Li, T.-T Vu, G Haffari, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>Skeleton-of-Thought: Prompting LLMs for Efficient Parallel Generation. X Ning, Z Lin, Z Zhou, Z Wang, H Yang, Y Wang, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, IEEE Transactions on Knowledge and Data Engineering. 367July 2024</p>
<p>Integrating Graphs with Large Language Models: Methods and Prospects. S Pan, Y Zheng, Y Liu, IEEE Intelligent Systems. 391Jan. 2024</p>
<p>Are NLP Models Really Able to Solve Simple Math Word Problems?. A Patel, S Bhattamishra, N Goyal ; In, K Toutanova, A Rumshisky, L Zettlemoyer, D Hakkani-Tur, I Beltagy, S Bethard, R Cotterell, T Chakraborty, Y Zhou, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL '21. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL '21Association for Computational LinguisticsJune 2021</p>
<p>Measuring and Narrowing the Compositionality Gap in Language Models. O Press, M Zhang, S Min, L Schmidt, N Smith, M Lewis, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks. R Puri, D S Kung, G Janssen, W Zhang, G Domeniconi, V Zolotov, J Dolby, J Chen, M Choudhury, L Decker, V Thost, L Buratti, S Pujar, S Ramji, U Finkler, S Malaika, F Reiss, arXiv:2105.12655Aug. 2021</p>
<p>The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models. J Qi, Z Xu, Y Shen, M Liu, D Jin, Q Wang, L Huang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDec. 202323</p>
<p>Can Large Language Models Empower Molecular Property Prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, arXiv:2307.07443July 2023</p>
<p>Reasoning with Language Model Prompting: A Survey. S Qiao, Y Ou, N Zhang, X Chen, Y Yao, S Deng, C Tan, F Huang, H Chen, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231ACL '23</p>
<p>Language Models are Unsupervised Multitask Learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019. Dec. 4, 2023</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, Journal of Machine Learning Research. 211402020</p>
<p>PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction. A Rasouli, I Kotseruba, T Kunic, J Tsotsos, Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision, ICCV '19. the 2019 IEEE/CVF International Conference on Computer Vision, ICCV '19Seoul, KoreaIEEE PressNov. 2019</p>
<p>AutoGPT: build &amp; use AI agents -Github. T B Richards, Mar. 2023. Jan. 23, 2024</p>
<p>Solving General Arithmetic Word Problems. S Roy, D Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP '15. L Màrquez, C Callison-Burch, J Su, the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP '15Lisbon, PortugalAssociation for Computational LinguisticsSept. 2015</p>
<p>Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. S Saha, O Levy, A Celikyilmaz, M Bansal, J Weston, X Li, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241NAACL '24</p>
<p>Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models. M Sakarvadia, A Ajith, A Khan, D Grzenda, N Hudson, A Bauer, K Chard, I Foster, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '23. Y Belinkov, S Hao, J Jumelet, N Kim, A Mccarthy, H Mohebbi, the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '23SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought. A Saparov, H He, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, E Hambro, L Zettlemoyer, N Cancedda, T Scialom, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models. B Sel, A Tawaha, V Khattar, R Jia, M Jin, Proceedings of the 41st International Conference on Machine Learning (ICML '24). R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, F Berkenkamp, the 41st International Conference on Machine Learning (ICML '24)Vienna, AustriaPMLRJuly 2024235of Proceedings of Machine Learning Research</p>
<p>Ambit: In-Memory Accelerator for Bulk Bitwise Operations Using Commodity DRAM Technology. V Seshadri, D Lee, T Mullins, H Hassan, A Boroumand, J Kim, M A Kozuch, O Mutlu, P B Gibbons, T C Mowry, Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '17. the 50th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '17Cambridge, MA, USAAssociation for Computing MachineryOct. 2017</p>
<p>Psychological Studies of Thought: Thoughts about a Concept of Thought. V Shadrikov, S Kurginyan, O Martynova, Psychology. Journal of Higher School of Economics. 1332016</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. M Shridhar, X Yuan, M.-A Ôté, Y Bisk, A Trischler, M Hausknecht, Proceedings of the Ninth International Conference on Learning Representations, ICLR '21, Virtual Event. the Ninth International Conference on Learning Representations, ICLR '21, Virtual EventOpenReviewMay 2021</p>
<p>Tree Prompting: Efficient Task Adaptation without Fine-Tuning. C Singh, J Morris, A Rush, J Gao, Y Deng, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP '23. H Bouamor, J Pino, K Bali, the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP '23SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>Explaining Data Patterns in Natural Language with Language Models. C Singh, J X Morris, J Aneja, A Rush, J Gao, Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '23. Y Belinkov, S Hao, J Jumelet, N Kim, A Mccarthy, H Mohebbi, the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '23SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>J Smallwood, A Turnbull, H Wang, N S Ho, G L Poerio, T Karapanagiotidis, D Konu, B Mckeown, M Zhang, C Murphy, D Vatansever, D Bzdok, M Konishi, R Leech, P Seli, J W Schooler, B Bernhardt, D S Margulies, E Jefferies, The Neural Correlates of Ongoing Conscious Thought. iScience. Mar. 202124102132</p>
<p>Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models. A Srivastava, Transactions on Machine Learning Research. May 2023Featured Certification</p>
<p>ASQA: Factoid Questions Meet Long-Form Answers. I Stelmakh, Y Luan, B Dhingra, M.-W Chang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22. Y Goldberg, Z Kozareva, Y Zhang, the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP '22Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDec. 2022</p>
<p>Asynchronous Distributed-Memory Triangle Counting and LCC with RMA Caching. A Strausz, F Vella, S Di Girolamo, M Besta, T Hoefler, Proceedings of the IEEE International Parallel and Distributed Processing Symposium, IPDPS '22. the IEEE International Parallel and Distributed Processing Symposium, IPDPS '22Lyon, FranceIEEE PressJune 2022</p>
<p>A Corpus of Natural Language for Visual Reasoning. A Suhr, M Lewis, J Yeh, Y Artzi, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. R Barzilay, M.-Y Kan, the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20172Short Papers), ACL '17</p>
<p>Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph. J Sun, C Xu, L Tang, S Wang, C Lin, Y Gong, L Ni, H.-Y Shum, J Guo, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Graph Prompt Learning: A Comprehensive Survey and Beyond. X Sun, J Zhang, X Wu, H Cheng, Y Xiong, J Li, arXiv:2311.16534Nov. 2023</p>
<p>Mining Heterogeneous Information Networks: Principles and Methodologies. Y Sun, J Han, Synthesis Lectures on Data Mining and Knowledge Discovery. SLDMKD2012Springer International Publishing</p>
<p>ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. O Tafjord, B Dalvi, P Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. C Zong, F Xia, W Li, R Navigli, Association for Computational LinguisticsAug. 2021</p>
<p>Common-senseQA: A Question Answering Challenge Targeting Commonsense Knowledge. A Talmor, J Herzig, N Lourie, J Berant, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. J Burstein, C Doran, T Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MN, USAAssociation for Computational LinguisticsJune 20191NAACL '19</p>
<p>GraphGPT: Graph Instruction Tuning for Large Language Models. J Tang, Y Yang, W Wei, L Shi, L Su, S Cheng, D Yin, C Huang, Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24. the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '24Washington DC, USAAssociation for Computing MachineryJuly 2024</p>
<p>Large Language Models in Medicine. A J Thirunavukarasu, D S J Ting, K Elangovan, L Gutierrez, T F Tan, D S W Ting, Nature Medicine. 298July 2023</p>
<p>Graph Neural Prompting with Large Language Models. Y Tian, H Song, Z Wang, H Wang, Z Hu, F Wang, N V Chawla, P Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>MuSiQue: Multihop Questions via Single-Hop Question Composition. H Trivedi, N Balasubramanian, T Khot, A Sabharwal, Transactions of the Association for Computational Linguistics. 102022</p>
<p>Why Can Large Language Models Generate Correct Chain-of-Thoughts?. R Tutunov, A Grosnit, J Ziomek, J Wang, H Bou-Ammar, arXiv:2310.13571June 2024</p>
<p>Can Language Models Solve Graph Problems in Natural Language?. H Wang, S Feng, T He, Z Tan, X Han, Y Tsvetkov, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Graph Neural Architecture Search with GPT-4. H Wang, Y Gao, X Zheng, P Zhang, H Chen, J Bu, arXiv:2310.01436Mar. 2024</p>
<p>Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. L Wang, W Xu, Y Lan, Z Hu, Y Lan, R K W Lee, E.-P Lim, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. A Rogers, J Boyd-Graber, N Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231ACL '23</p>
<p>Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks. M Wang, D Zheng, Z Ye, Q Gan, M Li, X Song, J Zhou, C Ma, L Yu, Y Gai, T Xiao, T He, G Karypis, J Li, Z Zhang, arXiv:1909.01315Aug. 2020</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases. X Wang, Q Yang, Y Qiu, J Liang, Q He, Z Gu, Y Xiao, W Wang, arXiv:2308.11761Aug. 2023</p>
<p>Knowledge Graph Prompting for Multi-Document Question Answering. Y Wang, N Lipka, R A Rossi, A Siu, R Zhang, T Derr, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>Describe, Explain, Plan and Select: Interactive Planning with LLMs Enables Open-World Multi-Task Agents. Z Wang, S Cai, G Chen, A Liu, X S Ma, Y Liang, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Z Wang, G Zhang, K Yang, N Shi, W Zhou, S Hao, G Xiong, Y Li, M Y Sim, X Chen, Q Zhu, Z Yang, A Nik, Q Liu, C Lin, S Wang, R Liu, W Chen, K Xu, D Liu, Y Guo, J Fu, arXiv:2305.13246Interactive Natural Language Processing. May 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Proceedings of the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22). S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, the Thirty-sixth Annual Conference on Neural Information Processing Systems (NeurIPS '22)New Orleans, LA, USACurran AssociatesDec. 202235</p>
<p>Knowledge Enhanced Pretrained Language Models: A Comprehensive Survey. X Wei, S Wang, D Zhang, P Bhatia, A Arnold, arXiv:2110.08455Oct. 2021</p>
<p>Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. Y Wen, Z Wang, J Sun, Mindmap, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241Long Papers), ACL '24</p>
<p>Large Language Models Are Better Reasoners with Self-Verification. Y Weng, M Zhu, F Xia, B Li, S He, S Liu, B Sun, K Liu, J Zhao, Findings of the Association for Computational Linguistics: EMNLP 2023. H Bouamor, J Pino, K Bali, SingaporeAssociation for Computational LinguisticsDec. 2023</p>
<p>System 2 Attention (Is Something You Might Need Too). J Weston, S Sukhbaatar, arXiv:2311.11829Nov. 2023</p>
<p>The Learnability of In-Context Learning. N Wies, Y Levine, A Shashua, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing SystemsNew Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>OCEAN: Offline Chain-of-Thought Evaluation and Alignment in Large Language Models. J Wu, X Li, R Wang, Y Xia, Y Xiong, J Wang, T Yu, X Chen, B Kveton, L Yao, J Shang, J Mcauley, Proceedings of the Thirteenth International Conference on Learning Representations, ICLR '25. the Thirteenth International Conference on Learning Representations, ICLR '25SingaporeOpenReviewApr. 2025</p>
<p>PromptChainer: Chaining Large Language Model Prompts Through Visual Programming. T Wu, E Jiang, A Donsbach, J Gray, A Molina, M Terry, C J Cai, Extended Abstracts of the Conference on Human Factors in Computing Systems, CHI EA '22. New Orleans, LA, USAAssociation for Computing MachineryMay 2022359</p>
<p>X Wu, K Zhou, M Sun, X Wang, N Liu, arXiv:2303.07275A Survey of Graph Prompting Methods: Techniques, Applications, and Challenges. May 2023</p>
<p>A Comprehensive Survey on Graph Neural Networks. Z Wu, S Pan, F Chen, G Long, C Zhang, P S Yu, IEEE Transactions on Neural Networks and Learning Systems. 321Jan. 2021</p>
<p>An Explanation of In-Context Learning as Implicit Bayesian Inference. S M Xie, A Raghunathan, P Liang, T Ma, Proceedings of the Tenth International Conference on Learning Representations, ICLR '22. the Tenth International Conference on Learning Representations, ICLR '22Kigali, RwandaOpenReviewApr. 2022</p>
<p>Self-Evaluation Guided Beam Search for Reasoning. Y Xie, K Kawaguchi, Y Zhao, J X Zhao, M.-Y Kan, J He, M Xie, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>kNN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. B Xu, Q Wang, Z Mao, Y Lyu, Q She, Y Zhang, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions. C Xu, Q Sun, K Zheng, X Geng, P Zhao, J Feng, C Tao, Q Lin, D Jiang, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>A Survey of Knowledge Enhanced Pre-Trained Language Models. J Yang, X Hu, G Xiao, Y Shen, ACM Trans. Asian Low-Resour. Lang. Inf. Process. Mar. 2024</p>
<p>Give us the Facts: Enhancing Large Language Models With Knowledge Graphs for Fact-Aware Language Modeling. L Yang, H Chen, Z Li, X Ding, X Wu, IEEE Transactions on Knowledge and Data Engineering. 367July 2024</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-Hop Question Answering. Z Yang, P Qi, S Zhang, Y Bengio, W Cohen, R Salakhutdinov, C D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP '18. E Riloff, D Chiang, J Hockenmaier, J Tsujii, the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP '18Brussels, BelgiumAssociation for Computational LinguisticsNov. 2018</p>
<p>Thinking Like an Expert: Multimodal Hypergraph-of-Thought (HoT) Reasoning to Boost Foundation Modals. F Yao, C Tian, J Liu, Z Zhang, Q Liu, L Jin, S Li, X Li, X Sun, arXiv:2308.06207Aug. 2023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23). A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing Systems (NeurIPS '23)New Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>ReAct: Synergizing Reasoning and Acting in Language Models. S Yao, J Zhao, D Yu, N Du, I Shafran, K R Narasimhan, Y Cao, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>GoT: Effective Graph-of-Thought Reasoning in Language Models. Y Yao, Z Li, H Zhao, Findings of the Association for Computational Linguistics: NAACL 2024. K Duh, H Gomez, S Bethard, Mexico City, MexicoAssociation for Computational LinguisticsJune 2024</p>
<p>Language Is All a Graph Needs. R Ye, C Zhang, R Wang, S Xu, Y Zhang, Findings of the Association for Computational Linguistics: EACL 2024. Y Graham, M Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMar. 2024</p>
<p>Large Language Models Are Versatile Decomposers: Decomposing Evidence and Questions for Table-Based Reasoning. Y Ye, B Hui, M Yang, B Li, F Huang, Y Li, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23. the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '23Taipei, TaiwanAssociation for Computing Machinery2023</p>
<p>Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models. J Yu, R He, R Ying, Proceedings of the Twelfth International Conference on Learning Representations, ICLR '24. the Twelfth International Conference on Learning Representations, ICLR '24Vienna, AustriaOpenReviewMay 2024</p>
<p>Forward Looking Active Retrieval Augmented Generation. L Zeit-Altpeter, Proceedings of the AI Summer School at Uni Jena (Poster). the AI Summer School at Uni Jena (Poster)2023</p>
<p>Heterogeneous Graph Neural Network. C Zhang, D Song, C Huang, A Swami, N V Chawla, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '19. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '19Anchorage, AK, USAAssociation for Computing MachineryAug. 2019</p>
<p>On the Paradox of Learning to Reason from Data. H Zhang, L H Li, T Meng, K.-W Chang, G V Broeck, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '23. E Elkind, the Thirty-Second International Joint Conference on Artificial Intelligence, IJCAI '23Macao, SARAug. 2023International Joint Conferences on Artificial Intelligence Organization. Main Track</p>
<p>Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. J Zhang, arXiv:2304.11116May 2023</p>
<p>Making Large Language Models Perform Better in Knowledge Graph Completion. Y Zhang, Z Chen, L Guo, Y Xu, W Zhang, H Chen, Proceedings of the 32nd ACM International Conference on Multimedia, MM '24. the 32nd ACM International Conference on Multimedia, MM '24Melbourne, AustraliaAssociation for Computing MachineryOct. 2024</p>
<p>Small Language Models Need Strong Verifiers to Self-Correct Reasoning. Y Zhang, M Khalifa, L Logeswaran, J Kim, M Lee, H Lee, L Wang, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAug. 2024</p>
<p>Cumulative Reasoning with Large Language Models. Y Zhang, J Yang, Y Yuan, A C Yao, Transactions on Machine Learning Research. July 2025</p>
<p>Deep Learning on Graphs: A Survey. Z Zhang, P Cui, W Zhu, IEEE Transactions on Knowledge and Data Engineering. 341Jan. 2022</p>
<p>Graph Meets LLMs: Towards Large Graph Models. Z Zhang, H Li, Z Zhang, Y Qin, X Wang, W Zhu, Proceedings of the Workshop on New Frontiers in Graph Learning, GLFrontiers '23. the Workshop on New Frontiers in Graph Learning, GLFrontiers '23New Orleans, LA, USAOpenReviewDec. 2023</p>
<p>LLM4DyG: Can Large Language Models Solve Spatial-Temporal Problems on Dynamic Graphs?. Z Zhang, X Wang, Z Zhang, H Li, Y Qin, W Zhu, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24Barcelona, SpainAssociation for Computing MachineryAug. 2024</p>
<p>Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents. Z Zhang, Y Yao, A Zhang, X Tang, X Ma, Z He, Y Wang, M Gerstein, R Wang, G Liu, H Zhao, arXiv:2311.11797Nov. 2023</p>
<p>GraphText: Graph Reasoning in Text Space. J Zhao, L Zhuo, Y Shen, M Qu, K Liu, M Bronstein, Z Zhu, J Tang, Proceedings of the Workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, AFM '24. the Workshop on Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, AFM '24Vancouver, CanadaOpenReviewDec. 2024</p>
<p>A Survey of Large Language Models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, P Liu, J.-Y Nie, J.-R Wen, arXiv:2303.18223Mar. 2025</p>
<p>Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models Through Logic. X Zhao, M Li, W Lu, C Weber, J H Lee, K Chu, S Wermter, N. Calzolari</p>
<p>V Kan, A Hoste, S Lenci, N Sakti, Xue, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING '24. the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING '24Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, H Zhang, J E Gonzalez, I Stoica, Proceedings of the Thirty-seventh Annual Conference on Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, the Thirty-seventh Annual Conference on Neural Information Processing SystemsNew Orleans, LA, USACurran AssociatesDec. 202336</p>
<p>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q V Le, E H Chi, Proceedings of the Eleventh International Conference on Learning Representations, ICLR '23. the Eleventh International Conference on Learning Representations, ICLR '23Kigali, RwandaOpenReviewMay 2023</p>
<p>J Zhou, G Cui, S Hu, Z Zhang, C Yang, Z Liu, L Wang, C Li, M Sun, Graph Neural Networks: A Review of Methods and Applications. 20201</p>
<p>LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities. Y Zhu, X Wang, J Chen, S Qiao, Y Ou, Y Yao, S Deng, H Chen, N Zhang, World Wide Web. 275Aug. 2024</p>
<p>Self-Supervised Multimodal Learning: A Survey. Y Zong, O M Aodha, T Hospedales, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2024LLM: 1. To get brick A, we find E is on top of A</p>
<p>We need to remove brick E first, as it is on top of brick A. </p>
<p>Brick A is now accessible and can be grabbed. So we get the result as E, A</p>            </div>
        </div>

    </div>
</body>
</html>
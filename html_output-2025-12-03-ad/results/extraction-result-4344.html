<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4344 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4344</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4344</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-270351696</p>
                <p><strong>Paper Title:</strong> Fine-tuning large language models for chemical text mining</p>
                <p><strong>Paper Abstract:</strong> Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists. The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature. This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition, reaction role labelling, metal–organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the conversion of reaction paragraphs to action sequences. The fine-tuned LLMs demonstrated impressive performance, significantly reducing the need for repetitive and extensive prompt engineering experiments. For comparison, we guided ChatGPT (GPT-3.5-turbo) and GPT-4 with prompt engineering and fine-tuned GPT-3.5-turbo as well as other open-source LLMs such as Mistral, Llama3, Llama2, T5, and BART. The results showed that the fine-tuned ChatGPT models excelled in all tasks. They achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data. They even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data. Notably, fine-tuned Mistral and Llama3 show competitive abilities. Given their versatility, robustness, and low-code capability, leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4344.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4344.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned GPT-3.5-turbo for chemical text mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised fine-tuning of OpenAI's GPT-3.5-turbo on small-to-moderate human-annotated chemical text datasets to extract structured quantitative experimental information (entities, reaction roles, conditions, NMR shifts, actionable steps). Achieves high exact-match accuracy and F1 on multiple extraction tasks with limited annotated data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuning GPT-3.5-turbo for structured extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Convert multiple chemical text-mining tasks to sequence-to-sequence examples (input paragraph -> formatted target string). Prepare jsonl training files and use OpenAI supervised fine-tuning API to adjust GPT-3.5-turbo weights (few epochs, low learning rate). At inference, the fine-tuned model generates formatted outputs directly; post-processing aligns model output to evaluation labels (minimal for GPT outputs). Tasks included Paragraph2Compound (entity extraction), Paragraph2RXNRole (product + role labelling), Paragraph2MOFInfo (multi-field MOF synthesis parameters), Paragraph2NMR (IUPAC name, frequency, solvent, 1H/13C shifts), and Paragraph2Action (convert protocol paragraphs to action sequences).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo (fine-tuned; versions 0613 and 0125 used; 0125 supports 16k context)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials (MOF) / Analytical (NMR) / Synthetic chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>varies by dataset (examples: USPTO-derived millions available for Paragraph2Compound; MOF dataset: 658 total split into 329 train/test; NMR: 600 annotations; Action sequences: 1,060 hand-annotated, augmented to 14,168)</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured experimental quantitative data extraction (temperatures, times, volumes/amounts, yields, NMR chemical shifts) — i.e., quantitative experimental parameters and relationships that enable downstream pattern discovery</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured, human-readable formatted strings encoding multiple fields per example (CSV-like / field:value lists); outputs intended to be parsed into structured tables (exact-match string format; Levenshtein similarity used for fuzzy matches)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Held-out test sets and evaluation metrics (precision/recall/F1 for entity/role tasks; Levenshtein similarity, exact-match accuracy, partial accuracy, and modified BLEU for multi-field and sentence-level tasks); human re-evaluation of some outputs (MOF extraction compared against human-evaluated ChatGPT outputs during dataset creation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported per-task best results: Paragraph2RXNRole product extraction F1 = 77.1% (fine-tuned GPT-3.5 best), reaction role labelling F1 = 83.0%; Paragraph2MOFInfo exact match accuracy single-reaction = 82.7%, multi-reaction = 68.8%; Paragraph2Action full-sentence exact accuracy up to 69.0% after augmentation (GPT-3.5-turbo fine-tuned on 14,168 augmented samples); Paragraph2Compound F1 up to ~90% with large training sets; Paragraph2NMR: high Levenshtein similarity and competitive exact-match accuracy (specific tables in ESI). The paper states overall exact accuracy ranges ~69%–95% across tasks for fine-tuned ChatGPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to prompt-only GPT-3.5/GPT-4 (zero/few-shot), fine-tuned GPT-3.5 outperformed substantially (e.g., Paragraph2Action: GPT-4 60-shot full-sentence exact accuracy = 32.7% vs fine-tuned GPT-3.5 up to 69.0%). Also compared to task-specific prior models: product extraction F1 77.1% vs ChemBERT 76.2%; reaction-role F1 83.0% vs ChemRxnBERT 78.7%. Compared to T5/BART fine-tuned baselines, fine-tuned GPTs, Mistral, and Llama3 achieved higher post-processing-free ratios and better exact-match metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires high-quality annotated examples for each task (but often only hundreds suffice); cost and API limits for proprietary fine-tuning (OpenAI costs and epoch/context limits); risk of overfitting with too many epochs; hallucination lessened but possible; formatting consistency must be enforced; multi-reaction paragraphs and complex nested fields remain challenging (exact-match lower for multi-reaction MOF paragraphs). Context-length limitations affect in-context learning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4344.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-only ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering with ChatGPT (GPT-3.5 / GPT-4) without fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use zero-shot or few-shot natural language prompts (examples included) to instruct GPT-3.5/GPT-4 to extract structured chemical information from paragraphs; requires iterative prompt design and is sensitive to examples/context length.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompt engineering (zero-shot / few-shot) with GPT-3.5/GPT-4 for extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Design descriptive instructions and include a varying number of example paragraph -> desired-output pairs in the model context (few-shot). For experiments the authors used GPT-3.5-turbo-0613 and GPT-4-0613 (later also GPT-3.5-turbo-0125 with extended context). Performance assessed as is — no parameter updates. Prompt variability introduced randomness in prompt-only experiments. Attempts included up to 60 examples (approaching token limits).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo (0613 and 0125), GPT-4 (0613)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry (same tasks as fine-tuning experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of quantitative experimental parameters (temperatures, times, amounts, yields, NMR shifts) via examples in prompts (i.e., collection of numeric data points from text)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Generated formatted strings following prompt instructions (but with formatting variability and occasional violations); intended to parse to structured tables</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Held-out test sets; same evaluation metrics as fine-tuned models (F1, Levenshtein similarity, exact match, modified BLEU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Generally poorer than fine-tuned models. Example: Paragraph2Action best in-context result (GPT-4, 60-shot) full-sentence exact accuracy = 32.7%, BLEU = 65.0, Levenshtein similarity = 72.8. For other tasks prompt-only GPT often gave lower exact-match and required iterative prompt re-design.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a baseline against fine-tuned LLMs and task-specific models; fine-tuned models outperformed prompt-only by large margins (often 10–20% absolute improvement in exact-match or BLEU when using same number of examples).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Highly sensitive to prompt phrasing and example selection; limited by model context window (4096–8192 tokens for tested versions; 16k for later 0125); outputs can violate strict formatting and therefore need post-processing; poor generalization across diverse paragraphs without expensive prompt tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4344.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-Open-LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuning open-source LLMs (Mistral, Llama3, Llama2, T5, BART)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Full-parameter or parameter-efficient fine-tuning of open-source generative LMs (Mistral-7b-instruct, Llama3-8b-instruct, Llama2-13b-chat via Q-LoRA, T5-base, BART-base) to perform the same sequence-to-sequence chemical extraction tasks, enabling local deployment and high exact-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Fine-tuning open-source LLMs for chemical extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Prepare sequence-to-sequence training data and fine-tune models: full-parameter fine-tuning for Mistral-7b-instruct-v0.2 and Llama3-8b-instruct on 4×A100 GPUs; Q-LoRA for Llama2-13b-chat on 1×A100 to reduce resources; multitask training for T5/BART for multi-attribute long outputs. Inference accelerated using vllm. Post-processing applied where tokenization/vocabulary caused misalignment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Mistral-7b-instruct-v0.2 (full fine-tune), Llama3-8b-instruct (full), Llama2-13b-chat (Q-LoRA), T5-base (fine-tuned), BART-base (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Materials / Analytical / Synthetic chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>same datasets as main experiments (see FT-GPT-3.5 entry); e.g., MOF dataset 658 samples total, Action dataset 1,060 (and augmented 14,168) — model training sizes varied per experiment</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of quantitative experimental parameters (temperatures, times, volumes/amounts, yields, NMR shifts) and transformation to structured action sequences — i.e., structured quantitative data useful for discovering relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Formatted strings / multi-field outputs; for some models tokenization issues required additional alignment post-processing</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Held-out test sets; evaluation by F1 for entities/roles and Levenshtein similarity / exact match / modified BLEU for multi-field and action-sequence tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Fine-tuned Mistral and Llama3 achieved competitive performance to fine-tuned GPT-3.5: e.g., Paragraph2Action Mistral-7b-instruct (fine-tuned on 1,060) full-sentence exact accuracy = 64.8%, GPT-3.5-turbo (fine-tuned on 1,060) = 63.6%; Mistral/Llama3 and GPTs led in exact-match for Paragraph2NMR. Post-processing-free output ratios >99% for fine-tuned GPTs, Mistral, and Llamas (better than T5/BART).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared against fine-tuned GPT-3.5 and prompt-only GPTs; open-source fine-tuned models were competitive, narrowing gap with proprietary GPTs. T5/BART underperformed on exact-match metrics due to tokenization/vocabulary alignment issues unless multitask tricks were applied.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Hardware and memory requirements for full fine-tuning (A100 GPUs); tokenization/vocabulary limitations in some models (T5/BART) cause misalignment; need hyperparameter tuning (learning rates, LoRA params) to avoid overfitting; still requires curated annotations; inference optimization required for practical throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4344.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vaucher et al. (task-adaptive pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-adaptive pre-trained transformers for converting synthesis paragraphs to action sequences (Vaucher et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that used task-adaptive pretraining on rule-based synthetic text data followed by supervised refinement to convert natural-language lab procedures into structured action sequences for automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated extraction of chemical synthesis actions from experimental procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Task-adaptive pretraining + supervised refinement for Paragraph->Action</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pretrain transformer models on large-scale, rule-derived synthetic data (2 million rule-based samples in the referenced work), then refine on human-annotated action-sequence pairs to map procedural paragraphs into an action-language (sequence generation). The referenced work used an ensemble of three models, with substantial task-adaptive pretraining on millions of synthetic examples.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Transformer-based sequence generation models (ensemble of three models in the cited work; specific architectures not detailed in this paper's text)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Synthetic chemistry / laboratory automation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Structured procedural knowledge and quantitative protocol parameters (ordered actions with quantities/times/temps) enabling robotic execution — effectively encoding quantitative stepwise relationships</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured action sequences (domain-specific action language / machine-executable steps)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Comparison on standard action-sequence benchmarks (held-out test set); reported in this paper as SOTA baseline the authors compared against (ensemble results from Vaucher et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In the referenced work, ensemble + task-adaptive pretraining on 2M rule-based samples + 1,060 hand-annotated samples achieved previous SOTA for Paragraph2Action (specific numbers reported by Vaucher et al.; in this paper fine-tuned GPT-3.5 and Mistral surpassed those SOTA numbers under some settings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as a key baseline for Paragraph2Action; in this paper fine-tuned GPT-3.5 (with augmentation to 14,168) and fine-tuned Mistral/Llama approaches matched or exceeded previously reported Vaucher ensemble results.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires vast amounts of rule-derived pretraining data to reach strong performance; generating and maintaining high-quality rule-based corpora is labor-intensive; ensemble complexity and pretraining cost are high.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4344.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chem-DataExtractor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemDataExtractor toolkit</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A rule-, dictionary-, and ML hybrid tool developed to extract chemical entities, properties, measurements and relationships from scientific documents using unsupervised word clustering, CRFs, grammar rules and dictionary matching.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Rule-based + statistical extraction toolkit (ChemDataExtractor)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Combines unsupervised word clustering, conditional random fields (CRFs), hand-written grammar rules and dictionary matching to identify chemical entities and associated properties/measurements from text (e.g., numeric properties). Outputs structured data suitable for database population.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not an LLM; classical NLP + statistical models (CRF) and rule-based components</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / materials / scientific literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of quantitative measurements and property-value pairs (physicochemical properties, measurement values) rather than deriving laws per se</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured records (entity-property-value triples, tables) intended for database ingestion</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Benchmarking on standard extraction tasks; manual/automatic evaluation of extraction precision/recall (as reported in original ChemDataExtractor papers referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not quantified in this paper beyond citation; cited historically as an earlier tool for extracting chemical entities and properties.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Positioned historically against earlier rule-based and CRF systems; contrasted in this paper with modern LLM-based methods which offer more flexible generalization across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Rule-heavy systems require extensive domain rules and are hard to adapt; brittle to diverse phrasing and complex sentence structure; needs complementary modules for complex relations.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4344.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemRxnExtractor / ChemBERT / ChemRxnBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-like models for reaction extraction (ChemRxnExtractor / ChemBERT / ChemRxnBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Token-classification BERT-like models developed for extracting reaction components and labelling reaction roles (reactant, product, catalyst, solvent, temperature, time, yield) from synthesis paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automated chemical reaction extraction from scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>BERT-like token classification for reaction role and product extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Frame extraction as token-level BIO classification with BERT-style encoders, possibly with two-stage pipelines (product extraction then role labelling). Outputs token tags which are post-processed into labeled spans. These models are task-adaptively pre-trained on large in-domain corpora (e.g., 944k reaction-inclusive sentences) before fine-tuning for specific extraction labels.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>BERT-like transformer encoders (task-adaptively pre-trained variants referred to as ChemBERT / ChemRxnBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Synthetic chemistry / reaction text mining</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of reaction components and associated quantitative conditions (yields, temperature, time) enabling downstream statistical analyses of reaction outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>BIO token tags converted to labeled entities and role-annotated spans; structured datasets (entity-role pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Held-out test sets, precision/recall/F1 at token and entity levels (as reported in cited works and compared in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Prior SOTA numbers cited: ChemBERT product extraction F1 = 76.2%; ChemRxnBERT reaction-role labelling F1 = 78.7%. Fine-tuned GPT-3.5 in this paper slightly outperformed those numbers (77.1% and 83.0% respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Used as baselines for product and role extraction; fine-tuned generative LLMs achieved comparable or better performance with far less in-domain pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Token-classification output sometimes requires alignment/post-processing; needs large in-domain pretraining corpora to reach high performance; architectures are less flexible for multi-field or generative reconstruction tasks compared to seq2seq LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4344.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4344.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng et al. ChatGPT MOF extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier study that applied prompt-engineered ChatGPT to extract MOF synthesis information from literature paragraphs; used as a reference point in this paper and compared qualitatively/quantitatively to fine-tuned approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Prompt-engineered ChatGPT for MOF synthesis extraction</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Use prompt engineering to guide ChatGPT to extract MOF synthesis parameters from paragraphs (zero/few-shot prompting). Dataset and evaluation in the referenced work were used for comparison; the present paper re-annotated Zheng et al.'s raw data into sequence-to-sequence format and evaluated fine-tuned LLMs against prompt-only GPT outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (GPT-3.5-family, prompt-engineered)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials chemistry (Metal–Organic Framework synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>Extraction of quantitative synthesis parameters (metal amounts, solvent volumes, temperatures, times, yields) — enabling construction of structured MOF synthesis datasets for downstream modelling</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Formatted multi-field extraction (11 parameters per reaction in cited dataset), parsed into structured records</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human-evaluated answers and comparisons; in this paper the authors used Levenshtein similarity and exact-match accuracy for evaluation on the re-annotated dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper reports that fine-tuned GPT-3.5 improved exact-match accuracy by >20% compared to prompt-engineered GPT-3.5 on MOF extraction; fine-tuned GPT-3.5 achieved exact accuracy 82.7% (single) / 68.8% (multiple reactions) on the re-annotated MOF test set.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Prompt-only ChatGPT often gave inconsistent formatting across different paragraphs, necessitating iterative prompt re-design; poor exact-match accuracy relative to fine-tuned models, especially for multi-reaction paragraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Fine-tuning large language models for chemical text mining', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automated extraction of chemical synthesis actions from experimental procedures. <em>(Rating: 2)</em></li>
                <li>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. <em>(Rating: 2)</em></li>
                <li>Automated chemical reaction extraction from scientific literature. <em>(Rating: 2)</em></li>
                <li>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature. <em>(Rating: 2)</em></li>
                <li>ReactionDataExtractor 2.0: a deep learning approach for data extraction from chemical reaction schemes. <em>(Rating: 1)</em></li>
                <li>MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation. <em>(Rating: 1)</em></li>
                <li>RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4344",
    "paper_id": "paper-270351696",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "FT-GPT-3.5-turbo",
            "name_full": "Fine-tuned GPT-3.5-turbo for chemical text mining",
            "brief_description": "Supervised fine-tuning of OpenAI's GPT-3.5-turbo on small-to-moderate human-annotated chemical text datasets to extract structured quantitative experimental information (entities, reaction roles, conditions, NMR shifts, actionable steps). Achieves high exact-match accuracy and F1 on multiple extraction tasks with limited annotated data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Fine-tuning GPT-3.5-turbo for structured extraction",
            "method_description": "Convert multiple chemical text-mining tasks to sequence-to-sequence examples (input paragraph -&gt; formatted target string). Prepare jsonl training files and use OpenAI supervised fine-tuning API to adjust GPT-3.5-turbo weights (few epochs, low learning rate). At inference, the fine-tuned model generates formatted outputs directly; post-processing aligns model output to evaluation labels (minimal for GPT outputs). Tasks included Paragraph2Compound (entity extraction), Paragraph2RXNRole (product + role labelling), Paragraph2MOFInfo (multi-field MOF synthesis parameters), Paragraph2NMR (IUPAC name, frequency, solvent, 1H/13C shifts), and Paragraph2Action (convert protocol paragraphs to action sequences).",
            "llm_model_used": "GPT-3.5-turbo (fine-tuned; versions 0613 and 0125 used; 0125 supports 16k context)",
            "scientific_domain": "Chemistry / Materials (MOF) / Analytical (NMR) / Synthetic chemistry",
            "number_of_papers": "varies by dataset (examples: USPTO-derived millions available for Paragraph2Compound; MOF dataset: 658 total split into 329 train/test; NMR: 600 annotations; Action sequences: 1,060 hand-annotated, augmented to 14,168)",
            "type_of_quantitative_law": "Structured experimental quantitative data extraction (temperatures, times, volumes/amounts, yields, NMR chemical shifts) — i.e., quantitative experimental parameters and relationships that enable downstream pattern discovery",
            "extraction_output_format": "Structured, human-readable formatted strings encoding multiple fields per example (CSV-like / field:value lists); outputs intended to be parsed into structured tables (exact-match string format; Levenshtein similarity used for fuzzy matches)",
            "validation_method": "Held-out test sets and evaluation metrics (precision/recall/F1 for entity/role tasks; Levenshtein similarity, exact-match accuracy, partial accuracy, and modified BLEU for multi-field and sentence-level tasks); human re-evaluation of some outputs (MOF extraction compared against human-evaluated ChatGPT outputs during dataset creation).",
            "performance_metrics": "Reported per-task best results: Paragraph2RXNRole product extraction F1 = 77.1% (fine-tuned GPT-3.5 best), reaction role labelling F1 = 83.0%; Paragraph2MOFInfo exact match accuracy single-reaction = 82.7%, multi-reaction = 68.8%; Paragraph2Action full-sentence exact accuracy up to 69.0% after augmentation (GPT-3.5-turbo fine-tuned on 14,168 augmented samples); Paragraph2Compound F1 up to ~90% with large training sets; Paragraph2NMR: high Levenshtein similarity and competitive exact-match accuracy (specific tables in ESI). The paper states overall exact accuracy ranges ~69%–95% across tasks for fine-tuned ChatGPT models.",
            "baseline_comparison": "Compared to prompt-only GPT-3.5/GPT-4 (zero/few-shot), fine-tuned GPT-3.5 outperformed substantially (e.g., Paragraph2Action: GPT-4 60-shot full-sentence exact accuracy = 32.7% vs fine-tuned GPT-3.5 up to 69.0%). Also compared to task-specific prior models: product extraction F1 77.1% vs ChemBERT 76.2%; reaction-role F1 83.0% vs ChemRxnBERT 78.7%. Compared to T5/BART fine-tuned baselines, fine-tuned GPTs, Mistral, and Llama3 achieved higher post-processing-free ratios and better exact-match metrics.",
            "challenges_limitations": "Requires high-quality annotated examples for each task (but often only hundreds suffice); cost and API limits for proprietary fine-tuning (OpenAI costs and epoch/context limits); risk of overfitting with too many epochs; hallucination lessened but possible; formatting consistency must be enforced; multi-reaction paragraphs and complex nested fields remain challenging (exact-match lower for multi-reaction MOF paragraphs). Context-length limitations affect in-context learning baselines.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.0",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompt-only ChatGPT",
            "name_full": "Prompt engineering with ChatGPT (GPT-3.5 / GPT-4) without fine-tuning",
            "brief_description": "Use zero-shot or few-shot natural language prompts (examples included) to instruct GPT-3.5/GPT-4 to extract structured chemical information from paragraphs; requires iterative prompt design and is sensitive to examples/context length.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Prompt engineering (zero-shot / few-shot) with GPT-3.5/GPT-4 for extraction",
            "method_description": "Design descriptive instructions and include a varying number of example paragraph -&gt; desired-output pairs in the model context (few-shot). For experiments the authors used GPT-3.5-turbo-0613 and GPT-4-0613 (later also GPT-3.5-turbo-0125 with extended context). Performance assessed as is — no parameter updates. Prompt variability introduced randomness in prompt-only experiments. Attempts included up to 60 examples (approaching token limits).",
            "llm_model_used": "GPT-3.5-turbo (0613 and 0125), GPT-4 (0613)",
            "scientific_domain": "Chemistry (same tasks as fine-tuning experiments)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of quantitative experimental parameters (temperatures, times, amounts, yields, NMR shifts) via examples in prompts (i.e., collection of numeric data points from text)",
            "extraction_output_format": "Generated formatted strings following prompt instructions (but with formatting variability and occasional violations); intended to parse to structured tables",
            "validation_method": "Held-out test sets; same evaluation metrics as fine-tuned models (F1, Levenshtein similarity, exact match, modified BLEU).",
            "performance_metrics": "Generally poorer than fine-tuned models. Example: Paragraph2Action best in-context result (GPT-4, 60-shot) full-sentence exact accuracy = 32.7%, BLEU = 65.0, Levenshtein similarity = 72.8. For other tasks prompt-only GPT often gave lower exact-match and required iterative prompt re-design.",
            "baseline_comparison": "Used as a baseline against fine-tuned LLMs and task-specific models; fine-tuned models outperformed prompt-only by large margins (often 10–20% absolute improvement in exact-match or BLEU when using same number of examples).",
            "challenges_limitations": "Highly sensitive to prompt phrasing and example selection; limited by model context window (4096–8192 tokens for tested versions; 16k for later 0125); outputs can violate strict formatting and therefore need post-processing; poor generalization across diverse paragraphs without expensive prompt tuning.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.1",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "FT-Open-LLMs",
            "name_full": "Fine-tuning open-source LLMs (Mistral, Llama3, Llama2, T5, BART)",
            "brief_description": "Full-parameter or parameter-efficient fine-tuning of open-source generative LMs (Mistral-7b-instruct, Llama3-8b-instruct, Llama2-13b-chat via Q-LoRA, T5-base, BART-base) to perform the same sequence-to-sequence chemical extraction tasks, enabling local deployment and high exact-match accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Fine-tuning open-source LLMs for chemical extraction",
            "method_description": "Prepare sequence-to-sequence training data and fine-tune models: full-parameter fine-tuning for Mistral-7b-instruct-v0.2 and Llama3-8b-instruct on 4×A100 GPUs; Q-LoRA for Llama2-13b-chat on 1×A100 to reduce resources; multitask training for T5/BART for multi-attribute long outputs. Inference accelerated using vllm. Post-processing applied where tokenization/vocabulary caused misalignment.",
            "llm_model_used": "Mistral-7b-instruct-v0.2 (full fine-tune), Llama3-8b-instruct (full), Llama2-13b-chat (Q-LoRA), T5-base (fine-tuned), BART-base (fine-tuned)",
            "scientific_domain": "Chemistry / Materials / Analytical / Synthetic chemistry",
            "number_of_papers": "same datasets as main experiments (see FT-GPT-3.5 entry); e.g., MOF dataset 658 samples total, Action dataset 1,060 (and augmented 14,168) — model training sizes varied per experiment",
            "type_of_quantitative_law": "Extraction of quantitative experimental parameters (temperatures, times, volumes/amounts, yields, NMR shifts) and transformation to structured action sequences — i.e., structured quantitative data useful for discovering relationships",
            "extraction_output_format": "Formatted strings / multi-field outputs; for some models tokenization issues required additional alignment post-processing",
            "validation_method": "Held-out test sets; evaluation by F1 for entities/roles and Levenshtein similarity / exact match / modified BLEU for multi-field and action-sequence tasks.",
            "performance_metrics": "Fine-tuned Mistral and Llama3 achieved competitive performance to fine-tuned GPT-3.5: e.g., Paragraph2Action Mistral-7b-instruct (fine-tuned on 1,060) full-sentence exact accuracy = 64.8%, GPT-3.5-turbo (fine-tuned on 1,060) = 63.6%; Mistral/Llama3 and GPTs led in exact-match for Paragraph2NMR. Post-processing-free output ratios &gt;99% for fine-tuned GPTs, Mistral, and Llamas (better than T5/BART).",
            "baseline_comparison": "Compared against fine-tuned GPT-3.5 and prompt-only GPTs; open-source fine-tuned models were competitive, narrowing gap with proprietary GPTs. T5/BART underperformed on exact-match metrics due to tokenization/vocabulary alignment issues unless multitask tricks were applied.",
            "challenges_limitations": "Hardware and memory requirements for full fine-tuning (A100 GPUs); tokenization/vocabulary limitations in some models (T5/BART) cause misalignment; need hyperparameter tuning (learning rates, LoRA params) to avoid overfitting; still requires curated annotations; inference optimization required for practical throughput.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.2",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Vaucher et al. (task-adaptive pretraining)",
            "name_full": "Task-adaptive pre-trained transformers for converting synthesis paragraphs to action sequences (Vaucher et al.)",
            "brief_description": "A prior approach that used task-adaptive pretraining on rule-based synthetic text data followed by supervised refinement to convert natural-language lab procedures into structured action sequences for automation.",
            "citation_title": "Automated extraction of chemical synthesis actions from experimental procedures.",
            "mention_or_use": "mention",
            "method_name": "Task-adaptive pretraining + supervised refinement for Paragraph-&gt;Action",
            "method_description": "Pretrain transformer models on large-scale, rule-derived synthetic data (2 million rule-based samples in the referenced work), then refine on human-annotated action-sequence pairs to map procedural paragraphs into an action-language (sequence generation). The referenced work used an ensemble of three models, with substantial task-adaptive pretraining on millions of synthetic examples.",
            "llm_model_used": "Transformer-based sequence generation models (ensemble of three models in the cited work; specific architectures not detailed in this paper's text)",
            "scientific_domain": "Synthetic chemistry / laboratory automation",
            "number_of_papers": null,
            "type_of_quantitative_law": "Structured procedural knowledge and quantitative protocol parameters (ordered actions with quantities/times/temps) enabling robotic execution — effectively encoding quantitative stepwise relationships",
            "extraction_output_format": "Structured action sequences (domain-specific action language / machine-executable steps)",
            "validation_method": "Comparison on standard action-sequence benchmarks (held-out test set); reported in this paper as SOTA baseline the authors compared against (ensemble results from Vaucher et al.).",
            "performance_metrics": "In the referenced work, ensemble + task-adaptive pretraining on 2M rule-based samples + 1,060 hand-annotated samples achieved previous SOTA for Paragraph2Action (specific numbers reported by Vaucher et al.; in this paper fine-tuned GPT-3.5 and Mistral surpassed those SOTA numbers under some settings).",
            "baseline_comparison": "Used as a key baseline for Paragraph2Action; in this paper fine-tuned GPT-3.5 (with augmentation to 14,168) and fine-tuned Mistral/Llama approaches matched or exceeded previously reported Vaucher ensemble results.",
            "challenges_limitations": "Requires vast amounts of rule-derived pretraining data to reach strong performance; generating and maintaining high-quality rule-based corpora is labor-intensive; ensemble complexity and pretraining cost are high.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.3",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chem-DataExtractor",
            "name_full": "ChemDataExtractor toolkit",
            "brief_description": "A rule-, dictionary-, and ML hybrid tool developed to extract chemical entities, properties, measurements and relationships from scientific documents using unsupervised word clustering, CRFs, grammar rules and dictionary matching.",
            "citation_title": "ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature.",
            "mention_or_use": "mention",
            "method_name": "Rule-based + statistical extraction toolkit (ChemDataExtractor)",
            "method_description": "Combines unsupervised word clustering, conditional random fields (CRFs), hand-written grammar rules and dictionary matching to identify chemical entities and associated properties/measurements from text (e.g., numeric properties). Outputs structured data suitable for database population.",
            "llm_model_used": "Not an LLM; classical NLP + statistical models (CRF) and rule-based components",
            "scientific_domain": "Chemistry / materials / scientific literature mining",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of quantitative measurements and property-value pairs (physicochemical properties, measurement values) rather than deriving laws per se",
            "extraction_output_format": "Structured records (entity-property-value triples, tables) intended for database ingestion",
            "validation_method": "Benchmarking on standard extraction tasks; manual/automatic evaluation of extraction precision/recall (as reported in original ChemDataExtractor papers referenced).",
            "performance_metrics": "Not quantified in this paper beyond citation; cited historically as an earlier tool for extracting chemical entities and properties.",
            "baseline_comparison": "Positioned historically against earlier rule-based and CRF systems; contrasted in this paper with modern LLM-based methods which offer more flexible generalization across tasks.",
            "challenges_limitations": "Rule-heavy systems require extensive domain rules and are hard to adapt; brittle to diverse phrasing and complex sentence structure; needs complementary modules for complex relations.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.4",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChemRxnExtractor / ChemBERT / ChemRxnBERT",
            "name_full": "BERT-like models for reaction extraction (ChemRxnExtractor / ChemBERT / ChemRxnBERT)",
            "brief_description": "Token-classification BERT-like models developed for extracting reaction components and labelling reaction roles (reactant, product, catalyst, solvent, temperature, time, yield) from synthesis paragraphs.",
            "citation_title": "Automated chemical reaction extraction from scientific literature.",
            "mention_or_use": "mention",
            "method_name": "BERT-like token classification for reaction role and product extraction",
            "method_description": "Frame extraction as token-level BIO classification with BERT-style encoders, possibly with two-stage pipelines (product extraction then role labelling). Outputs token tags which are post-processed into labeled spans. These models are task-adaptively pre-trained on large in-domain corpora (e.g., 944k reaction-inclusive sentences) before fine-tuning for specific extraction labels.",
            "llm_model_used": "BERT-like transformer encoders (task-adaptively pre-trained variants referred to as ChemBERT / ChemRxnBERT)",
            "scientific_domain": "Synthetic chemistry / reaction text mining",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of reaction components and associated quantitative conditions (yields, temperature, time) enabling downstream statistical analyses of reaction outcomes",
            "extraction_output_format": "BIO token tags converted to labeled entities and role-annotated spans; structured datasets (entity-role pairs)",
            "validation_method": "Held-out test sets, precision/recall/F1 at token and entity levels (as reported in cited works and compared in this paper).",
            "performance_metrics": "Prior SOTA numbers cited: ChemBERT product extraction F1 = 76.2%; ChemRxnBERT reaction-role labelling F1 = 78.7%. Fine-tuned GPT-3.5 in this paper slightly outperformed those numbers (77.1% and 83.0% respectively).",
            "baseline_comparison": "Used as baselines for product and role extraction; fine-tuned generative LLMs achieved comparable or better performance with far less in-domain pretraining data.",
            "challenges_limitations": "Token-classification output sometimes requires alignment/post-processing; needs large in-domain pretraining corpora to reach high performance; architectures are less flexible for multi-field or generative reconstruction tasks compared to seq2seq LLMs.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.5",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Zheng et al. ChatGPT MOF extraction",
            "name_full": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis",
            "brief_description": "An earlier study that applied prompt-engineered ChatGPT to extract MOF synthesis information from literature paragraphs; used as a reference point in this paper and compared qualitatively/quantitatively to fine-tuned approaches.",
            "citation_title": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis.",
            "mention_or_use": "mention",
            "method_name": "Prompt-engineered ChatGPT for MOF synthesis extraction",
            "method_description": "Use prompt engineering to guide ChatGPT to extract MOF synthesis parameters from paragraphs (zero/few-shot prompting). Dataset and evaluation in the referenced work were used for comparison; the present paper re-annotated Zheng et al.'s raw data into sequence-to-sequence format and evaluated fine-tuned LLMs against prompt-only GPT outputs.",
            "llm_model_used": "ChatGPT (GPT-3.5-family, prompt-engineered)",
            "scientific_domain": "Materials chemistry (Metal–Organic Framework synthesis)",
            "number_of_papers": null,
            "type_of_quantitative_law": "Extraction of quantitative synthesis parameters (metal amounts, solvent volumes, temperatures, times, yields) — enabling construction of structured MOF synthesis datasets for downstream modelling",
            "extraction_output_format": "Formatted multi-field extraction (11 parameters per reaction in cited dataset), parsed into structured records",
            "validation_method": "Human-evaluated answers and comparisons; in this paper the authors used Levenshtein similarity and exact-match accuracy for evaluation on the re-annotated dataset.",
            "performance_metrics": "This paper reports that fine-tuned GPT-3.5 improved exact-match accuracy by &gt;20% compared to prompt-engineered GPT-3.5 on MOF extraction; fine-tuned GPT-3.5 achieved exact accuracy 82.7% (single) / 68.8% (multiple reactions) on the re-annotated MOF test set.",
            "challenges_limitations": "Prompt-only ChatGPT often gave inconsistent formatting across different paragraphs, necessitating iterative prompt re-design; poor exact-match accuracy relative to fine-tuned models, especially for multi-reaction paragraphs.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4344.6",
            "source_info": {
                "paper_title": "Fine-tuning large language models for chemical text mining",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automated extraction of chemical synthesis actions from experimental procedures.",
            "rating": 2,
            "sanitized_title": "automated_extraction_of_chemical_synthesis_actions_from_experimental_procedures"
        },
        {
            "paper_title": "ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis.",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Automated chemical reaction extraction from scientific literature.",
            "rating": 2,
            "sanitized_title": "automated_chemical_reaction_extraction_from_scientific_literature"
        },
        {
            "paper_title": "ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature.",
            "rating": 2,
            "sanitized_title": "chemdataextractor_a_toolkit_for_automated_extraction_of_chemical_information_from_the_scientific_literature"
        },
        {
            "paper_title": "ReactionDataExtractor 2.0: a deep learning approach for data extraction from chemical reaction schemes.",
            "rating": 1,
            "sanitized_title": "reactiondataextractor_20_a_deep_learning_approach_for_data_extraction_from_chemical_reaction_schemes"
        },
        {
            "paper_title": "MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation.",
            "rating": 1,
            "sanitized_title": "molscribe_robust_molecular_structure_recognition_with_imagetograph_generation"
        },
        {
            "paper_title": "RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing.",
            "rating": 1,
            "sanitized_title": "rxnscribe_a_sequence_generation_model_for_reaction_diagram_parsing"
        }
    ],
    "cost": 0.0186405,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Fine-tuning large language models for chemical text mining †</p>
<p>Wei Zhang 
Qinggong Wang 
Nanjing University of Chinese Medicine
138 Xianlin Road210023NanjingChina</p>
<p>Xiangtai Kong 
Jiacheng Xiong 
Shengkun Ni 
Duanhua Cao 
Buying Niu 
Mingan Chen 
Yameng Li 
ProtonUnfold Technology Co
Ltd, SuzhouChina</p>
<p>Runze Zhang 
Yitian Wang 
Lehan Zhang 
Xutong Li 
Zhaopi Xiong 0000-0002-3323-3092
ProtonUnfold Technology Co
Ltd, SuzhouChina</p>
<p>Qian Shi 
Lingang Laboratory
200031ShanghaiChina</p>
<p>Ziming Huang 
Medizinische Klinik und Poliklinik I
Klinikum der Universität München
Ludwig-Maximilians-Universität
MunichGermany</p>
<p>Zunyun Fu fuzunyun@simm.ac.cn 
Drug Discovery and Design Center
State Key Laboratory of Drug Research
Shanghai Institute of Materia Medica
Chinese Academy of Sciences
555 Zuchongzhi Road201203ShanghaiChina</p>
<p>Mingyue Zheng myzheng@simm.ac.cn </p>
<p>University of Chinese Academy of Sciences
No. 19A Yuquan Road100049BeijingChina</p>
<p>College of Pharmaceutical Sciences
Innovation Institute for Articial Intelligence in Medicine of Zhejiang University
Zhejiang University
310058HangzhouZhejiangChina</p>
<p>School of Physical Science and Technology
ShanghaiTech University
201210ShanghaiChina</p>
<p>Fine-tuning large language models for chemical text mining †
EE258A80EFF6DD33EE851342B48D641310.1039/d4sc00924jReceived 6th February 2024 Accepted 2nd June 2024
Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists.The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature.This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition, reaction role labelling, metal-organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and the conversion of reaction paragraphs to action sequences.The fine-tuned LLMs demonstrated impressive performance, significantly reducing the need for repetitive and extensive prompt engineering experiments.For comparison, we guided ChatGPT (GPT-3.5-turbo)and GPT-4 with prompt engineering and fine-tuned GPT-3.5-turbo as well as other opensource LLMs such as Mistral, Llama3, Llama2, T5, and BART.The results showed that the fine-tuned ChatGPT models excelled in all tasks.They achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data.They even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data.Notably, finetuned Mistral and Llama3 show competitive abilities.Given their versatility, robustness, and low-code capability, leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction.</p>
<p>Introduction</p>
<p>Chemical text mining is a crucial foundation in chemical research.It creates extensive databases that provide access to physicochemical properties and synthetic routes for experimental chemists.Additionally, it accumulates rich data and insights for computational chemists to use for modelling and predicting.More than just extracting information from chemical texts, the rule-based transformation of chemical text is particularly interesting.4][5] This allows them to be understood and executed by robotics for automated syntheses.</p>
<p>However, converting structured data from intricate scientic literature is a challenging task, especially due to the complexity and heterogeneity of chemical language.As a result, a number of text-mining tools have been developed.For instance, Chem-DataExtractor 6,7 was created to extract chemical entities and their associated properties, measurements and relationships from chemical documents, using unsupervised word clustering, conditional random elds, rule-based grammar and dictionary matching.ChemRxnExtractor, 8 a BERT-like model, was designed to extract the product and label associated reaction roles such as the reactant, catalyst, solvent, and temperature from paragraphs of synthesis experiments.Vaucher et al. 1,2 developed task-adaptive pre-trained transformers to convert the synthesis protocol paragraphs into action sequences.Syn-thReader 3 was built to convert literature syntheses to executable XDL formats, containing a series of domain-specic algorithms with predened rules.Historically, the focus has been on designing models and algorithms specic to certain tasks, requiring extensive domain knowledge and sophisticated data processing.These tools, challenging to adapt for diverse extraction tasks, oen require complementary collaboration to manage complex information extraction tasks, thus limiting their versatility and practicality.</p>
<p>Recently, large language models (LLMs), represented by ChatGPT released in November 2022, have shown the potential for Articial General Intelligence (AGI).LLMs, such as GPT-3.5 and GPT-4, can generate logical insights or content that meets requirements based on human instructions.We are entering a new era where AGI and medicinal chemists might work together.0][11] However, LLMs tend to "hallucinate", meaning they generate unintended text that misaligns with established facts and real-world knowledge. 12,13Moreover, objectively evaluating the results of open-ended questions remains a signicant challenge.</p>
<p>At this juncture, LLMs may still nd it difficult to accurately answer factual and knowledge-based questions.However, using LLMs for knowledge extraction tasks should greatly alleviate hallucination and fully leverage their powerful text comprehension and processing capabilities, making them promising universal tools for chemical text mining.For instance, Zheng et al. 14 used prompt engineering to guide ChatGPT in extracting information about metal-organic framework (MOF) synthesis.Patiny et al. 15 tried to use ChatGPT to extract FAIR (Findable, Accessible, Interoperable, Reusable) data from publications.However, their approach of using LLMs simply based on prompt engineering tends to achieve poor performance in exact accuracy.According to the biomedical benchmark study by Chen et al., 16 ChatGPT performed signicantly worse on biomedical text mining compared to existing models.These ndings seem to contradict the common belief in the LLMs' superior comprehension abilities.Either way, LLMs have limitations due to their model architecture and memory, including a maximum length of prompt tokens.Besides, human expressions can be ambiguous, incomplete, vague, and difficult to rene.Outputs may not strictly adhere to formatting requirements, leading to misunderstanding and poor performance in mining complex text, such as patents or scientic literature.Therefore, zero-shot or few-shot prompts are oen insufficient to address the diversity of scenarios and cannot guarantee the quality of extracted data.</p>
<p>In this study, we extensively explored the effectiveness of ne-tuning LLMs on ve challenging tasks in chemical text mining: compound entity recognition, reaction role annotation, metal-organic framework (MOF) synthesis information extraction, nuclear magnetic resonance spectroscopy (NMR) data extraction, and conversion reaction paragraphs into action sequences.We found that ne-tuning GPT models signicantly enhances performance in text mining tasks, compared to prompt-only versions, while also reducing dependency on the repetitive and extensive prompt engineering experiments.Meanwhile, we also evaluated prevalent generative pre-trained language models, such as Mistral, 17 Llama3, 18 Llama2, 19 T5, 20 and BART. 21Among these, ne-tuned ChatGPT (GPT-3.5-turbo)models achieved state-of-the-art (SOTA) performance across all ve tasks.Remarkably, it even outperformed models that have been trained specically for each task and subsequently netuned, based on a signicantly larger amount of in-domain data.This study highlights the potential of ne-tuning LLMs to revolutionize complex knowledge extraction with their versatility, robustness, and low code capability.Fine-tuned LLMs can be easily generalizable and can optimize the laborintensive and time-consuming data collection workow, even with few data.This will accelerate the discovery and creation of novel substances, making them powerful tools for universal use.</p>
<p>Results and discussion</p>
<p>Overview of chemical text mining tasks</p>
<p>Given the complex and diverse information embedded in chemical literature, we designed ve extraction tasks to demonstrate the potential and practicality of LLMs in chemical text mining (Fig. 1).The Paragraph2Compound task is a relatively simple task, aiming to extract all chemical compound entities from the given paragraph.The Paragraph2RXNRole task is to label the reaction roles including the product, reactant, catalyst, temperature, solvent, time, and yield in the paragraph.The Paragraph2MOFInfo task is to extract all MOF synthesis information including the compound name, metal source, metal amount, linker, linker amount, modulator, modulator amount or volume, solvent, solvent volume, reaction temperature and reaction time.The Paragraph2NMR task is designed to extract the IUPAC name, experimental conditions including frequency and solvent as well as chemical shi data for both 1 H NMR and 13 C NMR spectra.The Paragraph2Action task is to convert experimental procedures to structured synthetic steps (action sequences).The details of datasets used for the ve chemical text mining tasks are listed in Table S1.† All tasks are unied to sequence-to-sequence formats to facilitate the use of LLMs.The details about using LLMs with promptengineering and ne-tuning can be found in the Methods section.</p>
<p>Paragraph2Compound-extract all chemical entities.Fig. 2a illustrates the process of random sampling from millions of paragraph-entity pairs, which refer to UPSTO annotations.It starts by randomly selecting 10 000 samples, followed by randomly picking 1000, then 100, and nally 10.This sampling process ensures that each smaller subset is included in the larger one, with each subset used for individual training.Fig. 2b demonstrates the performance of prompt-only models and netuned models, which are evaluated on a consistent evaluation set of 1000 samples across varying training data sizes.These results are obtained from three independent trials.In the case of prompt-only models, randomness is intentionally introduced by altering the prompt and examples (Fig. 2c and S2 †).Given the task's straightforward nature and clear instructions, even the prompt-only language models achieved decent F1 scores over 0.6.For ne-tuned models, the sampling and training process for the training set is repeated three times, as depicted in Fig. 2a.As shown in Fig. 2b, all ne-tuned models demonstrate a performance improvement, especially in terms of the F1 score and Jaccard index, proportional to the increase in dataset size.These models outperform the prompt-only models designed for this task.When the training data size is substantial enough, the F1 scores of the ne-tuned models can reach close to 90%, and the Jaccard index can approach 80%.Notably, ne-tuned LLMs such as GPT-3.5-turboshowed minimal uctuations and superior performance.However, it is essential to emphasize that the cost of ne-tuning GPT-3.5-turboincreased tenfold with each tenfold increase in data volume.Our experimentation was capped at 10 000 training samples for 3 epochs due to OpenAI's limitations, resulting in a nearly 90-dollar expense to ne-tune GPT-3.5-turbo-alow cost-effective investment in computational resources.In contrast, other ne-tuned language models have displayed notable cost advantages in this relatively simple compound name entity recognition task.</p>
<p>Paragraph2RXNRole-extract the product and label the reaction role.According to Guo et al., 8 the Paragraph2RXNRole task comprises two subtasks.The rst is to extract the central product, and the second is to label the associated reaction roles within specied paragraphs (Fig. 3a).For the two tasks, Guo et al. developed two-stage BERT-like token-multi-classication models.To enable a fair comparison with generative language models, we converted the data into sequence-to-sequence formats by adding <Role*Compound*Role> annotations to the input paragraphs.We then converted the language models' outputs back into lists of BIO-tags, followed by post-processing to align with the original BIO-tag labels for assessment.Notably, even when utilizing prompt engineering with 20-shot examples (Fig. S3 and S4 †), GPT-3.5 and GPT-4 perform poorly on two Paragraph2RXNRole tasks, which may result from the complicated syntax cases and limited context length (Fig. 3b and c).However, the ne-tuned GPT models perform well.</p>
<p>For product extraction, the ne-tuned GPT-3.5-turbo(best over one epoch) achieved an F1 score of 77.1%, slightly surpassing the previous SOTA approach, ChemBERT, which scored 76.2% (Fig. 3b).For reaction role labelling, the netuned GPT-3.5-turbo(best over ve epochs) achieved an F1 score of 83.0%, signicantly outperforming the previous SOTA approach, ChemRxnBERT, which scored 78.7% (Fig. 3c).It's notable that the ne-tuned GPT-3.5-turbomodels, which cost only $1 and $5 respectively, demonstrated extremely high costeffectiveness with small training datasets.In contrast, Chem-BERT was domain-adaptive pre-trained on 9 478 043 sentences from 200 000 journal articles, and ChemRxnBERT was further task-adaptive trained on 944 733 reaction-inclusive sentences.We should also mention that the outputs of ne-tuned GPTs, Mistrals and Llamas align almost perfectly with the input text, with over 99% post-processing-free ratios.On the other hand,  S7. † (c) Performance of reaction role labelling.Concrete values can be found in Table S8.† most outputs of ne-tuned T5 and BART require additional alignment due to their tokenization and vocabulary limitations, with a ratio of only 31% that does not require post-processing.Even aer post-processing, the F1 scores of T5 and BART were signicantly lower than those of token-classication BERT-like models or large language models.</p>
<p>Paragraph2MOFInfo-extraction of MOF synthesis information.Our re-annotated dataset for the Paragraph2MOFInfo task displayed in Fig. 4a mostly contains single reaction paragraphs with a few featuring multiple reactions.We used Levenshtein similarity and exact accuracy as metrics to objectively assess the models' ability to extract formatted data that fully comply with the customized requirements in the task.This approach is more objective and accurate with less manual intervention, compared to the manual analysis and evaluation used by Zheng et al. 14 The dataset is divided into a training set and a test set, each containing 329 samples.We evaluated the performance of ne-tuned GPT-3.5-turbo by varying the size of training data from 10 to 329, and observed convergence on the testing set, suggesting saturation in the amount of training data (Fig. 4b).The ne-tuned GPT-3.5-turbosignicantly outperforms the GPT-3.5-turbowith prompt engineering, improving exact match accuracy by over 20% for both single and multiple reactions (Fig. 4c, and S5 †).It also surpasses other ne-tuned models, especially when handling complex multi-reaction paragraphs.</p>
<p>Exact accuracy rates for single and multiple reactions are 82.7% and 68.8%, respectively (Fig. 4c).As depicted in Fig. 4d and e, while most models achieve high Levenshtein similarity across the 11 parameters, only a few maintain high exact accuracy, which is the golden metric that we mainly focus on.</p>
<p>Considering that some MOF synthesis paragraphs may include multiple reactions, we provide an example of multireaction extraction by various models in Fig. 4f.The paragraph includes two reactions, the rst with (R)-H3PIA and bipy as linkers, providing all reaction conditions explicitly, and the second with the substitution of (R)-H3PIA with (S)-H3PIA, keeping all other conditions unchanged.Most models successfully interpreted the semantics and extracted two reactions from the MOF synthesis paragraph.However, only the ne-tuned ChatGPT perfectly extracted information that matched our annotated ground truth.Other models showed varying degrees of incompleteness, particularly with items involving multiple components and their quantities.</p>
<p>Paragraph2NMR-extract NMR chemical shis and conditions.The impact of training set sizes and the use of prompt engineering on the performance of ne-tuning GPT-3.5-turbo in extracting NMR information is illustrated in Fig. 5a.Regardless of the training data size for ne-tuning (ranging from 25 to 300), or the presence of prompt engineering, there are hardly any signicant uctuations in performance.This holds true for metrics such as Levenshtein similarity and exact match accuracy of the ne-tuned GPT-3.5-turbo when the numbers of training samples exceed 50.This demonstrates the strong learning capability and robustness of LLMs.Fig. 5b illustrates the performance of different generative language models using the same 200 training data.In terms of Levenshtein similarity, a metric based on the edit distance, almost all ne-tuned language models achieved impressive scores, outperforming GPT models that solely rely on prompt engineering (Fig. 5b and S6 †).However, when considering the exact match accuracy metric, where each character must perfectly align with the ground truth count, LLMs such as GPTs, Mistral, and Llama3 take the lead.Though ne-tuned T5 and BART manage to extract the majority of the text, they oen miss or mistakenly copy several characters.This contributes to a signicant decrease in their exact match accuracy metric, as shown in Fig. 5c.In this context, the extraction of long complex text by LLMs is more standardized and high-quality, aligning more closely with human expectations.It is worth noting that using carefully designed prompts has almost no impact on the results, which proves that the ne-tuned LLMs are prompt independent.Most importantly, ne-tuning open-source LLMs such as Mistral-7b-instruct-v-0.2 and Llama3-8b-instruct provides an alternative approach for deploying text mining locally, given its exceptionally high exact match accuracy.</p>
<p>Paragraph2Action-action sequence extracted from an experimental procedure.The above-mentioned extraction tasks simply require the model to replicate specic information from the paragraph.However, the Paragraph2Action task requires the model to understand and transform the paragraph and convert experimental procedures to structured synthetic steps (action sequences).Clearly, GPT models with prompt engineering have difficulty with this task, especially when it involves multiple complex conversions and insufficient prompt descriptions (Table 1, Fig. S7 †).To gauge the maximum potential of GPT models using only prompts, we incrementally increased the number of transformation examples from 6 to 60.Despite encompassing all types of actions at least once and nearly reaching the token limit of 4096 for GPT-3.5-turbo and 8192 for GPT-4, their performance in the few-shot scenario remains disappointingly poor.The currently best-performing LLM GPT-4 with 60 examples for in-context learning achieved only 32.7% full sentence exact accuracy, a BLEU score of 65.0, and a Levenshtein similarity of 72.8.However, ne-tuning pretrained language models with a small amount of data could yield decent results (Table 1).Remarkably, ne-tuning LLMs such as Mistral-7b-instruct-v0.2 and GPT-3.5-turbo on 1060 hand annotated training data, we achieved 64.8% and 63.6% full sentence exact accuracy.The ne-tuning process took only 8 minutes (2 epochs) for Mistral on 4 × A100 and 1 hour (4 epochs) for GPT-3.5-turbo.These metrics surpass the SOTA results previously reported by Vaucher et al., 1 which used an ensemble of three models, each task-adaptively pre-trained on 2 million rule-based data and rened on 14 168 augmented data.Interestingly, further improvement was achieved by augmenting the training data size to 14 168 when ne-tuning GPT-3.5turbo.This resulted in 69.0%full sentence exact accuracy, an 86.4 modied BLEU score, and an 89.9% Levenshtein similarity (Table 1).For autonomous robots, it is challenging to generate instructions that follow strict syntax rules.Fine-tuning LLMs plays a crucial role in bridging the gap between fuzzy natural language and structured machine-executable programming languages, signicantly improving the accuracy of customization with a small amount of annotated data.In similar tasks involving "fuzzy rules" or hard-to-dene extraction, ne-tuning LLMs might offer considerable advantages in tailoring the transformation.</p>
<p>Comparison of different methods for chemical text mining</p>
<p>Chemical text mining expedites scientic discovery in chemistry.Previously, tasks involving complex chemical language and sophisticated processing depended on rule-based matching algorithms and custom-built domain-specic models.Now, leveraging universal LLMs' semantic understanding, long context window, and generation abilities offers promising and general approaches.These methods are illustrated in Fig. 6 Undoubtedly, leveraging LLMs with prompt engineering is the most attractive approach because it does not require writing any code or retraining model parameters, only interacting with the large model through natural language instructions.However, relying solely on instructions without any examples (zero-shot) also makes it difficult to standardize the output of LLMs, which is crucial for formatting data extraction tasks.In the case of extracting NMR based solely on instructions (Fig. S8 †), we repeatedly modify the instructions to ensure that the model can generate expected formatting results on a certain paragraph.However, when we used this carefully designed prompt for other paragraphs containing NMR, the extraction  S12 and S13.† (c) Examples of error extractions by T5 and BART, compared with the ground truth.Edge Article Chemical Science results did not meet the qualied formatting requirements again.This zero-shot approach resulted in poor performance across all ve tasks, even using GPT-4.</p>
<p>Apart from instructions, providing few example pairs of paragraph-extraction as context can help LLMs learn the extraction patterns.In these few-shot sceneries (Fig. 2c, S2-S7 †), as shown in Table 1, increasing the number of examples leads LLMs to extract more structured outputs.Ideally, the whole training set should serve as context.However, the upper limit of in-context learning is constrained by the maximum input length due to the memory limitation.The versions of GPT-3.5-Turbo-0613 and GPT-4-0613 we tested were limited to 4096 and 8192 tokens, respectively.Hence, comparing prompt engineering methods in zero-shot and few-shot sceneries to ne-tuned models trained with complete datasets can be somewhat unfair.</p>
<p>To compare the performance of in-context learning and netuning approaches objectively, we should use an equal number of examples for both context and the ne-tuning data set.Here, we tested the latest version of GPT-3.5-turbo-0125, which expands the context length to 16 K and supports ne-tuning.We used a variety of action sequences during sampling to cover as many action types as possible.As the number of examples increased from 30 to 60, 90 and 120, both the performances of in-context learning and ne-tuning are increasing (Table S14 †).Even when the same number of examples was provided for in-context learning as ne-tuning, the ne-tuned model typically outperforms by 10-20% on metrics like exact match accuracy and modied BLEU score.This could be attributed to information loss in in-context learning, while ne-tuning adjusts parameters to learn extraction patterns, thus maintaining higher accuracy.</p>
<p>In the test, we also nd two features of ne-tuning LLMs: rapid performance convergence with small amounts of data and efficient training generalization.For the four tasks utilizing manually annotated data, the LLM's performance rapidly improved and converged with increasing sample sizes (Fig. S11 †).This highlights that hundreds of high-quality data are enough to train an effective extractor, which is typically a manageable workload for manual annotation.Besides, LLMs can be easily adapted for specic text extraction tasks, requiring only a few epochs and a low learning rate for ne-tuning (Table S3 †).However, they are also prone to overtting if trained for an excessive number of epochs.</p>
<p>Promising performance and potential of ne-tuning LLMs on chemical data mining.In this study, we have demonstrated the impressive efficacy, exibility, and high exact accuracy of ne-tuning LLMs, regarding all kinds of text mining tasks as generative problems.An examination of incorrect predictions revealed that only a small proportion were entirely incorrect, while most were acceptable alternatives to the ground truth or even pointed out the incorrect labels (Fig. S12-S16 †).These errors can be attributed to inconsistent annotation standards and the inherent ambiguity of terms with multiple interpretations or functions.Therefore, improving the formatted data extraction requires continuous efforts, including the renement of specic rules and the enrichment of annotations prone to misinterpretation during training and inference.With detailed specications and high-quality formatted data, the ne-tuning method based on LLMs is highly reliable.</p>
<p>Starting with ve chemical extraction tasks, we have proved the effectiveness of ne-tuning LLMs in the relatively small testing sets.This approach, when utilized for large-scale extraction in the future, promises to greatly improve data collection efficiency and accelerate scientic research and experimentation.For the Paragraph2MOFInfo task, we can document the synthesis conditions along with other key information such as MOF structures, pore characteristics, and functional performance.Using these data, we can develop machine learning models to optimize the synthesis novel MOF materials with functions such as new catalysts, gas storage and separation.For the Para-graph2NMR task, we can collect extensive NMR data with the corresponding compound names from millions of synthesis literature documents.This can help create an NMR database for retrieving similar spectra and structures, as well as constructing predictive models to identify molecules structures and analysing complex mixtures, which support drug development and quality control.For the action sequence transformation task, the extracted information is benecial for automatic and robotic synthesis.It will improve reproducibility and minimize human errors, especially in high-throughput experiments.</p>
<p>Apart from the ve mentioned extraction tasks, it can be easily extended to tasks related to extracting information from scientic literature and transforming data into a simple user-friendly reaction format 22 that is both human-and machine-readable.This approach will signicantly contribute to the development of extensive databases like the Open Reaction Database, 23,24 Sci-Finder 25 and Reaxys, 26 which gather comprehensive synthesis data through automated curation and expert verication, to make data more ndable, accessible, interoperable, and reusable (FAIR).</p>
<p>Nevertheless, leveraging ne-tuned LLMs is still insufficient to extract all synthesis information from chemical literature, which contains extensive complex gure and form contents.Recently, some tools have been developed to recognize molecular images 27,28 and reaction diagrams 29,30 from the literature.Integrating LLMs with these image recognition tools or developing advanced large multimodal models (LMMs) may be a promising unied solution for further chemical data mining.Notably, when extracting large amounts of data from copyrighted literature, it's essential to access the necessary permissions from scientic publications.</p>
<p>Herein, we have scratched the surface of the vast potential of LLMs in chemistry and materials science by ne-tuning LLMs for chemical text mining.We may notice that the gap between opensource language models and proprietary GPTs (GPT-3.5-turboand GPT-4) has been narrowing from Llama2 to Llama3 and Mistral.This progress is due to the concerted efforts of researchers and communities in the direction of LLMs.Technically, advancements like more effective ne-tuning strategies, improved open-source model architectures, faster inference approaches, wider context windows, higher quality corpus, and lower computational costs in the era of LLMs are anticipated to further enhance text mining.Meanwhile, it's more essential to consider what else can be achieved with LLMs and how we can develop more effective LLMs for chemistry and materials science.For instance, LLMs have the potential to revolutionize predictive modelling by incorporating the extensive "fuzzy knowledge" encapsulated within scientic literature, especially in chemistry and drug discovery.By combining empirical results with documented knowledge, LLMs could assist chemists identify patterns in experiments that might otherwise be missed, predict properties of compounds and outcomes of reactions, and even generate new chemical hypotheses and theories.Furthermore, the integration of LLMs' comprehension with specialized tools could substantially lower the barrier of chemists to use these tools throughout the entire workow, thanks to interactive interfaces in natural language.Future research could investigate how to merge formatted laboratory data with the wealth of information in scientic literature and develop the multimodal capability to enrich specic domain knowledge for LLMs.This endeavour will require a sustained, long-term effort.</p>
<p>Conclusions</p>
<p>In this work, we have demonstrated the effectiveness of netuning LLMs in chemical text mining.We conducted ve complex tasks: compound entity recognition, reaction role labelling, MOF synthesis information extraction, NMR data extraction, and the transformation of reaction procedures to action sequences.Chemical text mining remains a challenging professional domain when leveraging language model mining, even with prompt engineering.However, LLMs that are netuned with appropriate annotations can produce structured outputs that perfectly full human requirements not easily expressed in natural language.This feature fully utilizes their natural language understanding and formatting capability.Using chemical text mining as an example, this study provides guidance on ne-tuning of LLMs to serve as universal knowledge extraction toolkits.These toolkits can be easily extended for automated extraction from documents and rule-based formatted transformations.Our work lays the groundwork for the applications of LLMs in information extraction within the chemical domain, which will catalyse data-driven innovations in chemical and materials science.</p>
<p>Methods</p>
<p>Dataset preparation</p>
<p>For the Paragraph2Compound task, we compiled an automatically annotated dataset.This dataset is based on the publicly accessed USPTO subset extracted by Lowe et al., 31,32 and includes millions of chemical reaction paragraphs from patents, each paired with compound tags.We used regular expressions to identify compound labels within each paragraph, separating them with the "j" symbol based on their sequential occurrence in the paragraph.For the Paragraph2RXNRole task, we used the manually annotated dataset by Guo et al., 8 following the same data partitioning strategy.We transformed the data from the BIO-token classication format into a sequence-tosequence format using the annotation scheme "<Role*compound*Role>".We processed paragraphs containing multiple central products and related reactions into several input and output pairs.For the Paragraph2MOFInfo task, we manually checked and re-annotated the raw data of Zheng et al., 14 transforming them into a sequence-to-sequence format.This dataset comprises MOF synthesis paragraphs, extraction by ChatGPT, and human-evaluated answers.For the Para-graph2NMR task, we manually curated a dataset of 600 highquality annotations.These were mainly sourced from various literature studies on PubMed to ensure a wide diversity.The task aims to extract information such as the IUPAC name, experimental conditions, including the frequency and solvent, and chemical shi data from both 1 H NMR and 13 C NMR spectra.For the Paragraph2Action task, we utilized the handannotated dataset by Vaucher et al., employing the same data partitioning strategy.This dataset is derived from the Pistachio dataset by NextMove soware. 33The details of datasets used for the ve chemical text mining tasks are listed in Table S1.†</p>
<p>Prompt-only ChatGPT</p>
<p>Prompt-only interaction enables users to efficiently communicate with large language models through simple prompts.This guides the model to produce relevant responses without further training.In a zero-shot scenario, the model generates responses using only a descriptive prompt and its pre-trained knowledge.However, in a few-shot approach, the model uses a small number of examples to improve its understanding and responses.To maximize the performance, we selected diverse examples and ensured a large number of tokens.We interacted with ChatGPT using API keys and employed model versions GPT-3.5-turbo-0613 and GPT-4-0613.The zero-shot and fewshot prompts for chemical text mining tasks can be found in Fig. S2-S8.†</p>
<p>Fine-tuning ChatGPT</p>
<p>Since late August 2023, supervised ne-tuning capabilities have been available for the GPT-3.5-turbomodel. 34The aim is to enhance performance in specic scenarios customized based on private data.In this study, we ne-tuned the GPT-3.5-turbo-0613model for chemical text mining scenarios on ve tasks.When discussing the performance in the Comparison of different methods for chemical text mining section, we netuned the latest GPT-3.5-turbo-0125model for fair comparison, which expanded the context length to 16 K and supported ne-tuning as well.We formatted the data into jsonl and uploaded them to OpenAI's cloud servers, and then initiated ne-tuning jobs.Once the training was complete, the ne-tuned GPT-3.5-turbomodel was ready for inference.API keys were requisite throughout the training and inference procedures.Fine-tuning for the GPT-4-turbo model is not available now and is highly expected in the future.</p>
<p>Fine-tuning open-source language models</p>
<p>We selected the most widely used and representative generative pre-trained language models such as Mistral, 17 Llama3, 18 Llama2, 19 T5, 20 and BART. 21These serve as baselines for a comprehensive comparison with the ne-tuned ChatGPT across ve chemical text mining tasks.Considering performance, efficiency, and hardware resource constraints, we used full parameter ne-tuning for Mistral-7b-instruct-v0.2 and Llama3-8b-instruct on 4 × 40 GB A100, and full parameter netuning for BART-base and T5-base on 1 × 40 GB A100.We applied multitask-learning to BART and T5 in the Para-graph2MOFInfo task and Paragraph2NMR task due to their limitations in generating multi-attribute long sentences (Fig. S9 and S10 †), aiming to enhance their performance.This approach signicantly improved their performance.For Llama2, we used Q-LoRA 35 to efficiently ne-tune llama2-13b-chat on 1 × 40 GB A100.This method maintains most of the performance of full parameter ne-tuning while signicantly reducing computational demands.We used vllm 36 to speed up the inference of LLMs such as Mistral-7b-instruct-v0.2, Llama3-8b-instruct, and Llama2-13b-chat, which is tens of times faster than Hugging Face's pipeline.The inference of all ne-tuned models can run on 1 × 40 GB A100.To ensure optimal performance, we adjusted hyperparameters such as learning rates, lora_r, and lora_alpha during the ne-tuning process of baseline models (Table S2 †).The hardware resources, memory cost, and runtimes of ne-tuning are provided for reference (Table S3 †).More details of training, pre-processing, and post-processing can be found in the ESI.†</p>
<p>Metrics for evaluation</p>
<p>Since ne-tuning ChatGPT does not allow for early stopping based on optimal validation loss, we report the performances of all models at the best epoch selected from the evaluation set for fair comparison.Given the task specics, we use metrics including precision, recall, and F1 score for evaluating entitylevel performance.For sentence-level performance assessment, we use Levenshtein similarity, exact match accuracy, partial accuracy, and a modied BLEU score.</p>
<p>Fig. 1
1
Fig. 1 Schematics of cheminformatics insights to be extracted from paragraphs.And illustration of the five practical tasks in chemical text mining with the respective example outputs, including Paragraph2Compound, Paragraph2RXNRole, Paragraph2MOFInfo, Paragraph2NMR, and Paragraph2Action.</p>
<p>Fig. 2
2
Fig. 2 (a) The workflow of sampling and training based on the USPTO dataset for the Paragraph2Compound task.(b) The performance of different models across varying sizes of the training set.The data point and the shaded areas represent, respectively, the mean values and standard deviations derived from three independent trials.(c) Example of the zero-shot and few-shot prompts utilized.</p>
<p>Fig. 3
3
Fig. 3 (a) Data formats of two subtasks in the Paragraph2RXNRole task.(b) Performance of product extraction.Concrete values can be found in TableS7.† (c) Performance of reaction role labelling.Concrete values can be found in TableS8.†</p>
<p>Fig. 4
4
Fig. 4 (a) A statistic of the Paragraph2MOFInfo dataset.(b) The performance of fine-tuned GPT-3.5-turboacross varying sizes of the training set.(c) Mean performance of Levenshtein similarity and exact match accuracy for extracting paragraphs containing single reactions and multiple reactions, respectively, by different models.Concrete values can be found in Table S9.† (d) Levenshtein similarity for 11 parameters in the Paragraph2MOFInfo task.Concrete values can be found in TableS10.† (e) Exact match accuracy for 11 parameters in the Paragraph2MOFInfo task.Concrete values can be found in TableS11.† (f) An example of extractions by different models from a multi-reaction MOF synthesis paragraph.The cells in yellow represented the ground truth.The cells in green represented the exact match predictions.The cells in blue represented the incorrect predictions.</p>
<p>. In prompt engineering scenarios, LLMs' parameters remain xed, solely relying on the provided examples to extract from new paragraphs.As for the training and ne-tuning process, a model learns the statistic extraction patterns from the training data by adjusting and optimizing the internal parameters.</p>
<p>Fig. 5
5
Fig. 5 (a) The performance of fine-tuned GPT-3.5-turbo with and without prompt engineering as it varies with training data size in the Par-graph2NMR task.(b) Heat map illustrating Levenshtein similarity and exact match accuracy of various models in extracting NMR information.Concrete values can be found in TablesS12 and S13.† (c) Examples of error extractions by T5 and BART, compared with the ground truth.</p>
<p>Fig. 6
6
Fig. 6 Diagram of different approaches for text extraction.</p>
<p>Table 1
1
Performance on the Paragraph2Action task a
Cost905 mean tokens1374 mean tokens1670 mean tokens2598 mean tokens3610 mean tokens861 mean tokens1357 mean tokens1631 mean tokens2546 mean tokens3611 mean tokens7010 mean tokens, $ 41-6 min on 1 × 40 GB A10010 min on 1 × 40 GB A10040 min on 1 × 40 GB A10030 min on 4 × 40 GB A1008 min on 4 × 40 GB A1004 epochs, total 1 h, $ 4-30 min on 1 × 40 GB A100100 min on 1 × 40 GB A1005 hours on 1 × 40 GB A100100 min on 4 × 40 GB A10030 min on 4 × 40 GB A1005 epochs, total 1.5 h, $ 92---Levenshteinsimilarity59.462.364.365.866.054.569.263.065.167.772.845.983.986.886.086.388.788.176.484.887.187.584.887.289.985.786.786.6Modied BLEUscore38.643.144.447.049.544.751.453.856.759.865.022.573.281.880.382.285.984.864.774.484.184.381.484.386.481.584.385.075% acc.34.742.342.645.547.244.951.156.558.261.663.321.977.683.280.783.285.582.762.880.182.484.180.486.486.980.481.882.490% acc.16.819.323.325.926.423.330.733.035.840.043.815.165.971.666.870.273.671.647.768.574.171.667.073.378.167.370.571.3100%acc.8.28.813.114.813.913.420.721.922.726.132.713.151.157.756.859.764.863.637.852.059.762.256.064.269.056.859.460.8StrategyPrompt engineeringwithout ne-tuningPrompt engineeringwithout ne-tuningNo task-adaptivepretraining and ne-tuningon hand-annotated data (1060)No task-adaptive pretraining and ne-tuning on augmenteddata (14 168)ne-Task-adaptive pretraining (2 M) andtuning on hand-annotatedata (1060)ne-Task-adaptive pretraining (2 M) andtuning on augmenteddata (14 168)ModelGPT-3.5-turbo (6-shot)GPT-3.5-turbo (12-shot)GPT-3.5-turbo (18-shot)GPT-3.5-turbo (24-shot)GPT-3.5-turbo (30-shot)GPT-4 (6-shot)GPT-4 (12-shot)GPT-4 (18-shot)GPT-4 (24-shot)GPT-4 (30-shot)GPT-4 (60-shot)Transformer (single model)<em>BART-base (ne-tuned)T5-base (ne-tuned)Lama2-13b-chat (qlora ne-tuned)Lama3-8b-instruct (ne-tuned)Mistral-7b-instruct-v0.2 (ne-tuned)GPT-3.5-turbo (ne-tuned)Transformer (single model)</em>BART-base (ne-tuned)T5-base (ne-tuned)Llama2-13b-chat (qlora ne-tuned)Lama3-8b-instruct (ne-tuned)Mistral-7b-instruct-v0.2 (ne-tuned)GPT-3.5-turbo (ne-tuned)Transformer (single model)<em>Transformer (single model)</em>Transformer (ensemble)<em>
aThe symbol "</em>" represented the results reported by Vaucher et al.The result in black bold is the best performance.The details of ne-tuning cost can be found in TableS3.©2024The Author(s).Published by the Royal Society of Chemistry Chem.Sci., 2024, 15, 10600-10611 | 10607</p>
<p>© 2024 The Author(s). Published by the Royal Society of Chemistry
AcknowledgementsWe thank all contributions of the open-source community on LLMs.We appreciate Yaghi's group for guiding in ChatGPT prompt engineering for chemistry tasks.This work was supported by the National Natural Science Foundation of China (T2225002 and 82273855 to M. Y. Z. and 82204278 to X. T. L.), the National Key Research and Development Program of China (2022YFC3400504 to M. Y. Z.), the SIMM-SHUTCM Traditional Chinese Medicine Innovation Joint Research Program (E2G805H to M. Y. Z.), the Shanghai Post-doctoral Excellence Program (2023693 to Z. Y. F.) and the Shanghai Municipal Science and Technology Major Project.Data availabilityAll data and code of this work are available at GitHub: https:// github.com/zw-SIMM/SFTLLMs_for_ChemText_Mining to allow replication of processing, ne-tuning and evaluation.All concrete values of performance in gures are listed in Section 5 of the ESI.†Author contributionsConflicts of interestThere are no conicts to declare.
Automated extraction of chemical synthesis actions from experimental procedures. A C Vaucher, F Zipoli, J Geluykens, V H Nair, P Schwaller, T Laino, Nat. Commun. 36012020</p>
<p>Language models and protocol standardization guidelines for accelerating synthesis planning in heterogeneous catalysis. M Suvarna, A C Vaucher, S Mitchell, T Laino, J Pérez-Ramírez, Nat. Commun. 79642023</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. S H M Mehr, M Craven, A I Leonov, G Keenan, L Cronin, Science. 3702020</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. S Steiner, J Wolf, S Glatzel, A Andreou, J M Granda, G Keenan, T Hinkley, G Aragon-Camarasa, P J Kitson, D Angelone, Science. 22112019</p>
<p>AI-driven robotic chemist for autonomous synthesis of organic molecules. T Ha, D Lee, Y Kwon, M S Park, S Lee, J Jang, B Choi, H Jeon, J Kim, H Choi, Sci. Adv. 4612023</p>
<p>ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientic literature. M C Swain, J M Cole, J. Chem. Inf. Model. 562016</p>
<p>ChemDataExtractor 2.0: autopopulated ontologies for materials science. J Mavracic, C J Court, T Isazawa, S R Elliott, J M Cole, J. Chem. Inf. Model. 612021</p>
<p>Automated chemical reaction extraction from scientic literature. J Guo, A S Ibanez-Lopez, H Gao, V Quach, C W Coley, K F Jensen, R Barzilay, J. Chem. Inf. Model. 622021</p>
<p>Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. C M Castro Nascimento, A S Pimentel, J. Chem. Inf. Model. 632023</p>
<p>Comparing the Performance of College Chemistry Students with ChatGPT for Calculations Involving Acids and Bases. T M Clark, E Anderson, N M Dickson-Karn, C Soltanirad, N Tani, J. Chem. Educ. 1002023</p>
<p>. T Guo, K Guo, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, 10.48550/arXiv.2108.09926arXiv:2305.18365arXiv, 2023, preprint</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Comput. Surv. 552023</p>
<p>. Y Zhang, Y Li, L Cui, D Cai, L Liu, T Fu, X Huang, E Zhao, Y Zhang, Y Chen, 10.48550/arXiv.2309.01219arXiv:2309.01219arXiv. 2023preprint</p>
<p>ChatGPT Chemistry Assistant for Text Mining and the Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, J. Am. Chem. Soc. 1452023</p>
<p>. L Patiny, G Godin, 10.26434/chemrxiv-2023-05v1b-v2ChemRxiv. 2023</p>
<p>. Q Chen, H Sun, H Liu, Y Jiang, T Ran, X Jin, X Xiao, Z Lin, H Chen, Z Niu, An Extensive Benchmark Study on Biomedical Text Generation and Mining with ChatGPT. 5572023Bioinformatics</p>
<p>. A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, 10.48550/arXiv.2310.06825arXiv:2310.06825arXiv, 2023, preprint</p>
<p>Llama3. April 26, 2024</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, 10.48550/arXiv.2307.09288arXiv:2307.09288arXiv. 2023preprint</p>
<p>Exploring the limits of transfer learning with a unied text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, J. Mach. Learn. Res. 212020</p>
<p>. M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, 10.48550/arXiv.1910.13461arXiv:1910.13462019preprint</p>
<p>. D F Nippa, A T Müller, K Atz, D B Konrad, U Grether, R E Martin, G Schneider, 10.26434/chemrxiv-2023-nfq7h-v2ChemRxiv. 2024preprint</p>
<p>The open reaction database. S M Kearnes, M R Maser, M Wleklinski, A Kast, A G Doyle, S D Dreher, J M Hawkins, K F Jensen, C W Coley, J. Am. Chem. Soc. 1432021</p>
<p>Data sharing in chemistry: lessons learned and a case for mandating structured reaction data. R Mercado, S M Kearnes, C W Coley, J. Chem. Inf. Model. 632023</p>
<p>SciFinder. August 29, 2023</p>
<p>Reaxys. August 29, 2023</p>
<p>aExtractor: a system for automatic extraction of chemical information from biomedical literature. J Xiong, X Liu, Z Li, H Xiao, G Wang, Z Niu, C Fei, F Zhong, G Wang, W Zhang, Sci. China: Life Sci. 672023</p>
<p>MolScribe: Robust Molecular Structure Recognition with Image-to-Graph Generation. Y Qian, J Guo, Z Tu, Z Li, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing. Y Qian, J Guo, Z Tu, C W Coley, R Barzilay, J. Chem. Inf. Model. 632023</p>
<p>ReactionDataExtractor 2.0: a deep learning approach for data extraction from chemical reaction schemes. D M Wilary, J M Cole, J. Chem. Inf. Model. 632023</p>
<p>D Lowe, 10.6084/m9.figshare.5104873.v1Chemical reactions from US patents. 1976-Sep2016. August 29, 2023</p>
<p>. D M Lowe, 2012University of CambridgePhD thesis</p>
<p>A Peng, M Wu, J Allard, L Kilpatrick, S Heidel, GPT-3.5 Turbo ne-tuning and API updates. August 22, 2023</p>
<p>. T Dettmers, A Pagnoni, A Holtzman, L Zettlemoyer, 10.48550/arXiv.2305.14314arXiv:2305.143142023preprint</p>
<p>W Kwon, Z Li, S Zhuang, Y Sheng, L Zheng, C H Yu, J Gonzalez, H Zhang, I Stoica, Proceedings of the 29th Symposium on Operating Systems Principles. the 29th Symposium on Operating Systems PrinciplesKoblenz, Germany2023presented in part at the</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6869 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6869</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6869</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-132.html">extraction-schema-132</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <p><strong>Paper ID:</strong> paper-276647668</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.19794v1.pdf" target="_blank">ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Goal-oriented de novo molecule design, namely generating molecules with specific property or substructure constraints, is a crucial yet challenging task in drug discovery. Existing methods, such as Bayesian optimization and reinforcement learning, often require training multiple property predictors and struggle to incorporate substructure constraints. Inspired by the success of Large Language Models (LLMs) in text generation, we propose ChatMol, a novel approach that leverages LLMs for molecule design across diverse constraint settings. Initially, we crafted a molecule representation compatible with LLMs and validated its efficacy across multiple online LLMs. Afterwards, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction. Finally, to address the limitations of LLMs in numerical recognition, we referred to the position encoding method and incorporated additional encoding for numerical values within the prompt. Experimental results across single-property, substructure-property, and multi-property constrained tasks demonstrate that ChatMol consistently outperforms state-of-the-art baselines, including VAE and RL-based methods. Notably, in multi-objective binding affinity maximization task, ChatMol achieves a significantly lower KD value of 0.25 for the protein target ESR1, while maintaining the highest overall performance, surpassing previous methods by 4.76%. Meanwhile, with numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules increased by up to 0.49. These findings highlight the potential of LLMs as a versatile framework for molecule generation, offering a promising alternative to traditional latent space and RL-based approaches.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6869.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6869.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatMol</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end molecule design framework that fine-tunes LLM backbones (Llama3 1B–8B) to generate molecules under property and substructure constraints using a simplified SELFIES representation, numerical embeddings, and a two-stage training procedure (SFT + sequence calibration with ranking loss).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatMol (fine-tuned Llama3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM (Llama3 backbone) fine-tuned with parameter-efficient LoRA; two-stage training (supervised fine-tuning + sequence-level ranking / sequence calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Llama3 backbones (1B and 8B tested); primary experiments reported with Llama3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Fine-tuned on ZINC250k molecules converted to a simplified SELFIES format; RECAP-fragmented molecules used for substructure training; numeric property labels (logP, docking-derived KD), QED and SA computed via cheminformatics tools and docking (AutoDock-GPU) supplied during training.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct conditional generation from textual prompts (property values and/or substructure sequences) in simplified SELFIES; two-stage fine-tuning (SFT using cross-entropy loss, then sequence calibration using ranking loss over multiple sampled candidates); numerical embeddings added to token embeddings for numbers; sampling to produce candidate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Simplified SELFIES (square brackets removed, tokens space-separated to be closer to natural language while retaining SELFIES validity guarantees)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug discovery: single-objective logP targeting, substructure-constrained logP extremization, and multi-objective binding affinity maximization (targets: ESR1 and ACAA1).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property windows (e.g., logP in (−2.5, −2)), fixed substructures (must contain a fragment), multi-property thresholds (e.g., high binding affinity / low KD, QED>0.4, SA<5.5), length penalty during ranking (β=2.0).</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>RDKit for property computation and substructure containment checks; AutoDock-GPU to compute docking scores / estimated KD; RECAP for fragment generation; LoRA for efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC250k (~250k drug-like purchasable molecules). Task-specific training sizes: single-objective logP targeting 636 samples; substructure-property constrained logP extremization 250k samples; multi-objective binding affinity maximization: 1,987 (ESR1) and 828 (ACAA1) samples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Validity, uniqueness, novelty; success rate (fraction of generated molecules meeting a target property window); diversity (1 − average pairwise Tanimoto similarity of Morgan fingerprints); Pearson correlation and RMSE between instructed numeric property values and properties of generated molecules; task-specific metrics KD (↓), QED (↑), SA (↓), and an overall score defined as log(QED × KD × SA) / or log(QED / KD × SA) as presented (paper nomenclature ambiguous but used consistently for ranking).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Validity ≈ 99–100%; single-objective logP targeting: best ChatMol success rate 94.5% (diversity ≈ 0.703); substructure-property extremization: expanded logP ranges (example S1: min −6.87, max 8.09; Pearson ρ up to 0.93); multi-objective binding affinity maximization: best KD achieved for ESR1 = 0.25 (unitless docking-derived estimate), ChatMol ranked highest by overall metric and outperformed previous methods by ~4.76%; numerical enhancement increased Pearson correlation between instructed and generated properties by up to ~0.49 and reduced KD RMSE from 3.184 to 0.684 (~78.5% reduction). Reported top-2 metric values for 100k generated molecules per protein target shown in Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>LLMs intrinsically weak at following numerical instructions without special encoding; existing representations (SMILES lacks robustness; original SELFIES tokenization causes long sequences) are suboptimal for LLMs; numerical embeddings were trained for ranges seen in the tasks and may not generalize to arbitrary numeric ranges; multi-property task training sets are relatively small which constrains performance; no reported wet-lab synthesis/biological validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6869.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama3 (foundation model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only foundation LLM family used as the backbone for ChatMol; Llama3 models of different sizes (1B–8B) were fine-tuned via LoRA for conditional molecule generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only LLM backbone; fine-tuned (parameter-efficient LoRA) for molecule generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Variants tested between 1B and 8B parameters; experiments emphasize Llama3 8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Fine-tuning with supervised fine-tuning and sequence calibration; conditional generation from text prompts (properties/substructures) producing simplified SELFIES strings.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Simplified SELFIES (input to the model during fine-tuning and generation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Drug molecule generation tasks (logP targeting, substructure + property generation, multi-property binding affinity optimization).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Same as ChatMol when used as backbone — property windows, substructure constraints, multi-property thresholds; length normalization for probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td>Used in conjunction with RDKit and AutoDock-GPU for scoring during training and evaluation; LoRA for parameter-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC250k converted to simplified SELFIES; task-specific splits as reported for ChatMol.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same task metrics as ChatMol (validity, success rate, diversity, Pearson correlation, RMSE, KD/QED/SA metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Larger-parameter Llama3 variants achieved higher success rates and better alignment with numeric prompts (numerical enhancement effects reported primarily on Llama3 8B). Numerical enhancement increased Pearson correlation by up to 0.49 in their ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Base Llama3 models weak at numeric instruction following without added numerical embeddings; need for careful tokenization of chemical representations; scaling expected to improve performance per scaling laws but not evaluated beyond 8B in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6869.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolReGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolReGPT (referenced LLM approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach using in-context few-shot learning with large language models (e.g., ChatGPT) to perform molecule-related tasks (molecule-caption translation and text-based molecule generation) without domain-specific pretraining or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolReGPT (in-context LLM approach)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-only / in-context few-shot usage of general-purpose LLMs (no fine-tuning required)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context few-shot prompting to translate between molecules and natural language and to generate molecules from text examples.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Not detailed in this paper; referenced as part of related work (likely SMILES/SELFIES in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation and text-based molecule generation.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Cited as demonstrating that LLMs can be used via in-context learning without domain-specific fine-tuning; the present paper's preliminary experiments found that SOTA online LLMs and few-shot ICL approaches struggle with incomplete molecules and with numeric constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Limited robustness for drug-discovery-style conditional generation, poor handling of complex substructure constraints and numeric instructions; representation tokenization conflicts (SELFIES long sequences) and SMILES brittleness were highlighted as obstacles when using in-context LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6869.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (as tested)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (online LLM variant tested in ICL experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial online LLM evaluated in in-context learning (ICL) experiments for single-property molecule generation using different molecule representations; showed improved success with the paper's simplified SELFIES but still limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>online general-purpose decoder-only LLM used in in-context few-shot experiments (prompt-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context few-shot prompting (3 examples per inference) to generate molecules conditioned on property targets.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Tested with SMILES, SELFIES, and the paper's simplified SELFIES representation.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Single-objective logP targeting (proof-of-concept ICL tests).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property window for logP in (−2.5, −2) in ICL experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Evaluation used ZINC250k examples for ICL prompts (212 ICL inferences reported overall across models).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate (molecules meeting logP target), inference time per query.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>With simplified SELFIES, GPT-4o achieved higher success than with SMILES/SELFIES; Table 3 reports 'ours' representation success for GPT-4o ≈ 11.63% on the single-property logP ICL task (compared to lower rates with other representations). Exact timing and some table values are provided but not uniformly comparable across different online LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>ICL performance limited — online LLMs struggled to replicate incomplete molecules; numeric instruction following remains weak without numerical encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6869.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ERNIE-Lite-8K (as tested)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ERNIE-Lite-8K-0922 (online LLM variant tested in ICL experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online language model variant tested in the paper's in-context learning comparisons; benefited from the simplified SELFIES representation but showed low absolute success on the single-property logP generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ERNIE-Lite-8K-0922</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>online LLM used in in-context few-shot experiments (prompt-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context few-shot prompting (3 examples per inference) to produce molecules conditioned on properties.</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>SMILES, SELFIES, and simplified SELFIES were compared; simplified SELFIES produced shorter token sequences and better ICL performance.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Single-objective logP targeting (ICL evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>logP window constraint used in prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC250k examples used in ICL prompts/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate, inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Reported very low success rates for ERNIE in Table 3 (e.g., SMILES and SELFIES both 0.47% in that table; simplified SELFIES increased success but remained low compared to other LLMs). Exact numbers in table are noisy but indicate simplified SELFIES improved ERNIE's performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Low ICL success overall; tokenization and representation length issues for SELFIES; weak numerical instruction following without numerical enhancement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6869.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>moonshot-v1-8k / KIMI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>moonshot-v1-8k (referred to as KIMI in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online LLM variant (referred to as KIMI) tested via ICL for molecule generation; capable of generating molecules but showed weak correlation with target properties and failed on complex/long substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>moonshot-v1-8k (KIMI)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>online LLM used with in-context few-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>In-context few-shot prompting (ICL) for conditional molecule generation (no fine-tuning reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Tested with SMILES, SELFIES, and simplified SELFIES; simplified SELFIES increased tokenization efficiency and ICL success.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Single-property and substructure-property molecule generation tasks (ICL evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td>Property windows and substructure constraints provided in the prompt for ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>ZINC250k examples used for prompts/evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Success rate, Pearson correlation between prompted and generated properties, inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Table 3 shows moonshot's 'ours' representation success up to ~38.68% on the single-property logP ICL task (substantially higher than some other online LLMs); however, in substructure extremization KIMI results had weak correlation (ρ≈0.01 reported for one test) and KIMI failed on complex long substructures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>ICL-generated molecules showed weak adherence to target properties in more complex tasks; fails on complex/long molecular structures; numeric instruction following remains a problem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6869.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6869.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to synthesize novel chemical compounds for specific applications, including model details, generation methods, target applications, chemical representations, evaluation metrics, constraints, integration with external tools, datasets, reported performance, experimental validation, and reported challenges or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MolT / MolT5 (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MolT5 (molecule–text transformer variant referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced molecule-focused language model (MolT/MolT5 family) used in related work on molecule↔text translation; included in comparison tables for representation/tokenization experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MolT / MolT5</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>molecule–text transformer (related work), used for molecule-caption translation experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Pretrained transformer models for molecule↔text translation; can be used with tokenized molecule strings (SMILES/SELFIES).</td>
                        </tr>
                        <tr>
                            <td><strong>chemical_representation</strong></td>
                            <td>Reported comparisons used SMILES, SELFIES and the simplified SELFIES described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_application</strong></td>
                            <td>Molecule-caption translation and related molecule generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>constraints_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>integration_with_external_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_used</strong></td>
                            <td>Referenced work evaluated on molecule datasets (e.g., ZINC / MOSES) but not specifically re-trained in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>ICL success rate and inference time in the representation comparison experiments (Table 3 references MolT performance).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_results</strong></td>
                            <td>Included in Table 3 comparisons: MolT had differing tokenization times and success rates; the paper reports that the simplified SELFIES reduced average token count by ~50% vs. SELFIES and improved inference efficiency across tested LLMs including MolT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Original SELFIES tokenization uses square brackets that increase token count and conflict with LLM tokenizers; SMILES can be brittle; these representation issues limit MolT family effectiveness unless representation is adapted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Limo: Latent inceptionism for targeted molecule generation <em>(Rating: 2)</em></li>
                <li>Translation between molecules and natural language <em>(Rating: 2)</em></li>
                <li>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective <em>(Rating: 2)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6869",
    "paper_id": "paper-276647668",
    "extraction_schema_id": "extraction-schema-132",
    "extracted_data": [
        {
            "name_short": "ChatMol",
            "name_full": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
            "brief_description": "An end-to-end molecule design framework that fine-tunes LLM backbones (Llama3 1B–8B) to generate molecules under property and substructure constraints using a simplified SELFIES representation, numerical embeddings, and a two-stage training procedure (SFT + sequence calibration with ranking loss).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatMol (fine-tuned Llama3)",
            "model_type": "decoder-only LLM (Llama3 backbone) fine-tuned with parameter-efficient LoRA; two-stage training (supervised fine-tuning + sequence-level ranking / sequence calibration)",
            "model_size": "Llama3 backbones (1B and 8B tested); primary experiments reported with Llama3 8B",
            "training_data_description": "Fine-tuned on ZINC250k molecules converted to a simplified SELFIES format; RECAP-fragmented molecules used for substructure training; numeric property labels (logP, docking-derived KD), QED and SA computed via cheminformatics tools and docking (AutoDock-GPU) supplied during training.",
            "generation_method": "Direct conditional generation from textual prompts (property values and/or substructure sequences) in simplified SELFIES; two-stage fine-tuning (SFT using cross-entropy loss, then sequence calibration using ranking loss over multiple sampled candidates); numerical embeddings added to token embeddings for numbers; sampling to produce candidate molecules.",
            "chemical_representation": "Simplified SELFIES (square brackets removed, tokens space-separated to be closer to natural language while retaining SELFIES validity guarantees)",
            "target_application": "Drug discovery: single-objective logP targeting, substructure-constrained logP extremization, and multi-objective binding affinity maximization (targets: ESR1 and ACAA1).",
            "constraints_used": "Property windows (e.g., logP in (−2.5, −2)), fixed substructures (must contain a fragment), multi-property thresholds (e.g., high binding affinity / low KD, QED&gt;0.4, SA&lt;5.5), length penalty during ranking (β=2.0).",
            "integration_with_external_tools": "RDKit for property computation and substructure containment checks; AutoDock-GPU to compute docking scores / estimated KD; RECAP for fragment generation; LoRA for efficient fine-tuning.",
            "dataset_used": "ZINC250k (~250k drug-like purchasable molecules). Task-specific training sizes: single-objective logP targeting 636 samples; substructure-property constrained logP extremization 250k samples; multi-objective binding affinity maximization: 1,987 (ESR1) and 828 (ACAA1) samples.",
            "evaluation_metrics": "Validity, uniqueness, novelty; success rate (fraction of generated molecules meeting a target property window); diversity (1 − average pairwise Tanimoto similarity of Morgan fingerprints); Pearson correlation and RMSE between instructed numeric property values and properties of generated molecules; task-specific metrics KD (↓), QED (↑), SA (↓), and an overall score defined as log(QED × KD × SA) / or log(QED / KD × SA) as presented (paper nomenclature ambiguous but used consistently for ranking).",
            "reported_results": "Validity ≈ 99–100%; single-objective logP targeting: best ChatMol success rate 94.5% (diversity ≈ 0.703); substructure-property extremization: expanded logP ranges (example S1: min −6.87, max 8.09; Pearson ρ up to 0.93); multi-objective binding affinity maximization: best KD achieved for ESR1 = 0.25 (unitless docking-derived estimate), ChatMol ranked highest by overall metric and outperformed previous methods by ~4.76%; numerical enhancement increased Pearson correlation between instructed and generated properties by up to ~0.49 and reduced KD RMSE from 3.184 to 0.684 (~78.5% reduction). Reported top-2 metric values for 100k generated molecules per protein target shown in Table 6.",
            "experimental_validation": false,
            "challenges_or_limitations": "LLMs intrinsically weak at following numerical instructions without special encoding; existing representations (SMILES lacks robustness; original SELFIES tokenization causes long sequences) are suboptimal for LLMs; numerical embeddings were trained for ranges seen in the tasks and may not generalize to arbitrary numeric ranges; multi-property task training sets are relatively small which constrains performance; no reported wet-lab synthesis/biological validation.",
            "uuid": "e6869.0",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3",
            "name_full": "Llama3 (foundation model)",
            "brief_description": "A decoder-only foundation LLM family used as the backbone for ChatMol; Llama3 models of different sizes (1B–8B) were fine-tuned via LoRA for conditional molecule generation tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama3",
            "model_type": "decoder-only LLM backbone; fine-tuned (parameter-efficient LoRA) for molecule generation",
            "model_size": "Variants tested between 1B and 8B parameters; experiments emphasize Llama3 8B",
            "training_data_description": null,
            "generation_method": "Fine-tuning with supervised fine-tuning and sequence calibration; conditional generation from text prompts (properties/substructures) producing simplified SELFIES strings.",
            "chemical_representation": "Simplified SELFIES (input to the model during fine-tuning and generation)",
            "target_application": "Drug molecule generation tasks (logP targeting, substructure + property generation, multi-property binding affinity optimization).",
            "constraints_used": "Same as ChatMol when used as backbone — property windows, substructure constraints, multi-property thresholds; length normalization for probabilities.",
            "integration_with_external_tools": "Used in conjunction with RDKit and AutoDock-GPU for scoring during training and evaluation; LoRA for parameter-efficient fine-tuning.",
            "dataset_used": "ZINC250k converted to simplified SELFIES; task-specific splits as reported for ChatMol.",
            "evaluation_metrics": "Same task metrics as ChatMol (validity, success rate, diversity, Pearson correlation, RMSE, KD/QED/SA metrics).",
            "reported_results": "Larger-parameter Llama3 variants achieved higher success rates and better alignment with numeric prompts (numerical enhancement effects reported primarily on Llama3 8B). Numerical enhancement increased Pearson correlation by up to 0.49 in their ablations.",
            "experimental_validation": false,
            "challenges_or_limitations": "Base Llama3 models weak at numeric instruction following without added numerical embeddings; need for careful tokenization of chemical representations; scaling expected to improve performance per scaling laws but not evaluated beyond 8B in this work.",
            "uuid": "e6869.1",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MolReGPT",
            "name_full": "MolReGPT (referenced LLM approach)",
            "brief_description": "A referenced approach using in-context few-shot learning with large language models (e.g., ChatGPT) to perform molecule-related tasks (molecule-caption translation and text-based molecule generation) without domain-specific pretraining or fine-tuning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolReGPT (in-context LLM approach)",
            "model_type": "prompt-only / in-context few-shot usage of general-purpose LLMs (no fine-tuning required)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "In-context few-shot prompting to translate between molecules and natural language and to generate molecules from text examples.",
            "chemical_representation": "Not detailed in this paper; referenced as part of related work (likely SMILES/SELFIES in original work).",
            "target_application": "Molecule-caption translation and text-based molecule generation.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": null,
            "evaluation_metrics": null,
            "reported_results": "Cited as demonstrating that LLMs can be used via in-context learning without domain-specific fine-tuning; the present paper's preliminary experiments found that SOTA online LLMs and few-shot ICL approaches struggle with incomplete molecules and with numeric constraints.",
            "experimental_validation": null,
            "challenges_or_limitations": "Limited robustness for drug-discovery-style conditional generation, poor handling of complex substructure constraints and numeric instructions; representation tokenization conflicts (SELFIES long sequences) and SMILES brittleness were highlighted as obstacles when using in-context LLMs.",
            "uuid": "e6869.2",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-4o (as tested)",
            "name_full": "GPT-4o (online LLM variant tested in ICL experiments)",
            "brief_description": "A commercial online LLM evaluated in in-context learning (ICL) experiments for single-property molecule generation using different molecule representations; showed improved success with the paper's simplified SELFIES but still limited.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4o",
            "model_type": "online general-purpose decoder-only LLM used in in-context few-shot experiments (prompt-only)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "In-context few-shot prompting (3 examples per inference) to generate molecules conditioned on property targets.",
            "chemical_representation": "Tested with SMILES, SELFIES, and the paper's simplified SELFIES representation.",
            "target_application": "Single-objective logP targeting (proof-of-concept ICL tests).",
            "constraints_used": "Property window for logP in (−2.5, −2) in ICL experiments.",
            "integration_with_external_tools": null,
            "dataset_used": "Evaluation used ZINC250k examples for ICL prompts (212 ICL inferences reported overall across models).",
            "evaluation_metrics": "Success rate (molecules meeting logP target), inference time per query.",
            "reported_results": "With simplified SELFIES, GPT-4o achieved higher success than with SMILES/SELFIES; Table 3 reports 'ours' representation success for GPT-4o ≈ 11.63% on the single-property logP ICL task (compared to lower rates with other representations). Exact timing and some table values are provided but not uniformly comparable across different online LLMs.",
            "experimental_validation": false,
            "challenges_or_limitations": "ICL performance limited — online LLMs struggled to replicate incomplete molecules; numeric instruction following remains weak without numerical encoding.",
            "uuid": "e6869.3",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ERNIE-Lite-8K (as tested)",
            "name_full": "ERNIE-Lite-8K-0922 (online LLM variant tested in ICL experiments)",
            "brief_description": "An online language model variant tested in the paper's in-context learning comparisons; benefited from the simplified SELFIES representation but showed low absolute success on the single-property logP generation task.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ERNIE-Lite-8K-0922",
            "model_type": "online LLM used in in-context few-shot experiments (prompt-only)",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "In-context few-shot prompting (3 examples per inference) to produce molecules conditioned on properties.",
            "chemical_representation": "SMILES, SELFIES, and simplified SELFIES were compared; simplified SELFIES produced shorter token sequences and better ICL performance.",
            "target_application": "Single-objective logP targeting (ICL evaluation).",
            "constraints_used": "logP window constraint used in prompts.",
            "integration_with_external_tools": null,
            "dataset_used": "ZINC250k examples used in ICL prompts/evaluation.",
            "evaluation_metrics": "Success rate, inference time.",
            "reported_results": "Reported very low success rates for ERNIE in Table 3 (e.g., SMILES and SELFIES both 0.47% in that table; simplified SELFIES increased success but remained low compared to other LLMs). Exact numbers in table are noisy but indicate simplified SELFIES improved ERNIE's performance.",
            "experimental_validation": false,
            "challenges_or_limitations": "Low ICL success overall; tokenization and representation length issues for SELFIES; weak numerical instruction following without numerical enhancement.",
            "uuid": "e6869.4",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "moonshot-v1-8k / KIMI",
            "name_full": "moonshot-v1-8k (referred to as KIMI in experiments)",
            "brief_description": "An online LLM variant (referred to as KIMI) tested via ICL for molecule generation; capable of generating molecules but showed weak correlation with target properties and failed on complex/long substructures.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "moonshot-v1-8k (KIMI)",
            "model_type": "online LLM used with in-context few-shot prompting",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "In-context few-shot prompting (ICL) for conditional molecule generation (no fine-tuning reported in this paper).",
            "chemical_representation": "Tested with SMILES, SELFIES, and simplified SELFIES; simplified SELFIES increased tokenization efficiency and ICL success.",
            "target_application": "Single-property and substructure-property molecule generation tasks (ICL evaluation).",
            "constraints_used": "Property windows and substructure constraints provided in the prompt for ICL.",
            "integration_with_external_tools": null,
            "dataset_used": "ZINC250k examples used for prompts/evaluation.",
            "evaluation_metrics": "Success rate, Pearson correlation between prompted and generated properties, inference time.",
            "reported_results": "Table 3 shows moonshot's 'ours' representation success up to ~38.68% on the single-property logP ICL task (substantially higher than some other online LLMs); however, in substructure extremization KIMI results had weak correlation (ρ≈0.01 reported for one test) and KIMI failed on complex long substructures.",
            "experimental_validation": false,
            "challenges_or_limitations": "ICL-generated molecules showed weak adherence to target properties in more complex tasks; fails on complex/long molecular structures; numeric instruction following remains a problem.",
            "uuid": "e6869.5",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "MolT / MolT5 (referenced)",
            "name_full": "MolT5 (molecule–text transformer variant referenced)",
            "brief_description": "A referenced molecule-focused language model (MolT/MolT5 family) used in related work on molecule↔text translation; included in comparison tables for representation/tokenization experiments.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "MolT / MolT5",
            "model_type": "molecule–text transformer (related work), used for molecule-caption translation experiments",
            "model_size": null,
            "training_data_description": null,
            "generation_method": "Pretrained transformer models for molecule↔text translation; can be used with tokenized molecule strings (SMILES/SELFIES).",
            "chemical_representation": "Reported comparisons used SMILES, SELFIES and the simplified SELFIES described in this paper.",
            "target_application": "Molecule-caption translation and related molecule generation tasks.",
            "constraints_used": null,
            "integration_with_external_tools": null,
            "dataset_used": "Referenced work evaluated on molecule datasets (e.g., ZINC / MOSES) but not specifically re-trained in this paper.",
            "evaluation_metrics": "ICL success rate and inference time in the representation comparison experiments (Table 3 references MolT performance).",
            "reported_results": "Included in Table 3 comparisons: MolT had differing tokenization times and success rates; the paper reports that the simplified SELFIES reduced average token count by ~50% vs. SELFIES and improved inference efficiency across tested LLMs including MolT variants.",
            "experimental_validation": false,
            "challenges_or_limitations": "Original SELFIES tokenization uses square brackets that increase token count and conflict with LLM tokenizers; SMILES can be brittle; these representation issues limit MolT family effectiveness unless representation is adapted.",
            "uuid": "e6869.6",
            "source_info": {
                "paper_title": "ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Limo: Latent inceptionism for targeted molecule generation",
            "rating": 2,
            "sanitized_title": "limo_latent_inceptionism_for_targeted_molecule_generation"
        },
        {
            "paper_title": "Translation between molecules and natural language",
            "rating": 2,
            "sanitized_title": "translation_between_molecules_and_natural_language"
        },
        {
            "paper_title": "Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective",
            "rating": 2,
            "sanitized_title": "empowering_molecule_discovery_for_moleculecaption_translation_with_large_language_models_a_chatgpt_perspective"
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 1,
            "sanitized_title": "junction_tree_variational_autoencoder_for_molecular_graph_generation"
        }
    ],
    "cost": 0.016985999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model
27 Feb 2025</p>
<p>Chuanliu Fan 
Ziqiang Cao 
Zicheng Ma 
Nan Yu 
Yimin Peng 
Jun Zhang 
Yiqin Gao 
Guohong Fu 
ChatMol: A Versatile Molecule Designer Based on the Numerically Enhanced Large Language Model
27 Feb 20250028DAB30E93EF52D092720DAA258F19arXiv:2502.19794v1[cs.CE]
Goal-oriented de novo molecule design, namely generating molecules with specific property or substructure constraints, is a crucial yet challenging task in drug discovery.Existing methods, such as Bayesian optimization and reinforcement learning, often require training multiple property predictors and struggle to incorporate substructure constraints.Inspired by the success of Large Language Models (LLMs) in text generation, we propose ChatMol, a novel approach that leverages LLMs for molecule design across diverse constraint settings.Initially, we crafted a molecule representation compatible with LLMs and validated its efficacy across multiple online LLMs.Afterwards, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction.Finally, to address the limitations of LLMs in numerical recognition, we referred to the position encoding method and incorporated additional encoding for numerical values within the prompt.Experimental results across singleproperty, substructure-property, and multi-property constrained tasks demonstrate that ChatMol consistently outperforms state-of-the-art baselines, including VAE and RL-based methods.Notably, in multi-objective binding affinity maximization task, ChatMol achieves a significantly lower K D value of 0.25 for the protein target ESR1, while maintaining the highest overall performance, surpassing previous methods by 4.76%.Meanwhile, with numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules increased by up to 0.49.These findings highlight the potential of LLMs as a versatile framework for molecule generation, offering a promising alternative to traditional latent space and RL-based approaches. 1</p>
<p>Introduction</p>
<p>Exploring the chemical space of small molecules to discover new drugs and materials is a pivotal issue in pharmacology and AI-assisted science research [Sanchez-Lengeling and Aspuru-Guzik, 2018].The primary challenge in the field of pharmacology is goal-oriented de novo drug molecule design [Du et al., 2022], namely generating novel and diverse drug molecules with specific biochemical properties.Typical examples involve designing compounds based on target features y, which can represent a single property, multiple properties, or a combination of substructure and property characteristics, such as optimizing molecules for high binding affinity to a designated protein target [Segler et al., 2018].</p>
<p>In light of this challenge, various AI-based methods have been developed with frameworks of variational autoencoders (VAEs) [Kingma and Welling, 2013], sequence-based language models (LMs) [Rumelhart et al., 1986;Hochreiter and Schmidhuber, 1997], and generative adversarial networks (GANs) [Goodfellow et al., 2014].Meanwhile, various optimization methods are used to generate molecules towards desirable properties, including Bayesian optimization [Eckmann et al., 2022;Kong et al., 2023], reinforcement learning [You et al., 2018;Luo et al., 2021], and genetic algorithms [Jensen, 2019].However, most existing approaches rely on training additional property predictors to incorporate conditions indirectly [Eckmann et al., 2022] and struggle with conditional generation involving specific substructures [Jin et al., 2018;You et al., 2018;Kong et al., 2023].Considering the inherent difficulty in accurately representing the combination of multiple property predictors [You et al., 2018], coupled with the high sensitivity of molecule properties to even minor structural changes [Kipf and Welling, 2016], it is crucial to develop a solution that effectively tackles both challenges simultaneously.</p>
<p>Large Language Models (LLMs) have been successfully applied to molecule-caption translation tasks [Edwards et al., 2022;Pei et al., 2023;Li et al., 2024], including molecule understanding and text-based molecule generation.For example, MolReGPT uses in-context few-shot learning to leverage the generalization capabilities of LLMs like ChatGPT [Ope-nAI et al., 2024] without requiring domain-specific pretraining or fine-tuning.However, our preliminary experiments revealed that even SOTA online LLMs struggled to replicate incomplete molecules in drug discovery tasks, as shown in Table 3.This highlights two key issues.First, existing molecule representations are suboptimal for LLMs.SMILES lacks sufficient robustness, while SELFIES generates excessively long and complicated sequences, making both unsuitable for efficient model processing.Second, LLMs currently lack the ability to fully understand the complex conditions required in drug design, especially when dealing with numerical constraints in drug design tasks.These limitations hinder the application of LLMs in drug discovery.</p>
<p>Given the problems above, we propose ChatMol, a versatile molecule designer that optimizes an LLM for de novo molecule design.Firstly, we simplified SELFIES to make the molecule representation more concise and closer to natural language while preserving its inherent high validity.Experiments conducted on three online LLMs revealed that the proposed molecule representation offers significant advantages in inference efficiency and achieves a high success rate according to Table 3.Secondly, we developed specific prompts geared towards diverse constrained molecule generation tasks to further fine-tune current LLMs while integrating feedback learning derived from property prediction.Finally, inspired by the position encoding method [Vaswani, 2017], we applied a unified numerical encoding to numbers within the prompt, enhancing the model's ability to follow the numerical instructions.</p>
<p>We tested LLM backbones of varying sizes, ranging from 1B to 8B parameters [Dubey et al., 2024], to evaluate the scalability of our approach.Experimental results across single-property, substructure-property, and multi-property constrained tasks verfied the effectiveness of ChatMol.In the multi-objective binding affinity maximization task, Chat-Mol achieves a significantly lower K D value of 0.25, while maintaining the highest overall performance, surpassing previous methods by 4.76%.Furthermore, our ablation study demonstrated that numerical enhancement significantly improves the ability of the model to follow numerical instruc-tions, increasing the Pearson correlation coefficient by up to 0.49.Notably, by setting prompts according to the distribution of training set properties, our approach also achieved promising results in the goal-free de novo generation task, as detailed in Appendix A.</p>
<p>Our contributions are as follows:</p>
<p>• We proposed an end-to-end versatile molecule design framework using LLMs, capable of performing goaloriented de novo molecule generation tasks under various biochemical constraints.</p>
<p>• We proposed a simplified molecule representation particularly tailored for LLMs.</p>
<p>• We designed a numerical encoding method for the numbers in the conditions, which improves the alignment between the instructed property values and those of the generated drug molecules.</p>
<p>2 Related Work</p>
<p>Molecule Generation</p>
<p>One important task in molecular design is goal-oriented de novo molecule generation [Zunger, 2018], which aims to design novel compounds with specific biochemical properties.</p>
<p>Our approach falls within this category, focusing on the generation of molecules under predefined constraints through a non-iterative process.In contrast, methods like LIMO [Eckmann et al., 2022] adopt an iterative approach to achieve similar goals.Another key task is goal-oriented molecule optimization, exemplified by methods such as MolMIM [Reidenbach et al., 2022], which iteratively refines molecules starting from a reference compounds.MOLGEN [Fang et al., 2023], on the other hand, is a non-iterative approach that also relies on a reference molecule as the starting point for optimization.Finally, unconditional de novo molecule generation [Flam-Shepherd et al., 2022;Jin et al., 2018;Liu et al., 2019] involves the generation of novel molecules without specific property constraints, where the model generates molecules that conform to the statistical distribution of the training data.A detailed comparison of these generative tasks is presented in Table 1.</p>
<p>Molecule Representations</p>
<p>From  et al., 2022].However, we focus on 1D string-based representation that uses simplified text encodings to describe the structure of a chemical species, which is highly interpretable compared to 3D coordinates.Sequence representation also offers computational efficiency and facilitates the exploration of longer and more diverse molecules [Fang et al., 2023].</p>
<p>Method</p>
<p>ChatMol generates molecules end-to-end by taking in a text description of desired properties and substructures and producing molecules in a string-based representation.To achieve this goal, we first designed a molecule representation tailored for LLMs, focusing on robustness and simplicity.Then, we improved numerical representation by adding numerical embeddings to the token embeddings for different numbers.The model was trained using a two-stage process.</p>
<p>Molecule Representation</p>
<p>SMILES and SELFIES are the most commonly used stringbased representations of a molecule [Weininger, 1988;Krenn et al., 2020].SMILES possesses the advantage of conciseness, while it lacks a mechanism to ensure the validity of molecule strings in terms of syntax and physical principles.Conversely, the square brackets in SELFIES representation, without expanding the vocabulary of LLM, often result in more tokens and conflicts during tokenization.To ensure both conciseness and chemical validity, we propose a simplified SELFIES representation by removing the square brackets and using spaces to separate each element, thus achieving a format closer to natural language for molecule representation.As illustrated in Table 2, the tokenization results for Llama3 8B on the ZINC250k [Irwin et al., 2012] and MOSES [Polykovskiy et al., 2020] dataset indicate that our approach produces an average token count half that of SELFIES representation.Consequently, we improved the molecule representation without expanding the vocabulary, significantly lowering the barrier to use.Furthermore, We tested three online LLMs using different types of molecule representations on the single-property task, i.e., generating molecules with logP targeted within the range of (−2.5, −2).For each inference, we provided three examples, conducting a total of 212 In-Context Learning inferences.Notably, the inference times across different LLMs are not directly comparable due to potential differences in inference configurations and network speeds.However, within each model, the comparison of inference times for the three representations highlights the effectiveness of our proposed method.</p>
<p>As illustrated in Table 3, our representation demonstrated the highest effectiveness in terms of success rates.Moreover, it was observed that the proposed representation consistently exhibited less inference times compared to SELFIES, and even surpassed SMILES on ERNIE.These ICL inference results, obtained without fine-tuning, further confirmed that the proposed simplified SELFIES representation is more suitable for Large Language Models, both in terms of effectiveness and efficiency.</p>
<p>Numerical Enhancement</p>
<p>We perform numerical enhancement for each number that appears in the input conditions.Inspired by the absolute positional encoding method [Vaswani, 2017], we use the following formula to obtain the numerical embedding, which is then added to the corresponding part of the text embedding.As shown in Equation ( 1), assume that the number num is tokenized into three tokens, and the dimension of the embedding hidden layer is d hidden size .Then we have:
N (num, 2i) = sin (num/10000 2i/d hidden size ) N (num, 2i + 1) = cos (num/10000 2i/d hidden size )(1)
Here, N represents the Numerical Embedding, num is the numerical value, and i ranges from 0 to 1/2 × d hidden size .The odd dimensions of the hidden layer use sine encoding, while the even dimensions use cosine encoding.</p>
<p>After deriving the numerical embedding based on the number, we add it to the embeddings of all tokens corresponding to that number in the condition prompt.Suppose T num is the set of numerical tokens, then the updated embedding for each token j ∈ T num is given by:
E j token = E j token + N , ∀j ∈ T num(2)
Where E j token denotes the token embedding of the number num.Since we have replaced the position with the number in the conventional positional encoding, all the tokens of the number will share the same numerical embedding, thereby enhancing the numerical information as a whole.</p>
<p>Two-Stage Training</p>
<p>The two-stage training process is illustrated in Appendix B. The goal of neural molecule design is to create a function g parameterized by a causal language model with parameter θ that takes a condition C consisting of property or substructure constraints and generates an appropriate molecule M described above.Denote {C, M * } is a specific training sample, tokens of the reference molecule M * is {t * 1 , ..., t * l } with a length l after tokenization.The training objective is to minimize the sum of the negative log-likelihoods associated with these tokens.The cross-entropy loss can be expressed as:
L ce = − l j=1 t p true t | C, M * &lt;j log p g θ t | C, M * &lt;j ; θ (3)
where t is the possible next token from the vocabulary, and
M * &lt;j is the partial reference token sequence {t * 1 , ..., t * j−1 }. While p g θ t | C, M * &lt;j ;
θ is the probability the model g θ generates the corresponding token t.We sum all the probability of possible token t.And Following [Liu et al., 2022], we require ChatMol to possess the capability to precisely predict the ranking order of a set of the sampled candidate molecules M, derived from the model trained in the supervised fine-tuning stage.The fundamental principle of sequence calibration involves generating multiple candidate results M ∈ M initially, followed by ranking these candidates using a custom discriminator S. Af- ter we rank the candidate molecules based on the S( M ), we can fine-tune the model from the SFT stage with a ranking loss [Hopkins and May, 2011;Zhong et al., 2020] as follows:
p true t | C, M * &lt;j = 1 if t = t * j , and p true t | C, M * <j = 0 if t ̸ = t * j ,L rank = i j>i max 0, f θ ( Mj ) − f θ ( Mi )(4)
where Mi and Mj are two candidate molecules from M and satisfy ∀i, j, i &lt; j, S( Mi ) &gt; S( Mj ).f θ ( M ) denotes the estimated log-probability of the candidate molecule M provided by ChatMol with parameter θ.To implement 4, we first generate six candidate molecules for each condition input of the training sample considering the constraints of cost, and then score them.</p>
<p>The quality of candidate molecules can be assessed through the measurement scorer S, which comprises the property scorer S p and the structure scorer S s .ChatMol is capable of handling both property and substructure constraints, as well as their combinations.Taking single property and substructure as an example, when given the target property y * , we first calculate the property of each candidate molecule M ∈ M using RDKit [Landrum and others, 2006] or AutoDock-GPU [Santos-Martins et al., 2021], denoted by f α ( M ).</p>
<p>The property score can be defined as
S p ( M ) = 1 − |y * −f α ( M )|/max(y * , f α ( M )).
Given the desired substructure M * f rag , the structure score S s ( M ) = 1 if the candidate molecule contains the substructure, and S s ( M ) = 0 otherwise.We use RDKit to determine whether candidate molecules contain the target substructure.Then we calculate the weighted scores S( M ) = w p • S p ( M ) + w s • S s ( M ).</p>
<p>To mitigate the influence of molecule length on the score f θ ( M ), we normalize the log-probability based on the molecule's length, following the approach proposed by [Cho et al., 2014]:
f θ ( M ) = l j=1 log p g θ tj | Ĉ, M&lt;j ; θ | M | β (5)
where Ĉ is the condition of the six candidate molecules and t is the next token of the partial candidate molecule M&lt;j .The length penalty hyperparameter β is set to 2.0 following [Liu et al., 2022].</p>
<p>The model fine-tuned solely with ranking loss may no longer be used as a generative model [Liu et al., 2022], so we merged the sequence-level ranking loss with token-level cross-entropy loss to preserve the generation ability of Chat-Mol:
L total = γ 1 L ce + γ 2 L rank (6)
following the setting in [Cho et al., 2014], we set γ 1 = 0.1 and γ 2 = 10.0 due to the difference in their magnitudes.</p>
<p>Experiments</p>
<p>Dataset</p>
<p>For all molecule generation tasks, we use ZINC250k as our dataset, which contains ≈ 250, 000 purchasable, drug-like molecules.We utilized AutoDock-GPU to generate docking scores for a SMILES string against two protein targets: the human estrogen receptor (ESR1) and human peroxisomal acetyl-CoA acyltransferase 1 (ACAA1).We employed the RECAP (Retrosynthetic Combinatorial Analysis Procedure) algorithm [Lewell et al., 1998] to fragment molecules in the ZINC250k dataset for substructure-related training, which is a computational technique designed to electronically fragment molecules based on chemical knowledge.Molecules lacking leaf nodes were excluded.Finally, we convert the SMILES sequences into the simplified SELFIES format as described in the molecule representation subsection.The size of the training sets for each task is as follows:</p>
<p>• Single-objective logP targeting: 636 samples.</p>
<p>• Substructure-property constrained logP extremization: 250k samples.</p>
<p>• Multi-objective binding affinity maximization: 1,987 samples (ESR1) and 828 samples (ACAA1).</p>
<p>Tasks and Metrics</p>
<p>In the single-objective logP targeting task, we aimed to generate molecules with logP in the range (−2.5, −2), associated with favorable pharmacokinetics [Eckmann et al., 2022].We defined two metrics: success rate (proportion of molecules within the desired logP range) and diversity (one minus the average pairwise Tanimoto similarity of Morgan fingerprints [Rogers and Hahn, 2010]).</p>
<p>In the substructure-property constrained logP extremization task, the goal is to generate molecules containing a specified substructure with extreme logP properties.The evaluation metrics verify whether the generated molecules contain the specified substructures and meet the logP value requirements.</p>
<p>The multi-objective binding affinity maximization task focuses on optimizing multiple drug properties, including targeting, pharmacokinetics, and synthesis feasibility.We defined an overall metric as log( QED K D ×SA ), integrating QED (higher scores for better drug suitability), K D (lower values for better binding affinity [Santos-Martins et al., 2021]), and the SA score (lower values for easier synthesis [Bickerton et al., 2012;Ertl and Schuffenhauer, 2009]).</p>
<p>Experimental Setup</p>
<p>The prompts for the three tasks are set as shown in Figure 1.We determine the prompts based on the range of properties required for each task.The prompts contain numerous numbers, which motivates the proposal of numerical enhancement.</p>
<p>The metrics for downstream tasks of drug discovery primarily focus on the maximum or minimum value of a specific property or require the property to fall within a certain range, which cannot fully reflect the model's adherence to prompts.We will discuss the impact of the numerical enhancement in the last subsection.</p>
<p>Baselines</p>
<p>We compare with the following works for conditional molecule design, VAE-based Bayesian optimization method JT-VAE [Jin et al., 2018], and LIMO [Eckmann et al., 2022], reinforcement learning-based method GCPN [You et al., 2018], and LSTM-based Bayesian optimization method SGDS [Kong et al., 2023].LIMO is the only approach capable of handling both properties and substructures.</p>
<p>Single-Objective logP Targeting</p>
<p>In this task, our prompt includes a logP value randomly sampled between −2.5 and −2 with two decimal places.As shown in Table 4, ChatMol outperforms other methods significantly in terms of success rate, achieving an impressive 94.5%, though it exhibits a slightly lower diversity.The success rates of models based on both types of backbones have improved after sequence calibration, with models utilizing larger parameters holding a greater advantage.</p>
<p>Substructure-Property Constrained logP Extremization</p>
<p>In this task, our inference prompt includes a substructure sequence and a property value.Following [Eckmann et al., 2022], we selected the same two starting molecules with the specified substructures identified by them.We sampled the property values around the extreme logP values of the training set distribution, which is far away from the logP values of the two starting molecules.Figure 3 illustrates the logP extremization results, the starting logP values are 1.81 and 5.05 respectively.As shown in Table 5, compared to LIMO, ChatMol expanded the range of logP values, while maintaining the substructures of the two starting molecules fixed.Although KIMI (moonshot-v1-8k) can generate some molecules through ICL, the properties of the generated molecules show a significant weak correlation with the target properties.Moreover, KIMI fails when encountering complex long molecular structures like substructure 2.</p>
<p>We also calculated the Pearson correlation coefficient ρ between the logP values set in the prompt and the actual logP values of the generated molecules within the whole inference batch.As shown in Table 5, the strong correlations indicate that our model effectively adheres to the desired property values in this task.</p>
<p>Although our model was trained on fragments from the RECAP algorithm and had not encountered these two specific substructures before, it is still capable of generating molecules containing these novel substructures.This highlights our model's strong generalization ability and confirms the effectiveness of incorporating structural information through our training approach.</p>
<p>Multi-Objective Binding Affinity Maximization</p>
<p>In this task, our inference prompt includes three property values, each sampled near the extreme values in their respective training set distributions, depending on the directions of the target properties.Following [Eckmann et al., 2022], we target the binding sites of two human proteins: estrogen receptor (PDB ESR1, UniProt P03372) and peroxisomal acetyl-CoA acyl transferase 1 (PDB ACAA1, UniProt P09110) and generate 100k molecules for each protein.Our objective is to generate molecules that exhibit high binding affinity (low K D values), a QED score greater than 0.4, and a SA score below 5.5.We generate 100k molecules for each protein target and present the top-2 scores for K D , QED, and SA.Table 6 illustrates that ChatMol demonstrates the best overall performance metrics across the two ligand generation tasks for the target proteins, achieving the lowest K D values for both ESR1 and ACAA1.Our ranking is based on the overall metric, so it is reasonable that the enhancement in overall performance may involve some compromise in individual metrics like SA score.</p>
<p>Analysis of Numerical Enhancement</p>
<p>Due to the nature of the above tasks in drug design, the evaluation metrics either focus on whether a certain molecule property falls within a specific range or on identifying the molecules with the maximum or minimum values of certain properties, which all center on individual molecules and are not related to specific numbers in the prompts.</p>
<p>These metrics fail to reflect how well the model adheres to the property values specified in the prompt across an entire batch of inferences.Therefore, we introduced the analysis experiments of numerical enhancement, using Pearson correlation coefficients and RMSE to assess the consistency between the target properties and the actual properties of the generated molecules.
Ligand ESR1 ACAA1 KD(↓) QED (↑) SA (↓) overall (↑) KD(↓) QED (↑) SA (↓) overall (↑)
To eliminate interference from other modules, we conducted SFT training based on the Llama3 8B model solely on the last two tasks, which had more complex prompts and involved more numerical data, with and without numerical enhancement.As shown in Table 7, the logP pertains to the substructure-property constrained logP extremization task, and K D , QED, and SA, are derived from the multiproperty constrained task.The results with numerical enhancement are on the right, while the results without it are on the left.</p>
<p>The RMSE of the K D value in the multi-property constrained task decreased from 3.184 to 0.684, representing a reduction of approximately 78.5%.Correspondingly, the Pearson correlation coefficient improved from -0.103 to 0.378, changing from a negative to a positive correlation.Although the effect of numerical enhancement for logP was less pronounced, it was significantly more evident in the multi-property setting, demonstrating that when a prompt contains a large number of numerical values (in this case, up to three values), the proposed numerical enhancement method significantly improves the model's compliance with numerical instructions.</p>
<p>Metric</p>
<p>Conclusion</p>
<p>We present ChatMol, a versatile molecule designer that leverages LLMs for de novo molecule design.Our approach first represented molecules as simplified SELFIES sequences, offering a natural language-like format that ensures chemical validity and conciseness at a low cost.Then, We fine-tuned Llama3 with property prediction feedback to align the model distribution with desired properties and substructures.Finally, to address the issue of weak numerical recognition in LLMs, we implemented numerical encoding for arbitrary numbers within the condition context.Experimental results demonstrate that our method surpasses all other baseline approaches across all three tasks.After numerical enhancement, the Pearson correlation coefficient between the instructed property values and those of the generated molecules showed a significant increase, reaching up to 0.49.This demonstrates that Large Language Models can excel in molecule design tasks in a more direct, versatile, and practical manner compared to latent space generative models or RL-based methods.</p>
<p>We conducted preliminary exploration utilizing the Llama3 8B model.In accordance with the scaling law [Kaplan et al., 2020], we are highly confident that scaling up the model parameters will lead to significant breakthroughs.</p>
<p>A Additional Experiments</p>
<p>A.1 Basic Metrics</p>
<p>We present the evaluation metrics for the generated molecules in comparison to each training set across three tasks: validity, uniqueness and novelty.Validity refers to the percentage of generated molecules that are chemically valid.Uniqueness denotes the percentage of generated molecules that are distinct and not duplicated.Novelty is defined as the percentage of valid molecules that do not appear in the training set.As shown in Table 8, our method achieves nearly 100% validity and novelty.Since structural constraints require the generated molecules to contain specific substructures, there is a higher repetition rate when calculating pairwise Tanimoto similarity.This explicit restriction results in the molecule generation task with substructure constraints performing slightly lower in terms of diversity.</p>
<p>A.2 Goal-Free Generation of Molecules</p>
<p>We also conducted additional unconditional generation experiments on the MOSES benchmark.The results from the random generation of 30,000 molecules are summarized in Table 9.We assume that the property distribution in the training set follows a normal distribution.By sampling property values from this distribution, we construct inputs to perform the random generation task.The high uniqueness and validity provides the foundation for ChatMol's ability to generate a diverse set of novel molecules with desirable properties.</p>
<p>B Implementation Details</p>
<p>All generation experiments are conducted on eight GeForce RTX 3090 GPUs, and we run the AutoDock-GPU on eight Tesla V100 with 32G memory size.We use the parameterefficient fine-tuning method of LoRA to fine-tune the Llama pre-trained models.The total learnable parameters are approximately 21 million, which constitutes about 0.26% of all parameters for Llama3 8B foundation model.We employed the bfloat16 format for efficiency.During the supervised finetuning stage, ChatMol is trained over 60 epochs with a learning rate of 2×10 −5 using AdamW optimizer with 0.05 weight decay, and the same applies to the sequence calibration stage.We generate 6 candidates for each training example considering the computational constraints and the two-stage training process was followed as illustrated in Figure 4.</p>
<p>We generate 100k molecules for single-property logP targeting task, 10k for substructure-property constrained logP extremization task, and 100k molecules for each protein in multi-objective binding affinity maximization task.</p>
<p>C Visualization of Generated Molecules</p>
<p>A subset of generated molecules from the single-property logP targeting task is visualized in Figure 5. Molecules generated from the substructure-property constrained task are shown in Figure 6 and Figure 7, while those from the multiproperty constrained task are presented in Figure 8</p>
<p>D Limitations</p>
<p>The numerical values we referred to are those appearing in three downstream tasks, and are not arbitrary.Their range is limited to the vicinity of the values observed within the tasks.These numerical embeddings are confined to specific ranges relevant to our tasks.Since the current values are concentrated at the edges of the molecule property distribution in the training set, which represents a relatively fixed range, our model is minimally affected by the cyclic nature of the position encoding.</p>
<p>RDKit is used as an example scorer, but our framework is indeed flexible and can be extended to incorporate other types of scoring mechanisms.As long as the scoring system can effectively assess the quality of the generated molecules, it can be used within our framework.We believe this flexibility allows the model to handle a variety of challenging properties, and we are confident that properties can be addressed by selecting appropriate features for ranking the candidate molecules.</p>
<p>We acknowledge that the training set for multi-property tasks is relatively small.While the model improves compliance, the limited size of relevant training data (particularly in the ZINC250k dataset [Irwin et al., 2012]) constrains the model's performance.Nevertheless, we have successfully pushed the boundaries of performance for this task.</p>
<p>Model</p>
<p>Valid(↑) Unique@1k(↑) Unique@10k(↑) IntDiv(↑) Novelty(↑)</p>
<p>Figure 1 :
1
Figure 1: Examples of diverse conditional generation tasks in drug design addressed by ChatMol: single-objective logP targeting, substructure-constrained logP optimization, and multi-objective binding affinity maximization.</p>
<p>Figure 2 :
2
Figure 2: Illustration of numerical enhancement in the training process.Property values are transformed through N (•) to obtain a holistic numerical encoding, which is then added to each numerical token's word embedding to produce the final encoding of the constraint conditions.E(•) represents the embedding layer.</p>
<p>where t * j is the reference next token of the partial reference molecule M * &lt;j given condition C.</p>
<p>Figure 3 :
3
Figure 3: Extremization of logP property with substructure (highlighted in red) fixed.The logP value within the red box represents our target value, and together with the substructure, it constitutes the prompt for this task.</p>
<p>and Figure 9.The specified substructures are highlighted in red.</p>
<p>Figure 4 :
4
Figure 4: Training stages of the ChatMol.The yellow squares below represent tokens used for calculating loss, and the white squares below refer to the condition tokens.During the sequence calibration stage, each training example corresponds to N candidate molecules.</p>
<p>Figure 5 :
5
Figure 5: Molecules generated in the single-objective logP targeting task.</p>
<p>Figure 6 :
6
Figure 6: Molecules generated in the substructure-property constrained logP extremization task for substructure1.The specified substructures are highlighted in red.</p>
<p>Figure 7 :
7
Figure 7: Molecules generated in the substructure-property constrained logP extremization task for substructure2.The specified substructures are highlighted in red.</p>
<p>Figure 8 :
8
Figure 8: Molecules generated in the multi-property constrained binding affinity maximization task for ESR1.The legends denote KD(↓), QED (↑), SA (↓).</p>
<p>Figure 9 :
9
Figure 9: Molecules generated in the multi-property constrained binding affinity maximization task for ACAA1.The legends denote KD(↓), QED (↑), SA (↓).</p>
<p>Table 1 :
1Generative TaskReference MoleculeModelOptimization MethodGoal-oriented de novo generationw/o w/oLIMO OursIterative Non-IterativeGoal-oriented optimizationw/ w/MolMIM MOLGENIterative Non-IterativeGoal-free de novo generationw/oSF-RNN-
Comparison of different generative tasks in molecule design."w/" indicates that the input includes a reference molecule as the starting point, while "w/o" means that no reference molecule is provided in the input.Goal-free generation involves no constraints on molecule properties or substructures, whereas goal-oriented generation incorporates specific constraints.</p>
<p>Table 2 :
2
Comparison of average tokenization lengths for different molecular representations on the ZINC250k and MOSES datasets.</p>
<p>Table 3 :
3MetricModelSMILES SELFIESOursERNIE0.47%0.47%3.77%moonshot11.32%33.96%38.68%Success(%)GPT-4o7.81%7.69%11.63%MolT53.93%--MolReGPT1.00%--ERNIE418.18584.63396.52moonshot538.394080.422280.56Time(s)GPT-4o243.21385.21348.77MolT51212.12--MolReGPT702.93--
[Qin et al., 2024]022]ference performance of MolT5[Edwards et al., 2022], MolReGPT[Li et al., 2024]and online LLMs (ERNIE-Lite-8K-0922[Sun et al., 2020], moonshot-v1-8k[Qin et al., 2024], andGPT-4o [OpenAI et al., 2024]) with different molecule representations on the task of single-objective logP targeting.</p>
<p>Table 4 :
4
Results for single-objective property targeting to −2.5 &lt;
MethodSuccess (%) DiversityZINC0.40.919JT-VAE11.30.846GCPN85.50.392LIMO10.40.914SGDS86.00.874ChatMol (1B)57.80.704ChatMol  †81.40.701ChatMol94.50.703
logP &lt; −2.0.ZINC is the distribution of the ZINC250k dataset and results for JT-VAE, GCPN, LIMO, and SGDS are taken from [Kong et al., 2023].† denotes without feedback training.</p>
<p>Table 5 :
5
Comparison of logP extremization and Pearson correlation coefficient ρ under the substructure-property constraints, S1 and S2 denote Substructure1 and Substructure2 specified by LIMO [Eckmann et al., 2022].† denotes without feedback training.
MethodS1 Min S1 Max S1 ρ S2 Min S2 Max S2 ρLIMO-1.485.29-1.576.89-KIMI-2.3911.14 0.01---ChatMol (1B) -4.357.62 0.83 0.956.46 0.20ChatMol  †-5.137.74 0.85 1.046.28 0.63ChatMol-6.878.09 0.93 -1.188.33 0.78</p>
<p>Table 6 :
6
[Eckmann et al., 2022;Kong et al., 2023]otein targets ESR1 and ACAA1 in the multi-property constrained generation task.Arrow (↑) indicates higher scores are preferable, and (↓) indicates lower scores are desired.We present the Top-2 scores for KD, QED, and SA.The best results are highlighted in bold, while the second-best results are highlighted with underline.1 st denotes the top-ranked molecule and 2 nd represents the second-best molecule from the 100k generated molecules.Define overall as log( QED K D ×SA ).Baseline results are obtained from[Eckmann et al., 2022;Kong et al., 2023].
Tamoxifen870.452.014.77----Raloxifene7.9 × 10 60.322.42.83----GCPN 1 st8100.434.211.7585000.694.29.87GCPN 2 nd2.7 × 10 40.803.78.9985000.544.39.60LIMO 1 st4.60.434.816.78280.575.515.12LIMO 2 nd2.80.644.917.66310.444.914.88SGDS 1 st0.360.443.9919.544.550.564.0717.22SGDS 2 nd1.280.443.8618.305.670.604.5816.96ChatMol (1B) 1 st0.690.613.8519.25160.623.8816.09ChatMol (1B) 2 nd1.400.643.4218.71170.693.8316.16ChatMol 1 st0.250.532.7420.474.480.784.2417.53ChatMol 2 nd0.360.714.1719.972.790.575.2117.48</p>
<p>Table 8 :
8
Validity, uniqueness, and novelty of the 10,000 generated molecules compared to the training sets for each task.Valid: the percentage of molecules that are chemically valid.Unique: percentage of generated molecules that are distinct.Novel: percentage of valid generated molecules not present in training set.Time denotes the inference time per molecule on each device.
TasksValid Unique Novel Time(s/mol)Single-Property1.000.700.990.06Structure-Property1.000.621.000.33Multi-Properties0.990.761.000.18</p>
<p>Table 9 :
9
[Eckmann et al., 2022]020]f molecules trained on MOSES benchmark.The results are calculated with the MOSES platform[Polykovskiy et al., 2020].Valid: percentage of molecules that are chemically valid.Unique@1k: percentage of 1,000 generated molecules that are unique.Unique@10k: percentage of 10,000 generated molecules that are unique.IntDiv: one minus average pairwise similarity between molecules.Novelty: percentage of valid generated molecules not present in training set.Results are taken from[Eckmann et al., 2022].
JT-VAE1.001.000.99960.8550.9143GRAPHDF1.001.000.99720.8871.00LIMO1.000.9980.97560.9071.00Ours1.001.000.99700.8950.9813
The code, data, and pre-trained model will be publicly available in the final version.</p>
<p>Equivariant energy-guided sde for inverse molecular design. Bao , The eleventh international conference on learning representations. 2022. 2022</p>
<p>Quantifying the chemical beauty of drugs. Bickerton, Nature chemistry. 422012. 2012</p>
<p>Molgensurvey: A systematic survey in machine learning models for molecule design. Cho, arXiv:1409.1259arXiv:2203.145002014. 2014. 2022arXiv preprintOn the properties of neural machine translation</p>
<p>Limo: Latent inceptionism for targeted molecule generation. Dubey, Proceedings of machine learning research. machine learning research2024. 2024. 2022. 20221625777The llama 3 herd of models</p>
<p>Edwards, arXiv:2204.11817Translation between molecules and natural language. 2022. 2022arXiv preprint</p>
<p>Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. Schuffenhauer Ertl, Journal of cheminformatics. 12009. 2009</p>
<p>Domainagnostic molecular generation with self-feedback. Fang, arXiv:2301.112592023. 2023arXiv preprint</p>
<p>Inverse design of 3d molecular structures with conditional generative neural networks. Flam-Shepherd, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingMark Hopkins and Jonathan May2022. 2022. 2022. 2022. 2014. 2014. 1997. 2022. 2022. May, 2011. 2011. 2012. 2019. 201913Chemical science</p>
<p>Junction tree variational autoencoder for molecular graph generation. Jin , International conference on machine learning. PMLR2018. 2018</p>
<p>Kaplan, arXiv:2001.08361Scaling laws for neural language models. 2020. 2020arXiv preprint</p>
<p>Molecule design by latent space energybased modeling and gradual distribution shifting. Welling ; Kingma, P Diederik, Max Kingma, ; Welling, N Thomas, Max Kipf, ; Welling, Kong, arXiv:1312.6114arXiv:1609.02907Uncertainty in Artificial Intelligence. PMLR2013. 2013. 2016. 2016. 2023. 2023arXiv preprintAuto-encoding variational bayes</p>
<p>AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. Krenn, Machine Learning: Science and Technology. 14450242020. 2020</p>
<p>Recap retrosynthetic combinatorial analysis procedure: a powerful new technique for identifying privileged molecular fragments with useful applications in combinatorial chemistry. Greg Landrum, Landrum, Journal of chemical information and computer sciences. 3832006. 2006. 1998. 1998Xiao Qing LewellRdkit: Open-source cheminformatics</p>
<p>Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective. Li, arXiv:2203.16804Advances in neural information processing systems. PMLR2024. 2024. 2019. 2019. 2022. 2022. 2021. 2021. 2021. 2024323156arXiv preprintSandhini. Gpt-4 technical report</p>
<p>Alan Aspuru-Guzik, and Alex Zhavoronkov. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. Pei , arXiv:2310.07276Frontiers in Pharmacology. 2023. 2023. 2020. 2020arXiv preprintBiot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations</p>
<p>Improving small molecule generation using mutual information machine. Qin, arXiv:2407.00079arXiv:2208.09016Benjamin Sanchez-Lengeling and Alán Aspuru-Guzik. Inverse molecular design using machine learning. Diogo Santos-Martins2024. 2024. 2022. 2010. 2010. 1986. 2018. 2018. 202150arXiv preprintScience</p>
<p>Generating focused molecule libraries for drug discovery with recurrent neural networks. Leonardo Solis-Vasquez, Andreas F Tillack, Michel F Sanner, Andreas Koch, Stefano Forli, ; Seff, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021. 2019. 2019. 2018. 2018. 202017Ernie 2.0: A continual pre-training framework for language understanding</p>
<p>Ashish Vaswani. Attention is all you need. Vaswani, arXiv:1706.037622017. 2017arXiv preprint</p>
<p>Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. David Weininger, ; Weininger, You, arXiv:2004.08795Xipeng Qiu, and Xuanjing Huang. Extractive summarization as text matching. 1988. 1988. 2018. 2018. 2020. 202028arXiv preprintAdvances in neural information processing systems</p>
<p>Inverse design in search of materials with target functionalities. Alex Zunger, Zunger, Nature Reviews Chemistry. 241212018. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
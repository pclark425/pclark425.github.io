<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8650 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8650</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8650</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-262064851</p>
                <p><strong>Paper Title:</strong> <a href="https://ojs.aaai.org/index.php/AAAI/article/download/27751/27545" target="_blank">Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds. However, for LLMs to focus on specific drug targets typically require experimentation with progressively more refined prompts. Results thus become dependent not just on what is known about the target, but also on what is known about the prompt-engineering. In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form, and a simple text-based query. We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the the logical component automatically, keeping the query unchanged. We describe an iterative procedure LMLF (“Language Models with Logical Feedback”) in which the constraints are progressively refined using a logical notion of generalisation. On any iteration, newly generated instances are verified against the constraint, providing “logical-feedback” for the next iteration’s refinement of the constraints. We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM). We show that LMLF, starting with the same logical constraints and query text, can guide both LLMs to generate potential leads. We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors. These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8650.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8650.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LMLF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Models with Logical Feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative neuro-symbolic procedure that conditions an LLM's sampling by (a) providing formal, machine-checkable domain constraints and background knowledge, (b) sampling molecules via a fixed natural-language query, and (c) using constraint-based labelling plus numeric constraint generalisation as logical feedback to refine constraints and repeat sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM (GPT-3 / PaLM) with Logical Feedback (LMLF)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>prompt-based transformer LLMs combined with an iterative symbolic constraint loop (neuro-symbolic)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not trained in this work; method uses pretrained foundation LLMs (GPT-3, PaLM) as black-box generative engines. Background datasets used for conditioning and evaluation include ChEMBL-derived labelled JAK2 and DRD2 molecule sets (≈4k molecules each).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo lead generation for protein targets (JAK2 and DRD2)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt-based generation: assemble a prompt composed from background knowledge (labelled examples and computed properties) plus formal logical constraints (encoded outside the LLM) and a simple natural-language query (e.g. 'Generate a valid SMILES...'); sample SMILES from the LLM; verify each sample against constraints (molecular validity, MW, logP, SAS, docking affinity) and add labelled results to background; then generalise numeric constraints (tighten/loosen thresholds iteratively) and repeat sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Qualitative novelty assessed: many generated molecules were not present in known databases per the query; among molecules identified as containing target-selective functional groups, 10 of 11 had Tanimoto similarity < 0.75 to existing inhibitors, indicating structural novelty for those candidates. Quantitative novelty fraction: chemist assessment found 15% (JAK2) and 40% (DRD2) of generated molecules had known target-selective functional groups (some overlap with novel scaffolds).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced via logical constraints including target-agnostic drug-likeness filters (MW 200–700, LogP < 5, SAS < 5) and target-specific binding-affinity thresholds (docking score thresholds: ≥7 for first 5 iterations, ≥8 for next 5). Generated molecules were evaluated by docking to the target (GNINA) to verify target-specific binding.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Computed molecular validity (RDKit), molecular weight, LogP, synthetic accessibility score (SAS), docking-based binding affinity estimates (GNINA docking scores), distributional comparison via Mann-Whitney U test for binding affinities, Tanimoto similarity for novelty, expert chemist substructure matches for selective functional groups.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Across both targets and both LLMs, adding logical feedback improved distributions of estimated binding affinity: (1) No-constraint < target-agnostic constraints < target-agnostic + target-specific constraints (LMLF++). Example statistics (JAK2): GPTLF ++ mean 7.74, median 7.71 (docking score units used in paper). PaLMLF ++ also improved vs baselines but to a lesser degree. Expert chemists identified 11 molecules as potentially effective inhibitors (7 from GPT-3 runs, 4 from PaLM), with many showing selective functional groups and several novel scaffolds (Tanimoto < 0.75). LMLF++ outperformed reported VAE-GNN baselines on median/mean docking scores in this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to prior VAE-GNN generative-discriminator pipelines (Dash et al. 2021), LMLF (LLM + constraints) produced molecules with higher median binding-affinity estimates. Authors attribute gains to (a) LLMs' broad pretraining giving access to richer implicit chemical knowledge, and (b) the ability to enforce constraints during iterative sampling; they note the VAE-GNN baseline lacked access to the same constraint-driven conditioning in prior reports.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Limitations reported in the paper include: (1) reliance on docking (GNINA) as a proxy for true binding affinity—docking errors/approximations; (2) constraint-generalisation is restricted to numeric inequalities only (non-numeric constraints left unchanged); (3) conceptual mismatch between human and machine representations remains (formal constraints needed); (4) dependency on the preexisting knowledge in the LLMs (LLMs not fine-tuned here) — differences between LLMs observed but not statistically significant; (5) potential reproducibility and prompting differences mitigated but not fully eliminated; (6) the approach does not guarantee synthetic accessibility beyond computed SAS and no wet-lab validation done; (7) model sizes/training corpora for the used LLMs are not controlled in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Implementation called PyLMLF (code linked in paper). Iterations capped at 10; temperature for LLM sampling set to 0.7; prompts assembled include labelled examples and constraints but the textual query is deliberately simple and fixed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8650.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8650.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyLMLF</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyLMLF (Python implementation of LMLF)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical Python implementation of the LMLF procedure used in experiments to assemble prompts, call LLM APIs (GPT-3, PaLM), verify generated molecules against formal constraints using RDKit and GNINA, update background knowledge, and generalise numeric constraints iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PyLMLF (procedure wrapper + prompt assembler)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>workflow/engine that orchestrates LLM API calls and symbolic constraint checking</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>None (wrapper). Uses background datasets: ChEMBL-derived labelled inhibitor/non-inhibitor sets for JAK2 and DRD2 (~4k molecules each) for initial background knowledge and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug lead generation (JAK2, DRD2) in the experiments</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Assembles prompt from B (background facts & labelled examples), C (formal constraints) and Q (simple textual query); invokes LLM text generation API to sample SMILES strings; validates and labels samples with external tools; iterates with constraint generalisation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Same as LMLF results (PyLMLF is the executing system): experts found many generated molecules not in known databases and a subset with Tanimoto < 0.75 to known inhibitors.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enforces target-specific affinity thresholds via GNINA docking during Satisfies(...) checks and updates constraints iteratively to focus sampling toward higher docking scores.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Uses RDKit for molecular validity, MW, LogP, SAS; GNINA 1.0 for docking-based affinity; statistical tests (Mann-Whitney U) to compare distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PyLMLF executed the LMLF protocol with both GPT-3 and PaLM, producing improved binding-affinity distributions when target-specific constraints were included; qualitative expert review identified potentially novel inhibitors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Enables comparison by providing an LLM-based sampling alternative to VAE-GNN pipelines; authors report better affinity distributions than previously reported VAE-GNN results on the same JAK2 dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Implementation-level issues: current generalisation heuristics only change numeric thresholds by fixed increments; bookkeeping/efficiency details omitted from paper; reliance on external software (RDKit, GNINA) for verification; possible API rate/consistency differences between LLM providers.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Code repository URL provided in paper: https://github.com/Shreyas-Bhat/LMLF</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8650.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8650.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.0 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large autoregressive transformer-based language model (GPT family) used here as a generative engine to produce SMILES strings conditioned by assembled prompts constructed by PyLMLF.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>autoregressive transformer (GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper; referred to as a foundation LLM pretrained on large and diverse corpora (standard GPT-3 training).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Used for de novo molecular generation (drug lead generation) via prompt sampling</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct prompt-based generation (sampling SMILES strings from a prompt assembled from labelled examples + textual query), temperature 0.7</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Chemist assessment indicates GPT-3 generated several candidate molecules judged potentially novel; among the 11 molecules flagged as possibly effective inhibitors, 7 were generated by GPT-3; many showed Tanimoto < 0.75 to known inhibitors in selected cases.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced externally by PyLMLF constraints (drug-like filters and docking thresholds); GPT-3 used only as conditional sampler.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same as LMLF: RDKit validity and properties, GNINA docking scores, statistical distribution comparisons, chemist substructure and similarity analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-3 runs (GPTLF++) achieved the highest reported mean/median docking scores among configurations tested for JAK2 and DRD2 (e.g., JAK2 GPTLF ++ mean 7.74, median 7.71), and produced the majority of expert-flagged candidate inhibitors in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed better than PaLM in quantitative distributions in this study and better than the reported VAE-GNN baseline on binding-affinity statistics; differences between LLMs were not statistically significant in expert assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Model internal training details and biases not controlled; possible hallucination or SMILES invalidity mitigated by RDKit checks; no fine-tuning performed on chemical data in this work; sampling variability depends on temperature and prompt composition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8650.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8650.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM (text-bison-001)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM (text-bison-001)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based language model from the PaLM family used as an alternative generative engine for sampling SMILES strings under the PyLMLF protocol; sampled with temperature 0.7 via API.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (text-bison-001)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (PaLM family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not specified in paper; treated as a pretrained foundation LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecular generation for drug lead discovery (JAK2, DRD2) under the LMLF procedure</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct prompt-based generation called via API as with GPT-3; integrated into PyLMLF pipeline for iterative sampling and verification</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Produced candidate molecules with improved docking distributions when used with logical feedback (PaLMLF ++). Expert-assessed novel inhibitors: 4 of the 11 candidates were from PaLM runs.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Enforced by external logical constraints and docking verification (same as GPT-3 usage)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>RDKit properties and GNINA docking scores; statistical distribution tests; chemist substructure and similarity checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>PaLM improved generation quality when used with LMLF constraints (PaLMLF ++ mean/median docking scores increased relative to no-constraint cases), but quantitative gains were generally smaller than with GPT-3 in reported statistics. PaLMLF ++ for JAK2 had mean 7.20 and median 7.40 in reported table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Performed worse than GPT-3 in the reported affinity statistics but still outperformed no-constraint baselines and the reported VAE-GNN baseline in aggregate affinity stats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Same limitations as GPT-3: no fine-tuning reported; internal training corpus not controlled; sample variability; differences in LLMs not strongly significant per qualitative assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8650.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8650.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAE-GNN baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VAE-GNN (Variational Autoencoder + Graph Neural Network discriminator) from Dash et al. 2021</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously reported generative pipeline combining variational autoencoders for molecule generation with a graph neural network discriminator used as a baseline comparison in the paper; reported lower mean/median docking scores than LMLF++ on the JAK2 dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VAE-GNN (reported in Dash et al. 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Variational autoencoder (sequence or graph VAE) for generation + graph neural network discriminator</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Described in Dash et al. 2021 (trained on dataset used in that work); not re-trained or run in this paper; used as a comparative baseline from literature.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo molecule generation for drug leads (same JAK2 dataset comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>VAE-based generative sampling with a GNN discriminator; details in the cited baseline paper rather than this work.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Reported in prior work; in this paper VAE-GNN showed lower mean binding-affinity estimates compared to LMLF++ (table comparison), novelty metrics not directly reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Baseline did not incorporate the same constraint-driven conditioning on binding affinity used in LMLF experiments (authors note this as a reason for performance differences).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Reported docking and affinity-related statistics in Dash et al. 2021; in this paper used only for cross-method comparison of docking score distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>VAE-GNN reported mean/median docking scores lower than LMLF++ on the JAK2 dataset (table in current paper), but direct apples-to-apples comparison is limited because LMLF used explicit affinity constraints in the iterative loop while reported VAE-GNN results did not.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Authors argue LLM-based LMLF outperforms VAE-GNN in their comparisons, partly because LLMs encode broader chemical knowledge and because LMLF enforces constraints during sampling; they note it is straightforward to adapt LMLF ideas to VAE-GNN architectures but did not do so here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Comparison constrained by differences in access to constraint information and pretraining; baseline not run under identical constraint regime in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adaptive language model training for molecular design <em>(Rating: 2)</em></li>
                <li>Generative Deep Learning for Targeted Compound Design <em>(Rating: 2)</em></li>
                <li>Junction tree variational autoencoder for molecular graph generation <em>(Rating: 2)</em></li>
                <li>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules <em>(Rating: 2)</em></li>
                <li>GNINA 1.0: molecular docking with deep learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8650",
    "paper_id": "paper-262064851",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "LMLF",
            "name_full": "Language Models with Logical Feedback",
            "brief_description": "An iterative neuro-symbolic procedure that conditions an LLM's sampling by (a) providing formal, machine-checkable domain constraints and background knowledge, (b) sampling molecules via a fixed natural-language query, and (c) using constraint-based labelling plus numeric constraint generalisation as logical feedback to refine constraints and repeat sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM (GPT-3 / PaLM) with Logical Feedback (LMLF)",
            "model_type": "prompt-based transformer LLMs combined with an iterative symbolic constraint loop (neuro-symbolic)",
            "model_size": null,
            "training_data": "Not trained in this work; method uses pretrained foundation LLMs (GPT-3, PaLM) as black-box generative engines. Background datasets used for conditioning and evaluation include ChEMBL-derived labelled JAK2 and DRD2 molecule sets (≈4k molecules each).",
            "application_domain": "Drug discovery — de novo lead generation for protein targets (JAK2 and DRD2)",
            "generation_method": "Prompt-based generation: assemble a prompt composed from background knowledge (labelled examples and computed properties) plus formal logical constraints (encoded outside the LLM) and a simple natural-language query (e.g. 'Generate a valid SMILES...'); sample SMILES from the LLM; verify each sample against constraints (molecular validity, MW, logP, SAS, docking affinity) and add labelled results to background; then generalise numeric constraints (tighten/loosen thresholds iteratively) and repeat sampling.",
            "novelty_of_chemicals": "Qualitative novelty assessed: many generated molecules were not present in known databases per the query; among molecules identified as containing target-selective functional groups, 10 of 11 had Tanimoto similarity &lt; 0.75 to existing inhibitors, indicating structural novelty for those candidates. Quantitative novelty fraction: chemist assessment found 15% (JAK2) and 40% (DRD2) of generated molecules had known target-selective functional groups (some overlap with novel scaffolds).",
            "application_specificity": "Specificity enforced via logical constraints including target-agnostic drug-likeness filters (MW 200–700, LogP &lt; 5, SAS &lt; 5) and target-specific binding-affinity thresholds (docking score thresholds: ≥7 for first 5 iterations, ≥8 for next 5). Generated molecules were evaluated by docking to the target (GNINA) to verify target-specific binding.",
            "evaluation_metrics": "Computed molecular validity (RDKit), molecular weight, LogP, synthetic accessibility score (SAS), docking-based binding affinity estimates (GNINA docking scores), distributional comparison via Mann-Whitney U test for binding affinities, Tanimoto similarity for novelty, expert chemist substructure matches for selective functional groups.",
            "results_summary": "Across both targets and both LLMs, adding logical feedback improved distributions of estimated binding affinity: (1) No-constraint &lt; target-agnostic constraints &lt; target-agnostic + target-specific constraints (LMLF++). Example statistics (JAK2): GPTLF ++ mean 7.74, median 7.71 (docking score units used in paper). PaLMLF ++ also improved vs baselines but to a lesser degree. Expert chemists identified 11 molecules as potentially effective inhibitors (7 from GPT-3 runs, 4 from PaLM), with many showing selective functional groups and several novel scaffolds (Tanimoto &lt; 0.75). LMLF++ outperformed reported VAE-GNN baselines on median/mean docking scores in this dataset.",
            "comparison_to_other_methods": "Compared to prior VAE-GNN generative-discriminator pipelines (Dash et al. 2021), LMLF (LLM + constraints) produced molecules with higher median binding-affinity estimates. Authors attribute gains to (a) LLMs' broad pretraining giving access to richer implicit chemical knowledge, and (b) the ability to enforce constraints during iterative sampling; they note the VAE-GNN baseline lacked access to the same constraint-driven conditioning in prior reports.",
            "limitations_and_challenges": "Limitations reported in the paper include: (1) reliance on docking (GNINA) as a proxy for true binding affinity—docking errors/approximations; (2) constraint-generalisation is restricted to numeric inequalities only (non-numeric constraints left unchanged); (3) conceptual mismatch between human and machine representations remains (formal constraints needed); (4) dependency on the preexisting knowledge in the LLMs (LLMs not fine-tuned here) — differences between LLMs observed but not statistically significant; (5) potential reproducibility and prompting differences mitigated but not fully eliminated; (6) the approach does not guarantee synthetic accessibility beyond computed SAS and no wet-lab validation done; (7) model sizes/training corpora for the used LLMs are not controlled in experiments.",
            "additional_notes": "Implementation called PyLMLF (code linked in paper). Iterations capped at 10; temperature for LLM sampling set to 0.7; prompts assembled include labelled examples and constraints but the textual query is deliberately simple and fixed.",
            "uuid": "e8650.0",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PyLMLF",
            "name_full": "PyLMLF (Python implementation of LMLF)",
            "brief_description": "A practical Python implementation of the LMLF procedure used in experiments to assemble prompts, call LLM APIs (GPT-3, PaLM), verify generated molecules against formal constraints using RDKit and GNINA, update background knowledge, and generalise numeric constraints iteratively.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PyLMLF (procedure wrapper + prompt assembler)",
            "model_type": "workflow/engine that orchestrates LLM API calls and symbolic constraint checking",
            "model_size": null,
            "training_data": "None (wrapper). Uses background datasets: ChEMBL-derived labelled inhibitor/non-inhibitor sets for JAK2 and DRD2 (~4k molecules each) for initial background knowledge and examples.",
            "application_domain": "Drug lead generation (JAK2, DRD2) in the experiments",
            "generation_method": "Assembles prompt from B (background facts & labelled examples), C (formal constraints) and Q (simple textual query); invokes LLM text generation API to sample SMILES strings; validates and labels samples with external tools; iterates with constraint generalisation.",
            "novelty_of_chemicals": "Same as LMLF results (PyLMLF is the executing system): experts found many generated molecules not in known databases and a subset with Tanimoto &lt; 0.75 to known inhibitors.",
            "application_specificity": "Enforces target-specific affinity thresholds via GNINA docking during Satisfies(...) checks and updates constraints iteratively to focus sampling toward higher docking scores.",
            "evaluation_metrics": "Uses RDKit for molecular validity, MW, LogP, SAS; GNINA 1.0 for docking-based affinity; statistical tests (Mann-Whitney U) to compare distributions.",
            "results_summary": "PyLMLF executed the LMLF protocol with both GPT-3 and PaLM, producing improved binding-affinity distributions when target-specific constraints were included; qualitative expert review identified potentially novel inhibitors.",
            "comparison_to_other_methods": "Enables comparison by providing an LLM-based sampling alternative to VAE-GNN pipelines; authors report better affinity distributions than previously reported VAE-GNN results on the same JAK2 dataset.",
            "limitations_and_challenges": "Implementation-level issues: current generalisation heuristics only change numeric thresholds by fixed increments; bookkeeping/efficiency details omitted from paper; reliance on external software (RDKit, GNINA) for verification; possible API rate/consistency differences between LLM providers.",
            "additional_notes": "Code repository URL provided in paper: https://github.com/Shreyas-Bhat/LMLF",
            "uuid": "e8650.1",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3 (text-davinci-003)",
            "name_full": "GPT-3.0 (text-davinci-003)",
            "brief_description": "A large autoregressive transformer-based language model (GPT family) used here as a generative engine to produce SMILES strings conditioned by assembled prompts constructed by PyLMLF.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-003)",
            "model_type": "autoregressive transformer (GPT family)",
            "model_size": null,
            "training_data": "Not specified in paper; referred to as a foundation LLM pretrained on large and diverse corpora (standard GPT-3 training).",
            "application_domain": "Used for de novo molecular generation (drug lead generation) via prompt sampling",
            "generation_method": "Direct prompt-based generation (sampling SMILES strings from a prompt assembled from labelled examples + textual query), temperature 0.7",
            "novelty_of_chemicals": "Chemist assessment indicates GPT-3 generated several candidate molecules judged potentially novel; among the 11 molecules flagged as possibly effective inhibitors, 7 were generated by GPT-3; many showed Tanimoto &lt; 0.75 to known inhibitors in selected cases.",
            "application_specificity": "Specificity enforced externally by PyLMLF constraints (drug-like filters and docking thresholds); GPT-3 used only as conditional sampler.",
            "evaluation_metrics": "Same as LMLF: RDKit validity and properties, GNINA docking scores, statistical distribution comparisons, chemist substructure and similarity analyses.",
            "results_summary": "GPT-3 runs (GPTLF++) achieved the highest reported mean/median docking scores among configurations tested for JAK2 and DRD2 (e.g., JAK2 GPTLF ++ mean 7.74, median 7.71), and produced the majority of expert-flagged candidate inhibitors in this study.",
            "comparison_to_other_methods": "Performed better than PaLM in quantitative distributions in this study and better than the reported VAE-GNN baseline on binding-affinity statistics; differences between LLMs were not statistically significant in expert assessments.",
            "limitations_and_challenges": "Model internal training details and biases not controlled; possible hallucination or SMILES invalidity mitigated by RDKit checks; no fine-tuning performed on chemical data in this work; sampling variability depends on temperature and prompt composition.",
            "uuid": "e8650.2",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PaLM (text-bison-001)",
            "name_full": "PaLM (text-bison-001)",
            "brief_description": "A large transformer-based language model from the PaLM family used as an alternative generative engine for sampling SMILES strings under the PyLMLF protocol; sampled with temperature 0.7 via API.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM (text-bison-001)",
            "model_type": "transformer (PaLM family)",
            "model_size": null,
            "training_data": "Not specified in paper; treated as a pretrained foundation LLM.",
            "application_domain": "De novo molecular generation for drug lead discovery (JAK2, DRD2) under the LMLF procedure",
            "generation_method": "Direct prompt-based generation called via API as with GPT-3; integrated into PyLMLF pipeline for iterative sampling and verification",
            "novelty_of_chemicals": "Produced candidate molecules with improved docking distributions when used with logical feedback (PaLMLF ++). Expert-assessed novel inhibitors: 4 of the 11 candidates were from PaLM runs.",
            "application_specificity": "Enforced by external logical constraints and docking verification (same as GPT-3 usage)",
            "evaluation_metrics": "RDKit properties and GNINA docking scores; statistical distribution tests; chemist substructure and similarity checks.",
            "results_summary": "PaLM improved generation quality when used with LMLF constraints (PaLMLF ++ mean/median docking scores increased relative to no-constraint cases), but quantitative gains were generally smaller than with GPT-3 in reported statistics. PaLMLF ++ for JAK2 had mean 7.20 and median 7.40 in reported table.",
            "comparison_to_other_methods": "Performed worse than GPT-3 in the reported affinity statistics but still outperformed no-constraint baselines and the reported VAE-GNN baseline in aggregate affinity stats.",
            "limitations_and_challenges": "Same limitations as GPT-3: no fine-tuning reported; internal training corpus not controlled; sample variability; differences in LLMs not strongly significant per qualitative assessment.",
            "uuid": "e8650.3",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "VAE-GNN baseline",
            "name_full": "VAE-GNN (Variational Autoencoder + Graph Neural Network discriminator) from Dash et al. 2021",
            "brief_description": "A previously reported generative pipeline combining variational autoencoders for molecule generation with a graph neural network discriminator used as a baseline comparison in the paper; reported lower mean/median docking scores than LMLF++ on the JAK2 dataset.",
            "citation_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
            "mention_or_use": "mention",
            "model_name": "VAE-GNN (reported in Dash et al. 2021)",
            "model_type": "Variational autoencoder (sequence or graph VAE) for generation + graph neural network discriminator",
            "model_size": null,
            "training_data": "Described in Dash et al. 2021 (trained on dataset used in that work); not re-trained or run in this paper; used as a comparative baseline from literature.",
            "application_domain": "De novo molecule generation for drug leads (same JAK2 dataset comparison)",
            "generation_method": "VAE-based generative sampling with a GNN discriminator; details in the cited baseline paper rather than this work.",
            "novelty_of_chemicals": "Reported in prior work; in this paper VAE-GNN showed lower mean binding-affinity estimates compared to LMLF++ (table comparison), novelty metrics not directly reported here.",
            "application_specificity": "Baseline did not incorporate the same constraint-driven conditioning on binding affinity used in LMLF experiments (authors note this as a reason for performance differences).",
            "evaluation_metrics": "Reported docking and affinity-related statistics in Dash et al. 2021; in this paper used only for cross-method comparison of docking score distributions.",
            "results_summary": "VAE-GNN reported mean/median docking scores lower than LMLF++ on the JAK2 dataset (table in current paper), but direct apples-to-apples comparison is limited because LMLF used explicit affinity constraints in the iterative loop while reported VAE-GNN results did not.",
            "comparison_to_other_methods": "Authors argue LLM-based LMLF outperforms VAE-GNN in their comparisons, partly because LLMs encode broader chemical knowledge and because LMLF enforces constraints during sampling; they note it is straightforward to adapt LMLF ideas to VAE-GNN architectures but did not do so here.",
            "limitations_and_challenges": "Comparison constrained by differences in access to constraint information and pretraining; baseline not run under identical constraint regime in this paper.",
            "uuid": "e8650.4",
            "source_info": {
                "paper_title": "Generating Novel Leads for Drug Discovery using LLMs with Logical Feedback",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adaptive language model training for molecular design",
            "rating": 2,
            "sanitized_title": "adaptive_language_model_training_for_molecular_design"
        },
        {
            "paper_title": "Generative Deep Learning for Targeted Compound Design",
            "rating": 2,
            "sanitized_title": "generative_deep_learning_for_targeted_compound_design"
        },
        {
            "paper_title": "Junction tree variational autoencoder for molecular graph generation",
            "rating": 2,
            "sanitized_title": "junction_tree_variational_autoencoder_for_molecular_graph_generation"
        },
        {
            "paper_title": "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules",
            "rating": 2,
            "sanitized_title": "automatic_chemical_design_using_a_datadriven_continuous_representation_of_molecules"
        },
        {
            "paper_title": "GNINA 1.0: molecular docking with deep learning",
            "rating": 1,
            "sanitized_title": "gnina_10_molecular_docking_with_deep_learning"
        }
    ],
    "cost": 0.01318825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generating Novel Leads for Drug Discovery Using LLMs with Logical Feedback</p>
<p>Shreyas Bhat Brahmavar 
Department of Electrical and Electronics Engineering
BITS Pilani
Goa CampusIndia</p>
<p>Department of Biological Sciences
BITS Pilani
Goa CampusIndia</p>
<p>Ashwin Srinivasan ashwin@goa.bits-pilani.ac.in 
Department of Computer Science
BITS Pilani
Goa CampusIndia</p>
<p>Tirtharaj Dash tirtharaj@goa.bits-pilani.ac.in 
Department of Pediatrics
University of California
San DiegoUSA</p>
<p>Sowmya Ramaswamy Krishnan sowmya.rk1@tcs.com 
TCS Innovation Labs (Life Sciences Division)
India</p>
<p>Lovekesh Vig lovekesh.vig@tcs.com 
TCS Research
India</p>
<p>Arijit Roy roy.arijit3@tcs.com 
TCS Innovation Labs (Life Sciences Division)
India</p>
<p>Raviprasad Aduri aduri@goa.bits-pilani.ac.in 
Department of Biological Sciences
BITS Pilani
Goa CampusIndia</p>
<p>Generating Novel Leads for Drug Discovery Using LLMs with Logical Feedback
376A1CF780BD76ECEBDC7A89E2C078D2
Large Language Models (LLMs) can be used as repositories of biological and chemical information to generate pharmacological lead compounds.However, for LLMs to focus on specific drug targets typically require experimentation with progressively more refined prompts.Results thus become dependent not just on what is known about the target, but also on what is known about the prompt-engineering.In this paper, we separate the prompt into domain-constraints that can be written in a standard logical form, and a simple textbased query.We investigate whether LLMs can be guided, not by refining prompts manually, but by refining the the logical component automatically, keeping the query unchanged.We describe an iterative procedure LMLF ("Language Models with Logical Feedback") in which the constraints are progressively refined using a logical notion of generalisation.On any iteration, newly generated instances are verified against the constraint, providing "logical-feedback" for the next iteration's refinement of the constraints.We evaluate LMLF using two well-known targets (inhibition of the Janus Kinase 2; and Dopamine Receptor D2); and two different LLMs (GPT-3 and PaLM).We show that LMLF, starting with the same logical constraints and query text, can guide both LLMs to generate potential leads.We find: (a) Binding affinities of LMLF-generated molecules are skewed towards higher binding affinities than those from existing baselines; (b) LMLF results in generating molecules that are skewed towards higher binding affinities than without logical feedback; (c) Assessment by a computational chemist suggests that LMLF generated compounds may be novel inhibitors.These findings suggest that LLMs with logical feedback may provide a mechanism for generating new leads without requiring the domain-specialist to acquire sophisticated skills in prompt-engineering.</p>
<p>Introduction</p>
<p>In 1982, Edward Feigenbaum identified a difficulty in the transfer of human-knowledge to a machine, now famous as "the Feigenbaum bottleneck" (Feigenbaum et al. 1977).In a curious twist of fate, we now appear confronted by a "reverse bottleneck".Machine knowledge, such as those contained in large foundation models, is at least as difficult for humans to access as it was to represent human knowledge in a machine-understandable form.Surprisingly, this reverse bottleneck also appears to have been first identified in 1982.The problem of 'The Human Window' (Kopec 1982;Michie 1982) refers to the difficulties faced by humans when interacting with a complex computing system, due to a mismatch in representations of the human and machine.Modern large language models (LLMs) would appear to have resolved this difficulty through their impressive facility to use natural language as a mechanism of communicating with humans (Brown et al. 2020;Narang and Chowdhery 2022).In fact, the true difficulties concerning the Human Window arise from the need for a conceptual interface, not simply a linguistic interface.That is, the mismatch has to be addressed at a concept-level. 1 The intense interest in the methods and practices of 'prompt engineering' as an approach to extract useful information from LLMs could be seen as evidence of the deeper, conceptual mismatch that exists between LLMs and human representations.In this paper, we are concerned with an immediate practical manifestation of this, namely in the apparent need to be a sophisticated prompt-engineer to be able to use the capability of an LLM best.</p>
<p>Our specific interest is in the generation of novel leads for early-stage drug-design.Here, the human is typically a computational-or synthetic-chemist, who often uses knowledge that can be expressed in a logical form.For example, this may be in the form of generic constraints on values like molecular weight, hydrophobicity, synthetic accessibility score, etc.; and specific constraints like estimated binding energy to the target site, size of the binding site, presence of any specific anchors and so on.The usual route to provide this as input to an LLM would be through prompts, which combine what the chemist knows, and what the chemist needs from the LLM.However, the free-text interface prompts make it difficult to settle on a single form for this input, and the process becomes one of experimentation with phrasings or text, and orderings of textual se-quences.Results are, therefore, dependent not just on what is known bio-chemically, but on the content and sequence of text provided.This makes the experiments highly subjective and difficult to reproduce.</p>
<p>In this paper, we attempt to reduce this subjectivity while retaining one very important feature of LLMs, namely, the ability to adapt quickly to new probability distributions and to sample effectively from them.Our approach is to separate the content of a prompt into two parts: a domainspecific component, and a domain-independent component.In this paper, by 'domain-specific' here, we mean the drugdesign aspects that allow the LLM to update its probability distribution.The domain-independent part will be a simple query enabling the LLM to generate instances from its (updated) distribution.Further, we will require that the domain-specific component be encoded in a standardised form that can be refined automatically and that the domainindependent form is simple enough to be independent of the LLM used.This is done to ensure clarity and repeatability of the experimental protocol.Here, the standardised form we employ is formal logic, and LLM updates are done through a procedure LMLF that employs what we call 'logical feedback'.</p>
<p>The rest of the paper is organised as follows.In the Background section, we summarise the need for automated discovery of new leads in early-stage drug-design, and a description of some constraints on lead-generation.Although we expect readers to be familiar with LLMs, at least in their use through interactive interfaces like ChatGPT, we nevertheless include a short description of LLMs as implementations of probabilistic generative models.In the section following, we describe the LMLF procedure as a method of using LLMs in conjunction with satisfaction of logical constraints acting as feedback to update its probability distribution.In the Empirical Evaluation section, we describe our evaluation of LMLF empirically using two benchmark drug-design targets and two well-known LLMs.Finally, we present some related works, followed by conclusions.</p>
<p>Background Lead Discovery in Early-Stage Drug Design</p>
<p>Drugs are small molecules that usually attach themselves to parts of a larger molecule (like a protein), called a 'target'.This attachment takes place at a location known as the 'target site'.The attachment occurs mainly by the usual physical electrostatic mechanisms, and the process is known as binding.Binding results in structural and functional change of the target molecule.Usually, this change means stopping some activity, and the small molecule is said to inhibit the target.Leads are small molecules that could potentially bind to a target molecule.</p>
<p>Artificial Intelligence is currently revolutionizing drug development (Williams et al. 2015), especially in various steps of early-stage drug design (see Fig. 1(a) showing the use of a Robot Scientist) as virtual screening, identifying qualitative and quantitative structure-activity relations (SARs) and so on.The broader picture is of a semi-automated scientific discovery pipeline involving feedback from from computa-tional chemists, synthetic chemists, and biologists and manufacturers, using results from simulation, synthesis protocols, and biological testing (see Fig. 1(b) and (Zenil et al. 2023) for the broader context of closed-loop scientific discovery).In this paper, we restrict ourselves to domain-specialists in the form of computational chemists with knowledge of chemical synthesis.We envisage 2 kinds of interaction between such this specialist and the computational engine: (A) Provision of chemical knowledge.This could be of a general nature on drug-likeness, or specific to the target or output of the computational engine; (B) Asking chemical queries, usually about possible new structures, or specific aspects of existing structures.</p>
<p>If the computational engine is a large language model (LLM), then all specialists in Fig. 1(b) should be able to interact using natural language.But, as pointed out in Sec., the very flexibility allowed by natural language instruction poses difficulties to the construction a pipeline capable of repeatable, standardised experiments.We will be looking at a mechanism that requires: the specialist's knowledge (A) to be provided in a standardised form that can then be refined automatically; and the chemical queries (B) that are to be posed as simple text concerning the generation of new molecular structures.In effect, (A) and its subsequent refinements are used to alter automatically the conditioning information (used here in a probabilistic sense) provided to the LLM; and (B) is used to sample from the resulting conditional probability distribution over small molecules.</p>
<p>Language Models as Probabilistic Machines</p>
<p>A language model is a probabilistic model of natural language that learns a probability distribution over sequences of tokens called sentences.Let W denote one such sentence with N tokens, W = (w 1 , . . ., w N ), where, N is arbitrary.A language model estimates the probability of observing the sentence W , denoted by the joint probability P (W ).In practice, however, P (W ) is approximated using n-gram models (Jelinek 1980;Katz 1987) or Neural language models or NLMs (Bengio, Ducharme, and Vincent 2000).These models use Markov's assumption that the probability of a word</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence  depends only on the previous n &lt; N words.That is,
P (W ) = N i=1 P (w i |w i−(n−1) , . . . , w i−1 )(1)
Neural language models or NLM (Bengio, Ducharme, and Vincent 2000) are probabilistic language models based on (deep) neural networks that can handle the problems associated with n-gram models, such as handling long-range dependency, context understanding, handling noise and ambiguity, learning complex relationships by learning a distributed representation of text tokens.An NLM approximates each term on the right-hand side of Eq. ( 1) using a neural network.Large Language Models (LLMs) such as GPTs (Radford et al. 2019) and PALM (Narang and Chowdhery 2022) are large and complex neural language models that use the transformer architecture (Vaswani et al. 2017) to learn from vast and diverse corpora of text data.</p>
<p>Prompt-based LLMs, such as ChatGPT and BARD are LLMs that can learn with human-feedback (Ouyang et al. 2022).A prompt is an input sequence written by a human in a natural language, which serves as the starting point that sets the context of the LLM to generate the next (highly) probable text sequence in an auto-regressive manner.</p>
<p>Using Language Models with Logical Feedback (LMLF)</p>
<p>We differentiate the information provided by a human in a prompt for an LLM into 2 parts: contextual information, which can be encoded in a formal language, and a query, which is in a natural language.It is further helpful for us to distinguish the former into background knowledge B consisting of definitions, functions, procedures and factual statements, and C, consisting of domain-constraints.For the specific task of lead-discovery that we are interested in:</p>
<p>• B will include: example molecules; facts about the molecules obtained from computation by a generalpurpose molecular modelling package (computing, for example, bulk properties like molecular weight, synthesis accessibility scores etc.); facts about the molecule obtained from computation by special-purpose molecular modelling package (computing for example, binding affinity to the target site).We also consider part of B any standard mathematical and arithmetic procedures used in the constraints C. • C will typically be a conjunction of desirable properties of leads, like molecular weight between 200 and 700; logP below 5; SA score below 5; and binding affinity is −8 or less, etc.</p>
<p>In developing LMLF, we are inspired by the MIMIC algorithm (De Bonet, Isbell, and Viola 1996), which uses an iterative procedure for model-assisted sampling.MIMIC assumes that we are looking to generate instances with low values of an objective function F .On any iteration i, MIMIC has access to a sample of instances; and a model M i that can be used both for discrimination and for sampling (generation).True F -values are computed for the sample of known instances, and M i is revised to M i+1 that can discriminate accurately between instances with F -value below and above some threshold θ i .That is, M discriminates between F (x) ≤ θ (labelled "1") and F (x) &gt; θ (labelled "0").M i+1 is then used to generate new data instances, the threshold θ i is lowered to θ i+1 , and the process is repeated (say k times).</p>
<p>We first recast MIMIC in terms of background knowledge and constraints.Let B denote the function F , the thresholds θ i , and standard arithmetic definitions of ≤, &gt;.Let C be the conjunction
C 1 ∧ C 2 ∧ • • • C n , where C i = (F (x) ≤ θ i ).
We are able to abstract two general principles about the algorithm:</p>
<p>• On any iteration i, feedback to M i is provided by instances labelled based on whether (B ∧ C i ) is true (label 1) or false (label 0).We call this the "constraint-based labelling" property.
• Since θ i+1 ≤ θ i , if F (x) ≤ θ i+1 then F (x) ≤ θ i . That is, (B ∧ C i+1 ) |= (B ∧ C i ).
We call this the "constraintgeneralisation" property.</p>
<p>We now devise a general iterative procedure with these two properties to alter the conditioning sequence for an LLM for discrimination and generation.The steps are shown in Procedure 1.For reasons of space, we do not provide procedures for the auxiliary functions.An idealised worked example below is intended to help clarify their intended behaviour.</p>
<p>Procedure 1: Incremental sampling from an LLM's conditional distribution using iterative constraint-based labelling and constraint generalisation.Input: L: an LLM; B 0 : background knowledge, which contains a sample D 0 of labelled instances; C 0 : a logical formula representing constraints; Q: a query; k: an upperbound on the number of iterations; and n: an upper-bound on the number of samples Output: a set of instances 1: j := 1 2: while (j ≤ k and D j−1 ̸ = ∅) do 3:
P j := AssembleP rompt(B j−1 , C j−1 , Q) 4:
E j := Sample(n, L, P j ) 5:</p>
<p>D j := {(e, l) : e ∈ E j and l = Satisf ies(e, B j−1 , C j−1 )} 6:
B j := U pdateBack(B j−1 , D j ) 7: C j := GeneraliseConstraint(B j , C j−1 )
8:</p>
<p>j := j + 1 9: end while 10: return D j Example 1.We want to generate molecules to inhibit the target protein, Janus Kinase 2 (JAK2).We work through one complete iteration of LMLF, albeit without actual details.For ease of explanation, we will be using a logic-based syntax to describe background knowledge and constraints, and restricted natural language (Kuhn 2014) to describe the query (the actual implementation used in experiments does not use either of these representations).</p>
<p>1.The background knowledge B 0 contains facts about the target, some known molecules, and labels (say, 1 and 0 inhibitors).</p>
<p>• These are conjunctions of facts.For example: target(jak2) ∧ mol(m 1 ) ∧ label(m 1 , 1) ∧ mol(m 2 ) ∧ label(m2, 0) ∧ . . . .• Additional facts may describe the properties of the molecules; for example, molwt(m 1 , 245.5) ∧ logp(m 1 , 4.0) ∧ . . . .B 0 also contains information for use by the constraints.</p>
<p>• This may be facts.For example, a threshold for binding affinity, like aff thresh(8.0);or a range of allowed values for molecular weight, like molwt ([200, 700]) and so on.• B 0 also contains functions for performing computation.For example, the definition for computing affinity scores: aff inity(M ol, T arget) = Score) if (GNINA(M ol, T arget) = Score), . . . . 2. C 0 describes a set of constraints required to be satisfied.Some examples are:
• mol(m 10 ) ∧ molwt(m 10 , w) ∧ molwt([x, y]) ∧ (x ≤ w) ∧ (w ≤ y) • mol(m 10 )∧aff inity(m 10 , a)∧aff thresh(z)∧(a &gt; z).
Here a, w, x, y, z are all variables.3. The query Q we use is a simple textual one: Generate valid SMILES string for n molecules that are labelled "1" and are not found in any known database.4. Using the information in B 0 and C 0 , and Q, AssembleP rompt returns text string that includes strings for labelled molecules (like 1 m 1 , 0 m 2 and so on) and the query Q.The LLM uses this as a prompt to sample n new molecules.5.Each new molecule e is tested against the constraints.</p>
<p>The function Satisf ies returns 1 if e satisfies B 0 ∧ C 0 and 0 otherwise.6.The background knowledge is updated to B 1 with the newly labelled instances.7. The constraint C 0 is generalised to C 1 by GeneraliseConstraint. Generalisation is restricted to numeric constraints with inequalities (all other constraints are left unchanged).A constraint of the form x ≤ θ (where θ is some numeric value) is generalised to x ≤ θ ′ where θ ′ &lt; θ.Similarly x &gt; θ is changed to x &gt; θ ′ .It is assumed that the background knowledge contains a function to compute θ ′ given θ (for example, increment and decrement functions that add or subtract pre-specified amounts to θ).</p>
<p>The implementation used for experiments in the paper has aspects related to efficiency and book-keeping, which introduce unnecessary detail but retains the essential feature of iteration over constraint-based labelling and constraintgeneralisation.In the following, we will call the implementation PyLMLF.For our purpose, PyLMLF will be used as a tool for investigating the use of LMLF to generate new leads for early-stage drug-design.The code for PyLMLF can be found at: https://github.com/Shreyas-Bhat/LMLF.</p>
<p>Empirical Evaluation Aims</p>
<p>We use PyLMLF as a tool to investigate the following conjecture:</p>
<p>• The use of LLMs with logical feedback generates better lead molecules for early-stage drug-design than LLMs without logical feedback.</p>
<p>We will make the following design choices to conduct the experiments: (a) We consider two classic drug-design targets and two well-known LLMs; (b) We will use two methods of assessing the results: quantitatively, using the distribution of binding affinities of generated molecules; and qualitatively, using assessments by a computational chemist.</p>
<p>Materials</p>
<p>Biological Targets and LLMs We conduct our evaluations on JAK2, with 4100 molecules provided with labels (3700 active) and DRD2 (4070 molecules with labels, of which 3670 are active).These datasets were collected from ChEMBL (Gaulton et al. to some generic bulk properties of the small molecules.Specifically, we use the generic constraints used in (Dash et al. 2021) for one of the datasets used here (JAK2).These constraints encode the following requirements of potential leads: (i) Molecular weight must be between 200 and 700;</p>
<p>(ii) Synthetic accessibility score (SAS) must be below 5;</p>
<p>(iii) LogP value must be below 5; and (iv) Binding affinity must be above 7.For experiments here, we limit constraints in (c) to estimates of binding affinity to the target site obtained from the approach described in (McNutt et al. 2021).For a small molecule m, the constraint encodes the condition: aff inity(m, x) ∧ x ≥ θ.We ensure updates to the background knowledge performed by the PyLMLF implementation of the LMLF procedure ensure that θ values</p>
<p>The Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</p>
<p>either stay the same or increase on each iteration.This ensures the constraint-generalisation condition is not violated.</p>
<p>In the description of the experimental method below, we will denote the trivial case of not having any constraints as C ϕ 0 ; the case with only target-agnostic constraints as C + 0 ; and the case with both target-agnostic and target-specific constraints as C ++ 0 .Query A query in our experiments is a restricted (unambiguous) English statement: Generate a molecule that is valid and not in any known database.The query, in combination with the available background knowledge and constraints, is assembled to construct the prompt for the LLMs.</p>
<p>Algorithms and Machines</p>
<p>All the experiments are conducted using a Linux (Ubuntu) based workstation with 64GB of main memory and 16-core Intel Xeon 3.10GHz processors.All the implementations are in Python3, with API calls to the respective model engines for GPT-3.0 and PaLM.We use RDKit (version: 2022.9.5) for computing molecular properties and GNINA 1.0 for computing docking scores (binding affinities) of molecules.</p>
<p>Method</p>
<p>Our method is straightforward: 1.For each biological target T ∈ {JAK2, DRD2}:</p>
<p>(a) For each LLM L ∈ {GP T 3, P aLM } i.Let M T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C ϕ 0 for target T ; ii.Let M + T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C + 0 for target T ; and iii.Let M ++ T,i be the set of molecules returned by PyLMLF provided with LLM L, background B 0 and constraints C ++ 0 for target T .iv. Compare the sets M , M + 0 and M ++ quantitatively using the distribution of estimated target-specific binding affinities v. Compare the sets M , M + and M ++ qualitatively using assessments by domain-specialists The following additional details are relevant:</p>
<p>• We make API calls to text-davinci-003 for GPT-3.0 and text-bison-001 for PaLM.For both LLMs, temperature is set to 0.7.• The upper-bound on the number of iterations (k in Procedure 1) is 10.• In our constraint C, we use a threshold of 7 on binding affinity for the first 5 iterations and 8 for the next 5 iterations.• Quantitative comparison of performance is done as follows.For any set of molecules, we obtain a histogram of binding affinities to act as an estimate of the probability distribution of affinities.Comparison of any 2 sets of molecules M 1 and M 2 is done by using the nonparametric Mann-Whitney U test on these estimated distributions.If p &lt; 0.05, then we reject the null hypothesis that M 1 and M 2 are from the same distribution.If the null hypothesis is rejected, and the median values of binding affinities from M 1 are higher than those from M 2 , then we will say the performance of the procedure generating M 1 is better (respectively equal or worse).</p>
<p>Results</p>
<p>In the following, we use GPTLF to denote PyLMLF provided with GPT3, background B 0 , and constraints C ϕ 0 ; GPTLF + to denote PyLMLF provided with GPT3, background B 0 and constraints C + 0 ; and GPTLF ++ to denote PyLMLF with GPT3, background B 0 , and constraints C ++ 0 .Similarly for PaLMLF, PaLMLF + and PaLMLF ++ .Summaries of quantitative results are in Table 1.Histograms showing the distribution of estimated binding affinities are in Figure 2. It is evident from both the tabulation and diagrams that for both targets and both LLMs: (a) LLMs are capable of generating molecules without any constraints (C ϕ 0 ); (b) When the LLMs are provided target-agnostic constraints (C + 0 ), performance is better than without constraints (case (a) above); and (c) when the LLMs are provided with both target-agnostic and targetspecific constraints (C ++ 0 ), performance is better than with just generic constraints (case (b) above).These results provide quantitative support to the conjecture that logical feedback is beneficial in using LLMs to generate potential leads.</p>
<p>Qualitative Assessment by Chemists.The chemists were provided with 20 molecules each of potential JAK2 and DRD2 inhibitors (10 of each were generated by GPTLF ++ and PaLMLF ++ , although the chemists were not told which LLM was involved).A summary of the assessment made by the chemists is reproduced below.Additional details are available at: https://doi.org/10.1101/2023.09.14.557698.</p>
<p>Efficacy.(a) JAK2.A set of 13 JAK2-selective functional groups was identified based on patent literature and used for exact substructure match against the generated molecules.From the search, 3 generated molecules (15%) were found to have at least one JAK2-selective functional group.4,6-Diamino pyrimidine, morpholine, [1,2,4]-triazolo[1,5-a]pyridine and 3-amino pyrazole groups were predominantly observed to enhance JAK2 selectivity in the generated small molecules (b) DRD2.A set of 11 functional groups were identified from patented DRD2 inhibitors.Since these functional groups have been proven to enhance selectivity toward DRD2, an exact substructure match was performed with the generated small molecules using RDKit, to identify the presence of these selectivity features from patent literature.From the search, 8 generated molecules (40%) were found to have at least one DRD2-selective functional group indicating the model's capability to optimize molecular features to capture selectivity, based on the docking score observed.Dimethyl piperazine and chlorobenzene was observed to be the predominant DRD2-selective group among the Novelty.10 of the 11 molecules that contain JAK2 or DRD2 selective functional groups of interest also had less than 0.75 Tanimoto similarity to the existing JAK2 or DRD2 inhibitors, respectively.Therefore, it can be interpreted  that although some fractions of the molecules generated have less similarity to existing inhibitors, the presence of selective functional groups indicates their potential to act as novel and selective inhibitors for the target protein of interest.Overall.It appears that the model has learned to generate more inhibitors with better similarity to existing JAK2 inhibitors (50%) compared to DRD2 (15%).But this difference is compensated by the fact that 40% of generated DRD2 inhibitors have highly selective functional groups, while only 15% of generated JAK2 inhibitors have selective groups.Hence, the model exhibits a balance in generation of similar and novel molecules depending on the nature and diversity of the training dataset used for the target protein of interest.</p>
<p>We now report on some additional comparisons that do not directly impinge on the experimental conjecture in Sec., but are nevertheless of interest to practitioners.First, the quantitative and qualitative assessments provide us with an opportunity to compare the capabilities of GPT3 and PaLM under controlled conditions.It is evident from the tabulation in Table 1 that the LMLF using GPT appears to be better than using PaLM.However, the expert assessment provides a slightly different story.Of the 11 molecules identified to be possibly effective JAK2 or DRD2 inhibitors, 7 were obtained using GPT3 and 4 was obtained using PaLM.Of the possible inhibitors that were also identified as being possibly novel, 7 was from GPT3, and 3 was from PaLM.These numbers are indicative of some benefit in using GPT.However, the differences are not statistically significant.</p>
<p>Secondly, we are able to perform a comparison of the use of LLMs against baselines provided by: (a) known inhibitors of JAK2 and DRD2; and (b) results reported on the same dataset(s) on novel lead-generation.Results have been reported on the JAK2 dataset most recently in (Dash et al. 2021).This uses a combination of 2 variational autoencoders (VAEs) for generating molecules, and a graph-based neural network (GNN) that acts as a discriminator, which found to be perform better than the previous reports (in (Krishnan et al. 2021)) using reinforcement learning in combination</p>
<p>Related Work</p>
<p>Over the last few years, deep generative models have been used successfully in generating novel compounds for specific biological targets and with desired molecular properties.A comprehensive review of some of the techniques can be found in (Sousa et al. 2021).Among these techniques, deep sequence models such as variational autoencoders (VAE), deep structure-based models such as graph neural networks (GNNs) are shown to be very effective.Some of these techniques are paired with each other and also with reinforcement learning to allow these models to be biased towards generating models with desired properties (Jin, Barzilay, and Kang and Kim 2023;Born and Manica 2022).LLMs with some (human) feedback are also adopted for molecular generation (Blanchard et al. 2023;Fang et al. 2023).Our present work is in a similar vein, albeit with additional domain-knowledge and desired constraints used to progressively guide the LLM's sampling engine to draw molecules from a more restricted joint distribution, allowing more novelty and diversity in molecule generation against a specific biological target.</p>
<p>Conclusion</p>
<p>In this paper, we have proposed a simple iterative procedure called LMLF that progressively alters the conditioning string provided to a large language model (LLM).The alterations are the result of testing answers generated by the LLM against domain-specific constraints represented in a formal, rather than natural language.We investigate the performance of LMLF in the area of lead-discovery, and find that the logical constraints, enforced using our proposed feedback mechanism, provide much more effective conditioning information to the LLM.As a result, we are able to use the internal knowledge contained within the LLMs much more effectively to generate potentially novel inhibitors for specific biological targets.We present quantitative results supporting this claim on two separate targets, using two different LLMs; and qualitative results in the form of preliminary assessments by computational chemists.</p>
<p>Large 'foundation' models that have been constructed with vast amounts of data can be seen as storehouses of factual and hypothesised knowledge that can be of great value in tackling complex tasks in areas like drug-design.But how are human problem-solvers-like chemists and biologists-to draw on such knowledge?A long recognised concern of mismatch between human-and machine-representations of knowledge suggests that this is not an easy task.On the surface, it would seem that this will not be a concern when using LLMs, given their capability to interact with humans in a natural language.However, this may not follow for at least two reasons.First, the issue is of a mismatch in representation (what concepts are being used), and not of communication language.For example, the machine may be using a concept for which there is no simple description in a natural language.We are not addressing this problem here.Secondly, the flexibility of natural language introduces ambiguity and imprecision.Thus, human-concepts can be conveyed to a machine in many ways, not all of which may map to the same machine-concepts (however mismatched).The latter issue poses difficulties in using LLMs in a controlled manner.The position adopted in this paper is that for certain kinds of scientific problems-like the lead-discovery tasks here-it is possible to side-step the second difficulty and still use LLMs effectively.Specifically, we are concerned with tasks for which we are able to formulate task-specific requirements in a sufficiently formal language, which can in turn be used in conjunction with an LLM.We suggest that this simple neuro-symbolic approach could provide an effective basis for using LLMs in closed-loop scientific discovery of the kind envisaged in (Zenil et al. 2023).</p>
<p>Figure 1 :
1
Figure 1: (a) Early Stage Drug Design; and (b) Computational drug discovery with specialists-in-the-loop.The dotted arrows represent 2-way 'interactions'.</p>
<p>2012), which are selected based on their IC 50 values and docking scores with active JAK2 and DRD2 proteins less than −7.0.For all our experiments, we use 2 LLMs: GPT-3.0(Brown et al. 2020) and PaLM(Narang and Chowdhery 2022).Background knowledge There are 3 categories of background knowledge: (a) Factual statements: referring to what is already known about drug targets, for example, some subset of experimentally known inhibitors for JAK2 and DRD2; (b) Functional definitions to compute bulk molecular properties; and and (c) Procedures needed to assemble the prompt for sampling.For (b), we use the definitions available within the molecular modelling packages RDKit (Landrum et al. 2013) and GNINA 1.0 (McNutt et al. 2021) to compute the validity of SMILES string, molecular weight, synthetic accessibility score (SAS), LogP, binding affinity.For (c), we use Python's f-string syntax to incorporate the molecules and their class-labels (inhibitor or non-inhibitor) represented in (a).Constraints We conduct experiments with 3 categories of constraints: (a) No constraints; (b) Target-agnostic constraints; and (c) Target-specific constraints.Of these, (a) is self-explanatory.Constraints in category (b) refer only</p>
<p>Figure 2 :
2
Figure 2: Plot showing the distribution of binding affinities for molecules generated by the LLMs and the two drug targets.</p>
<p>Table 1 :
1
Statistics of binding affinities of LLM-generated molecules against the drug targets: (Left) JAK2, (Right) DRD2.
MethodMean Median S.D.RangeMethodMean Median S.D.RangeGPTLF6.506.611.07 4.21 -7.55GPTLF6.486.510.75 5.33 -7.42GPTLF +7.537.570.88 4.83 -8.29GPTLF +7.327.210.32 6.45 -8.02GPTLF ++7.747.710.30 7.18 -8.52GPTLF ++7.667.710.29 7.25 -8.29PaLMLF6.096.010.77 3.86 -7.55PaLMLF6.056.131.05 4.33 -7.49PaLMLF +6.126.240.69 4.65 -8.11PaLMLF +6.226.230.48 5.67 -8.24PaLMLF ++7.207.400.58 7.03 -8.36PaLMLF ++7.607.550.37 7.01 -8.19</p>
<p>Table 2 :
2
(Dash et al. 2021)ing affinities for the LMLF-generated molecules (Left: JAK2, Right: DRD2), in comparison to those of known inhibitors, and the molecules generated by VAE-GNN in(Dash et al. 2021).'-' denotes 'data not available'.
MethodMean Median S.D.RangeMethodMean Median S.D.RangeKnown Inhibitors7.267.420.64 4.19 -8.34Known Inhibitors6.866.970.63 4.21 -8.31VAE-GNN6.536.951.18 2.10 -8.18VAE-GNN----GPTLF ++7.747.710.30 7.18 -8.52GPTLF ++7.667.710.29 7.25 -8.29PaLMLF ++7.207.400.58 7.03 -8.36PaLMLF ++7.607.550.37 7.01 -8.19with an MLP. Table 2 compares and shows both LLMs per-form better than the VAE-GNN combination. To some ex-tent, this is unsurprising for 2 primary reasons: (1) the LLMshave access to substantially more information than the VAE-GNN model, and (2) the VAE-GNN model does not have ac-cess to the constraint(s) on the binding affinity of molecules.It is relatively straightforward to develop a variant of LMLFfor other kinds generative models like the VAE-GNN model.This would address (2), but it is unclear how the gap in (1)can be bridged.
 Kopec distinguishes between 'surface-level' and 'structurallevel'  mismatches. The use of natural language addresses the surface-level mismatch, but it does not necessarily alleviate the mismatch between the concepts employed.The Thirty-Eighth AAAI Conference on Artificial Intelligence 
The Thirty-Eighth AAAI Conference on Artificial Intelligence 
AcknowledgementsRA and AS acknowledge the funding from DBT-NNP (Grant No. BT/PR40236/BTIS/137/51/2022).The authors thank Shreyas V. for his assistance in setting up PaLM API.AS is a TCS-affiliated Professor and the Head of APPCAIR, BITS Pilani; a Visiting Professor at the Centre for Health Informatics, Macquarie University, Sydney; and a Visiting Professorial Fellow at the School of CSE, UNSW, Sydney.This work was initiated when TD was at BITS Pilani, Goa Campus.LV is a Professor of Practice at the Department of CS &amp; IS, BITS Pilani, Goa Campus.
A neural probabilistic language model. Advances in neural information processing systems. Y Bengio, R Ducharme, P Vincent, 200013</p>
<p>Adaptive language model training for molecular design. A E Blanchard, D Bhowmik, Z Fox, J Gounley, J Glaser, B S Akpa, S Irle, Journal of Cheminformatics. 1512023</p>
<p>Regression Transformer: Concurrent Conditional Generation and Regression by Blending Numerical and Textual Tokens. J Born, M Manica, CoRR, abs/2202.013382022</p>
<p>Language Models are Few-Shot Learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, CoRR, abs/2005.141652020</p>
<p>Do Large Language Models Understand Chemistry? A Conversation with ChatGPT. N D Cao, T Kipf, C M Castro Nascimento, A S Pimentel, arXiv:1805.1197336926868Journal of Chemical Information and Modeling. 6362022. 2023MolGAN: An implicit generative model for small molecular graphs</p>
<p>A review of some techniques for inclusion of domainknowledge into deep neural networks. T Dash, S Chitlangia, A Ahuja, A Srinivasan, Scientific Reports. 12110402022</p>
<p>Inclusion of domain-knowledge into gnns using mode-directed inverse entailment. T Dash, A Srinivasan, A Baskar, Machine Learning. 2022</p>
<p>Using domain-knowledge to assist lead discovery in early-stage drug design. T Dash, A Srinivasan, L Vig, A Roy, International Conference on Inductive Logic Programming. Springer2021</p>
<p>MIMIC: Finding optima by estimating probability densities. J De Bonet, C Isbell, P Viola, Advances in neural information processing systems. 19969</p>
<p>The art of artificial intelligence: Themes and case studies of knowledge engineering. Y Fang, N Zhang, Z Chen, X Fan, H Chen, E A Feigenbaum, CoRR, abs/2301.11259Computer Science Department, School of Humanities and Sciences. 40D12023. 1977. 2012Stanford University. Gaulton,Nucleic acids research</p>
<p>Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. R Gómez-Bombarelli, J N Wei, D Duvenaud, J M Hernández-Lobato, B Sánchez-Lengeling, 29532027ACS Central Science. 422018</p>
<p>Interpolated estimation of Markov source parameters from sparse data. F Jelinek, Proc. Workshop on Pattern Recognition in Practice. Workshop on Pattern Recognition in Practice1980. 1980</p>
<p>Junction tree variational autoencoder for molecular graph generation. W Jin, R Barzilay, T Jaakkola, International conference on machine learning. PMLR2018</p>
<p>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. Y Kang, J Kim, arXiv:2308.014232023arXiv preprint</p>
<p>Estimation of probabilities from sparse data for the language model component of a speech recognizer. S Katz, IEEE transactions on acoustics, speech, and signal processing. 198735</p>
<p>Human and machine representations of knowledge. D Kopec, 1982University of EdinburghPh.D. thesis</p>
<p>Accelerating de novo drug design against novel proteins using deep learning. S R Krishnan, N Bung, G Bulusu, A Roy, Journal of Chemical Information and Modeling. 6122021</p>
<p>RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling. T ; Kuhn, 2014. 2013Greg Landrum4031Landrum, GA survey and classification of controlled natural languages. Computational linguistics</p>
<p>Constrained graph variational autoencoders for molecule design. Q Liu, M Allamanis, M Brockschmidt, A Gaunt, Advances in neural information processing systems. 201831</p>
<p>GNINA 1.0: molecular docking with deep learning. A T Mcnutt, P Francoeur, R Aggarwal, T Masuda, R Meli, M Ragoza, J Sunseri, D R Koes, Journal of cheminformatics. 1312021</p>
<p>Experiments on the Mechanization of Game-Learning. 2-Rule-Based Learning and the Human Window. D Michie, Comput. J. 2511982</p>
<p>Pathways language model (palm): Scaling to 540 billion parameters for breakthrough performance. S Narang, A Chowdhery, 2022Google AI Blog</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, Advances in Neural Information Processing Systems. 202235</p>
<p>Better language models and their implications. A Radford, J Wu, D Amodei, D Amodei, J Clark, M Brundage, I Sutskever, OpenAI blog. 212019</p>
<p>Generative Deep Learning for Targeted Compound Design. T Sousa, J Correia, V Pereira, M Rocha, 34699719Journal of Chemical Information and Modeling. 61112021</p>
<p>Attention is all you need. Advances in neural information processing systems. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ł Kaiser, I Polosukhin, 201730</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K De Grave, J Ramon, M De Clare, W Sirawaraporn, S G Oliver, R D King, Journal of The Royal Society Interface. 12104201412892015</p>
<p>H Zenil, J Tegnér, F S Abrahão, A Lavin, V Kumar, arXiv:2307.07522The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
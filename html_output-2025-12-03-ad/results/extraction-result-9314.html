<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9314 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9314</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9314</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-273549211</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.18856v3.pdf" target="_blank">Demystifying Large Language Models for Medicine: A Primer</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions. Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions. In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices. This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment. We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface. We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks. Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed. By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9314.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9314.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot learning (FSL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategy that includes a small number (typically 1–5) of demonstration input-output examples (‘shots’) in the prompt to guide an LLM's behavior for the target task, improving specification of style and edge-case handling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general; e.g., GPT-4, Llama 3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (e.g., patient-to-trial matching, classification, generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where demonstrating the desired input→output mapping within the prompt helps the model generalize to new instances.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompt: include 1–5 representative and diverse demonstration examples (shots) inside the prompt; examples should contain both input and desired output; dynamic selection of examples by semantic similarity is suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Zero-shot and one-shot (the paper recommends starting from zero-shot and incrementally adding shots); also compares different numbers of shots (0, 1, >1 such as 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Providing demonstration examples better specifies response style and helps handle edge cases; representative examples of all potential labels should be shown for classification tasks. Dynamically selecting semantically similar examples can further improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Recommendation: use 1–5 shots; start from zero-shot and add examples incrementally; demonstrations should be diverse and representative; examples include both input and desired output. Used in exemplar patient-to-trial matching prompt described in Figure 4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9314.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompting strategy where the model receives only the task instruction (and input) with no demonstration examples to elicit an answer directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language models are few-shot learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general; e.g., GPT-4, Llama 3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (e.g., question answering, trial matching)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where the model must perform given only an instruction and instance without examples.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Zero-shot prompt: instruction + input, no exemplars; recommended as the starting point before adding shots.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to one-shot and few-shot; paper recommends starting zero-shot then adding shots to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Zero-shot gives a baseline; adding shots can improve performance and handle edge cases, so zero-shot is a starting point rather than optimal for many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper recommends incremental approach: begin with zero-shot, then add examples as needed. No numeric performance reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9314.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt engineering technique that instructs the model to produce step-by-step reasoning (a rationale) before or alongside a short answer to elicit multi-step reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general; examples given include GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex reasoning/medical decision-making tasks (e.g., clinical trial eligibility, complex QA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multi-step domain reasoning where intermediate rationale helps reach a correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Add explicit instruction to generate rationale (e.g., 'Let's think step-by-step') and request the chain-of-thought rationale and then the short answer; prompt may request JSON containing rationale and answer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively to standard prompting without rationale; paper also references self-consistency (cited) as an enhancement to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>CoT prompting improves explainability and can improve performance on complex medical decision tasks by making the model's reasoning explicit, aiding clinician verification; recommended as default.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Example in Fig. 4: CoT used to produce criterion-level rationale and eligibility label for clinical trial matching; instruct model to output rationale then final label; recommendation to always ask model to 'think-step-by-step'.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9314.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt augmentation technique where relevant external documents (retrieved via search) are included in the prompt so the model can ground answers on provided evidence and up-to-date content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking retrieval-augmented generation for medicine</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general; e.g., GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge-intensive tasks (e.g., medical question answering, patient-to-trial matching requiring definitions or guidelines)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require factual, up-to-date domain knowledge beyond the model's parameterized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Include retrieved relevant textual snippets (articles, guidelines, systematic reviews) in prompt context so model grounds generation in those passages; used with search engine + prompt assembly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively against plain prompting without retrieval; suggested as a method to reduce hallucinations and update outdated model knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>By grounding responses on retrieved domain-specific documents, RAG reduces hallucinations and improves evidence-based, up-to-date answers; recommended to use high-quality domain corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper recommends RAG for knowledge-intensive tasks and suggests using systematic reviews, textbooks, and guidelines. Example: RAG to provide definitions for patient eligibility classification in trial matching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9314.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tool learning / function calling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tool learning (function calling to domain tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing LLMs access to external domain-specific tools or APIs (e.g., databases, calculators, EHR readers) via function-calling interfaces so the model can query or compute with structured utilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GeneGPT: augmenting large language models with domain tools for improved access to biomedical information</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs with function-calling capability (e.g., GPT-4 with tool calls)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks requiring specialized utilities or data access (e.g., reading raw EHRs, medical calculators, domain databases)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>When domain-specific computations or database lookups are needed to solve the task reliably and reproducibly.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Augment prompt/system with function-call hooks or tool descriptions and allow model to call these tools during completion; results are integrated into the prompt/context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared conceptually to pure-text prompting; tool learning can retrieve precise structured data not practical to encode in prompt text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Use of tools enables access to precise or sensitive data and domain utilities, improving completeness and accuracy for tasks where textual prompt alone is insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Example: in clinical-trial matching, provide tools to read raw EHR to extract missing details not present in patient summary. Function-calling mechanism recommended (Box 1 Best Practice 7).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9314.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temperature setting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sampling temperature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding parameter that controls randomness of LLM outputs; lower temperature yields more deterministic outputs and higher temperature yields more diverse outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Any generation task (e.g., clinical documentation, QA, summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Controls diversity vs determinism of generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Set temperature parameter (typically via API); recommended default temperature = 0 for deterministic, reproducible outputs; increase temperature only for diversity or ensembling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively between low (0) vs higher temperatures for reproducibility vs creative diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Lower temperatures produce consistent outputs useful for reproducibility and evaluation; higher temperatures may help ensemble diverse responses but reduce determinism.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Recommendation: start with temperature 0 for reproducibility; higher values used only when diversity or ensembling is desired (Box 1 Best Practice 8). Temperatures typically set via API or local parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9314.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Structured output / JSON formatting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured output formatting (e.g., JSON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt instruction to make model generate machine-parseable structured outputs (like JSON dictionaries) to facilitate automatic parsing and downstream processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Any task where automated parsing of LLM outputs is needed (e.g., patient-to-trial matching, structured extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Output format requirement to enable reliable downstream parsing and application integration.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Explicitly instruct model to return a JSON dictionary with designated fields (e.g., rationale, short answer, labels) to improve parseability and reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to free-form text outputs which are harder to parse automatically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Structured outputs (JSON) reduce parsing difficulty and increase reproducibility of downstream systems; recommended for development and deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper recommends instructing LLMs to generate structured JSON outputs (Box 1 Best Practice 8). Example outputs in Fig. 4 include CoT rationale and short answer organized in JSON.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9314.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-context issues (lost-in-the-middle)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lost-in-the-middle long-context failure</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed failure mode where LLMs with long input contexts fail to effectively utilize information appearing in the middle of very long prompts, reducing reliability for long-prompt tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lost in the middle: How language models use long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Long-context LLMs (e.g., GPT-4 long-context variants, Gemini, Llama 3 long context)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Tasks with very long prompts/inputs (e.g., RAG with many documents, multi-document summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring the model to utilize information distributed across large context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Very long prompts or concatenated documents spanning the model's context window (8k–1M tokens depending on model); problems arise when important facts are in the middle of the context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Empirical observations indicate LLMs may underutilize information in the middle of long prompts ('lost-in-the-middle'), degrading performance on long-context tasks; users must consider context window and information placement.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Paper notes context windows of models (e.g., Llama 3 8k, GPT-4 up to 128k, Gemini up to 1M tokens) and warns about long-context failure modes like lost-in-the-middle (cited). Recommends selecting models with adequate context windows and structuring prompts carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9314.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9314.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt engineering vs fine-tuning decision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt engineering first, then fine-tuning when needed</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Guideline that advanced prompt engineering (few-shot, CoT, RAG) should be tried before committing to fine-tuning; fine-tuning is recommended when prompt methods fail, sufficient labeled data exist, or prompt length/cost is prohibitive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (general; e.g., FLAN-T5 example in Table 3)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (e.g., summarization, clinical tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Decision process for whether to adapt models via prompting or update model weights (fine-tuning/PEFT).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt engineering techniques (few-shot, CoT, RAG, tool calls) attempted first; if insufficient, then consider full-model fine-tuning or parameter-efficient fine-tuning (LoRA/QLoRA).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Prompt-only approaches compared to PEFT / full fine-tuning approaches; paper discusses trade-offs but does not report numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prompt engineering can often solve tasks without modifying model weights; fine-tuning is recommended when prompt methods can't reach desired performance, when large high-quality data are available, or when prompt cost is too high. PEFT methods can be competitive and reduce hardware needs.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Situations for fine-tuning: 1) prompt engineering fails, 2) abundant high-quality training data, 3) prompt too long/costly. PEFT examples: LoRA, QLoRA; Van Veen et al. used QLoRA for clinical summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Demystifying Large Language Models for Medicine: A Primer', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
                <li>Benchmarking retrieval-augmented generation for medicine <em>(Rating: 2)</em></li>
                <li>GeneGPT: augmenting large language models with domain tools for improved access to biomedical information <em>(Rating: 1)</em></li>
                <li>Lost in the middle: How language models use long contexts <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9314",
    "paper_id": "paper-273549211",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "Few-shot learning (FSL)",
            "name_full": "Few-shot learning",
            "brief_description": "Prompting strategy that includes a small number (typically 1–5) of demonstration input-output examples (‘shots’) in the prompt to guide an LLM's behavior for the target task, improving specification of style and edge-case handling.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "LLMs (general; e.g., GPT-4, Llama 3)",
            "model_size": null,
            "task_name": "Various (e.g., patient-to-trial matching, classification, generation)",
            "task_description": "Tasks where demonstrating the desired input→output mapping within the prompt helps the model generalize to new instances.",
            "presentation_format": "Few-shot prompt: include 1–5 representative and diverse demonstration examples (shots) inside the prompt; examples should contain both input and desired output; dynamic selection of examples by semantic similarity is suggested.",
            "comparison_format": "Zero-shot and one-shot (the paper recommends starting from zero-shot and incrementally adding shots); also compares different numbers of shots (0, 1, &gt;1 such as 5).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Providing demonstration examples better specifies response style and helps handle edge cases; representative examples of all potential labels should be shown for classification tasks. Dynamically selecting semantically similar examples can further improve performance.",
            "null_or_negative_result": false,
            "experimental_details": "Recommendation: use 1–5 shots; start from zero-shot and add examples incrementally; demonstrations should be diverse and representative; examples include both input and desired output. Used in exemplar patient-to-trial matching prompt described in Figure 4.",
            "uuid": "e9314.0",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Zero-shot learning",
            "name_full": "Zero-shot prompting",
            "brief_description": "Prompting strategy where the model receives only the task instruction (and input) with no demonstration examples to elicit an answer directly.",
            "citation_title": "Language models are few-shot learners",
            "mention_or_use": "mention",
            "model_name": "LLMs (general; e.g., GPT-4, Llama 3)",
            "model_size": null,
            "task_name": "Various (e.g., question answering, trial matching)",
            "task_description": "Tasks where the model must perform given only an instruction and instance without examples.",
            "presentation_format": "Zero-shot prompt: instruction + input, no exemplars; recommended as the starting point before adding shots.",
            "comparison_format": "Compared conceptually to one-shot and few-shot; paper recommends starting zero-shot then adding shots to improve performance.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Zero-shot gives a baseline; adding shots can improve performance and handle edge cases, so zero-shot is a starting point rather than optimal for many tasks.",
            "null_or_negative_result": false,
            "experimental_details": "Paper recommends incremental approach: begin with zero-shot, then add examples as needed. No numeric performance reported in this paper.",
            "uuid": "e9314.1",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "Prompt engineering technique that instructs the model to produce step-by-step reasoning (a rationale) before or alongside a short answer to elicit multi-step reasoning.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (general; examples given include GPT-4)",
            "model_size": null,
            "task_name": "Complex reasoning/medical decision-making tasks (e.g., clinical trial eligibility, complex QA)",
            "task_description": "Tasks requiring multi-step domain reasoning where intermediate rationale helps reach a correct final answer.",
            "presentation_format": "Add explicit instruction to generate rationale (e.g., 'Let's think step-by-step') and request the chain-of-thought rationale and then the short answer; prompt may request JSON containing rationale and answer.",
            "comparison_format": "Compared qualitatively to standard prompting without rationale; paper also references self-consistency (cited) as an enhancement to CoT.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "null",
            "explanation_or_hypothesis": "CoT prompting improves explainability and can improve performance on complex medical decision tasks by making the model's reasoning explicit, aiding clinician verification; recommended as default.",
            "null_or_negative_result": false,
            "experimental_details": "Example in Fig. 4: CoT used to produce criterion-level rationale and eligibility label for clinical trial matching; instruct model to output rationale then final label; recommendation to always ask model to 'think-step-by-step'.",
            "uuid": "e9314.2",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A prompt augmentation technique where relevant external documents (retrieved via search) are included in the prompt so the model can ground answers on provided evidence and up-to-date content.",
            "citation_title": "Benchmarking retrieval-augmented generation for medicine",
            "mention_or_use": "mention",
            "model_name": "LLMs (general; e.g., GPT-4)",
            "model_size": null,
            "task_name": "Knowledge-intensive tasks (e.g., medical question answering, patient-to-trial matching requiring definitions or guidelines)",
            "task_description": "Tasks that require factual, up-to-date domain knowledge beyond the model's parameterized knowledge.",
            "presentation_format": "Include retrieved relevant textual snippets (articles, guidelines, systematic reviews) in prompt context so model grounds generation in those passages; used with search engine + prompt assembly.",
            "comparison_format": "Compared qualitatively against plain prompting without retrieval; suggested as a method to reduce hallucinations and update outdated model knowledge.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "By grounding responses on retrieved domain-specific documents, RAG reduces hallucinations and improves evidence-based, up-to-date answers; recommended to use high-quality domain corpora.",
            "null_or_negative_result": false,
            "experimental_details": "Paper recommends RAG for knowledge-intensive tasks and suggests using systematic reviews, textbooks, and guidelines. Example: RAG to provide definitions for patient eligibility classification in trial matching.",
            "uuid": "e9314.3",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Tool learning / function calling",
            "name_full": "Tool learning (function calling to domain tools)",
            "brief_description": "Providing LLMs access to external domain-specific tools or APIs (e.g., databases, calculators, EHR readers) via function-calling interfaces so the model can query or compute with structured utilities.",
            "citation_title": "GeneGPT: augmenting large language models with domain tools for improved access to biomedical information",
            "mention_or_use": "mention",
            "model_name": "LLMs with function-calling capability (e.g., GPT-4 with tool calls)",
            "model_size": null,
            "task_name": "Tasks requiring specialized utilities or data access (e.g., reading raw EHRs, medical calculators, domain databases)",
            "task_description": "When domain-specific computations or database lookups are needed to solve the task reliably and reproducibly.",
            "presentation_format": "Augment prompt/system with function-call hooks or tool descriptions and allow model to call these tools during completion; results are integrated into the prompt/context.",
            "comparison_format": "Compared conceptually to pure-text prompting; tool learning can retrieve precise structured data not practical to encode in prompt text.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Use of tools enables access to precise or sensitive data and domain utilities, improving completeness and accuracy for tasks where textual prompt alone is insufficient.",
            "null_or_negative_result": false,
            "experimental_details": "Example: in clinical-trial matching, provide tools to read raw EHR to extract missing details not present in patient summary. Function-calling mechanism recommended (Box 1 Best Practice 7).",
            "uuid": "e9314.4",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Temperature setting",
            "name_full": "Sampling temperature",
            "brief_description": "Decoding parameter that controls randomness of LLM outputs; lower temperature yields more deterministic outputs and higher temperature yields more diverse outputs.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (general)",
            "model_size": null,
            "task_name": "Any generation task (e.g., clinical documentation, QA, summarization)",
            "task_description": "Controls diversity vs determinism of generated text.",
            "presentation_format": "Set temperature parameter (typically via API); recommended default temperature = 0 for deterministic, reproducible outputs; increase temperature only for diversity or ensembling.",
            "comparison_format": "Compared qualitatively between low (0) vs higher temperatures for reproducibility vs creative diversity.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Lower temperatures produce consistent outputs useful for reproducibility and evaluation; higher temperatures may help ensemble diverse responses but reduce determinism.",
            "null_or_negative_result": false,
            "experimental_details": "Recommendation: start with temperature 0 for reproducibility; higher values used only when diversity or ensembling is desired (Box 1 Best Practice 8). Temperatures typically set via API or local parameterization.",
            "uuid": "e9314.5",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Structured output / JSON formatting",
            "name_full": "Structured output formatting (e.g., JSON)",
            "brief_description": "Prompt instruction to make model generate machine-parseable structured outputs (like JSON dictionaries) to facilitate automatic parsing and downstream processing.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (general)",
            "model_size": null,
            "task_name": "Any task where automated parsing of LLM outputs is needed (e.g., patient-to-trial matching, structured extraction)",
            "task_description": "Output format requirement to enable reliable downstream parsing and application integration.",
            "presentation_format": "Explicitly instruct model to return a JSON dictionary with designated fields (e.g., rationale, short answer, labels) to improve parseability and reproducibility.",
            "comparison_format": "Compared to free-form text outputs which are harder to parse automatically.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Structured outputs (JSON) reduce parsing difficulty and increase reproducibility of downstream systems; recommended for development and deployment.",
            "null_or_negative_result": false,
            "experimental_details": "Paper recommends instructing LLMs to generate structured JSON outputs (Box 1 Best Practice 8). Example outputs in Fig. 4 include CoT rationale and short answer organized in JSON.",
            "uuid": "e9314.6",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Long-context issues (lost-in-the-middle)",
            "name_full": "Lost-in-the-middle long-context failure",
            "brief_description": "Observed failure mode where LLMs with long input contexts fail to effectively utilize information appearing in the middle of very long prompts, reducing reliability for long-prompt tasks.",
            "citation_title": "Lost in the middle: How language models use long contexts",
            "mention_or_use": "mention",
            "model_name": "Long-context LLMs (e.g., GPT-4 long-context variants, Gemini, Llama 3 long context)",
            "model_size": null,
            "task_name": "Tasks with very long prompts/inputs (e.g., RAG with many documents, multi-document summarization)",
            "task_description": "Tasks requiring the model to utilize information distributed across large context windows.",
            "presentation_format": "Very long prompts or concatenated documents spanning the model's context window (8k–1M tokens depending on model); problems arise when important facts are in the middle of the context.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Empirical observations indicate LLMs may underutilize information in the middle of long prompts ('lost-in-the-middle'), degrading performance on long-context tasks; users must consider context window and information placement.",
            "null_or_negative_result": true,
            "experimental_details": "Paper notes context windows of models (e.g., Llama 3 8k, GPT-4 up to 128k, Gemini up to 1M tokens) and warns about long-context failure modes like lost-in-the-middle (cited). Recommends selecting models with adequate context windows and structuring prompts carefully.",
            "uuid": "e9314.7",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Prompt engineering vs fine-tuning decision",
            "name_full": "Prompt engineering first, then fine-tuning when needed",
            "brief_description": "Guideline that advanced prompt engineering (few-shot, CoT, RAG) should be tried before committing to fine-tuning; fine-tuning is recommended when prompt methods fail, sufficient labeled data exist, or prompt length/cost is prohibitive.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs (general; e.g., FLAN-T5 example in Table 3)",
            "model_size": null,
            "task_name": "Various (e.g., summarization, clinical tasks)",
            "task_description": "Decision process for whether to adapt models via prompting or update model weights (fine-tuning/PEFT).",
            "presentation_format": "Prompt engineering techniques (few-shot, CoT, RAG, tool calls) attempted first; if insufficient, then consider full-model fine-tuning or parameter-efficient fine-tuning (LoRA/QLoRA).",
            "comparison_format": "Prompt-only approaches compared to PEFT / full fine-tuning approaches; paper discusses trade-offs but does not report numeric comparisons.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Prompt engineering can often solve tasks without modifying model weights; fine-tuning is recommended when prompt methods can't reach desired performance, when large high-quality data are available, or when prompt cost is too high. PEFT methods can be competitive and reduce hardware needs.",
            "null_or_negative_result": false,
            "experimental_details": "Situations for fine-tuning: 1) prompt engineering fails, 2) abundant high-quality training data, 3) prompt too long/costly. PEFT examples: LoRA, QLoRA; Van Veen et al. used QLoRA for clinical summarization.",
            "uuid": "e9314.8",
            "source_info": {
                "paper_title": "Demystifying Large Language Models for Medicine: A Primer",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Benchmarking retrieval-augmented generation for medicine",
            "rating": 2,
            "sanitized_title": "benchmarking_retrievalaugmented_generation_for_medicine"
        },
        {
            "paper_title": "GeneGPT: augmenting large language models with domain tools for improved access to biomedical information",
            "rating": 1,
            "sanitized_title": "genegpt_augmenting_large_language_models_with_domain_tools_for_improved_access_to_biomedical_information"
        },
        {
            "paper_title": "Lost in the middle: How language models use long contexts",
            "rating": 2,
            "sanitized_title": "lost_in_the_middle_how_language_models_use_long_contexts"
        }
    ],
    "cost": 0.013573499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Demystifying Large Language Models for Medicine: A Primer</p>
<p>Qiao Jin 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Nicholas Wan 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Robert Leaman 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Shubo Tian 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Zhizheng Wang 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Yifan Yang 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Zifeng Wang 
Department of Computer Science
University of Illinois Urbana-Champaign
ChampaignILUSA</p>
<p>Guangzhi Xiong 
Department of Computer Science
University of Virginia
CharlottesvilleVAUSA</p>
<p>Po-Ting Lai 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Qingqing Zhu 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Benjamin Hou 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Maame Sarfo- Gyamfi 
Gongbo Zhang 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Department of Biomedical Informatics
Columbia University
New YorkNYUSA</p>
<p>Aidan Gilson 
School of Medicine
Yale University
New HavenCTUSA</p>
<p>Balu Bhasuran 
School of Information
Florida State University
TallahasseeFLUSA</p>
<p>Zhe He 
School of Information
Florida State University
TallahasseeFLUSA</p>
<p>Aidong Zhang 
Department of Computer Science
University of Virginia
CharlottesvilleVAUSA</p>
<p>Jimeng Sun 
Department of Computer Science
University of Illinois Urbana-Champaign
ChampaignILUSA</p>
<p>Chunhua Weng 
Department of Biomedical Informatics
Columbia University
New YorkNYUSA</p>
<p>Ronald M Summers 
Department of Radiology and Imaging Sciences
NIH Clinical Center
BethesdaMDUSA</p>
<p>Qingyu Chen 
School of Medicine
Yale University
New HavenCTUSA</p>
<p>Yifan Peng 
Department of Population Health Sciences
Weill Cornell Medicine
New YorkNYUSA</p>
<p>Ph.DZhiyong Lu zhiyong.lu@nih.gov 
National Library of Medicine (NLM)
National Institutes of Health (NIH)
BethesdaMDUSA</p>
<p>Senior Investigator National Library of Medicine National Institutes of Health 8600 Rockville Pike Bethesda
20894MDUSA</p>
<p>Demystifying Large Language Models for Medicine: A Primer
E86916F4DD40A8446F476E65AA4FE6DB
Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions.Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions.In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices.This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment.We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface.We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks.Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed.By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.</p>
<p>Introduction</p>
<p>Large language models (LLMs), exemplified by GPT-4 1 , Claude 3 2 , Gemini 1.5 3 , and Llama 3 4 , are artificial intelligence (AI) models that can generate human-like responses under various conversational contexts and adapt to novel tasks by following human instructions 5,6 .They have shown great promise in diverse biomedical and healthcare applications 7,8,9,10,11 , such as question answering 12,13,14,15 , clinical trial matching 16,17,18 , clinical documentation 19,20,21 , and multi-modal comprehension 22,23 .</p>
<p>Despite the accelerated progress of LLMs in patient care and clinical practice, biomedical and health sciences research, and education 8,9,24 , there is a noticeable lack of practical, actionable guidelines for their application from bench to bedside and beyond.A lot of current use of LLMs, such as ad-hoc prompting with ChatGPT, is far from sufficient in medical tasks 25,26 .This gap can lead to both underutilization and misapplication of these technologies, potentially affecting patient outcomes.Our work aims to close this gap by providing a detailed, structured framework to guide the utilization and integration of LLMs into medical workflows (Figure 1).Specifically, this framework consists of task formulation, model selection, prompt engineering, fine-tuning, and deployment considerations.We further provide a set of best practices (Box 1) while supporting adherence to ethical use, evaluation metrics, and compliance standards.The tutorial code that contains step-by-step instructions is publicly available at https://github.com/ncbi-nlp/LLM-Medicine-Primer.We hope this work will help equip healthcare professionals with the necessary knowledge to effectively leverage LLMs in their practices.</p>
<p>Figure 1.</p>
<p>Overview of the proposed systematic approach to utilizing large language models in medicine.Users need to first formulate the medical task and select the LLM accordingly.</p>
<p>Then, users can try different prompt engineering approaches with the selected LLM to solve the task.If the results are not satisfying, users can fine-tune the LLMs.After the method development, users also need to consider various factors at the deployment stage.</p>
<p>Corresponding best practices in Box 1 are also listed in each phase.</p>
<p>Box 1. Best Practices</p>
<ol>
<li>Organize your task into one of the five categories: (1) knowledge and reasoning, (2)   summarization, (3) translation, (4) structurization, and (5) multi-modal data analysis.</li>
</ol>
<p>Collect up to 100 test cases for evaluation with task-specific metrics.(Figure 2) 2. Ensure that the LLM usage is compliant to Health Insurance Portability and Accountability Act (HIPAA) if working with sensitive data.3. Choose an LLM that can process the data modality and has sufficient context length (length of the input prompt) used in the task.(Figure 3) 4. Select LLMs based on task-specific performance evaluated by experts in the literature.</p>
<p>Scores on medical examinations can be used for the initial screening of LLMs (Figure 3)</p>
<p>Task Formulation</p>
<p>Adapting an abstractive medical need into one or more concrete tasks that can be addressed by an LLM requires users to first understand the core capabilities of LLMs, which we classify into five broad categories: (1) knowledge and reasoning, (2) summarization, (3)   translation, (4) structurization, and (5) multi-modal data analysis.We recommend beginning by identifying the primary LLM capability relevant to your task.Once the task is formulated, one should aim to collect a diverse set of instances (test cases) that contain input and output data elements for development (Box 1, Best Practice 1).We recommend collecting about 100 test instances, following several evaluation studies of LLMs in medicine 14,20,27 .</p>
<p>Knowledge and reasoning</p>
<p>LLMs can use the medical knowledge encoded in their parameters to perform domainspecific reasoning given different contextual information 10,12,13,14,28,29 .This capability can enable a variety of medical applications, such as answering medical questions 30 , clinical decision support 31 , and matching patients to clinical trials 16,17,18 .Task instances usually include question (input), explanation (reasoning and context output), and short answer (output).When formulating a reasoning task, users can quickly evaluate short answers (e.g., yes/no) between LLM predictions and ground truth and proceed to in-depth analyses of explanations if short answers are satisfying 27 .can also be used to support multi-modal data analysis such as interpreting medical images.</p>
<p>Summarization</p>
<p>LLMs can summarize complex documents into concise paragraphs or sentences.</p>
<p>Summarization tasks in biomedicine primarily fall into two categories: (1) summarizing long clinical notes into shorter texts, such as generating the progress notes and discharge summaries 20,21 ; (2) summarizing medical literature for evidence synthesis, such as generating the systematic reviews given a list of clinical studies 32,33,34 .These labor-intensive tasks can be potentially streamlined by LLMs 19,35 .For a summarization task, instances include instruction (input), original text (input), and summarized text (output).Users may leverage metrics like BLEU 36 , ROUGE 37 , and BERTScore 38 to compare the generated texts and the reference summaries.However, it should be noted that automatic metrics do not always correlate well with the gold-standard human judgments 39 .</p>
<p>Translation</p>
<p>LLMs also have the capability to translate text, not only between different languages but also between writing styles appropriate for different audiences.This ability can enable applications such as sharing medical knowledge across different language demographics 40 , supporting medical education 41,42 , and facilitating communication with patients 43 .A translation task, similar to a summarization task, includes instances made of instruction (input), source text (input), and target text (output) and is evaluated similarly to a summarization task.</p>
<p>Structurization</p>
<p>LLMs can be leveraged to convert free-text input into structured outputs such as a list of keyvalue pairs.Medical structurization problems include classifying an input text into controlled vocabularies like the diagnosis-related groups 44 and extracting biomedical concepts as well as their relationships (e.g., variant-causing-disease) from unstructured text 45 .Structurization instances include task instruction (input), source text (input), and a list of extracted concepts and relations in structured forms (output).The evaluations are made to match the output with the reference answers.As such, the performance is often typically measured by precision, recall, and F1 score, etc.</p>
<p>Multi-modality</p>
<p>Multi-modal LLMs like GPT-4 can analyze and integrate diverse data types such as text, images, audio, and even genomic information, potentially serving as a generalist medical AI 22 .For example, these models can support clinical tasks, such as generating radiology reports and guiding clinical decisions 46,47,48 based on real-world multimodal patient data.</p>
<p>The multi-modal task instances and evaluations are similar to those of the knowledge and reasoning tasks, except that the input question typically contains data in multiple modalities (e.g., past medical history in EHR with current imaging results).</p>
<p>Large Language Model Selection</p>
<p>Users should choose an appropriate LLM based on their task characteristics.There are a wide variety of LLMs, including proprietary models such as GPT-4 1 , Gemini 3 , Claude 2 , and open-source models such as Llama 4 and Mistral 49 .They can range in size from several billion parameters (e.g., Llama3.1-8B) to hundreds of billion parameters (e.g., Llama3.1-405B).</p>
<p>Typically, larger models exhibit more proficient responses 50 .Some models are trained for more general applications, while others such as PMC-LlaMA 51 and MEDITRON 52 are finetuned for specific domains or applications.When choosing the LLM, users should consider three key factors in their needs: Task and Data, Performance Requirements, and Model Interface.Figure 3 shows an overview of the LLM selection considerations discussed in this section, and Table 1 shows the characteristics of commonly used LLMs.</p>
<p>Task and Data</p>
<p>The first and most critical factor to consider is the nature of the data the user is working with and the specific task to perform.Ensuring that the chosen model aligns with the data type and task requirements is foundational to successful LLM implementation.When working with sensitive patient data, it is important to ensure privacy and compliance with regulations such as Health Insurance Privacy and Accountability Act (HIPAA).Proprietary models accessed through APIs like OpenAI's GPT-4 are typically not HIPAA-compliant, so they should not be used for patient data.In contrast, certain cloud service providers such as Azure and Anthropic provide HIPAA-compliant access to LLMs, which could be potentially used for sensitive data.Alternatively, local models such as Llama or Mistral can be used for enhanced control over privacy and security when processing sensitive medical information.</p>
<p>Model Interface</p>
<p>Once the LLM(s) has been chosen, the users also need to select a point of access to the LLMs based on their needs.Broadly, there are three ways to access LLMs: web applications, model application programming interfaces (APIs), and locally hosted implementations.</p>
<p>Web applications, such as ChatGPT, are inexpensive and easy to use compared to APIs and Given the variety of LLMs available for medical applications, LLM selection requires careful considerations.While the supported data modalities and context windows are hard constraints, users must navigate trade-offs between model interface and medical capability based on their specific needs.For example, users may want to select proprietary LLMs of larger size when general capability has high priority.In contrast, users may opt to use local models when customizability is a concern.Ultimately, users must examine their task and select one (or more) LLM that satisfies their priorities.</p>
<p>Prompt engineering</p>
<p>Harnessing LLM capabilities for application to a specific task requires careful consideration of the prompt (input content) given to the model.Prompt engineering is the process of designing and optimizing prompts to effectively guide LLMs in generating accurate and coherent responses 64 .Prompts can vary from one simple instruction to many documents retrieved by a search system, allowing users to elicit a variety of behaviors without the need to modify the parameters of LLM.In general, more complex tasks typically require more sophisticated prompts.Figure 4 shows a concrete task example of clinical trial matching, where a simple prompt that merely describes the patient and lists the clinical trial criteria (shown in Fig. 4e) might not work well.As such, prompt engineering and fine-tuning methods should be used.Table 2 shows resource requirements, advantages, disadvantages, and use cases of different approaches.Some detailed case studies are also listed in Table 3. where the patient summary and the clinical trial eligibility criterion are given.f, An overview of fine-tuning, including when and how to perform it.g, The inputs to LLMs are known as "prompts", and their outputs are "completions".h, An example output of LLMs that contain the CoT rationale as well as the short answer, organized in the JSON format.n denotes the number of shots for few-shot learning, and N denotes the number of instances for finetuning.</p>
<p>Table 2. Characteristics of different approaches to use large language models in medicine.</p>
<p>Approach</p>
<p>Chain-of-thought (CoT) prompting</p>
<p>As shown in Fig. 4c, CoT prompting involves designing prompts that lead the model through a step-by-step reasoning process 69 .For example, providing the explanation of the patientcriterion relation helps the users efficiently verify the LLM-predicted eligibility labels.One can simply add "Let's think step-by-step" to the end of the input for CoT prompting.This technique is particularly useful in complex medical decision-making tasks, where an explanation of reasoning can improve model performance and aid clinicians in understanding and verifying AI-generated advice.We recommend using CoT prompting as the default as it improves the explainability of AI responses and potentially the performance as well (Box 1, Best Practice 6).</p>
<p>Retrieval-augmented generation (RAG)</p>
<p>In RAG (Fig. 4d), a search engine retrieves relevant documents, such as scientific articles, to be included in the prompt, allowing the model to better solve knowledge-intensive tasks such as answering questions 70 (Box 1, Best Practice 7).By grounding the LLMs to respond based on the provided relevant textual snippets, RAG can potentially reduce hallucinations 71 (the generation of incorrect information) and improve upon outdated knowledge encoded in large language models.For example, LLMs can get access to the definition of certain medical concepts with RAG to better classify the patient eligibility in the application of patient-to-trial matching.We recommend using high-quality domain-specific corpora, such as systematic reviews, medical textbooks, and clinical guidelines, for RAG systems in medicine 54 .</p>
<p>Tool learning</p>
<p>Certain medical tasks require the use of domain-specific tools such as database utilities or medical calculators.If these tools are implemented as application programming interfaces (APIs), LLMs can utilize them through a function calling mechanism (Box 1, Best Practice 7).</p>
<p>In the example case of clinical trial matching, LLMs can be provided with tools for reading raw electronic health records to capture detailed information that might be missing from a patient summary (shown in Fig. 4c).</p>
<p>Setting the temperature</p>
<p>Besides the prompt, LLMs also require a temperature parameter that controls the amount of randomness when generating the response.Lower temperatures result in a more consistent output, while higher temperatures lead to more creative responses.Temperature can usually be set via API or local parameterization, but usually not via Web app.We recommend that users start with a temperature of 0 to get deterministic results for reproducibility and only consider increasing the temperature to get diverse responses for purposes such as ensembling 72 (Box 1, Best Practice 8).</p>
<p>Additional considerations</p>
<p>There are several additional aspects that users should consider during prompt engineering, such as approaches for multi-modal data types and formatting the output.Effectively integrating various data types and crafting precise prompts is crucial for maximizing the utility of models like GPT-4.For example, single cell RNA sequencing data can be transformed into detailed textual prompts that include gene markers and expression levels 73 .Similarly, biosensor monitoring signals can also be transformed into texts to enable the generation of personal health insights and exercise recommendations 74 .These prompts help the model to accurately perform multi-modal data analysis.Users should also consider the output format during prompt engineering, with the primary consideration being the difficulty of automatically parsing the response output.We therefore recommend instructing LLMs to generate a structured output, such as a JSON dictionary, to allow the result to be easily parsed into different answer sections (Box 1, Best Practice 8).</p>
<p>Table 3. Representative case studies of utilizing large language models in medicine.</p>
<p>Study Task</p>
<p>Formulation</p>
<p>LLM(s) Selection Technique Evaluation</p>
<p>Van Veen et al. 20 Summarization FLAN-T5</p>
<p>Fine-tuning</p>
<p>Although LLMs can solve many tasks using prompt engineering without explicit model modification, there are at least three situations where fine-tuning may be considered: (1)     prompt engineering techniques like few-shot learning and RAG cannot sufficiently improve results, (2) high-quality training data is readily available in large scale, (3) the working prompt is too long to be feasible in terms of cost.(Box 1, Best Practice 9).</p>
<p>LLMs can be fully or partially fine-tuned.Full model fine-tuning updates all the parameters of an LLM, while parameter-efficient fine-tuning (PEFT) methods 83,84,85,86 , such as Low-Rank Adaptation (LoRA 83 ), update a subset of LLM parameters or add additional trainable weights to the LLM.In general, smaller and more specific data is suitable for PEFT to prevent overfitting 87 , while larger and more diverse data is suitable for full-scale fine-tuning to better train the model 75 .For example, Med-PaLM 2 28 used a diverse set of instances spanning medical exams, consumer health information, and medical research.The model utilizes both full fine-tuning and a novel method known as ensemble refinement, achieving high results on several benchmarks.PEFT greatly reduces the hardware requirements for finetuning.By using a small set of trainable parameters, quantized LoRA (QLoRA) 84 uses quantization and adapter methods 84,88  In summary, we suggest performing full or partial fine-tuning depending on computational resources and dataset features.In addition to open-source LLMs, some proprietary LLMs, such as GPT-4, can also be fine-tuned via file upload to their web applications.However, the implementation details of these fine-tuning approaches are unknown to the public and might raise concerns about transparency and privacy.Similar to prompt engineering approaches, fine-tuned models also need to be evaluated on an independent test set to verify the performance improvement of training.</p>
<p>Deployment considerations</p>
<p>Regulatory compliance</p>
<p>Deploying LLMs in the biomedical domain requires adherence to privacy standards such as HIPAA and the General Data Protection Regulation (GDPR 90 ) to protect patient data.When utilizing proprietary LLMs, it is essential to ensure that the platforms are HIPAA-compliant.</p>
<p>Alternatively, processing data locally using an open-source model can enhance data safety 7 .For example, while the OpenAI API is not currently compliant with HIPAA, Azure services provide HIPAA-compliant access to OpenAI's models.Similarly, Anthropic provides HIPAA-certified API hosting for its Claude models.AI algorithms like LLMs are potentially regulated as medical devices, especially when used in clinical settings or for decision support.This adds an additional layer of regulatory scrutiny, requiring compliance with relevant standards such as those established by the FDA or other regulatory bodies overseeing medical device software.In the end, users must maintain the ethical and legal integrity of their deployment by carefully selecting protocols that align with compliance requirements and clinical standards (Box 1, Best Practice 10).</p>
<p>Equity and fairness</p>
<p>Users should evaluate potential biases in LLM's training data and algorithms to ensure fair and equitable outcomes 91 .Prior work has shown that even the most successful proprietary models can exhibit racial bias.Studies have shown that, when presented with identical patient profiles differing only by race, LLMs can yield varying predictions for treatment, cost, or outcome.Such differences can result in healthcare disparities during production.Thus, it is necessary to evaluate model fairness before deployment 92,93 .When examining data or algorithms is not viable, such as in the case of many proprietary models, users may still use existing benchmark datasets for evaluation 94,95 .This approach provides an idea of whether the models are fair or biased, and to what extent they exhibit bias.</p>
<p>Costs</p>
<p>When considering the costs associated with deploying large language models, it is important to distinguish between proprietary and open-source models.Proprietary LLMs require usage fees for each request made to the service provider.In the case of OpenAI's GPT-4 model, this pay-as-you-go (PAYG) system processes each token at a cost of $0.03 per 1,000 prompt tokens for models with 8k context lengths, with additional charges for completion tokens at $0.06 per 1,000 tokens.For a typical MIMIC-III 91 discharge note containing around 4,000 tokens, processing would cost approximately $0.12 for prompt tokens, with additional costs depending on the response length generated by the model.Some proprietary model providers also offer customization services such as fine-tuning for an additional fee.Though proprietary models typically offer robust support, this pricing structure can cause delays in customization and updates.Utilizing services in this way does not guarantee access to the most advanced updates and limits customization, as new updates undergo extensive quality checks and alignments before companies release them.</p>
<p>In contrast, open-source LLMs involve procurement costs for the necessary hardware, like GPUs, and ongoing costs related to maintenance.While the up-front costs are higher when running the model locally, these can be offset by the lower ongoing costs.In addition, an open-source setup can offer additional benefits like the ability to fine-tune the model to specific applications with protected data and more control over system responsiveness and updates.However, users should consider the potential for increased latency and reduced throughput during periods of high local demand.Local devices running LLMs might not match the speed and response time of large companies hosting LLMs.</p>
<p>Post-deployment</p>
<p>After the deployment of LLMs in healthcare, continuous monitoring is essential (Box 1, Best Practice 10).Users should ensure that LLM outputs are responsibly used as support tools, not as independent replacements for the judgment of healthcare practitioners.Effective training programs 96 are crucial to help healthcare professionals understand how to interpret and utilize these outputs while managing potential risks.Additionally, the successful integration of LLMs in medical practices demands active collaboration with patients and local communities.This involves leveraging engagement methods, such as community advisory boards and patient panels, to gather meaningful feedback and perspectives 97 .</p>
<p>Such inclusive strategies help tailor LLM applications to the real-world diversity of patient experiences and enhance the effectiveness of these technologies in medical practice.</p>
<p>Conclusions</p>
<p>Large language models (LLMs) have the potential to revolutionize healthcare by enhancing clinical workflows, decision-making, and patient outcomes.However, their effective integration into medical practice requires a systematic and thoughtful approach.This tutorial provides a comprehensive framework for utilizing LLMs in medicine, emphasizing critical stages such as task formulation, model selection, prompt engineering, and deployment.By following these guidelines, healthcare professionals can maximize the benefits of LLMs while addressing challenges related to regulatory compliance, fairness, and cost.As AI continues to evolve, the careful application of these methods will be essential in ensuring that LLMs are used responsibly and effectively, ultimately improving the quality of care delivered to patients.</p>
<p>5 .
5
Use one to five representative and diverse examples in few-shot learning to better specify the response style and handle edge cases.(Figure 4) 6. Always ask LLMs to "think-step-by-step" and generate the rationale before the answer for better explainability and improved performance.(chain-of-thought prompting, Figure 4) 7. Use retrieval-augmented generation (RAG) or tool learning if knowledge or domain utilities are needed and to make evidence-based and up-to-date generation.(Figure 4) 8. Use a temperature of 0 and JSON formatting to generate reproducible and structured outputs that can be easily parsed.(Figure 4) 9. Consider fine-tuning when prompt engineering fails to reach the desired performance, the working prompt is too costly, or abundant training data is readily available.(Figure 4) 10.Safeguard against potential risks by monitoring fairness, equity, bias, and cost when deploying LLMs in the biomedical domain.</p>
<p>Figure 2 .
2
Figure 2.An overview of five common task formulations enabled by LLMs in medicine, with</p>
<p>Figure 3 .
3
Figure 3. Considerations of choosing the LLMs.Users need to choose LLMs that can handle</p>
<p>local models; however, they do not provide interfaces that allow flexible control of model behavior and large-scale evaluations.In addition, most LLM web applications do not have clear compliance with standards such as HIPAA, which further raises security issues when dealing with sensitive patient data.Consequently, we do not recommend using web applications during the development phase.In contrast, model APIs are controlled points of access via the web that provide an interface to proprietary models such as PaLM and open-source models like Llama 3.They are typically easier to use than implementing local LLMs, and some of the model APIs provide HIPAA-compliant services.Lastly, locally hosted LLM implementations are often derived from open-source models.In general, local models provide greater control and privacy 7 .For instance, accessing specific parameters and getting the raw prediction of the next token distribution is possible in a local model but often not with a model API or via Web applications.Overall, while cutting-edge proprietary LLMs often deliver better performance in general tasks, users may have less control over customization, privacy, and safety.Conversely, open-source LLMs allow for greater customization and security but demand more GPU resources and technical expertise for both the development and the deployment phases.</p>
<p>Figure 4 .
4
Figure 4.An overview of prompt engineering and fine-tuning techniques.a, Task examples</p>
<p>As shown in Fig.4a, FSL includes a few examples (i.e., "shots") within the prompt to better specify the task for the model5 .Each demonstration example should include both the input and desired output.In the case of patient-to-trial matching, the input contains patient information, clinical trial criterion, and the task instruction; the output contains the rationale and the criterion-level eligibility label.Zero, one, and more than one examples (e.g., five) are respectively denoted as zero-shot, one-shot, and few-shot learning and are the most commonly used in practice.The examples should be as representative and diverse as possible.For example, demonstrations of all potential labels (e.g., disease sub-types) should be shown for a classification task.Another useful approach to few-shot learning is to generate examples dynamically, based on semantic similarity to the instances being predicted44 .We recommend starting from zero-shot learning and incrementally adding examples to increase performance or deal with edge cases (Box 1, Best Practice 5).</p>
<p>Few</p>
<p>Table 1 .
1
Characteristics of different LLMs, sorted by the best reported MedQA-USMLE 53 (4 options) score.T: text; I: image; V: video; A: audio.The GPT-4 series includes GPT-4, GPT-4turbo, and GPT-4o.The GPT-3.5 series includes Codex and GPT-3.5-turbo.
LLMWeights SizeInterfaceModality ContextMedQAMed-Gemini ClosedNAWeb, APIT, I, V, A1M, 2M91.1% 6GPT-4ClosedNAWeb, APIT, I8k, 32k, 128k90.2% 8Med-PaLM 2 ClosedNAAPIT8k86.5% 28Llama 3Open8B, 70B, 405B API, Local T8k80.9% 54GPT-3.5ClosedNAWeb, APIT4k, 16k68.7% 55Med-PaLMClosed540BAPIT8k67.6% 14Gemini 1.0ClosedNAWeb, APIT, I, V32k67.0% 56</p>
<p>63e diversity of healthcare tasks necessitates processing various data modalities in addition to free texts.Radiology and pathology applications, for example, may require models that can interpret and generate insights from 2D or 3D medical images, which requires models beyond text-only LLMs like GPT-3.5 and Llama 3. Medical conversations produce audio data, which can be transcribed into text for processing by text-based LLMs.Genomics data, including DNA sequences and RNA expressions, require the knowledge of omics data interpretation58.Similarly, time-series data, such as monitoring vital signs, need models that can analyze long temporal patterns in structured EHR.While both genomics and time-series data can be represented as free-text, it remains unclear whether general LLMs can be noted that issues such as lost-in-the-middle59(where the model fails to utilize information in the middle of the prompt) also appear in long-context LLMs.such as MedQA-USMLE 53 , PubMedQA 62 , MedMCQA63, have been commonly used to evaluate the knowledge and reasoning capabilities of LLMs 14 .These benchmarks should only be used to filter out models that cannot meet basic performance standards.However, higher scores on these datasets do not necessarily translate to better clinical utility, as there are no choices provided in real-life applications.After a model passes initial screening via MCQ evaluations, it must be further assessed for clinical utility.This involves rigorous testing, such as randomized controlled trials, to ensure the model's outputs are trustworthy and beneficial in real-world healthcare settings.In summary, MCQ evaluation can be used to screen LLMs for basic medical capability in a scalable way, while clinical evaluation can provide a gold standard relevant to patient care at the cost of greater human effort.When selecting LLMs, we recommend that users choose LLMs based on clinically evaluated results.However, clinical evaluation is challenging and may not be readily available.In such scenarios, users should consider using medical examination results to guide the selection of LLMs for further clinical evaluations.(Box 1,
Performance RequirementsThe model's medical capabilities are one of the most critical factors to consider whenselecting LLMs. Typically, greater capabilities come with larger model sizes which requireBest Practice 4)like GPT-4 (128k tokens context window), Claude 3 (200k tokens), and Gemini 1.5 Pro (1Mtokens) can process approximately 320, 800, and 2,500 PubMed articles, respectively.
61fectively handle such inputs without further training or adaptation.Users should determine what data modalities are essential to their task and select an LLM that can support such modalities (Box 1, Best Practice 3).For tasks that involve large inputs, the length of the input data that the model can handle is crucial.Users must understand the length distribution of their datasets and choose an LLM with a context window that can accommodate the input data (Box 1, Best Practice 3).The title and abstract of a PubMed article, for instance, consist of roughly 250 words, or about 300-400 tokens (model input units).As demonstrated in the online tutorial, one token is about 0.8 words as text is often tokenized into subwords and individual characters.Some open-source models like Llama 3 have a limited context window (the longest input prompt it can process) of 8,000 tokens, which is about 20 abstracts.In contrast, long-context LLMs Selecting LLMs with appropriate context windows ensures efficient processing of long input, but it should more resources for development and customization.Conversely, smaller LLMs may not perform as well as their larger counterparts, but they are often more sustainable and less costly.While LLM capabilities vary across different model sizes, capabilities are also affected by the target applications.For example, LLMs are better for tasks that require medical knowledge and clinical reasoning but do not often outperform fine-tuned BERT models60in simpler tasks such as structurization61.There are two main approaches to evaluating the medical capabilities of LLMs: multi-choice question (MCQ) evaluation and clinical evaluation.Medical examination and questionanswering tasks,</p>
<p>that reduce fine-tuning memory requirements (e.g., from over 780GB to 48GB) while maintaining fixed model parameters.PEFT approaches are often competitive with full model fine-tuning methods and even outperform them in lowdata environments.For example, Van Veen et al used QLoRA to fine-tune LLMs for clinical text summarization with thousands of training instances and only one NVIDIA Quadro RTX</p>
<p>AcknowledgmentsThis research was supported by NIH Intramural Research Program, National Library of Medicine.Box 2. Glossary1. Large language model: A large language model is a type of artificial intelligence designed to process and generate human-like text.The model is built using deep learning techniques and is trained on text corpora to perform tasks such as language translation, summarization, and question answering.2. Instance: An instance refers to a specific example used for training or testing a model.Each instance typically includes input data and corresponding output, which the model is expected to predict or generate.Rationale:Rationale is text output produced by a language model that provides additional context or reasoning for an output answer or value.Multi-modal comprehension:Refers to the ability of models like GPT-4 to analyze and integrate diverse data types, including text, images, audio, and genomic information, for various applications such as medicine and research.5.Parameter: A parameter is a variable within a large language model that is learned from training data and used to make predictions.The value of a parameter is often adjusted to minimize error during the training phase.Context Window:Context window refers to the number of tokens that a language model can receive as input.Larger context windows can increase the ability to perform in-context learning within a large language model prompt.7.Token: A token is the smallest unit of information that a language model can process.Tokens often represent text information like single letters, spaces, sub-words, words, or phrases.According to empirical evidence, one token is about 0.75 of a word.8. Prompt engineering: Prompt engineering involves crafting inputs or "prompts" that guide large language models to generate desired outputs without changing their parameters.Effective prompt design can significantly enhance the performance of LLMs.12. Chain-of-thought prompting: Chain-of-thought prompting is a technique used with large language models to encourage step-by-step reasoning in their responses.By explicitly asking the model to think through the steps of a problem, it can provide more transparent and logically structured solutions.13.Temperature: Temperature controls the randomness of the generated responses from large language models.A higher temperature increases diversity in the output, leading to more creative and varied responses, while a lower temperature produces more predictable and consistent outputs.DisclosuresThe recommendations in this article are those of the authors and do not necessarily represent the official position of the National Institutes of Health.
. J Achiam, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>The claude 3 model family: Opus, sonnet, haiku. A Anthropic, Claude-3 Model Card. 2024</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. M Reid, arXiv:2403.055302024arXiv preprint</p>
<p>arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Feasibility of Using the Privacy-preserving Large Language Model Vicuna for Labeling Radiology Reports. P Mukherjee, B Hou, R B Lanfredi, R M Summers, Radiology. 309e2311472023</p>
<p>Opportunities and challenges for ChatGPT and large language models in biomedicine and health. S Tian, Brief Bioinform. 252023</p>
<p>Large language models in medicine. A J Thirunavukarasu, Nat. Med. 291930-1940 (2023</p>
<p>H Nori, N King, S M Mckinney, D Carignan, E Horvitz, arXiv:2303.13375Capabilities of gpt-4 on medical challenge problems. 2023arXiv preprint</p>
<p>Capabilities of gemini models in medicine. K Saab, arXiv:2404.184162024arXiv preprint</p>
<p>Can large language models reason about medical questions? Patterns. V Liévin, C E Hother, A G Motzfeldt, O Winther, 20245100943N. Y.</p>
<p>Can generalist foundation models outcompete special-purpose tuning? case study in medicine. H Nori, arXiv:2311.164522023arXiv preprint</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 6202023</p>
<p>Interpretable medical image visual question answering via multi-modal relationship graph learning. X Hu, Med. Image Anal. 971032792024</p>
<p>Matching Patients to. Q Jin, Clinical Trials with Large Language Models. arXiv. 2024</p>
<p>Scaling clinical trial matching using large language models: A case study in oncology. C Wong, Machine Learning for Healthcare Conference 846-862. PMLR2023</p>
<p>. M Wornow, arXiv:2402.051252024Zero-Shot Clinical Trial Patient Matching with LLMs. arXiv preprint</p>
<p>Large language models for reducing clinicians' documentation burden. K Roberts, Nat. Med. 302024</p>
<p>Adapted large language models can outperform medical experts in clinical text summarization. D Van Veen, Nat. Med. 2024</p>
<p>ChatGPT: the future of discharge summaries?. S B Patel, K Lam, Lancet Digit. Health. 52023</p>
<p>Foundation models for generalist medical artificial intelligence. M Moor, Nature. 6162023</p>
<p>Multimodal biomedical AI. J N Acosta, G J Falcone, P Rajpurkar, E J Topol, Nat. Med. 282022</p>
<p>Large language models in medicine: the potentials and pitfalls: a narrative review. J A Omiye, H Gui, S J Rezaei, J Zou, R Daneshjou, Ann. Intern. Med. 1772024</p>
<p>Improving large language models for clinical named entity recognition via prompt engineering. Y Hu, J. Am. Med. Inform. Assoc. 312024</p>
<p>Prompt engineering in consistency and reliability with the evidencebased guideline for LLMs. L Wang, NPJ Digit. Med. 7412024</p>
<p>Hidden flaws behind expert-level accuracy of multimodal GPT-4 vision in medicine. Q Jin, NPJ Digit Med. 71902024</p>
<p>How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment. A Gilson, JMIR Med. Educ. 9e453122023</p>
<p>Towards expert-level medical question answering with large language models. K Singhal, arXiv:2305.096172023arXiv preprint</p>
<p>How Will ChatGPT Affect Information Seeking from the Medical Literature?. Q Jin, R Leaman, Z Lu, Retrieve, Summarize, Verify, J. Am. Soc. Nephrol. 342023</p>
<p>Using AI-generated suggestions from ChatGPT to optimize clinical decision support. S Liu, J. Am. Med. Inform. Assoc. 302023</p>
<p>Evaluating large language models on medical evidence summarization. L Tang, NPJ Digit. Med. 61582023</p>
<p>Summarizing, simplifying, and synthesizing medical evidence using GPT-3 (with varying success). C Shaib, arXiv:2305.062992023arXiv preprint</p>
<p>Closing the gap between open source and commercial large language models for medical evidence summarization. G Zhang, NPJ Digit. Med. 72392024</p>
<p>AI-generated text may have a role in evidence-based medicine. Y Peng, J F Rousseau, E H Shortliffe, C Weng, Nat. Med. 292023</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proc. Assoc. Comput. Linguist. 2002</p>
<p>Rouge: A package for automatic evaluation of summaries. C.-Y Lin, Text summarization branches out. 2004</p>
<p>T Zhang, V Kishore, F Wu, K Q Weinberger, Y Artzi, Bertscore, Evaluating Text Generation with BERT. International Conference on Learning Representations. 2019</p>
<p>Automated Metrics for Medical Multi-Document Summarization Disagree with Human Evaluations. L L Wang, Proc. Assoc. Comput. Linguist. 2023</p>
<p>Scaling neural machine translation to 200 languages. Nature. 162024</p>
<p>Large language model (LLM)-driven chatbots for neuro-ophthalmic medical education. E Waisberg, J Ong, M Masalkhi, A G Lee, Eye. 382024</p>
<p>The role of ChatGPT, generative language models, and artificial intelligence in medical education: a conversation with ChatGPT and a call for papers. G Eysenbach, JMIR Med. Educ. 9e468852023</p>
<p>Using chatgpt to facilitate truly informed medical consent. F N Mirza, NEJM AI. 1AIcs2300145 (2024</p>
<p>DRG-LLaMA: tuning LLaMA model to predict diagnosis-related group for hospitalized patients. H Wang, C Gao, C Dantona, B Hull, J Sun, NPJ Digit. Med. 7162024</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, Nat. Commun. 1514182024</p>
<p>As artificial intelligence goes multimodal, medical applications multiply. E J Topol, Science. 38161392023</p>
<p>How to develop machine learning models for healthcare. P C Chen, Y Liu, L Peng, Nat. Mater. 182019</p>
<p>Quantitative Evaluation of Large Language Models to Streamline Radiology Report Impressions: A Multimodal Retrospective Analysis. R Doshi, Radiology. 310e2315932024</p>
<p>. A Q Jiang, arXiv:2401.040882024Mixtral of experts. arXiv preprint</p>
<p>Large language models: A survey. S Minaee, arXiv:2402.061962024arXiv preprint</p>
<p>PMC-LLaMA: toward building open-source language models for medicine. C Wu, J Am Med Inform Assoc. 312024</p>
<p>Meditron-70b: Scaling medical pretraining for large language models. Z Chen, arXiv:2311.160792023arXiv preprint</p>
<p>What Disease Does This Patient Have? A large-scale open domain question answering dataset from medical exams. D Jin, Appl. Sci. 1164212021</p>
<p>Benchmarking retrieval-augmented generation for medicine. G Xiong, Q Jin, Z Lu, A Zhang, 2024Findings of the Association for Computational Linguistics</p>
<p>MedAdapter: Efficient Test-Time Adaptation of Large Language Models towards Medical Reasoning. W Shi, arXiv:2405.030002024arXiv preprint</p>
<p>Gemini goes to med school: exploring the capabilities of multimodal large language models on medical challenge problems &amp; hallucinations. A Pal, M Sankarasubbu, arXiv:2402.070232024arXiv preprint</p>
<p>Holistic Evaluation of Language Models (HELM). 2024Stanford University</p>
<p>scGPT: toward building a foundation model for single-cell multi-omics using generative AI. H Cui, Nat. Methods. 212024</p>
<p>Lost in the middle: How language models use long contexts. N F Liu, Trans. Assoc. Comput. Linguist. 122024</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, Assoc. Comput. Linguist. 2019</p>
<p>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. Q Chen, arXiv:2305.163262023arXiv preprint</p>
<p>PubMedQA: A Dataset for Biomedical Research Question Answering. Q Jin, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>A large-scale multi-subject multi-choice dataset for medical domain question answering. A Pal, L K Umapathi, M Sankarasubbu, Medmcqa, Conference on Health, Inference, and Learning. PMLR2022</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, ACM Comput. Surv. 552023</p>
<p>GeneGPT: augmenting large language models with domain tools for improved access to biomedical information. Q Jin, Y Yang, Q Chen, Z Lu, Bioinformatics. 402024</p>
<p>EHRAgent: Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records. W Shi, ICLR 2024 Workshop on Large Language Model (LLM) Agents. </p>
<p>Augmenting large language models with chemistry tools. A Bran, Nat Mach Intell. 62024</p>
<p>Almanac -Retrieval-Augmented Language Models for Clinical Medicine. C Zakka, NEJM AI. 12024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Biomedical question answering: a survey of approaches and challenges. Q Jin, ACM Comput. Surv. 552022</p>
<p>Survey of hallucination in natural language generation. Z Ji, ACM Computing Surveys. 552023</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, arXiv:2203.111712022arXiv preprint</p>
<p>Assessing GPT-4 for cell type annotation in single-cell RNA-seq analysis. W Hou, Z Ji, Nat. Methods. 2024</p>
<p>Towards a Personal Health Large Language Model. Justin Cosentino, arXiv:2406.064742024arXiv preprint</p>
<p>Scaling instruction-finetuned language models. H W Chung, J. Mach. Learn. Res. 252024</p>
<p>Ul2: Unifying language learning paradigms. Y Tay, arXiv:2205.051312022arXiv preprint</p>
<p>. Alpaca Team Stanford, Alpaca, 2024</p>
<p>MedAlpaca--an open-source collection of medical conversational AI models and training data. T Han, arXiv:2304.082472023arXiv preprint</p>
<p>Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. W.-L Chiang, 2023</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, J. Mach. Learn. Res. 242023</p>
<p>Publicly available clinical BERT embeddings. E Alsentzer, Proceedings of the 2nd Clinical Natural Language Processing Workshop. the 2nd Clinical Natural Language Processing Workshop2019</p>
<p>A generalist vision-language foundation model for diverse biomedical tasks. K Zhang, Nat. Med. 2024</p>
<p>LoRA: Low-Rank Adaptation of Large Language Models. E J Hu, International Conference on Learning Representations. 2021</p>
<p>Qlora: Efficient finetuning of quantized llms. T Dettmers, Adv. Neural Inf. Process. Syst. 362024</p>
<p>Norm tweaking: high-performance low-bit quantization of large language models. L Li, Q Li, B Zhang, X Chu, Proc. AAAI Conf. AAAI Conf202438</p>
<p>Scaling down to scale up: A guide to parameter-efficient fine-tuning. V Lialin, V Deshpande, A Rumshisky, arXiv:2303.156472023arXiv preprint</p>
<p>Lora learns less and forgets less. D Biderman, arXiv:2405.096732024arXiv preprint</p>
<p>Fp8 quantization: The power of the exponent. A Kuzmin, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Adapted large language models can outperform medical experts in clinical text summarization. D Van Veen, Nat Med. 302024</p>
<p>General Data Protection Regulation (GDPR) -Legal Text. General Data Protection Regulation (GDPR). 2024</p>
<p>A survey of recent methods for addressing AI fairness and bias in biomedicine. Y Yang, J. Biomed. Inform. 1046462024</p>
<p>Large language models propagate race-based medicine. J A Omiye, J C Lester, S Spichak, V Rotemberg, R Daneshjou, NPJ Digit. Med. 61952023</p>
<p>Leveraging generative AI for clinical evidence synthesis needs to ensure trustworthiness. G Zhang, J Biomed Inform. 1531046402024</p>
<p>L Sun, arXiv:2401.05561Trustworthiness in large language models. 2024arXiv preprint</p>
<p>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. B Wang, Adv. Neural Inf. Process. Syst. 2023</p>
<p>The imperative for regulatory oversight of large language models (or generative AI) in healthcare. B Meskó, E J Topol, NPJ Digit. Med. 61202023</p>
<p>Health Care Equity in the Use of Advanced Analytics and Artificial Intelligence Technologies in Primary Care. C R Clark, J. Gen. Intern. Med. 362021</p>            </div>
        </div>

    </div>
</body>
</html>